<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9748 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9748</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9748</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-278740365</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.12201v1.pdf" target="_blank">How Reliable is Multilingual LLM-as-a-Judge?</a></p>
                <p><strong>Paper Abstract:</strong> LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced large language models assess generation results in alignment with human instructions. While these models serve as a promising alternative to human annotators, their reliability in multilingual evaluation remains uncertain. To bridge this gap, we conduct a comprehensive analysis of multilingual LLM-as-a-Judge. Specifically, we evaluate five models from different model families across five diverse tasks involving 25 languages. Our findings reveal that LLMs struggle to achieve consistent judgment results across languages, with an average Fleiss' Kappa of approximately 0.3, and some models performing even worse. To investigate the cause of inconsistency, we analyze various influencing factors. We observe that consistency varies significantly across languages, with particularly poor performance in low-resource languages. Additionally, we find that neither training on multilingual data nor increasing model scale directly improves judgment consistency. These findings suggest that LLMs are not yet reliable for evaluating multilingual predictions. We finally propose an ensemble strategy which improves the consistency of the multilingual judge in real-world applications.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9748.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9748.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human multilingual consistency (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluator consistency across parallel multilingual data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper treats human evaluators as the gold-standard baseline who judge parallel question-answer pairs consistently across languages, i.e., their judgments depend on content not language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multilingual evaluation (e.g., question answering, summarization, dialogue, translation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>n/a (human baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Described qualitatively as human annotators assessing parallel inputs with uniform criteria; no detailed experimental protocol or annotator counts provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Described conceptually as consistent judgments across languages (no numeric human-agreement values provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When replacing humans with LLM judges, language-invariant consistency is lost — LLM judgments vary by language rather than relying solely on content.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Figure 1 and text: human annotators evaluate parallel QA pairs consistently across languages, while LLM-as-a-Judge shows inconsistency across the same parallel inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The paper does not run parallel human annotator experiments; it treats human consistency as the expected baseline rather than an empirically measured comparator within this study.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Introduction and Figure 1 (human evaluators described as providing uniform criteria; contrasted with LLM inconsistency)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable is Multilingual LLM-as-a-Judge?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9748.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9748.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM multilingual inconsistency (main finding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inconsistent judgments of multilingual LLM-as-a-Judge across parallel inputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experimental finding that multiple LLMs produce judgments that vary across languages on parallel data, with average Fleiss' Kappa around 0.3 and substantial per-model and per-task variability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Question answering, math QA, summarization, dialogue generation, machine translation (parallel multilingual datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-3.5-turbo, GPT-4o-2024-08-06, Llama-3.3-70b, Qwen-2.5-72b, Aya-Expanse-32b</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pointwise evaluation prompts (English prompt with specified <eval_language>), two judgment criteria: Yes/No (binary accuracy) and Grade (1–5 score); prompts include role, rubric, output format; explanations optionally requested.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No empirical human annotation experiment run in this paper; human evaluation discussed as conceptual baseline (no numeric human annotations provided).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Fleiss' Kappa used to measure consistency across languages (reported average FK ≈ 0.3; example: GPT-4o FK=0.5424 on WikiLingua Yes/No and FK=0.3209 on XQuAD Grade).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of cross-lingual consistency: LLM judges do not reliably give the same judgment for content-equivalent inputs in different languages; consistency is far from ideal (Kappa << 1).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Table 2: many models show low FK across datasets; overall average Fleiss' Kappa ≈ 0.3; the paper highlights large variance between languages and models (GPT-4o best but still far from perfect).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>GPT-4o achieves substantially higher Kappa than other models on some datasets (e.g., WikiLingua Yes/No), and ensemble of open models improves consistency in practice; binary (Yes/No) judgments are more consistent than graded judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 4 (Main Result, Consistency Result) and Table 2</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable is Multilingual LLM-as-a-Judge?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9748.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9748.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Low-resource language degradation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Marked degradation of LLM-as-a-Judge performance on low-resource languages</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that LLM judges perform significantly worse on low-resource languages compared to high-resource/European languages, reducing reliability in multilingual evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Cross-lingual judgment consistency across multiple tasks (e.g., MGSM math QA, XQuAD QA)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Examples shown for Llama-3.3-70B, Aya-Expanse-32B, Qwen-2.5-72B, GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Same multiclass pointwise prompts across languages; consistency compared between English judgments and other languages using Cohen's Kappa.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human evaluators assumed to be consistent across languages (conceptual baseline); no human annotations presented per language in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's Kappa between English and other language judgments; example extreme: Llama-3.3-70B on MGSM Telugu vs English Cohen's Kappa = 0.002 (almost no agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM judges lose reliability and cross-lingual transfer in low-resource languages — judgments can diverge dramatically from English judgments (and thus from expected human-like invariance).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Cited example: on MGSM the Cohen's Kappa between Llama-3.3-70B judgments for Telugu and English is as low as 0.002; general trend of poor consistency for Arabic, Telugu, Swahili, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Even multilingual-specialized model Aya-Expanse-32B shows poor consistency on some low-resource languages and tasks; however, European languages (Spanish, German) show higher consistency across models.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'What Factors cause inconsistency? -> Correlation between Languages' and Figure 4</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable is Multilingual LLM-as-a-Judge?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9748.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9748.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grade vs binary divergence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graded (1–5) judgments are less consistent across languages than binary (Yes/No) judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that LLM-as-a-Judge achieves higher agreement for binary correctness judgments than for multi-point grade judgments, indicating loss of fine-grained judgment consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>All evaluated tasks (Yes/No vs Grade settings)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>All five evaluated models (GPT-4o, GPT-3.5, Qwen-2.5, Llama-3.3, Aya-Expanse)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pointwise prompts offering either binary Yes/No or 1–5 Grade outputs; sometimes with explanation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human graders implied to be able to assign consistent grades across languages (baseline), but no human grade experiments included.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Fleiss' Kappa for both Yes/No and Grade; paper computes ∆ = KappaYes/No − KappaGrade and shows most ∆ positive (Yes/No more consistent).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of reliable fine-grained scoring: LLM judges produce less cross-lingual consistency when asked to output detailed grades, meaning subtle distinctions humans make may be degraded.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Figure 3 and text: most gap values are positive; Grade judgments show lower Kappa values than binary judgments across models and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Binary judgments (Yes/No) often show substantially higher agreement, so restricting options can improve multilingual consistency of LLM judges; prompts that require explanations can partially improve grade consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Consistency Result' (Yes / No VS. Grade) and Figure 3</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable is Multilingual LLM-as-a-Judge?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9748.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9748.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Accuracy vs consistency mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM judges can be strict or inconsistent such that accuracy and cross-lingual consistency do not align</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds that higher per-language accuracy (or average grade) does not necessarily correlate with greater consistency across languages; e.g., GPT-4o may be stricter yielding lower accuracy but higher consistency in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>All evaluated tasks (correlation of accuracy/avg grade with Kappa)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o, Qwen-2.5, Llama-3.3, Aya-Expanse, GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pointwise evaluation; compute Spearman correlation between Accuracy (Yes/No) or Avg Grade and Fleiss' Kappa across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No direct human-vs-LLM experiment reported in this study; human correctness used as conceptual target (ground truth treated as candidate).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman correlation reported in Table 3; correlations vary by dataset and criterion (e.g., WikiLingua positive 0.7 for Yes/No, others are negative or zero).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Replacing humans with LLM judges can lose alignment between correctness and consistent rulings; an LLM may be consistent (or strict) but not necessarily accurate in human terms.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Table 3: correlations vary irregularly (some datasets show negative or zero correlation), indicating that higher accuracy doesn't imply higher multilingual consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>GPT-4o attains relatively high Kappa while sometimes showing lower accuracy by the authors' interpretation (attributed to stricter standards); ensemble methods can improve consistency without necessarily harming accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Consistency Result' (Acc / Avg VS. Kappa) and Table 3</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable is Multilingual LLM-as-a-Judge?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9748.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9748.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Higher scoring bias vs humans (Hada et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM evaluators tend to assign higher scores than human annotators in multilingual settings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited external finding that LLM-based evaluators systematically give higher scores compared to human annotations in multilingual evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Are large language model-based evaluators the solution to scaling up multilingual evaluation?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multilingual evaluation (general, as reported by the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLM evaluators (general; specific models in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Reported as a finding in the cited study (Hada et al. 2024); details are in that paper rather than reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Reported comparison in the cited study between LLM evaluator scores and human annotations (details in Hada et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Reported tendency (qualitative) that LLM evaluators assign higher scores than human annotations; exact numeric metrics not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Shift in score distribution compared to humans (systematic upward bias) — LLM judges may overrate outputs relative to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Mention in Related Work: 'Hada et al. (2024) observed that LLM evaluators tend to assign higher scores in multilingual settings compared to human annotations.'</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>This paper's experiments focus on consistency rather than absolute score differences; it notes that high consistency could still be uniformly incorrect, so upward bias is an orthogonal concern.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Related Work ('LLM-as-a-judge') paragraph referencing Hada et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable is Multilingual LLM-as-a-Judge?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9748.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9748.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt/explanation effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact of prompt design and requiring explanations on LLM vs human-like judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds that prompts which ask the LLM to provide explanations alongside predictions increase cross-lingual consistency compared to prediction-only prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Grade judgments across multilingual datasets (examples: XQuAD, WMT23)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Demonstrated results primarily with GPT-4o (also applicable to other evaluated LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Comparative prompt experiments: general vs specific rubrics, and prediction-only vs prediction+explanation; consistency (Kappa) measured.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Humans naturally produce explanations when asked; no human explanatory annotation experiment conducted here.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Fleiss' Kappa compared between prompt variants; Table 4 shows prompts with explanation produce higher Kappa (e.g., XQuAD Grade Kappa increases from 0.2517 to 0.3209 for GPT-4o with explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Without explanation prompts, LLMs lose some of the reasoning trace that helps align judgments across languages; asking for explanations partially restores more human-like consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Table 4: GPT-4o Grade Kappa improves when explanation is requested (① vs ② and ③ vs ④ comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Providing highly specific rubrics did not always improve consistency, suggesting diminishing returns from overly prescriptive instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Prompt Design' and Table 4</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable is Multilingual LLM-as-a-Judge?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9748.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9748.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model scale / multilingual train mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Increasing model size or explicit multilingual training does not reliably close the gap to human-like multilingual consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows no clear improvement in cross-lingual consistency from larger model scales or from using a multilingual-specialized model; larger models sometimes do better but results are inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multilingual judgment tasks (WMT23, XQuAD, MGSM, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Scale series: Qwen-2.5 (7b to 72b), Aya-Expanse (7b to 32b); also comparison across GPT-3.5 vs GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Cross-scale experiments measuring Fleiss' Kappa across languages and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No human-scale comparators (humans not scaled), human reliability assumed as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Fleiss' Kappa across models and scales; specific observations: no monotonic trend, 14b Qwen sometimes better than 72b on WMT23; Aya-Expanse 32b shows limited improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Expectation that simply enlarging models or training on multilingual corpora will match human cross-lingual consistency is not borne out; thus reliance on model scale/training does not recover human-like reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Section 'Model Scale' and Table 5: inconsistent trends across scales; Aya-Expanse specialized training did not guarantee better consistency on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Some larger or specialized models (GPT-4o in particular) do show superior consistency compared to smaller/open models, so scale/training can help in specific cases but is not a general solution.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Model Scale' and Table 5</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable is Multilingual LLM-as-a-Judge?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9748.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9748.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensemble improvement vs human parity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority-vote ensemble of open-source LLMs can improve consistency relative to individual LLM judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors propose and test an ensemble (majority vote) of three open-source LLMs which increases cross-lingual Fleiss' Kappa compared to the worst single-model choice, partially mitigating losses from using single LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multilingual judgment tasks (XQuAD, MGSM, WMT23, XDailyDialog, WikiLingua)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Ensemble of Llama-3.3-70B, Qwen-2.5-72B, Aya-Expanse-32B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Majority-vote aggregation of three open-source LLM judges; compared ensemble FK to minimum single-model FK and reported ∆ = Ens − Min.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No direct human ensemble comparison; ensemble is proposed to approach more reliable behavior similar to human panels.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Fleiss' Kappa reported for ensemble (Table 5); ensemble increases Kappa vs worst-case model in most cases (∆ positive for most datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Single LLM judge instability (language- and task-dependent) can be reduced but not eliminated by ensembling; still not a replacement for human evaluators in all settings.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Table 5: example ∆ values show ensemble improves FK in XQuAD (Yes/No Min 0.0748 -> Ens 0.3227, ∆=0.2479) but small or negative ∆ on some cases (e.g., small negative ∆ on WMT23 Grade).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Ensembling helps in many cases but is not uniformly superior; some datasets show negligible or slight negative improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'How to choose a Judge in the wild?' and Table 5</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable is Multilingual LLM-as-a-Judge?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9748.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9748.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Biases that differentiate LLM and human judges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Known LLM evaluator biases (position, length, authority, familiarity, self-enhancement) that cause divergences from human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references prior work documenting systematic biases of LLM evaluators (e.g., position bias, verbosity/length bias, authority bias, familiar-knowledge bias, self-enhancement) which can make LLM judgments differ from human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Ranking and evaluation tasks more generally (pairwise and pointwise evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>General LLM evaluators (cited prior studies apply to various LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prior studies identified how context/order/verbosity/source cues influence LLM evaluations; current paper cites these as additional sources of divergence from humans.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human evaluators are less susceptible to some of these automated biases (context-specific), so bias-induced divergences represent a loss vs human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Prior works measure position/length/authority bias with ranking/score differentials; this paper does not re-report those metrics but cites the body of work.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM-as-a-Judge is vulnerable to superficial contextual biases that humans may not share, leading to systematic differences (e.g., preferring verbose answers, favoring self-generated outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Related Work and Bias section mention position bias (Wang et al., 2024a; Shi et al., 2024), verbosity/length bias (Dubois et al., 2024), authority and familiar-knowledge biases (Park et al., 2024), and self-enhancement (Ye et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Some debiasing strategies exist (e.g., length-controlled adjustments, ensembles, regression corrections) but are not fully explored in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Related Work ('Bias') and citations throughout the paper</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable is Multilingual LLM-as-a-Judge?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Are large language model-based evaluators the solution to scaling up multilingual evaluation? <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Length-controlled alpacaeval: A simple debiasing of automatic evaluators <em>(Rating: 1)</em></li>
                <li>Replacing judges with juries: Evaluating llm generations with a panel of diverse models <em>(Rating: 2)</em></li>
                <li>Justice or prejudice? quantifying biases in llm-as-a-judge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9748",
    "paper_id": "paper-278740365",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Human multilingual consistency (baseline)",
            "name_full": "Human evaluator consistency across parallel multilingual data",
            "brief_description": "The paper treats human evaluators as the gold-standard baseline who judge parallel question-answer pairs consistently across languages, i.e., their judgments depend on content not language.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "multilingual evaluation (e.g., question answering, summarization, dialogue, translation)",
            "llm_judge_model": "n/a (human baseline)",
            "llm_judge_setup": "n/a",
            "human_evaluation_setup": "Described qualitatively as human annotators assessing parallel inputs with uniform criteria; no detailed experimental protocol or annotator counts provided in this paper.",
            "agreement_metric": "Described conceptually as consistent judgments across languages (no numeric human-agreement values provided in this paper).",
            "losses_identified": "When replacing humans with LLM judges, language-invariant consistency is lost — LLM judgments vary by language rather than relying solely on content.",
            "examples_of_loss": "Figure 1 and text: human annotators evaluate parallel QA pairs consistently across languages, while LLM-as-a-Judge shows inconsistency across the same parallel inputs.",
            "counterexamples_or_caveats": "The paper does not run parallel human annotator experiments; it treats human consistency as the expected baseline rather than an empirically measured comparator within this study.",
            "paper_reference": "Introduction and Figure 1 (human evaluators described as providing uniform criteria; contrasted with LLM inconsistency)",
            "uuid": "e9748.0",
            "source_info": {
                "paper_title": "How Reliable is Multilingual LLM-as-a-Judge?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLM multilingual inconsistency (main finding)",
            "name_full": "Inconsistent judgments of multilingual LLM-as-a-Judge across parallel inputs",
            "brief_description": "Experimental finding that multiple LLMs produce judgments that vary across languages on parallel data, with average Fleiss' Kappa around 0.3 and substantial per-model and per-task variability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Question answering, math QA, summarization, dialogue generation, machine translation (parallel multilingual datasets)",
            "llm_judge_model": "GPT-3.5-turbo, GPT-4o-2024-08-06, Llama-3.3-70b, Qwen-2.5-72b, Aya-Expanse-32b",
            "llm_judge_setup": "Pointwise evaluation prompts (English prompt with specified &lt;eval_language&gt;), two judgment criteria: Yes/No (binary accuracy) and Grade (1–5 score); prompts include role, rubric, output format; explanations optionally requested.",
            "human_evaluation_setup": "No empirical human annotation experiment run in this paper; human evaluation discussed as conceptual baseline (no numeric human annotations provided).",
            "agreement_metric": "Fleiss' Kappa used to measure consistency across languages (reported average FK ≈ 0.3; example: GPT-4o FK=0.5424 on WikiLingua Yes/No and FK=0.3209 on XQuAD Grade).",
            "losses_identified": "Loss of cross-lingual consistency: LLM judges do not reliably give the same judgment for content-equivalent inputs in different languages; consistency is far from ideal (Kappa &lt;&lt; 1).",
            "examples_of_loss": "Table 2: many models show low FK across datasets; overall average Fleiss' Kappa ≈ 0.3; the paper highlights large variance between languages and models (GPT-4o best but still far from perfect).",
            "counterexamples_or_caveats": "GPT-4o achieves substantially higher Kappa than other models on some datasets (e.g., WikiLingua Yes/No), and ensemble of open models improves consistency in practice; binary (Yes/No) judgments are more consistent than graded judgments.",
            "paper_reference": "Sections 4 (Main Result, Consistency Result) and Table 2",
            "uuid": "e9748.1",
            "source_info": {
                "paper_title": "How Reliable is Multilingual LLM-as-a-Judge?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Low-resource language degradation",
            "name_full": "Marked degradation of LLM-as-a-Judge performance on low-resource languages",
            "brief_description": "The paper documents that LLM judges perform significantly worse on low-resource languages compared to high-resource/European languages, reducing reliability in multilingual evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Cross-lingual judgment consistency across multiple tasks (e.g., MGSM math QA, XQuAD QA)",
            "llm_judge_model": "Examples shown for Llama-3.3-70B, Aya-Expanse-32B, Qwen-2.5-72B, GPT-4o",
            "llm_judge_setup": "Same multiclass pointwise prompts across languages; consistency compared between English judgments and other languages using Cohen's Kappa.",
            "human_evaluation_setup": "Human evaluators assumed to be consistent across languages (conceptual baseline); no human annotations presented per language in this study.",
            "agreement_metric": "Cohen's Kappa between English and other language judgments; example extreme: Llama-3.3-70B on MGSM Telugu vs English Cohen's Kappa = 0.002 (almost no agreement).",
            "losses_identified": "LLM judges lose reliability and cross-lingual transfer in low-resource languages — judgments can diverge dramatically from English judgments (and thus from expected human-like invariance).",
            "examples_of_loss": "Cited example: on MGSM the Cohen's Kappa between Llama-3.3-70B judgments for Telugu and English is as low as 0.002; general trend of poor consistency for Arabic, Telugu, Swahili, etc.",
            "counterexamples_or_caveats": "Even multilingual-specialized model Aya-Expanse-32B shows poor consistency on some low-resource languages and tasks; however, European languages (Spanish, German) show higher consistency across models.",
            "paper_reference": "Section 'What Factors cause inconsistency? -&gt; Correlation between Languages' and Figure 4",
            "uuid": "e9748.2",
            "source_info": {
                "paper_title": "How Reliable is Multilingual LLM-as-a-Judge?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Grade vs binary divergence",
            "name_full": "Graded (1–5) judgments are less consistent across languages than binary (Yes/No) judgments",
            "brief_description": "The paper reports that LLM-as-a-Judge achieves higher agreement for binary correctness judgments than for multi-point grade judgments, indicating loss of fine-grained judgment consistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "All evaluated tasks (Yes/No vs Grade settings)",
            "llm_judge_model": "All five evaluated models (GPT-4o, GPT-3.5, Qwen-2.5, Llama-3.3, Aya-Expanse)",
            "llm_judge_setup": "Pointwise prompts offering either binary Yes/No or 1–5 Grade outputs; sometimes with explanation generation.",
            "human_evaluation_setup": "Human graders implied to be able to assign consistent grades across languages (baseline), but no human grade experiments included.",
            "agreement_metric": "Fleiss' Kappa for both Yes/No and Grade; paper computes ∆ = KappaYes/No − KappaGrade and shows most ∆ positive (Yes/No more consistent).",
            "losses_identified": "Loss of reliable fine-grained scoring: LLM judges produce less cross-lingual consistency when asked to output detailed grades, meaning subtle distinctions humans make may be degraded.",
            "examples_of_loss": "Figure 3 and text: most gap values are positive; Grade judgments show lower Kappa values than binary judgments across models and datasets.",
            "counterexamples_or_caveats": "Binary judgments (Yes/No) often show substantially higher agreement, so restricting options can improve multilingual consistency of LLM judges; prompts that require explanations can partially improve grade consistency.",
            "paper_reference": "Section 'Consistency Result' (Yes / No VS. Grade) and Figure 3",
            "uuid": "e9748.3",
            "source_info": {
                "paper_title": "How Reliable is Multilingual LLM-as-a-Judge?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Accuracy vs consistency mismatch",
            "name_full": "LLM judges can be strict or inconsistent such that accuracy and cross-lingual consistency do not align",
            "brief_description": "The paper finds that higher per-language accuracy (or average grade) does not necessarily correlate with greater consistency across languages; e.g., GPT-4o may be stricter yielding lower accuracy but higher consistency in some cases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "All evaluated tasks (correlation of accuracy/avg grade with Kappa)",
            "llm_judge_model": "GPT-4o, Qwen-2.5, Llama-3.3, Aya-Expanse, GPT-3.5",
            "llm_judge_setup": "Pointwise evaluation; compute Spearman correlation between Accuracy (Yes/No) or Avg Grade and Fleiss' Kappa across languages.",
            "human_evaluation_setup": "No direct human-vs-LLM experiment reported in this study; human correctness used as conceptual target (ground truth treated as candidate).",
            "agreement_metric": "Spearman correlation reported in Table 3; correlations vary by dataset and criterion (e.g., WikiLingua positive 0.7 for Yes/No, others are negative or zero).",
            "losses_identified": "Replacing humans with LLM judges can lose alignment between correctness and consistent rulings; an LLM may be consistent (or strict) but not necessarily accurate in human terms.",
            "examples_of_loss": "Table 3: correlations vary irregularly (some datasets show negative or zero correlation), indicating that higher accuracy doesn't imply higher multilingual consistency.",
            "counterexamples_or_caveats": "GPT-4o attains relatively high Kappa while sometimes showing lower accuracy by the authors' interpretation (attributed to stricter standards); ensemble methods can improve consistency without necessarily harming accuracy.",
            "paper_reference": "Section 'Consistency Result' (Acc / Avg VS. Kappa) and Table 3",
            "uuid": "e9748.4",
            "source_info": {
                "paper_title": "How Reliable is Multilingual LLM-as-a-Judge?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Higher scoring bias vs humans (Hada et al.)",
            "name_full": "LLM evaluators tend to assign higher scores than human annotators in multilingual settings",
            "brief_description": "Cited external finding that LLM-based evaluators systematically give higher scores compared to human annotations in multilingual evaluations.",
            "citation_title": "Are large language model-based evaluators the solution to scaling up multilingual evaluation?",
            "mention_or_use": "mention",
            "task_domain": "Multilingual evaluation (general, as reported by the cited work)",
            "llm_judge_model": "LLM evaluators (general; specific models in the cited work)",
            "llm_judge_setup": "Reported as a finding in the cited study (Hada et al. 2024); details are in that paper rather than reproduced here.",
            "human_evaluation_setup": "Reported comparison in the cited study between LLM evaluator scores and human annotations (details in Hada et al.).",
            "agreement_metric": "Reported tendency (qualitative) that LLM evaluators assign higher scores than human annotations; exact numeric metrics not reproduced here.",
            "losses_identified": "Shift in score distribution compared to humans (systematic upward bias) — LLM judges may overrate outputs relative to human judgments.",
            "examples_of_loss": "Mention in Related Work: 'Hada et al. (2024) observed that LLM evaluators tend to assign higher scores in multilingual settings compared to human annotations.'",
            "counterexamples_or_caveats": "This paper's experiments focus on consistency rather than absolute score differences; it notes that high consistency could still be uniformly incorrect, so upward bias is an orthogonal concern.",
            "paper_reference": "Related Work ('LLM-as-a-judge') paragraph referencing Hada et al. (2024)",
            "uuid": "e9748.5",
            "source_info": {
                "paper_title": "How Reliable is Multilingual LLM-as-a-Judge?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Prompt/explanation effect",
            "name_full": "Impact of prompt design and requiring explanations on LLM vs human-like judgments",
            "brief_description": "The paper finds that prompts which ask the LLM to provide explanations alongside predictions increase cross-lingual consistency compared to prediction-only prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Grade judgments across multilingual datasets (examples: XQuAD, WMT23)",
            "llm_judge_model": "Demonstrated results primarily with GPT-4o (also applicable to other evaluated LLMs)",
            "llm_judge_setup": "Comparative prompt experiments: general vs specific rubrics, and prediction-only vs prediction+explanation; consistency (Kappa) measured.",
            "human_evaluation_setup": "Humans naturally produce explanations when asked; no human explanatory annotation experiment conducted here.",
            "agreement_metric": "Fleiss' Kappa compared between prompt variants; Table 4 shows prompts with explanation produce higher Kappa (e.g., XQuAD Grade Kappa increases from 0.2517 to 0.3209 for GPT-4o with explanation).",
            "losses_identified": "Without explanation prompts, LLMs lose some of the reasoning trace that helps align judgments across languages; asking for explanations partially restores more human-like consistency.",
            "examples_of_loss": "Table 4: GPT-4o Grade Kappa improves when explanation is requested (① vs ② and ③ vs ④ comparisons).",
            "counterexamples_or_caveats": "Providing highly specific rubrics did not always improve consistency, suggesting diminishing returns from overly prescriptive instructions.",
            "paper_reference": "Section 'Prompt Design' and Table 4",
            "uuid": "e9748.6",
            "source_info": {
                "paper_title": "How Reliable is Multilingual LLM-as-a-Judge?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Model scale / multilingual train mismatch",
            "name_full": "Increasing model size or explicit multilingual training does not reliably close the gap to human-like multilingual consistency",
            "brief_description": "The paper shows no clear improvement in cross-lingual consistency from larger model scales or from using a multilingual-specialized model; larger models sometimes do better but results are inconsistent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Multilingual judgment tasks (WMT23, XQuAD, MGSM, etc.)",
            "llm_judge_model": "Scale series: Qwen-2.5 (7b to 72b), Aya-Expanse (7b to 32b); also comparison across GPT-3.5 vs GPT-4o",
            "llm_judge_setup": "Cross-scale experiments measuring Fleiss' Kappa across languages and tasks.",
            "human_evaluation_setup": "No human-scale comparators (humans not scaled), human reliability assumed as baseline.",
            "agreement_metric": "Fleiss' Kappa across models and scales; specific observations: no monotonic trend, 14b Qwen sometimes better than 72b on WMT23; Aya-Expanse 32b shows limited improvement.",
            "losses_identified": "Expectation that simply enlarging models or training on multilingual corpora will match human cross-lingual consistency is not borne out; thus reliance on model scale/training does not recover human-like reliability.",
            "examples_of_loss": "Section 'Model Scale' and Table 5: inconsistent trends across scales; Aya-Expanse specialized training did not guarantee better consistency on many tasks.",
            "counterexamples_or_caveats": "Some larger or specialized models (GPT-4o in particular) do show superior consistency compared to smaller/open models, so scale/training can help in specific cases but is not a general solution.",
            "paper_reference": "Section 'Model Scale' and Table 5",
            "uuid": "e9748.7",
            "source_info": {
                "paper_title": "How Reliable is Multilingual LLM-as-a-Judge?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Ensemble improvement vs human parity",
            "name_full": "Majority-vote ensemble of open-source LLMs can improve consistency relative to individual LLM judges",
            "brief_description": "The authors propose and test an ensemble (majority vote) of three open-source LLMs which increases cross-lingual Fleiss' Kappa compared to the worst single-model choice, partially mitigating losses from using single LLM judges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Multilingual judgment tasks (XQuAD, MGSM, WMT23, XDailyDialog, WikiLingua)",
            "llm_judge_model": "Ensemble of Llama-3.3-70B, Qwen-2.5-72B, Aya-Expanse-32B",
            "llm_judge_setup": "Majority-vote aggregation of three open-source LLM judges; compared ensemble FK to minimum single-model FK and reported ∆ = Ens − Min.",
            "human_evaluation_setup": "No direct human ensemble comparison; ensemble is proposed to approach more reliable behavior similar to human panels.",
            "agreement_metric": "Fleiss' Kappa reported for ensemble (Table 5); ensemble increases Kappa vs worst-case model in most cases (∆ positive for most datasets).",
            "losses_identified": "Single LLM judge instability (language- and task-dependent) can be reduced but not eliminated by ensembling; still not a replacement for human evaluators in all settings.",
            "examples_of_loss": "Table 5: example ∆ values show ensemble improves FK in XQuAD (Yes/No Min 0.0748 -&gt; Ens 0.3227, ∆=0.2479) but small or negative ∆ on some cases (e.g., small negative ∆ on WMT23 Grade).",
            "counterexamples_or_caveats": "Ensembling helps in many cases but is not uniformly superior; some datasets show negligible or slight negative improvements.",
            "paper_reference": "Section 'How to choose a Judge in the wild?' and Table 5",
            "uuid": "e9748.8",
            "source_info": {
                "paper_title": "How Reliable is Multilingual LLM-as-a-Judge?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Biases that differentiate LLM and human judges",
            "name_full": "Known LLM evaluator biases (position, length, authority, familiarity, self-enhancement) that cause divergences from human judgments",
            "brief_description": "The paper references prior work documenting systematic biases of LLM evaluators (e.g., position bias, verbosity/length bias, authority bias, familiar-knowledge bias, self-enhancement) which can make LLM judgments differ from human evaluations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "task_domain": "Ranking and evaluation tasks more generally (pairwise and pointwise evaluations)",
            "llm_judge_model": "General LLM evaluators (cited prior studies apply to various LLMs)",
            "llm_judge_setup": "Prior studies identified how context/order/verbosity/source cues influence LLM evaluations; current paper cites these as additional sources of divergence from humans.",
            "human_evaluation_setup": "Human evaluators are less susceptible to some of these automated biases (context-specific), so bias-induced divergences represent a loss vs human judgments.",
            "agreement_metric": "Prior works measure position/length/authority bias with ranking/score differentials; this paper does not re-report those metrics but cites the body of work.",
            "losses_identified": "LLM-as-a-Judge is vulnerable to superficial contextual biases that humans may not share, leading to systematic differences (e.g., preferring verbose answers, favoring self-generated outputs).",
            "examples_of_loss": "Related Work and Bias section mention position bias (Wang et al., 2024a; Shi et al., 2024), verbosity/length bias (Dubois et al., 2024), authority and familiar-knowledge biases (Park et al., 2024), and self-enhancement (Ye et al., 2024).",
            "counterexamples_or_caveats": "Some debiasing strategies exist (e.g., length-controlled adjustments, ensembles, regression corrections) but are not fully explored in this study.",
            "paper_reference": "Related Work ('Bias') and citations throughout the paper",
            "uuid": "e9748.9",
            "source_info": {
                "paper_title": "How Reliable is Multilingual LLM-as-a-Judge?",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Are large language model-based evaluators the solution to scaling up multilingual evaluation?",
            "rating": 2,
            "sanitized_title": "are_large_language_modelbased_evaluators_the_solution_to_scaling_up_multilingual_evaluation"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Length-controlled alpacaeval: A simple debiasing of automatic evaluators",
            "rating": 1,
            "sanitized_title": "lengthcontrolled_alpacaeval_a_simple_debiasing_of_automatic_evaluators"
        },
        {
            "paper_title": "Replacing judges with juries: Evaluating llm generations with a panel of diverse models",
            "rating": 2,
            "sanitized_title": "replacing_judges_with_juries_evaluating_llm_generations_with_a_panel_of_diverse_models"
        },
        {
            "paper_title": "Justice or prejudice? quantifying biases in llm-as-a-judge",
            "rating": 1,
            "sanitized_title": "justice_or_prejudice_quantifying_biases_in_llmasajudge"
        }
    ],
    "cost": 0.015809249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>How Reliable is Multilingual LLM-as-a-Judge?
18 May 2025</p>
<p>Xiyan Fu 
Independent Researcher</p>
<p>Wei Liu wei.liu@h-its.org 
Heidelberg Institute for Theoretical Studies gGmbH</p>
<p>How Reliable is Multilingual LLM-as-a-Judge?
18 May 2025A5D207311A0E1BF06EAECDC8037E4A4DarXiv:2505.12201v1[cs.CL]
LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced large language models assess generation results in alignment with human instructions.While these models serve as a promising alternative to human annotators, their reliability in multilingual evaluation remains uncertain.To bridge this gap, we conduct a comprehensive analysis of multilingual LLM-as-a-Judge.Specifically, we evaluate five models from different model families across five diverse tasks involving 25 languages.Our findings reveal that LLMs struggle to achieve consistent judgment results across languages, with an average Fleiss' Kappa of approximately 0.3, and some models performing even worse.To investigate the cause of inconsistency, we analyze various influencing factors.We observe that consistency varies significantly across languages, with particularly poor performance in low-resource languages.Additionally, we find that neither training on multilingual data nor increasing model scale directly improves judgment consistency.These findings suggest that LLMs are not yet reliable for evaluating multilingual predictions.We finally propose an ensemble strategy which improves the consistency of the multilingual judge in real-world applications.</p>
<p>Introduction</p>
<p>The success of various approaches based on neural networks has inspired the development of robust evaluation methods to track advances in the field of NLP (Sai et al., 2022;Chang et al., 2024).Evaluation aims to assess the quality and performance of NLP models, typically performed using evaluation metrics.Prior metrics vary depending on tasks and evaluation aspects, such as accuracy and F1score for classification tasks, and BLUE (Papineni et al., 2002) and ROUGE (Lin, 2004) for generation tasks.While these metrics benefit evaluations for various downstream tasks, their reliance on human-annotated references and n-gram matching All question-answer pairs are parallel and perfectly aligned across languages.Human evaluators assess the results with uniform criteria.In contrast, LLM-as-a-Judge demonstrates inconsistency in its judgments, failing to maintain consistency across languages.</p>
<p>limits their flexibility and effectiveness.With the development of deep learning, pre-trained language model-based evaluations are introduced, such as BLEURT (Sellam et al., 2020) and BARTScore (Yuan et al., 2021).They assess output quality by using pre-trained language model representations and generation probability.</p>
<p>To offer more efficient and powerful evaluation, some researchers propose LLM-as-a-judge (Zheng et al., 2023;Li et al., 2024;Gu et al., 2024), which use powerful LLMs such as GPT4 (Achiam et al., 2023) to evaluate generated response.Fu et al. (2024) defined evaluation schemes in the prompt template, and rely on existing LLMs as a judge to offer an evaluation.To avoid the high cost and potential data leakage, Zhu et al. (2023) fine-tunes LLMs as their local evaluators.Existing works (Chiang and Lee, 2023) show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation.These methods are subsequently applied to the evaluation of vari-ous tasks (Shen et al., 2023;Fernandes et al., 2023).</p>
<p>Given its superior performance, LLM-as-a-Judge has been extended to multilingual scenarios, where LLMs are expected to evaluate responses across different languages (Rau et al., 2024).However, whether LLM-as-a-Judge is truly trustworthy for multilingual evaluation remains uncertain.A reliable multilingual judge should be consistent, i.e., its judgments should depend on the content of the response rather than the language in which it is presented.Figure 1 illustrates a multilingual Question Answering example, where question-answer pairs are parallel across various languages.A human annotator evaluates these responses consistently, without being influenced by language differences.To assess the reliability of multilingual LLM-as-a-Judge, we collect five datasets covering different tasks, each with parallel data across multiple languages.We evaluate five models and find that, despite achieving reasonable accuracy within each task, they all struggle to maintain consistent judgments across languages.</p>
<p>To further understand the factors affecting consistency, we analyze results across different dimensions.Notably, we observe that consistency scores for low-resource languages are significantly lower, even for multilingual LLMs designed for strong cross-lingual performance, such as Aya-Expanse (Dang et al., 2024).Furthermore, we find that the LLM's judgment consistency is influenced by its task-specific ability, highlighting the need to consider the alignment between the evaluation task and the model's domain expertise.Overall, our findings shed light on the challenges of using LLM-as-a-Judge in multilingual settings and provide insights for future research on improving its reliability.</p>
<p>Our main contributions are as following:</p>
<p>• We investigate the reliability of multilingual LLM-as-a-Judge by assessing its consistency across parallel multilingual data.Our findings reveal that LLMs struggle to provide consistent judgments across languages.</p>
<p>• We conduct a detailed analysis of factors that affect the LLM's consistency across languages.Experimental results show that multilingual LLM-as-a-Judge performs poorly in low-resource languages, and that the model's size and whether it undergoes multilingual training does not affect its consistency.</p>
<p>• We introduce an ensemble strategy to improve the consistency of the multilingual judge in real-world applications.</p>
<p>Preliminary</p>
<p>2.1 LLM-as-a-Judge LLM-as-a-Judge (Zheng et al., 2023) is a popular method that evaluates generated outputs without focusing on word-level matching or relying on highly cost human annotators.Instead, it uses powerful LLMs such as GPT4 (Achiam et al., 2023) for evaluations covering multiple dimensions.Following Gu et al. (2024), we define a typical LLM-as-a-Judge as:
p ← LLM(C ⊗ x) (1)
where x is the input data awaiting evaluation, C is the context of the input x, ⊗ is a combination operator that merges the input x with the context C, LLM is the model used for the judgment, and p is the evaluation results from the whole LLM-as-a-Judge process.The context C is usually a prompt template, containing (i) role definition, which defines the task of the LLM; (ii) evaluation rubric, which provides criteria and guidelines for evaluation; and (iii) output, which regulates output formats and contents.Figure 2 shows a prompt example in the English Question Answering task.Given the format of input x, LLM-as-a-Judge can be divided into two groups: (i) pointwise comparison (Gao et al., 2023), where x is a single candidate; (ii) pairwise comparison (Fu et al., 2024), where x is a pair involving candidate and reference.In this paper, we adopt pointwise evaluation for our experiments, as obtaining parallel multi-lingual candidates is challenging.Based on the format of the output, two judgment criteria exist: (i) Yes / No requires a binary judgment from LLMs, i.e., correct or incorrect.In this case, LLM-as-a-Judge solely focuses on accuracy.(ii) Score requires a discrete score from LLMs.Following Chiang and Lee (2023), we define the score range as 1-5 given its superior evaluation performance.We use both criteria for the following experiments.</p>
<p>Multilingual LLM-as-a-Judge</p>
<p>In practice, multilingual evaluation is essential for assessing outputs across different languages, e.g., multilingual summarization.However, finding human annotators proficient in multiple languages is both challenging and costly.To address this, LLMas-a-Judge is extended to Multilingual LLM-as-a-Judge.Compared to standard LLM-as-a-Judge, the input x in this framework can appear in multiple languages beyond English.Figure 1 illustrates an example.A reliable Multilingual LLM-as-a-Judge is expected to provide consistent judgments across parallel instances in different languages.</p>
<p>Experiment Setup</p>
<p>Models</p>
<p>We select five LLMs for experiments, including (i) GPT-3.5-turbo, GPT-4o-2024-08-06 (OpenAI, 2024), since they are leading closed-source models which achieve State-ot-the-art results in a large range of NLP tasks; (ii) Llama-3.3-70b(Dubey et al., 2024), Qwen-2.5-72b(Yang et al., 2024), well known open source models; and (iii) Ayaexpanse-32b (Dang et al., 2024), multilingual specific model.The model is carefully trained using multilingual data arbitrage, multilingual preference optimization, and model merging methods, aiming to achieve robust multilingual capabilities.All the above models are commonly used as judges (Gu et al., 2024).</p>
<p>Tasks and Datasets</p>
<p>Given our focus on exploring the consistency of LLM-as-a-judge in multilingual scenarios, we select datasets that contain parallel data across all tested languages.The parallel structure of the dataset ensures that the input information remains identical across instances, with language being the only variable.The selected datasets cover a variety of NLP tasks, including Question Answering (Artetxe et al., 2020), Math Question Answering (Shi et al., 2023), Summarization (Ladhak et al., 2020), Dialogue Generation (Liu et al., 2023), and Machine Translation (Kocmi et al., 2023), aiming to provide a comprehensive evaluation.Table 1 provides the details about these datasets.</p>
<p>Prompts</p>
<p>For each test sample, we select ground truth as evaluated answers.This is to ensure precise parallel data alignment across all languages.Judgment instructions are then constructed as described in Section 2 and subsequently adapted into final prompts tailored for different models.Full templates are provided in the Appendix A.1.</p>
<p>Existing studies (Sclar et al., 2024)  gual scenarios further amplify the challenges for LLM-as-a-Judge.Following (Ahuja et al., 2023), we adopt an English prompt with a specified target language indicated by '<eval_language>' within the prompt, given its superior performance.</p>
<p>Evaluation Metrics</p>
<p>In this study, we focus on whether the performance of multilingual LLM-as-a-Judge varies significantly across parallel data in different languages.</p>
<p>That is, whether it exhibits bias toward specific languages.Therefore, we select Fleiss' Kappa (FK), a statistical measure of inter-rater agreement for more than two raters, to measure the consistency of the LLM-as-a-Judge results across languages.</p>
<p>Here, we treat each model's output in a particular language as a rater's judgment.While this study focuses on the consistency of LLM-as-a-Judge across languages, a truly excellent multilingual judge must also ensure accuracy.High consistency alone does not guarantee correctness, as it can result from uniformly incorrect judgments.To address this, we incorporate quality metrics to complement our evaluation: (i) Accuracy (Acc): For Yes/No judgments we use accuracy to evaluate binary prediction.(ii) Average Grade (AG): For Grade judgment, we use average value to evaluate discrete grade prediction.Notably, since we treat the ground truth as the predicted output to ensure precise parallel data alignment, the average accuracy and grade are expected to be 100% and a score of 5, respectively.</p>
<p>4 How does multilingual LLM-as-a-Judge perform?</p>
<p>Main Result</p>
<p>Table 2 summarizes the performance of all multilingual LLMs-as-a-Judge across two judgment criteria: Yes/No and Grade.Based on Fleiss's Kappa metric, which measures consistency, GPT-4o achieves the highest performance, with a score of 0.5424 on WikiLingua for the Yes/No criterion and 0.3209 on XQuAD for the Grade criterion.</p>
<p>However, these values remain far from the ideal consistency value of 1, and the Kappa scores of other models are even lower.This highlights that even powerful LLMs struggle to act as fair and consistent multilingual judges.</p>
<p>In addition, we observe significant variance in judgment consistency across different model groups.GPT-4o demonstrates superior Fleiss' Kappa compared to other models, aligning with its state-of-the-art status in a wide range of NLP tasks.In contrast, GPT-3.5, a model from the same series as GPT-4o, exhibits notably lower consistency, with its Kappa scores typically around half of GPT-4o's for both judgment criteria.However, despite GPT-4o attaining the highest Kappa consistency values, its judgment accuracy is not always the best.This contradicts the expectation that a strong judge should excel in both evaluation metrics.We speculate that this discrepancy arises from GPT-4o applying stricter evaluation standards rather than reflecting weaker performance.Such strictness makes</p>
<p>Consistency Result</p>
<p>To gain a deeper understanding of the performance of multilingual LLM-as-a-Judge, we further analyze the trends of Kappa consistency under the following settings:</p>
<p>Acc / Avg VS.Kappa.We analyze the relationship between prediction performance which is measured by Accuracy for Yes/No and Average Score for Grade and consistency measured by Kappa values.Specifically, we compute the Spearman correlation between accuracy (or average score) and Fleiss' Kappa.Table 3 presents the results.We observe that the Spearman correlation varies inconsistently, depending on the evaluation tasks and judgment criteria.For the WikiLingua (WikiL) dataset, results show a positive correlation under two judgment criteria, 0.7 and 0.4 respectively.In contrast, other datasets present contrasting correlations, either positive or negative, two of them even 0.This suggests that higher prediction accuracy does not necessarily imply greater judgment consistency.</p>
<p>Yes / No VS.Grade.We further analyze the consistency, measured by Kappa values, across the two evaluation criteria: Yes / No and Grade.Specifically, we calculate the gap between the two criteria, defined as ∆ = Kappa Y es/N o -Kappa Grade .Figure 3 illustrates the gap across all datasets.We observe that most gap values are positive, i.e., consistency in Yes / No evaluations is consistently higher than in Grade evaluations.It indicates that grade judgment is more challenging than binary judgment.This result may be due to more options in the grade scale.In practice, limiting the options for LLM-asa-Judge may enhance its effectiveness in applications that demand high multilingual consistency.</p>
<p>What Factors cause inconsistency?</p>
<p>To further understand the inferior consistency of multilingual LLM-as-a-Judge observed in the main results, we investigate potential causes in this section.</p>
<p>Correlation between Languages</p>
<p>Existing works found that the training corpus of LLMs is usually dominated by English, so LLMs may perform strongly in English while being relatively weaker in other languages.Hence, we conduct an experiment to explore how close LLM-asa-Judge performs in non-English languages compared to English.Specifically, we calculate the consistency (using Cohen's Kappa1 ) between LLMs' judge results on English and those on other languages.We select three LLMs-GPT-4o, Qwen-2.5-70b, and Aya-Expanse-32b for experiments since they are a good mix of closed-source, open-source, and multilingual LLMs. Figure 4 shows Cohen's Kappa results of four tasks2 with two judge criteria.The consistency radar charts for all tasks exhibit noticeable convex and concave patterns, indicating that consistency results with English vary  across languages.Specifically, LLMs tend to show higher consistency with European languages.For example, on the XQuAD task, all judge results for Spanish and German show high consistency, with Cohen's Kappa values ranging from 0.30 to 0.61.This is likely due to (i) the LLMs' training corpus containing more data in these languages, and (ii) their linguistic proximity to English (belonging to the same language family).In contrast, LLMs struggle with low-resource languages like Arabic (ar) and Telugu (te).For instance, on the MGSM task, the Cohen's Kappa value between Llama-3.3-70B judge results for Telugu and English is as low as 0.002.This trend persists even with Aya-Expanse-32B, a multilingual LLM with strong capabilities.These findings suggest that we must be cautious when using LLM evaluation results for low-resource languages, as they may be unreliable.</p>
<p>Impact of the judged task</p>
<p>Figure 4 also shows that the radar charts vary significantly across different tasks.Specifically, on the XQuAD task, the consistency between LLMs' judge results on English and other languages generally ranges from 0.2 to 0.4, with GPT-4o and Qwen-2.5-72bperforming the best.In contrast, the consistency results on the MGSM task drop to around 0.2, and the results of Qwen-2.5-72b and Aya-Expanse-32b for some languages are even close to 0 in terms of consistency with the results in English.However, on the WikiLingua task, the consistency results (in the Yes/No setting) climb to as high as 0.8.This suggests that when choosing a multilingual LLM-as-a-Judge for tasks, one should consider the LLM's task-related capabilities.The results of Aya-Expanse-32b confirm this to some extent.Aya-Expanse-32b is an LLM carefully trained to aim for strong multilingual capacities.However, surprisingly, it shows the worst consistency between judge results on English and other languages, especially on the MGSM task.We speculate that this is because Aya-Expanse-32b has not been primarily trained to solve reasoning and mathematical problems.This leads to its poor performance when evaluating the MGSM task, which consists of mathematical questions.Furthermore, we find that GPT-4o exhibits the best consistency across all tasks and languages, indicating its superiority in building multilingual LLM-as-a-Judge.</p>
<p>Prompt Design</p>
<p>Existing research (Sclar et al., 2024) has identified prompt design as a key factor in LLM-as-a-Judge performance.Therefore, we investigate how prompt design influences multilingual judgment consistency.As described in Section 2, the instruction prompt in this work consists of three components: role definition, evaluation rubric, and output format.Since the role definition of LLM-as-a-Judge is generally static, our experiments primarily focus on the latter two components.For the evaluation rubric, we tested: (i) a general rubric, which defines a grading scale with simplified descriptions for evaluation, and (ii) a specific rubric, which defines a grading scale where each grade is accompanied by detailed rules and explanations.For the output format, we tested: (i) prediction only, where LLMs output a simple binary prediction or evaluation grade, and (ii) prediction with explanation, where LLMs provide both the prediction and the reasoning behind their judgment.Table 4 shows the results for different prompt designs by combining these two factors.By comparing the consistency values (Kappa) between prompts with and without explanation generation (i.e., ① vs. ② and ③ vs. ④), we observe that prompts with explanation generation consistently achieve superior results.This indicates that generating explanations to support judgments can enhance evaluation consistency across all languages.Additionally, we compare prompts with general and specific rubrics (i.e., ① vs. ③ and ② vs. ④).Interestingly, we find that providing specific rules does not always improve consistency.We speculate that this may be because LLMs are already familiar with commonly used tasks, making very specific rubrics unnecessary in certain cases.</p>
<p>Model Scale</p>
<p>We further investigate whether the scale of LLMs affects inconsistency across languages.Specifically, we examine the open-access model Qwen-2.5, which ranges from 7 billion to 72 billion parameters, and the multilingual-specific model Aya-Expanse, which ranges from 7 billion to 32 billion parameters.Table 5 presents the results.</p>
<p>For Qwen-2.5 across different model scales, we do not observe any consistent trend.On the WMT23 dataset, the 14-billion-parameter Qwen-2.5 model even achieves higher consistency com- pared to the 72-billion version.Additionally, while the 32-billion Aya-Expanse outperforms its smaller counterparts, its improvement on WMT23 remains limited.These findings suggest that increasing the model scale does not directly lead to enhanced consistency in multilingual LLM-as-a-Judge.</p>
<p>6 How to choose a Judge in the wild?</p>
<p>Existing results show that Multilingual LLM-as-a-Judge exhibits varying consistency across different languages and tasks.This raises a natural question: How can we choose a suitable LLM-as-a-Judge for real-world applications to ensure relatively consistent evaluations across languages?Table 2 indicates that GPT-4o generally achieves the highest consistency, making it an ideal choice.However, its high cost and potential risk of data leakage pose challenges.To address this, we propose an Ensemble strategy that leverages a majority vote among open-source LLMs for judgment, inspired by Verga et al. (2024);Raina et al. (2024).Specifically, we conduct experiments using three open-source LLMs: Llama-3.3-70B,Qwen-2.5-72B, and Aya-Expanse-32B, taking their majority vote as the final prediction.The ensemble results (Ens) are shown in Table 5.For comparison, we also report the minimum value (Min) among the three models, representing the worst-case scenario when the least reliable judge is unknowingly selected.Furthermore, we compute the gap between the ensemble results and the minimum value, denoted as ∆ = Ens − M in, which reflects the improvement over the worst-case performance.As shown in can enhance consistency in real-world applications where the least reliable LLM might be unknowingly chosen.</p>
<p>7 Related Work</p>
<p>LLM-as-a-judge</p>
<p>With the remarkable performance of LLMs, researchers have increasingly leveraged them to evaluate generation results in alignment with human instructions (Zheng et al., 2023), known as LLM-asa-judge.To apply LLM-as-a-judge, it is common to start using In-Context Learning (Brown et al., 2020) methods with advanced LLMs, such as GPT-4 (Achiam et al., 2023).Li et al. (2024) categorized evaluation prompts into two primary groups: (i) pairwise comparison, where an LLM is given two candidates along with context to determine which response is superior (Gao et al., 2023); and (ii) pointwise evaluation, where an LLM assesses a single candidate based on specified evaluation criteria (Fu et al., 2024).To further enhance LLMs' judging capabilities, other line works apply preference learning techniques (Wang et al., 2024b;Wu et al., 2024) and fine-tuning mechanism (Zhu et al., 2023).These methodologies have been extensively applied across various tasks, including summarization (Shen et al., 2023;Wang et al., 2023), translation (Kocmi and Federmann, 2023;Fernandes et al., 2023), and written discourse coherence (Naismith et al., 2023).The widespread adoption of LLM-as-a-judge raises questions about its reliability and effectiveness.Addressing this, Chiang and Lee (2023) validated its efficacy by comparing evaluation outcomes from human judges and LLMas-a-judge, further highlighting its potential to significantly enhance efficiency.As a complement to existing research, we focus on LLM-as-a-Judge in multilingual scenarios.Recently, Hada et al. (2024) observed that LLM evaluators tend to assign higher scores in multilingual settings compared to human annotations.Unlike their work, we investigate the reliability of LLMs-as-a-Judge by examining their consistency across different languages using parallel multilingual data.</p>
<p>Bias</p>
<p>Despite the success of LLM-based evaluators, there have been studies showing that they have some biases (Zheng et al., 2023).One well-explored bias is position bias (Wang et al., 2024a;Shi et al., 2024) that the evaluation ranking of candidate responses can be easily hacked by altering their order of appearance in the context.</p>
<p>Conclusion</p>
<p>In this paper, we conduct an in-depth analysis of multilingual LLM-as-a-Judge, focusing on the consistency of its judgments across parallel data in different languages.Our results show that even advanced LLMs struggle with consistent judgment, exhibiting significant variance across languages.Moreover, neither larger model scales nor specific multilingual training improves judgment reliability.Our comprehensive analysis provides novel insights into multilingual LLM-as-a-Judge.</p>
<p>For LLM-as-a-Judge, we focus on pointwise judgment, as obtaining parallel multilingual incorrect candidates is challenging.This limits its applicability in real-world scenarios.An interesting avenue for future work would be to construct a parallel pairwise corpus for evaluation.Moreover, due to GPU constraints, we evaluate only open-access models up to approximately 70 billion parameters.Future work will explore judgments from larger LLMs.</p>
<p>Figure 1 :
1
Figure 1: Inconsistency in multilingual LLM-as-a-Judge.Left part shows a multilingual Question Answering example.All question-answer pairs are parallel and perfectly aligned across languages.Human evaluators assess the results with uniform criteria.In contrast, LLM-as-a-Judge demonstrates inconsistency in its judgments, failing to maintain consistency across languages.</p>
<p>Figure 3 :
3
Figure 3: Fleiss Kappa value gap (∆) between Yes / No and Grade evaluation criteria of various multilingual LLM-as-a-Judge models.</p>
<p>Figure 4 :
4
Figure 4: Consistency (Cohen's Kappa) of LLMs' judge results between English and other languages across four datasets and two judge criteria, Yes / No and Grade.</p>
<p>Figure 5 :
5
Figure 5: Variation of Fleiss Kappa for Grade judgment (K Grade ) across Qwen-2.5 and Aya-Expanse in different model scale.</p>
<p>You are an AI assistant whose purpose is to evaluate the correctness of answers to questions in <eval_language>.Given a context, a question, and an answer, your goal is to judge whether the generated answer is correct according to the provided context.Your evaluation should consider correctness and helpfulness.Do not allow the length of the responses to influence evaluation.Do not favor certain names of the assistants.Be as objective as possible.
Please format your response as follows:<result><justification>[Explain why select the grade for the answer.Use one or two sentences at most. Keep explaination asconcise as possible.]</justification><answer>[correct or incorrect]</answer></result>Context: <context>Question: <question>Answer: <answer>Figure 2: Prompt template for using LLM-as-a-Judge in a Question Answering task. Placeholders<eval_language>, <context>, <question>, <answer>are replaced by the input language, and its correspond-ing context, question and answer. The text in the promptis color-coded to represent different sections: for roledefinition, for evaluation rubric, for output.</p>
<p>Table 1 :
1
Datasets for multilingual LLM-as-a-Judge evaluation, all involving parallel data across provided languages.Num indicates the number of data samples in one language.
DatasetTaskAnswer TypeLanguagesNumXQuAD Artetxe et al. (2020)Question AnsweringExtractive SpanEnglish, German, Russian, Spanish, Chinese, Vietnamese, Turkish, Greek, Romanian, Thai, Hindi1191MGSM Shi et al. (2023)Math Question Answering SentenceSpanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, Telugu250WMT23 Kocmi et al. (2023)Machine TranslationSentenceEnglish, Chinese, German, Japanese, Russian, Czech, Ukrainian, Hebrew196WikiLingua Ladhak et al. (2020)SummarizationSentenceEnglish, Spanish, Castilian, Portuguese, French, German, Vietnamese, Thai, Japanese, Korean, Hindi, Czech, Turkish Russian, Italian, Indonesian, Dutch, Flemish, Arabic, Chinese,142XDailyDialog Liu et al. (2023)Dialogue GenerationSentenceEnglish, Italian, Chinese, German996</p>
<p>Table 2 :
2
have highlighted the critical role of prompt selection, as it significantly impacts final performance.Multilin-Performance of multilingual LLM-as-a-Judge across five datasets, evaluated on two settings: (i) Yes/No,
ModelXQuADMGSMWMT23XDailyDialogWikiLinguaAccFKAccFKAccFKAccFKAccFKAya-Expanse 96.86 0.2999 56.29 0.1895 92.64 0.1307 86.90 0.3812 89.87 0.3421Yes / NoLlama-3.3 Qwen-2.5 GPT-3.5 GPT-4o79.03 0.0748 64.25 0.0991 53.57 0.1463 74.50 0.2425 59.78 0.2325 93.47 0.3620 75.93 0.2631 92.42 0.0775 78.31 0.3093 67.68 0.3531 97.67 0.1399 74.51 0.1855 94.17 0.1327 83.46 0.2127 56.14 0.1748 92.04 0.3694 84.98 0.2352 85.88 0.1691 79.92 0.3692 65.57 0.5424AvgFKAvgFKAvgFKAvgFKAvgFKAya-Expanse 4.86 0.2399 3.70 0.0260 4.58 0.1434 4.44 0.3049 4.46 0.1865GradeLlama-3.3 Qwen-2.5 GPT-3.54.64 0.1558 3.64 0.1084 3.18 0.2082 3.73 0.1635 3.50 0.1412 4.72 0.2926 4.62 0.0654 4.79 0.1471 4.23 0.2602 3.63 0.2946 4.71 0.0971 3.57 0.0660 4.36 0.1039 4.06 0.1240 3.23 0.0487GPT-4o4.57 0.3209 3.66 0.2041 4.57 0.1281 4.24 0.2405 3.07 0.2803
with binary evaluation accuracy (Acc), and (ii) Grade, with average grade value (Avg) ranging from 1 to 5. Fleiss's Kappa (FK) is calculated for both settings to measure judgment consistency across parallel data.</p>
<p>Table 3 :
3
Spearman Correlation across five datasets for two judgment criteria: (i) Yes/ No, the correlation between accuracy and kappa; and (ii) Grade, the correlation between average value and kappa.
XQuAD MGSM WMT23 XDailyD WikiLYES/No00.6-0.5-0.30.7Grade-0.2-0.500.50.4it more challenging to achieve both high accuracy(exact correctness) or high ratings (score of 5) andhigh consistency simultaneously. Notably, we findthat powerful open-source models, such as Qwen-2.5, achieve comparable performance to OpenAImodels in multilingual judgment tasks. However,another open-source model, Llama-3.3, exhibitsmore limited performance. Furthermore, we ex-periment with Aya-Expanse, a multilingual LLMspecifically fine-tuned on multilingual data. De-spite this specialization, Aya fails to demonstratenoticeable improvements. This suggests that fine-tuning with multilingual data may not directly en-hance a model's ability to perform accurate multi-lingual judgments.</p>
<p>Table 4 :
4
Variation of Accuracy (Acc) and Fleiss Kappa with different prompt templates for Grade judgment of GPT-4o.rubric and out represent evaluation guideline and output requests as shown in Section 2.
ID PromptXQuADWMT23Avg Kappa Avg Kappa①rubric: general out: prediction4.66 0.2517 4.55 0.1133②rubric: general out: prediction + explaination4.57 0.3209 4.57 0.1281③rubric: specific out: prediction4.63 0.2145 4.57 0.1145④rubric: specific out: prediction + explaination4.67 0.2239 4.63 0.1196</p>
<p>Table 5 ,
5
most gap values are positive, except for -0.0046 in WMT23 and -0.0142 in another case.Given that other improvements are generally above 0.1, we conclude that the ensemble strategy
XQuAD MGSM WMT23 XDailyD WikiLYes / No :Min 0.07480.09910.07750.24250.2325Ens0.32270.21620.07290.40530.4217∆0.24790.1171 -0.00460.16280.1892Grade :Min 0.15580.06540.14340.16350.1412Ens0.26170.05120.20780.16750.2931∆0.1059 -0.0142 0.06440.00400.1519</p>
<p>Table 5 :
5
Ensemble results (Ens) of Aya, QWen, and Llama.Min indicates the minimum consistency of the above three models.∆ shows the gap between ensemble results and minimum value, i.e., ∆ = Ens -Min.</p>
<p>Ye et al. (2024)4)4)Park et al. (2024)introduced length bias that LLMs prefer more verbose answers even if they have similar qualities, and authority bias that LLMs favor responses with specific details, e.g., citation of authoritative sources.To address the effect of length,Dubois et al. (2024)introduced a debiasing strategy given regression-based adjustments for observational causal inference.Beyond these superficial biases,Park et al. (2024)identified four additional biases, such as familiar knowledge bias which refers to a preference for responses describing commonly encountered knowledge in real-world data.Ye et al. (2024)highlighted the self-enhancement bias, where LLMs tend to favor responses generated by themselves.Instead, we evaluate biases in LLM-as-a-Judge with a focus on multilingual bias.We found that LLM-as-a-Judge struggles to provide consistent judgments across parallel inputs in different languages, with performance being particularly inferior for low-resource languages.</p>
<p>Fleiss' Kappa is ignored as it works for more than
raters. 2 WMT23 is ignored here given experimented machine translation samples all contain English.</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>MEGA: Multilingual evaluation of generative AI. Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed Ahmed, Kalika Bali, Sunayana Sitaram, 10.18653/v1/2023.emnlp-main.258Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>On the cross-lingual transferability of monolingual representations. Mikel Artetxe, Sebastian Ruder, Dani Yogatama, 10.18653/v1/2020.acl-main.421Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Aleksandra Piktus, Roman Castagné. John Dang, Shivalika Singh, D' Daniel, Arash Souza, Alejandro Ahmadian, Madeline Salamanca, Aidan Smith, Sungjin Peppin, Manoj Hong, Terrence Govindassamy, Sandra Zhao, Meor Kublik, Viraat Amer, Jon Ander Aryabumi, Yi-Chern Campos, Tom Tan, Florian Kocmi, Nathan Strub, Yannis Grinsztajn, Acyr Flet-Berliac, Hangyu Locatelli, Dwarak Lin, Bharat Talupuru, David Venkitesh, Bowen Cairuz, Tim Yang, Wei-Yin Chung, Sylvie Shang Ko, Amir Shi, Sammie Shukayev, Bae, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Üstün, and Sara Hooker2024Aya expanse: Combining research breakthroughs for a new multilingual frontier</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Length-controlled alpacaeval: A simple debiasing of automatic evaluators. Yann Dubois, Percy Liang, Tatsunori Hashimoto, First Conference on Language Modeling. 2024</p>
<p>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, Orhan Firat, 10.18653/v1/2023.wmt-1.100Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingapore2023Association for Computational Linguistics</p>
<p>GPTScore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, 10.18653/v1/2024.naacl-long.365Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, Mexico20241Association for Computational Linguistics</p>
<p>Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, arXiv:2304.02554Shiping Yang, and Xiaojun Wan. 2023. Human-like summarization evaluation with chatgpt. arXiv preprint</p>
<p>Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Yuanzhuo Wang, Jian Guo, arXiv:2411.15594A survey on llm-as-a-judge. 2024arXiv preprint</p>
<p>Are large language model-based evaluators the solution to scaling up multilingual evaluation?. Rishav Hada, Varun Gumma, Adrian Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram, Findings of the Association for Computational Linguistics: EACL 2024. St. Julian's, MaltaAssociation for Computational Linguistics2024</p>
<p>Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popović, Mariya Shmatova, 10.18653/v1/2023.wmt-1.1Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingapore2023Association for Computational Linguistics</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023</p>
<p>WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization. Faisal Ladhak, Esin Durmus, Claire Cardie, Kathleen Mckeown, 10.18653/v1/2020.findings-emnlp.360Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, arXiv:2411.16594From generation to judgment: Opportunities and challenges of llm-as-a-judge. 2024arXiv preprint</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>XDailyDialog: A multilingual parallel dialogue corpus. Zeming Liu, Ping Nie, Jie Cai, Haifeng Wang, Zheng-Yu Niu, Peng Zhang, Mrinmaya Sachan, Kaiping Peng, 10.18653/v1/2023.acl-long.684Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Automated evaluation of written discourse coherence using GPT-4. Ben Naismith, Phoebe Mulcaire, Jill Burstein, 10.18653/v1/2023.bea-1.32Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023). the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Introducing gpt-4o. OpenAI Blog. Accessed. 2024OpenAI</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>OffsetBias: Leveraging debiased data for tuning evaluators. Junsoo Park, Seungyeon Jwa, Ren Meiying, Daeyoung Kim, Sanghyuk Choi, 10.18653/v1/2024.findings-emnlp.57Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Is LLM-as-a-judge robust? investigating universal adversarial attacks on zero-shot LLM assessment. Adian Vyas Raina, Mark Liusie, Gales, 10.18653/v1/2024.emnlp-main.427Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>BERGEN: A benchmarking library for retrieval-augmented generation. David Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, 10.18653/v1/2024.findings-emnlp.449Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024Stéphane Clinchant, and Vassilina Nikoulina</p>
<p>A survey of evaluation metrics used for nlg systems. Akash Ananya B Sai, Mitesh M Kumar Mohankumar, Khapra, ACM Computing Surveys (CSUR). 5522022</p>
<p>Keita Saito, Akifumi Wachi, Koki Wataoka, Youhei Akimoto, arXiv:2310.10076Verbosity bias in preference labeling by large language models. 2023arXiv preprint</p>
<p>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr, The Twelfth International Conference on Learning Representations. 2024</p>
<p>BLEURT: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur Parikh, 10.18653/v1/2020.acl-main.704Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Large language models are not yet human-level evaluators for abstractive summarization. Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, Lidong Bing, 10.18653/v1/2023.findings-emnlp.278Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms. Lin Shi, Chiyu Ma, Wenhua Liang, Weicheng Ma, Soroush Vosoughi, arXiv:2406.077912024arXiv preprint</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis, arXiv:2404.187962024arXiv preprint</p>
<p>Is ChatGPT a good NLG evaluator? a preliminary study. Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, 10.18653/v1/2023.newsum-1.1Proceedings of the 4th New Frontiers in Summarization Workshop. the 4th New Frontiers in Summarization WorkshopSingapore2023Association for Computational Linguistics</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, Zhifang Sui, 10.18653/v1/2024.acl-long.511Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024a1</p>
<p>Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, Xian Li, arXiv:2408.02666Self-taught evaluators. 2024barXiv preprint</p>
<p>Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar, arXiv:2407.19594Meta-rewarding language models: Self-improving alignment with llm-as-ameta-judge. 2024arXiv preprint</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.15115Qwen2. 5 technical report. 2024arXiv preprint</p>
<p>Justice or prejudice? quantifying biases in llm-as-a-judge. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, arXiv:2410.027362024arXiv preprint</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, arXiv:2310.17631Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges. Curran Associates, Inc202336arXiv preprintAdvances in Neural Information Processing Systems</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1380 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1380</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1380</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-cfb68baa23048e3e0f8845c099fa013797bd623f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cfb68baa23048e3e0f8845c099fa013797bd623f" target="_blank">Explainable Reinforcement Learning Through a Causal Lens</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This paper presents an approach that learns a structural causal model during reinforcement learning and encodes causal relationships between variables of interest and shows that causal model explanations perform better on these measures compared to two other baseline explanation models.</p>
                <p><strong>Paper Abstract:</strong> Prominent theories in cognitive science propose that humans understand and represent the knowledge of the world through causal relationships. In making sense of the world, we build causal models in our mind to encode cause-effect relations of events and use these to explain why new events happen by referring to counterfactuals — things that did not happen. In this paper, we use causal models to derive causal explanations of the behaviour of model-free reinforcement learning agents. We present an approach that learns a structural causal model during reinforcement learning and encodes causal relationships between variables of interest. This model is then used to generate explanations of behaviour based on counterfactual analysis of the causal model. We computationally evaluate the model in 6 domains and measure performance and task prediction accuracy. We report on a study with 120 participants who observe agents playing a real-time strategy game (Starcraft II) and then receive explanations of the agents' behaviour. We investigate: 1) participants' understanding gained by explanations through task prediction; 2) explanation satisfaction and 3) trust. Our results show that causal model explanations perform better on these measures compared to two other baseline explanation models.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1380.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1380.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AIM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action Influence Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An action-augmented structural causal world model learned during model-free RL: a causal DAG (given) where each state variable has per-action structural equations F_{X.A} learned by regression; used to generate counterfactual, contrastive and minimally-complete explanations and to perform task-prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Action Influence Model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An explicit, action-augmented structural causal model: a given DAG of variables and directed causal edges (edges annotated by actions) where each endogenous state variable X has a set of structural equations F_{X.A} (one per influencing action A). Structural equations are approximated with regression learners (linear SGD regression, decision-tree regression, or multilayer perceptron regressors) trained from experience-replay tuples (s_t, a_t, r_t, s_{t+1}). The model is instantiated with current state values to answer 'why'/'why not' queries via forward simulation of the structural equations to produce counterfactual instantiations and contrastive explanations; explanations are generated as minimally-complete causal chains and textual templates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit causal world model (action-augmented SCM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning benchmark domains: OpenAI Gym domains (CartPole, MountainCar, Taxi, LunarLander, BipedalWalker) and a toned-down StarCraft II 1v1 scenario (used in human study)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task-prediction accuracy (percent of next-action predictions correct) computed by using learned structural equations to predict next-state variables and selecting the action associated with the largest predicted correction (also per-variable absolute prediction error |S_y - P_y| used internally); training time for regressors (seconds) reported as computational fidelity/overhead measure.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Task-prediction accuracies from Table 1 (mean over 100 episodes after training): Cartpole-PG — LR 83.8%, DT 81.6%, MLP 86.0%; MountainCar-DQN — LR 69.7%, DT 57.8%, MLP 69.6%; Taxi-SARSA — LR 68.2%, DT 74.2%, MLP 67.9%; LunarLander-DDQN — LR 68.4%, DT 63.7%, MLP 72.1%; BipedalWalker-PPO — LR 56.9%, DT 56.4%, MLP 56.7%; Starcraft-A2C — LR 94.7%, DT 91.8%, MLP 91.4%. Training times (regressor fitting) reported per domain and learner: LR {Cartpole 0.007s, MountainCar 0.020s, Taxi 0.001s, LunarLander 0.002s, BipedalWalker 0.010s, Starcraft 0.144s}; DT {0.018s, 0.037s, 0.001s, 0.002s, 0.015s, 0.025s}; MLP {0.03s, 0.32s, 0.49s, 0.33s, 0.41s, 3.33s}.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High interpretability at the structural level: the causal DAG and per-action edges make causal relationships explicit and support human-understandable, contrastive counterfactual explanations (minimally-complete chains). The regressors implementing structural equations vary in transparency: linear regressors and decision trees are more interpretable; MLP regressors are black-box. Human evaluation shows explanations from this model significantly improve task-prediction and explanation-satisfaction compared to baselines, indicating practical interpretability for users.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Explicit causal graph visualization (sub-graph shown to users), generation of minimally-complete and contrastive explanations, template-based natural-language rendering of contrasted variable values, and human-subject evaluation (task-prediction scores and Likert explanation-quality scales). Choice of regression family (LR/DT/MLP) also affects interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Regressor training costs are small per Table 1 (LR and DT training reported in the 0.001–0.144s range per evaluation snapshot; MLP can be substantially slower, up to 3.33s for StarCraft in the reported setup). Overhead during RL training consists of experience-replay storage and periodic mini-batch updates of only the structural equations associated with experienced actions. Authors report a negligible performance hit overall except where MLP regressors were used and in continuous-action domains.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Authors report 'little gained by using MLP' relative to LR (i.e., linear regression is often adequate) while MLP incurs higher computational cost (noted by larger training times). Compared to learning full environment dynamics (model-based RL), the approach is lighter-weight because only per-action structural equations are learned (approximate equations sufficient for explanations), but no quantitative head-to-head runtime baseline vs model-based RL is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Using AIM explanations improved human task-prediction scores and explanation-satisfaction: mean task-prediction score for causal-abstract model C was 10.90 (out of 16) vs Relevant-variable R 8.97 and No-explanation N 8.53; overall ANOVA p=0.003; pairwise C-N p=0.006 and C-R p=0.034. Explanation-quality Likert measures also show statistically significant improvements over the relevant-variable baseline on several metrics (complete/sufficient/satisfying; p-values reported in Table 3). Trust measures did not show statistically significant improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>In domains with clear causal structure (StarCraft), high prediction fidelity (94.7% with LR) translates to strong task-prediction utility and high explanation satisfaction. In domains with continuous or complex actions (BipedalWalker, continuous actions), fidelity is low (~57%) and utility is limited. Authors argue that approximate structural equations can be 'good enough' for explanation generation even if they are not perfect environment simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs discussed: (1) interpretability vs modeling power — linear regressors are interpretable and often sufficient, while MLPs add modeling power but little explanation benefit and higher cost; (2) fidelity vs effort — authors require a pre-specified causal DAG (hand-coded directionality), reducing discovery cost but placing a burden on designers; (3) completeness vs cognitive load — full causal chains may overwhelm users, so minimally-complete contrastive explanations are used; (4) continuous-action domains are not well-handled in current formulation (reduced fidelity).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: require an a priori DAG specifying causal directions; learn only structural equations F_{X.A} per action (not a full transition model); use experience-replay and minibatch regression updates per action; choose regression learner family (LR, DT, MLP) depending on domain; generate minimally-complete and contrastive explanations rather than full chains to reduce cognitive load; use difference condition to identify explanans.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally to two baselines: 'relevant variable explanations' (state-relevant templates) and 'no explanation'. Action Influence Model outperformed both on human task-prediction and explanation-quality metrics. Authors contrast AIM with model-based RL (learning full dynamics): AIM only learns per-action structural equations and aims for approximate models sufficient for counterfactual explanations rather than optimized planning performance; no direct quantitative comparison to model-based RL performance is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Empirical recommendations in the paper: use simple regression (linear) where possible (authors found linear regression adequate in many domains and much cheaper than MLP); provide a DAG of causal directions (hand-specified) to simplify learning; generate minimally-complete contrastive explanations to balance informativeness and cognitive load; extend the framework to handle continuous actions in future work. The paper highlights that an 'optimal' configuration is domain-dependent: prefer simple, interpretable regressors and small, human-meaningful variable sets when the goal is explainability rather than precise dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning Through a Causal Lens', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1380.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1380.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Causal Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formal framework (Halpern & Pearl style) representing systems with exogenous and endogenous variables connected by structural equations; supports counterfactual reasoning, actual-cause definitions, and explanation extraction via interventions and simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causes and explanations: A structural-model approach. part II: Explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Structural Causal Model (SCM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Symbolic causal representation: variables partitioned into exogenous and endogenous sets, with a set of structural functions/equations F_X mapping parent variables (and exogenous variables) to each endogenous variable's value; represented as a DAG; supports counterfactual interventions (do-operations) and formal definitions of actual cause and explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>symbolic causal world model (explicit causal model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General-purpose causal modelling applicable to explanation and counterfactual reasoning in RL and other domains (used here to underpin explanations in RL agents).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Highly interpretable by design: the causal graph and structural equations make causal pathways explicit and support counterfactual queries that are intelligible to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Graphical causal DAGs, explicit structural equations, counterfactual simulation/interventions and extraction of minimal causes; authors apply SCM concepts to produce minimally-complete and contrastive explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Cost depends on how structural functions are represented/learned; in this paper SCMs are instantiated by learned regressors per variable/action (cost as reported for those regressors). SCMs themselves are a conceptual framework with no fixed compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Used as the formal language for generating counterfactual explanations and determining actual causes; the utility in this paper is that SCM-derived explanations improved human task-prediction and explanation satisfaction when operationalized as action influence models.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>While SCMs provide clarity and support counterfactuals, they require knowledge (or an estimate) of causal directionality and structural functions; learning these functions in complex domains can be challenging and may need approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Paper assumes the DAG (causal directions) is given; only the structural equations are estimated from experience; authors choose regression-based approximations as the representation for F_{X.A}.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>SCMs are contrasted implicitly with opaque black-box predictors and with model-based RL dynamics models; SCMs provide counterfactual semantics and interpretable causal structure that black-box models lack unless additional structure is extracted.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests combining a hand-specified DAG (for directions) with simple, interpretable approximators (linear regression) for structural equations in order to balance interpretability, fidelity (sufficient for explanations), and computational efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning Through a Causal Lens', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1380.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1380.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>model-based RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-based Reinforcement Learning (general)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of RL methods that learn or use an explicit model of environment dynamics (transition/reward functions) to plan or simulate future outcomes; mentioned as related work and contrasted with learning only per-action structural equations for explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Model-based RL (general dynamics model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns or uses an explicit transition model P(s'|s,a) and possibly reward models to support planning (e.g., via model-predictive control, tree search, or policy optimization with imagination); typically represented with parametric regressors, neural networks, or simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit dynamics model / model-based approach</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General RL domains where planning via learned dynamics is useful (not specifically evaluated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Varies by implementation; not discussed in detail in this paper. The authors note AIM differs from model-based RL by only learning structural equations sufficient for explanations rather than full dynamics optimized for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Paper positions AIM as lighter-weight for explanation purposes than full model-based RL (because AIM learns only per-action structural equations and targets approximate counterfactuals), but provides no quantitative head-to-head comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Authors claim AIM trades off full predictive fidelity (a full dynamics model) for lower-cost, interpretable, explanation-oriented approximations; explicit tradeoff numbers are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Mentioned as an alternative approach: unlike model-based RL which learns full dynamics for planning, AIM learns per-action causal equations only to support counterfactual explanation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning Through a Causal Lens', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Causes and explanations: A structural-model approach. part II: Explanations. <em>(Rating: 2)</em></li>
                <li>Contrastive explanation: A structural-model approach. <em>(Rating: 2)</em></li>
                <li>Contrastive explanations for reinforcement learning in terms of expected consequences <em>(Rating: 2)</em></li>
                <li>Minimal sufficient explanations for factored markov decision processes <em>(Rating: 2)</em></li>
                <li>Human-level control through deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1380",
    "paper_id": "paper-cfb68baa23048e3e0f8845c099fa013797bd623f",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "AIM",
            "name_full": "Action Influence Model",
            "brief_description": "An action-augmented structural causal world model learned during model-free RL: a causal DAG (given) where each state variable has per-action structural equations F_{X.A} learned by regression; used to generate counterfactual, contrastive and minimally-complete explanations and to perform task-prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Action Influence Model",
            "model_description": "An explicit, action-augmented structural causal model: a given DAG of variables and directed causal edges (edges annotated by actions) where each endogenous state variable X has a set of structural equations F_{X.A} (one per influencing action A). Structural equations are approximated with regression learners (linear SGD regression, decision-tree regression, or multilayer perceptron regressors) trained from experience-replay tuples (s_t, a_t, r_t, s_{t+1}). The model is instantiated with current state values to answer 'why'/'why not' queries via forward simulation of the structural equations to produce counterfactual instantiations and contrastive explanations; explanations are generated as minimally-complete causal chains and textual templates.",
            "model_type": "explicit causal world model (action-augmented SCM)",
            "task_domain": "Reinforcement learning benchmark domains: OpenAI Gym domains (CartPole, MountainCar, Taxi, LunarLander, BipedalWalker) and a toned-down StarCraft II 1v1 scenario (used in human study)",
            "fidelity_metric": "Task-prediction accuracy (percent of next-action predictions correct) computed by using learned structural equations to predict next-state variables and selecting the action associated with the largest predicted correction (also per-variable absolute prediction error |S_y - P_y| used internally); training time for regressors (seconds) reported as computational fidelity/overhead measure.",
            "fidelity_performance": "Task-prediction accuracies from Table 1 (mean over 100 episodes after training): Cartpole-PG — LR 83.8%, DT 81.6%, MLP 86.0%; MountainCar-DQN — LR 69.7%, DT 57.8%, MLP 69.6%; Taxi-SARSA — LR 68.2%, DT 74.2%, MLP 67.9%; LunarLander-DDQN — LR 68.4%, DT 63.7%, MLP 72.1%; BipedalWalker-PPO — LR 56.9%, DT 56.4%, MLP 56.7%; Starcraft-A2C — LR 94.7%, DT 91.8%, MLP 91.4%. Training times (regressor fitting) reported per domain and learner: LR {Cartpole 0.007s, MountainCar 0.020s, Taxi 0.001s, LunarLander 0.002s, BipedalWalker 0.010s, Starcraft 0.144s}; DT {0.018s, 0.037s, 0.001s, 0.002s, 0.015s, 0.025s}; MLP {0.03s, 0.32s, 0.49s, 0.33s, 0.41s, 3.33s}.",
            "interpretability_assessment": "High interpretability at the structural level: the causal DAG and per-action edges make causal relationships explicit and support human-understandable, contrastive counterfactual explanations (minimally-complete chains). The regressors implementing structural equations vary in transparency: linear regressors and decision trees are more interpretable; MLP regressors are black-box. Human evaluation shows explanations from this model significantly improve task-prediction and explanation-satisfaction compared to baselines, indicating practical interpretability for users.",
            "interpretability_method": "Explicit causal graph visualization (sub-graph shown to users), generation of minimally-complete and contrastive explanations, template-based natural-language rendering of contrasted variable values, and human-subject evaluation (task-prediction scores and Likert explanation-quality scales). Choice of regression family (LR/DT/MLP) also affects interpretability.",
            "computational_cost": "Regressor training costs are small per Table 1 (LR and DT training reported in the 0.001–0.144s range per evaluation snapshot; MLP can be substantially slower, up to 3.33s for StarCraft in the reported setup). Overhead during RL training consists of experience-replay storage and periodic mini-batch updates of only the structural equations associated with experienced actions. Authors report a negligible performance hit overall except where MLP regressors were used and in continuous-action domains.",
            "efficiency_comparison": "Authors report 'little gained by using MLP' relative to LR (i.e., linear regression is often adequate) while MLP incurs higher computational cost (noted by larger training times). Compared to learning full environment dynamics (model-based RL), the approach is lighter-weight because only per-action structural equations are learned (approximate equations sufficient for explanations), but no quantitative head-to-head runtime baseline vs model-based RL is provided.",
            "task_performance": "Using AIM explanations improved human task-prediction scores and explanation-satisfaction: mean task-prediction score for causal-abstract model C was 10.90 (out of 16) vs Relevant-variable R 8.97 and No-explanation N 8.53; overall ANOVA p=0.003; pairwise C-N p=0.006 and C-R p=0.034. Explanation-quality Likert measures also show statistically significant improvements over the relevant-variable baseline on several metrics (complete/sufficient/satisfying; p-values reported in Table 3). Trust measures did not show statistically significant improvement.",
            "task_utility_analysis": "In domains with clear causal structure (StarCraft), high prediction fidelity (94.7% with LR) translates to strong task-prediction utility and high explanation satisfaction. In domains with continuous or complex actions (BipedalWalker, continuous actions), fidelity is low (~57%) and utility is limited. Authors argue that approximate structural equations can be 'good enough' for explanation generation even if they are not perfect environment simulators.",
            "tradeoffs_observed": "Trade-offs discussed: (1) interpretability vs modeling power — linear regressors are interpretable and often sufficient, while MLPs add modeling power but little explanation benefit and higher cost; (2) fidelity vs effort — authors require a pre-specified causal DAG (hand-coded directionality), reducing discovery cost but placing a burden on designers; (3) completeness vs cognitive load — full causal chains may overwhelm users, so minimally-complete contrastive explanations are used; (4) continuous-action domains are not well-handled in current formulation (reduced fidelity).",
            "design_choices": "Key choices: require an a priori DAG specifying causal directions; learn only structural equations F_{X.A} per action (not a full transition model); use experience-replay and minibatch regression updates per action; choose regression learner family (LR, DT, MLP) depending on domain; generate minimally-complete and contrastive explanations rather than full chains to reduce cognitive load; use difference condition to identify explanans.",
            "comparison_to_alternatives": "Compared experimentally to two baselines: 'relevant variable explanations' (state-relevant templates) and 'no explanation'. Action Influence Model outperformed both on human task-prediction and explanation-quality metrics. Authors contrast AIM with model-based RL (learning full dynamics): AIM only learns per-action structural equations and aims for approximate models sufficient for counterfactual explanations rather than optimized planning performance; no direct quantitative comparison to model-based RL performance is provided.",
            "optimal_configuration": "Empirical recommendations in the paper: use simple regression (linear) where possible (authors found linear regression adequate in many domains and much cheaper than MLP); provide a DAG of causal directions (hand-specified) to simplify learning; generate minimally-complete contrastive explanations to balance informativeness and cognitive load; extend the framework to handle continuous actions in future work. The paper highlights that an 'optimal' configuration is domain-dependent: prefer simple, interpretable regressors and small, human-meaningful variable sets when the goal is explainability rather than precise dynamics.",
            "uuid": "e1380.0",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning Through a Causal Lens",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "SCM",
            "name_full": "Structural Causal Model",
            "brief_description": "A formal framework (Halpern & Pearl style) representing systems with exogenous and endogenous variables connected by structural equations; supports counterfactual reasoning, actual-cause definitions, and explanation extraction via interventions and simulation.",
            "citation_title": "Causes and explanations: A structural-model approach. part II: Explanations.",
            "mention_or_use": "use",
            "model_name": "Structural Causal Model (SCM)",
            "model_description": "Symbolic causal representation: variables partitioned into exogenous and endogenous sets, with a set of structural functions/equations F_X mapping parent variables (and exogenous variables) to each endogenous variable's value; represented as a DAG; supports counterfactual interventions (do-operations) and formal definitions of actual cause and explanation.",
            "model_type": "symbolic causal world model (explicit causal model)",
            "task_domain": "General-purpose causal modelling applicable to explanation and counterfactual reasoning in RL and other domains (used here to underpin explanations in RL agents).",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Highly interpretable by design: the causal graph and structural equations make causal pathways explicit and support counterfactual queries that are intelligible to humans.",
            "interpretability_method": "Graphical causal DAGs, explicit structural equations, counterfactual simulation/interventions and extraction of minimal causes; authors apply SCM concepts to produce minimally-complete and contrastive explanations.",
            "computational_cost": "Cost depends on how structural functions are represented/learned; in this paper SCMs are instantiated by learned regressors per variable/action (cost as reported for those regressors). SCMs themselves are a conceptual framework with no fixed compute cost.",
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Used as the formal language for generating counterfactual explanations and determining actual causes; the utility in this paper is that SCM-derived explanations improved human task-prediction and explanation satisfaction when operationalized as action influence models.",
            "tradeoffs_observed": "While SCMs provide clarity and support counterfactuals, they require knowledge (or an estimate) of causal directionality and structural functions; learning these functions in complex domains can be challenging and may need approximations.",
            "design_choices": "Paper assumes the DAG (causal directions) is given; only the structural equations are estimated from experience; authors choose regression-based approximations as the representation for F_{X.A}.",
            "comparison_to_alternatives": "SCMs are contrasted implicitly with opaque black-box predictors and with model-based RL dynamics models; SCMs provide counterfactual semantics and interpretable causal structure that black-box models lack unless additional structure is extracted.",
            "optimal_configuration": "Paper suggests combining a hand-specified DAG (for directions) with simple, interpretable approximators (linear regression) for structural equations in order to balance interpretability, fidelity (sufficient for explanations), and computational efficiency.",
            "uuid": "e1380.1",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning Through a Causal Lens",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "model-based RL",
            "name_full": "Model-based Reinforcement Learning (general)",
            "brief_description": "A family of RL methods that learn or use an explicit model of environment dynamics (transition/reward functions) to plan or simulate future outcomes; mentioned as related work and contrasted with learning only per-action structural equations for explanation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Model-based RL (general dynamics model)",
            "model_description": "Learns or uses an explicit transition model P(s'|s,a) and possibly reward models to support planning (e.g., via model-predictive control, tree search, or policy optimization with imagination); typically represented with parametric regressors, neural networks, or simulators.",
            "model_type": "explicit dynamics model / model-based approach",
            "task_domain": "General RL domains where planning via learned dynamics is useful (not specifically evaluated in this paper).",
            "fidelity_metric": null,
            "fidelity_performance": null,
            "interpretability_assessment": "Varies by implementation; not discussed in detail in this paper. The authors note AIM differs from model-based RL by only learning structural equations sufficient for explanations rather than full dynamics optimized for planning.",
            "interpretability_method": null,
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Paper positions AIM as lighter-weight for explanation purposes than full model-based RL (because AIM learns only per-action structural equations and targets approximate counterfactuals), but provides no quantitative head-to-head comparison.",
            "tradeoffs_observed": "Authors claim AIM trades off full predictive fidelity (a full dynamics model) for lower-cost, interpretable, explanation-oriented approximations; explicit tradeoff numbers are not provided.",
            "design_choices": null,
            "comparison_to_alternatives": "Mentioned as an alternative approach: unlike model-based RL which learns full dynamics for planning, AIM learns per-action causal equations only to support counterfactual explanation generation.",
            "optimal_configuration": null,
            "uuid": "e1380.2",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning Through a Causal Lens",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Causes and explanations: A structural-model approach. part II: Explanations.",
            "rating": 2
        },
        {
            "paper_title": "Contrastive explanation: A structural-model approach.",
            "rating": 2
        },
        {
            "paper_title": "Contrastive explanations for reinforcement learning in terms of expected consequences",
            "rating": 2
        },
        {
            "paper_title": "Minimal sufficient explanations for factored markov decision processes",
            "rating": 2
        },
        {
            "paper_title": "Human-level control through deep reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.014211499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Explainable Reinforcement Learning through a Causal Lens</h1>
<p>Prashan Madumal, Tim Miller, Liz Sonenberg, Frank Vetere<br>Victoria, Australia<br>pmathugama@student.unimelb.edu.au, {tmiller, l.sonenberg, f.vetere}@unimelb.edu.au</p>
<h4>Abstract</h4>
<p>Prominent theories in cognitive science propose that humans understand and represent the knowledge of the world through causal relationships. In making sense of the world, we build causal models in our mind to encode cause-effect relations of events and use these to explain why new events happen by referring to counterfactuals - things that did not happen. In this paper, we use causal models to derive causal explanations of the behaviour of model-free reinforcement learning agents. We present an approach that learns a structural causal model during reinforcement learning and encodes causal relationships between variables of interest. This model is then used to generate explanations of behaviour based on counterfactual analysis of the causal model. We computationally evaluate the model in 6 domains and measure performance and task prediction accuracy. We report on a study with $\mathbf{1 2 0}$ participants who observe agents playing a real-time strategy game (Starcraft II) and then receive explanations of the agents' behaviour. We investigate: 1) participants' understanding gained by explanations through task prediction; 2) explanation satisfaction and 3) trust. Our results show that causal model explanations perform better on these measures compared to two other baseline explanation models.</p>
<p>Driven by lack of trust from users and proposed regulations, there are many calls for Artificial Intelligence (AI) systems to become more transparent, interpretable and explainable. This has renewed the interest in Explainable AI (XAI), which has been explored since the expert systems era (Chandrasekaran, Tanner, and Josephson 1989). A key pillar of XAI is explanation, a justification given for decisions and actions of the system.</p>
<p>However, much research and practice in XAI pays little attention to people as intended users of these systems (Miller 2018b). If we are to build systems that are capable of providing 'good' explanations, it is plausible that explanation models should mimic models of human explanation (De Graaf and Malle 2017). Thus, to build XAI models it is essential to begin with a strong understanding of how people define, generate, select and evaluate explanations.</p>
<p>There is a wealth of pertinent literature in cognitive psychology that explore the nature of explanations and how</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>people understand them. As humans, we view the world through a causal lens (Sloman 2005), building mental models with causal relationships to act in the world, to understand new events and also to explain events. Importantly, causal models give people the ability to consider counterfactuals - events that did not happen, but could have under different situations. Although this notion of causal explanation is also backed by literature in philosophy and social psychology (Hilton 2007), causality and counterfactuals are only just becoming more prevalent in XAI. Further, compared to the burst of XAI research in supervised learning, explainability in model-free reinforcement learning is hardly explored.</p>
<p>We introduce an action influence model for model-free reinforcement learning (RL) agents and provide a formalisation of the model using structural causal models (Halpern and Pearl 2005). Action influence models approximate the causal model of the environment relative to actions taken by an agent. Our approach differs from previous work in explainable RL in that we use causal models to generate contrastive explanations for why and why not questions, which previous models lack. Given assumptions about the direction of causal relationships between variables, during the policy learning process, we also learn the quantitative influences that actions have on variables. Which enable our model to reason approximately about counterfactual states and actions. We define how to generate explanations for 'why?' and 'why not?' questions from the action influence model. We define minimally complete explanations taking inspiration from social psychology literature (McClure and Hilton 1997).</p>
<p>We computationally evaluated our approach on 6 RL benchmarks domains using 6 different RL algorithms. Results indicate that these models are robust and accurate enough to perform task prediction (Hoffman et al. 2018, p.12) with a negligible performance impact. We conducted a human study using the implemented model for RL agents trained to play the real-time strategy game Starcraft II. Experiments were run for $\mathbf{1 2 0}$ participants, in which we evaluated the participants' performance in task prediction, explanation satisfaction, and trust. Results show that our model performs better than the tested baseline, but its impact on</p>
<p>trust is not statistically significant.
The main contribution of this paper is twofold: 1) We introduce and formalise the action influence model based on structural causal models and present definitions to generate explanations; 2) We conduct a between-subject human study to evaluate the proposed model with baselines.</p>
<h2>Related Work</h2>
<p>There exists a substantial body of literature that explores explaining the policies and actions of Markov Decision Processes (MDP), though most of them do not explicitly focus on reinforcement learning. Elizalde et al. (2009) generated explanations by selecting and using 'relevant' variables of states of factored MDPs, evaluated by domain experts. Taking the long term effect an action has, Khan, Poupart, and Black (2009) proposed generating sufficient and minimal explanations for MDPs using domain independent templates.</p>
<p>Policy explanations in human-agent interaction settings have been used to achieve transparency (Hayes and Shah 2017) and provide summaries of the policies (Amir and Amir 2018). Explanation in reinforcement learning has been explored, using interactive RL to generate explanations from instructions of a human (Fukuchi et al. 2017) and to provide contrastive explanations (van der Waa et al. 2018). Soft decision trees have been used to generate more interpretable policies (Coppens et al. 2019), and reward decomposition has been utilized to provide minimum sufficient explanations in RL (Juozapaitis et al. 2019). However, these explanations are not based on an underlying causal model.</p>
<p>Other work on causal explanation has focused on scientific explanations (Salmon 1984) and explanations using causal trees (Nielsen, Pellet, and Elisseeff 2012). Although some recent work has emphasized the importance of causal explanation for explainable AI systems (Miller 2018b; 2018a; Madumal et al. 2019; Madumal 2019), work on generating explanations from causal explanation models for MDPs and RL agents have been absent.</p>
<h2>Causal Models for Explanations</h2>
<p>In this section, we introduce the action influence model, which is based on structural causal models of Halpern and Pearl (2005). We first introduce the Starcraft II domain, a partially observable real-time strategy game environment with a large state and action space. For the purpose of implementing RL agents for explanation, we use a toned-down version of the full Starcraft II 1v1 match (an adversarial scenario) with 4 actions and 9 state variables for the agent's model (see Figure 1). In the following sections we use this Starcraft II scenario accompanied by Figure 1 as our running example.</p>
<h2>Preliminaries : Structural Causal Models</h2>
<p>Structural causal models (SCMs) (Halpern and Pearl 2005) represent the world using random variables, divided into exogenous (external) and endogenous (internal), some of which might have causal relationships which each other.</p>
<p>These relationships can be described with a set of structural equations. Formally, a signature $S$ is a tuple $(\mathcal{U}, \mathcal{V}, \mathcal{R})$, where $\mathcal{U}$ is the set of exogenous variables, $\mathcal{V}$ the set of endogenous variables, and $\mathcal{R}$ is a function that denotes the range of values for every variable $\mathcal{Y} \in \mathcal{U} \cup \mathcal{V}$.
Definition 1. A structural causal model is a tuple $M=$ $(\mathcal{S}, \mathcal{F})$, where $\mathcal{F}$ denotes a set of structural equations, one for each $X \in \mathcal{V}$, such that $F_{X}:\left(\times_{U \in \mathcal{U}} \mathcal{R}(U)\right) \times$ $\left(\times_{Y \in \mathcal{V}-{X}} \mathcal{R}(Y)\right) \rightarrow \mathcal{R}(X)$ give the value of $X$ based on other variables in $\mathcal{U} \cup \mathcal{V}$. That is, the equation $F_{X}$ defines the value of $X$ based on some other variables in the model.</p>
<p>A context $\vec{u}$ is a vector of unique values of each exogenous variable $u \in \mathcal{U}$. A situation is defined as a model/context pair $(M, \vec{u})$. An instantiation is defined by assigning variables the values corresponding to those defined by their structural equations. An actual cause of an event $\varphi$ is a vector of endogenous variables and their values such that there is some counterfactual context in which the variables in the cause are different and the event $\varphi$ does not occur. An explanation is those causes that an explainee does not already know. For a more complete review of SCM's we direct the reader to (Halpern and Pearl 2005).</p>
<h2>Causal Models for Reinforcement Learning Agents</h2>
<p>Our intent in this paper is not to provide explanations of $e v$ idence from the environment, but to provide explanations of the agent's behaviour based on the knowledge of how actions influence the environment. As such, we extend the notion of SCMs to include actions as part of the causal relationships.</p>
<p>We incorporate action influence models for MDP-based RL agents, extending SCMs with the addition of actions. An MDP is a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R} \gamma)$, where $S$ and $A$ give state and action spaces respectively (here we assume the state and action space is finite and state features are described by a set of variables $\phi) ; \mathcal{T}=\left{P_{s a}\right}$ a set of state transition functions ( $P_{s a}$ denotes state transition distribution of taking action $a$ in state $s) ; \mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is a reward function and $\gamma=[0,1)$ is a discount factor. The objective of an RL agent is to find a policy $\pi$ that maps states to actions maximizing the expected discounted sum of rewards. We define the action influence model for RL agents as follows.</p>
<p>Formally, a signature $S_{a}$ for an action influence model is a tuple $(\mathcal{U}, \mathcal{V}, \mathcal{R}, \mathcal{A})$, in which $\mathcal{U}, \mathcal{V}$, and $\mathcal{R}$ are as in SCMs, and $\mathcal{A}$ is the set of actions.
Definition 2. An action influence model is a tuple $\left(S_{a}, \mathcal{F}\right)$, where $S_{a}$ is as above, and $\mathcal{F}$ is the set of structural equations, in which we have multiple for each $X \in \mathcal{V}$ - one for each unique action set that influences $X$. A function $F_{X . A}$, for $A \in \mathcal{A}$, defines the causal effect on $X$ from applying action $A$. The set of reward variables $X_{r} \subseteq \mathcal{V}$ are defined by the set of nodes with an out-degree of 0 ; that is, the set of sink nodes.</p>
<p>We define the actual instantiation of a model $M$ as $M_{\mathcal{C}_{a}, \vec{S}}$, in which $\vec{S}$ is the vector of state variable values from an MDP. In an actual instantiation, we set the values of all state variables in the model, effectively making the exogenous variables irrelevant.</p>
<p>Figure 1 shows the graphical representation of Definition 2 as an action influence graph of the Starcraft II agent described in the previous section, with exogenous variables hidden. These action influence models are SCMs except that each edge is associated with an action. In the action influence model, each state variable has a set of structural equations: one for each unique incoming action. As an example, from Figure 1, variable $A_{n}$ is causally influenced by $S$ and $B$ only when action $A_{m}$ is executed, thus the structural equation $F_{A_{n} \cdot A_{m}}(S, B)$ captures that relationship.</p>
<h2>Explanation Generation</h2>
<p>In this section, we present definitions that generate explanations from an action influence model. The process of explanation generation has 3 phases: 1) defining the qualitative causal relationships of variables as an action influence model; 2) learning the structural equations during RL; and 3) generating explanans from SCMs using the definitions given below.</p>
<p>We define an explanation as a pair that consist of: 1) an explanandum, the event to be explained; and 2) an explanan, the subset of causes given as the explanation (Miller 2018b). Consider the example 'Why did you do $P$ ?' and the explanation 'Because of $Q$ '. Here, the explanandum is $P$ and explanan is $Q$. Identifying the explanandum from a question is not a trivial task. In this paper, we define explanations for questions of the form 'Why $A$ ?' or 'Why not $A$ ?', where $A$ is an action. In the context of a RL agent we define a complete explanan below.
Definition 3. A complete explanan for an action $a$ under the actual instantiation $M_{\mathcal{G} \leftarrow \mathcal{S}}$ is a tuple $\left(\vec{X}<em r="r">{r}=\vec{x}</em>}, \vec{X<em h="h">{h}=\vec{x}</em>}, \vec{X<em i="i">{i}=\vec{x}</em>}\right)$, in which $\vec{X<em h="h">{r}$ is the vector of reward variables reached by following the causal chain of the graph to sink nodes; $\vec{X}</em>}$ the vector of variables of the head node of action $a, \vec{X<em r="r">{i}$ the vector of intermediate nodes between head and reward nodes, and $\vec{x}</em>}, \vec{x<em i="i">{h}, \vec{x}</em>$.}$ gives the values of these variables under $M_{\mathcal{G} \leftarrow \mathcal{S}</p>
<p>Informally, this defines a complete explanan for action $a$ as the complete causal chain from action $a$ to any future reward that it can receive. From Figure 1, the causal chain for action $A_{s}$ is depicted in bold edges, and the extracted explanan tuple $\left([S=s] ;\left[A_{n}=a_{n}\right],\left[D_{u}=d_{u}, D_{b}=d_{b}\right]\right)$ is shown as darkened nodes. We use depth-first search to traverse the graph until all the sink nodes are reached from the head node of the action edge.</p>
<h2>'Why?' Questions</h2>
<p>Lim, Dey, and Avrahami (2009) found that the most demanded explanatory questions are Why and Why not questions. To this end, we focus on explanation generation for why and why not questions in this paper.
Minimally Complete Explanations Striking a balance between complete and minimal explanations depend on the epistemic state of the explainee (Miller 2018b). In this paper, we assume that we know nothing about the epistemic state of the explainee.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Action influence graph of a Starcraft II agent</p>
<p>Recall from the definition of explanans (Definition 3), a 'complete' explanation would include explanans of all the intermediate nodes between the head and reward node of the causal chain. Clearly, for a large graph, this risks overwhelming the explainee. For this reason, we define minimally complete explanations.</p>
<p>McClure and Hilton (1997) show that referring to the goal as being the most important for explaining actions. In our causal models, the rewards are the 'goals', but these alone do not form meaningful explanations because they are merely numbers. We define the human interpretable 'goal' using the variables in the predecessor nodes of the rewards. These define the immediate causes of the reward, and therefore which states will result in rewards. However, this alone is only a longer-term motivation for taking an action. As such, we also include the head node of the action edge as the immediate reason for doing the action. We use this model to define our minimally complete explanations.
Definition 4. A minimally complete explanation is a tuple $\left(\vec{X}<em r="r">{r}=\vec{x}</em>}, \vec{X<em h="h">{h}=\vec{x}</em>}, \vec{X<em p="p">{p}=\vec{x}</em>}\right)$, in which $\vec{X<em r="r">{r}=\vec{x}</em>}$ and $\vec{X<em h="h">{h}=\vec{x}</em>}$ do not change from Definition 3, and $\vec{X<em p="p">{p}=\vec{x}</em>$ the values in the actual instantiation.}$ is the vector of variables that are immediate predecessors of any variable in $X_{r}$ within the causal chain, with $\vec{x}_{p</p>
<p>Informally, for a complete causal chain, we take the first and last arcs of the causal chain, with their source and destination nodes, omitting intermediate nodes, as the minimal explanation. From Figure 1, for the action $A_{s}$, the minimally complete explanation is just the complete explanation, as there are no intermediate nodes.</p>
<p>Clearly, one could define other heuristics to decide which intermediate nodes to use as explanations, such as the knowledge of the explainee. However, for the purposes of this paper, we use this simple definition.</p>
<h2>'Why not?' Questions</h2>
<p>Why not questions let the explainee ask why an event has not occurred, thus allowing counterfactuals to be explained; something that is known to be a powerful explanation mechanism (Miller 2018b; Byrne 2019). Our model generates counterfactual explanations by comparing causal chains of the actual event occurred and the explanandum (counterfactual action). First, we define a counterfactual instantiation</p>
<p>that specifies the optimal state variable values under which the counterfactual action $B$ would be chosen.
Definition 5. A counterfactual instantiation for a counterfactual action $B$ is a model $M_{\vec{Z} s-S_{Z}^{\prime}}$, where $\vec{Z}$ gives the instantiation of all predecessor variables of action $B$ with current state values and the instantiation of all successor nodes (of $B$ ) of the causal chain by forward simulating, using the structural equations.</p>
<p>Informally, this gives the 'optimal' conditions (according to the action influence model) under which we would select counterfactual action $B$, simulated through structural equations. We unravel this further in the Example 1 discussion using the Starcraft II scenario.</p>
<p>In the following definition, we use $\vec{X}=\vec{x}$ to represent the tuple $\left(\vec{X}<em p="p">{p}=\vec{x</em>}}, \vec{X<em h="h">{h}=\vec{x}</em>}, \vec{X<em r="r">{r}=\vec{x}</em>$ for readability.
Definition 6. Given a minimally complete explanation $\vec{X}=$ $\vec{x}$ for action $A$ under the actual instantation, and a minimally complete explanation $\vec{Y}=\vec{y}$ for action $B$ under the counterfactual instantiation $M_{\vec{Z} s-S_{Z}^{\prime}}$ (from Definition 5), we define a minimally complete contrastive explanation as the tuple $\left(\vec{X}^{\prime}=\vec{x^{\prime}}, \vec{Y}^{\prime}=\vec{y^{\prime}}, \vec{X}}\right)$, and similar for $\vec{Y}=\vec{y<em r="r">{r}=\vec{x}</em>$ gives the reward nodes of action $A$.}\right)$ such that $\vec{X}^{\prime}$ is the maximal set of variables in $\vec{X}$ in which $\left(\vec{X}^{\prime}=\vec{x^{\prime}}\right) \cap\left(\vec{Y}^{\prime}=\vec{y^{\prime}}\right) \neq \emptyset$, where $\overrightarrow{x^{\prime}}$ is then contrasted with $\overrightarrow{y^{\prime}}$. That is, we only explain things that are different between the actual and counterfactual. This corresponds to the difference condition (Miller 2018a). And $\vec{X}_{r</p>
<p>Intuitively, a contrastive explanation extracts the actual causal chain for the taken action $A$, and the counterfactual causal chain for the $B$, and finds the differences.
Example 1. Consider the question, asking why a Starcraft II agent built supply depots, rather than choosing to build barracks:
Question Why not build_barracks $\left(A_{b}\right)$ ?
Explanation Because it is more desirable to do action build_supply_depot $\left(A_{s}\right)$ to have more Supply Depots $(S)$ as the goal is to have more Destroyed Units $\left(D_{u}\right)$ and Destroyed buildings $\left(D_{b}\right)$.</p>
<p>First we get the actual instantiation $m=$ $\left[W=12, S=1, B=2, A_{n}=22, D_{u}=10, D_{b}=7\right]$ (instantiation should include all variables in the current state, only the required ones are shown for readability). The causal chain for the actual action 'why $A_{s}$ ?' would be as in Figure 1, and for the counterfactual action 'why not $A_{b}$ ?', the causal chain nodes would be $B \rightarrow A_{n} \rightarrow\left[D_{u}, D_{b}\right]$. We then get the counterfactual instantiation $m^{\prime}=$ $\left[W=12, S=3, B=2, A_{n}=22, D_{u}=10, D_{b}=7\right]$ using Definition 5. Applying the difference condition here, we obtain the minimally complete contrastive explanation (from Definition 6) as the tuple $\left([S=1],[S=3],\left[D_{u}=10, D_{b}=7\right]\right) \quad$ and $\quad$ contrast $[S=1]$ with $[S=3]$ to obtain the explanation of Example 1 (generated using a simple NLP template).</p>
<h2>Learning Structural Causal Equations</h2>
<p>Our approach so far relies on knowing the structural model, in particular, to determine the effects of counterfactual actions. Why not questions are inherently counterfactual (Balke and Pearl 1995), and having just the policy of an agent is not enough to generate explanations as counterfactuals refers to possible worlds that did not happen. Consider the Example 1, to generate this explanation, the optimal/maximum value of the state variable $S$ is needed in the given time instance.</p>
<p>However, in model-free reinforcement learning, such environment dynamics are not known. And learning a model of the environment is a difficult problem. Though, when given a graph of causal relations between variables, learning a set of structural equations that are approximate yet 'good enough' to give counterfactual explanations maybe feasible.</p>
<p>To this end, we assume that a DAG specifying causal direction between variables is given, and learn the structural equations as multivariate regression models during the training phase of the RL agent. We perform experience replay (Mnih et al. 2015) by saving $e_{t}=\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$ at each time step $t$ in a data set $D_{t}=\left{e_{1}, \ldots, e_{t}\right}$. Then we update the sub-set of structural equations $F_{X . A}$ using a regression learner $\widehat{\mathbb{L}}<em s="s">{\left(s, a, r, s^{\prime}\right) \sim U(D)}$, in that we only update structural equations associated with the specific action in the experience frame, drawn uniformly as mini-batches from $D$. For example, from Figure 1, for any experience frame with the action $A</em>$, such as multi-layer perceptron regressors.}$, only the equation $F_{S . A_{s}}(W)$ will be updated. Any regression learner can be used as the learning model $\widehat{\mathbb{L}</p>
<p>While this approach may seem similar to learning environment dynamics of model-based RL methods, we only learn the structural equations, and we are only after an approximation that is good enough for explaining instances. Thus they can be approximate but still useful for explanation. Further, specifying the assumptions about the causal direction between variables is a much easier problem to encode by hand, and can be tested with the data.</p>
<h2>Computational Evaluation</h2>
<p>We evaluate action influence models in 5 OpenAI RL benchmark domains (Brockman et al. 2016) and in the Starcraft II domain. The goal of this evaluation is to determine if learning action influence models leads to models that are faithful to the problem. Task prediction accuracy and training time for the structural causal equations are measured. The purpose of task prediction is to evaluate if the model is accurate enough to predict what an agent will do next, under the assumption that if it is not, then the model will not be of use to a human explainee.</p>
<p>We computationally simulate task prediction using Algorithm 1. Here we instantiate all the equations (which are the set of regression models $\mathcal{L}$ ) with the values of the current state $S$ of the agent. We identify the equation that has maximum difference with the predicted state variable value and the actual, then get the action associated with it. This is informed by the reasoning that the agent will try to follow the</p>
<p>Algorithm 1 Task Prediction:Action Influence Model
Input: trained regression models $\mathcal{L}$, current state $S_{t}$
Output: predicted action $a$
1: $\hat{F_{p}} \leftarrow[]$; vector of predicted difference
2: for every $\widehat{L} \in \mathcal{L}$ do
3: $\quad P_{y} \leftarrow \widehat{L} \cdot \operatorname{predict}\left(S_{x . t}\right)$; predict variable $S_{y}$ at $S_{t+1}$
4: $\quad \hat{F_{p}} \leftarrow\left|S_{y}-P_{y}\right|$; difference with actual $S_{y}$ value
5: end for
6: return $\max \left(\hat{F_{p}}\right) \cdot \operatorname{getAction}()$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Env - RL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accuracy (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Performance (s)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">LR</td>
<td style="text-align: center;">DT</td>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">LR</td>
<td style="text-align: center;">DT</td>
<td style="text-align: center;">MLP</td>
</tr>
<tr>
<td style="text-align: center;">Cartpole-PG</td>
<td style="text-align: center;">$4 / 2$</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: center;">MountainCar-DQN</td>
<td style="text-align: center;">$3 / 3$</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">0.037</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: center;">Taxi-SARSA</td>
<td style="text-align: center;">$4 / 6$</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.49</td>
</tr>
<tr>
<td style="text-align: center;">LunarLander-DDQN</td>
<td style="text-align: center;">$8 / 4$</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">BipedalWalker-PPO</td>
<td style="text-align: center;">$14 / 4$</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">0.010</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;">Starcraft-A2C</td>
<td style="text-align: center;">$9 / 4$</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">3.33</td>
</tr>
</tbody>
</table>
<p>Table 1: Action influence model evaluation in 6 benchmark reinforcement learning domains (using different RL algorithms, PG, DQN etc.), measuring mean task prediction accuracy and training time of the structural causal equations in 100 episodes after training.
optimal policy, and the action with the biggest impact to correct the policy will be executed. The impact is measured by the above mentioned difference. This is itself an approximation, but is a useful guide for task prediction.</p>
<p>We use linear SGD regression (LR), decision tree regression (DT) and multilayer perceptron regression (MLP) as the learners that approximate the structural equations. We choose benchmark domains based on varying levels of complexity, size (state features/number of actions) and train them using various RL algorithms to demonstrate the robustness of the model. Table 1 summarises the results of task prediction and time taken to train the structural equations given the replay data.</p>
<p>Overall, the results show the model did a reasonable job of task prediction, providing evidence that this could be useful for explanations. Domains that have a clear causal structure (e.g Starcraft) performs best in task prediction. Considering the performance cost it incurs, there was little gained by using MLP to approximate the equations, where in most cases linear regression is adequate. Apart from the BipedalWalker domain, our model performs well in task prediction with a negligible performance hit. The bipedalWalker domain has continuous actions, which our current model cannot handle accurately. We plan to extend our model to continuous actions in future work.</p>
<h2>Empirical Evaluation: Human Study</h2>
<p>A human-grounded evaluation is essential to evaluate the explainability of a system, thus we carry out human-subject experiments involving explaining RL agents. We present two main hypotheses for the empirical evaluation; H1) Causal-
model-based explanations build better mental models of the agent leading to a better understanding of its strategies (We make the assumption here that there is no intermediate effect on the mental model from other sources); and H2) Better understanding of an agent's strategies promotes trust in the agent.</p>
<p>Methodology: We use StarCraft II, a real-time strategy game and a popular RL environment (Vinyals et al. 2017) as the domain. We implemented a RL agent for our experiment that competes in the default map.</p>
<p>To evaluate hypothesis (H1), we use the method of task prediction (Hoffman et al. 2018). Task prediction can provide a quick view of the explainee's mental model formed through explanations, where the task is for the participant to predict 'What will the agent do next?'. We use the 5-point Likert Explanation Satisfaction Scale developed by Hoffman et al. (2018, p.39) to measure the subjective quality of explanations. To evaluate hypothesis (H2), we use the 5point Likert Trust Scale of Hoffman et al. (2018, p.49). We obtained ethics approval from The University of Melbourne human research ethics committee (ID-1953619.1).</p>
<p>Experiment Design: We use a recording of a full gameplay video ( 22 min ) with the RL agents playing against ingame bot AI. The experiment has 4 phases.</p>
<p>Phase 1 involves collecting demographic information and training the participants. Using five gameplay video clips, the participant is trained to understand and differentiate the actions of the agent.</p>
<p>In phase 2, a clip of the gameplay video ( 15 sec ) is played in a web-based UI, with a textual description of the scene. The participant can select the question type (why/why not) and the action, which together forms a question 'Why/Why not action $A$ ?'. Then, the textual explanation for the question with a figure of the relevant sub-graph of the agent's action influence graph is displayed. Explanations are pre-generated from our implemented algorithm. The participant can ask multiple questions in a single gameplay video. After every gameplay video, the participant completes the Explanation Satisfaction Scale. This process is repeated so we have data for each participant from five videos.</p>
<p>The third phase measures the understanding the explainee has after seeing the gameplay and the explanations. We measure understanding using the task prediction method as follows: the participant is presented with another gameplay video ( 10 sec ), and presented with three selections of textual descriptions of what action the agent will do in next step; the participant selects an option, which includes 'I don't know'. We expect the participant is projecting forward the local strategy of the agent using their mental model. This mental model is formed through (or helped by) explanations seen in phase 1. This process is repeated for 8 tasks. In 4 of the task predictions, the behaviour is explainable using a causal chain previously seen in the training, but with different variable values. In the other 4 tasks, the behaviour is novel, but can be inferred by combining causal chains from different training tasks. In the fourth phase, the participant completes a 5-point Trust Scale.</p>
<p>We conducted the experiments on Amazon MTurk,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Box plot of task prediction scores of explanation models, $\mathrm{T}=$ total score, $\mathrm{F}=$ familiar score, $\mathrm{N}=$ novel score (higher is better, means represented as bold dots).
a crowd-sourcing platform popular for obtaining data for human-subject experiments (Buhrmester, Kwang, and Gosling 2011). The experiment was fully implemented in an interactive web-based environment. We excluded noisy data of users in 3 ways. First, we tested participants to ensure they had learnt about the agent's actions by prompting them to identify them. If the participant failed this, the experiment did not proceed. Then, for participants who completed, we omitted their data from analysis based on two criteria: 1) if the threshold of the time the participant spent on viewing explanations and answering tasks is below a few seconds, which was deemed too short to learn anything useful; and 2) if the participant's textual responses to explain their task prediction choice were gibberish text or a 1-2 word response, as this indicated lack of engagement and care in the task. We controlled for language by only recruiting participants from the US.</p>
<p>Experiment Parameters: The experiment was run with 4 independent variables. We tested abstract (C) and detailed (D) versions of our action influence models and 2 baseline models described below: 1) Gameplay video without any explanations (N); 2) Relevant variable explanations (R). These explanations are generated using state relevant variables using template 1 of Khan, Poupart, and Black (2009, p.3) and visualized through a state-action graph, e.g 'Action $A$ is likely to increase relevant variable $P$ '; 3) Detailed action influence model explanations, where the causal graph is augmented to include atomic actions.</p>
<p>We ran experiments for $\mathbf{1 2 0}$ participants, allocated evenly to the independent variables. Each experiment ran for approximately 40 minutes. We scored each participant on task prediction, 2 points for a correct prediction; 1 for responding 'I don't know' and 0 for an incorrect prediction for a total of 16 points. Scores were tallied. We compensated each participant with 8.5 USD . Of the 120 participants, 36 were female, 82 male and 2 were not given. Participants were aged between 19 to $59(\mu=34.2)$ and had an average self-rated gaming experience and Starcraft II experience of 3.38 and 2.02 (5-point Likert) respectively.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Box plot of explanation quality (likert scale 1-5, higher is better, means represented as dots).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Box plot of trust (likert scale 1-5, higher is better, means represented as dots).</p>
<h2>Results</h2>
<p>Task Prediction: For the first hypothesis, the corresponding null and alternative hypotheses are: 1) $H_{0}: \mu_{C}=\mu_{R}=$ $\mu_{D}=\mu_{N}$; 2) $H_{1}: \mu_{C} \geq \mu_{R}$; 3) $H_{2}: \mu_{C} \geq \mu_{D}$; 4) $H_{3}: \mu_{C} \geq \mu_{N}$, where abstract causal explanations (our model), detailed causal explanations, relevant variable explanations, and no explanations are given by $\mathrm{C}, \mathrm{D}, \mathrm{R}$, and N respectively.
We conduct one-way ANOVA (Figure 2 illustrates the task score variance with explanation models). We obtained a p-value of $\mathbf{0 . 0 0 3}\left(\mu_{C}=10.90, \mu_{D}=10.20, \mu_{R}=8.97\right.$, $\mu_{N}=8.53$ ), thus we conclude there are significant differences between models on task prediction scores. We performed Tukey multiple pairwise-comparisons to obtain the significance between groups. From Table 2, the differences between the causal explanation model paired with other explanation models are significant for $\mathrm{C}-\mathrm{R}$ and $\mathrm{C}-\mathrm{N}$ pairs with p -values of $\mathbf{0 . 0 0 6}$ and $\mathbf{0 . 0 3 4}$. Additionally, we calculate the effect of the number of questions on the score, and obtain no statistical correlation using a correlation test (number of questions vs score, $\mathrm{p}=0.33$, model C) among same models. Because participants could select "I don't know" and receive 8 out of 16 , we also further analyse scores based on $2=$ 'correct', $0=$ 'incorrect or 'I don't know', and obtain results that are still significant ( $\mathrm{p}=0.004$ ), means ( $\mathrm{C}=10.90$, $\mathrm{D}=10.10, \mathrm{R}=8.93, \mathrm{~N}=8.47$ ), for model pairs ( $\mathrm{C}-\mathrm{N} \mathrm{p}=0.005$, $\mathrm{C}-\mathrm{R} \mathrm{p}=0.035$ ). We conducted Pearson's Chi-Square as a non-parametric test on the task prediction scores, which showed significant results ( p -value $=0.008$, X -squared $=$ 17.281).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model pair</th>
<th style="text-align: center;">mean-diff</th>
<th style="text-align: center;">lwr</th>
<th style="text-align: center;">upr</th>
<th style="text-align: center;">p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">C - N</td>
<td style="text-align: center;">$\mathbf{2 . 4 0 0}$</td>
<td style="text-align: center;">0.534</td>
<td style="text-align: center;">4.265</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 6}$</td>
</tr>
<tr>
<td style="text-align: left;">C - R</td>
<td style="text-align: center;">$\mathbf{1 . 9 6 6}$</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">3.832</td>
<td style="text-align: center;">$\mathbf{0 . 0 3 4}$</td>
</tr>
<tr>
<td style="text-align: left;">D - N</td>
<td style="text-align: center;">1.666</td>
<td style="text-align: center;">-0.198</td>
<td style="text-align: center;">3.532</td>
<td style="text-align: center;">0.097</td>
</tr>
<tr>
<td style="text-align: left;">D - R</td>
<td style="text-align: center;">1.233</td>
<td style="text-align: center;">-0.632</td>
<td style="text-align: center;">3.098</td>
<td style="text-align: center;">0.316</td>
</tr>
<tr>
<td style="text-align: left;">C - D</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 3}$</td>
<td style="text-align: center;">-1.132</td>
<td style="text-align: center;">2.598</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 5}$</td>
</tr>
<tr>
<td style="text-align: left;">R - N</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">-1.432</td>
<td style="text-align: center;">2.298</td>
<td style="text-align: center;">0.930</td>
</tr>
</tbody>
</table>
<p>Table 2: Pairwise-comparisons of explanation models of task prediction scores (higher positive diff is better)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">Mdl-pair</th>
<th style="text-align: center;">Mean-dif</th>
<th style="text-align: center;">Median-dif</th>
<th style="text-align: center;">p-val</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Complete</td>
<td style="text-align: center;">C-N</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.061</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">C-R</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">$\mathbf{0 . 0 1 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Sufficient</td>
<td style="text-align: center;">C-N</td>
<td style="text-align: center;">0.746</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">$\mathbf{0 . 0 3 9}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">C-R</td>
<td style="text-align: center;">1.013</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Satisfying</td>
<td style="text-align: center;">C-N</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.082</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">C-R</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">$\mathbf{0 . 0 2 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Understand</td>
<td style="text-align: center;">C-N</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.497</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">C-R</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.316</td>
</tr>
</tbody>
</table>
<p>Table 3: Explanation quality (likert scale data 1-5)</p>
<p>Therefore we reject $H_{0}$ and $H_{2}$ and accept all other alternative hypotheses. Our results show that causal model explanations lead to a significantly better understanding of agent's strategies than the 2 baselines we evaluated, especially against previous models of relevant explanations. Participants did slightly worse on tasks with novel behaviour.</p>
<p>Explanation Quality: Figure 3 depicts the likert scale data of explanation metrics (understand, satisfying, sufficient detail and complete) for aggregated video explanations of explanation models. As before we performed a pair-wise ANOVA test, results are summarised in Table 3. Our model obtained statistically significant results and outperformed the benchmark 'relevant explanation' (R) for all metrics except 'Understand'.</p>
<p>Trust: For the second main hypothesis (H2) that investigate whether explanation models promote trust, the obtained p-values for trust metrics confident, predictable, reliable and safe were not statistically significant (using pairwise ANOVA). Although the difference is not significant we can see causal models have high means and medians (see Figure 4). We conclude that while the explanation quality and scores are significantly better for our model, to promote trust further interaction is necessary; or perhaps our RL agent is simply not a trustworthy Starcraft II player.</p>
<p>We further analysed self-reported demographic data to see if there is a correlation between task prediction scores and self-reported Starcraft II experience level (5-point Likert). Pearson's correlation test was not significant ( $\mathrm{p}=0.45$ ) thus we conclude there is no correlation between scores and experience level. This can possibly be attributed to our Starcraft II scenario differing from the standard game.</p>
<p>A limitation of our experiment is that we made a strong linearity assumption for Starcraft II, which enabled linear
regression to learn SCMs for a relatively small number (9) of state variables.</p>
<h2>Conclusion</h2>
<p>In this paper, we introduced action influence models for model-free reinforcement learning agents. Our approach learns a structural causal model (SCM) during reinforcement learning and has the ability to generate explanations for why and why not questions by counterfactual analysis of the learned SCM. We computationally evaluated our model in 6 benchmark RL domains on task prediction. We then conducted a human study ( $\mathbf{n}=\mathbf{1 2 0}$ ) to evaluate our model on 1) task prediction, 2) explanation 'goodness' and 3) trust. Results show that our model performs significantly better in the first 2 evaluation criteria. One weakness of our approach is that the causal model must be given beforehand. Future work includes using epistemic knowledge of the explainee to provide explanations that are more targeted, and extending the model to continuous domains.</p>
<h2>Acknowledgment</h2>
<p>This research was supported by the University of Melbourne research scholarship (MRS) and by Australian Research Council Discovery Grant DP190103414: Explanation in Artificial Intelligence: A Human-Centred Approach. The authors also thank Fatma Faruq for the valuable feedback on early drafts of this paper.</p>
<h2>References</h2>
<p>Amir, D., and Amir, O. 2018. Highlights: Summarizing agent behavior to people. In Proc. of the 17th International conference on Autonomous Agents and Multi-Agent Systems (AAMAS).
Balke, A., and Pearl, J. 1995. Counterfactuals and policy analysis in structural models. In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence, 11-18. Morgan Kaufmann Publishers Inc.
Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Schulman, J.; Tang, J.; and Zaremba, W. 2016. Openai gym.
Buhrmester, M.; Kwang, T.; and Gosling, S. D. 2011. Amazon's mechanical turk: A new source of inexpensive, yet high-quality, data? Perspectives on psychological science $6(1): 3-5$.
Byrne, R. M. J. 2019. Counterfactuals in explainable artificial intelligence (xai): Evidence from human reasoning. In Proceedings of the 28th International Joint Conferences on Artificial Intelligence, 6276-6282.
Chandrasekaran, B.; Tanner, M. C.; and Josephson, J. R. 1989. Explaining control strategies in problem solving. IEEE Intelligent Systems (1):9-15.
Coppens, Y.; Efthymiadis, K.; Lenaerts, T.; and Nowe, A. 2019. Distilling deep reinforcement learning policies in soft decision trees. In Miller, T.; Weber, R.; and Magazzeni, D., eds., Proceedings of the IJCAI 2019 Workshop on Explainable Artificial Intelligence, 1-6.</p>
<p>De Graaf, M. M. A., and Malle, B. F. 2017. How People Explain Action (and Autonomous Intelligent Systems Should Too). AAAI 2017 Fall Symposium on "AI-HRI" 19-26.
Elizalde, F., and Sucar, L. E. 2009. Expert evaluation of probabilistic explanations. In ExaCt, 1-12.
Elizalde, F.; Sucar, L. E.; Luque, M.; Diez, J.; and Reyes, A. 2008. Policy explanation in factored markov decision processes. In Proceedings of the 4th European Workshop on Probabilistic Graphical Models (PGM 2008), 97-104.
Fukuchi, Y.; Osawa, M.; Yamakawa, H.; and Imai, M. 2017. Autonomous self-explanation of behavior for interactive reinforcement learning agents. In Proceedings of the 5th International Conference on Human Agent Interaction, 97-101. ACM.
Halpern, J. Y., and Pearl, J. 2005. Causes and explanations: A structural-model approach. part II: Explanations. The British journal for the philosophy of science 56(4):889911.</p>
<p>Hayes, B., and Shah, J. A. 2017. Improving robot controller transparency through autonomous policy explanation. In Proceedings of the 2017 ACM/IEEE international conference on human-robot interaction, 303-312. ACM.
Hilton, D. 2007. Causal explanation. Social psychology: Handbook of basic principles 232-253.
Hoffman, R. R.; Mueller, S. T.; Klein, G.; and Litman, J. 2018. Metrics for explainable AI: Challenges and prospects. arXiv preprint arXiv:1812.04608.
Juozapaitis, Z.; Koul, A.; Ferm, A.; Erwig, M.; and DoshiVelez, F. 2019. Explainable reinforcement learning via reward decomposition. In Miller, T.; Weber, R.; and Magazzeni, D., eds., Proceedings of the IJCAI 2019 Workshop on Explainable Artificial Intelligence, 47-53.
Khan, O. Z.; Poupart, P.; and Black, J. P. 2009. Minimal sufficient explanations for factored markov decision processes. In ICAPS.
Lim, B. Y.; Dey, A. K.; and Avrahami, D. 2009. Why and why not explanations improve the intelligibility of context-aware intelligent systems. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 2119-2128. ACM.
Madumal, P.; Miller, T.; Sonenberg, L.; and Vetere, F. 2019. A grounded interaction protocol for explainable artificial intelligence. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, 10331041.</p>
<p>Madumal, P. 2019. Explainable agency in intelligent agents: Doctoral consortium. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, 2432-2434.
McClure, J., and Hilton, D. 1997. For you can't always get what you want: When preconditions are better explanations than goals. British Journal of Social Psychology 36(2):223240.</p>
<p>Miller, T. 2018a. Contrastive explanation: A structuralmodel approach. arXiv preprint arXiv:1811.03163.</p>
<p>Miller, T. 2018b. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al. 2015. Humanlevel control through deep reinforcement learning. Nature 518(7540):529.
Nielsen, U.; Pellet, J.-P.; and Elisseeff, A. 2012. Explanation trees for causal bayesian networks. arXiv preprint arXiv:1206.3276.
Salmon, W. C. 1984. Scientific explanation and the causal structure of the world. Princeton University Press.
Sloman, S. 2005. Causal models: How people think about the world and its alternatives. Oxford University Press.
van der Waa, J.; van Diggelen, J.; Bosch, K. v. d.; and Neerincx, M. 2018. Contrastive explanations for reinforcement learning in terms of expected consequences. arXiv preprint arXiv:1807.08706.
Vinyals, O.; Ewalds, T.; Bartunov, S.; Georgiev, P.; Vezhnevets, A. S.; Yeo, M.; Makhzani, A.; Küttler, H.; Agapiou, J.; Schrittwieser, J.; et al. 2017. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Copyright (C) 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
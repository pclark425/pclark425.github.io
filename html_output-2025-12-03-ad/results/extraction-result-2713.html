<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2713 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2713</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2713</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-2872916</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/1705.05637v1.pdf" target="_blank">Text-based Adventures of the Golovin AI Agent</a></p>
                <p><strong>Paper Abstract:</strong> The domain of text-based adventure games has been recently established as a new challenge of creating the agent that is both able to understand natural language, and acts intelligently in text-described environments. In this paper, we present our approach to tackle the problem. Our agent, named Golovin, takes advantage of the limited game domain. We use genre-related corpora (including fantasy books and decompiled games) to create language models suitable to this domain. Moreover, we embed mechanisms that allow us to specify, and separately handle, important tasks as fighting opponents, managing inventory, and navigating on the game map. We validated usefulness of these mechanisms, measuring agent's performance on the set of 50 interactive fiction games. Finally, we show that our agent plays on a level comparable to the winner of the last year Text-Based Adventure AI Competition.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2713.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2713.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Golovin AI agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent for playing text-based interactive fiction that combines domain-specific command pattern databases, word2vec embeddings, LSTM+attention language models for weighting text, and several memory-like mechanisms (map graph, per-location blacklists, best-run replay) to guide action selection and navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hybrid system that uses (1) a large database of extracted command patterns (from tutorials, walkthroughs, decompiled games), (2) word2vec embeddings to find synonyms and score command relevance, (3) an LSTM network with attention to weight words in scene descriptions for command scoring, plus task-specific subsystems (battle mode, inventory manager) and memory mechanisms (map graph of locations, per-location blacklists of failed commands, replayable best-action-sequence).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>50 Z-machine interactive fiction games (Text-Based Adventure AI Competition set)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Classic single-player text-based adventure (interactive fiction) games run in a Z-machine interpreter; tasks involve natural-language understanding of room descriptions, object interactions, inventory management, navigation in rooms/mazes, puzzle solving and combat; scoring is game-specific and often sparse/delayed.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic / graph-based / short-term replay memory (multiple mechanisms)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Graph-based map (nodes labeled by first sentence of area description, directed edges labeled by move commands), per-location blacklists of failed commands, and stored best-run action sequence (linear list) for replay; inventories and counts of past commands also recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Past movements (paths), area labels (first-sentence descriptions), movement edges (commands used to move), lists of commands tried and blacklisted per location (failed commands), the best-scoring action sequence from previous runs, and inventory changes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Heuristic graph search over stored map (shortest-path planning to candidate destinations, scoring destinations by distance plus a curiosity metric based on untested commands and command scores); per-location lookup of blacklists to skip failed commands; replay of stored best sequence on final trial or after restart; when area description changes, regenerate command lists using current state and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Map is updated incrementally from the agent's past movements; MergeNodes procedure attempts to merge nodes with the same label by joining outgoing edges (recursive merging) or rolling back if merge fails; a command that leaves the description unchanged is recorded on the current location's blacklist; blacklists are cleared after any change in inventory; best-sequence is updated when a better-score run is found; restarts avoid repeating recently fatal command sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Navigation (use stored map to select promising unexplored or curiosity-rich destinations), state tracking (avoid repeating failed actions at the same location via blacklists), efficiency/imitation (replay best action sequence found across runs), and to help exploration planning (curiosity scoring of nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: authors report that enabling the map-memory improves performance 'but only to some extent' (see Figure 1 in paper). No per-game numeric performance is attributed solely to memory in text, but overall Golovin (with its memory mechanisms active in main experiments) achieved competitive results vs BYU-Agent: Golovin scored better in 12 games and worse in 11 games (out of 24 games where any agent scored), and achieved nonzero scores on several games the comparator did not.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Qualitative: ablation experiments toggling map (memory) on/off are reported; authors state 'map is useful (but only to some extent)' and present a four-condition comparison (map on/off × battle mode on/off) in Figure 1. No explicit numeric breakdown for 'with memory' vs 'without memory' is printed in text, so precise numeric baselines are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Reported findings: (1) map (a graph-like memory) provides measurable but modest benefit to performance; (2) battle-mode (a reactive combat behavior) is more strongly beneficial than map-memory; (3) per-location blacklists and replay of best sequences help avoid repeating failing or fatal actions and improve effective exploration; (4) limitations of memory arise from ambiguous or changing descriptions (same description for different locations or changing area descriptions), which reduces map reliability and makes naive mapping risky.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Authors note key limitations: area descriptions can change or different areas can share identical descriptions (labeling nodes by first sentence introduces ambiguity), which makes mapping imperfect; these ambiguities constrain the usefulness of the map and require conservative merge heuristics; additionally, they do not report extensive capacity/scalability analysis so practical memory limits are unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Paper's experiments indicate configuration with memory (map enabled) and battle mode enabled is overall best; the authors explicitly tested map on vs off and found the 'map is useful (but only to some extent)', while battle mode had a larger positive effect.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2713.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2713.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Map-memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-based map memory (node-label merging)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph memory of visited areas where nodes are labeled by the first sentence of the area's description and edges are labeled with the movement command; a MergeNodes heuristic attempts to merge nodes with identical labels while preserving outgoing transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>See Golovin entry; map-memory is a component of Golovin used for navigation/planning.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>50 Z-machine interactive fiction games (Text-Based Adventure AI Competition set)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>See Golovin entry.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based (environmental/episodic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Directed graph: nodes labeled by first sentence of area description; edges labeled by movement commands; MergeNodes procedure merges nodes and their outgoing edges recursively when heuristics deem labels equivalent.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Visited nodes (area labels), recorded outgoing movement commands and transitions, and metadata used to evaluate curiosity/unexplored actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Graph search (shortest path) to candidate destination nodes; destinations scored by distance plus a curiosity factor (number of tested commands divided by node curiosity derived from available command scores and untested movements).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>On each move the path is appended to the map; periodically pairs of nodes with the same label are considered for MergeNodes merging; merges are applied if consistent or rolled back otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Guide exploration/navigation to promising or under-explored nodes; to avoid redundant exploration and to allow planning multi-step movements toward nodes with high curiosity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: 'map is useful (but only to some extent)' as reported by authors; presented as part of an ablation comparing map on/off (see Figure 1) but no numeric values are printed in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Qualitative: ablation suggests some drop in effectiveness when map disabled, but exact metrics are not provided in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Effectiveness is limited by ambiguous or mutable textual descriptions; using just the first sentence as label works often but causes potential incorrect merges when different locations share the same first sentence or when descriptions change, reducing the map's reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Ambiguity from: (a) same textual description for different locations, (b) area descriptions changing over time; MergeNodes must be conservative and can fail, limiting map completeness; no quantitative capacity or long-horizon planning evaluation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Map enabled together with other mechanisms (especially battle mode) produced best qualitative performance; map alone gives modest gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2713.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2713.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Per-location blacklist & replay memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-location failed-command blacklist and best-run replay memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A short-term episodic memory mechanism that records commands that failed at a location (when the game state doesn't change after executing a command) and stores the best action sequence found across runs to be replayed on a final trial, also avoiding recently fatal sequences after restarts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>See Golovin entry; these mechanisms are used to avoid repeating failed actions and to exploit successful sequences found during exploration/restarts.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>50 Z-machine interactive fiction games (Text-Based Adventure AI Competition set)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>See Golovin entry.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic short-term action memory / behavioral memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Per-location blacklist (set/list of commands flagged as failed for that location) and a stored linear sequence representing the best-scoring run (list of commands/movements).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Failed commands at particular locations, stored best action/movement sequence leading to highest score found, and records of last commands that led to death for avoidance after restart.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>On generating command lists for current location, exclude commands present in that location's blacklist; after a restart, avoid repeating the last-fatal commands and optionally replay stored best sequence for final trial.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>When a command leaves the game description unchanged it is appended to the current location's blacklist; blacklists are cleared whenever inventory changes; when a run reaches a new best score the best-sequence storage is updated.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Avoid repeating previously failing commands (reduces wasted turns), mitigate repeated deaths by avoiding recently fatal sequences, and exploit successful runs by replaying the best sequence as a final result.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: authors assert these mechanisms improve survival and efficiency (e.g. prevent repeated deaths and spending turns on ineffective commands); no numeric breakdown isolating their contribution is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not explicitly quantified in the paper; per-location blacklists and replay are part of the full agent in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Blacklisting failed commands prevents repeated futile attempts at the same location and clearing blacklists on inventory change allows reattempts when context changed; replaying best sequences emulates human retry behavior and is used as the agent's final reported result.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Blacklists can be over-restrictive if the environment changes (but authors mitigate by clearing on inventory change); storing only the best sequence may miss other useful variations; no analysis of false-negative/false-positive blacklisting rates is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Using both per-location blacklists (with clearing on inventory change) and best-sequence replay in combination with other systems (map, language models, battle mode) produced the reported competitive results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Understanding for Text-based Games using Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Goal Achievement in Partially Known, Partially Observable Domains <em>(Rating: 2)</em></li>
                <li>Knowledge-gathering agents in adventure games <em>(Rating: 1)</em></li>
                <li>What can you do with a rock? Affordance extraction via word embeddings <em>(Rating: 1)</em></li>
                <li>Learning to Win by Reading Manuals in a Monte-Carlo Framework <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2713",
    "paper_id": "paper-2872916",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "Golovin",
            "name_full": "Golovin AI agent",
            "brief_description": "An agent for playing text-based interactive fiction that combines domain-specific command pattern databases, word2vec embeddings, LSTM+attention language models for weighting text, and several memory-like mechanisms (map graph, per-location blacklists, best-run replay) to guide action selection and navigation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Golovin",
            "agent_description": "Hybrid system that uses (1) a large database of extracted command patterns (from tutorials, walkthroughs, decompiled games), (2) word2vec embeddings to find synonyms and score command relevance, (3) an LSTM network with attention to weight words in scene descriptions for command scoring, plus task-specific subsystems (battle mode, inventory manager) and memory mechanisms (map graph of locations, per-location blacklists of failed commands, replayable best-action-sequence).",
            "base_model_size": null,
            "game_benchmark_name": "50 Z-machine interactive fiction games (Text-Based Adventure AI Competition set)",
            "game_description": "Classic single-player text-based adventure (interactive fiction) games run in a Z-machine interpreter; tasks involve natural-language understanding of room descriptions, object interactions, inventory management, navigation in rooms/mazes, puzzle solving and combat; scoring is game-specific and often sparse/delayed.",
            "uses_memory": true,
            "memory_type": "episodic / graph-based / short-term replay memory (multiple mechanisms)",
            "memory_structure": "Graph-based map (nodes labeled by first sentence of area description, directed edges labeled by move commands), per-location blacklists of failed commands, and stored best-run action sequence (linear list) for replay; inventories and counts of past commands also recorded.",
            "memory_content": "Past movements (paths), area labels (first-sentence descriptions), movement edges (commands used to move), lists of commands tried and blacklisted per location (failed commands), the best-scoring action sequence from previous runs, and inventory changes.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Heuristic graph search over stored map (shortest-path planning to candidate destinations, scoring destinations by distance plus a curiosity metric based on untested commands and command scores); per-location lookup of blacklists to skip failed commands; replay of stored best sequence on final trial or after restart; when area description changes, regenerate command lists using current state and memory.",
            "memory_update_strategy": "Map is updated incrementally from the agent's past movements; MergeNodes procedure attempts to merge nodes with the same label by joining outgoing edges (recursive merging) or rolling back if merge fails; a command that leaves the description unchanged is recorded on the current location's blacklist; blacklists are cleared after any change in inventory; best-sequence is updated when a better-score run is found; restarts avoid repeating recently fatal command sequences.",
            "memory_usage_purpose": "Navigation (use stored map to select promising unexplored or curiosity-rich destinations), state tracking (avoid repeating failed actions at the same location via blacklists), efficiency/imitation (replay best action sequence found across runs), and to help exploration planning (curiosity scoring of nodes).",
            "performance_with_memory": "Qualitative: authors report that enabling the map-memory improves performance 'but only to some extent' (see Figure 1 in paper). No per-game numeric performance is attributed solely to memory in text, but overall Golovin (with its memory mechanisms active in main experiments) achieved competitive results vs BYU-Agent: Golovin scored better in 12 games and worse in 11 games (out of 24 games where any agent scored), and achieved nonzero scores on several games the comparator did not.",
            "performance_without_memory": "Qualitative: ablation experiments toggling map (memory) on/off are reported; authors state 'map is useful (but only to some extent)' and present a four-condition comparison (map on/off × battle mode on/off) in Figure 1. No explicit numeric breakdown for 'with memory' vs 'without memory' is printed in text, so precise numeric baselines are not provided.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Reported findings: (1) map (a graph-like memory) provides measurable but modest benefit to performance; (2) battle-mode (a reactive combat behavior) is more strongly beneficial than map-memory; (3) per-location blacklists and replay of best sequences help avoid repeating failing or fatal actions and improve effective exploration; (4) limitations of memory arise from ambiguous or changing descriptions (same description for different locations or changing area descriptions), which reduces map reliability and makes naive mapping risky.",
            "memory_limitations": "Authors note key limitations: area descriptions can change or different areas can share identical descriptions (labeling nodes by first sentence introduces ambiguity), which makes mapping imperfect; these ambiguities constrain the usefulness of the map and require conservative merge heuristics; additionally, they do not report extensive capacity/scalability analysis so practical memory limits are unspecified.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": "Paper's experiments indicate configuration with memory (map enabled) and battle mode enabled is overall best; the authors explicitly tested map on vs off and found the 'map is useful (but only to some extent)', while battle mode had a larger positive effect.",
            "uuid": "e2713.0"
        },
        {
            "name_short": "Map-memory",
            "name_full": "Graph-based map memory (node-label merging)",
            "brief_description": "A graph memory of visited areas where nodes are labeled by the first sentence of the area's description and edges are labeled with the movement command; a MergeNodes heuristic attempts to merge nodes with identical labels while preserving outgoing transitions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Golovin",
            "agent_description": "See Golovin entry; map-memory is a component of Golovin used for navigation/planning.",
            "base_model_size": null,
            "game_benchmark_name": "50 Z-machine interactive fiction games (Text-Based Adventure AI Competition set)",
            "game_description": "See Golovin entry.",
            "uses_memory": true,
            "memory_type": "graph-based (environmental/episodic memory)",
            "memory_structure": "Directed graph: nodes labeled by first sentence of area description; edges labeled by movement commands; MergeNodes procedure merges nodes and their outgoing edges recursively when heuristics deem labels equivalent.",
            "memory_content": "Visited nodes (area labels), recorded outgoing movement commands and transitions, and metadata used to evaluate curiosity/unexplored actions.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Graph search (shortest path) to candidate destination nodes; destinations scored by distance plus a curiosity factor (number of tested commands divided by node curiosity derived from available command scores and untested movements).",
            "memory_update_strategy": "On each move the path is appended to the map; periodically pairs of nodes with the same label are considered for MergeNodes merging; merges are applied if consistent or rolled back otherwise.",
            "memory_usage_purpose": "Guide exploration/navigation to promising or under-explored nodes; to avoid redundant exploration and to allow planning multi-step movements toward nodes with high curiosity.",
            "performance_with_memory": "Qualitative: 'map is useful (but only to some extent)' as reported by authors; presented as part of an ablation comparing map on/off (see Figure 1) but no numeric values are printed in text.",
            "performance_without_memory": "Qualitative: ablation suggests some drop in effectiveness when map disabled, but exact metrics are not provided in the paper text.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Effectiveness is limited by ambiguous or mutable textual descriptions; using just the first sentence as label works often but causes potential incorrect merges when different locations share the same first sentence or when descriptions change, reducing the map's reliability.",
            "memory_limitations": "Ambiguity from: (a) same textual description for different locations, (b) area descriptions changing over time; MergeNodes must be conservative and can fail, limiting map completeness; no quantitative capacity or long-horizon planning evaluation reported.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": "Map enabled together with other mechanisms (especially battle mode) produced best qualitative performance; map alone gives modest gains.",
            "uuid": "e2713.1"
        },
        {
            "name_short": "Per-location blacklist & replay memory",
            "name_full": "Per-location failed-command blacklist and best-run replay memory",
            "brief_description": "A short-term episodic memory mechanism that records commands that failed at a location (when the game state doesn't change after executing a command) and stores the best action sequence found across runs to be replayed on a final trial, also avoiding recently fatal sequences after restarts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Golovin",
            "agent_description": "See Golovin entry; these mechanisms are used to avoid repeating failed actions and to exploit successful sequences found during exploration/restarts.",
            "base_model_size": null,
            "game_benchmark_name": "50 Z-machine interactive fiction games (Text-Based Adventure AI Competition set)",
            "game_description": "See Golovin entry.",
            "uses_memory": true,
            "memory_type": "episodic short-term action memory / behavioral memory",
            "memory_structure": "Per-location blacklist (set/list of commands flagged as failed for that location) and a stored linear sequence representing the best-scoring run (list of commands/movements).",
            "memory_content": "Failed commands at particular locations, stored best action/movement sequence leading to highest score found, and records of last commands that led to death for avoidance after restart.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "On generating command lists for current location, exclude commands present in that location's blacklist; after a restart, avoid repeating the last-fatal commands and optionally replay stored best sequence for final trial.",
            "memory_update_strategy": "When a command leaves the game description unchanged it is appended to the current location's blacklist; blacklists are cleared whenever inventory changes; when a run reaches a new best score the best-sequence storage is updated.",
            "memory_usage_purpose": "Avoid repeating previously failing commands (reduces wasted turns), mitigate repeated deaths by avoiding recently fatal sequences, and exploit successful runs by replaying the best sequence as a final result.",
            "performance_with_memory": "Qualitative: authors assert these mechanisms improve survival and efficiency (e.g. prevent repeated deaths and spending turns on ineffective commands); no numeric breakdown isolating their contribution is provided.",
            "performance_without_memory": "Not explicitly quantified in the paper; per-location blacklists and replay are part of the full agent in reported experiments.",
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Blacklisting failed commands prevents repeated futile attempts at the same location and clearing blacklists on inventory change allows reattempts when context changed; replaying best sequences emulates human retry behavior and is used as the agent's final reported result.",
            "memory_limitations": "Blacklists can be over-restrictive if the environment changes (but authors mitigate by clearing on inventory change); storing only the best sequence may miss other useful variations; no analysis of false-negative/false-positive blacklisting rates is provided.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": "Using both per-location blacklists (with clearing on inventory change) and best-sequence replay in combination with other systems (map, language models, battle mode) produced the reported competitive results.",
            "uuid": "e2713.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Understanding for Text-based Games using Deep Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "language_understanding_for_textbased_games_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "Goal Achievement in Partially Known, Partially Observable Domains",
            "rating": 2,
            "sanitized_title": "goal_achievement_in_partially_known_partially_observable_domains"
        },
        {
            "paper_title": "Knowledge-gathering agents in adventure games",
            "rating": 1,
            "sanitized_title": "knowledgegathering_agents_in_adventure_games"
        },
        {
            "paper_title": "What can you do with a rock? Affordance extraction via word embeddings",
            "rating": 1,
            "sanitized_title": "what_can_you_do_with_a_rock_affordance_extraction_via_word_embeddings"
        },
        {
            "paper_title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework",
            "rating": 1,
            "sanitized_title": "learning_to_win_by_reading_manuals_in_a_montecarlo_framework"
        }
    ],
    "cost": 0.012589999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Text-based Adventures of the Golovin AI Agent</p>
<p>Bartosz Kostka 
Institute of Computer Science
University of Wrocław
Poland</p>
<p>Jarosław Kwiecień jaroslaw.kwiecien@stud.cs.uni.wroc.pl 
Institute of Computer Science
University of Wrocław
Poland</p>
<p>Jakub Kowalski 
Institute of Computer Science
University of Wrocław
Poland</p>
<p>Paweł Rychlikowski 
Institute of Computer Science
University of Wrocław
Poland</p>
<p>Text-based Adventures of the Golovin AI Agent</p>
<p>The domain of text-based adventure games has been recently established as a new challenge of creating the agent that is both able to understand natural language, and acts intelligently in text-described environments.In this paper, we present our approach to tackle the problem. Our agent, named Golovin, takes advantage of the limited game domain. We use genre-related corpora (including fantasy books and decompiled games) to create language models suitable to this domain. Moreover, we embed mechanisms that allow us to specify, and separately handle, important tasks as fighting opponents, managing inventory, and navigating on the game map.We validated usefulness of these mechanisms, measuring agent's performance on the set of 50 interactive fiction games. Finally, we show that our agent plays on a level comparable to the winner of the last year Text-Based Adventure AI Competition.</p>
<p>I. INTRODUCTION</p>
<p>The standard approach to develop an agent playing a given game is to analyze the game rules, choose an appropriate AI technique, and incrementally increase the agent's performance by exploiting these rules, utilizing domain-dependent features and fixing unwanted behaviors. This strategy allowed to beat the single games which were set as the milestones for the AI development: Chess [1] and Go [2].</p>
<p>An alternative approach called General Game Playing (GGP), operating on a higher level of abstraction has recently gained in popularity. Its goal is to develop an agent that can play any previously unseen game without human intervention. Equivalently, we can say that the game is one of the agent's inputs [3].</p>
<p>Currently, there are two main, well-established GGP domains providing their own game specification languages and competitions [4]. The first one is the Stanford's GGP, emerged in 2005 and it is based on the Game Description Language (GDL), which can describe all finite, turn-based, deterministic games with full information [5], and its extensions (GDL-II [6] and rtGDL [7]).</p>
<p>The second one is the General Video Game AI framework (GVGAI) from 2014, which focuses on arcade video games [8]. In contrast to Stanford's GGP agents are provided with the forward game model instead of the game rules. The domain is more restrictive but the associated competition provides multiple tracks, including procedural content generation challenges [9], [10].</p>
<p>What the above-mentioned approaches have in common is usually a well-defined game state the agent is dealing with. It contains the available information about the state (which may be partially-observable), legal moves, and some kind of scoring function (at least for the endgame states). Even in a GGP case, the set of available moves is known to the agent, and the state is provided using some higher-level structure (logic predicates or state observations).</p>
<p>In contrast, the recently proposed Text-Based Adventure AI Competition, held during the IEEE Conference on Computational Intelligence and Games (CIG) in 2016, provides a new kind of challenge by putting more emphasis on interaction with the game environment. The agent has access only to the natural language description about his surroundings and effects of his actions.</p>
<p>Thus, to play successfully, it has to analyze the given descriptions and extract high-level features of the game state by itself. Moreover, the set of possible actions, which are also expected to be in the natural language, is not available and an agent has to deduct it from his knowledge about the game's world and the current state.</p>
<p>In some sense, this approach is coherent with the experiments on learning Atari 2600 games using the Arcade Learning Environment (ALE), where the agent's inputs were only raw screen capture and a score counter [11], [12]. Although in that scenario the set of possible commands is known in advance.</p>
<p>The bar for text-based adventure games challenge is set high -agents should be able to play any interactive fiction (IF) game, developed by humans for the humans. Such environment, at least in theory, requires to actually understand the text in order to act, so completing this task in its full spectrum, means building a strong AI.</p>
<p>Although some approaches tackling similar problems exist since early 2000s ( [13], [14]), we are still at the entry point for this kind of problems, which are closely related to the general problem solving. However, recent successes of the machine learning techniques combined with the power of modern computers, give hope that some vital progress in the domain can be achieved.</p>
<p>We pick up the gauntlet, and in this work we present our autonomous agent that can successfully play many interactive fiction games. We took advantage of the specific game domain, and trained agent using matching sources: fantasy books and texts from decompiled IF games. Moreover, we embed some rpg-game-based mechanisms, that allow us to improve fighting opponents, managing hero's inventory, and navigating in the maze of games' locations.</p>
<p>We evaluated our agent on a set of 50 games, testing the influence of each specific component on the final score. Also, we tested our agent against the winner of the last year competition [15]. The achieved results are comparable. Our agent scored better in 12 games and worse in 11 games.</p>
<p>The paper is organized as follows. Section II provides background for the domain of interactive fiction, Natural Language Processing (NLP), Text-Based Adventure AI Competition, and the related work. In Section III, we presented detailed description of our agent. Section IV contains the results of the performed experiments. Finally, in Section V, we conclude and give perspective of the future research.</p>
<p>II. BACKGROUND</p>
<p>A. Interactive Fiction</p>
<p>Interactive Fiction (IF), emerged in 1970s, is a domain of text-based adventure or role playing games, where the player uses text commands to control characters and influence the environment. One of the most famous example is the Zork series developed by the Infocom company. From the formal point of view, they are single player, non-deterministic games with imperfect information. IF genre is closely related to MUDs (Multi-User Dungeons), but (being single-player) more focused on plot and puzzles than fighting and interacting with other players. IF was popular up to late 1980s, where the graphical interfaces become available and, as much user friendlier, more popular. Nevertheless, new IF games are still created, and there are annual competitions for game authors (such as The Interactive Fiction Competition).</p>
<p>IF games usually (but not always) take place in some fantasy worlds. The space the character is traversing has a form of labyrinth consisting of so called rooms (which despite the name can be open areas like forest). Entering the room, the game displays its description, and the player can interact with the objects and game characters it contains, or try to leave the room moving to some direction. However, reversing a movement direction (e.g. go south ↔ go north) not necessarily returns the character to the previous room.</p>
<p>As a standard, the player character can collect objects from the world, store them in his equipment, and combine with other objects to achieve some effects on the environment (e.g. put the lamp and sword in the case). Thus, many games require solving some kind of logical puzzle to push the action forward. After performing an action, the game describes its effect. Many available actions are viable, i.e. game engine understands them, but they are not required to solve the game, or even serve only for the player amusement.</p>
<p>Some of the games provide score to evaluate the player's progress, however the change in the score is often the result of a complex series of moves rather than quick "frame to frame" decisions, or the score is given only after the game's end. Other games do not contain any scoring function and the only output is win or lose.</p>
<p>B. Playing Text-Based Games</p>
<p>Although the challenge of playing text-based games was not take on often, there are several attempts described in the literature, mostly based on the MUD games rather than the classic IF games.</p>
<p>Adventure games has been carefully revised as the field of study for the cognitive robotics in [14]. First, the authors identify the features of "tradition adventure game environment" to point-out the specifics of the domain. Second, they enumerate and discuss existing challenges, including e.g. the need for commonsense knowledge (its learning, revising, organization, and using), gradually revealing state space and action space, vague goal specification and reward specification.</p>
<p>In [13], the agent able to live and survive in an existing MUD game have been described. The authors used layered architecture: high level planning system consisting of reasoning engine based on hand-crafted logic trees, and a low level system responsible for sensing the environment, executing commands to fulfill the global plan, detecting and reacting in emergency situations.</p>
<p>While not directly-related to playing algorithms, it is worth to note the usage of computational linguistics and theorem proving to build an engine for playing text-based adventure games [16]. Some of the challenges are similar for both tasks, as generating engine requires e.g. object identification (given user input and a state description) and understanding dependencies between the objects.</p>
<p>The approach focused on tracking the state of the world in text-based games, and translating it into the first-order logic, has been presented in [17]. Proposed solution was able to efficiently update agent's belief state from a sequence of actions and observations.</p>
<p>The extension of the above approach, presents the agent that can solve puzzle-like tasks in partially observable domain that is not known in advance, assuming actions are deterministic and without conditional effects [18]. It generates solutions by interleaving planning (based on the traditional logic-based approach) and execution phases. The correctness of the algorithm is formally proved.</p>
<p>Recently, an advanced MUD playing agent has been described in [19]. Its architecture consists of two modules. First, responsible for converting textual descriptions to state representation is based on the Long Short-term Memory (LSTM) networks [20]. Second, uses Deep Q-Networks [11] to learn approximated evaluations for each action in a given state. Provided results show that the agent is able to to successfully complete quests in small, and even medium size, games.</p>
<p>C. Natural Language Processing</p>
<p>Natural Language Processing is present in the history of computers almost from the very beginning. Alan Turing in his famous paper [21] state (approximately) that "exhibit intelligent behavior" means "understand natural language and use it properly in conversations with human being". So, since 1950 Turing test is the way of checking whether computer has reached strong AI capability.</p>
<p>First natural language processing systems were rule based. Thanks to the growing amount of text data and increase of the computer power, during last decades one can observe the shift towards the data driven approaches (statistical or machine learning). Nowadays, NLP very often is done "almost from scratch", as it was done if [22] where the authors have used neural network in order to solve many NLP tasks, including part-of-speech tagging, named entity recognition and semantic role labeling. The base for this was the neural language model. Moreover, this systems produced (as a side effect) for every word in a vocabulary a dense vector which reflected word properties. This vectors are called word embeddings and can be obtained in many ways. One of the most popular is the one proposed in [23] that uses very simple, linear language model and is suitable to large collections of texts.</p>
<p>Language models allow to compute probability of the sentence treated as a sequence of items (characters, morphemes or words). This task was traditionally done using Markov models (with some smoothing procedures, see [24]). Since predicting current words is often dependent on the long part of history, Markov models (which, by definition, looks only small numbers of words behind) are outperformed by the modern methods that can model long distance dependencies. This methods use recursive (deep) neural networks, often augmented with some memory.</p>
<p>We will use both word embeddings (to model words similarity) and LSTM neural networks [25] with attention mechanism (see [26] and [27]). We are particularly interested in the information given from the attention mechanism, which allows us to estimate how important is each word, when we try to predict the next word in the text.</p>
<p>D. The Text-Based Adventure AI Competition</p>
<p>The first Text-Based Adventure AI Competition 1 , organized by the group from the University of York, has been announced in May 2016 and took place at the 2016 IEEE CIG conference in September. The second, will be held this year, also colocated with CIG.</p>
<p>The purpose of the competition is to stimulate research towards the transcendent goal of creating General Problem Solver, the task stated nearly six decades ago [28]. The organizers plan to gradually increase the level of given challenges, with time providing more complex games that require more sophisticated approaches from the competitors. Thus, finally force them to develop agents that can autonomously acquire knowledge required to solve given tasks from the restricted domain of text-based games.</p>
<p>The domain of the competition is specified as any game that can be described by the Z-machine, the classic text adventuring engine. Interactive Fiction games are distributed as compiled byte code, requiring the special interpreter to run them. The first Z-machine has been developed in 1979 by Infocom, and supports games developed using a LISP-like programming language named Infocom's ZIL (Zork Implementation 1 http://atkrye.github.io/IEEE-CIG-Text-Adventurer-Competition/. Language). The Text-based AI Competition uses Frotz 2 , the modern version of the Z-machine, compatible with the original interpreter.</p>
<p>The competition organizers provide a Java package managing the communication between a game file and an agent process. Also, example random agents in Java and Python 3 are available. The interpreter is extended by the three additional commends, allowing players to quit the game, restart it with a new instance of the agent, and restart without modifying the agent. Given that, the text-based AI framework supports learning and simulation-based approaches.</p>
<p>Little details about the competition insides are available. In particular, the number of participants is unknown, and the test environment game used to evaluate agents remained hidden, as it is likely to be used again this year. The game has been developed especially for the purpose of the competition and supports graduated scale of scoring points, depending on the quality of agent's solution.</p>
<p>The winner of the first edition was the BYU-Agent 3 from the Perception Control and Cognition lab at Brigham Young University, which achieved a score 18 out of 100. The idea behind the agent has been described in [15]. It uses Q-learning [29] to estimate the utility of an action in a given game state, identified as the hash of its textual description.</p>
<p>The main contribution concerns affordance detection, used to generating reasonable set of actions. Based on the word2vec [30], an algorithm mapping words into a vector representations based on their contextual similarities, and the Wikipedia as the word corpus, the verb-noun affordances are generated. Thus, the algorithm is able to detect, for an in-game object, words with a similar meaning, and provide a set of actions that are possible to undertake with that object.</p>
<p>Provided results, based on the IF games compatible with Zmachine interpreter, shows the overall ability of the algorithm to successfully play text-based games. Usually, the learning process results in increasing score, and requires playing a game at least 200 times to reach the peek. However, there are some games that achieve that point much slower, or even the score drops as the learning continues.</p>
<p>III. THE GAME PLAYING AGENT</p>
<p>A. Overview</p>
<p>Our agent is characterized by the following features.:</p>
<p>• it uses a huge set of predefined command patterns, obtained by analyzing various domain-related sources; the actual commands are obtained by suitable replacements; • it uses language models based on selection of fantasy books; • it takes advantage of the game-specific behaviors, natural for adventure games, like fight mode, equipment management, movement strategy;</p>
<p>• it memorizes and uses some aspects of the current play history; • it tries to imitate human behavior: after playing several games and exploring the game universe it repeats the most promising sequence of commands. We treat the result reached in this final trial as the agent's result in this game. The agent was named "Golovin", as one of the first answers it gives after asking Hey bot, what is your name?, was your name is Golovin, a phrase from the game Doomlords.</p>
<p>B. Preprocessing 1) Language Models: We used language models for two purposes. First, they allow us to define words similarity (which in turns gives us opportunity to replace some words in commands with their synonyms). For this task we use word2vec [30] (and its implementation in TensorFlow [31]). Secondly, we use neural network language models to determine which words in the scene description plays more important role than other (and so are better candidates to be a part of the next command). We use the LSTM neural networks operating on words [25], augmented by the attention mechanism ( [26] and [27]). This combination was previously tested in [32].</p>
<p>Since the action of many games is situated in fantasy universe, we decided to train our models on the collection of 3000 fantasy books from bookrix.com (instead of using Wikipedia, as in [15]).</p>
<p>2) Commands: In order to secure out agent against overfitting, we fix the set of games used in tests (the same 50 games as in [15]). No data somehow related to this games were used in any stage of preprocessing.</p>
<p>We considered three methods to gather commands:</p>
<p>• walkthroughs -for several games, sequence of commands from winning path can be found in the Internet. This source provides raw valid commands, that are useful in some games. • tutorials -on the other hand, some games have tutorials written in natural language. Analyzing such tutorials 4 seemed to be a promising way of getting valid command. Concept of reading manuals has been successfully used to learn how to play Civilization II game [33]. • games -at the end, there are many games that don't have tutorials nor walkthroughs. We downloaded a big collection of games, decompiled their codes, and extracted all texts from them. The last two sources required slightly more complicated preprocessing. After splitting texts into sentences, we parsed them using PCFG parser from NLTK package [34]. Since commands are (in general) verb phrases, we found all minimal VP phrases from parse trees. After reviewing some of them, we decided not to take every found phrase, but manually create the list of conditions which characterizes 'verb phrases useful in games'. In this way we obtained the collections of approximately 250,000 commands (or, to be more precisely, command patterns). We also remember the count of every command (i.e. the number of parse tree it occurs in).</p>
<p>Some of the commands have special tag: "useful in the battle". We have manually chosen five verbs, as the most commonly related to fighting: attack, kill, fight, shoot, and punch. Approximately 70 most frequent commands containing one of these verbs received this tag.</p>
<p>The commands used by our agent were created from these patterns by replacing (some) nouns by nouns taken from the game texts.</p>
<p>C. Playing Algorithm</p>
<p>The algorithm uses 5 types of command generators: battle mode, gathering items, inventory commands, general actions (interacting with environment), and movement. The generators are fired in the given order, until a non-empty set of commands is proposed.</p>
<p>There are multiple reasons why some generator may not produce any results, e.g. the agent reaches the predefined limit of making actions of that type, all the candidates are blacklisted, or simply we cannot find any appropriate command in the database. We will describe other special cases later.</p>
<p>When the description of the area changes, all the command lists are regenerated.</p>
<p>1) Generating Commands: Our general method to compute available commands and choose the one which is carried out, looks as follows:</p>
<p>1) Find all nouns in the state description and agent's equipment. (We accept all type of nouns classified by the nltk.pos_tag function.) 2) Determine their synonyms, based on the cosine similarity between word vectors. (We use n-best approach with n being subject to Spearmint optimization; see IV-A.) 3) Find the commands containing nouns from the above described set. If a command contains a synonym, it is replaced by the word originally found in the description. 4) Score each command taking into account:</p>
<p>• cosine similarity between used synonyms and the original words • uniqueness of the words in command, computed as the inverse of number of occurrences in the corpora. • the weight given by the neural network model • the number of words occurring both in the description and in the command The score is computed as the popularity of the command (number of occurrences in the command database) multiplied by the product of the above. The formula uses some additional constants influencing the weights of the components. 5) Then, using the score as the command's weight, randomly pick one command using the roulette wheel selection. 2) Battle Mode: The battle mode has been introduced to improve the agent's ability to survive. It prevents from what has been the main cause of agent's death before -careless walking into an enemy or spending too much time searching for the proper, battle-oriented and life-saving, action.</p>
<p>The agent starts working in battle mode after choosing one of the "fight commands". Being in this mode, the agent strongly prefers using battle command, moreover it repeats the same command several times (even if it fails), because in many games the opponent has to be attacked multiple times to be defeated. Therefore, between the consecutive fighting actions we prevent using standard commands (like look, examine), as wasting precious turns usually gives the opponent an advantage.</p>
<p>3) Inventory Management (gathering and using items): In every new area, the algorithm searches its description for interesting objects. It creates a list of nouns ordered by the weight given by the neural network model and their rarity. Then, the agent tries take them.</p>
<p>If it succeeds (the content of the inventory has changed), a new list of commands using only the newly acquired item is generated (using the general method). The constant number of highest scored commands is immediately executed.</p>
<p>4) Exploration:</p>
<p>The task of building an IF game map is difficult for two reasons. One, because a single area can be presented using multiple descriptions and they may change as the game proceeds. Two, because there may be different areas described by the same text. Our map building algorithm tries to handle these problems.</p>
<p>We have found that usually the first sentence of the area description remains unchanged, so we use it to label the nodes of the graph (we have tried other heuristics as well but they performed worse). This heuristic divides all visited nodes into the classes suggesting that corresponding areas may be equivalent. The edges of the graph are labeled by the move commands used to translocate between the nodes (we assume that movement is deterministic).</p>
<p>We initialize the map graph using the paths corresponding to the past movements of the agent. Then, the algorithm takes all pairs of nodes with the same label and considers them in a specific, heuristic-based, order. For every pair, the MergeNodes procedure (Listing 1) is fired. The procedure merges two states joining their outcoming edges and recursively proceeds to the pairs of states that are reachable using the same move command. If the procedure succeeds, we replaces current map graph with the minimized one, otherwise the changes are withdrawn.</p>
<p>We use a small fixed set of movement commands (south, northwest, up, left, etc.) to reveal new areas and improve the knowledge about the game layout. When the agent decides to leave the area, it tries a random direction, unless it already discovered all outgoing edges -then it uses map to find a promising destination. We evaluate destination nodes minimizing the distance to that node plus the number of tested commands divided by the node's curiosity (depending on scores of available commands and untested movement commands). Then, the agent follows the shortest path to the best scored destination. if ¬ MergeNodes(A , B ) then return False end if 12: end for 13: return True 5) Failing Commands: When, after executing a command, the game state (description) remains unchanged, we assume the command failed. Thus, the algorithm puts it on a blacklist assigned to the current location. The command on the location's black list is skipped by the command generators.</p>
<p>After any change in the agent's inventory, all blacklists are cleared.</p>
<p>6) Restarts: The Frotz environment used for the contest allows to restart the game, i.e. abandon current play and start again from the beginning.</p>
<p>Me make use of this possibility in a commonsense imitating of the human behavior. When the agent dies, it restarts the game and, to minimize the chance of the further deaths, it avoids repeating the last commands of his previous lives. The agent also remembers the sequence of moves that lead to the best score and eventually repeats it. The final trial's result is used as the agent's result in the game.</p>
<p>IV. EXPERIMENTS</p>
<p>Our experiments had two main objectives: creating the most effective agent, and analyze how some parameters influence the agents performance.</p>
<p>The most natural way to measure the agent performance is to use the score given by the game (divided by the maximum score, when we want to compare different games). However, there are many games in which our agent (as well as BYU-Agent) has problems with receiving non zero points. So, we have decided to reward any positive score and add to the positive agent result arbitrarily chosen constant 0.2. Therefore, optimal (hypothetical) agent would get 1.2 in every game.</p>
<p>We selected 20 games for the training purposes, for all of them the maximum score is known. The performance of the agent is an averaged (modified) score computed on these games.</p>
<p>A. Creating The Best Agent</p>
<p>The agent's play is determined by some parameters, for instance:</p>
<p>• the set of command patterns, • the number of synonyms taken from word2vec, • the number of items, we are trying to gather, after visiting new place, • the number of standard command, tried after gathering phase, • how to reward the commands containing many words from description (the actual reward is b k , where k is the number of common words, and b is a positive constant), • how to punish the commands containing words with no good reason to be used (neither in state description nor in generated synonyms), the score is divided by p k , where k is the number of such words, and p is a constant, • how many command should be done before trying movement command. Furthermore we wanted to check, whether using battle mode or a map has an observable effect on agent performance. The number of parameter combinations was too large for grid search, so we decided to use Spearmint 5 .</p>
<p>We started this optimization process with (total) score equal to 0.02, and after some hours of computation we end with 0.08 (which means that the score has been multiplied 4 times). From now all parameters (if not stated otherwise) will be taken from the final Spearmint result.</p>
<p>B. Evaluation of Domain-based Mechanisms</p>
<p>We wanted to check whether some more advanced features of our agent give observable influence on agent performance. We checked the following 4 configurations with battle-mode and map turned on or off. The results are presented in Figure  1. One can see that map is useful (but only to some extent), and battle mode is undoubtedly useful. </p>
<p>C. Evaluation of Language Model Sources</p>
<p>Commands were taken from 3 sources: tutorials (T), walkthroughs (W), and state description from games (G). We compared the agents used command from all combination of these sources. The results are presented in Figure 2. The optimal configuration uses only two sources: T and W 6 . We, however, still believe that decompiled games can be a useful source for game commands. But they cannot be found in descriptions, but in command interpreter -which requires more advanced automated code analysis. We left it as a future work. </p>
<p>D. Gameplay Examples</p>
<p>While playing detective, our agent finds himself in a closet. We get the following state description:</p>
<p>Game: You are in a closet. There is a gun on the floor. Better get it. To exit, go east. Our agent determines items: closet, gun, floor, exit. Our agent is choosing from the commands listed in Table I. We see that what is the most important for the agent is gun and how to take it, which is reasonable and even suggested by the game. Moreover, the agent also tries to search using synonyms of the word gun in order to find proper commands (for instance: we can see that he recognizes gun as some kind of weapon, and some weapons, as for example knife, can be sharpened). Fortunately, after using a command get gun, Golovin obtains a black little pistol. Another example comes from zork3. We start with the following location description:</p>
<p>Game: You are in a shadowy land of low, rolling hills stretching out to the west and south. The land is bordered to the north by a massive stone wall. Ancient and weathered, the wall has crumbled enough at one point to permit passage. A cloaked and hooded person, carrying a sword not unlike your own, is standing blocking the way to the northeast. The hooded figure has a great deal of strength, perhaps matching your own. Golovin: examine figure Game: There is nothing special to be seen. Your quick reflexes cannot stop the hooded figure's stroke! You are hit! In this very moment, in the agent's inventory are: a sword, a lamp. In the command list we can find for instance kill man with sword. As the word man is close enough to figure, and Golovin also have sword, he chooses to use command kill figure with sword. Because the fighting command was chosen, the battle mode is turned on, and the game continues as follows:</p>
<p>Golovin: kill figure with sword Game: The hooded figure is hit with a quick slash! The figure has a light wound which hasn't affected its seemingly great strength. the agent's score improves and the fight continues. . .</p>
<p>Golovin: kill figure with sword Game: You charge, but the hooded figure jumps nimbly aside. The hooded figure stabs nonchalantly with its sword and misses.</p>
<p>E. The Comparison with BYU-Agent</p>
<p>Finally, we validate our approach by comparing it with the BYU-Agent. We were using the same set of 50 Z-machine games 7 as in [15].</p>
<p>The results of the comparison are presented in Table II. The BYU agent was trained for 1000 epochs (each epoch containing 1000 game steps), and its score was noted after each epoch. Because the learning curves vary depending on the game, including degeneration of the results (see [15, Figure 5]), as the main measure we took the maximum score achieved over all epochs.</p>
<p>As for the Golovin, we restricted his playing time to 1000 steps (i.e. an equivalent of one epoch) and use our commonsense restarting mechanism.</p>
<p>The BYU-Agent results are obtained using the verb and action space reduction algorithm, except the games marked with an asterisk, where the verb space reduction experienced errors, so we present scores obtained by the action space reduction variant instead.</p>
<p>Eventually, there are 24 games, out of 50, where some of the agents received any positive reward. Golovin scored better in 12 games, including 7 games where BYU-Agent received no reward. BYU-Agent scored better in 11 games, including 6 games where Golovin scored no points. One game is a nonzero tie.</p>
<p>Thus, despite significantly shorter learning time (i.e. available number of steps), our agent is able to outperform BYU-Agent on a larger number of games than he is outperformed on. On the other hand, BYU-Agent gains in the games where the Q-learning is effective and gradually increases score through the epochs, e.g. curses, detective or Parc.</p>
<p>Last observation concerns the number of games where only one of the agents scored 0, which is surprisingly large. This may suggest that the two compared approaches are effective on a different types of games, and may, in some sense, complement each other. </p>
<p>V. CONCLUSIONS AND FUTURE WORK</p>
<p>We have presented an agent able to play any interactive fiction game created for human players, on the level comparable to the winner of the last year Text-Based Adventure AI Competition. Due to the number of domain-based mechanisms, our agent can successfully handle the game in a limited number of available steps. The results of the presented experiments show that the mechanisms we embed (battle mode, mapping) and a choice of learning sources, indeed improves the agent's performance.</p>
<p>Although the results are promising, we are still at the beginning of the path towards creating the agent that can really understand the natural language descriptions in order to efficiently play the text-based adventure games.</p>
<p>There are multiple future work directions we would like to point out. First, and one of the most important, is to embed a learning mechanisms: the in-game learning, that uses restart functionality to improve player efficiency in one particular game; and preliminary learning, that is able to gain useful knowledge from playing entire set of games. Also, we plan to take a closer look at the decompiled game codes, as we believe that analyzing them may provide very useful knowledge.</p>
<p>We would like to improve the battle mode behavior, especially mitigate the agent and make it more sensitive to the particular situation. We hope that the mapping mechanism can be further extended to allow the casual approach, where the agent travels to distant locations for some specific reason (e.g. item usage), instead of a simple reactive system that we have now.</p>
<p>Lastly, we would like to continue the domain-based approach, and so focus our efforts on discovering the subgames (like we did with fighting and exploring) that we are able to properly detect, and handle significantly better than the general case.</p>
<p>if A = B then return True end if 2: if label(A) =label(B) then return False end if 3: mergelist ← {} 4: for all m ∈ M for all (A , B ) ∈ mergelist do 11:</p>
<p>Fig. 1 .
1Comparison of agent version with and without map and battle mode. Best variant scaled to 100%.</p>
<p>Fig. 2 .
2Comparison of agent using different sources of commands. Best variant scaled to 100%.</p>
<p>TABLE I
IBEST 10 (OUT OF 25) COMMANDS PROPOSED BY OUR AGENT FOR THE SITUATION DESCRIBED IN THE D E T E C T I V E EXAMPLE (SECTION IV-D)action 
score 
get gun 
0.1736 
drop gun 
0.1129 
take gun 
0.0887 
open closet 
0.0557 
examine gun 
0.0309 
fire gun 
0.0252 
load gun 
0.0237 
examine closet 
0.0128 
buy gun 
0.0042 
sharp gun 
0.0006 </p>
<p>TABLE II AVERAGE
IISCORES FOR 10 RUNS OF EACH GAME. FOR BYU-AGENT WE TOOK THE MAXIMUM ACHIEVED SCORE DURING THE 1000 EPOCHS TRAINING. GOLOVIN PLAYS FOR ONE EPOCH. IN THE GAMES THAT ARE NOT LISTED BOTH AGENTS GAIN NO REWARD. THE ASTERISK MARKS GAMES THAT USES OTHER VERSION OF BYU-AGENTgame 
Golovin BYU-Agent 
max score 
balances 
9.0 
0 
51 
break-in 
0 
0.3 
150 
bunny 
2.7 
2.0 
60 
candy 
10.0 
10.0 
41 
cavetrip 
15.0 
10.5 
500 
curses 
0.4 
1.9 
550 
deephome 
1.0 
0 
300 
detective 
71.0 
213.0 
360 
gold 
0.3 
0 
100 
library 
5.0 
0 
30 
mansion 
0.1 
2.2 
68 
Murdac 
10.0 
0 
250 
night 
0.8 
0 
10 
omniquest 
7,5 
5.0 
50 
parallel 
0 
5.0 
150 
Parc 
1.6 
5.0 
50 
reverb 
0 
1.8 
50 
spirit 
3.2 
2.0 
250 
tryst205 
0.2 
2.0 
350 
zenon 
0 
2.8 
20 
zork1 
13.5 
<em>8.8 
350 
zork2 
-0.1 
</em>3.3 
400 
zork3 
0.7 
*0 
7 
ztuu 
0 
0.5 
100 
better in: 
12 
11 
games </p>
<p>http://frotz.sourceforge.net.3 The agent is open source and available at https://github.com/danielricks/ BYU-Agent-2016.
This tutorials were downloaded from the following sites: http://www. ifarchive.org/, https://solutionarchive.com/, http://www.gameboomers.com/, http://www.plover.net/~davidw/sol/
Spearmint is a package which performs Bayesian optimization of hyperparameters. It allows to treat the optimized function as a black-box, and tries to choose the parameters for the next run considering the knowledge gathered during previous runs. See[35].
The difference between T+W and G+T+W is not very big. In the previous version of this experiment the winner was G+T+W.
The game set is available at https://github.com/danielricks/textplayer/tree/ master/games.
ACKNOWLEDGMENTSThe authors would like to thank Szymon Malik for his valuable contribution in the early stage of developing Golovin.We would also like to thank Nancy Fulda for helpful answers to our questions and providing up-to date results of the BYU-Agent.
Deep Blue. M Campbell, A J Hoane, F Hsu, Artificial intelligence. 1341M. Campbell, A. J. Hoane, and F. Hsu, "Deep Blue," Artificial intelli- gence, vol. 134, no. 1, pp. 57-83, 2002.</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, Nature. 529D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanc- tot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis, "Mastering the game of Go with deep neural networks and tree search," Nature, vol. 529, pp. 484-503, 2016.</p>
<p>M Genesereth, M Thielscher, General Game Playing. Morgan &amp; ClaypoolM. Genesereth and M. Thielscher, General Game Playing. Morgan &amp; Claypool, 2014.</p>
<p>General Game Playing: Overview of the AAAI Competition. M Genesereth, N Love, B Pell, AI Magazine. 26M. Genesereth, N. Love, and B. Pell, "General Game Playing: Overview of the AAAI Competition," AI Magazine, vol. 26, pp. 62-72, 2005.</p>
<p>General Game Playing: Game Description Language Specification. N Love, T Hinrichs, D Haley, E Schkufza, M Genesereth, Stanford Logic Group, Tech. Rep.N. Love, T. Hinrichs, D. Haley, E. Schkufza, and M. Genesereth, "General Game Playing: Game Description Language Specification," Stanford Logic Group, Tech. Rep., 2006.</p>
<p>A General Game Description Language for Incomplete Information Games. M Thielscher, AAAI Conference on Artificial Intelligence. M. Thielscher, "A General Game Description Language for Incomplete Information Games," in AAAI Conference on Artificial Intelligence, 2010, pp. 994-999.</p>
<p>Towards a Real-time Game Description Language. J Kowalski, A Kisielewicz, International Conference on Agents and Artificial Intelligence. 2J. Kowalski and A. Kisielewicz, "Towards a Real-time Game Descrip- tion Language," in International Conference on Agents and Artificial Intelligence, vol. 2, 2016, pp. 494-499.</p>
<p>The 2014 General Video Game Playing Competition. D Perez, S Samothrakis, J Togelius, T Schaul, S Lucas, A Couëtoux, J Lee, C Lim, T Thompson, IEEE Transactions on Computational Intelligence and AI in Games. 83D. Perez, S. Samothrakis, J. Togelius, T. Schaul, S. Lucas, A. Couëtoux, J. Lee, C. Lim, and T. Thompson, "The 2014 General Video Game Playing Competition," IEEE Transactions on Computational Intelligence and AI in Games, vol. 8, no. 3, pp. 229-243, 2015.</p>
<p>General Video Game AI: Competition, Challenges and Opportunities. D Perez, S Samothrakis, J Togelius, T Schaul, S M Lucas, AAAI Conference on Artificial Intelligence. D. Perez, S. Samothrakis, J. Togelius, T. Schaul, and S. M. Lucas, "General Video Game AI: Competition, Challenges and Opportunities," in AAAI Conference on Artificial Intelligence, 2016, pp. 4335-4337.</p>
<p>General Video Game Level Generation. A Khalifa, D Perez, S Lucas, J Togelius, Genetic and Evolutionary Computation Conference. A. Khalifa, D. Perez, S. Lucas, and J. Togelius, "General Video Game Level Generation," in Genetic and Evolutionary Computation Conference, 2016, pp. 253-259.</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., "Human-level control through deep reinforcement learning," Nature, vol. 518, no. 7540, pp. 529-533, 2015.</p>
<p>Emergent Tangled Graph Representations for Atari Game Playing Agents. S Kelly, M I Heywood, EuroGP 2017: Genetic Programming, ser. 10196S. Kelly and M. I. Heywood, "Emergent Tangled Graph Representations for Atari Game Playing Agents," in EuroGP 2017: Genetic Program- ming, ser. LNCS, 2017, vol. 10196, pp. 64-79.</p>
<p>being-in-the-world. M A Depristo, R Zubek, Proceedings of the 2001 AAAI Spring Symposium on Artificial Intelligence and Interactive Entertainment. the 2001 AAAI Spring Symposium on Artificial Intelligence and Interactive EntertainmentM. A. DePristo and R. Zubek, "being-in-the-world," in Proceedings of the 2001 AAAI Spring Symposium on Artificial Intelligence and Interactive Entertainment, 2001, pp. 31-34.</p>
<p>Adventure games: A challenge for cognitive robotics. E Amir, P Doyle, Proc. Int. Cognitive Robotics Workshop. Int. Cognitive Robotics WorkshopE. Amir and P. Doyle, "Adventure games: A challenge for cognitive robotics," in Proc. Int. Cognitive Robotics Workshop, 2002, pp. 148- 155.</p>
<p>What can you do with a rock? Affordance extraction via word embeddings. N Fulda, D Ricks, B Murdoch, D Wingate, International Joint Conference on Artificial Intelligence. to appearN. Fulda, D. Ricks, B. Murdoch, and D. Wingate, "What can you do with a rock? Affordance extraction via word embeddings," in International Joint Conference on Artificial Intelligence, 2017, (to appear).</p>
<p>Put my galakmid coin into the dispenser and kick it: Computational linguistics and theorem proving in a computer game. A Koller, R Debusmann, M Gabsdil, K Striegnitz, Journal of Logic, Language and Information. 132A. Koller, R. Debusmann, M. Gabsdil, and K. Striegnitz, "Put my galakmid coin into the dispenser and kick it: Computational linguistics and theorem proving in a computer game," Journal of Logic, Language and Information, vol. 13, no. 2, pp. 187-206, 2004.</p>
<p>Knowledge-gathering agents in adventure games. B Hlubocky, E Amir, AAAI-04 workshop on Challenges in Game AI. B. Hlubocky and E. Amir, "Knowledge-gathering agents in adventure games," in AAAI-04 workshop on Challenges in Game AI, 2004.</p>
<p>Goal Achievement in Partially Known, Partially Observable Domains. A Chang, E Amir, Proceedings of the Sixteenth International Conference on International Conference on Automated Planning and Scheduling. the Sixteenth International Conference on International Conference on Automated Planning and SchedulingA. Chang and E. Amir, "Goal Achievement in Partially Known, Partially Observable Domains," in Proceedings of the Sixteenth International Conference on International Conference on Automated Planning and Scheduling, 2006, pp. 203-211.</p>
<p>Language Understanding for Text-based Games using Deep Reinforcement Learning. K Narasimhan, T Kulkarni, R Barzilay, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingK. Narasimhan, T. Kulkarni, and R. Barzilay, "Language Understanding for Text-based Games using Deep Reinforcement Learning," in Proceed- ings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1-11.</p>
<p>Long Short-Term Memory. S Hochreiter, J Schmidhuber, Neural Comput. 98S. Hochreiter and J. Schmidhuber, "Long Short-Term Memory," Neural Comput., vol. 9, no. 8, pp. 1735-1780, 1997.</p>
<p>Computing machinery and intelligence. A M Turing, 59MindA. M. Turing, "Computing machinery and intelligence," Mind, vol. 59, no. 236, pp. 433-460, 1950. [Online]. Available: http: //www.jstor.org/stable/2251299</p>
<p>Natural language processing (almost) from scratch. R Collobert, J Weston, L Bottou, M Karlen, K Kavukcuoglu, P P Kuksa, abs/1103.0398CoRR. R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P. Kuksa, "Natural language processing (almost) from scratch," CoRR, vol. abs/1103.0398, 2011. [Online]. Available: http://arxiv.org/abs/1103.0398</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, abs/1301.3781CoRR. T. Mikolov, K. Chen, G. Corrado, and J. Dean, "Efficient estimation of word representations in vector space," CoRR, vol. abs/1301.3781, 2013. [Online]. Available: http://arxiv.org/abs/1301.3781</p>
<p>An empirical study of smoothing techniques for language modeling. S F Chen, J Goodman, Proceedings of the 34th. the 34thS. F. Chen and J. Goodman, "An empirical study of smoothing techniques for language modeling," in Proceedings of the 34th</p>
<p>10.3115/981863.981904Annual Meeting on Association for Computational Linguistics, ser. ACL '96. Stroudsburg, PA, USAAssociation for Computational LinguisticsAnnual Meeting on Association for Computational Linguistics, ser. ACL '96. Stroudsburg, PA, USA: Association for Computational Linguistics, 1996, pp. 310-318. [Online]. Available: http://dx.doi.org/ 10.3115/981863.981904</p>
<p>A neural probabilistic language model. Y Bengio, R Ducharme, P Vincent, C Jauvin, JOURNAL OF MACHINE LEARNING RE-SEARCH. 3Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, "A neural prob- abilistic language model," JOURNAL OF MACHINE LEARNING RE- SEARCH, vol. 3, pp. 1137-1155, 2003.</p>
<p>Feed-forward networks with attention can solve some long-term memory problems. C Raffel, D P W Ellis, abs/1512.08756CoRR. C. Raffel and D. P. W. Ellis, "Feed-forward networks with attention can solve some long-term memory problems," CoRR, vol. abs/1512.08756, 2015. [Online]. Available: http://arxiv.org/abs/1512.08756</p>
<p>Long short-term memory-networks for machine reading. J Cheng, L Dong, M Lapata, abs/1601.06733CoRR. J. Cheng, L. Dong, and M. Lapata, "Long short-term memory-networks for machine reading," CoRR, vol. abs/1601.06733, 2016. [Online].</p>
<p>Report on a general problem solving program. A Newell, J C Shaw, H A Simon, Proceedings of the International Conference on Information Processing. the International Conference on Information ProcessingA. Newell, J. C. Shaw, and H. A. Simon, "Report on a general problem solving program," in Proceedings of the International Conference on Information Processing, 1959, pp. 256-264.</p>
<p>Q-learning. C J Watkins, P Dayan, Machine learning. 83-4C. J. Watkins and P. Dayan, "Q-learning," Machine learning, vol. 8, no. 3-4, pp. 279-292, 1992.</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781cs.CLT. Mikolov, K. Chen, G. Corrado, and J. Dean, "Efficient estimation of word representations in vector space," 2013, arXiv:1301.3781 [cs.CL].</p>
<p>TensorFlow: Large-scale machine learning on heterogeneous systems. M A , 2015, software available from tensorflow.org. M. A. et al., "TensorFlow: Large-scale machine learning on heterogeneous systems," 2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/</p>
<p>Application of artificial neural networks with attention mechanism for discovering distant dependencies in time series. S Malik, University of WrocławBachelor ThesisS. Malik, "Application of artificial neural networks with attention mech- anism for discovering distant dependencies in time series," Bachelor Thesis, University of Wrocław, 2016.</p>
<p>Learning to Win by Reading Manuals in a Monte-Carlo Framework. S Branavan, D Silver, R Barzilay, Journal of Artificial Intelligence Research. 43S. Branavan, D. Silver, and R. Barzilay, "Learning to Win by Reading Manuals in a Monte-Carlo Framework," Journal of Artificial Intelligence Research, vol. 43, pp. 661-704, 2012.</p>
<p>S Bird, E Klein, E Loper, Natural Language Processing with Python. Reilly Media, Inc1st ed. O'S. Bird, E. Klein, and E. Loper, Natural Language Processing with Python, 1st ed. O'Reilly Media, Inc., 2009.</p>
<p>Practical bayesian optimization of machine learning algorithms. J Snoek, H Larochelle, R P Adams, Advances in Neural Information Processing Systems. F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. WeinbergerCurran Associates, Inc25J. Snoek, H. Larochelle, and R. P. Adams, "Practical bayesian optimiza- tion of machine learning algorithms," in Advances in Neural Information Processing Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2012, pp. 2951-2959.</p>            </div>
        </div>

    </div>
</body>
</html>
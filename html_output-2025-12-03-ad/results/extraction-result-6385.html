<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6385 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6385</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6385</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-271039030</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.04078v3.pdf" target="_blank">DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have made impressive progress in handling simple math problems, yet they still struggle with more challenging and complex mathematical tasks. In this paper, we introduce a series of LLMs that employs the Decomposition of thought with code assistance and self-correction for mathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex mathematical tasks by decomposing them into simpler logical subtasks, leveraging code to solve these subtasks, obtaining fine-grained feedback from the code interpreter, and engaging in self-reflection and correction. By annotating diverse interactive tool-use trajectories and employing query evolution on GSM8K and MATH datasets, we generate an instruction fine-tuning dataset called DotaMathQA with 574K query-response pairs. We train a series of base LLMs using imitation learning on DotaMathQA, resulting in DotaMath models that achieve remarkable performance compared to open-source LLMs across various in-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases an outstanding performance of 64.8% on the competitive MATH dataset and 86.7% on GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a series of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward, we anticipate that the DotaMath paradigm will open new pathways for addressing intricate mathematical problems. Our code is publicly available at https://github.com/ChengpengLi1003/DotaMath.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6385.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6385.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DotaMath (paradigm)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A problem-solving paradigm and instruction-finetuned model family that decomposes math problems into subtasks, emits Python code that prints intermediate subtask results, invokes a Python interpreter for execution feedback, and uses multi-turn self-correction to refine solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DotaMath (series)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (LLaMA-family variants fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>primarily 7B (also experiments with 8B LLaMA3)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Supervised fine-tuning on DotaMathQA (574K examples): GPT-4-annotated Python-execution trajectories, includes code using sympy and explicit printed intermediate variables; dataset contains single-turn (one tool call) and multi-turn (self-correction) examples plus large augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH, Mathematics (Saxton), SVAMP, TabMWP, ASDiv</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems, arithmetic, algebra, geometry, counting & probability, competition-level math</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems solved via plan+Python program; model prints intermediate values inside code (COT-like within code block)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>range from grade-school (GSM8K) to hard competition problems (MATH Level 5)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>instruction fine-tuning with program-of-thought / tool-use: decomposition + program synthesis (```Python ... ```output markers), intermediate-process display (prints), and multi-turn self-correction; GPT-4 used to generate training trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact numeric answer matching after sympy normalization/rounding)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Best DotaMath-deepseek-7B: GSM8K 86.7% (with tool), MATH 64.8% (with tool); average across six benchmarks ~80.1% (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper reports that printing intermediate subtask results inside code increases human-readability and enables the model to learn to simulate Python execution; DotaMath models can often infer interpreter outputs (simulate execution) and thus work even without the tool, with smaller accuracy drops than other tool-trained models. The model frequently relies on symbolic libraries (e.g., sympy) in training traces and therefore learns patterns of library usage and printed intermediate states.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When solving hard (MATH) problems, large reliance on external library calls (sympy) and complex computations makes accurate simulation without tool difficult; mistakes include incorrect algebraic simplification, incorrect decomposition or variable handling leading to execution errors, and initial incorrect code that requires self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Fine-tuning (DotaMathQA) yields large gains even at 7B; DotaMath-7B models outperform many 70B open-source models on GSM8K and MATH when using tool-based trajectories and self-correction. Data augmentation and multi-turn self-correction particularly benefit harder questions (larger relative gains on MATH Level 5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6385.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6385.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DotaMath-deepseek-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DotaMath fine-tuned on DeepSeekMath-Base-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B open-source LLM (DeepSeekMath-Base backbone) fine-tuned with DotaMathQA to produce programmatic plan+code trajectories and perform self-correction; shows strong performance on both GSM8K and MATH with Python tool access.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DotaMath-deepseek-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (DeepSeekMath-Base backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on DotaMathQA (574K) containing GPT-4 generated code-invocation trajectories, printed intermediate variables, and both single-turn and multi-turn self-correction examples; augmented queries included.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH, Mathematics, SVAMP, TabMWP, ASDiv</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems, algebra, geometry, counting & probability, competition math</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language question → decomposition → Python program (prints subresults) → Python interpreter output fed back → optional self-correction → boxed numeric answer</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K (grade-school), MATH (competition; sublevels Level 1–5 with Level 5 hardest)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>supervised fine-tuning to emit plan+code; uses program-of-thought style prompts, tool-use (Python interpreter), printed intermediate-process COT inside code, and multi-turn correction prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (answer correctness after sympy-normalized comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 86.7% (with tool) / 79.3% (without tool, −8.5%); MATH: 64.8% (with tool) / 38.2% (without tool, −41.0%); average (six benchmarks) reported ≈ 80.1%</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Shows improved ability to simulate Python execution compared to other tool-trained models: modest performance drop when tools are removed (8.5% on GSM8K) indicating learned execution simulation; nevertheless, on hard MATH problems simulation is much harder due to reliance on library calls and complex symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Larger performance collapse on complex problems when interpreter unavailable; failure cases include wrong algebraic simplification, variable initialization/order errors in generated code, and inability to predict outputs of invoked library functions precisely for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Fine-tuning with decomposition, intermediate process display, and self-correction yields large relative improvements on harder problems (MATH Level 5 gains largest); 7B DotaMath-deepseek surpasses many larger open-source models, indicating that targeted SFT + tool traces can substitute for raw parameter scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6385.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6385.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DotaMathQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DotaMathQA (instruction-following dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 574K-example supervised fine-tuning dataset built by GPT-4 annotation and model self-annotation containing single-turn and multi-turn (self-correction) Python execution trajectories that print intermediate subtask results and are filtered by Python execution correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DotaMathQA</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>dataset for decoder-only transformer fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Contains D_seed-single (80K correct GPT-4 code traces), D_auto-multi (≈2K GPT-4 corrective trajectories), D_rule-multi (10K rule-generated corrective trajectories), and D_aug-single (482K augmented queries); traces include explicit code prints and sympy usage; many examples validated by executing generated python code and filtering out buggy runs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>used to fine-tune models evaluated on GSM8K and MATH (and out-of-domain benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and symbolic math tasks; includes single-step execution and multi-turn correction trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language → annotated plan+Python code blocks that print intermediate results → interpreter output → optional correction sequence</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>spans easy (GSM8K) to very hard (MATH Level 5)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>GPT-4 generative prompts (plan+code guidelines include printing all intermediate subtasks and avoiding decimals), corrective prompt to produce corrected solutions, and rule-based transformations for self-correction examples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>improvement in downstream model accuracy on GSM8K and MATH after SFT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablations: removing augmentation reduced GSM8K by ~3.0% and MATH by ~6.0%; at Level 5 MATH augmentation increased accuracy from 35.5% to 41.9% (+18.7% relative reported), showing large impact on hard problems.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Dataset design (decomposition + intermediate prints + self-correction) encourages models to learn to predict interpreter outputs and to perform program-like reasoning; the dataset enables model to simulate code execution without interpreter better than comparable datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Rule-based multi-turn data less efficient per-example than GPT-4 generated corrective examples; incorrect or buggy GPT-4 solutions must be re-sampled or filtered, and residual annotation errors can reduce usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Larger quantities of correct tool-execution traces and augmentation lead to better performance on harder math domains; self-correction examples have complementary benefits and GPT-4 generated corrective trajectories appear more sample-efficient than simple rule-based corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6385.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6385.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (used for annotation & baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (including GPT-4 Code Interpreter / gpt-4-turbo-2024-04-09 used for annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large LLM used both as a high-quality annotator to synthesize DotaMathQA program+execution trajectories and as a baseline competitor (with and without code-interpreter tool).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-4 Code Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (proprietary OpenAI family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not detailed in paper; used as an oracle annotator to produce programmatic solutions and corrections, including code that executes under Python/sympy</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH (used as baselines and to synthesize training data)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and competition math; also used to generate and correct training traces</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language questions, code-augmented responses when using Code Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to competition-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>generative prompt to produce plan+code with printed intermediate results; corrective prompts to produce multi-turn corrections; nucleus sampling used for generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported baselines: GPT-4 (original): GSM8K 92.0% / MATH 42.5%; GPT-4 Code Interpreter: GSM8K 97.0% / MATH 69.7% (as listed in paper's Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Used to bootstrap large-scale instruction trajectories; GPT-4 produced many correct program traces but some incorrect ones required re-sampling or corrective prompting; GPT-4 corrective outputs were used to create multi-turn self-correction examples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Even GPT-4 produces incorrect program solutions sometimes; incorrect GPT-4 traces are handled via re-sampling, corrective prompts, or removal by execution filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Using a stronger annotator (GPT-4) to create SFT data yields larger downstream gains than weaker self-annotation methods; GPT-4 Code Interpreter provides especially strong performance when allowed to run code.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6385.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6385.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToRA (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToRA (tool‑augmented reasoning approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior open-source tool-based model that integrates code/tool invocation into reasoning; used as a baseline showing strong gains with tools but poor simulation ability without interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ToRA (variants: 70B and 7B reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (tool-augmented models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>reported 70B and 7B variants in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained with tool traces / code invocation outputs (details in referenced work rather than in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems, arithmetic, competition math</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language with code/tool invocation</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to competition</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>tool-use/program synthesis (code interpreter), chain-of-thought + code</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>As reported: ToRA (70B) GSM8K 84.3% / MATH 49.7% (Table 2); ToRA-7B with tool 68.8% GSM8K, but without tool 9.6% (huge collapse without interpreter) as shown in Table 7</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Trained on Python interpreter outputs but the paper reports ToRA struggles to correctly simulate interpreter outputs without access to the tool (very large accuracy drops when tool removed).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Extremely tool-dependent; without interpreter, model fails to predict code outputs accurately leading to >80% relative drop on some benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Tool access largely determines performance on complex tasks; model size plus tool usage improves performance but simulation capability without tool was poor in reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6385.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6385.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-MATH-Instruct (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-MATH-Instruct (Shao et al., 2024) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B model trained specifically for math tasks and used as a backbone and baseline; a DeepSeek-based variant fine-tuned with DotaMathQA becomes DotaMath-deepseek-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-MATH-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (DeepSeek variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained/continually trained with math-specific corpora (as described in Shao et al.); used as a backbone in experiments and baseline comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic, algebra, other math domains</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems; has variants trained with tool traces</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K (grade-school) to MATH (competition)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>some variants use tool-execution traces; standard chain-of-thought and code-invocation training used in prior work</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported: DeepSeek-MATH-Instruct GSM8K 83.7% (with tool) / 39.9% (without tool, −52.3%); MATH 57.4% (with tool) / 24.1% (without tool, −58.0%) (Table 7)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Although trained with interpreter outputs, DeepSeek-MATH-Instruct performs poorly at simulating execution without the tool compared to DotaMath variants; larger accuracy collapse when tool removed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Large reliance on interpreter at inference; struggles to predict outputs of symbolic library calls and complex program results.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Fine-tuning on high-quality tool-execution traces (as in DotaMathQA) can substantially improve performance over the pre-existing DeepSeek-MATH-Instruct baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification <em>(Rating: 2)</em></li>
                <li>Deepseekmath: Pushing the limits of mathematical reasoning in open language models <em>(Rating: 2)</em></li>
                <li>Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Making language models better tool learners with execution feedback <em>(Rating: 2)</em></li>
                <li>ToRA <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6385",
    "paper_id": "paper-271039030",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "DotaMath (paradigm)",
            "name_full": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning",
            "brief_description": "A problem-solving paradigm and instruction-finetuned model family that decomposes math problems into subtasks, emits Python code that prints intermediate subtask results, invokes a Python interpreter for execution feedback, and uses multi-turn self-correction to refine solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DotaMath (series)",
            "model_family": "decoder-only transformer (LLaMA-family variants fine-tuned)",
            "model_size": "primarily 7B (also experiments with 8B LLaMA3)",
            "training_data_description": "Supervised fine-tuning on DotaMathQA (574K examples): GPT-4-annotated Python-execution trajectories, includes code using sympy and explicit printed intermediate variables; dataset contains single-turn (one tool call) and multi-turn (self-correction) examples plus large augmentation.",
            "benchmark_name": "GSM8K, MATH, Mathematics (Saxton), SVAMP, TabMWP, ASDiv",
            "task_type": "multi-step word problems, arithmetic, algebra, geometry, counting & probability, competition-level math",
            "problem_format": "natural-language word problems solved via plan+Python program; model prints intermediate values inside code (COT-like within code block)",
            "difficulty_level": "range from grade-school (GSM8K) to hard competition problems (MATH Level 5)",
            "prompting_method": "instruction fine-tuning with program-of-thought / tool-use: decomposition + program synthesis (```Python ... ```output markers), intermediate-process display (prints), and multi-turn self-correction; GPT-4 used to generate training trajectories",
            "performance_metric": "accuracy (exact numeric answer matching after sympy normalization/rounding)",
            "performance_value": "Best DotaMath-deepseek-7B: GSM8K 86.7% (with tool), MATH 64.8% (with tool); average across six benchmarks ~80.1% (reported)",
            "internal_analysis": "Paper reports that printing intermediate subtask results inside code increases human-readability and enables the model to learn to simulate Python execution; DotaMath models can often infer interpreter outputs (simulate execution) and thus work even without the tool, with smaller accuracy drops than other tool-trained models. The model frequently relies on symbolic libraries (e.g., sympy) in training traces and therefore learns patterns of library usage and printed intermediate states.",
            "failure_modes": "When solving hard (MATH) problems, large reliance on external library calls (sympy) and complex computations makes accurate simulation without tool difficult; mistakes include incorrect algebraic simplification, incorrect decomposition or variable handling leading to execution errors, and initial incorrect code that requires self-correction.",
            "scaling_trend": "Fine-tuning (DotaMathQA) yields large gains even at 7B; DotaMath-7B models outperform many 70B open-source models on GSM8K and MATH when using tool-based trajectories and self-correction. Data augmentation and multi-turn self-correction particularly benefit harder questions (larger relative gains on MATH Level 5).",
            "uuid": "e6385.0",
            "source_info": {
                "paper_title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "DotaMath-deepseek-7B",
            "name_full": "DotaMath fine-tuned on DeepSeekMath-Base-7B",
            "brief_description": "A 7B open-source LLM (DeepSeekMath-Base backbone) fine-tuned with DotaMathQA to produce programmatic plan+code trajectories and perform self-correction; shows strong performance on both GSM8K and MATH with Python tool access.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DotaMath-deepseek-7B",
            "model_family": "decoder-only transformer (DeepSeekMath-Base backbone)",
            "model_size": "7B",
            "training_data_description": "Fine-tuned on DotaMathQA (574K) containing GPT-4 generated code-invocation trajectories, printed intermediate variables, and both single-turn and multi-turn self-correction examples; augmented queries included.",
            "benchmark_name": "GSM8K, MATH, Mathematics, SVAMP, TabMWP, ASDiv",
            "task_type": "multi-step arithmetic word problems, algebra, geometry, counting & probability, competition math",
            "problem_format": "natural-language question → decomposition → Python program (prints subresults) → Python interpreter output fed back → optional self-correction → boxed numeric answer",
            "difficulty_level": "GSM8K (grade-school), MATH (competition; sublevels Level 1–5 with Level 5 hardest)",
            "prompting_method": "supervised fine-tuning to emit plan+code; uses program-of-thought style prompts, tool-use (Python interpreter), printed intermediate-process COT inside code, and multi-turn correction prompts",
            "performance_metric": "accuracy (answer correctness after sympy-normalized comparison)",
            "performance_value": "GSM8K: 86.7% (with tool) / 79.3% (without tool, −8.5%); MATH: 64.8% (with tool) / 38.2% (without tool, −41.0%); average (six benchmarks) reported ≈ 80.1%",
            "internal_analysis": "Shows improved ability to simulate Python execution compared to other tool-trained models: modest performance drop when tools are removed (8.5% on GSM8K) indicating learned execution simulation; nevertheless, on hard MATH problems simulation is much harder due to reliance on library calls and complex symbolic computation.",
            "failure_modes": "Larger performance collapse on complex problems when interpreter unavailable; failure cases include wrong algebraic simplification, variable initialization/order errors in generated code, and inability to predict outputs of invoked library functions precisely for complex tasks.",
            "scaling_trend": "Fine-tuning with decomposition, intermediate process display, and self-correction yields large relative improvements on harder problems (MATH Level 5 gains largest); 7B DotaMath-deepseek surpasses many larger open-source models, indicating that targeted SFT + tool traces can substitute for raw parameter scale.",
            "uuid": "e6385.1",
            "source_info": {
                "paper_title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "DotaMathQA",
            "name_full": "DotaMathQA (instruction-following dataset)",
            "brief_description": "A 574K-example supervised fine-tuning dataset built by GPT-4 annotation and model self-annotation containing single-turn and multi-turn (self-correction) Python execution trajectories that print intermediate subtask results and are filtered by Python execution correctness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DotaMathQA",
            "model_family": "dataset for decoder-only transformer fine-tuning",
            "model_size": "N/A (dataset)",
            "training_data_description": "Contains D_seed-single (80K correct GPT-4 code traces), D_auto-multi (≈2K GPT-4 corrective trajectories), D_rule-multi (10K rule-generated corrective trajectories), and D_aug-single (482K augmented queries); traces include explicit code prints and sympy usage; many examples validated by executing generated python code and filtering out buggy runs.",
            "benchmark_name": "used to fine-tune models evaluated on GSM8K and MATH (and out-of-domain benchmarks)",
            "task_type": "multi-step arithmetic and symbolic math tasks; includes single-step execution and multi-turn correction trajectories",
            "problem_format": "natural-language → annotated plan+Python code blocks that print intermediate results → interpreter output → optional correction sequence",
            "difficulty_level": "spans easy (GSM8K) to very hard (MATH Level 5)",
            "prompting_method": "GPT-4 generative prompts (plan+code guidelines include printing all intermediate subtasks and avoiding decimals), corrective prompt to produce corrected solutions, and rule-based transformations for self-correction examples",
            "performance_metric": "improvement in downstream model accuracy on GSM8K and MATH after SFT",
            "performance_value": "Ablations: removing augmentation reduced GSM8K by ~3.0% and MATH by ~6.0%; at Level 5 MATH augmentation increased accuracy from 35.5% to 41.9% (+18.7% relative reported), showing large impact on hard problems.",
            "internal_analysis": "Dataset design (decomposition + intermediate prints + self-correction) encourages models to learn to predict interpreter outputs and to perform program-like reasoning; the dataset enables model to simulate code execution without interpreter better than comparable datasets.",
            "failure_modes": "Rule-based multi-turn data less efficient per-example than GPT-4 generated corrective examples; incorrect or buggy GPT-4 solutions must be re-sampled or filtered, and residual annotation errors can reduce usefulness.",
            "scaling_trend": "Larger quantities of correct tool-execution traces and augmentation lead to better performance on harder math domains; self-correction examples have complementary benefits and GPT-4 generated corrective trajectories appear more sample-efficient than simple rule-based corrections.",
            "uuid": "e6385.2",
            "source_info": {
                "paper_title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 (used for annotation & baseline)",
            "name_full": "GPT-4 (including GPT-4 Code Interpreter / gpt-4-turbo-2024-04-09 used for annotation)",
            "brief_description": "A proprietary large LLM used both as a high-quality annotator to synthesize DotaMathQA program+execution trajectories and as a baseline competitor (with and without code-interpreter tool).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 / GPT-4 Code Interpreter",
            "model_family": "decoder-only transformer (proprietary OpenAI family)",
            "model_size": "not specified in paper",
            "training_data_description": "Not detailed in paper; used as an oracle annotator to produce programmatic solutions and corrections, including code that executes under Python/sympy",
            "benchmark_name": "GSM8K, MATH (used as baselines and to synthesize training data)",
            "task_type": "multi-step arithmetic and competition math; also used to generate and correct training traces",
            "problem_format": "natural-language questions, code-augmented responses when using Code Interpreter",
            "difficulty_level": "grade-school to competition-level",
            "prompting_method": "generative prompt to produce plan+code with printed intermediate results; corrective prompts to produce multi-turn corrections; nucleus sampling used for generation",
            "performance_metric": "accuracy",
            "performance_value": "Reported baselines: GPT-4 (original): GSM8K 92.0% / MATH 42.5%; GPT-4 Code Interpreter: GSM8K 97.0% / MATH 69.7% (as listed in paper's Table 2)",
            "internal_analysis": "Used to bootstrap large-scale instruction trajectories; GPT-4 produced many correct program traces but some incorrect ones required re-sampling or corrective prompting; GPT-4 corrective outputs were used to create multi-turn self-correction examples.",
            "failure_modes": "Even GPT-4 produces incorrect program solutions sometimes; incorrect GPT-4 traces are handled via re-sampling, corrective prompts, or removal by execution filtering.",
            "scaling_trend": "Using a stronger annotator (GPT-4) to create SFT data yields larger downstream gains than weaker self-annotation methods; GPT-4 Code Interpreter provides especially strong performance when allowed to run code.",
            "uuid": "e6385.3",
            "source_info": {
                "paper_title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ToRA (baseline)",
            "name_full": "ToRA (tool‑augmented reasoning approach)",
            "brief_description": "A prior open-source tool-based model that integrates code/tool invocation into reasoning; used as a baseline showing strong gains with tools but poor simulation ability without interpreter.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ToRA (variants: 70B and 7B reported)",
            "model_family": "decoder-only transformer (tool-augmented models)",
            "model_size": "reported 70B and 7B variants in comparisons",
            "training_data_description": "Trained with tool traces / code invocation outputs (details in referenced work rather than in this paper)",
            "benchmark_name": "GSM8K, MATH",
            "task_type": "multi-step word problems, arithmetic, competition math",
            "problem_format": "natural-language with code/tool invocation",
            "difficulty_level": "grade-school to competition",
            "prompting_method": "tool-use/program synthesis (code interpreter), chain-of-thought + code",
            "performance_metric": "accuracy",
            "performance_value": "As reported: ToRA (70B) GSM8K 84.3% / MATH 49.7% (Table 2); ToRA-7B with tool 68.8% GSM8K, but without tool 9.6% (huge collapse without interpreter) as shown in Table 7",
            "internal_analysis": "Trained on Python interpreter outputs but the paper reports ToRA struggles to correctly simulate interpreter outputs without access to the tool (very large accuracy drops when tool removed).",
            "failure_modes": "Extremely tool-dependent; without interpreter, model fails to predict code outputs accurately leading to &gt;80% relative drop on some benchmarks.",
            "scaling_trend": "Tool access largely determines performance on complex tasks; model size plus tool usage improves performance but simulation capability without tool was poor in reported comparisons.",
            "uuid": "e6385.4",
            "source_info": {
                "paper_title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "DeepSeek-MATH-Instruct (baseline)",
            "name_full": "DeepSeek-MATH-Instruct (Shao et al., 2024) baseline",
            "brief_description": "An open-source 7B model trained specifically for math tasks and used as a backbone and baseline; a DeepSeek-based variant fine-tuned with DotaMathQA becomes DotaMath-deepseek-7B.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-MATH-Instruct",
            "model_family": "decoder-only transformer (DeepSeek variant)",
            "model_size": "7B (reported)",
            "training_data_description": "Pretrained/continually trained with math-specific corpora (as described in Shao et al.); used as a backbone in experiments and baseline comparisons",
            "benchmark_name": "GSM8K, MATH",
            "task_type": "multi-step arithmetic, algebra, other math domains",
            "problem_format": "natural-language word problems; has variants trained with tool traces",
            "difficulty_level": "GSM8K (grade-school) to MATH (competition)",
            "prompting_method": "some variants use tool-execution traces; standard chain-of-thought and code-invocation training used in prior work",
            "performance_metric": "accuracy",
            "performance_value": "Reported: DeepSeek-MATH-Instruct GSM8K 83.7% (with tool) / 39.9% (without tool, −52.3%); MATH 57.4% (with tool) / 24.1% (without tool, −58.0%) (Table 7)",
            "internal_analysis": "Although trained with interpreter outputs, DeepSeek-MATH-Instruct performs poorly at simulating execution without the tool compared to DotaMath variants; larger accuracy collapse when tool removed.",
            "failure_modes": "Large reliance on interpreter at inference; struggles to predict outputs of symbolic library calls and complex program results.",
            "scaling_trend": "Fine-tuning on high-quality tool-execution traces (as in DotaMathQA) can substantially improve performance over the pre-existing DeepSeek-MATH-Instruct baseline.",
            "uuid": "e6385.5",
            "source_info": {
                "paper_title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification",
            "rating": 2,
            "sanitized_title": "solving_challenging_math_word_problems_using_gpt4_code_interpreter_with_codebased_selfverification"
        },
        {
            "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
            "rating": 2,
            "sanitized_title": "deepseekmath_pushing_the_limits_of_mathematical_reasoning_in_open_language_models"
        },
        {
            "paper_title": "Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning.",
            "rating": 2,
            "sanitized_title": "mathcoder_seamless_code_integration_in_llms_for_enhanced_mathematical_reasoning"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Making language models better tool learners with execution feedback",
            "rating": 2,
            "sanitized_title": "making_language_models_better_tool_learners_with_execution_feedback"
        },
        {
            "paper_title": "ToRA",
            "rating": 1
        }
    ],
    "cost": 0.01638175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning
17 Jul 2024</p>
<p>Chengpeng Li 
Guanting Dong 
Mingfeng Xue 
Ru Peng 
Xiang Wang 
University of Science</p>
<p>Dayiheng Liu 
Mark Chen 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Henrique Ponde 
Oliveira Pinto 
Jared Ka- Plan 
Harri Edwards 
Yuri Burda 
Nicholas Joseph 
Greg Brockman 
Alex Ray 
Raul Puri 
Gretchen Krueger 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Scott Gray 
Nick Ryder 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Clemens Winter 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
Ariel Herbert-Voss 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Christopher Hesse 
Andrew N Carr 
Jan Leike 
Josh Achiam 
Vedant Misra 
Evan Morikawa 
Alec Radford 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Dario Amodei 
Sam Mccandlish 
Ilya Sutskever 
Wojciech 2021 Zaremba 
Evaluating 
Wenhu Chen 
Xueguang Ma 
Xinyi Wang 
William W Cohen 
Xinyun Chen 
Maxwell Lin 
Nathanael Schärli 
QuocDenny 2023 Zhou 
Karl 2023 Cobbe 
Vineet Kosaraju 
Jacob Hilton 
Reiichiro Nakano 
Yiming Huang 
Xiao Liu 
Yeyun Gong 
Zhibin Gou 
Yelong Shen 
Nan Duan 
Weizhu Chen 
Tushar Khot 
Harsh Trivedi 
Matthew Finlayson 
Yao Fu 
Kyle Richardson 
Peter Clark 
Ashish Sabharwal 
Takeshi Kojima 
Shane Shixiang 
Machel Gu 
Yu Reid 
Minpeng Liao 
Wei Luo 
Chengxi Li 
Jing Wu 
Kai Fan 2024 Mario 
Hunter Lightman 
Yura Burda 
Bowen Baker 
Teddy Lee 
John Schulman 
Shayne Longpre 
Le Hou 
Tu Vu 
Albert Webson 
Hyung Won Chung 
Yi Tay 
V Le 
Barret Zoph 
Jason Wei 
Pan Lu 
Liang Qiu 
Kai-Wei Chang 
Ying Nian Wu 
Song-Chun Zhu 
Tanmay Rajpurohit 
Ashwin 2023 Kalyan 
Long Ouyang 
Jeff Wu 
Xu Jiang 
Diogo Almeida 
Car- Roll L Wainwright 
Chong Zhang 
Sandhini Agarwal 
Katarina Slama 
Fraser Kelton 
Luke Miller 
Maddie Simens 
Amanda Askell 
Paul Christiano 
Ryan 2022 Lowe </p>
<p>Albert Q Jiang
Blanche Savary
Devendra Singh Chaplot
Diego de las Casas
Alexandre Sablayrolles
Arthur Mensch, Chris Bam-ford, Emma Bou Hanna, Florian Bressand2024Antoine Roux</p>
<p>DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning
17 Jul 2024128B5360C845A4DEC1967AE81E760B4AarXiv:2407.04078v3[cs.CL]
and Technology of China 2 Alibaba Group.{lichengpeng.lcp,liudayiheng.ldyh}@alibaba-inc.com"Divide each difficulty into as many parts as is feasible and necessary to resolve it."-René Descartes</p>
<p>Introduction</p>
<p>The emergence of large language models (LLMs) (Ouyang et al., 2022;Anil et al., 2023b;OpenAI, 2024;Anil et al., 2023a;Anthropic, 2024;Yang et al., 2024) has profoundly revolutionized a diverse range of natural language processing benchmarks (Chen et al., 2021;Longpre et al., 2023;Wei et al., 2023;Luo et al., 2023b;Qiao et al., 2024;Song et al., 2024).However, in the challenging field of mathematical reasoning, enabling open-source LLMs to possess reasoning abilities for complex mathematical tasks remains a significant challenge (Gou et al., 2023;Yue et al., 2023Yue et al., , 2024a)).</p>
<p>Existing works have attempted to enhance the reasoning capabilities of LLMs through methods such as chain-of-thought (COT) (Wei et al., 2022), program-of-thought (POT) (Chen et al., 2022;Gao et al.), and tool-integrated reasoning approaches (Tool-based) (Gou et al., 2023;Wang et al., 2023a).The Tool-based approach effectively merges COT's semantic and abstract reasoning with POT's computational precision, demonstrating commendable performance.Meanwhile, several efforts utilize state-of-the-art proprietary models like GPT-4 to augment existing mathematical reasoning datasets (Yu et al., 2023;Luo et al., 2023a;Li et al., 2023a), thereby improving the reasoning capabilities of LLMs during the supervised fine-tuning (SFT) phase.Building on prior works, open-source LLMs have achieved commendable performance on simple math problems.For example, on GSM8K (Cobbe et al., 2021) which contains grade school math word problems, many math-specific LLMs exceeding 80% accuracy.However, they continue to struggle with complex mathematical reasoning tasks.For instance, on MATH (Hendrycks et al., 2021) dataset comprising challenging competition problems, almost all open-source LLMs cannot exceed 60% accu-racy.Through our investigation, we find that these open-source LLMs lack meticulous design for complex mathematical tasks.They do not consider the necessity of task decomposition for complex tasks, nor do they account for the need for LLMs to obtain more feedback signals from tools to facilitate comprehensive analysis.</p>
<p>To improve the capabilities of open-source LLMs in complex mathematical reasoning tasks, this paper introduces DotaMath models, a series of LLMs which employ the Decomposition of thought with code assistance and self-correction for mathematical Reasoning.There are three special designs in DotaMath for complex mathematical tasks, as depicted in Figure 1.(1) Decomposition of thought: The principle of divide-andconquer often allows complex tasks to be decomposed into more easily solvable subtasks.Inspired by some relevant works (Khot et al.;Li et al., 2023b), DotaMath break down mathematical problems into logical subtasks and use code to solve them.(2) Intermediate process display: While previous tool-based math-specifical LLMs (Wang et al., 2023a;Gou et al., 2023;Shao et al., 2024) obtain only single mathematical expressions from code interpreters, we aim for DotaMATH to receive more fine-grained feedback from the code interpreter for subsequent comprehensive analysis.To achieve this goal, we facilitate the model to print the results of all subtasks in the form of chain of thought within the code.This design also contributes to enhancing the human readability of the model's responses.(3) Self-correction: When solving complex tasks, the issue of not succeeding at once sometimes arises.Self-reflection and correction are appropriate for resolving this problem (Shinn et al., 2023;Zhang et al., 2024).We construct two types of instruction fine-tuning data to endow DotaMath with self-correction capabilities.</p>
<p>For data construction, we introduce an instruction-following dataset named Dota-MathQA, based on the human-curated mathematical datasets GSM8K and MATH.As shown in Figure 2, DotaMathQA contains two types of data: one involves data that requires a single invocation of Python code, referred to as Single-turn QA; the other includes data with a self-correction process, necessitating multiple invocations of Python code, referred to as Multi-turn QA.Inspired by previous works (Luo et al., 2023a;Yu et al., 2023;Li et al., 2023a), we adopt the query evolution to bootstrap mathematical questions in GSM8K and MATH with the augmentation prompt in Appendix D.</p>
<p>With DotaMathQA, we fine-tune a series of backbone models, including Llama2-7B (Touvron et al., 2023), Llama3-8B (Meta, 2024), Llemma-7B (Azerbayev et al., 2024) and DeepSeekMath-Base-7B (Shao et al., 2024).As shown in Table 2, DotaMath outperforms open-source models across all scales on in-domain GSM8K and MATH datasets and four out-of-domain mathemathical benchmarks overall.Notably, DotaMathdeepseek-7B showcases an outstanding performance of 64.8% on the competitive MATH dataset.Besides, DotaMath-deepseek-7B maintains strong competitiveness on GSM8K (86.7%) and achieves an anverage of 80.1% on two in-domain benchmarks and four out-of-domain benchmarks.Looking ahead, we believe the DotaMath paradigm will pave a new avenue for solving complex mathematical tasks.</p>
<p>Related Work</p>
<p>Mathematical Reasoning Techniques in LLMs.Early attempts to solve mathematical problems using large language models rely on basic computational expressions and results presented as fewshot examples (Brown et al., 2020).Moreover, Wei et al. (2022); Kojima et al. (2022) employ intermediate steps to enhance the capability of large language models in tackling complex mathematical reasoning tasks.Building on this approach, Chen et al. ( 2022); Gao et al. introduce the use of code tools in the problem-solving process.Subsequent researches explore various collaborative paradigms involving Chain-of-Thoughts and coding, which lead to significant improvements in the accuracy of solutions provided by large language models (Yue et al., 2023;Gou et al., 2023;Liao et al., 2024;Ying et al., 2024).Different from them, we introduces DotaMath, a novel math problemsolving paradigm which decomposes mathematical problems into logical subtasks and utilizes code to address these tasks.Our approach demonstrates superior performance across two in-domain and four out-of-domain math datasets.</p>
<p>Data Augmentation for Improved Math Reasoning.Beyond the exploration of paradigms, recent researches have increasingly focused on utilizing high-quality data to enhance the mathematical capabilities of large language models.Some initiatives involve curating diverse collections of Step 1: Standard form of the circle: (x + 4)^2 + (y -3)^2 = 0.</p>
<p>Step 2: Squared radius is 0.</p>
<p>Step 3: Radius of the circle is 0.</p>
<p>The error lies in the incorrect simplification of the equation after completing the square, leading to an incorrect radius calculation.Step 1: Complete the square for x and y terms, after completing the square: Eq((x + 4)<strong>2 + (y -3)</strong>2 -25, 0).</p>
<p>Step 2: Extract the value of r^2 = 25.</p>
<p>Step3: The radius of the circle is 5.</p>
<p>The radius of the circle is directly obtained from the code as 5. Therefore, the final answer is \boxed{5}.mathematical data to train specialized base models tailored specifically for mathematical tasks (Azerbayev et al., 2024;Paster et al., 2023;Wang et al., 2023b;Ying et al., 2024;Shao et al., 2024).Other studies generate synthetic mathematical questionanswer pairs by querying advanced large language models, such as GPTs (Sun et al., 2023), Qwen (Alibaba, 2023), and Mixtral (Jiang et al., 2024), to create Supervised Fine-Tuning (SFT) datasets (Luo et al., 2023a;Yu et al., 2023;Li et al., 2023a;Yue et al., 2024b).In this paper, we presents a synthetic dataset aligned with the DotaMath paradigm, named DotaMathQA, which includes both single-turn-dialog form and multi-turn-dialog form that incorporate a self-correction process.We demonstrate the effectiveness and generalizability of DotaMathQA across various backbone models and benchmarks.
Answer 𝑎 Problem 𝑞 Decomposition1 𝑑 ! = 𝑑 ! ! ⊕ 𝑑 ! " ⊕ 𝑑 ! # Decomposition2 𝑑 " Result2 𝑟 " Result1 𝑟 ! = 𝑟 ! ! ⊕ 𝑟 ! " ⊕ 𝑟 !</p>
<p>Method</p>
<p>In this section, we first introduce how DotaMath performs mathematical reasoning through interaction with Python interpreter ( §3.1 &amp; Fig. 1).</p>
<p>Next, we introduce the pipline of using GPT-4 for data annotation to synthesize the instruction-tuning dataset, DotaMathQA.1 ( §3.2 &amp; Fig. 2).Finally, we discuss the process of supervised fine-tuning a range of foundational LLMs on the DotaMathQA dataset ( §3.3).</p>
<p>Inference Procedure</p>
<p>Motivated by a series of efforts that integrate the Python interpreter's output as supervision (Le et al., 2022;Chen et al., 2023;Qiao et al., 2023;Dong et al., 2024), DotaMath solves mathematical problems through several operations, including task decomposition, writing Python programs, invoking the Python interpreter and self-correction (Figure 1).For a given problem q and system prompt p in Appendix D, DotaMath(M) initially decompose it into some sub-tasks, yielding
d 1 = d 1 1 ⊕ d 2 1 ⊕ d 3 1
, where ⊕ means concatenation.where P M means the probability distribution of a LLM M. Subsequently, DotaMath generates a Python code segment c 1 to address these sub-tasks, presenting the reasoning process of each sub-task internally via substituting the intermediate results of sub-tasks with variables in the code.In line with with (Gou et al., 2023), the Python code utilizes a specific start token "<code>Python" and a distinct end token "</code>output" to determine when to cease generation for invoking the Python interpreter:
d 1 ∼ P M (• | p ⊕ q),(1)c 1 ∼ P M (• | p ⊕ q ⊕ d 1 ).(2)
The execution result r 1 = r 1 1 ⊕ r 2 1 ⊕ r 3 1 (including results of all subtasks) obtained via the Python interpreter is fed back to DotaMath for further analysis.If the problem is resolved, DotaMath will generate the final result a and place the answer within "\boxed{}" for evaluation:
a ∼ P M (• | p ⊕ q ⊕ d 1 ⊕ c 1 ⊕ r 1 ).
(3)</p>
<p>This type of data is referred to as single-turn QA.</p>
<p>Otherwise, DotaMath reflects on the previous decomposition and code based on the results of all sub-tasks, providing an explanation for any errors e 1 :
e 1 ∼ P M (• | p ⊕ q ⊕ d 1 ⊕ c 1 ⊕ r 1 ).(4)
Subsequently, the process of task decomposition, code generation, and invoking the Python interpreter is repeated until the problem is resolved or a predetermined maximum number of tool invocations is reached.This iterative process is referred to as self-correction, and the corresponding data is termed multi-turn QA.Overall, the interaction trajectory between the model and the Python interpreter can be summarized as follows:
τ = d 1 ⊕ c 1 ⊕ r 1 ⊕ e 1 • • • d n ⊕ c n ⊕ r n ⊕ a. (5)</p>
<p>DotaMathQA Dataset Construction</p>
<p>As illustrated in Figure 2 (Yu et al., 2023) 395K GPT-4 We conduct nucleus sampling with a temperature of 0.5 and top-p of 1.0, generating four responses per query.Subsequently, we pass all the generated responses to the Python interpreter.We employ both rules and manual inspection to verify whether the answers match the reference answers in the original GSM8K and MATH datasets.If they match, we place them in D seed-single .If the solutions generated by GPT-4 are incorrect, we revert to using the generative prompt with a temperature of 0.7 and top-p of 1.0 to sample 10 more responses per query.This re-annotation process is repeated up to five times.
× MATH &amp; GSM8K MuggleMATH (Li et al., 2023a) 600K GPT-4 × MATH &amp; GSM8K DotaMathQA 574K GPT-4 ✓ MATH &amp; GSM8K
D aug-single Construction.Utilizing GPT-4 to bootstrap queries in the training dataset for diversification significantly enhances the in-domain reasoning capabilities of large models (Luo et al., 2023a;Yu et al., 2023;Li et al., 2023a).Similar to Li et al., we modify mathematical problems in GSM8K and MATH for query augmentation using the augmentation prompt in Appendix D. Then, we annotate the augmented queries them using generative prompt with GPT-4.Given the absence of standard answers for augmented queries, we only filter out responses that contain python execution bugs.</p>
<p>D auto-multi Construction.For incorrect solutions generated by GPT-4, we use the corrective prompt in Appendix D to instruct GPT-4 to correct them.</p>
<p>The new solution is then sent to the Python interpreter.If the self-correction is successful, we concatenate the incorrect and corrected solutions to create error-correcting data, adding this data to D auto-multi .We fine-tune several DeepSeekMATH-Base models using subsets of D seed-single and then use them to annotate the training sets of GSM8K and MATH.This process aims to enrich the diversity of queries and responses in the error-correcting data, enabling the model to identify and correct a wider range of error patterns.</p>
<p>D rule-multi Construction.The efficiency of obtaining error-correcting data through 3.2 has been observed to be relatively low due to GPT-4's limited success rate in correcting incorrect responses.</p>
<p>To address the issue of low efficiency in obtaining self-correction data, a new method for constructing self-correction data has been designed.For queries that possess both correct and incorrect responses, GPT-4 is directed to analyze the incorrect solution based on the correct one and explain the reasons for the error.The incorrect solution, the explanation of its error, a linking sentence "let's correct the solution", and the correct solution are then concatenated together to form new self-correction data.</p>
<p>Table 1 compares DotaMathQA with several recently introduced mathematical datasets.Overall, D seed-single , D auto-multi and D rule-multi ensure that 99.3% of GSM8K queries and 96% of MATH queries have at least one correct solution or a solution that has undergone correction.The sizes of D seed-single , D auto-multi ,D rule-multi and D aug-single are 80K, 2K, 10K, and 482K, respectively.</p>
<p>Supervised Fine-tuning</p>
<p>We describe D DotaMathQA as D DotaMathQA = {(q i , τ i )} i , where q i represents a math question and τ i indicates an interaction trajectory of natural language and tools in Eq.5.We apply supervised fine-tuning on a series of base models, including Llama2-7B (Touvron et al., 2023), LLaMA3-8B (Meta, 2024), Llemma-7B (Azerbayev et al., 2024), and DeepSeekMath-Base-7B (Shao et al., 2024), resulting in a series of DotaMath-LLMs.For a LLM (parameterized by θ), the optimization objective is to maximize the log likelihood of the reasoning trajectory conditioned on the question,
L(θ) = (q i ,τ i )∈D DotaMathQA log P(τ i | qi ; θ), (6)
where qi represents the content of q i equipped with a system prompt in Appendix D. Metrics.We use the accuracy of predicted answers to evaluate LLMs.Following (Lightman et al., 2023), we round numbers and parsed expressions using sympy2.</p>
<p>Baselines</p>
<p>We benchmark our models with following proprietary Models and Open-Source Models:</p>
<p>• Proprietary Models: Claude-3 (Anthropic, 2024), GPT-3.5 (Brown et al., 2020), GPT-4 (OpenAI, 2023), etc.</p>
<p>• Open-Source Models: WizardMATH (Luo et al., 2023a), MetaMATH (Yu et al., 2023), MuggleMATH (Li et al., 2023a), RFT (Yuan et al., 2023), MATHCoder (Wang et al., 2023a), ToRA (Gou et al., 2023), MARIO (Liao et al., 2024), etc.</p>
<p>For space saving, only part results are listed in Table 2.More results can be found in Table 10.</p>
<p>Main Results</p>
<p>Table 2 compares DotaMath with a range of state of art mathematical methods across in-domain and out-of-domain benchmarks.We can draw several conclusions:</p>
<p>(1) On the elementary mathematical task GSM8K, most state-of-the-art 70B open-source models achieve a performance of over 80, regardless of tool usage.This indicates that the tool-based paradigm does not offer a significant advantage over Chain-of-Thought (COT) methods in simple mathematical reasoning tasks.However, DotaMath, with a size of just 7B, surpasses most of the 70B open-source models, demonstrating strong competitiveness.</p>
<p>(2) On the competition-level mathematical task MATH, models utilizing tools significantly outperform those that do not, emphasizing the necessity of the tool-based paradigm for complex mathematical tasks.DotaMath substantially outperforms all open-source models and even exceeds the strongest proprietary model, Claude-3 Opus.</p>
<p>(3) The DotaMath series also demonstrates the best performance on untrained Out-of-Domain datasets, indicating our model's strong generalization capabilities and comprehensive mathematical reasoning abilities.On average, DotaMathdeepseek outperforms the previous best opensource SFT model, DeepSeek-MATH-Instruct, by 4.4 points..</p>
<p>(4) On in-domain benchmarks, DotaMath-LLaMA2-7B, DotaMath-llemma-7B, DotaMath-LLaMA3-7B, and DotaMath-deepseek-7B exhibit incremental performance improvements.These differences are likely attributable to the quantity and quality of math-related data used in their pretraining or continual pre-training.</p>
<p>Ablation Study</p>
<p>To verify whether our designs enhance the mathematical reasoning capabilities of models, we performed ablation studies on different parts of the data.All results are based on fine-tuning the DeepSeekMath-Base model.In summary, we conducted ablation studies on two components: data format and data augmentation.The data format   3, we observe a decrease in accuracy of 3.0% on GSM8K and 6.0% on MATH, underscoring the effectiveness of decomposition of thought, particularly for the more challenging MATH dataset.Similarly, we remove all print statements from the code in the dataset responses except those printing the final result, and using a Python interpreter, we execute the modified code to obtain new data D DotaMathQA-w/o-aug-w/o-inter .The performance gains at different levels are roughly positively correlated with question difficulty.This indicates that data augmentation is more effective for more difficult questions.At the most difficult Level 5, data augmentation increases the model's performance from 35.5 to 41.9, achieving an impressive 18.7% improvement.For decomposition of thought, Intermediate Process Display, and selfcorrection, the most significant changes also occur with Level 5 questions.This indicates that such designs are effective in enhancing the model's reasoning capabilities for complex problems.</p>
<p>Analysis of Self-correction Data.Since we designed two types of self-correction data, we further analyze and compare them.Despite the 5 times difference in dataset sizes, with automatic multi-turn QA comprising 1,934 instances and rulebased multi-turn QA containing 10,150, the gains from the former on GSM8K are 0.5 lower than the latter but 0.2 lower on MATH form table 5. We reduce the size of rule-based multi-turn QA to the same level of multi-turn QA, the performance of the former is worse than the latter.This suggests that self-correction data generated by GPT-4 may be more efficient than that produced by rulebased methods.Two possible reasons are: (1) The correct components within the rule-generated selfcorrection data might have already been learned by the model, resulting in relatively lower benefits;</p>
<p>(2) The explanations for errors and correct answers in the rule-generated data might not fully align, leading to lower efficiency in learning to correct mistakes.However, rule-based multi-turn data requires only minimal GPT-4 annotation and avoids annotation failures.Combining the two types of self-correction data yields better performance than using either type alone, demonstrating their complementary effect.</p>
<p>Analysis of Data Augmentation Strategies.We compare the model fine-tuned on D DotaMathQA-self where augmented augmented queries are annotated using a model SFT on D DotaMathQA-w/o-aug against DotaMath models, to investigate the benefits of varying annotation strategies.We observe that after employing self-annotation, model performance decreases on GSM8K but increases on MATH.Given the high baseline accuracy of the current model on GSM8K, self-annotation yields a performace decrease.This suggests that the effectiveness of self-annotation is related to the current model's performance on different datasets.Compared with self-annotation, as GPT-4 outperforms the current model, annotations derived from it continue to yield substantial performance improvements.</p>
<p>In the Appendix B, We also analyze the program execution simulation capabilities comparison of DotaMATH and other tool-based LLMs, the effect of filtering out buggy responses during the data augmentation phase, the impact of data augmentation and data format on different subtopics of MATH.</p>
<p>Conclusion</p>
<p>In this paper, we introduce DotaMath, a series of LLMs which adopt decomposition of thought, code assistance, intermediate process display and self-correction to solve complex math problems.To train DotaMath, we construct an instruction fine-tuning datset named DotaMathQA with 574K query-response pairs.In detail, we use query evolution to GSM8K and MATH to augment to the existing queries.Then we use gpt-4 to annotate interactive tool-use trajectories on solve the original and augmented math problems.Ultimately, we fine-tune LLaMA2, LLaMA3, LLeMA, and DeepSeekMath-Base models using DotaMathQA, resulting in the DotaMath series models.Across two in-domain and four out-of-domain mathematical benchmarks, DotaMATH achieves the best or near-best performance among all open-source models and significantly improves performance on the competition-level MATH dataset.Upon analysis, we find that our designed module provides greater assistance with difficult problems in the MATH dataset, validating the rationale of our components for complex tasks.Interestingly, our design significantly enhances the model's ability to simulate program results, allowing DotaMATH to achieve strong performance even without invoking tools.Overall, DotaMATH has further enhanced the capabilities of open-source LLMs on complex mathematical tasks, offering insights for subsequent research in LLM for mathematics.</p>
<p>Appendix A Implementation Details</p>
<p>We fine-tune LLaMA-2-7B, LLaMA-3-7B, llema-7B, and DeepSeekMATH-Base-7B with D DotaMathQA to get DotaMATH-LLMs.We train these base models with key settings including a 5e-5 learning rate, 256 global batch size, a cosine scheduler with a 3% warm-up, a maximum context length of 4,096 (except for LLaMA2, which uses 2,048) tokens and 3 training epochs.Responses are generated using greedy decoding with a maximum sequence length of 4,096 (except for LLaMA2, which applies 2,048) and a limit of 3 maximum tool uses.Checkpoints are not selected with early stops.The hardware setup involves 32 NVIDIA A100 GPUs.</p>
<p>B Additional Analysis</p>
<p>B.1 Analysis on Simulating Code Execution</p>
<p>To enhance the readability of the model's output, we designed a unique mechanism that enables the model's code to print sub-tasks of a problem in a chain-of-thought manner.Interestingly, this mechanism also endows the model with the capability to simulate program execution.Specifically, we enable the model to infer the results that would typically be produced by the Python interpreter, thereby deducing the answers to problems.From Table 7, we observe that even without relying on the Python interpreter, DotaMath was still able to achieve satisfactory results, demonstrating its accurate prediction of the Python interpreter's execution outcomes.While the ToRA model and DeepseekMATH-Instruct model are also trained using the output of the Python interpreter, these models struggle to make correct inferences in its absence.On the GSM8K, DotaMath-deepseek experience only an 8.5% decrease in accuracy without the use of tools, while both ToRA and ToRA-code see performance drops of over 80%, and DeepSeek-MATH-Instruct suffer a 52.3% loss.On the MATH dataset, DotaMATH experience a significant performance loss of 41.0%, while other models showe similar proportionate declines as observed on the GSM8K.The underlying reason may be attributed to models frequently invoking Python libraries such as sympy2 for complex calculations when solving intricate problems in MATH, resulting in a significantly increased difficulty in predicting code execution.This also highlights the necessity of utilizing tools when solving complex mathematical problems.</p>
<p>B.2 The Effectiveness of Data Augmentation and Format in MATH Sub-topics</p>
<p>Regarding data augmentation, we observe significant improvements in three subjects: Counting &amp; Prob., Geometry, and Int.Algebra, with increases over 16%, while the smallest enhancement is seen in Algebra at 3.4%.According to conclusions from Li et al., the stronger the base model's capability, the higher the accuracy demand for augmented data, and the more challenging it is to enhance performance.In Algebra, the model already reaches 78.4 without utilizing augmented data, which explains the minimal improvement.</p>
<p>Removing decomposition of thought, except Prealgebra, all other sub-topics are affected.In different subjects, removing display immediate process and self-correction has the most significant impact on Pre-calculus, indicating that the harder the problem, the more necessary it is to engage in self-correction (as they are more prone to errors).</p>
<p>B.3 The Effectiveness of Bug Filter</p>
<p>From Table 9, we can know that without python bug filter, the accuracy of DotaMath-deepseek drops from 86.7 to 85.5 on GSM8K and from 64.8 to 63.6 on MATH.</p>
<p>C Additional results</p>
<p>Our additional results are shown in</p>
<p>Generative Prompt</p>
<p>You are an exceptionally strong competitor in both math and programming contests, proficient in a wide range of mathematical knowledge and skilled in Python programming.Your command of Pre-algebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus is unparalleled.Your thinking is meticulous and profound, and the code you write always runs flawlessly and without error.Integrate step-by-step reasoning and Python code to solve math problems using the following guidelines:</p>
<ol>
<li>
<p>Break the problem into subtasks.</p>
</li>
<li>
<p>Write functions to solve the problem; the function should not take any arguments.</p>
</li>
<li>
<p>Print the results of every subtask in the Python code, using the intermediate variables in Python programs to represent intermediate results, refer to the example below.</p>
</li>
<li>
<p>When writing the python program, avoid using decimal.Utilize functions from sympy and other necessary Python library, and simplify all fractions and square roots without converting them to decimal values.</p>
</li>
</ol>
<p>5.</p>
<p>Print the final answer on the last line.</p>
<p>Here is an example you may refer to:
Problem: Let f (x) =    ax + 3, if x &gt; 2, x − 5, if − 2 ≤ x ≤ 2, 2x − b, if x &lt; −2.
Find a + b if the piecewise function is continuous (which means that its graph can be drawn without lifting your pencil from the paper).Solution: We can decompose this problem into following sub-tasks:</p>
<ol>
<li>Solve for a by equating ax + 3 to x − 5 at x = 2.We find that the sum of a and b is 0. This ensures the piecewise function is continuous across its entire domain.Therefore, the final answer is 0 .</li>
</ol>
<p>Correction Prompt</p>
<p>You are an exceptionally strong competitor in both math and programming contests, proficient in a wide range of mathematical knowledge and skilled in Python programming.Your command of Pre-algebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus is unparalleled.Your thinking is meticulous and profound, and the code you write always runs flawlessly and without error.You solve the problem with PLAN-CODE format.</p>
<p>Integrate step-by-step reasoning and Python code to solve math problems using the following guidelines:</p>
<ol>
<li>
<p>Break the problem into subtasks.</p>
</li>
<li>
<p>Write functions to solve the problem; the function should not take any arguments.</p>
</li>
<li>
<p>Print the results of every subtask in the Python code.</p>
</li>
<li>
<p>When writing the python program, avoid using decimal.Utilize functions from sympy and other necessary python library, and simplify all fractions and square roots without converting them to decimal values.</p>
</li>
</ol>
<p>5.</p>
<p>Print the numeric answer on the last line.</p>
<p>Here is an example you may refer to:
Problem: Let f (x) =    ax + 3, if x &gt; 2, x − 5, if − 2 ≤ x ≤ 2, 2x − b, if x &lt; −2.
Find a + b if the piecewise function is continuous (which means that its graph can be drawn without lifting your pencil from the paper).Solution: We can decompose this problem into the following sub-tasks:</p>
<ol>
<li>Solve for a by equating ax + 3 to x − 5 at x = 2. ``G iven a math problem, an incorrect solution, and you need to correct the incorrect solution.The response format is like: (short error interpretations)+(new solution).</li>
</ol>
<p>Explanatory Prompt</p>
<p>You are an exceptionally strong competitor in both math and programming contests, proficient in a wide range of mathematical knowledge and skilled in Python programming.Your command of Pre-algebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus is unparalleled.Your thinking is meticulous and profound, and the code you write always runs flawlessly and without error.Given a math problem, an incorrect solution and a correct solution, your task is to give the incorrect reason for the incorrect solution.Note that the reason is not allowed to be too long.You don't need to give the correct answer.You need to start your answer with: 'The solution is wrong since...'</p>
<p>E Case Study E.1 Singl-turn Successful cases</p>
<p>Single-trun successful case in GSM8K</p>
<p>Problem: Janet's ducks lay 16 eggs per day.She eats three for breakfast every morning and bakes muffins for her friends every day with four.She sells the remainder at the farmers' market daily for 2 per fresh duck egg.How much in dollars does she make every day at the farmers' market?Solution: To solve the problem of calculating Janet's daily earnings from selling fresh duck eggs at the farmers' market, we can follow these steps:</p>
<ol>
<li>
<p>Calculate the total number of eggs laid by the ducks each day.</p>
</li>
<li>
<p>Subtract the number of eggs Janet eats for breakfast and the number used in baking muffins to find the number of eggs available for sale.</p>
</li>
<li>
<p>Calculate the earnings from selling the remaining eggs at 2 per egg.Following these calculations, Janet makes 18 dollars every day at the farmers' market by selling fresh duck eggs.</p>
</li>
</ol>
<p>Single-trun successful case in MATH</p>
<p>Problem: Find all positive integer values of c such that the equation x 2 − 7x + c = 0 only has roots that are real and rational.Express them in decreasing order, separated by commas.Solution: For the roots to be real and rational, the discriminant must be a perfect square.Therefore, (−7) 2 − 4 • 1 • c = 49 − 4c must be a perfect square.The only positive perfect squares less than 49 are 1, 4, 9, 16, 25, and 36.The perfect squares that give an integer value of c are 1, 9, and 25.Thus, we follow these steps:</p>
<ol>
<li>Solve the equation 49 − 4c = 1 to find a value of c.Following these steps, the positive integer values of c that ensure the equation x 2 − 7x + c = 0 has real and rational roots by making the discriminant a perfect square, are 12, 10, and 6.Therefore, the final answer is 12, 10, 6 .</li>
</ol>
<p>E.2 Multi-turn Successful cases</p>
<p>Multi-turn successful case in GSM8K</p>
<p>Problem:India has 4 more than half the number of states in the USA.What's the total number of states in both countries together?Solution:To solve the problem, we can break it down into the following subtasks:</p>
<ol>
<li>
<p>Define the number of states in the USA.</p>
</li>
<li>
<p>Calculate the number of states in India based on the given relationship.</p>
</li>
<li>
<p>Sum the number of states in both countries.</p>
</li>
</ol>
<p>Let's implement this in Python using the sympy library to handle symbolic computation and ensure that all operations are exact.The error occurs because the variable states india is referenced before it is assigned a value in the equation.This is a typical scenario where the variable should be defined before it is used in an equation.Let's correct this by defining states india as a symbol and then using it in the equation.```python from sympy import symbols, Eq, solve def total_states(): # Define the number of states in the USA states_usa = symbols('states_usa') # Define the number of states in India as a symbol states_india = symbols('states_india') # Given that India has 4 more than half the number of states in the USA equation = Eq(states_india, 4 + 1/2 * states_usa) # Solve for the number of states in the USA (assuming the number of states # in the USA is known) For the sake of example, let's assume the USA has 50 states states_usa_value = 50 states_india_value = solve(equation.subs(states_usa,states_usa_value))[0] # Calculate the total number of states total = states_usa_value + states_india_value # Print the results of every subtask print(f"Number of states in the USA: {states_usa_value}") print(f"Number of states in India (4 more than half of the USA):</p>
<p>{states_india_value}") print(f"Total number of states in both countries: {total}") return total Call the function and print the final answer total_number_of_states = total_states() print(total_number_of_states) output Number of states in the USA: 50 Number of states in India (4 more than half of the USA): 29.0000000000000Total number of states in both countries: 79.0000000000000 79.0000000000000</p>
<p>The total number of states in both the USA and India, calculated by adding the number of states in the USA (50) to the number of states in India (29), is 79 .</p>
<p>Multi-turn successful case in MATH</p>
<p>Problem: A lattice point in the xy-plane is a point both of whose coordinates are integers (not necessarily positive).How many lattice points lie on the hyperbola x 2 − y 2 = 17?Solution: To find the number of lattice points on the hyperbola x 2 − y 2 = 17, we can follow these steps:</p>
<ol>
<li>
<p>Recognize that x 2 − y 2 = (x + y)(x − y).</p>
</li>
<li>
<p>Since x and y are integers, (x + y) and (x − y) must be factors of 17.</p>
</li>
<li>
<p>The number 17 is a prime number, so its only factors are 1 and 17 or −1 and −17.</p>
</li>
<li>
<p>We will set up equations for x + y = 17 and x − y = 1, and also for x + y = −17 and</p>
</li>
</ol>
<p>x − y = −1.</p>
<ol>
<li>
<p>Solve these equations to find the integer solutions for x and y.</p>
</li>
<li>
<p>Count the number of solutions to determine the number of lattice points on the hyperbola.</p>
</li>
</ol>
<p>from sympy import symbols, Eq, solve # Define symbolic variables x, y = symbols('x y')</p>
<h1>Define the equations based on the factors of 17 equations = [ (Eq(x + y, 17), Eq(x -y, 1)), (Eq(x + y, -17), Eq(x -y, -1)) ] # Function to solve the equations and count the lattice points def count_lattice_points(): lattice_points = [] for eq1, eq2 in equations: solutions = solve((eq1, eq2), (x, y)) lattice_points.append(solutions)print(f"Solving the equations {eq1} and {eq2} result: {solutions}.")return len(lattice_points) # Count the lattice points on the hyperbola number_of_lattice_points = count_lattice_points() print(number_of_lattice_points)</h1>
<p>Output:</p>
<p>Solving the equations Eq(x + y, 17) and Eq(x -y, 1) result: {x: 9, y: 8}.Solving the equations Eq(x + y, -17) and Eq(x -y, -1) result: {x: -9, y: -8}. 2</p>
<p>The solution is wrong since it fails to consider the negative factors of 17, which are also necessary to find all lattice points on the hyperbola.The correct approach should include both positive and negative factors of 17, leading to additional solutions.Let's correct the solution.To find the number of lattice points on the hyperbola x 2 − y 2 = 17, we can follow these steps:</p>
<ol>
<li>
<p>Recognize that x 2 − y 2 = (x + y)(x − y). 5.The solutions to the system of equations will give us the lattice points.</p>
</li>
<li>
<p>Count the number of unique lattice points obtained from the solutions.Output:</p>
</li>
</ol>
<p>The factors of 17 are: [(1, 17), (-1, -17), (17, 1), (-17, -1)] The lattice points on the hyperbola are: [{x: 9, y: -8}, {x: -9, y: 8}, {x: 9, y: 8}, {x: -9, y: -8}] 4</p>
<p>The code successfully finds all lattice points on the hyperbola x 2 − y 2 = 17 by considering all pairs of factors of 17 and solving the system of equations for each pair.Since the hyperbola equation can be factored into (x + y)(x − y) = 17, and 17 is a prime number, it only has the positive factors (1, 17) and (17, 1), and their negative counterparts (−1, −17) and (−17, −1).Solving these pairs as systems of equations yields the lattice points (9, −8), (−9, 8), (9, 8), and (−9, −8).Therefore, there are 4 lattice points on the hyperbola.</p>
<p>Find</p>
<p>the radius of the circle with equation  !+ 8 +  !− 6 = 0. 1. Complete the square for both  and  terms to rewrite the equation in the standard form of a circle's equation, ( − ℎ) !+( − ) !=  ! .2. Extract the value of  !from the standard.3. Compute the radius.from sympy import symbols, Eq, solve, sqrt x, y = symbols('x y') circle_eq = x<strong>2 + 8*x + y</strong>2 -6<em>y x_term = (x + 4)</em><em>2 -4</em><em>2, y_term = (y -3)</em><em>2 -3</em><em>2 standard_form = x_term + y_term standard_form_eq = Eq(standard_form, 0) r_squared = simplify(-standard_form_eq.rhs)radius = simplify(r_squared</em>*0.5)print(f"Standard form of the circle: (x + 4)^2 + (y -3)^2 = {r_squared}.")print(f"Squared radius is {r_squared}.")print(f"Radius of the circle is {radius}.")</p>
<ol>
<li>Complete the square for both  and  terms to rewrite the equation in the standard form of a circle's equation, ( − ℎ) !+( − ) !=  ! 2. Extract the value of  !from the standard form and then compute the radius.3. Calculate the radius of the circle.… # Move constants to the right side r_squared = 4<strong>2 + 3</strong>2 nprint(f"Extract the value of r^2 = {r_squared}\") r= sqrt(r_squared)</li>
</ol>
<p>Figure 1 :
1
Figure 1: An example shows how the DotaMath use decomposition and self-correction to solve a probem form MATH test set.For problem p, the model initially decomposes it into three subtasks and subsequently writes corresponding code.The Python interpreter then executes this code, producing three subresults.Upon identifying that the results are incorrect, the model elucidates the error's cause and undertakes a revised decomposition.Following this, new code is crafted and executed by the Python interpreter, leading to the correct resolution of the problem and presentation of the final solution.</p>
<p>Figure 2 :
2
Figure 2: Dataset construction for DotaMathQA.The detailed desciption are in Section 3.2.All prompts used are listed in Appendix D. Data Filter1 means retaining correct instances by comparing the results of the generated code from the Python interpreter with the ground truth answers in the dataset.Data Filter2 means filtering out instances where the generated code results in execution errors.</p>
<p>SetupDatasets.The models are evaluated on two indomain datasets, GSM8K(Cobbe et al., 2021)   andMATH (Hendrycks et al., 2021), as wel as four out-of-domain datasets: Mathematics(Saxton et al., 2019),SVAMP (Patel et al., 2021),  TabMWP (Lu et al., 2023), and ASDiv (Miao et al.,  2020).These six datasets encompass a wide range of difficulty levels, from grade school math word problems to challenging competition mathematics problems, covering multiple domains including Algebra, Number Theory, Counting and Probability, Geometry, and Precalculus.They include various types of questions, such as tabular-based, free-form, and multiple-choice, offering a comprehensive assessment of the model's mathematical reasoning capabilities.</p>
<p>DFigure 3 :
3
Figure 3: The comparison of DeepSeekMath-Base fine-tuned with D DotaMathQA and D DotaMathQA-w/o-aug on GSM8K and MATH.</p>
<p>2.</p>
<p>Solve for b by equating x − 5 to 2x − b at x = −2.3. Add the values of a and b together to find the sum.```python from sympy import symbols, Eq, solve def sum_a_and_b(): a = symbols('a') b = symbols('b') equation1 = Eq(a * 2 + 3, 2 -5) equation2 = Eq(-2 -5, 2<em>(-2) -b) solution_a = solve(equation1, a) solution_b = solve(equation2, b) sum_ab = solution_a[0] + solution_b[0] # print the results of every subtask print(f"Equating the function at x = 2 gives us the equation {equation1}.")print(f"Solving this equation gives us the value of a: a = {solution_a[0]}.")print(f"Equating the function at x = -2 gives us the equation {equation2}.")print(f"Solving this equation gives us the value of b: b = {solution_b[0]}.")print(f"hence, a + b equals to {solution_a[0]}+{solution_b[0]} = {sum_ab}.atx = 2 gives us the equation Eq(2</em>a + 3, -3).Solving this equation gives us the value of a: a = -3.Equating the function at x = -2 gives us the equation Eq(-7, -b -4).Solving this equation gives us the value of b: b = 3. hence, a + b equals to -3+3 = 0. 0</p>
<p>2.</p>
<p>Solve for b by equating x − 5 to 2x − b at x = −2.3. Add the values of a and b together to find the sum.results of every subtask print(f"Equating the function at x = 2 gives us the equation {equation1}.")print(f"Solving this equation gives us the value of a: a = {solution_a[0]}.")print(f"Equating the function at x = -2 gives us the equation {equation2}.")print(f"Solving this equation gives us the value of b: b = {solution_b[0]}.")print(f"hence, a + b equals to {solution_a[0]}+{solution_b[0]} = {sum_ab}.")</p>
<p>number of eggs available for sale eggs_for_sale = total_eggs_per_day -(eggs_for_breakfast + eggs_for_muffins) # Calculate the earnings from selling the eggs daily_earnings = eggs_for_sale * price_per_egg # Print the results of every subtask print(f"Total number of eggs laid per day: {total_eggs_per_day}") print(f"Number of eggs eaten for breakfast: {eggs_for_breakfast}") print(f"Number of eggs used in baking muffins: {eggs_for_muffins}") print(f"Number of eggs available for sale: {eggs_for_sale}") print(f"Daily earnings from selling eggs: ${daily_earnings}") eggs laid per day: 16 Number of eggs eaten for breakfast: 3 Number of eggs used in baking muffins: 4 Number of eggs available for sale: 9 Daily earnings from selling eggs: $18</p>
<p>2.</p>
<p>Solve the equation 49 − 4c = 9 to find another value of c. 3. Solve the equation 49 − 4c = 25 to find the last value of c. ```python from sympy import symbols, solve, sqrt def find_c_values(): c = symbols('c') discriminant = (-7)**2 -41c # Compute the discriminant # List to store valid c values valid_c_values = [] # Check for each c if the discriminant is a perfect square for c_value in range(1, 100): # Assuming a reasonable upper limit for c disc_value = discriminant.subs(c,c_value) if sqrt(disc_value).is_integer():valid_c_values.append(c_value)print(f"For c = {c_value}, the discriminant is {disc_value}.")# Print the valid c values in decreasing order valid_c_values.sort(reverse=True)print("Valid c values in decreasing order:", ", ".join(map(str, valid_c_values)12, the discriminant is 1.Valid c values in decreasing order: 12, 10, 6</p>
<p>2.</p>
<p>Since x and y are integers, (x + y) and (x − y) must be factors of 17. 3. List all pairs of factors of 17, which are (1, 17) and (−1, −17), and their reverses.4. For each pair of factors, solve the system of equations (x + y = a) and (x − y = b) where (a, b) is a pair of factors.</p>
<p>from sympy import symbols, Eq, solve # Define symbolic variables x, y = symbols('x y') # Define the factors of 17 factors = [(1, 17), (-1, -17), (17, 1), (-17, -1)] # Function to find lattice points def find_lattice_points(factors): lattice_points = [] for a, b in factors: # System of equations based on the factors eq1 = Eq(x + y, a) eq2 = Eq(x -y, b) # Solve the system of equations solution = solve((eq1, eq2), (x, y)) # Add the solution to the list of lattice points lattice_points.append(solution)return lattice_points # Find the lattice points lattice_points = find_lattice_points(factors) # Print the detailed reasoning process print(f"The factors of 17 are: {factors}") print(f"The lattice points on the hyperbola are: {lattice_points}") # Print the final answer print(len(lattice_points))</p>
<p>i) Generating Seed Single-turn QA Generative Prompt Query
SeedSingle-turnQAGPT-4</p>
<p>ii) Generating Augmented Single-turn QA Generative Prompt
IncorrectAnswer1CorrectAnswer1Answer withAugmentedcode bugSingle-turnAnswer withoutQAcode bug</p>
<p>iii) Generating Automatic Multi-turn QA Corrective Prompt Automatic Multi-turn QA iv) Generating Rule-based Multi-turn QA
Data Filter1𝑫 𝐬𝐞𝐞𝐝$𝐬𝐢𝐧𝐠𝐥𝐞AugmentationQueryPrompt Augmented QueryGPT-4Data Filter2GPT-4𝑫 𝐚𝐮𝐠$𝐬𝐢𝐧𝐠𝐥𝐞IncorrectIncorrect Answer1 &amp; QueryGPT-4Data Filter1Answer2 Answer2 Correct𝑫 𝐚𝐮𝐭𝐨$𝐦𝐮𝐥𝐭𝐢Query &amp;Interpreted PromptError Rationale1,2Rule-based Multi-turnIncorrect andGPT-4QACorrect Answer1,2𝑫 𝐫𝐮𝐥𝐞$𝐦𝐮𝐥𝐭𝐢</p>
<p>Table 1 :
1
Dataset Statistics and Comparison.</p>
<p>D seed-single Construction.Leveraging the powerful instruction-following capability of GPT-4, we prompt it to generate solutions in the desired Dota-Math format for queries from the GSM8K and MATH training datasets.This is achieved by manually writing a single query-response demonstration derived from the MATH training set to guide GPT-4 in producing the corresponding data format, as outlined in the generative prompt in Appendix D.</p>
<p>Table 2 :
2
Comparison of DotaMath with open-source and proprietary source LLMs on in-domain and out-of-domain benchmarks.The base model of open-source LLMs above DotaMath-LLaMA2-7B are LLaMA2.The average score is derived from a weighted average of scores across benchmarks, with weights the proportion of questions in each benchmark.The best results are highlighted in bold, and the second-best results are underlined.
ModelSize Use Tool?In-domainOut-of-domainGSM8K MATH Mathematics SVAMP TabMWP ASDiv AverageProprietary ModelClaude-3 Opus (Anthropic, 2024)-✕95.060.1-----GPT-4(original version) (OpenAI, 2023)-✕92.042.5-93.167.191.3-GPT-4 Code Interpreter (Zhou et al., 2023)-✓97.069.7-----GPT-4 (PAL) (Gou et al., 2023)-✓94.251.894.895.992.6-Open-Source ModelMetaMATH (Yu et al., 2023)70B✕82.326.6-85.863.484.0-MuggleMATH (Li et al., 2023a)70B✕82.736.334.683.459.781.857.8MAmmoTH (Yue et al., 2023)70B✓76.941.855.682.438.270.248.0ToRA (Gou et al., 2023)70B✓84.349.772.682.774.086.870.5MathGenieLM (Lu et al., 2024)70B✓88.451.276.087.7---ToRA (Gou et al., 2023)7B✓68.840.158.368.242.473.949.0MathGenieLM (Lu et al., 2024)7B✓71.733.065.078.5---DotaMath-LLaMA2-7B7B✓79.650.165.084.264.182.364.7MAmmoTH2-8B8B✕70.435.8-----DotaMath-LLaMA3-8B8B✓84.258.974.288.370.485.171.1ToRA-llema (Gou et al., 2023)7B✓74.849.5-76.063.582.3-MARIO-llema-7B (Liao et al., 2024)7B✓70.146.3-----DotaMath-llemma-7B7B✓81.257.776.887.877.785.174.3DeepSeek-MATH-Instruct (Shao et al., 2024) 7B✓83.757.482.885.779.786.675.7KPMath-Plus-deepseek (Huang et al., 2024)7B✕83.948.8-81.578.788.9-MARIO-deepseek (Liao et al., 2024)7B✓78.456.1-----DotaMath-deepseek-7B7B✓86.764.879.189.584.288.580.1</p>
<p>Table 4 :
4
Sub-levels performance of different models on MATH.Level5 is hardest.The most significant change appears in green .The number of questions from Level 1 to Level 5 are437, 894, 1131, 1214, and 1324, respectively.
ModelLevel 1Level 2Level 3Level 4Level 5D DotaMathQA91.3 (↑ 4.5%) 80.5 (↑ 6.3%) 74.7 (↑ 11.8%) 59.3 (↑ 8.0%) 41.9 (↑ 18.7%)D DotaMathQA-w/o-aug87.475.766.854.935.3D DotaMathQA-w/o-aug-w/o-dot85.1 (↓ 2.6%) 72.1 (↓ 4.8%) 65.4 (↓ 2.1%) 49.6 (↓ 9.7%) 31.6 (↓ 10.5%)D DotaMathQA-w/o-aug-w/o-inter 85.8 (↓ 1.8%) 73.3 (↓ 3.2%) 66.6 (↓ 0.3%) 53.7 (↓ 2.2%) 33.3 (↓ 5.7%)D DotaMathQA-w/o-aug-w/o-multi 86.7 (↓ 0.8%) 73.9 (↓ 2.4%) 64.3 (↓ 3.7%) 52.4 (↓ 4.6%) 32.8 (↓ 7.1%)Training SetGSM8K MATHD seed-single81.356.7D seed-single +D auto-multi (2K)81.858.4D seed-single +D rule-multi (10K)82.358.6D seed-single +D rule-multi (2K)81.657.5D seed-single +D multi82.859.0</p>
<p>Table 5 :
5
Effectiveness of different self-correction data.
this
corresponding performance decrease is 1.1% on GSM8K and 2.7% on MATH, illustrating the effectiveness of displaying intermediate processes.To examine the role of error correction, we remove D multi from D DotaMathQA-w/o-aug to get D DotaMathQA-w/o-aug-w/o-multi .Following</p>
<p>Table 6 :
6
Self-annotation vs. GPT-4 annotation.</p>
<p>Table 6 to 10.
ModelGSM8KMATHwith toolwithout toolwith toolwithout toolToRA-7B68.89.6(↓ 86.0%)40.17.4(↓ 81.5%)ToRA-code-7B72.611.0(↓ 84.8%)44.68.4(↓ 81.2%)DeepSeek-MATH-Instruct83.739.9(↓ 52.3%)57.424.1(↓ 58.0%)DotaMath-deepseek86.779.3 (↓ 8.5%)64.838.2(↓ 41.0%)</p>
<p>Table 7 :
7
The performance comparisons of DotaMath-deepseek and other models without access to Python interpreter.
ModelAlgebraCounting &amp; Prob.GeometryInt. AlgebraNumber TheoryPrealgebraPrecalculusD DotaMathQA81.1 (↑ 3.4%)63.5 (↑ 17.6%)47.4 (↑ 16.5%) 44.0 (↑ 18.0%) 76.7 (↑ 13.8%) 78.3 (↑ 8.4%) 46.7 (↑ 7.1%)D DotaMathQA-w/o-aug78.454.040.737.367.472.243.6D DotaMathQA-w/o-aug-w/o-dot71.5 (↓ 8.8%)48.5 (↓ 10.2%)38.8 (↓ 4.7%)35.4 (↓ 5.1%)61.1 (↓ 9.3%)73.5 (↑ 1.8%) 40.7 (↓ 6.7%)D DotaMathQA-w/o-aug-w/o-inter 74.4 (↓ 5.0%)51.5 (↓ 4.6%)40.9 (↑ 0.5%)37.8 (↑ 1.3%)62.6 (↓ 7.1%)74.4 (↑ 3.0%) 40.3 (↓ 7.6%)D DotaMathQA-w/o-aug-w/o-multi 73.8 (↓ 5.9%)54.4 (↑ 0.7%)41.1 (↑ 1.0%)36.5 (↓ 2.1%)63.7 (↓ 5.5%)71.6 (↓ 0.8%) 38.1 (↓ 12.6%)</p>
<p>Table 8 :
8
Sub-topics performance of different models on MATH.
GSM8K MATH</p>
<p>Table 9 :
9
The effectiveness of data filtering.
ModelSize Use Tool?In-domainOut-of-domainGSM8K MATH Mathematics SVAMP TabMWP ASDiv Averageproprietary ModelClaude-2 (Anthropic., 2023)-✕85.232.5-----PaLM-2 (Anil et al., 2023b)-✕80.734.3-----Open-Source ModelQwen-1.5-110B (Alibaba, 2023)110B✕85.449.4-86.2-85.1-LLaMA-2 RFT (Yuan et al., 2023)7B✕51.2------MAmmoTH-Coder (Yue et al., 2023)34B✓72.743.665.485.3---MATHCoder-CL (Wang et al., 2023a)34B✓81.745.275.9----ToRA-CODE (Gou et al., 2023)34B✓80.750.877.980.570.584.268.7WizardMATH (Luo et al., 2023a)70B✕81.622.7-80.049.876.2-MAmmoTH2-8x7B-Plus (Yue et al., 2024b) 56B✕86.447.0-----MATHCoder-L (Wang et al., 2023a)70B✓83.945.174.484.9---Platypus-2 (Lee et al., 2023)70B✕45.915.0-74.347.372.7-DotaMath-LLaMA2-7B7B✓79.650.165.084.264.182.364.7DotaMath-LLaMA3-8B8B✓84.258.974.288.370.485.171.1DotaMath-llemma-7B7B✓81.257.776.887.877.785.174.3DotaMath-deepseek-7B7B✓86.764.879.189.584.288.580.1</p>
<p>Table 10 :
10
Additional comparison of DotaMath with open-source and proprietary source LLMs on in-domain and out-of-domain benchmarks.
D Prompt TemplatesOur prompt templates are shown below:Augmentation PromptI want you to act as a math teacher. You should think of some ways to help students do variationtraining for challenging competition mathematics problems.Here are some ways you can refer: Introduce fractions or percentages, Combine multiple concepts,Include a conditional statement, Increase the complexity of the problem and so on. Response withspecific format, like:Introduce fractions or percentages: ##1 new question1 ##1Combine multiple concepts: ##2 new question2 ##2...Increase the complexity of the problem: ##10: new question10 ##10The nth problem must be strictly limited to between ##n and ##n for our subsequent regularextraction.Now you are give a math problem, think for 10 different ways.Given new problem:{Query}System Prompt&lt;|user|&gt;:{Query}&lt;|assistant|&gt;:
In this paper, all data generated by GPT-4 are derived from the gpt-4-turbo-2024-04-09 api.
taka Matsuo, and Yusuke Iwasawa.2022.Large language models are zero-shot reasoners.In Advances in Neural Information Processing Systems.
Alibaba, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Lakshman Tamara Von Glehn, Mehran Yagati, Lucas Kazemi, Misha Gonzalez, Jakub Khalman, Sygnowski, and et al. 2023a. Gemini: A family of highly capable multimodal models</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, arXiv:2305.10403Zhifeng Chen, et al. 2023b. Palm 2 technical report. arXiv preprint</p>
<p>Model card and evaluations for claude models. Anthropic, 2023</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, Llemma: An open language model for mathematics. 2024</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T J Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, 2020Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordIlya Sutskever, and Dario Amodei</p>
<p>Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma Gongque, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, arXiv:2407.01284We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint. 2024</p>
<p>Making language models better tool learners with execution feedback. Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, Ningyu Zhang, arXiv:2305.130682023arXiv preprint</p>
<p>Analysing mathematical reasoning abilities of neural models. David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli, 2019In ICLR 2019</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, 2024CoRR</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, 2023CoRR</p>
<p>Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, arXiv:2406.08587Cs-bench: A comprehensive benchmark for large language models towards computer science mastery. 2024arXiv preprint</p>
<p>Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren, Is chatgpt good at search? investigating large language models as re-ranking agents. 2023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, ; Jian, Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</p>
<p>Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li, 2023aCoRR</p>
<p>Generative AI for math: Part I -mathpile: A billion-tokenscale pretraining corpus for math. Zengzhi Wang, Rui Xia, Pengfei Liu, 2023bCoRR</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022. 2022</p>
<p>. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian</p>
<p>Jianhong Yang, Jianwei Tu, Jianxin Zhang, Jin Ma, Jingren Xu, Jinze Zhou, Jinzheng Bai, Junyang He, Kai Lin, Keming Dang, Keqin Lu, Kexin Chen, Mei Yang, Mingfeng Li, Na Xue, Pei Ni, Peng Zhang, Ru Wang, Rui Peng, Ruize Men, Runji Gao, Shijie Lin, Shuai Wang, Sinan Bai, Tianhang Tan, Tianhao Zhu, Tianyu Li, Wenbin Liu, Xiaodong Ge, Xiaohuan Deng, Xingzhang Zhou, Xinyu Ren, Xipin Zhang, Xuancheng Wei, Yang Ren, Yang Fan, Yichang Yao, Yu Zhang, Yunfei Wan, Yuqiong Chu, Zeyu Liu, Zhenru Cui, Zhihao Zhang, Fan, arXiv:2407.10671Qwen2 technical report. 2024arXiv preprint</p>
<p>Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Internlmmath: Open math large language models toward verifiable reasoning. Xipeng Qiu, Jiayu Wang, Kai Chen, Dahua Lin, 2024</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, Metamath: Bootstrap your own mathematical questions for large language models. 2023</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, Chang Zhou, arXiv:2308.01825Scaling relationship on learning mathematical reasoning with large language models. 2023arXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653MAmmoTH: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Xiang Yue, Tuney Zheng, Ge Zhang, Wenhu Chen, Mammoth2: Scaling instructions from the web. 2024a</p>
<p>Mammoth2: Scaling instructions from the web. Xiang Yue, Tuney Zheng, Ge Zhang, Wenhu Chen, CoRR, abs/2405.035482024b</p>
<p>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang, 2024</p>
<p>Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li, 2023CoRR</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-658 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-658</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-658</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-cf4aa38ae31b43fd07abe13b4ffdb265babb7be1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cf4aa38ae31b43fd07abe13b4ffdb265babb7be1" target="_blank">The Curious Case of Neural Text Degeneration</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.</p>
                <p><strong>Paper Abstract:</strong> Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. 
In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e658.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e658.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nucleus (top-p) Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nucleus Sampling (top-p)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic stochastic decoding method that truncates the model distribution to the smallest set of tokens whose cumulative probability >= p, then re-normalizes and samples from that nucleus; proposed in this paper to avoid sampling the unreliable tail and reduce degeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (Generative Pre-trained Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>762M</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Open-ended conditional text continuation / long-form text generation (generate continuations given 1-40 token contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Stochastic sampling randomness; the nucleus threshold p (value of p controls candidate set size); local shape of the model's predicted distribution (flat vs. peaked) which makes nucleus size expand/contract; input context variability (different contexts produce different nucleus sizes); renormalization factor p' varying by time-step.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Perplexity (of generated text under the model), Self-BLEU (diversity), Zipf coefficient (vocabulary distribution), repetition percentage (repetition rate), HUSE (combined human+statistical score); also observed variance in likelihood / likelihood distribution across generated tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Nucleus (p=0.95) produced perplexity 13.13, Self-BLEU 0.32, Zipf coefficient 0.95, Repetition 0.36%, HUSE 0.97 (Table 1). Nucleus dynamically produced candidate pools ranging from ~1 to ~1000 tokens depending on context (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of distributional statistics (perplexity, Self-BLEU, Zipf, repetition %) across decoding methods and parameter sweeps; HUSE combining human judgments and model log-probability via a KNN discriminator (k=13).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Nucleus Sampling (p=0.95) matched human perplexity most closely among tested methods and achieved the highest HUSE (0.97), indicating reproducible performance across the evaluated metrics and human judgments; outperforming pure sampling (perplexity 22.73, HUSE 0.67) and typical top-k settings unless k is very large.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Truncation causes zero-probability mass for most tokens which biased some evaluation (HUSE) until the authors interpolated 0.1 mass back to the original distribution for evaluation; sensitivity to choice of p (although less fragile than fixed k); distributional variability caused by context-dependent nucleus size.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use of Nucleus (top-p) Sampling to truncate unreliable tail; tuning p to match human perplexity; interpolation (mixing a small mass of original distribution back) for evaluation to avoid truncation artifact in HUSE; general recommendation to truncate model tail instead of sampling full distribution or using fixed top-k.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantitative improvements vs baselines: reduced perplexity from pure sampling (22.73) to 13.13, increased HUSE from 0.67 to 0.97, maintained Self-BLEU and Zipf close to human values; reduced repetition compared to greed/beam (repetition 0.36% vs greedy 73.66% and beam b=16 28.94%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>5,000 generated passages (overall generation experiments); Self-BLEU computed on 1,000 sampled generations compared to other 4,999; HUSE computed on 200 generations with 20 human annotations each (4,000 annotations per decoding method).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Nucleus Sampling (top-p) is an effective mitigation of sampling-induced variability: by truncating the unreliable tail dynamically it produces text whose perplexity, diversity (Self-BLEU/Zipf), and human-judged quality (HUSE=0.97) closely match human text while avoiding repetition and incoherence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Curious Case of Neural Text Degeneration', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e658.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e658.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-k Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-k Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A truncation-based stochastic decoding method that samples the next token from the fixed set of k most probable tokens (renormalized); widely used but shown here to be suboptimal because a fixed k cannot adapt to context-dependent distribution shape.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (Generative Pre-trained Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>762M</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Open-ended conditional text continuation</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Fixed k parameter (choice of k), stochastic sampling randomness, interaction with temperature when combined, renormalization factor p' that varies by time-step, context-dependent distribution shape (flat vs peaked).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Perplexity, Self-BLEU, Zipf coefficient, repetition %, HUSE; observed variance in likelihood for high-k settings.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Examples from Table 1: Top-k k=40: perplexity 6.88, Self-BLEU 0.39, Zipf 0.96, repetition 0.78%, HUSE 0.19. Top-k k=640: perplexity 13.82, Self-BLEU 0.32, Zipf 0.96, repetition 0.28%, HUSE 0.94. Top-k k=40,t=0.7: perplexity 3.48, Self-BLEU 0.44, Zipf 1.00, repetition 8.86%, HUSE 0.08. High k (e.g. 640) can match human-like perplexity but leads to high variance/coherency issues unless tuned carefully.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Same distributional statistics and HUSE comparisons across k sweep; qualitative analysis of variance and incoherence at high k.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Top-k can approximate human statistics if k is large (k=640 achieved perplexity 13.82 and HUSE 0.94) but requires careful tuning and still shows more variability and incoherence than Nucleus Sampling; small k leads to overly generic/bland text.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>A single global k is sub-optimal across contexts: when head distribution is flat k must be large to include reasonable options, but large k in peaked contexts includes inappropriate low-probability tokens (renormalization inflates them), producing incoherence and high variance; renormalization p' varies wildly each time-step.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Tune k (possibly large) and/or combine with temperature; but authors recommend dynamic truncation (top-p) instead of fixed k for better trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Large k (e.g., 640) improved metrics (perplexity 13.82, HUSE 0.94) approaching Nucleus, but at the cost of observed higher variance/incoherence and sensitivity; smaller k reduces diversity and leads to bland text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Same experimental setup: 5,000 generated passages overall; specific parameter sweeps reported in Table 1; Self-BLEU computation used 1,000 sampled generations for diversity estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Top-k sampling is a useful truncation method but is brittle because a fixed k cannot adapt to the per-step distribution shape; matching human distribution often requires large k which increases the variance/coherency problems compared to Nucleus (top-p).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Curious Case of Neural Text Degeneration', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e658.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e658.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling with temperature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that rescales the logits by temperature t before softmax (u / t), where lower t sharpens the distribution towards high-probability tokens and higher t flattens it; used to trade off diversity vs coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (Generative Pre-trained Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>762M</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Open-ended conditional text continuation</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Temperature parameter t, interaction with truncation (top-k or top-p), stochastic sampling randomness, context-dependent distribution shape.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Perplexity, Self-BLEU, Zipf, repetition %; qualitative coherence assessments and HUSE.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Sampling with t=0.9: perplexity 10.25, Self-BLEU 0.35, Zipf 0.96, repetition 0.66%, HUSE 0.79 (Table 1). Lower temperatures (e.g., t=0.7 when combined with top-k) reduce tail sampling but increase repetition and decrease diversity (example top-k k=40,t=0.7 shows high repetition and low HUSE).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Parameter sweeps over t and combined methods; compared distributional statistics and HUSE across temperature settings.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Lowering temperature improves apparent coherence (by suppressing tail) but at cost of reduced diversity and increased repetition; intermediate t (e.g., 0.9) improves over pure sampling but still inferior to Nucleus (p=0.95) on combined metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Trade-off between reducing tail noise (low t) and maintaining diversity; low temperature can mimic greedy behavior and cause repetition; effects depend on interaction with truncation strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Tune temperature, and/or combine temperature shaping with truncation (top-k or top-p); authors favor dynamic top-p truncation over relying solely on temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Temperature 0.9 gave improved perplexity (10.25) and HUSE (0.79) vs pure sampling; but did not reach Nucleus performance (HUSE 0.97) and lower temperatures increased repetition.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Reported within same experiments: 5,000 passages overall; specific parameter settings presented in Table 1 and figures.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Temperature controls the diversity/coherence tradeoff but cannot by itself robustly avoid tail-induced incoherence; combining with truncation helps, but dynamic top-p truncation is a superior control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Curious Case of Neural Text Degeneration', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e658.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e658.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Beam Search (maximization)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam Search (maximization-based decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deterministic search-based decoding strategy that seeks high-likelihood sequences by keeping top-b hypotheses at each step; shown to produce degenerate outputs (generic, repetitive) in open-ended generation despite low perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (Generative Pre-trained Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>762M</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Open-ended conditional text continuation</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Beam width b (controls search breadth), search objective (maximizing probability), interaction with model's per-token probabilities, average generated length (increases/decreases with b) which affects repetition measures.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Perplexity (very low), repetition % (high), distinct n-grams, rank-frequency distributions; qualitative human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Greedy: perplexity 1.50, Self-BLEU 0.50, Zipf 1.00, Repetition 73.66%. Beam b=16: perplexity 1.48, Self-BLEU 0.44, Zipf 0.94, Repetition 28.94% (Table 1). Beam outputs have unnaturally low perplexity and high repetition relative to human text.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison across beam widths b; n-gram distinct counts vs human; perplexity and repetition measures.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Beam search consistently produced low-perplexity, highly repetitive outputs across beam widths; increasing beam width often shortened average output length which partly reduced measured repetition but did not yield human-like diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Maximization objective is intrinsically misaligned with open-ended human text: model assigns very high probability to repetitive/generic continuations; search completeness (larger beams) does not fix this and can cause shorter outputs; beam degeneracy is robust across contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Diverse beam variants and diversity scoring functions are discussed in related work (not primary solutions here); authors advocate stochastic truncation (top-p) instead.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Diverse beam methods are cited but not experimentally shown superior here; beam search remained inferior on measured metrics compared to stochastic truncated sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Beam results reported within the same 5,000 generation experiment and analyzed across beam-width sweeps (e.g., Figure 10 shows trigram counts vs beam width).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Maximization via beam search leads to degenerate, low-perplexity but low-quality outputs (repetition/genericness) in open-ended generation; search improvements alone do not resolve the mismatch between model per-token objectives and human-like text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Curious Case of Neural Text Degeneration', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e658.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e658.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pure Sampling (full distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pure Sampling (sampling from full model distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sampling directly from the full softmax output at each timestep (no truncation or temperature), which preserves the model's full stochasticity but leads here to incoherent outputs due to over-sampling of an 'unreliable tail'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (Generative Pre-trained Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>762M</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Open-ended conditional text continuation</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Intrinsic stochastic sampling randomness, large and unreliable low-probability tail of the distribution (many rare tokens), context-dependent distribution shape leading to sampling many unlikely tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Perplexity, Self-BLEU, Zipf coefficient, repetition %, HUSE, and qualitative coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Pure sampling: perplexity 22.73 (worse than human 12.38), Self-BLEU 0.28, Zipf 0.93, Repetition 0.22%, HUSE 0.67 (Table 1); pure sampling overestimates rare words and produces incoherent text.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Distributional statistics across generated samples and comparison to gold text; perplexity of generated text under the same model.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Pure sampling produced higher perplexity than human references and qualitatively incoherent outputs, indicating poor recovery of human-like distribution despite preserving stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>The unreliable tail is over-represented in aggregate when sampling without truncation, leading to contexts that are hard to recover from and causing incoherence; stochasticity alone is not sufficient without truncation control.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Truncation via Top-k or Nucleus Sampling; lowering temperature to reduce tail mass (but with tradeoffs).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Truncation (Nucleus p=0.95) reduced perplexity from 22.73 to 13.13 and increased HUSE from 0.67 to 0.97; lowering temperature helped somewhat but reduced diversity and could increase repetition.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Included in the 5,000-passage generation experiments; distributional statistics computed across these samples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling from the full model distribution preserves stochasticity but over-samples an unreliable tail, producing incoherent text and higher perplexity â€” truncation (top-p) is needed to control this source of variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Curious Case of Neural Text Degeneration', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e658.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e658.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HUSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Unified with Statistical Evaluation (HUSE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A combined evaluation method that trains a discriminator to distinguish human vs model text using two features: the model-assigned probability and human judgements of typicality, thereby jointly measuring quality and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unifying human and statistical evaluation for natural language generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (Generative Pre-trained Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>762M</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language generation / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Human + statistical evaluation of generated text (assessing overall quality and diversity of decoding strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Truncation of distributions (top-k / top-p) produces zero-probability mass that biased HUSE; human annotation variability (typicality judgments); model log-probabilities vary across decoding methods and truncation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>HUSE score computed with a KNN classifier (k=13) using features: model log-probability and aggregated human typicality judgments; additional smoothing/interpolation applied for truncated distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Reported HUSE scores in Table 1: Nucleus p=0.95 -> 0.97 (best), Top-k=640 -> 0.94, Sampling t=0.9 -> 0.79, Pure Sampling -> 0.67, Top-k=40 -> 0.19. Truncation artifacts caused near-zero HUSE initially until interpolation (mixing 0.1 mass of original distribution) was applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>KNN discrimination accuracy as HUSE score; uses 200 generations annotated (20 annotators each) resulting in 4,000 human annotations per decoding method.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>After interpolation smoothing, HUSE effectively discriminated methods and ranked Nucleus highest; interpolation was necessary to avoid evaluation artifacts from truncation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Truncation to zero-probability mass breaks HUSE's reliance on model log-probabilities; human annotation noise and choice of interpolation magnitude affect HUSE results.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Interpolating 0.1 mass of the original (un-truncated) distribution back into truncated distributions when computing HUSE to avoid zero-probability distortions; using KNN (k=13) on combined features.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Interpolation allowed top-k and top-p methods to receive meaningful HUSE scores (rather than near-zero) and enabled comparison; using this produced the ranking shown in Table 1 where Nucleus scored highest (0.97).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>HUSE computed on 200 generations per decoding algorithm annotated by 20 annotators each (4,000 annotations per decoding method); KNN classifier with k=13.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>HUSE, when adjusted for truncation artifacts via interpolation, provides a useful combined metric showing that Nucleus Sampling achieves the best joint trade-off of quality and diversity among evaluated decoding strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Curious Case of Neural Text Degeneration', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e658.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e658.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unreliable tail / distribution shape</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unreliable tail of the model's predicted probability distribution (and flat vs peaked head shapes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies an 'unreliable tail' of many low-probability candidate tokens which, when sampled from (pure sampling) or included (large top-k), leads to incoherence, and notes that per-step distribution shapes vary (flat heads vs peaked heads) causing sensitivity to truncation choices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (Generative Pre-trained Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>762M</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language generation / probabilistic modeling</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Analysis of decoding behavior and distributional properties of model predictions in open-ended generation</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Many low-probability tokens in the tail (unreliable tail), context-dependent head shape (flat vs peaked), renormalization effects when truncating, sampling randomness that over-represents tail tokens in aggregate, and positive feedback loops that increase probability of repeated phrases.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Perplexity of generated text under the model, Zipf/rank-frequency plots, histogram/variance of per-token probabilities (visualized), repetition probability as repetition grows (Figure 4), nucleus size (qualitative counts).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Evidence: pure sampling perplexity 22.73 (worse than human 12.38) indicating tail sampling harms recovery of human distribution; authors observe nucleus sizes typically range from ~1 to ~1000 candidates depending on context; Figure 4 shows repetition probabilities increase with each repetition (positive feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Distributional comparisons (rank-frequency, perplexity), experiments across decoding strategies showing the tail's impact (pure sampling vs truncation).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Truncation removing the tail (top-p or tuned top-k) materially improves perplexity and human judgments relative to sampling the full distribution; sampling from the tail produces high variance and incoherence across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>The tail is context-dependent and large (tens of thousands of low-probability tokens), making fixed truncation brittle; measuring and correcting for tail effects requires dynamic methods or careful calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Dynamic top-p truncation (Nucleus Sampling) to exclude the unreliable tail; tuning p to match human perplexity; temperature reduction and top-k are alternative mitigations but have trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Nucleus p=0.95 reduced negative tail effects relative to pure sampling (perplexity 13.13 vs 22.73) and achieved strong human-evaluated quality (HUSE 0.97); temperature can reduce tail but at the cost of diversity and potential repetition.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Observations derived from the same 5,000-generation experiments and parameter sweeps reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The unreliable tail and per-step distribution shape are primary sources of stochastic variability and incoherence; dynamically truncating the tail (top-p) is an effective control that mitigates these problems while preserving human-like diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Curious Case of Neural Text Degeneration', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are unsupervised multitask learners <em>(Rating: 2)</em></li>
                <li>Hierarchical neural story generation <em>(Rating: 1)</em></li>
                <li>Unifying human and statistical evaluation for natural language generation <em>(Rating: 2)</em></li>
                <li>Learning to write with cooperative discriminators <em>(Rating: 1)</em></li>
                <li>Neural text generation with unlikelihood training <em>(Rating: 2)</em></li>
                <li>Texygen: A benchmarking platform for text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-658",
    "paper_id": "paper-cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Nucleus (top-p) Sampling",
            "name_full": "Nucleus Sampling (top-p)",
            "brief_description": "A dynamic stochastic decoding method that truncates the model distribution to the smallest set of tokens whose cumulative probability &gt;= p, then re-normalizes and samples from that nucleus; proposed in this paper to avoid sampling the unreliable tail and reduce degeneration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (Generative Pre-trained Transformer 2)",
            "model_size": "762M",
            "scientific_domain": "Natural language generation / NLP",
            "experimental_task": "Open-ended conditional text continuation / long-form text generation (generate continuations given 1-40 token contexts)",
            "variability_sources": "Stochastic sampling randomness; the nucleus threshold p (value of p controls candidate set size); local shape of the model's predicted distribution (flat vs. peaked) which makes nucleus size expand/contract; input context variability (different contexts produce different nucleus sizes); renormalization factor p' varying by time-step.",
            "variability_measured": true,
            "variability_metrics": "Perplexity (of generated text under the model), Self-BLEU (diversity), Zipf coefficient (vocabulary distribution), repetition percentage (repetition rate), HUSE (combined human+statistical score); also observed variance in likelihood / likelihood distribution across generated tokens.",
            "variability_results": "Nucleus (p=0.95) produced perplexity 13.13, Self-BLEU 0.32, Zipf coefficient 0.95, Repetition 0.36%, HUSE 0.97 (Table 1). Nucleus dynamically produced candidate pools ranging from ~1 to ~1000 tokens depending on context (qualitative).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of distributional statistics (perplexity, Self-BLEU, Zipf, repetition %) across decoding methods and parameter sweeps; HUSE combining human judgments and model log-probability via a KNN discriminator (k=13).",
            "reproducibility_results": "Nucleus Sampling (p=0.95) matched human perplexity most closely among tested methods and achieved the highest HUSE (0.97), indicating reproducible performance across the evaluated metrics and human judgments; outperforming pure sampling (perplexity 22.73, HUSE 0.67) and typical top-k settings unless k is very large.",
            "reproducibility_challenges": "Truncation causes zero-probability mass for most tokens which biased some evaluation (HUSE) until the authors interpolated 0.1 mass back to the original distribution for evaluation; sensitivity to choice of p (although less fragile than fixed k); distributional variability caused by context-dependent nucleus size.",
            "mitigation_methods": "Use of Nucleus (top-p) Sampling to truncate unreliable tail; tuning p to match human perplexity; interpolation (mixing a small mass of original distribution back) for evaluation to avoid truncation artifact in HUSE; general recommendation to truncate model tail instead of sampling full distribution or using fixed top-k.",
            "mitigation_effectiveness": "Quantitative improvements vs baselines: reduced perplexity from pure sampling (22.73) to 13.13, increased HUSE from 0.67 to 0.97, maintained Self-BLEU and Zipf close to human values; reduced repetition compared to greed/beam (repetition 0.36% vs greedy 73.66% and beam b=16 28.94%).",
            "comparison_with_without_controls": true,
            "number_of_runs": "5,000 generated passages (overall generation experiments); Self-BLEU computed on 1,000 sampled generations compared to other 4,999; HUSE computed on 200 generations with 20 human annotations each (4,000 annotations per decoding method).",
            "key_findings": "Nucleus Sampling (top-p) is an effective mitigation of sampling-induced variability: by truncating the unreliable tail dynamically it produces text whose perplexity, diversity (Self-BLEU/Zipf), and human-judged quality (HUSE=0.97) closely match human text while avoiding repetition and incoherence.",
            "uuid": "e658.0",
            "source_info": {
                "paper_title": "The Curious Case of Neural Text Degeneration",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Top-k Sampling",
            "name_full": "Top-k Sampling",
            "brief_description": "A truncation-based stochastic decoding method that samples the next token from the fixed set of k most probable tokens (renormalized); widely used but shown here to be suboptimal because a fixed k cannot adapt to context-dependent distribution shape.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (Generative Pre-trained Transformer 2)",
            "model_size": "762M",
            "scientific_domain": "Natural language generation / NLP",
            "experimental_task": "Open-ended conditional text continuation",
            "variability_sources": "Fixed k parameter (choice of k), stochastic sampling randomness, interaction with temperature when combined, renormalization factor p' that varies by time-step, context-dependent distribution shape (flat vs peaked).",
            "variability_measured": true,
            "variability_metrics": "Perplexity, Self-BLEU, Zipf coefficient, repetition %, HUSE; observed variance in likelihood for high-k settings.",
            "variability_results": "Examples from Table 1: Top-k k=40: perplexity 6.88, Self-BLEU 0.39, Zipf 0.96, repetition 0.78%, HUSE 0.19. Top-k k=640: perplexity 13.82, Self-BLEU 0.32, Zipf 0.96, repetition 0.28%, HUSE 0.94. Top-k k=40,t=0.7: perplexity 3.48, Self-BLEU 0.44, Zipf 1.00, repetition 8.86%, HUSE 0.08. High k (e.g. 640) can match human-like perplexity but leads to high variance/coherency issues unless tuned carefully.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Same distributional statistics and HUSE comparisons across k sweep; qualitative analysis of variance and incoherence at high k.",
            "reproducibility_results": "Top-k can approximate human statistics if k is large (k=640 achieved perplexity 13.82 and HUSE 0.94) but requires careful tuning and still shows more variability and incoherence than Nucleus Sampling; small k leads to overly generic/bland text.",
            "reproducibility_challenges": "A single global k is sub-optimal across contexts: when head distribution is flat k must be large to include reasonable options, but large k in peaked contexts includes inappropriate low-probability tokens (renormalization inflates them), producing incoherence and high variance; renormalization p' varies wildly each time-step.",
            "mitigation_methods": "Tune k (possibly large) and/or combine with temperature; but authors recommend dynamic truncation (top-p) instead of fixed k for better trade-offs.",
            "mitigation_effectiveness": "Large k (e.g., 640) improved metrics (perplexity 13.82, HUSE 0.94) approaching Nucleus, but at the cost of observed higher variance/incoherence and sensitivity; smaller k reduces diversity and leads to bland text.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Same experimental setup: 5,000 generated passages overall; specific parameter sweeps reported in Table 1; Self-BLEU computation used 1,000 sampled generations for diversity estimates.",
            "key_findings": "Top-k sampling is a useful truncation method but is brittle because a fixed k cannot adapt to the per-step distribution shape; matching human distribution often requires large k which increases the variance/coherency problems compared to Nucleus (top-p).",
            "uuid": "e658.1",
            "source_info": {
                "paper_title": "The Curious Case of Neural Text Degeneration",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Temperature Sampling",
            "name_full": "Sampling with temperature",
            "brief_description": "A method that rescales the logits by temperature t before softmax (u / t), where lower t sharpens the distribution towards high-probability tokens and higher t flattens it; used to trade off diversity vs coherence.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (Generative Pre-trained Transformer 2)",
            "model_size": "762M",
            "scientific_domain": "Natural language generation / NLP",
            "experimental_task": "Open-ended conditional text continuation",
            "variability_sources": "Temperature parameter t, interaction with truncation (top-k or top-p), stochastic sampling randomness, context-dependent distribution shape.",
            "variability_measured": true,
            "variability_metrics": "Perplexity, Self-BLEU, Zipf, repetition %; qualitative coherence assessments and HUSE.",
            "variability_results": "Sampling with t=0.9: perplexity 10.25, Self-BLEU 0.35, Zipf 0.96, repetition 0.66%, HUSE 0.79 (Table 1). Lower temperatures (e.g., t=0.7 when combined with top-k) reduce tail sampling but increase repetition and decrease diversity (example top-k k=40,t=0.7 shows high repetition and low HUSE).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Parameter sweeps over t and combined methods; compared distributional statistics and HUSE across temperature settings.",
            "reproducibility_results": "Lowering temperature improves apparent coherence (by suppressing tail) but at cost of reduced diversity and increased repetition; intermediate t (e.g., 0.9) improves over pure sampling but still inferior to Nucleus (p=0.95) on combined metrics.",
            "reproducibility_challenges": "Trade-off between reducing tail noise (low t) and maintaining diversity; low temperature can mimic greedy behavior and cause repetition; effects depend on interaction with truncation strategy.",
            "mitigation_methods": "Tune temperature, and/or combine temperature shaping with truncation (top-k or top-p); authors favor dynamic top-p truncation over relying solely on temperature.",
            "mitigation_effectiveness": "Temperature 0.9 gave improved perplexity (10.25) and HUSE (0.79) vs pure sampling; but did not reach Nucleus performance (HUSE 0.97) and lower temperatures increased repetition.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Reported within same experiments: 5,000 passages overall; specific parameter settings presented in Table 1 and figures.",
            "key_findings": "Temperature controls the diversity/coherence tradeoff but cannot by itself robustly avoid tail-induced incoherence; combining with truncation helps, but dynamic top-p truncation is a superior control.",
            "uuid": "e658.2",
            "source_info": {
                "paper_title": "The Curious Case of Neural Text Degeneration",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Beam Search (maximization)",
            "name_full": "Beam Search (maximization-based decoding)",
            "brief_description": "A deterministic search-based decoding strategy that seeks high-likelihood sequences by keeping top-b hypotheses at each step; shown to produce degenerate outputs (generic, repetitive) in open-ended generation despite low perplexity.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (Generative Pre-trained Transformer 2)",
            "model_size": "762M",
            "scientific_domain": "Natural language generation / NLP",
            "experimental_task": "Open-ended conditional text continuation",
            "variability_sources": "Beam width b (controls search breadth), search objective (maximizing probability), interaction with model's per-token probabilities, average generated length (increases/decreases with b) which affects repetition measures.",
            "variability_measured": true,
            "variability_metrics": "Perplexity (very low), repetition % (high), distinct n-grams, rank-frequency distributions; qualitative human judgments.",
            "variability_results": "Greedy: perplexity 1.50, Self-BLEU 0.50, Zipf 1.00, Repetition 73.66%. Beam b=16: perplexity 1.48, Self-BLEU 0.44, Zipf 0.94, Repetition 28.94% (Table 1). Beam outputs have unnaturally low perplexity and high repetition relative to human text.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison across beam widths b; n-gram distinct counts vs human; perplexity and repetition measures.",
            "reproducibility_results": "Beam search consistently produced low-perplexity, highly repetitive outputs across beam widths; increasing beam width often shortened average output length which partly reduced measured repetition but did not yield human-like diversity.",
            "reproducibility_challenges": "Maximization objective is intrinsically misaligned with open-ended human text: model assigns very high probability to repetitive/generic continuations; search completeness (larger beams) does not fix this and can cause shorter outputs; beam degeneracy is robust across contexts.",
            "mitigation_methods": "Diverse beam variants and diversity scoring functions are discussed in related work (not primary solutions here); authors advocate stochastic truncation (top-p) instead.",
            "mitigation_effectiveness": "Diverse beam methods are cited but not experimentally shown superior here; beam search remained inferior on measured metrics compared to stochastic truncated sampling.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Beam results reported within the same 5,000 generation experiment and analyzed across beam-width sweeps (e.g., Figure 10 shows trigram counts vs beam width).",
            "key_findings": "Maximization via beam search leads to degenerate, low-perplexity but low-quality outputs (repetition/genericness) in open-ended generation; search improvements alone do not resolve the mismatch between model per-token objectives and human-like text.",
            "uuid": "e658.3",
            "source_info": {
                "paper_title": "The Curious Case of Neural Text Degeneration",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Pure Sampling (full distribution)",
            "name_full": "Pure Sampling (sampling from full model distribution)",
            "brief_description": "Sampling directly from the full softmax output at each timestep (no truncation or temperature), which preserves the model's full stochasticity but leads here to incoherent outputs due to over-sampling of an 'unreliable tail'.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (Generative Pre-trained Transformer 2)",
            "model_size": "762M",
            "scientific_domain": "Natural language generation / NLP",
            "experimental_task": "Open-ended conditional text continuation",
            "variability_sources": "Intrinsic stochastic sampling randomness, large and unreliable low-probability tail of the distribution (many rare tokens), context-dependent distribution shape leading to sampling many unlikely tokens.",
            "variability_measured": true,
            "variability_metrics": "Perplexity, Self-BLEU, Zipf coefficient, repetition %, HUSE, and qualitative coherence.",
            "variability_results": "Pure sampling: perplexity 22.73 (worse than human 12.38), Self-BLEU 0.28, Zipf 0.93, Repetition 0.22%, HUSE 0.67 (Table 1); pure sampling overestimates rare words and produces incoherent text.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Distributional statistics across generated samples and comparison to gold text; perplexity of generated text under the same model.",
            "reproducibility_results": "Pure sampling produced higher perplexity than human references and qualitatively incoherent outputs, indicating poor recovery of human-like distribution despite preserving stochasticity.",
            "reproducibility_challenges": "The unreliable tail is over-represented in aggregate when sampling without truncation, leading to contexts that are hard to recover from and causing incoherence; stochasticity alone is not sufficient without truncation control.",
            "mitigation_methods": "Truncation via Top-k or Nucleus Sampling; lowering temperature to reduce tail mass (but with tradeoffs).",
            "mitigation_effectiveness": "Truncation (Nucleus p=0.95) reduced perplexity from 22.73 to 13.13 and increased HUSE from 0.67 to 0.97; lowering temperature helped somewhat but reduced diversity and could increase repetition.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Included in the 5,000-passage generation experiments; distributional statistics computed across these samples.",
            "key_findings": "Sampling from the full model distribution preserves stochasticity but over-samples an unreliable tail, producing incoherent text and higher perplexity â€” truncation (top-p) is needed to control this source of variability.",
            "uuid": "e658.4",
            "source_info": {
                "paper_title": "The Curious Case of Neural Text Degeneration",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "HUSE",
            "name_full": "Human Unified with Statistical Evaluation (HUSE)",
            "brief_description": "A combined evaluation method that trains a discriminator to distinguish human vs model text using two features: the model-assigned probability and human judgements of typicality, thereby jointly measuring quality and diversity.",
            "citation_title": "Unifying human and statistical evaluation for natural language generation",
            "mention_or_use": "use",
            "model_name": "GPT-2 (Generative Pre-trained Transformer 2)",
            "model_size": "762M",
            "scientific_domain": "Natural language generation / NLP evaluation",
            "experimental_task": "Human + statistical evaluation of generated text (assessing overall quality and diversity of decoding strategies)",
            "variability_sources": "Truncation of distributions (top-k / top-p) produces zero-probability mass that biased HUSE; human annotation variability (typicality judgments); model log-probabilities vary across decoding methods and truncation strategies.",
            "variability_measured": true,
            "variability_metrics": "HUSE score computed with a KNN classifier (k=13) using features: model log-probability and aggregated human typicality judgments; additional smoothing/interpolation applied for truncated distributions.",
            "variability_results": "Reported HUSE scores in Table 1: Nucleus p=0.95 -&gt; 0.97 (best), Top-k=640 -&gt; 0.94, Sampling t=0.9 -&gt; 0.79, Pure Sampling -&gt; 0.67, Top-k=40 -&gt; 0.19. Truncation artifacts caused near-zero HUSE initially until interpolation (mixing 0.1 mass of original distribution) was applied.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "KNN discrimination accuracy as HUSE score; uses 200 generations annotated (20 annotators each) resulting in 4,000 human annotations per decoding method.",
            "reproducibility_results": "After interpolation smoothing, HUSE effectively discriminated methods and ranked Nucleus highest; interpolation was necessary to avoid evaluation artifacts from truncation.",
            "reproducibility_challenges": "Truncation to zero-probability mass breaks HUSE's reliance on model log-probabilities; human annotation noise and choice of interpolation magnitude affect HUSE results.",
            "mitigation_methods": "Interpolating 0.1 mass of the original (un-truncated) distribution back into truncated distributions when computing HUSE to avoid zero-probability distortions; using KNN (k=13) on combined features.",
            "mitigation_effectiveness": "Interpolation allowed top-k and top-p methods to receive meaningful HUSE scores (rather than near-zero) and enabled comparison; using this produced the ranking shown in Table 1 where Nucleus scored highest (0.97).",
            "comparison_with_without_controls": true,
            "number_of_runs": "HUSE computed on 200 generations per decoding algorithm annotated by 20 annotators each (4,000 annotations per decoding method); KNN classifier with k=13.",
            "key_findings": "HUSE, when adjusted for truncation artifacts via interpolation, provides a useful combined metric showing that Nucleus Sampling achieves the best joint trade-off of quality and diversity among evaluated decoding strategies.",
            "uuid": "e658.5",
            "source_info": {
                "paper_title": "The Curious Case of Neural Text Degeneration",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Unreliable tail / distribution shape",
            "name_full": "Unreliable tail of the model's predicted probability distribution (and flat vs peaked head shapes)",
            "brief_description": "The paper identifies an 'unreliable tail' of many low-probability candidate tokens which, when sampled from (pure sampling) or included (large top-k), leads to incoherence, and notes that per-step distribution shapes vary (flat heads vs peaked heads) causing sensitivity to truncation choices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (Generative Pre-trained Transformer 2)",
            "model_size": "762M",
            "scientific_domain": "Natural language generation / probabilistic modeling",
            "experimental_task": "Analysis of decoding behavior and distributional properties of model predictions in open-ended generation",
            "variability_sources": "Many low-probability tokens in the tail (unreliable tail), context-dependent head shape (flat vs peaked), renormalization effects when truncating, sampling randomness that over-represents tail tokens in aggregate, and positive feedback loops that increase probability of repeated phrases.",
            "variability_measured": true,
            "variability_metrics": "Perplexity of generated text under the model, Zipf/rank-frequency plots, histogram/variance of per-token probabilities (visualized), repetition probability as repetition grows (Figure 4), nucleus size (qualitative counts).",
            "variability_results": "Evidence: pure sampling perplexity 22.73 (worse than human 12.38) indicating tail sampling harms recovery of human distribution; authors observe nucleus sizes typically range from ~1 to ~1000 candidates depending on context; Figure 4 shows repetition probabilities increase with each repetition (positive feedback).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Distributional comparisons (rank-frequency, perplexity), experiments across decoding strategies showing the tail's impact (pure sampling vs truncation).",
            "reproducibility_results": "Truncation removing the tail (top-p or tuned top-k) materially improves perplexity and human judgments relative to sampling the full distribution; sampling from the tail produces high variance and incoherence across samples.",
            "reproducibility_challenges": "The tail is context-dependent and large (tens of thousands of low-probability tokens), making fixed truncation brittle; measuring and correcting for tail effects requires dynamic methods or careful calibration.",
            "mitigation_methods": "Dynamic top-p truncation (Nucleus Sampling) to exclude the unreliable tail; tuning p to match human perplexity; temperature reduction and top-k are alternative mitigations but have trade-offs.",
            "mitigation_effectiveness": "Nucleus p=0.95 reduced negative tail effects relative to pure sampling (perplexity 13.13 vs 22.73) and achieved strong human-evaluated quality (HUSE 0.97); temperature can reduce tail but at the cost of diversity and potential repetition.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Observations derived from the same 5,000-generation experiments and parameter sweeps reported in the paper.",
            "key_findings": "The unreliable tail and per-step distribution shape are primary sources of stochastic variability and incoherence; dynamically truncating the tail (top-p) is an effective control that mitigates these problems while preserving human-like diversity.",
            "uuid": "e658.6",
            "source_info": {
                "paper_title": "The Curious Case of Neural Text Degeneration",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are unsupervised multitask learners",
            "rating": 2
        },
        {
            "paper_title": "Hierarchical neural story generation",
            "rating": 1
        },
        {
            "paper_title": "Unifying human and statistical evaluation for natural language generation",
            "rating": 2
        },
        {
            "paper_title": "Learning to write with cooperative discriminators",
            "rating": 1
        },
        {
            "paper_title": "Neural text generation with unlikelihood training",
            "rating": 2
        },
        {
            "paper_title": "Texygen: A benchmarking platform for text generation",
            "rating": 1
        }
    ],
    "cost": 0.020309499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Curious Case of Neural Text Degeneration</h1>
<p>Ari Holtzman ${ }^{\dagger \ddagger} \quad$ Jan Buys ${ }^{5 \dagger} \quad$ Li Du ${ }^{\dagger} \quad$ Maxwell Forbes ${ }^{\dagger \ddagger} \quad$ Yejin Choi ${ }^{\dagger \ddagger}$<br>${ }^{\dagger}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington<br>${ }^{\ddagger}$ Allen Institute for Artificial Intelligence<br>${ }^{5}$ Department of Computer Science, University of Cape Town<br>{ahai, dul2, mbforbes, yejin}@cs.washington.edu, jbuys@cs.uct.ac.za</p>
<h4>Abstract</h4>
<p>Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration - output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models than previous decoding strategies. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for openended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is currently the best available decoding strategy for generating long-form text that is both high-quality - as measured by human evaluation - and as diverse as human-written text.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Pure Sampling</h2>
<p>They were cattle called Bolivian Cavalleros; they live in a remote desert uninterrupted by town, and they speak huge, beautiful, paradisiacal Bolivian linguistic thing. They say, 'Lunch, marge.' They don't tell what the lunch is.' director Professor Chuperas Omwell told Sky News. "They've only been talking to scientists, like we're being interviewed by TV reporters. We don't even stick around to be interviewed by TV reporters. Maybe that's how they figured out that they're cosplaying as the Bolivian Cavalleros."</p>
<p>Figure 1: Even with substantial human context and the powerful GPT-2 Large language model, Beam Search (size 32) leads to degenerate repetition (highlighted in blue) while pure sampling leads to incoherent gibberish (highlighted in red). When $b \geq 64$, both GPT-2 Large and XL (774M and 1542 M parameters, respectively) prefer to stop generating immediately after the given context.</p>
<h2>1 INTRODUCTION</h2>
<p>On February 14th 2019, OpenAI surprised the scientific community with an impressively highquality article about Ovid's Unicorn, written by GPT-2. ${ }^{\dagger}$ Notably, the top-quality generations ob-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>tained from the model rely on randomness in the decoding method, in particular through top- $k$ sampling that samples the next word from the top $k$ most probable choices (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019), instead of aiming to decode text that maximizes likelihood.</p>
<p>In fact, decoding strategies that optimize for output with high probability, such as beam search, lead to text that is incredibly degenerate, even when using state-of-the-art models such as GPT-2 Large, as shown in Figure 1. This may seem counter-intuitive, as one would expect that good models would assign higher probability to more human-like, grammatical text. Indeed, language models do generally assign high scores to well-formed text, yet the highest scores for longer texts are often generic, repetitive, and awkward. Figure 2 exposes how different the distribution of probabilities assigned to beam search decoded text and naturally occurring text really are.</p>
<p>Perhaps equally surprising is the right side of Figure 1, which shows that pure sampling â€” sampling directly from the probabilities predicted by the model â€” results in text that is incoherent and almost unrelated to the context. Why is text produced by pure sampling so degenerate? In this work we show that the "unreliable tail" is to blame. This unreliable tail is composed of tens of thousands of candidate tokens with relatively low probability that are over-represented in the aggregate.</p>
<p>To overcome these issues we introduce Nucleus Sampling (Â§3.1). The key intuition of Nucleus Sampling is that the vast majority of probability mass at each time step is concentrated in the nucleus, a small subset of the vocabulary that tends to range between one and a thousand candidates. Instead of relying on a fixed top- $k$, or using a temperature parameter to control the shape of the distribution without sufficiently suppressing the unreliable tail, we propose sampling from the top- $p$ portion of the probability mass, expanding and contracting the candidate pool dynamically.</p>
<p>In order to compare current methods to Nucleus Sampling, we compare various distributional properties of generated text to the reference distribution, such as the likelihood of veering into repetition and the perplexity of generated text. The latter reveals that text generated by maximization or top- $k$ sampling is too probable, indicating a lack of diversity and divergence in vocabulary usage from the human distribution. On the other hand, pure sampling produces text that is significantly less likely than the gold, corresponding to lower generation quality.</p>
<p>Vocabulary usage and Self-BLEU (Zhu et al., 2018) statistics reveal that high values of $k$ are needed to make top- $k$ sampling match human statistics. Yet, generations based on high values of $k$ often have high variance in likelihood, hinting at qualitatively observable incoherency issues. Nucleus Sampling can easily match reference perplexity through tuning the value of $p$, avoiding the incoherence caused by setting $k$ high enough to match distributional statistics.</p>
<p>Finally, we perform Human Unified with Statistical Evaluation (HUSE; Hashimoto et al., 2019) to jointly assess the overall quality and diversity of the decoding strategies, which cannot be captured using either human or automatic evaluation alone. The HUSE evaluation demonstrates that Nucleus Sampling is the best overall decoding strategy. We include generated examples for qualitative analysis - see Figure 3 for a representative example, and further examples in the appendix. ${ }^{2}$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: The probability assigned to tokens generated by Beam Search and humans, given the same context. Note the increased variance that characterizes human text, in contrast with the endless repetition of text decoded by Beam Search.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2 BACKGROUND</h1>
<h3>2.1 Text Generation Decoding Strategies</h3>
<p>A number of recent works have alluded to the disadvantages of generation by maximization, which tend to generate output with high grammaticality but low diversity (Kulikov et al., 2019; Holtzman et al., 2018; Fan et al., 2018). Generative Adversarial Networks (GANs) have been a prominent research direction (Yu et al., 2017; Xu et al., 2018), but recent work has shown that when quality and diversity are considered jointly, GAN-generated text fails to outperform generations from language models (Caccia et al., 2018; Tevet et al., 2019; Semeniuta et al., 2018). Work on neural dialog systems have proposed methods for diverse beam search, using a task-specific diversity scoring function or constraining beam hypotheses to be sufficiently different (Li et al., 2016a; Vijayakumar et al., 2018; Kulikov et al., 2019; Pal et al., 2006). While such utility functions encourage desirable properties in generations, they do not remove the need to choose an appropriate decoding strategy, and we believe that Nucleus Sampling will have complementary advantages in such approaches. Finally, Welleck et al. (2020) begin to address the problem of neural text degeneration through an "unlikelihood loss", which decreases training loss on repeated tokens and thus implicitly reduces gradients on frequent tokens as well. Our focus is on exposing neural text degeneration and providing a decoding solution that can be used with arbitrary models, but future work will likely combine training-time and inference-time solutions.</p>
<h3>2.2 OPEN-ENDED VS DIRECTED GENERATION</h3>
<p>Many text generation tasks are defined through (input, output) pairs, such that the output is a constrained transformation of the input. Example applications include machine translation (Bahdanau et al., 2015), data-to-text generation (Wiseman et al., 2017), and summarization (Nallapati et al., 2016). We refer to these tasks as directed generation. Typically encoder-decoder architectures are used, often with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) or using attention-based architectures such as the Transformer (Vaswani et al., 2017). Generation is usually performed using beam search; since output is tightly scoped by the input, repetition and genericness are not as problematic. Still, similar issues have been reported when using large beam sizes (Koehn \&amp; Knowles, 2017) and more recently with exact inference (Stahlberg \&amp; Byrne, 2019), a counter-intuitive observation since more comprehensive search helps maximize probability.</p>
<p>Open-ended generation, which includes conditional story generation and contextual text continuation (as in Figure 1), has recently become a promising research direction due to significant advances in neural language models (Clark et al., 2018; Holtzman et al., 2018; Fan et al., 2018; Peng et al., 2018; Radford et al., 2019). While the input context restricts the space of acceptable output generations, there is a considerable degree of freedom in what can plausibly come next, unlike in directed generation settings. Our work addresses the challenges faced by neural text generation with this increased level of freedom, but we note that some tasks, such as goal-oriented dialog, may fall somewhere in between open-ended and directed generation.</p>
<h2>3 LANGUAGE MODEL DECODING</h2>
<p>Given an input text passage as context, the task of open-ended generation is to generate text that forms a coherent continuation from the given context. More formally, given a sequence of $m$ tokens $x_{1} \ldots x_{m}$ as context, the task is to generate the next $n$ continuation tokens to obtain the completed sequence $x_{1} \ldots x_{m+n}$. We assume that models compute $P\left(x_{1: m+n}\right)$ using the common left-to-right decomposition of the text probability,</p>
<p>$$
P\left(x_{1: m+n}\right)=\prod_{i=1}^{m+n} P\left(x_{i} \mid x_{1} \ldots x_{i-1}\right)
$$</p>
<p>which is used to generate the generation token-by-token using a particular decoding strategy.
Maximization-based decoding The most commonly used decoding objective, in particular for directed generation, is maximization-based decoding. Assuming that the model assigns higher probability to higher quality text, these decoding strategies search for the continuation with the highest</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Beam Search, $b=16$
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>PPure Sampling
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Sampling, $t=0.9$
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Pumping Station #3 shut down due to construction damage Find more at: www.abc.net.au/environment/species-worry/ in-the-top-10-killer-whale-catastrophes-in-history.html
"In the top 10 killer whale catastrophes in history:</p>
<p>1) 1986: Up to 12 orcas struck by lightning; many drowned and many more badly injured.</p>
<p>The whale's fate was confirmed late last week when the animal was found by fishermen off the coast of Bundaberg. Experts believe the whale was struck by a fishing vessel off the coast of Bundaberg, and died after being sucked into the ocean. The whale's fate was confirmed late last week when the animal was found by fishermen off the coast of Bundaberg.</p>
<p>There has been an unprecedented number of calves caught in the nets of whaling stations that operate in WA. Pilot whales continue to migrate to feeding grounds to feed their calves. They are now vulnerable due to the decline of wild populations; they are restricted to one breeding site each year. Image copyright Yoon Bo Kim But, with sharp decline in wild populations the size of the Petrels are shrinking and dwindling population means there will only be room for a few new fowl.</p>
<p>Poor nutrition has led to a rise in the number of stranded humpback whales on the West Australian coast, veterinary researchers have said. Carly Holyoake, from Murdoch University, at the Australian Veterinary Association's annual conference in Perth on Wednesday, said an unprecedented number of mostly young whales had become stranded on the coast since 2008.</p>
<p>Figure 3: Example generations continuing an initial sentence. Maximization and top- $k$ truncation methods lead to copious repetition (highlighted in blue), while sampling with and without temperature tends to lead to incoherence (highlighted in red). Nucleus Sampling largely avoids both issues.
likelihood. Since finding the optimum argmax sequence from recurrent neural language models or Transformers is not tractable (Chen et al., 2018), common practice is to use beam search (Li et al., 2016b; Shen et al., 2017; Wiseman et al., 2017). However, several recent studies on open-ended generation have reported that maximization-based decoding does not lead to high quality text (Fan et al., 2018; Holtzman et al., 2018).</p>
<h1>3.1 NUCLEUS SAMPLING</h1>
<p>We propose a new stochastic decoding method: Nucleus Sampling. The key idea is to use the shape of the probability distribution to determine the set of tokens to be sampled from. Given a distribution $P\left(x \mid x_{1: i-1}\right)$, we define its top- $p$ vocabulary $V^{(p)} \subset V$ as the smallest set such that</p>
<p>$$
\sum_{x \in V^{(p)}} P\left(x \mid x_{1: i-1}\right) \geq p
$$</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 4: The probability of a repeated phrase increases with each repetition, creating a positive feedback loop. We found this effect to hold for the vast majority of phrases we tested, regardless of phrase length or if the phrases were sampled randomly rather than taken from human text.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 5: The probability mass assigned to partial human sentences. Flat distributions lead to many moderately probable tokens, while peaked distributions concentrate most probability mass into just a few tokens. The presence of flat distributions makes the use of a small $k$ in top- $k$ sampling problematic, while the presence of peaked distributions makes large $k$ 's problematic.</p>
<p>Let $p^{\prime}=\sum_{x \in V^{(p)}} P\left(x \mid x_{1: i-1}\right)$. The original distribution is re-scaled to a new distribution, from which the next word is sampled:</p>
<p>$$
P^{\prime}\left(x \mid x_{1: i-1}\right)= \begin{cases}P\left(x \mid x_{1: i-1}\right) / p^{\prime} &amp; \text { if } x \in V^{(p)} \ 0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>In practice this means selecting the highest probability tokens whose cumulative probability mass exceeds the pre-chosen threshold $p$. The size of the sampling set will adjust dynamically based on the shape of the probability distribution at each time step. For high values of $p$, this is a small subset of vocabulary that takes up vast majority of the probability mass - the nucleus.</p>
<h1>3.2 TOP-k SAMPLING</h1>
<p>Top- $k$ sampling has recently become a popular alternative sampling procedure (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019). Nucleus Sampling and top- $k$ both sample from truncated Neural LM distributions, differing only in the strategy of where to truncate. Choosing where to truncate can be interpreted as determining the generative model's trustworthy prediction zone.</p>
<p>At each time step, the top $k$ possible next tokens are sampled from according to their relative probabilities. Formally, given a distribution $P\left(x \mid x_{1: i-1}\right)$, we define its top- $k$ vocabulary $V^{(k)} \subset V$ as the set of size $k$ which maximizes $\sum_{x \in V^{(k)}} P\left(x \mid x_{1: i-1}\right)$. Let $p^{\prime}=\sum_{x \in V^{(k)}} P\left(x \mid x_{1: i-1}\right)$. The distribution is then re-scaled as in equation 3, and sampling is performed based on that distribution. Note that the scaling factor $p^{\prime}$ can vary wildly at each time-step, in contrast to Nucleus Sampling.</p>
<p>Difficulty in choosing a suitable value of $k$ While top- $k$ sampling leads to considerably higher quality text than either beam search or sampling from the full distribution, the use of a constant $k$ is</p>
<p>sub-optimal across varying contexts. As illustrated on the left of Figure 5, in some contexts the head of the next word distribution can be flat across tens or hundreds of reasonable options (e.g. nouns or verbs in generic contexts), while in other contexts most of the probability mass is concentrated in one or a small number of tokens, as on the right of the figure. Therefore if $k$ is small, in some contexts there is a risk of generating bland or generic text, while if $k$ is large the top- $k$ vocabulary will include inappropriate candidates which will have their probability of being sampled increased by the renormalization. Under Nucleus Sampling, the number of candidates considered rises and falls dynamically, corresponding to the changes in the model's confidence region over the vocabulary which top- $k$ sampling fails to capture for any one choice of $k$.</p>
<h1>3.3 SAMPLING WITH TEMPERATURE</h1>
<p>Another common approach to sampling-based generation is to shape a probability distribution through temperature (Ackley et al., 1985). Temperature sampling has been applied widely to text generation (Ficler \&amp; Goldberg, 2017; Fan et al., 2018; Caccia et al., 2018). Given the logits $u_{1:|V|}$ and temperature $t$, the softmax is re-estimated as</p>
<p>$$
p\left(x=V_{l} \mid x_{1: i-1}\right)=\frac{\exp \left(u_{l} / t\right)}{\sum_{l^{\prime}} \exp \left(u_{l}^{\prime} / t\right)}
$$</p>
<p>Setting $t \in[0,1)$ skews the distribution towards high probability events, which implicitly lowers the mass in the tail distribution. Low temperature sampling has also been used to partially alleviate the issues of top- $k$ sampling discussed above, by shaping the distribution before top- $k$ sampling (Radford et al., 2018; Fan et al., 2018). However, recent analysis has shown that, while lowering the temperature improves generation quality, it comes at the cost of decreasing diversity (Caccia et al., 2018; Hashimoto et al., 2019).</p>
<h2>4 LIKELIHOOD EVALUATION</h2>
<h3>4.1 EXPERIMENTAL SETUP</h3>
<p>While many neural network architectures have been proposed for language modeling, including LSTMs (Sundermeyer et al., 2012) and convolutional networks (Dauphin et al., 2017), the Transformer architecture (Vaswani et al., 2017) has been the most successful in the extremely large-scale training setups in recent literature (Radford et al., 2018; 2019). In this study we use the Generatively Pre-trained Transformer, version 2 (GPT2; Radford et al., 2019), which was trained on WebText, a 40GB collection of text scraped from the web. ${ }^{3}$ We perform experiments using the Large model ( 762 M parameters). Our analysis is based on generating 5,000 text passages, which end upon reaching an end-of-document token or a maximum length of 200 tokens. Texts are generated conditionally, conditioned on the initial paragraph (restricted to 1-40 tokens) of documents in the held-out portion of WebText, except where otherwise mentioned.</p>
<h3>4.2 PERPLEXITY</h3>
<p>Our first evaluation is to compute the perplexity of generated text using various decoding strategies, according to the model that is being generated from. We compare these perplexities against that of the gold text (Figure 6). Importantly, we argue that the optimal generation strategy should produce text which has a perplexity close to that of the gold text: Even though the model has the ability to generate text that has lower perplexity (higher probability), such text tends to have low diversity and get stuck in repetition loops, as shown in $\S 5$ and illustrated in Figure 4.</p>
<p>We see that perplexity of text obtained from pure sampling is worse than the perplexity of the gold. This indicates that the model is confusing itself: sampling too many unlikely tokens and creating context that makes it difficult to recover the human distribution of text, as in Figure 1. Yet, setting the temperature lower creates diversity and repetition issues, as we shall see in $\S 5$. Even with our relatively fine-grained parameter sweep, Nucleus Sampling obtains closest perplexity to human text, as shown in Table 1.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Perplexity</th>
<th style="text-align: center;">Self-BLEU4</th>
<th style="text-align: center;">Zipf Coefficient</th>
<th style="text-align: center;">Repetition \%</th>
<th style="text-align: center;">HUSE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">12.38</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Greedy</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">73.66</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Beam, b=16</td>
<td style="text-align: center;">1.48</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">28.94</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Stochastic Beam, b=16</td>
<td style="text-align: center;">19.20</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Pure Sampling</td>
<td style="text-align: center;">22.73</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">Sampling, $t=0.9$</td>
<td style="text-align: center;">10.25</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: center;">Top- $k=40$</td>
<td style="text-align: center;">6.88</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.19</td>
</tr>
<tr>
<td style="text-align: center;">Top- $k=640$</td>
<td style="text-align: center;">13.82</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.94</td>
</tr>
<tr>
<td style="text-align: center;">Top- $k=40, t=0.7$</td>
<td style="text-align: center;">3.48</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">8.86</td>
<td style="text-align: center;">0.08</td>
</tr>
<tr>
<td style="text-align: center;">Nucleus $p=0.95$</td>
<td style="text-align: center;">13.13</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.97</td>
</tr>
</tbody>
</table>
<p>Table 1: Main results for comparing all decoding methods with selected parameters of each method. The numbers closest to human scores are in bold except for HUSE (Hashimoto et al., 2019), a combined human and statistical evaluation, where the highest (best) value is bolded. For Top- $k$ and Nucleus Sampling, HUSE is computed with interpolation rather than truncation (see $\S 6.1$ ).</p>
<h1>4.3 Natural Language Does Not Maximize Probability</h1>
<p>One might wonder if the issue with maximization is a search error, i.e., there are higher quality sentences to which the model assigns higher probability than to the decoded ones, beam search has just failed to find them. Yet Figures $2 \&amp; 6$ show that the per-token probability of natural text is, on average, much lower than text generated by beam search. Natural language rarely remains in a high probability zone for multiple consecutive time steps, instead veering into lower-probability but more informative tokens. Nor does natural language tend to fall into repetition loops, even though the model tends to assign high probability to this, as seen in Figure 4.</p>
<p>Why is human-written text not the most probable text? We conjecture that this is an intrinsic property of human language. Language models that assign probabilities one word at a time without a global model of the text will have trouble capturing this effect. Grice's Maxims of Communication (Grice, 1975) show that people optimize against stating the obvious. Thus, making every word as predictable as possible will be disfavored. This makes solving the problem simply by training larger models or improving neural architectures using standard per-word learning objectives unlikely: such models are forced to favor the lowest common denominator, rather than informative language.</p>
<h2>5 Distributional Statistical Evaluation</h2>
<h3>5.1 Zipf Distribution Analysis</h3>
<p>In order to compare generations to the reference text, we begin by analyzing their use of vocabulary. Zipf's law suggests that there is an exponential relationship between the rank of a word and its frequency in text. The Zipfian coefficient $s$ can be used to compare the distribution in a given text
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6: Perplexities of generations from various decoding methods. Note that beam search has unnaturally low perplexities. A similar effect is seen using a temperature of 0.7 with top- $k$ as in both Radford et al. (2019) and Fan et al. (2018). Sampling, Top- $k$, and Nucleus can all be calibrated to human perplexities, but the first two face coherency issues when their parameters are set this high.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 7: A rank-frequency plot of the distributional differences between $n$-gram frequencies of human and machine text. Sampling and Nucleus Sampling are by far the closest to the human distribution, while Beam Search clearly follows a very different distribution than natural language.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 8: Self-BLEU calculated on the unconditional generations produced by stochastic decoding methods; lower Self-BLEU scores imply higher diversity. Horizontal blue and orange lines represent human self-BLEU scores. Note how common values of $t \in [0.5,1]$ and $k \in [1,100]$ result in high self-similarity, whereas "normal" values of $p \in [0.9,1)$ closely match the human distribution of text.</p>
<p>To a theoretically perfect exponential curve, where $s=1$ (Piantadosi, 2014). Figure 7 shows the vocabulary distributions along with estimated Zipf coefficients for selected parameters of different decoding methods. As expected, pure sampling is the closest to the human distribution, followed by Nucleus Sampling. The visualization of the distribution shows that pure sampling slightly <em>overestimates</em> the use of rare words, likely one reason why pure sampling also has higher perplexity than human text. Furthermore, lower temperature sampling avoids sampling these rare words from the tail, which is why it has been used in some recent work (Fan et al., 2018; Radford et al., 2019).</p>
<h3>5.2 SELF-BLEU</h3>
<p>We follow previous work and compute Self-BLEU (Zhu et al., 2018) as a metric of diversity. Self-BLEU is calculated by computing the BLEU score of each generated document using <em>all other generations</em> in the evaluation set as references. Due to the expense of computing such an operation, we sample 1000 generations, each of which is compared with <em>all 4999 other generations as references</em>. A lower Self-BLEU score implies higher diversity. Figure 8 shows that Self-BLEU results largely follow that of the Zipfian distribution analysis as a diversity measure. It is worth noting that</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 9: We visualize how often different decoding methods get "stuck" in loops within the first 200 tokens. A phrase (minimum length 2) is considered a repetition when it repeats at least three times at the end of the generation. We label points with their parameter values except for $t$ and $p$ which follow the x -axis. Values of $k$ greater than 100 are rarely used in practice and values of $p$ are usually in $[0.9,1)$; therefore Nucleus Sampling is far closer to the human distribution in its usual parameter range. Sampling with temperatures lower than 0.9 severely increase repetition. Finally, although beam search becomes less repetitive according to this metric as beam width increases, this is largely because average length gets shorter as $b$ increases (see Appendix A).
very high values of $k$ and $t$ are needed to get close to the reference distribution, though these result in unnaturally high perplexity $(\S 4)$.</p>
<h1>5.3 REPETITION</h1>
<p>One attribute of text quality that we can quantify is repetition. Figure 9 shows that Nucleus Sampling and top- $k$ sampling have the least repetition for reasonable parameter ranges. Generations from temperature sampling have more repetition unless very high temperatures are used, which we have shown negatively affects coherence (as measured by high perplexity). Further, all stochastic methods face repetition issues when their tuning parameters are set too low, which tends to overtruncate, mimicking greedy search. Therefore we conclude that only Nucleus Sampling satisfies all the distributional criteria for desirable generations.</p>
<h2>6 Human Evaluation</h2>
<h3>6.1 Human Unified with Statistical Evaluation (HUSE)</h3>
<p>Statistical evaluations are unable to measure the coherence of generated text properly. While the metrics in previous sections gave us vital insights into the different decoding methods we compare, human evaluation is still required to get a full measure of the quality of the generated text. However, pure human evaluation does not take into account the diversity of the generated text; therefore we use HUSE (Hashimoto et al., 2019) to combine human and statistical evaluation. HUSE is computed by training a discriminator to distinguish between text drawn from the human and model distributions, based on only two features: The probability assigned by the language model, and human judgements of typicality of generations. Text that is close to the human distribution in terms of quality and diversity should perform well on both likelihood evaluation and human judgements.
As explored in the previous sections, the current best-performing decoding methods rely on truncation of the probability distribution, which yields a probability of 0 for the vast majority of potential tokens. Initial exploration of applying HUSE directly led to top- $k$ and Nucleus Sampling receiving scores of nearly 0 due to truncation, despite humans favoring these methods. As a proxy, when generating the text used to compute HUSE, we interpolate (with mass 0.1) the original probability distribution with the top- $k$ and Nucleus Sampling distribution, smoothing the truncated distribution.
For each decoding algorithm we annotate 200 generations for typicality, with each generation receiving 20 annotations from 20 different annotators. This results in a total of 4000 annotations per a</p>
<p>decoding scheme. We use a KNN classifier to compute HUSE, as in the original paper, with $k=13$ neighbors, which we found led to the higher accuracy in discrimination. The results in Table 1 shows that Nucleus Sampling obtains the highest HUSE score, with Top- $k$ sampling performing second best.</p>
<h1>6.2 Qualitative Analysis</h1>
<p>Figure 3 shows representative example generations. Unsurprisingly, beam search gets stuck in a repetition loop it cannot escape. Of the stochastic decoding schemes, the output of full sampling is clearly the hardest to understand, even inventing a new word "umidauda", apparently a species of bird. The generation produced by Nucleus Sampling isn't perfect - the model appears to confuse whales with birds, and begins writing about those instead. Yet, top- $k$ sampling immediately veers off into an unrelated event. When top- $k$ sampling is combined with a temperature of 0.7 , as is commonly done (Radford et al., 2019; Fan et al., 2018), the output devolves into repetition, exhibiting the classic issues of low-temperature decoding. More generations are available in Appendix B.</p>
<h2>7 CONCLUSION</h2>
<p>This paper provided a deep analysis into the properties of the most common decoding methods for open-ended language generation. We have shown that likelihood maximizing decoding causes repetition and overly generic language usage, while sampling methods without truncation risk sampling from the low-confidence tail of a model's predicted distribution. Further, we proposed Nucleus Sampling as a solution that captures the region of confidence of language models effectively. In future work, we wish to dynamically characterize this region of confidence and include a more semantic utility function to guide the decoding process.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This research was supported in part by NSF (IIS-1524371), the National Science Foundation Graduate Research Fellowship under Grant No. DGE1256082, DARPA CwC through ARO (W911NF15-1- 0543), DARPA MCS program through NIWC Pacific (N66001-19-2-4031), the South African Centre for Artificial Intelligence Research, and the Allen Institute for AI.</p>
<h2>REFERENCES</h2>
<p>David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for boltzmann machines. Cognitive science, 9(1):147-169, 1985.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. Proceedings of the 2015 International Conference on Learning Representations, 2015.</p>
<p>Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. Language gans falling short. In Critiquing and Correcting Trends in Machine Learning: NeurIPS 2018 Workshop, 2018. URL http://arxiv.org/abs/1811.02549.</p>
<p>Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. Recurrent neural networks as weighted language recognizers. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2261-2271, New Orleans, Louisiana, June 2018.</p>
<p>Elizabeth Clark, Yangfeng Ji, and Noah A. Smith. Neural text generation in stories using entity representations as context. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2250-2260, New Orleans, Louisiana, June 2018.</p>
<p>Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Machine Learning, pp. 933-941, 2017.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 889-898, 2018.</p>
<p>Jessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language generation. In Proceedings of the Workshop on Stylistic Variation, pp. 94-104, 2017.</p>
<p>H Paul Grice. Logic and conversation. In P Cole and J L Morgan (eds.), Speech Acts, volume 3 of Syntax and Semantics, pp. 41-58. Academic Press, 1975.</p>
<p>Tatsunori B. Hashimoto, Hugh Zhang, and Percy Liang. Unifying human and statistical evaluation for natural language generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019.</p>
<p>Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. Learning to write with cooperative discriminators. In Proceedings of the Association for Computational Linguistics, 2018.</p>
<p>Philipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation, pp. 28-39, 2017.</p>
<p>Ilya Kulikov, Alexander H Miller, Kyunghyun Cho, and Jason Weston. Importance of search and evaluation strategies in neural dialogue modeling. International Conference on Natural Language Generation, 2019.</p>
<p>Jiwei Li, Will Monroe, and Dan Jurafsky. A simple, fast diverse decoding algorithm for neural generation. arXiv preprint arXiv:1611.08562, 2016a.</p>
<p>Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep reinforcement learning for dialogue generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1192-1202, 2016b.</p>
<p>Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1412-1421, 2015.</p>
<p>Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pp. 280-290, 2016.</p>
<p>Chris Pal, Charles Sutton, and Andrew McCallum. Sparse forward-backward using minimum divergence beams for fast training of conditional random fields. In 2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings, volume 5, May 2006.</p>
<p>Nanyun Peng, Marjan Ghazvininejad, Jonathan May, and Kevin Knight. Towards controllable story generation. In Proceedings of the First Workshop on Storytelling, pp. 43-49, New Orleans, Louisiana, June 2018. doi: 10.18653/v1/W18-1505.</p>
<p>Steven T Piantadosi. Zipfs word frequency law in natural language: A critical review and future directions. Psychonomic bulletin \&amp; review, 21(5):1112-1130, 2014.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018. URL https://s3-us-west-2.amazonaws. com/openai-assets/research-covers/language-unsupervised/ language_understanding_paper.pdf. Unpublished manuscript.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, February 2019. URL https: //d4mucfpksywv.cloudfront.net/better-language-models/language_ models_are_unsupervised_multitask_learners.pdf. Unpublished manuscript.</p>
<p>Stanislau Semeniuta, Aliaksei Severyn, and Sylvain Gelly. On accurate evaluation of gans for language generation. arXiv preprint arXiv:1806.04936, 2018.</p>
<p>Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. Style transfer from non-parallel text by cross-alignment. In Advances in neural information processing systems, pp. 6830-6841, 2017.</p>
<p>Felix Stahlberg and Bill Byrne. On nmt search errors and model errors: Cat got your tongue? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. $3347-3353,2019$.</p>
<p>Martin Sundermeyer, Ralf SchlÃ¼ter, and Hermann Ney. Lstm neural networks for language modeling. In Thirteenth annual conference of the international speech communication association, 2012.</p>
<p>Guy Tevet, Gavriel Habib, Vered Shwartz, and Jonathan Berant. Evaluating text gans as language models. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2241-2247, 2019.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998-6008, 2017.</p>
<p>Ashwin K. Vijayakumar, Michael Cogswell, Ramprasaath R. Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.</p>
<p>Sam Wiseman, Stuart Shieber, and Alexander Rush. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2253-2263, Copenhagen, Denmark, September 2017.</p>
<p>Jingjing Xu, Xuancheng Ren, Junyang Lin, and Xu Sun. Diversity-promoting gan: A cross-entropy based generative adversarial network for diversified text generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3940-3949, Brussels, Belgium, oct 2018.</p>
<p>Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, 2017.</p>
<p>Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: A benchmarking platform for text generation models. SIGIR, 2018.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 10: The total number of trigrams produced by Beam Search with varying beam widths, with gold (human) data for comparison. Note how the average length of generations goes down linearly with beam width, while the number of distinct trigrams stays constant and extremely low in comparison to gold data.</p>
<h1>B EXAMPLE GENERATIONS</h1>
<p>We include a set of examples for further qualitative comparison.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Beam Search, $b=16$
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Pure Sampling
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Sampling, $t=0.9$
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Nucleus, $p=0.95$
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<h2>Top Customer Questions</h2>
<p>Q: I have a question about the new "S" series. I have a question about the new \"S\" series A: The new "S" series is a new line of high-performance, high-performance, highperformance, high-performance, high-performance, highperformance, highperformance, high-performance, high-performance, highperformance, high-performance, high-performance, high-performance, high-performance, high-performance, high-performance, high-performance, high-performance, high-performance, high-performance, $h$ When will you start sending this stuff for register until someone else orders? If anyone wants to order, we would like to keep the next batch of binders as being a vaiible sample. It ASKED for the logistics and cost of shipping. I wish for a refund if actions like this are to set us back another beat. Are there any instructions for understanding the CDI CAL 0519-019? If there are, l'd be very keen to find them.</p>
<p>What's the best way to read this one? The blog post from the authors discusses the differences between how the marketing mix plays out in terms of visibility, trustworthiness, and engagement, among other things. In my experience, this difference isn't necessarily obvious; it's hard to explain to a layperson, but in all honesty it's probably the most important point to consider when reviewing social media.</p>
<p>8 questions Asked By: Bizzabo - 28th February 2016 Product: Jaybo Comfort: Durability I have a small pad for the calf and on the off chance it all breaks I would like to know that how do you square up the circumference of the pad with something as comfortable as a new t shirts. Thanks. Has the compression system at all moved????? Asked By: Kiran R 28th February 2016 Product: Jaybo Comfort: Durability</p>
<p>How can I change the wallpaper for my computer screen? If you have the latest (current version) of your operating system, then you can change the background wallpaper for your screen. How can I change the wallpaper for my computer screen? Open the settings app. Tap on your user name and tap on Settings. Click on the Background. The wallpaper change will apply. How can I change the background wallpaper for my screen?</p>
<p>Where can I purchase my prototypes and printed books in the USA? Currently we have a sign for purchase available in the guide. Since we are based in the States we will be shipping to you by FedEx. Do you make books in America? We do! We love our folks in the US. We haven't had a chance to make a documentary so we love to share our love for our culture. How does the book publisher pay for the book and art?</p>
<p>Why are your caustics superior? Purest Grades Available. All US Manufactured. No Asian Imports. Quality Control on all incoming and outgoing shipments. Most suppliers only supply tech grade product. What is difference between Food and Tech Grade Lye? Food grade has a lower parts per million of heavy metal impurities. $80 \%$ of our soap makers prefer our food grade product.</p>
<p>Figure 11: More example generations from an initial tag line. All generations available at https: //github.com/ari-holtzman/degen</p>
<p><img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Beam Search, $b=16$
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>01/05/19 - Visited The Subaru Canada Heritage15th Star for my vehicle research. That was a wonderful experience. Thanks again for your knowledge and support. 05/23/19 - I was thrilled to receive my Subaru Legacy character! It fits me perfectly and makes me feel like a kid again!
06/15/19 - Just regained my licence for my different coloured 2016 WRX / STI...
<img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Sampling, $t=0.9$
<img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Top-k, $k=640$,
Top-k, $k=640, t=0.7$
<img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Just got back from vacation.
09/07/18 - Happy 9th birthday to me! Happy 9th birthday to me! 09/01/18 - Sofwidden also! Sofwidden also! 08/25/18 - Happy birthday to me! Happy birthday to me! 08/17/18 - Night shift! $\square \square$ Night shift! $\square \square$ 08/17/18 - Lucky me! Lucky me!
Just got back from vacation.
09/03/18 - After more than 40 years, I might have an apartment in Virginia Beach. After more than 40 years, I might have an apartment in Virginia Beach. 08/20/18 - Going for a hike at Mount Eerie in Northeast Virginia Spring Going for a hike at Mount Eerie in Northeast Virginia Spring
Just got back from vacation.
09/08/18 - I've gotten really sick. - I've gotten really sick.
09/07/18 - My wife and I are getting married in February.</p>
<ul>
<li>My wife and I are getting married in February.</li>
</ul>
<p>09/06/18 - I'm so excited to go back to college this fall.</p>
<ul>
<li>I'm so excited to go back to college this fall.</li>
</ul>
<p>Just got back from vacation.
07/12/18 - Happy birthday to Swingu, who is nearly 5 years old. I would like to say hi to him on the road as well as when I ride with him. You cannot go to work without feeling physically sick or psychologically exhausted because you can barely breathe. Even if you ride on rollercoaster even once, it is easy to recover from the physical side of it.
<img alt="img-22.jpeg" src="img-22.jpeg" /></p>
<p>I just got back from a much needed and really great nine day vacation to my remote Arizona property. It was a really restful and relaxing visit. I got a lot accomplished while I was there, but still found time to just goof off and have fun too. I got to do some astronomy, even though the weather was pretty cloudy most of the time. Here is a 50 minute exposure of M101. It turned out pretty good.</p>
<p>Figure 12: More example generations from an initial tag line. Note that Pure Sampling and Nucleus Sampling is the only algorithms that can escape the repetition loop, with Nucleus Sampling's generation far closer in style to the ground truth text. All generations available at https: //github.com/ari-holtzman/degen</p>
<p><img alt="img-23.jpeg" src="img-23.jpeg" /></p>
<p>Beam Search, $b=16$
<img alt="img-24.jpeg" src="img-24.jpeg" /></p>
<p>PPure Sampling</p>
<p>Sampling, $t=0.9$
<img alt="img-25.jpeg" src="img-25.jpeg" /></p>
<p>Nucleus, $p=0.95$</p>
<h2>University of Wisconsin - Madison</h2>
<p>University of Wisconsin - Madison University of Wisconsin - Oshkosh</p>
<p>Mental Health
4240 New Florence Rd, Henderson, WI 54711
(262)-335-7453</p>
<p>MentalHealth@uiowa.edu
University of Wisconsin Madison (at UW Campus)
Jonathan Hedberg
Department of Comparative and Applied Economics
University of California - Berkeley
Michael Amante
Department of Economics
Colorado State University Denver
Claremont McKenna College
Merrimack College
New England Institute of Technology
UConn School of Medicine
Wisconsin School of Engineering
University of Wisconsin - Milwaukee
University of Wisconsin - Oshkosh
University of Wisconsin - Racine
University of Wisconsin - Red Wing
University of Wisconsin - Stevens Point
University of Wisconsin - Stevens Point
Thomas Jefferson, 1777-1826
Who Is Mike Barnacle?
Well, at the start, this was clearly a project designed to help people get the gist of classic art in its myriad ways. Now we find ourselves with an interesting set of recordings of 200 BC . Who are these guys? This one, apparently, are the descendants of Greek historian Euclid. He famously analyzed straight lines so we know those are straight.</p>
<p>In late 1998, a UW-Madison group led by James Thomson was the first to isolate and culture human embryonic stem cells, master undifferentiated cells that arise at the earliest stages of development and are capable of becoming any of the 220 types of cells and tissues in the human body.</p>
<p>Figure 13: More example generations from an initial tag line. All generations available at https: //github.com/ari-holtzman/degen</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Available at https://github.com/openai/gpt-2-output-dataset&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{\ddagger}$ https://openai.com/blog/better-language-models/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
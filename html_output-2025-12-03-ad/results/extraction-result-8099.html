<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8099 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8099</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8099</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-2db77485736cf29778a4464fe500a289bd46e7ac</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2db77485736cf29778a4464fe500a289bd46e7ac" target="_blank">ChatGPT as a Factual Inconsistency Evaluator for Abstractive Text Summarization</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Experimental re-sults show that ChatGPT outperforms previous SOTA evaluation metrics on 6/9 datasets across three tasks, demonstrating its great potential for assessing factual inconsistency in the zero-shot setting.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8099.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8099.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entailment - CoGenSum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (zero-shot CoT) balanced-accuracy on SUMMAC: CoGenSum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot chain-of-thought ChatGPT (gpt-3.5-turbo-0301) evaluated binary consistency (entailment) on the CoGenSum subset of the SUMMAC benchmark and is compared to existing automatic metrics and the dataset's human-derived labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>factual inconsistency detection (entailment inference / binary classification)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CoGenSum (SUMMAC benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI gpt-3.5-turbo-0301 via API (InstructGPT + RLHF); used zero-shot and zero-shot chain-of-thought prompts</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>ground-truth binary labels aggregated from dataset annotators (SUMMAC standardized labels)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>balanced accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>74.3</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>preference for lexical similarity; low sensitivity to inconsistent summaries; false reasoning; inadequate instruction-following</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT (ZS-COT) achieved 74.3% balanced accuracy, outperforming several baselines; however authors observe high specificity but low sensitivity (i.e., it often labels summaries as consistent especially when lexical overlap is high) and noted failures where missing modifiers (e.g., 'half of') are ignored.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Strong zero-shot performance without task-specific training; improved over many SOTA automatic metrics on this dataset</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompt: zero-shot CoT ('Explain your reasoning step by step then answer (yes or no)'); single API call per sample; only definitive 'consistent' judgments counted as positive</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8099.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8099.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entailment - XSumFaith</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (zero-shot CoT) balanced-accuracy on SUMMAC: XSumFaith</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT (ZS-COT) evaluated binary consistency on XSumFaith (SUMMAC) and compared to existing metrics and the dataset labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>factual inconsistency detection (entailment inference / binary classification)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>XSumFaith (SUMMAC benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI gpt-3.5-turbo-0301; zero-shot and zero-shot chain-of-thought prompts tested</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>ground-truth binary labels aggregated from dataset annotators (SUMMAC standardized labels)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>balanced accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>63.1</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>reduced performance on abstractive summaries (lower lexical overlap); tendency to label abstractive cases as inconsistent; false reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>On XSumFaith ChatGPT's balanced accuracy (63.1%) is lower than the best compared model (SummaC_Conv 66.4%); authors attribute this to the abstractive nature of XSum summaries (lower lexical overlap) causing more 'inconsistent' predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Still competitive in zero-shot setting; no task-specific training required</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot CoT prompt; considered only explicit 'consistent' outputs as positive; per-sample API calls</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8099.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8099.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entailment - Polytope</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (zero-shot CoT) balanced-accuracy on SUMMAC: Polytope</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT (ZS-COT) evaluated binary consistency on the Polytope subset and compared to other automatic metrics using SUMMAC human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>factual inconsistency detection (entailment inference / binary classification)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Polytope (SUMMAC benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>gpt-3.5-turbo-0301 (InstructGPT + RLHF); zero-shot & ZS-COT prompts</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>ground-truth binary labels aggregated from dataset annotators (SUMMAC standardized labels)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>balanced accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>61.4</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>low sensitivity to inconsistencies; lexical-bias-driven errors</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT_ZS-COT scored 61.4% balanced accuracy on Polytope, below some QA-based metrics; authors note ChatGPT often retrieves consistent examples (high specificity) but misses many inconsistencies.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Zero-shot applicability and reasonable performance across datasets</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ZS-COT prompt; decision aggregation treating non-definitive labels (e.g., 'mostly consistent') as inconsistent</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8099.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8099.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entailment - FactCC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (zero-shot CoT) balanced-accuracy on SUMMAC: FactCC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT (ZS-COT) tested on FactCC subset of SUMMAC and compared to specialized metrics trained on synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>factual inconsistency detection (entailment inference / binary classification)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FactCC (SUMMAC benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>gpt-3.5-turbo-0301 via API; zero-shot & ZS-COT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>ground-truth binary labels aggregated from dataset annotators (SUMMAC standardized labels)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>balanced accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>79.5</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>still misses some inconsistent cases; lexical-bias tendencies</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT_ZS-COT achieved 79.5% balanced accuracy (below SummaC_Conv at 89.5%) indicating competitive but not uniformly dominant performance; authors note substantial improvement from ZS to ZS-COT.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Good zero-shot ability without synthetic-data training required by some baselines</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ZS-COT prompt; thresholds for scoring models tuned on validation sets where applicable</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8099.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8099.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entailment - SummEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (zero-shot CoT) balanced-accuracy on SUMMAC: SummEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of ChatGPT (ZS-COT) vs human labels in SummEval consistency (binary after SUMMAC standardization).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>factual inconsistency detection (entailment inference / binary classification)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval (SUMMAC benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>gpt-3.5-turbo-0301; used zero-shot and zero-shot CoT prompts</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>3 expert annotators in original SummEval (averaged/standardized to binary in SUMMAC)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>balanced accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>83.3</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>occasionally fails to follow strict definition of consistency; may rate partial/mostly consistent as inconsistent in binary framing</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT_ZS-COT scored 83.3% balanced accuracy, slightly ahead of many baselines; authors point out unstable outputs like 'partially consistent' being treated as inconsistent in evaluation protocol, affecting comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High performance on this dataset and strong alignment with annotated labels</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ZS-COT prompt; binary aggregation rules: only explicit 'consistent' accepted as positive</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8099.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8099.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entailment - FRANK (SUMMAC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (zero-shot CoT) balanced-accuracy on SUMMAC: FRANK</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT (ZS-COT) evaluated against FRANK dataset human annotations (standardized binary labels in SUMMAC) and compared to other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>factual inconsistency detection (entailment inference / binary classification)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FRANK (SUMMAC benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>gpt-3.5-turbo-0301; zero-shot & ZS-COT prompts</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>annotators used in FRANK (dataset-level aggregation standardized in SUMMAC)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>balanced accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>82.6</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>false inferences; inadequate instruction adherence; sensitivity issues</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT_ZS-COT (82.6%) comparable to/above many baselines; analysis shows strong specificity but lower sensitivity—retrieves consistent examples well but misses inconsistent ones; examples show false inferential steps where ChatGPT treats 'bill passed lower house' as sufficient evidence that PM 'won the vote'.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Outperforms many prior automatic metrics; zero-shot usage</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ZS-COT prompt; per-sample independent API calls; considered partial labels as inconsistent for binary scoring</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8099.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8099.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summary Ranking (Falke 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT vs human accuracy on pairwise summary ranking (which of two is more consistent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT (zero-shot) was used to choose the more consistent summary from two candidates (one faithful, one not) on the dataset introduced by Falke et al. (2019); authors compare ChatGPT accuracy to other models and a reported human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summary ranking (choose more consistent of two summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Falke et al. (2019) ranking dataset (CNN/DM sampled; 373 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>gpt-3.5-turbo-0301 via API; zero-shot prompt (direct A or B answer)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>human judgments reported in Falke et al. (2019) (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (choose correct consistent summary)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>85.2</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>occasionally inconsistent with binary entailment outputs; may produce non-binary outputs (both consistent/none) which are treated as failures</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT achieved 85.2% accuracy, slightly above the reported human baseline (83.9%) and above other automatic methods; authors note ChatGPT can detect subtle semantic differences when given pairwise contrastive context, even when it failed on single-sample entailment.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Outperformed humans and other metrics on this ranking dataset in zero-shot; able to pick up minor semantic differences when comparative context is provided</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompt asking 'Answer (A or B):' given article sentence and two candidate summaries; outputs labeling both or neither as consistent counted as failure</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8099.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8099.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency Rating - FRANK (overall)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT correlation with human ratings on FRANK (Pearson and Spearman reported)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT provided 1-10 consistency ratings and authors computed Pearson and Spearman correlations with human-annotated consistency scores on the FRANK dataset (overall and splits).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>consistency rating (1-10) alignment with human scores</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FRANK (whole)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>gpt-3.5-turbo-0301 via API; zero-shot rating prompt requesting score 1-10</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>FRANK annotators (sentence-level annotators aggregated to summary-level scores)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r; Spearman rho</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.7</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>reduced correlation on abstractive (XSum) subsets; sometimes misapplies definition of 'consistency' focusing on coverage</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Authors report strong Pearson (0.70) and Spearman (0.69) correlations between ChatGPT scores and human judgements on FRANK overall, far higher than baselines (many single-digit correlations). Performance declines when splitting FRANK into CNN/DM vs XSum outputs (higher on CNN/DM).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Substantially higher correlation with human ratings than prior automatic metrics, in a zero-shot setup</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompt: ask ChatGPT to 'Score the following summary ... from 1 to 10' with provided definition; per-sample single API calls; correlations computed against dataset human scores</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8099.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8099.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency Rating - FRANK (CNN/DM split)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT correlation with human ratings on FRANK (CNN/DM subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Correlation of ChatGPT 1-10 ratings with human judgements on the FRANK subset corresponding to CNN/DM-style summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>consistency rating (1-10) alignment with human scores (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FRANK (CNN/DM subset)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>gpt-3.5-turbo-0301; zero-shot rating prompt</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>FRANK annotators (subset grouped by source model)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r; Spearman rho</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.5</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>lower correlation on more abstractive summaries; lexical-similarity bias less problematic on extractive CNN/DM but still present</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Pearson 0.50 and Spearman 0.46 on the CNN/DM split—stronger than baselines but lower than FRANK overall; authors attribute decline on XSum split to abstractive nature and lower lexical overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Reasonable alignment with humans on less-abstractive (CNN/DM) summaries</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same 1-10 rating prompt; FRANK split into CNN/DM vs XSum and correlations computed separately</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8099.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8099.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency Rating - FRANK (XSum split)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT correlation with human ratings on FRANK (XSum subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Correlation of ChatGPT 1-10 ratings with human judgements on FRANK subset of summaries generated by models trained on XSum (more abstractive).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>consistency rating (1-10) alignment with human scores (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FRANK (XSum subset)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>gpt-3.5-turbo-0301; zero-shot rating prompt</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>FRANK annotators (subset grouped by source model)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r; Spearman rho</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.34</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>significant performance drop on abstractive summaries; lexical similarity bias; lower correlation with human ratings</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Pearson 0.34 and Spearman 0.27 on XSum-produced summaries; authors note decline due to lower lexical overlap and increased abstractive rewriting.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Still outperforms many baseline metrics even on abstractive subsets</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>1-10 rating prompt; correlations computed for the XSum subset</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8099.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8099.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency Rating - SummEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT correlation with expert ratings on SummEval (consistency aspect)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT 1-10 consistency ratings compared to SummEval human expert annotations (originally 5-point scale, averaged then standardized).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>consistency rating (scale vs expert annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval (consistency scores)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>gpt-3.5-turbo-0301; zero-shot rating prompt (1-10)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>3 expert annotators per summary (SummEval)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r; Spearman rho</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.49</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>inadequate prompt adherence in some cases (rating based on coverage rather than strict entailment); tendency to undervalue short but accurate summaries</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Pearson 0.49 and Spearman 0.35 correlations with expert averaged scores; authors present an example where ChatGPT rated a clearly-consistent short summary 1/10 while experts gave 5/5, illustrating prompt-understanding failures.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Higher correlation with expert judgment than prior automatic metrics in this study</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompt asks for 1-10 consistency; SummEval original 5-point expert ratings averaged and compared to ChatGPT scores via correlation metrics</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Is chatgpt a good nlg evaluator? a preliminary study <em>(Rating: 2)</em></li>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 2)</em></li>
                <li>Ranking generated summaries by correctness: An interesting but challenging application for natural language inference <em>(Rating: 2)</em></li>
                <li>Summac: Re-visiting nli-based models for inconsistency detection in summarization <em>(Rating: 2)</em></li>
                <li>Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8099",
    "paper_id": "paper-2db77485736cf29778a4464fe500a289bd46e7ac",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "Entailment - CoGenSum",
            "name_full": "ChatGPT (zero-shot CoT) balanced-accuracy on SUMMAC: CoGenSum",
            "brief_description": "Zero-shot chain-of-thought ChatGPT (gpt-3.5-turbo-0301) evaluated binary consistency (entailment) on the CoGenSum subset of the SUMMAC benchmark and is compared to existing automatic metrics and the dataset's human-derived labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
            "evaluation_task": "factual inconsistency detection (entailment inference / binary classification)",
            "dataset_name": "CoGenSum (SUMMAC benchmark)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "OpenAI gpt-3.5-turbo-0301 via API (InstructGPT + RLHF); used zero-shot and zero-shot chain-of-thought prompts",
            "human_evaluator_type": "ground-truth binary labels aggregated from dataset annotators (SUMMAC standardized labels)",
            "agreement_metric": "balanced accuracy",
            "agreement_score": 74.3,
            "reported_loss_aspects": "preference for lexical similarity; low sensitivity to inconsistent summaries; false reasoning; inadequate instruction-following",
            "qualitative_findings": "ChatGPT (ZS-COT) achieved 74.3% balanced accuracy, outperforming several baselines; however authors observe high specificity but low sensitivity (i.e., it often labels summaries as consistent especially when lexical overlap is high) and noted failures where missing modifiers (e.g., 'half of') are ignored.",
            "advantages_of_llm_judge": "Strong zero-shot performance without task-specific training; improved over many SOTA automatic metrics on this dataset",
            "experimental_setting": "Prompt: zero-shot CoT ('Explain your reasoning step by step then answer (yes or no)'); single API call per sample; only definitive 'consistent' judgments counted as positive",
            "uuid": "e8099.0"
        },
        {
            "name_short": "Entailment - XSumFaith",
            "name_full": "ChatGPT (zero-shot CoT) balanced-accuracy on SUMMAC: XSumFaith",
            "brief_description": "ChatGPT (ZS-COT) evaluated binary consistency on XSumFaith (SUMMAC) and compared to existing metrics and the dataset labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
            "evaluation_task": "factual inconsistency detection (entailment inference / binary classification)",
            "dataset_name": "XSumFaith (SUMMAC benchmark)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "OpenAI gpt-3.5-turbo-0301; zero-shot and zero-shot chain-of-thought prompts tested",
            "human_evaluator_type": "ground-truth binary labels aggregated from dataset annotators (SUMMAC standardized labels)",
            "agreement_metric": "balanced accuracy",
            "agreement_score": 63.1,
            "reported_loss_aspects": "reduced performance on abstractive summaries (lower lexical overlap); tendency to label abstractive cases as inconsistent; false reasoning",
            "qualitative_findings": "On XSumFaith ChatGPT's balanced accuracy (63.1%) is lower than the best compared model (SummaC_Conv 66.4%); authors attribute this to the abstractive nature of XSum summaries (lower lexical overlap) causing more 'inconsistent' predictions.",
            "advantages_of_llm_judge": "Still competitive in zero-shot setting; no task-specific training required",
            "experimental_setting": "Zero-shot CoT prompt; considered only explicit 'consistent' outputs as positive; per-sample API calls",
            "uuid": "e8099.1"
        },
        {
            "name_short": "Entailment - Polytope",
            "name_full": "ChatGPT (zero-shot CoT) balanced-accuracy on SUMMAC: Polytope",
            "brief_description": "ChatGPT (ZS-COT) evaluated binary consistency on the Polytope subset and compared to other automatic metrics using SUMMAC human labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
            "evaluation_task": "factual inconsistency detection (entailment inference / binary classification)",
            "dataset_name": "Polytope (SUMMAC benchmark)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "gpt-3.5-turbo-0301 (InstructGPT + RLHF); zero-shot & ZS-COT prompts",
            "human_evaluator_type": "ground-truth binary labels aggregated from dataset annotators (SUMMAC standardized labels)",
            "agreement_metric": "balanced accuracy",
            "agreement_score": 61.4,
            "reported_loss_aspects": "low sensitivity to inconsistencies; lexical-bias-driven errors",
            "qualitative_findings": "ChatGPT_ZS-COT scored 61.4% balanced accuracy on Polytope, below some QA-based metrics; authors note ChatGPT often retrieves consistent examples (high specificity) but misses many inconsistencies.",
            "advantages_of_llm_judge": "Zero-shot applicability and reasonable performance across datasets",
            "experimental_setting": "ZS-COT prompt; decision aggregation treating non-definitive labels (e.g., 'mostly consistent') as inconsistent",
            "uuid": "e8099.2"
        },
        {
            "name_short": "Entailment - FactCC",
            "name_full": "ChatGPT (zero-shot CoT) balanced-accuracy on SUMMAC: FactCC",
            "brief_description": "ChatGPT (ZS-COT) tested on FactCC subset of SUMMAC and compared to specialized metrics trained on synthetic data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
            "evaluation_task": "factual inconsistency detection (entailment inference / binary classification)",
            "dataset_name": "FactCC (SUMMAC benchmark)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "gpt-3.5-turbo-0301 via API; zero-shot & ZS-COT prompting",
            "human_evaluator_type": "ground-truth binary labels aggregated from dataset annotators (SUMMAC standardized labels)",
            "agreement_metric": "balanced accuracy",
            "agreement_score": 79.5,
            "reported_loss_aspects": "still misses some inconsistent cases; lexical-bias tendencies",
            "qualitative_findings": "ChatGPT_ZS-COT achieved 79.5% balanced accuracy (below SummaC_Conv at 89.5%) indicating competitive but not uniformly dominant performance; authors note substantial improvement from ZS to ZS-COT.",
            "advantages_of_llm_judge": "Good zero-shot ability without synthetic-data training required by some baselines",
            "experimental_setting": "ZS-COT prompt; thresholds for scoring models tuned on validation sets where applicable",
            "uuid": "e8099.3"
        },
        {
            "name_short": "Entailment - SummEval",
            "name_full": "ChatGPT (zero-shot CoT) balanced-accuracy on SUMMAC: SummEval",
            "brief_description": "Evaluation of ChatGPT (ZS-COT) vs human labels in SummEval consistency (binary after SUMMAC standardization).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
            "evaluation_task": "factual inconsistency detection (entailment inference / binary classification)",
            "dataset_name": "SummEval (SUMMAC benchmark)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "gpt-3.5-turbo-0301; used zero-shot and zero-shot CoT prompts",
            "human_evaluator_type": "3 expert annotators in original SummEval (averaged/standardized to binary in SUMMAC)",
            "agreement_metric": "balanced accuracy",
            "agreement_score": 83.3,
            "reported_loss_aspects": "occasionally fails to follow strict definition of consistency; may rate partial/mostly consistent as inconsistent in binary framing",
            "qualitative_findings": "ChatGPT_ZS-COT scored 83.3% balanced accuracy, slightly ahead of many baselines; authors point out unstable outputs like 'partially consistent' being treated as inconsistent in evaluation protocol, affecting comparisons.",
            "advantages_of_llm_judge": "High performance on this dataset and strong alignment with annotated labels",
            "experimental_setting": "ZS-COT prompt; binary aggregation rules: only explicit 'consistent' accepted as positive",
            "uuid": "e8099.4"
        },
        {
            "name_short": "Entailment - FRANK (SUMMAC)",
            "name_full": "ChatGPT (zero-shot CoT) balanced-accuracy on SUMMAC: FRANK",
            "brief_description": "ChatGPT (ZS-COT) evaluated against FRANK dataset human annotations (standardized binary labels in SUMMAC) and compared to other metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
            "evaluation_task": "factual inconsistency detection (entailment inference / binary classification)",
            "dataset_name": "FRANK (SUMMAC benchmark)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "gpt-3.5-turbo-0301; zero-shot & ZS-COT prompts",
            "human_evaluator_type": "annotators used in FRANK (dataset-level aggregation standardized in SUMMAC)",
            "agreement_metric": "balanced accuracy",
            "agreement_score": 82.6,
            "reported_loss_aspects": "false inferences; inadequate instruction adherence; sensitivity issues",
            "qualitative_findings": "ChatGPT_ZS-COT (82.6%) comparable to/above many baselines; analysis shows strong specificity but lower sensitivity—retrieves consistent examples well but misses inconsistent ones; examples show false inferential steps where ChatGPT treats 'bill passed lower house' as sufficient evidence that PM 'won the vote'.",
            "advantages_of_llm_judge": "Outperforms many prior automatic metrics; zero-shot usage",
            "experimental_setting": "ZS-COT prompt; per-sample independent API calls; considered partial labels as inconsistent for binary scoring",
            "uuid": "e8099.5"
        },
        {
            "name_short": "Summary Ranking (Falke 2019)",
            "name_full": "ChatGPT vs human accuracy on pairwise summary ranking (which of two is more consistent)",
            "brief_description": "ChatGPT (zero-shot) was used to choose the more consistent summary from two candidates (one faithful, one not) on the dataset introduced by Falke et al. (2019); authors compare ChatGPT accuracy to other models and a reported human baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
            "evaluation_task": "summary ranking (choose more consistent of two summaries)",
            "dataset_name": "Falke et al. (2019) ranking dataset (CNN/DM sampled; 373 samples)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "gpt-3.5-turbo-0301 via API; zero-shot prompt (direct A or B answer)",
            "human_evaluator_type": "human judgments reported in Falke et al. (2019) (used as baseline)",
            "agreement_metric": "accuracy (choose correct consistent summary)",
            "agreement_score": 85.2,
            "reported_loss_aspects": "occasionally inconsistent with binary entailment outputs; may produce non-binary outputs (both consistent/none) which are treated as failures",
            "qualitative_findings": "ChatGPT achieved 85.2% accuracy, slightly above the reported human baseline (83.9%) and above other automatic methods; authors note ChatGPT can detect subtle semantic differences when given pairwise contrastive context, even when it failed on single-sample entailment.",
            "advantages_of_llm_judge": "Outperformed humans and other metrics on this ranking dataset in zero-shot; able to pick up minor semantic differences when comparative context is provided",
            "experimental_setting": "Zero-shot prompt asking 'Answer (A or B):' given article sentence and two candidate summaries; outputs labeling both or neither as consistent counted as failure",
            "uuid": "e8099.6"
        },
        {
            "name_short": "Consistency Rating - FRANK (overall)",
            "name_full": "ChatGPT correlation with human ratings on FRANK (Pearson and Spearman reported)",
            "brief_description": "ChatGPT provided 1-10 consistency ratings and authors computed Pearson and Spearman correlations with human-annotated consistency scores on the FRANK dataset (overall and splits).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
            "evaluation_task": "consistency rating (1-10) alignment with human scores",
            "dataset_name": "FRANK (whole)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "gpt-3.5-turbo-0301 via API; zero-shot rating prompt requesting score 1-10",
            "human_evaluator_type": "FRANK annotators (sentence-level annotators aggregated to summary-level scores)",
            "agreement_metric": "Pearson r; Spearman rho",
            "agreement_score": 0.7,
            "reported_loss_aspects": "reduced correlation on abstractive (XSum) subsets; sometimes misapplies definition of 'consistency' focusing on coverage",
            "qualitative_findings": "Authors report strong Pearson (0.70) and Spearman (0.69) correlations between ChatGPT scores and human judgements on FRANK overall, far higher than baselines (many single-digit correlations). Performance declines when splitting FRANK into CNN/DM vs XSum outputs (higher on CNN/DM).",
            "advantages_of_llm_judge": "Substantially higher correlation with human ratings than prior automatic metrics, in a zero-shot setup",
            "experimental_setting": "Prompt: ask ChatGPT to 'Score the following summary ... from 1 to 10' with provided definition; per-sample single API calls; correlations computed against dataset human scores",
            "uuid": "e8099.7"
        },
        {
            "name_short": "Consistency Rating - FRANK (CNN/DM split)",
            "name_full": "ChatGPT correlation with human ratings on FRANK (CNN/DM subset)",
            "brief_description": "Correlation of ChatGPT 1-10 ratings with human judgements on the FRANK subset corresponding to CNN/DM-style summaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
            "evaluation_task": "consistency rating (1-10) alignment with human scores (subset)",
            "dataset_name": "FRANK (CNN/DM subset)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "gpt-3.5-turbo-0301; zero-shot rating prompt",
            "human_evaluator_type": "FRANK annotators (subset grouped by source model)",
            "agreement_metric": "Pearson r; Spearman rho",
            "agreement_score": 0.5,
            "reported_loss_aspects": "lower correlation on more abstractive summaries; lexical-similarity bias less problematic on extractive CNN/DM but still present",
            "qualitative_findings": "Pearson 0.50 and Spearman 0.46 on the CNN/DM split—stronger than baselines but lower than FRANK overall; authors attribute decline on XSum split to abstractive nature and lower lexical overlap.",
            "advantages_of_llm_judge": "Reasonable alignment with humans on less-abstractive (CNN/DM) summaries",
            "experimental_setting": "Same 1-10 rating prompt; FRANK split into CNN/DM vs XSum and correlations computed separately",
            "uuid": "e8099.8"
        },
        {
            "name_short": "Consistency Rating - FRANK (XSum split)",
            "name_full": "ChatGPT correlation with human ratings on FRANK (XSum subset)",
            "brief_description": "Correlation of ChatGPT 1-10 ratings with human judgements on FRANK subset of summaries generated by models trained on XSum (more abstractive).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
            "evaluation_task": "consistency rating (1-10) alignment with human scores (subset)",
            "dataset_name": "FRANK (XSum subset)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "gpt-3.5-turbo-0301; zero-shot rating prompt",
            "human_evaluator_type": "FRANK annotators (subset grouped by source model)",
            "agreement_metric": "Pearson r; Spearman rho",
            "agreement_score": 0.34,
            "reported_loss_aspects": "significant performance drop on abstractive summaries; lexical similarity bias; lower correlation with human ratings",
            "qualitative_findings": "Pearson 0.34 and Spearman 0.27 on XSum-produced summaries; authors note decline due to lower lexical overlap and increased abstractive rewriting.",
            "advantages_of_llm_judge": "Still outperforms many baseline metrics even on abstractive subsets",
            "experimental_setting": "1-10 rating prompt; correlations computed for the XSum subset",
            "uuid": "e8099.9"
        },
        {
            "name_short": "Consistency Rating - SummEval",
            "name_full": "ChatGPT correlation with expert ratings on SummEval (consistency aspect)",
            "brief_description": "ChatGPT 1-10 consistency ratings compared to SummEval human expert annotations (originally 5-point scale, averaged then standardized).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
            "evaluation_task": "consistency rating (scale vs expert annotators)",
            "dataset_name": "SummEval (consistency scores)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "gpt-3.5-turbo-0301; zero-shot rating prompt (1-10)",
            "human_evaluator_type": "3 expert annotators per summary (SummEval)",
            "agreement_metric": "Pearson r; Spearman rho",
            "agreement_score": 0.49,
            "reported_loss_aspects": "inadequate prompt adherence in some cases (rating based on coverage rather than strict entailment); tendency to undervalue short but accurate summaries",
            "qualitative_findings": "Pearson 0.49 and Spearman 0.35 correlations with expert averaged scores; authors present an example where ChatGPT rated a clearly-consistent short summary 1/10 while experts gave 5/5, illustrating prompt-understanding failures.",
            "advantages_of_llm_judge": "Higher correlation with expert judgment than prior automatic metrics in this study",
            "experimental_setting": "Prompt asks for 1-10 consistency; SummEval original 5-point expert ratings averaged and compared to ChatGPT scores via correlation metrics",
            "uuid": "e8099.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "rating": 2
        },
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 2
        },
        {
            "paper_title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
            "rating": 2
        },
        {
            "paper_title": "Summac: Re-visiting nli-based models for inconsistency detection in summarization",
            "rating": 2
        },
        {
            "paper_title": "Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics",
            "rating": 2
        }
    ],
    "cost": 0.01820975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ChatGPT as a Factual Inconsistency Evaluator for Text Summarization</h1>
<p>Zheheng Luo, Qianqian Xie, S Sophia Ananiadou<br>Department of Computer Science, The University of Manchester<br>{zheheng.luo, qianqian.xie, sophia.ananiadou}@manchester.ac.uk</p>
<h4>Abstract</h4>
<p>The performance of text summarization has been greatly boosted by pre-trained language models. A main concern of existing methods is that most generated summaries are not factually inconsistent with their source documents. To alleviate the problem, many efforts have focused on developing effective factuality evaluation metrics based on natural language inference, question answering, and syntactic dependency et al. However, these approaches are limited by either their high computational complexity or dependence on annotated data. Most recently, large language models(LLMs) such as ChatGPT have shown excellent performance in not only text generation but also language comprehension. In this paper, we particularly explore ChatGPT's ability to evaluate factual inconsistency under a zero-shot setting by examining it on both coarse-grained and fine-grained evaluation tasks including binary entailment inference, summary ranking, and consistency rating. Experimental results indicate that ChatGPT generally outperforms previous evaluation metrics across the three tasks, indicating its great potential for factual inconsistency evaluation. However, a closer inspection of ChatGPT's output reveals certain limitations including its preference for more lexically similar candidates, false reasoning, and inadequate understanding of instructions.</p>
<h2>1 Introduction</h2>
<p>Recently, pre-trained language models have greatly improved the performance of automatic text summarization (Liu and Lapata, 2019; Lewis et al., 2020; Zhang et al., 2020). However, a major concern that has limited existing state-of-the-art text summarization methods is factual inconsistency, namely, the generated summaries containing information that is not entailed by input documents ${ }^{1}$ (Kryściński et al., 2020; Maynez et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2020). To fill the gap, significant efforts have been made in developing automatic evaluation metrics for assessing the factuality of generated summaries, such as semi-supervised method FactCC (Kryściński et al., 2020), question-answering based approach FEQA (Wang et al., 2020) and QuestEval (Scialom et al., 2021), and natural language inference (NLI) based method SummaC (Laban et al., 2022). Nevertheless, existing evaluation metrics either have high computational complexity which requires training on a huge amount of data or rely on multi-model combined pipelines, putting in more uncertainties during inferences. Moreover, evaluations based on these metrics exhibit limited agreement with human assessments (Pagnoni et al., 2021). Inspired by the ability of pre-trained language models (PLMs) on natural language understanding and generation, a few efforts have been devoted to building data and computationally efficient evaluation metrics based on PLMs like BARTScore (Yuan et al., 2021).</p>
<p>Most recently, large language models (LLMs), such as GPT-3 (Brown et al., 2020), InstructGPT (Ouyang et al., 2022), PaLM (Chowdhery et al., 2022), and BLOOM (Scao et al., 2022), have dwarfed small scale fine-tuned models in various natural language processing tasks often requiring only few-shot or zero-shot learning. These LLMs have demonstrated exceptional performance not only in natural language understanding and generation but also in their ability to perform inference and reasoning tasks. ${ }^{\text {a }}$ Specifically, equipped with explicitly designed prompts, LLMs can better solve a range of various reasoning tasks in terms of arithmetics, symbolic, and logic (Kojima et al., 2022; Wei et al., 2022). Moreover, the most recent effort ChatGPT (OpenAI, 2022) in particular has been proven to obtain strong natural language inference ability, surpassing fine-tuned pre-trained language models (PLMs) on several datasets (Zhong et al., 2023). As a result, researchers have paid</p>
<p>closer attention to using large language models (LLMs) to evaluate generated text. <em>Kocmi and Federmann (2023a)</em> investigated the use of rating-based prompts in translation evaluation and achieved better accuracy compared to other metrics across three language pairs. Inspired by this work, <em>Wang et al. (2023b)</em> extends the method into a broader natural language generation field including summarisation, where ChatGPT shows dominating alignment with human rating on four attributes including coherence, relevance, fluency, and consistency. However, their experiments only use a single summarisation evaluation dataset and rather focus on exploring ChatGPT to evaluate the overall quality of generated summaries. In addition, they solely framed the evaluation as a marking task and compared the results with only general text generation metrics such as ROUGE <em>Lin (2004)</em> and BERTScore <em>Zhang et al. (2023a)</em>, which have been proven to be not effective in assessing factual consistency <em>Maynez et al. (2020)</em>. Metrics proposed specifically for assessing inconsistency such as FactCC, DAE <em>Goyal and Durrett (2020)</em>, SummaC have not been examined, leaving a huge gap for a thorough exploration of using ChatGPT to assess the factual consistency in text summarisation.</p>
<p>To fill the gap, in this paper, we conduct a preliminary study of how ChatGPT can perform in both coarse-grained and fine-grained factual inconsistency evaluation from three tasks including inconsistency detection as entailment inference (EI), consistency comparison as summary ranking, and quantitative judgement as consistency rating. We design different prompts on both zero-shot and zero-shot chain-of-thought (CoT) <em>Kojima et al. (2022)</em> to explore the factuality assessment ability of ChatGPT. We conduct experiments on the benchmark of the EI-based inconsistency detection task including six large standardized datasets, and existing datasets on the other two tasks, and compare the results with SOTA evaluation methods. From experimental results and analysis, we have the following findings:</p>
<ol>
<li>ChatGPT shows great potential for evaluation factuality of text summarisation under the zero-shot setting and outperforms previous SOTA evaluation methods on most datasets across three tested tasks.</li>
<li>Though showing remarkable performance measured by numeric metrics, ChatGPT is found to have the preference to predict a document and a claim is consistent when the lexical similarity is high without considering the semantic entailment between them. Moreover, evidence of ChatGPT conducting false inferences has been observed, revealing the limitation of ChatGPT's language reasoning ability.</li>
<li>Despite effectively instructing ChatGPT to detect inconsistency, the tested prompts are not able to keep the output constantly sticking to the given requirements, indicating the insufficient prompting of ChatGPT.</li>
</ol>
<p>To the best of our knowledge, we are the first to systematically explore ChatGPT's ability in evaluating factual consistency for text summarization. Overall, our results show a comparable if not better performance of ChatGPT than SOTA evaluation metrics, but concerns remain on lexical biases, false reasoning, and inadequate alignment which are expected to be addressed to improve its reliability.</p>
<h2>2 Related Work</h2>
<h3>2.1 Factuality Evaluation in Text Summarization</h3>
<p>Existing factuality evaluation metrics generally can be classified into unsupervised and semi-supervised methods. Unsupervised evaluation metrics generally include information extraction (IE) based methods, natural language inference (NLI) based methods, and question answering (QA) based methods. <em>Goodrich et al. (2019)</em> proposed the model-based factuality evaluation metric to calculate the overlap of relation tuples (subject, relation, object) that are extracted from generated summaries and the ground truth by the information extraction (IE) model. <em>Nan et al. (2021)</em> proposed the new evaluation metric assessing the entity-level factuality consistency of generated summaries.</p>
<p>Besides the IE-based methods, natural language inference (NLI) is also explored for factuality evaluation by assessing whether the generated summary is entailed by the input document. <em>Falke et al. (2019a)</em> found the factuality evaluation methods trained on the NLI datasets have a poor ability to the assessment of text summarization. <em>Mishra et al. (2021)</em> further found that the poor performance of the evaluation methods training with the NLI datasets is caused by the short length of premises in NLI datasets. Most recently, <em>Laban et al. (2022)</em> revisited the use of NLI in inconsistency detection by</p>
<p>calculating the factuality score based on sentence pairs, and proposed the novel benchmark SUMMAC (Summary Consistency) with six datasets. SUMMAC is used in our experiments.</p>
<p>Moreover, there is also question answeringbased metrics such as FEQA (Durmus et al., 2020a), QAGS (Wang et al., 2020), and QuestEval (Scialom et al., 2021), by assessing the alignment of the generated answer based on the generated summary and the source, with the given question. Different from unsupervised NLI-based methods, the semi-supervised methods further utilize the synthetic data from text summarization for weakly supervised learning, such as FactCC (Kryściński et al., 2020). However, these methods are usually computationally expensive or rely on annotated data (Huang et al., 2021). Inspired by the effectiveness of PLMs, there are efforts on developing factuality evaluation metrics based on the likelihoods of PLMs, that are computation and dataefficient such as BARTScore (Yuan et al., 2021) and T5Score (Qin et al., 2022).</p>
<h3>2.2 ChatGPT for Natural Language Processing</h3>
<p>Most recently, many efforts have explored the zeroshot ability of ChatGPT on various natural language processing tasks (Jiao et al., 2023; Zhong et al., 2023; Qin et al., 2023; Bang et al., 2023; Yang et al., 2023a). ChatGPT has been proven to exhibit good performance on machine translation (Jiao et al., 2023). On the GLUE benchmark, Zhong et al. (2023) has found ChatGPT shows significantly better performance on inference tasks, has comparable performance on sentiment analysis and question-answering tasks, and has poor performance on paraphrase and similarity tasks when compared with 4 representative BERT-based fintuning methods. Qin et al. (2023) further shows ChatGPT has superior performance on reasoningrequired tasks including dialogue tasks, natural language inference tasks, and question-answering tasks than GPT-3.5, and has worse performance on the summarization task than GPT-3.5. ChatGPT and GPT-3.5 have comparable performance on sentiment analysis. Bang et al. (2023) shown ChatGPT outperforms SOTA zero-shot methods in 9/13 NLP datasets and has poor performance on low-resource languages such as Marathi, Sundanese, and Buginese. Yang et al. (2023b) and Wang et al. (2023a) explored the query and aspect-
based text summarization and cross-lingual summarization with ChatGPT, where it shows comparable performance with the fine-tuning-based methods. (Soni and Wade, 2023) conducted a human evaluation and found reviewers struggle to distinguish hand-written summaries against generated ones from ChatGPT. Wang et al. (2023b) examined the ability of ChatGPT on evaluating natural language generation (NLG) tasks such as summarization, story generation, and data-to-text tasks. ChatGPT shows great potential as the NLG metric, whose evaluation results have a high correlation with human judgment. However, they only utilized one summarisation dataset and focused on exploring ChatGPT to evaluate the relevance by comparing it with non-factuality evaluation metrics such as ROUGE and BERTScore, leaving a huge gap for a thorough exploration of the ability of ChatGPT to assess the factual consistency in text summarisation.</p>
<h2>3 ChatGPT as a Factual Inconsistency Evaluator</h2>
<p>In this section, we introduce the details of three different tasks for detecting inconsistency with ChatGPT including the prompt designing, evaluation setting, tested datasets, and baseline models.</p>
<h3>3.1 Entailment Inference</h3>
<p>Evaluation Setting. Inconsistency evaluation of the generated summary can be cast as a binary natural language inference classification, in which the evaluation model is solely required to assess if the summary is consistent with the source document rather than rating the consistent levels (Laban et al., 2022). Under this framework, two parameters are needed for the prompts: source document and summary. We provide ChatGPT with the question including the source document and the corresponding generated summary and ask it to answer yes or no to infer the consistency between the source document and the corresponding generated summary, and then we collect the decisions from the outputs and aggregate the results.
Prompts. We experiment with two different zeroshot prompts in the NLI setting. The first one is based on direct assessment by directly asking ChatGPT to answer yes or no given the question. Another is based on zero-shot Chain-of-Thought inspired by previous work (Kojima et al., 2022) of adding "let's think step by step" in prompt to en-</p>
<p>courage LLMs unfolding a chain-of-thought style reasoning process, which has been proved to be effective on several reasoning tasks. We follow the approach to create the second prompt. The zero-shot template is shown below:</p>
<p>Decide if the following summary is consistent with the corresponding article. Note that consistency means all information in the summary is supported by the article.</p>
<p>Article: [Article]
Summary: [Summary]
Answer (yes or no):
The zero-shot CoT template is:
Decide if the following summary is consistent with the corresponding article. Note that consistency means all information in the summary is supported by the article.</p>
<p>Article: [Article]
Summary: [Summary]
Explain your reasoning step by step then answer (yes or no) the question:</p>
<p>When processing the responses, we only consider solid judgment like "the summary is consistent with the article" as consistency, claims such as "partially consistent" or 'mostly consistent' are all deemed as inconsistent. We also tried to use few-shot prompts. However, we found the performance unstable when changing the label, order, and amount of examples, so we decide to leave it for further exploration.
Datasets We evaluate ChatGPT's performance on the SUMMAC benchmark (Laban et al., 2022) which includes six largest summary inconsistency detection datasets FactCC (Kryściński et al., 2020), CoGenSumm (Falke et al., 2019a), XSumFaith (Maynez et al., 2020), SummEval (Fabbri et al., 2021), FRANK (Pagnoni et al., 2021), and Polytope (Huang et al., 2020). Notably, not all the datasets in the SUMMAC benchmark are built for binary consistency classification. For example, in SummEval (Fabbri et al., 2021), generated summaries are marked on consistency over a range from 1-5 points. SUMMAC standardizes the six datasets into a binary classification format where each instance contains a triplet of (document, summary, label). The label is either consistent or inconsistent. Moreover, they manually created validation and test split for datasets where a such split is not conducted and computed the inter-annotator agreement for data with multiple annotators. The statistics of the benchmark are shown in Table 1.
Baseline Models. We compare ChatGPT's performance with the following methods:</p>
<ul>
<li>NER Overlap uses the named entity recognition (NER) model to detect inconsistency by examining if an entity in the summary is in the document (Laban et al., 2021). The tested model considers only a subset of entity types such as PERSON, LOCATION, ORGANIZATION, etc.</li>
<li>MNLI-doc fine-tunes a Roberta model (Liu et al., 2019) on the MNLI dataset (Williams et al., 2018) and labels the documentsummary pair by the predicted probability of entailment.</li>
<li>FactCC (Kryściński et al., 2020) is a Roberta model fine-tuned on data synthesized by corrupting sentences in the original documents as inconsistent candidates.</li>
<li>DAE (Goyal and Durrett, 2020) is a parsingbased model evaluating inconsistency by examining the entailment of individual dependency arcs.</li>
<li>FEQA (Durmus et al., 2020b) first generates question-answer pairs from candidate summaries, then compare the answers extracted from the source documents by asking the same questions. Then the answer sets are compared to determine the consistency.</li>
<li>QuestEval (Scialom et al., 2021) extends the methods above by adding an information recall score to a QA-based metric.</li>
<li>SummaC (Laban et al., 2022) builds an NLI matrix by splitting the document and summary into sentence sets, then predicts a score for each sentence pair in the matrix. SummaC zero-shot $\left(\right.$ SummaC $<em _conv="{conv" _text="\text">{z s}$ ) first obtain the maximum along the columns then average over to get a final consistency score. SummaC convolution (SummaC $C</em>$ ) instead trains a convolution layer to predict a score for each column and then uses the mean output as the summary-level score.}</li>
</ul>
<p>Detailed implementations of the above models used to compare can be found in (Laban et al.,</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Valid.</th>
<th>Test</th>
<th>%Positive</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>size</td>
<td>size</td>
<td></td>
<td></td>
</tr>
<tr>
<td>CoGenSumm</td>
<td>1281</td>
<td>400</td>
<td>49.8</td>
<td>C</td>
</tr>
<tr>
<td>XSumFaith</td>
<td>1250</td>
<td>1250</td>
<td>10.2</td>
<td>X</td>
</tr>
<tr>
<td>Polytope</td>
<td>634</td>
<td>634</td>
<td>6.6</td>
<td>C</td>
</tr>
<tr>
<td>FactCC</td>
<td>931</td>
<td>503</td>
<td>85.0</td>
<td>C</td>
</tr>
<tr>
<td>SummEval</td>
<td>850</td>
<td>850</td>
<td>90.6</td>
<td>C</td>
</tr>
<tr>
<td>FRANK</td>
<td>671</td>
<td>1575</td>
<td>33.2</td>
<td>C+X</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of datasets in SUMMAC Benchmark.
2022). For scoring models, the threshold is selected using the validation set and allowed to vary over different datasets.
Metric. Due to the unbalanced distribution of positive and negative samples in the testing sets, we choose balanced accuracy (Brodersen et al., 2010) as the main metric since it is more sensitive to predictions difference for data of smaller proportions. Balanced accuracy is defined as the following:</p>
<p>$$
b A C C=\frac{1}{2} *\left(\frac{T P}{T P+F N}+\frac{T N}{T N+F P}\right)
$$</p>
<p>The first term in the equation is sensitivity, which represents the recall of true positives while the next one is specificity standing for the recall of true negatives. We specifically counted the two submetrics to analyze ChatGPT's behavior.</p>
<h3>3.2 Summary Ranking</h3>
<p>Evaluation Setting Except binary NLI, a model's awareness of factual inconsistency can also be tested on how whether it can rank a consistent summary over an inconsistent one. In this section, we introduce another evaluation task Summary Ranking which is introduced in Falke et al. (2019a) and has been tested in other previous work. Specifically, the model will be asked to choose the consistent one over two candidate summaries (one is faithful, the other one is not) given the source document.
Prompts We use a zero-shot prompt which directly asks ChatGPT to answer which sentence out of the two candidates is more consistent with the given article sentence.</p>
<p>Decide which of the following summary is more consistent with the article sentence. Note that consistency means all information in the summary is supported by the article.</p>
<p>Article Sentence: [article]
Summary A: [correct summary]
Summary B: [incorrect summary]
Answer (A or B):</p>
<p>Dataset Here we use the dataset built by Falke et al. (2019a) which contains 373 samples, each containing an input source document from CNN/DM (Nallapati et al., 2016) and two summary sentences covering the same content. One of the summary sentences are consistent with the article while the other is inconsistent.
Baseline Models We compare other evaluation models that reported their performance on this dataset including the aforementioned FactCC (Kryściński et al., 2020), MNLI-doc, DAE (Goyal and Durrett, 2020) and a human judgement from (Falke et al., 2019a).
Metric We report the accuracy of models successfully choosing consistent summary over inconsistent one. Specifically, when collecting responses from ChatGPT, we only deem claims that confirm the correct sentence is consistent as correct. Outputs alleging both candidate sentences are consistent or inconsistent are rendered as failures.</p>
<h3>3.3 Consistency Rating</h3>
<p>Evaluation Setting. Recently, several studies have found when given accordingly request prompts, LLMs are able to mark the quality of generated text from different aspects (Kocmi and Federmann, 2023b; Fu et al., 2023; Wang et al., 2023c). These scores show high correlations with human assessment, suggesting the potential of ChatGPT in predicting fine-grained consistency levels for summarisation. Moreover, in the experiments of the NLI task in Section 3.1, we found that part of the output judgments is "partially consistent" or "mostly consistent", indicating ChatGPT's awareness of different inconsistency degrees. Therefore, we apply the consistency rating task on ChatGPT by asking it to mark the consistency of a summary with the reference to its source document on a scale from 1-10 points, where 1 point stands for total inconsistency, and 10 represents full consistency.
Prompts. Following Kocmi and Federmann (2023b)'s approach, we design a prompt that requests ChatGPT to evaluate the consistency of a candidate summary w.r.t the source article in a [110] scale:</p>
<p>Score the following summary given the corresponding article with respect to consistency from 1 to 10 . Note that consistency measures how much information included in the summary is present in the source article. 10 points indicate</p>
<p>the summary contains only statements that are entailed by the source document.
[Summary]:</p>
<p>The definition of consistency is added for the model to better understand the aspect it is to rate.
Datasets. The original versions of SummEval and FRANK datasets are used on this task given there are detailed consistency scores in their annotations. In SummEval, 1600 summaries were labeled using a 5-point Likert scale along four categories: coherence, consistency, fluency, and relevance by 3 expert annotators. We average the points in the consistency aspect as the final score. FRANK has a binary consistency score for each sentence in a summary labeled by annotators, then aggregates a summary-level score from 0-1, resulting in 2250 marked summaries in total.
Baseline Models We compare other evaluation models that reported their performance on this dataset including the aforementioned FactCC, FEQA, DAE and QAGS (Wang et al., 2020), which is a QA-based faithfulness evaluation method.
Metrics To evaluate to what extent the examined models align with human judgment. Two widelyused correlation measures are adopted: (1) Spearman correlation (Zar, 2005) assesses the monotonic relationships between two variables; (2) Pearman correlation (Mukaka, 2012) measures the linear relationships between two sets of data; (3) Kendall's Tau (Kendall, 1938)evaluates the ordinal association between two measured quantities.</p>
<h2>4 Experiment</h2>
<p>We conduct our experiments using the API of ChatGPT (gpt-3.5-turbo-0301) which is trained based on InstructGPT (Ouyang et al., 2022) with reinforce learning from human feedback (RLHF). To avoid the effects of historical dialogues, we sent each request individually to obtain the response.</p>
<h3>4.1 Entailment Inference</h3>
<p>The full results of the entailment inference task are shown in Table 2. Overall, ChatGPT is able to achieve comparable performance or even better performance compared to the previous state-of-the-art evaluation models without training on relevant tasks, demonstrating the potential of ChatGPT-like LLMs on detecting inconsistency
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The results of sensitivity and specificity of ChatGPT ${ }<em _ZS="{ZS" _text="\text">{\text {ZS-COT }}$.
between two pieces of text in a zero-shot setting. Specifically, ChatGPT with zero-shot CoT prompt produces the best results and outperforms the previous SOTA method SummaC ${ }</em>$ by $3.9 \%$, $1.6 \%$ and $1.0 \%$ on CoGenSum, SummEval, and FRANK datasets correspondingly. It remains comparable to the best models on the rest three datasets including XsumFaith ( $63.1 \%$ compared to SummaC $}<em _Conv="{Conv" _text="\text">{\text {Conv }}$ with $66.4 \%$ ), Polytope ( $61.4 \%$ compared to QuestEval with $70.3 \%$ ), FactCC ( $79.5 \%$ compared to SummaC $</em>}}$ with $89.5 \%$ ). In almost all datasets, the ChatGPT ${ <em _ZS="{ZS" _text="\text">{\text {ZS-COT }}$ which guides the ChatGPT with the chain-of-thought prompt has significantly better performance than ChatGPT ${ }</em>}}$, In detail, ChatGPT ${ <em _ZS="{ZS" _text="\text">{\text {ZS-COT }}$ outperforms ChatGPT ${ }</em>$ by $11.0 \%, 4.5 \%, 4.8 \%, 6.8 \%$ and $1.7 \%$ on the CoGenSum, Polytope, FactCC, SummEval, and FRANK datasets correspondingly. It shows great potential to better explore the factuality evaluation ability of ChatGPT by prompt engineering in the future.}</p>
<p>To further investigate ChatGPT's performance in consistent and inconsistent instances, we break the balanced accuracy results of ChatGPT ${ }_{\text {ZS-COT }}$ into sensitivity (positive recall) and specificity (negative recall), the comparison is in Fig 1. In five out of the total six datasets, ChatGPT can successfully retrieve more than $95 \%$ consistent summaries (high negative recall namely specificity), while performing rather poorly on identifying all the inconsistent ones (low positive recall namely sensitivity). Based on this observation, we assume that during inference, ChatGPT might still rely more on semantic similarity to make its decision on consistency detection since most of the candidate summaries are lexically close to sentences in the source arti-</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>SUMMAC Benchmark Datasets</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>CoGenSum</td>
<td>XsumFaith</td>
<td>Polytope</td>
<td>FactCC</td>
<td>SummEval</td>
<td>FRANK</td>
</tr>
<tr>
<td>NER Overlap</td>
<td>53.0</td>
<td>63.3</td>
<td>52.0</td>
<td>55.0</td>
<td>56.8</td>
<td>60.9</td>
</tr>
<tr>
<td>MNLI-doc</td>
<td>57.6</td>
<td>57.5</td>
<td>61.0</td>
<td>61.3</td>
<td>66.6</td>
<td>63.6</td>
</tr>
<tr>
<td>FactCC-CLS</td>
<td>63.1</td>
<td>57.6</td>
<td>61.0</td>
<td>75.9</td>
<td>60.1</td>
<td>59.4</td>
</tr>
<tr>
<td>DAE</td>
<td>63.4</td>
<td>50.8</td>
<td>62.8</td>
<td>75.9</td>
<td>70.3</td>
<td>61.7</td>
</tr>
<tr>
<td>FEQA</td>
<td>61.0</td>
<td>56.0</td>
<td>57.8</td>
<td>53.6</td>
<td>53.8</td>
<td>69.9</td>
</tr>
<tr>
<td>QuestEval</td>
<td>62.6</td>
<td>62.1</td>
<td>70.3</td>
<td>66.6</td>
<td>72.5</td>
<td>82.1</td>
</tr>
<tr>
<td>SummaC ${ }_{\text {ZS }}$</td>
<td>70.4</td>
<td>58.4</td>
<td>62.0</td>
<td>83.8</td>
<td>78.7</td>
<td>79.0</td>
</tr>
<tr>
<td>SummaC $_{\text {Conv }}$</td>
<td>64.7</td>
<td>66.4</td>
<td>62.7</td>
<td>89.5</td>
<td>81.7</td>
<td>81.6</td>
</tr>
<tr>
<td>ChatGPT ${ }_{\text {ZS }}$</td>
<td>63.3</td>
<td>64.7</td>
<td>56.9</td>
<td>74.7</td>
<td>76.5</td>
<td>80.9</td>
</tr>
<tr>
<td>ChatGPT ${ }_{\text {ZS-COT }}$</td>
<td>74.3</td>
<td>63.1</td>
<td>61.4</td>
<td>79.5</td>
<td>83.3</td>
<td>82.6</td>
</tr>
</tbody>
</table>
<p>Table 2: Balanced accuracy results of inconsistency detect models on the test set of SummaC. Results of baselines are referenced from the paper (Laban et al., 2022).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Ranking Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td>FactCC</td>
<td>70.0</td>
</tr>
<tr>
<td>MNLI-doc</td>
<td>78.3</td>
</tr>
<tr>
<td>Rule-based dependency</td>
<td>74.8</td>
</tr>
<tr>
<td>DAE</td>
<td>83.6</td>
</tr>
<tr>
<td>Human</td>
<td>83.9</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>$\mathbf{8 5 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of models on the summary ranking task. Results of baselines are reported in Goyal and Durrett (2020).
cles, causing its vulnerability in finding these trivial modifications in inconsistent summaries that changes the meaning of the source document. This could be further demonstrated in ChatGPT's reverse performance on the two types of candidate summaries in the XSumFaith dataset which contains summaries generated by models trained on the XSum dataset in Table 2. Previous works (Durmus et al., 2020a) have shown that the generated summaries are highly affected by training data and models trained on CNN/DM produce nearly extractive summaries while the same model trained on XSum will give significantly more abstractive ones. Abstrativeness brings the decline of lexical similarity between the candidate summary and the source document which might be the main reason why in XSumFaith, ChatGPT tends to predict more cases as inconsistent.</p>
<h3>4.2 Summary Ranking</h3>
<p>The results of the summary ranking task are shown in Table 3. It shows that ChatGPT without any in-context learning can outperform not only existing methods but also a human assessment reported in Falke et al. (2019b). Notably, the ranking dataset is sampled from the output of models trained on CNN/DM. Therefore, the candidate summaries are mostly identical to some sentences in the source document, the inconsistent ones tend to contain minor adjustments corrupting the meaning like deleting modifiers like "half of" as shown in Figure 2. Though we conclude from Section 4.1 that ChatGPT relies heavily on lexical similarity to decide the consistency degree of sentences, in this summary ranking task, we see that ChatGPT can detect the trivial semantic differences even when given two highly similar candidates and pick out the consistent one. For example, in the second case of Figure 2, ChatGPT can correctly assess that sentence B is more consistent with the input article, given the highly lexical similarity between sentence B and sentence A. In our manual inspection, we found that ChatGPT is able to point out the inconsistency in some cases where it failed in the entailment inference when ranking them compared to their consistent counterparts. As shown in the first case of Figure 2, ChatGPT failed in detecting the inconsistency of the summary with the input article in the entailment inference task. While it can correctly pick out the more consistent one when given two summaries with highly lexical similarity in the summary ranking task, as shown in the second case of Figure 2. This indicates the importance of prompt engineering with useful contexts in better triggering ChatGPT's capability.</p>
<table>
<thead>
<tr>
<th></th>
<th>FRANK</th>
<th></th>
<th>FRANK(CNN/DM)</th>
<th></th>
<th>FRANK(XSum)</th>
<th></th>
<th>SummEval</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Metrics</td>
<td>Pear.</td>
<td>Spear.</td>
<td>Pear.</td>
<td>Spear.</td>
<td>Pear.</td>
<td>Spear.</td>
<td>Pear.</td>
<td>Spear.</td>
</tr>
<tr>
<td></td>
<td>$\rho$</td>
<td>$r$</td>
<td>$\rho$</td>
<td>$r$</td>
<td>$\rho$</td>
<td>$r$</td>
<td>$\rho$</td>
<td>$r$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>FEQA</td>
<td>0.00</td>
<td>0.01</td>
<td>-0.01</td>
<td>-0.01</td>
<td>0.02</td>
<td>0.07</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>QAGS</td>
<td>0.06</td>
<td>0.08</td>
<td>0.13</td>
<td>0.09</td>
<td>-0.02</td>
<td>0.01</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>DAE</td>
<td>0.16</td>
<td>0.14</td>
<td>0.25</td>
<td>0.24</td>
<td>0.04</td>
<td>$\mathbf{0 . 2 8}$</td>
<td>0.20</td>
<td>0.27</td>
</tr>
<tr>
<td>FactCC</td>
<td>0.20</td>
<td>0.30</td>
<td>0.36</td>
<td>0.33</td>
<td>0.07</td>
<td>0.25</td>
<td>0.32</td>
<td>0.34</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>$\mathbf{0 . 7 0}$</td>
<td>$\mathbf{0 . 6 9}$</td>
<td>$\mathbf{0 . 5 0}$</td>
<td>$\mathbf{0 . 4 6}$</td>
<td>$\mathbf{0 . 3 4}$</td>
<td>0.27</td>
<td>$\mathbf{0 . 4 9}$</td>
<td>$\mathbf{0 . 3 5}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Pearson correlation, and spearman rank correlation coefficients between human judgements and evaluation scores of different methods.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Decide</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">following</span><span class="w"> </span><span class="nv">summary</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">consistent</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">corresponding</span>
<span class="nv">article</span>.<span class="w"> </span><span class="nv">Note</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">consistency</span><span class="w"> </span><span class="nv">means</span><span class="w"> </span><span class="nv">all</span><span class="w"> </span><span class="nv">information</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">summary</span><span class="w"> </span><span class="nv">is</span>
<span class="nv">supported</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">article</span>.
<span class="nv">Article</span>:<span class="w"> </span><span class="err">&quot;Nearly half of the Radio Rentals customers have been found to</span>
<span class="err">be on welfare payments, and they are not just renting household</span>
<span class="err">necessities. The ABC have reported that those who receive Centrelink</span>
<span class="err">payments made up half of Radio Rental&#39;s income last year ..... with no</span>
<span class="nv">immediate</span><span class="w"> </span><span class="nv">deposits</span><span class="w"> </span><span class="nv">required</span>.<span class="err">&quot;</span>
<span class="nv">Summary</span>:<span class="w"> </span><span class="err">&quot;the abc have reported that those who receive centrelink</span>
<span class="nv">payments</span><span class="w"> </span><span class="nv">made</span><span class="w"> </span><span class="nv">up</span><span class="w"> </span><span class="nv">radio</span><span class="w"> </span><span class="nv">rental</span><span class="err">&#39;s income last year.&quot;</span>
<span class="err">The summary is consistent with the corresponding article X</span>
<span class="err">Decide which of the following sentence is more consistent with</span>
<span class="err">the article. Note that consistency means all information in the claim is</span>
<span class="err">supported by the article.</span>
<span class="err">Article: &quot;Nearly half of the Radio Rentals customers have been found to</span>
<span class="err">be on welfare payments, and they are not just renting household</span>
<span class="err">necessities. The ABC have reported that those who receive Centrelink</span>
<span class="nv">payments</span><span class="w"> </span><span class="nv">made</span><span class="w"> </span><span class="nv">up</span><span class="w"> </span><span class="nv">half</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">Radio</span><span class="w"> </span><span class="nv">Rental</span><span class="err">&#39;s income last year...... with no</span>
<span class="err">immediate deposits required.&quot;</span>
<span class="err">Sentence A: &quot;the abc have reported that those who receive centrelink</span>
<span class="nv">payments</span><span class="w"> </span><span class="nv">made</span><span class="w"> </span><span class="nv">up</span><span class="w"> </span><span class="nv">radio</span><span class="w"> </span><span class="nv">rental</span><span class="err">&#39;s income last year.&quot;</span>
<span class="err">Sentence B: &quot;those who receive centrelink payments made up half of</span>
<span class="nv">radio</span><span class="w"> </span><span class="nv">rental</span><span class="err">&#39;s income last year.&quot;</span>
<span class="err">Sentence B: &quot;those who receive Centrelink payments made up half of</span>
<span class="nv">Radio</span><span class="w"> </span><span class="nv">Rental</span><span class="err">&#39;s income last year&quot; is more consistent with the article. The</span>
<span class="err">article clearly states that &quot;those who receive Centrelink payments made</span>
<span class="nv">up</span><span class="w"> </span><span class="nv">half</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">Radio</span><span class="w"> </span><span class="nv">Rental</span><span class="err">&#39;s income last year&quot; and that &quot;$90 million came</span>
<span class="err">from the bill-managing Centrelink service, Centrepay&quot;. Sentence A is</span>
<span class="err">similar but is missing the crucial information about the proportion of</span>
<span class="err">income that came from Centrelink payments. O</span>
</code></pre></div>

<p>Figure 2: ChatGPT's actions when given the same source document and an inconsistent summary but with and without a consistent one. The red underlined text in the article is content highly related to the candidate summaries.</p>
<h3>4.3 Consistency Rating</h3>
<p>We further show the performance of all methods on the consistency rating task in Table 4, where we compare the correlations of their rating results with human judgement. Still, without in-context training, ChatGPT outperforms other consistency metrics by aligning closer to human assessments. Especially in the whole FRANK dataset, ChatGPT leads other metrics by a large margin, emphasising its superior ability in measuring the consistency</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Score</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">following</span><span class="w"> </span><span class="nv">summary</span><span class="w"> </span><span class="nv">given</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">corresponding</span><span class="w"> </span><span class="nv">article</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">respect</span><span class="w"> </span><span class="nv">to</span>
<span class="nv">consistency</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="mi">10</span>.<span class="w"> </span><span class="nv">Note</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">consistency</span><span class="w"> </span><span class="nv">measures</span><span class="w"> </span><span class="nv">how</span><span class="w"> </span><span class="nv">much</span>
<span class="nv">information</span><span class="w"> </span><span class="nv">included</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">summary</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">present</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">source</span><span class="w"> </span><span class="nv">article</span>.<span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="nv">points</span>
<span class="nv">indicate</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">summary</span><span class="w"> </span><span class="nv">contains</span><span class="w"> </span><span class="nv">only</span><span class="w"> </span><span class="nv">statements</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">entailed</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">source</span>
<span class="nv">document</span>.
<span class="nv">Article</span>:<span class="w"> </span><span class="err">&quot;Paul Merson has restarted.....Townsend was brought on in the 83rd</span>
<span class="err">minute for Tottenham as they drew 0-0 against Burnley. .Andros Townsend</span>
<span class="err">scores England&#39;s equaliser in their 1-1 friendly draw with Italy in Turin on</span>
<span class="err">Tuesday night. The former Arsenal man was proven wrong when Townsend hit a</span>
<span class="err">stunning equaliser for England against Italy and he duly admitted his mistake.</span>
<span class="err">&#39;It&#39;s not as though I was watching hoping he wouldn&#39;t score for England, I&#39;m</span>
<span class="err">genuinely pleased for him and fair play to him - it was a great goal.&#39; Merson</span>
<span class="nv">said</span>...<span class="err">&quot;</span>
<span class="nv">Summary</span>:<span class="w"> </span><span class="err">&quot;Paul Merson is not happy with Andros Townsend&#39;s call-up to the</span>
<span class="nv">England</span><span class="w"> </span><span class="nv">squad</span><span class="w"> </span><span class="nv">last</span><span class="w"> </span><span class="nv">week</span>.<span class="err">&quot;</span>
</code></pre></div>

<p>1/10. Although the summary mentions that Paul Merson is not happy with Andros Townsend, it lacks important details such as the context of the dispute and the fact that it was about Townsend's limited playing time in a Tottenham match against Burnley. The summary also does not mention that Merson later congratulated Townsend on his goal for England and admitted he was wrong about his potential in the squad (Experts Score: [5/5, 5/5, 5/5])</p>
<p>Figure 3: An example of ChatGPT fail to stick to the given definition of consistency.
degree than the baseline models.
In particular, when splitting the FRANK dataset into summaries from CNN/DM and XSum, the correlations of ChatGPT show a considerable decline from CNN/DM to XSum, which matches our analysis in the previous two parts. The difference might come from the abstractiveness of summaries generated from models trained on XSum, so their lower lexical similarity with the source document affects the model's judgement of consistency, leading to the worse performance in the FRANK XSum dataset. However, though the abstractiveness of XSum summaries lowers the correlations generally, ChatGPT's pearson's correlation is still much higher than the single-digit results of the baselines, suggesting its better language understanding and inference ability.</p>
<h3>4.4 Error Analysis</h3>
<p>In this part, we show some example cases of ChatGPT in the three tasks to showcase its limitations and attempt to provide a hint to understand Chat-</p>
<p>GPT's behavior in the aforementioned tasks.
In Figure 2, we show an example from the CoGenSumm dataset where ChatGPT failed in the entailment inference task. The model neglects the disappearance of "half of " in the candidate summary which significantly changes the meaning and decides the summary is consistent with the article. However, when putting the same summary and article into the summary ranking task combined with a consistent claim, ChatGPT successfully picks the consistent one and gives the right reasoning of why "Summary A" is inconsistent. The first case of Figure 2 supports our assumption of ChatGPT counting on lexical similarity to determine consistency as the high lexical overlap between inconsistent summary and the red-underlined part in the article cheats ChatGPT. Nevertheless, when another summary is both lexically and semantically closer to the article, ChatGPT detects the difference and manages to answer correctly in the second case of Figure 2.</p>
<p>With further investigation of failure cases, we found ChatGPT makes false inferences as shown in Figure 4. The summary claims that "prime minister matteo renzi" won the vote while the red underlined part in the article clearly says the bill has passed the lower house but is held up to be approved by both houses. However, ChatGPT determines this summary is consistent and tries to justify it by using "the bill is approved by the lower house" as evidence. This example, combined with the upper case in the first example, demonstrates that ChatGPT still has a limitation on understanding and inferencing of natural language. Furthermore, a CoT-style prompt is applied in this example to encourage the model to generate a reasoning process to assist its judgment. But ChatGPT directly produces the conclusion first and then unfolds its inference progress afterwards. According to the autoregressive training nature of GPT, the explanation is then conditioned on the "consistent" conclusion and thus cannot guide the decision while following the judgment. In our manual inspection, answers with the conclusion at first are not rare, suggesting zero-shot CoT-style prompts might not be the optimal instruction for ChatGPT to conduct a language inference task with reasoning progress. We suppose fined-engineered few-shot prompts might help to guide ChatGPT's generation and further improve its performance and will investigate it in the future.</p>
<p>1Decide if the following summary is consistent with the corresponding article. Note that consistency means all information in the summary is supported by the article.</p>
<p>Article: "The measure guarantees a majority of seats to the party that wins the most votes in an election ..... The lower house of parliament gave final approval to the bill by 334 votes to 61 ..... Mr Renzi wants to further transform the Italian system by abolishing the Senate and replacing it with a non-elected body with lesser powers. Currently, legislation is often held up because identical versions of bills have to be approved by both houses."</p>
<p>Summary: "Italian prime minister matteo renzi has won the latest stage in a vote in Italy's parliament."
Explain your reasoning step by step then answer (yes or no) the question:</p>
<p>The summary is consistent with the article as it mentions that the lower house of parliament gave final approval to the electoral reform bill by 334 votes to 61 , which means that Italian Prime Minister Matteo Renzi has won the latest stage in a vote in Italy's parliament. Therefore, the answer is yes.</p>
<p>Figure 4: An example of ChatGPT conducts false reasoning.</p>
<p>Moreover, there are examples that ChatGPT demonstrates limited comprehension of given prompts. Fig 3 shows a case of the SummEval dataset in the consistency rating task. Though the summary is short, the fact within it is consistent with the article which ChatGPT also admits in the answer. Therefore, all three experts mark 5 out of 5 for the summary. However, ChatGPT then only rates the summary 1 point as it does not cover other facts in the article which is not in the given marking rubric, showing an inadequate understanding of giving prompts. This example demonstrates the insufficient alignment brought by our tested prompt. Prompt engineering including human-in-the-loop alignment optimization and few-shot incontext learning might be helpful to better calibrate ChatGPT's output.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we comprehensively investigate the factual inconsistency evaluation ability of ChatGPT in the zero-shot setting with three coarse-grained and fine-grained factual inconsistency detection tasks. Our experimental results empirically show the great potential of ChatGPT as a good factual inconsistency evaluator, where it outperforms SOTA evaluation metrics on six out of nine datasets. Although its great potential, ChatGPT is also found to have limitations on evaluation bias, false reasoning, and hallucination, which should be further addressed for its reliable use. The experiments also show that ChatGPT's performance can be sig-</p>
<p>nificantly boosted by the chain-of-thought prompt. Lastly, We analyzed the limitation of the chain-ofthought prompt, which highlights the importance of alignment research in future work. The study in our paper is just the initial step in exploring the factual inconsistency evaluation ability of ChatGPT, which we hope can provide useful insights for future work in this direction.</p>
<h2>Limitations</h2>
<p>Our study has the following limitations: 1) Due to the cost limitation of using the API of ChatGPT, we only investigated the effectiveness of using zero-shot prompts on three tasks. More effective prompts such as the few-shot prompts can be explored in future work; 2) We only evaluated the performance of ChatGPT on the factual inconsistency evaluation. A thorough comparison of different large language models (LLMs) such as GPT-3.5 and GPT-4 can be studied in future work, to help us figure out the superiors and limitations of different LLMs.</p>
<h2>References</h2>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.</p>
<p>Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim M Buhmann. 2010. The balanced accuracy and its posterior distribution. In 2010 20th international conference on pattern recognition, pages 3121-3124. IEEE.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Esin Durmus, He He, and Mona Diab. 2020a. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055-5070, Online. Association for Computational Linguistics.</p>
<p>Esin Durmus, He He, and Mona T Diab. 2020b. Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization. ArXiv, abs/2005.03754.</p>
<p>Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.</p>
<p>Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019a. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214-2220, Florence, Italy. Association for Computational Linguistics.</p>
<p>Tobias Falke, Leonardo F R Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019b. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. ArXiv, abs/2302.04166.</p>
<p>Ben Goodrich, Vinay Rao, Peter J Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of generated text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, pages 166-175.</p>
<p>Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. pages 3592-3603. Association for Computational Linguistics (ACL).</p>
<p>Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. What have we achieved on text summarization? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 446-469, Online. Association for Computational Linguistics.</p>
<p>Yichong Huang, Xiaohong Feng, Xiaocheng Feng, and Bing Qin. 2021. The factual inconsistency problem in abstractive text summarization: A survey. arXiv preprint arXiv:2104.14839.</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? a preliminary study. arXiv preprint arXiv:2301.08745.</p>
<p>Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81-93.</p>
<p>Tom Kocmi and Christian Federmann. 2023a. Large language models are state-of-the-art evaluators of translation quality. ArXiv, abs/2302.14520.</p>
<p>Tom Kocmi and Christian Federmann. 2023b. Large language models are state-of-the-art evaluators of translation quality. ArXiv, abs/2302.14520.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916.</p>
<p>Wojciech Kryściński, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. pages 9332-9346. Association for Computational Linguistics (ACL).</p>
<p>Philippe Laban, Tobias Schnabel, Paul Bennett, and Marti A. Hearst. 2021. Keep it simple: Unsupervised simplification of multi-paragraph text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 63656378, Online. Association for Computational Linguistics.</p>
<p>Philippe Laban, Tobias Schnabel, Paul N Bennett, and Marti A Hearst. 2022. Summac: Re-visiting nlibased models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163-177.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Yang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730-3740, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan T McDonald. 2020. On faithfulness and factuality in abstractive summarization. ArXiv, abs/2005.00661.</p>
<p>Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Lorraine Li, Pavan Kapanipathi, and Kartik Talamadupula. 2021. Looking beyond sentencelevel natural language inference for question answering and text summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1322-1336, Online. Association for Computational Linguistics.</p>
<p>Mavuto M Mukaka. 2012. A guide to appropriate use of correlation coefficient in medical research. Malawi medical journal, 24(3):69-71.</p>
<p>Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023.</p>
<p>Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero Nogueira dos Santos, Henghui Zhu, Dejiao Zhang, Kathleen McKeown, and Bing Xiang. 2021. Entitylevel factual consistency of abstractive text summarization. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 27272733, Online. Association for Computational Linguistics.</p>
<p>OpenAI. 2022. Chatgpt. https://openai.com/blog/chatgpt.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.</p>
<p>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics.</p>
<p>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476.</p>
<p>Yiwei Qin, Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2022. T5score: Discriminative fine-tuning of generative evaluation metrics. arXiv preprint arXiv:2212.05726.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Elizabeth-Jane Pavlick, and Suzana Ili'c et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. ArXiv, abs/2211.05100.</p>
<p>Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, and Alex Wang. 2021. Questeval: Summarization asks for fact-based evaluation.</p>
<p>Mayank Soni and Vincent P. Wade. 2023. Comparing abstractive summaries generated by chatgpt to real summaries through blinded reviewers and text classification algorithms.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020, Online. Association for Computational Linguistics.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2023a. Crosslingual summarization via chatgpt. arXiv preprint arXiv:2302.14229.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023b. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023c. Is chatgpt a good nlg evaluator? a preliminary study. ArXiv, abs/2303.04048.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, and Sophia Ananiadou. 2023a. On the evaluations of chatgpt and emotion-enhanced prompting for mental health analysis. arXiv preprint arXiv:2304.03347.</p>
<p>Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei Cheng. 2023b. Exploring the limits of chatgpt for query or aspect-based text summarization. arXiv preprint arXiv:2302.08081.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263-27277.</p>
<p>Jerrold H Zar. 2005. Spearman rank correlation. Encyclopedia of biostatistics, 7.</p>
<p>Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328-11339. PMLR.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. 2023. Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert. ArXiv, abs/2302.10198.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Corresponding author
${ }^{1}$ the problem is also referred to as unfaithfulness, we use these terms interchangeably in the following&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
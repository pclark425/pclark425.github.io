<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3337 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3337</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3337</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-63549bf78e4b1e7e1cec505ce65e6e8f90474f41</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/63549bf78e4b1e7e1cec505ce65e6e8f90474f41" target="_blank">ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Experiments demonstrate that ReConcile significantly improves LLMs' reasoning -- both individually and as a team -- surpassing prior single-agent and multi-agent baselines by up to 11.4% and even outperforming GPT-4 on three datasets.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents. ReConcile enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism that leads to a better consensus. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that ReConcile significantly improves LLMs' reasoning -- both individually and as a team -- surpassing prior single-agent and multi-agent baselines by up to 11.4% and even outperforming GPT-4 on three datasets. ReConcile also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models, leading to an 8% improvement on MATH. Finally, we analyze the individual components of ReConcile, demonstrating that the diversity originating from different models is critical to its superior performance. Code: https://github.com/dinobby/ReConcile</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3337.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3337.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RECONCILE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RECONCILE: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-model multi-agent round-table framework that lets diverse LLM agents iteratively discuss, attempt to convince each other using in-context corrective explanations, estimate confidences, and produce a confidence-weighted team answer after multiple discussion rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-model multi-agent (ChatGPT, Bard, Claude2; variants include GPT-4, LLaMA-2-70B, DeepSeekMath)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RECONCILE is an interaction protocol applied on top of black-box LLM agents (API-based and open-source). Primary experiments use ChatGPT (gpt-3.5-turbo-0613 Azure), Bard (PaLM2 chat-bison-001), and Claude2; extensions include GPT-4, LLaMA-2-70B-chat, and DeepSeekMath.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT) / step-by-step prompts', 'Multi-round discussion with grouped answers', 'In-context demonstrations of answer-rectifying human explanations ("convincing samples")', 'Agent self-reported confidence estimation with recalibrated confidence-weighted voting', 'Iterative revision (agents generate revised explanation/answer each round)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Agents are prompted for CoT-style step-by-step explanations. RECONCILE runs R discussion rounds where each agent receives a discussion prompt containing grouped answers/explanations from all agents, each agent's prior round confidence, and convincing human explanation demonstrations for other agents; agents then output revised answer, explanation, and a self-reported confidence. A recalibration function f(p) maps reported confidences to discrete weights and the team answer is chosen by a weighted vote. Convincing samples are chosen as examples where a human explanation rectifies an agent's initial wrong answer and are provided in-context to teach agents how to produce persuading explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse methods and diverse agents: RECONCILE leverages diversity both at the model-family level (different LLMs produce diverse reasoning styles and knowledge) and at the prompting/multi-round-discussion level (convincing-sample in-context learning plus confidence-weighted voting). The paper demonstrates increased response diversity vs. multi-instance single-model baselines via a BERTScore-based similarity metric.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>StrategyQA, CommonsenseQA (CSQA), GSM8K, AQuA, MATH, Date Understanding, ANLI</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A set of seven reasoning benchmarks spanning open-domain multi-step commonsense (StrategyQA), multiple-choice commonsense (CSQA), mathematical word problems (GSM8K, AQuA, MATH), date-understanding tasks (BIG-bench subset), and adversarial NLI (ANLI).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>When run with ChatGPT, Bard, and Claude2, RECONCILE achieved (accuracy ± std) StrategyQA 79.0±1.6, CSQA 74.7±0.4, GSM8K 85.3±2.2, AQuA 66.0±0.8, Date 86.7±1.2 (Table 2). On MATH when using a DeepSeekMath agent (7B) plus GPT-4 and Claude2, RECONCILE reached 58.3 (Table 4). On ANLI RECONCILE reached 57.7 (Table 6). RECONCILE also outperformed GPT-4 zero-shot on some commonsense tasks (e.g., StrategyQA and CSQA) and matched or exceeded many baselines across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to single-model multi-agent debate baselines (multiple instances of the same LLM), RECONCILE (multi-model) outperformed debate by large margins on multiple tasks (example: date-understanding +11.4% absolute; StrategyQA +7.7% vs the strongest Claude2 debate baseline). Ablations show removing the multi-model component drops StrategyQA accuracy from 79.0% to 72.2% (−6.8%). Removing convincing samples drops to 74.5% (−4.5%); removing grouping or confidence-estimation also reduces performance. Diversity metric (sum of pairwise BERTScore similarities) is lowest (most diverse) for multi-model RECONCILE (0.8739) vs single-model instances (0.9102) and paraphrases (0.9398). RECONCILE reaches consensus faster and achieves higher per-round accuracy vs debate baselines (peak 79.0% vs 71.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining diverse LLM agents with discussion prompts that include other agents' grouped answers, confidence estimates, and in-context convincing demonstrations produces more diverse reasoning, faster/better consensus, and higher final accuracy than (a) single-agent methods (CoT, Self-Refine, Self-Consistency) and (b) multi-agent debate systems composed of multiple instances of the same model. Diversity of models is a critical component; convincing examples and confidence-weighted voting materially contribute to gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Improvements on math tasks were more modest when agents were general-purpose (because some agents like ChatGPT were already strong). Debate with multiple ChatGPT instances sometimes improved math results, showing that single-model multi-agent debate can help in some settings; however debate with multiple Bard or Claude2 instances was ineffective due to insufficient response diversity. Also, accuracy saturates after ~2 rounds of discussion in RECONCILE.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3337.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3337.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Debate (multi-agent single-model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-agent debate with multiple instances of the same underlying LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent interaction paradigm where multiple instances (copies) of the same LLM debate/rebut each other's answers to improve reasoning, typically implemented as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving factuality and reasoning in language models through multiagent debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Single-model multi-agent (multiple instances of ChatGPT / Bard / Claude2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Debate baselines instantiate multiple independent sessions of the same LLM (e.g., ChatGPT) and run a debate protocol where agents present arguments and counter-arguments; diversity is induced by sampling (higher temperature) rather than model-family heterogeneity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought prompting (CoT) within each agent', 'Multi-agent debate interaction (argument and counter-argument)', 'Diversity via sampling (high temperature) across instances']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Each instance produces CoT explanations and counter-arguments; diversity arises from different stochastic samples (high decoding temperature) across instances of the same model. Some variants include a separate judge or meta-evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar styles: limited diversity because all agents share identical architecture and pretraining; diversity relies on sampling variability and prompting, which the paper shows is inferior to multi-model diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same seven benchmarks (StrategyQA, CSQA, GSM8K, AQuA, MATH, Date, ANLI)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used as a comparison baseline across the same tasks to test whether multiple instances of one model can match multi-model gains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Representative debate baseline (three ChatGPT instances) reported StrategyQA 66.7±3.1, CSQA 62.7±1.2, GSM8K 83.0±2.2, AQuA 65.3±3.1, Date 68.0±1.6 (Table 2). Typical debate+judge gives slightly different numbers (e.g., Debate+Judge ChatGPT×3: StrategyQA 69.7±2.1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>RECONCILE (multi-model) outperforms debate baselines by substantial margins on several benchmarks (e.g., StrategyQA 79.0% vs debate 66.7% in one comparison; date understanding +11.4% absolute). Ablations show that replacing the multi-model component with same-model agents reduces RECONCILE performance from 79.0% to 72.2% on StrategyQA. Adding convincing human explanations to the debate baseline yields small improvements (debatew/convincing expl 69.5±1.7 vs debate 66.7±3.1) but still remains below RECONCILE.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Debate among multiple instances of the same model can sometimes improve over single-agent baselines (especially on math via sampling diversity), but lacks the complementary knowledge, perspective, and explanation styles that multi-model systems provide, limiting gains and causing echo chambers where incorrect answers persist.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Debate with multiple ChatGPT instances can help math tasks (e.g., improving GSM8K), but debate with multiple Bard or Claude2 instances was ineffective due to low diversity. Adding convincing samples to debate yields modest gains but does not bridge the gap to multi-model discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3337.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3337.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency: sampling multiple chain-of-thoughts and taking the most consistent answer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-agent technique that samples many independent CoT reasoning traces and returns the most frequent answer (majority vote) to improve chain-of-thought robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Single-agent (e.g., ChatGPT with multiple sampled CoT traces)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-Consistency runs a single LLM multiple times to generate diverse reasoning paths via sampling; final answer is chosen by majority/consistency among sampled outputs. In the paper, SC is implemented with parity in total LLM calls for fair comparison to RECONCILE.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought prompting', 'Diversity via sampling multiple reasoning traces (self-consistency voting)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate N sampled CoT traces (stochastic decoding) for a single LLM and choose the answer with highest frequency; can be combined with iterative refinement (SR+SC).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diversity is achieved via stochastic sampling of reasoning traces (similar style, same model); the paper contrasts this with cross-model diversity and finds sampling-based diversity insufficient to match multi-model benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>StrategyQA, CSQA, GSM8K, AQuA, Date, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used as an advanced single-agent baseline across the same benchmarks; the paper tests 3-way and 9-way SC variants matching RECONCILE's LLM-call budget.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Self-Consistency (ChatGPT) reported StrategyQA 73.3±0.5, CSQA 73.0±0.8, GSM8K 82.7±0.5, AQuA 60.3±1.2, Date 69.3±0.4 (Table 2). 9-way SC listed StrategyQA 74.7±0.8 in Table 11.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>RECONCILE (multi-model discussion) outperforms Self-Consistency run with similar total LLM-call budgets on most tasks (e.g., StrategyQA 79.0% RECONCILE vs 74.7% 9-way SC). The paper emphasizes that sampling diversity is less effective than multi-model diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling multiple CoT traces helps but does not substitute for model-family diversity; RECONCILE's cross-model feedback yields larger gains than SC with comparable compute/calls.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On certain math tasks (GSM8K), self-consistency with powerful models can be competitive and SC sometimes closes part of the gap; nonetheless RECONCILE still matches or exceeds SC in most reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3337.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3337.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine (SR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-generated feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-agent iterative technique where a model generates feedback (critique) about its own outputs and uses that feedback to refine its answer across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Single-agent (e.g., ChatGPT in iterative self-refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SR prompts the model to critique and improve its own output across multiple iterations; it can be combined with self-consistency for larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought prompting', 'Iterative self-critique and refinement']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The agent generates an answer and self-feedback, then refines its answer conditioned on that feedback. The paper runs SR and SR+SC baselines for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single-method, single-model: the diversity arises only from the model's internal generation across iterations; it lacks external diverse perspectives.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>StrategyQA, CSQA, GSM8K, AQuA, Date</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used as an advanced single-agent baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Self-Refine (ChatGPT) reported StrategyQA 66.7±2.7, CSQA 68.1±1.8, GSM8K 74.3±2.5, AQuA 45.3±2.2, Date 66.3±2.1 (Table 2). Combined SR+SC yields intermediate improvements (e.g., StrategyQA 72.2±1.9).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>RECONCILE outperforms SR and SR+SC across the evaluated benchmarks. The paper notes SR's limitations such as Degeneration-of-Thought (over-confident models failing to generate novel thoughts) and shows external diverse feedback helps overcome those limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-refinement can correct some errors but is limited by the model's own biases and overconfidence; multi-model discussion that leverages external corrective demonstrations and cross-model persuasion gives larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>SR struggles on many benchmarks relative to RECONCILE; the paper cites Degeneration-of-Thought as a failure mode for SR when the model is overly confident.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3337.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3337.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (as agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large proprietary multi-modal-capable LLM from OpenAI; used in the paper both as a baseline single-agent (zero-shot CoT) and as an optional agent inside RECONCILE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4, evaluated zero-shot with CoT prompting as a strong single-agent baseline and also used as an agent in multi-model RECONCILE variants; exact pretraining details and parameter counts are proprietary / unspecified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Zero-shot Chain-of-Thought (CoT)', 'Used within multi-model RECONCILE discussion in some variants']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Zero-shot CoT prompts elicit step-by-step reasoning; when included in RECONCILE, GPT-4 participates in multi-round discussion and receives convincing demonstrations from other agents.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single-model default; when combined with other models in RECONCILE it contributes a different model-family viewpoint increasing cross-model diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>StrategyQA, CSQA, GSM8K, AQuA, Date, MATH (via variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used as a strong single-agent baseline; measured zero-shot CoT performance and compared to RECONCILE team performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>GPT-4 zero-shot CoT reported StrategyQA 75.6±4.7, CSQA 73.3±0.4, GSM8K 90.7±1.7, AQuA 65.7±4.6, Date 89.0±2.2 (Table 2). Including GPT-4 inside RECONCILE (as one of agents) increased StrategyQA team accuracy to 87.7 in one reported variant (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>RECONCILE with multi-model agents (including or excluding GPT-4) can outperform GPT-4 zero-shot on several commonsense tasks (e.g., StrategyQA improvements up to +12.1% reported when GPT-4 is in the team). However, GPT-4 remains extremely strong on some math datasets (GSM8K), possibly due to overlaps with pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Even a strong model like GPT-4 benefits from external feedback from weaker models when assembled into a RECONCILE team; team-level consensus can surpass GPT-4 zero-shot on several benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On GSM8K GPT-4's zero-shot performance was especially strong (90.7%), making RECONCILE gains limited on that task unless specialized math agents were included.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3337.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3337.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeekMath</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeekMath (specialized math model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific open model pre-trained on math-related web corpora that, when included as an agent in RECONCILE, improves team performance on hard math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseekmath: Pushing the limits of mathematical reasoning in open language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeekMath</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B parameter specialized model for mathematical reasoning (paper reports a 7B model) trained on math-focused corpora; used as an agent inside RECONCILE to provide domain expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought style reasoning', 'Specialized pretraining for math tasks', 'Participation in RECONCILE multi-round discussions']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>DeepSeekMath generates CoT explanations for math problems and acts as a domain-specialist agent in RECONCILE; its outputs are combined with other agents' explanations and confidences in the discussion rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Provides domain-specific reasoning style and evidence/steps different from general-purpose LLMs, increasing model-family diversity in RECONCILE.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH benchmark (Hendrycks et al., 2021) and other math datasets</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A challenging mathematical problem dataset requiring long-form reasoning and domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>On MATH, representative numbers: GPT-4 zero-shot 44.0, best single-agent (zero-shot) 50.5, best multi-agent (Debate) 48.7, RECONCILE with GPT-4, Claude2, and DeepSeekMath achieved 58.3 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Including DeepSeekMath in RECONCILE produced large gains on MATH (+7.8% over zero-shot DeepSeekMath and +9.6% over GPT-4-based debate baselines in the reported comparisons), showing that domain-specific models can contribute complementary expertise when combined via the RECONCILE protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Specialized agents substantially boost team performance on domain-specific tasks when assembled into a cross-model discussion framework; RECONCILE effectively leverages such domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>None explicit in the paper; authors note that when using only general-purpose agents, math improvements were moderate, highlighting the importance of including domain-specialists for large gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Examining the inter-consistency of large language models: An in-depth analysis via debate <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3337",
    "paper_id": "paper-63549bf78e4b1e7e1cec505ce65e6e8f90474f41",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "RECONCILE",
            "name_full": "RECONCILE: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs",
            "brief_description": "A multi-model multi-agent round-table framework that lets diverse LLM agents iteratively discuss, attempt to convince each other using in-context corrective explanations, estimate confidences, and produce a confidence-weighted team answer after multiple discussion rounds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multi-model multi-agent (ChatGPT, Bard, Claude2; variants include GPT-4, LLaMA-2-70B, DeepSeekMath)",
            "model_description": "RECONCILE is an interaction protocol applied on top of black-box LLM agents (API-based and open-source). Primary experiments use ChatGPT (gpt-3.5-turbo-0613 Azure), Bard (PaLM2 chat-bison-001), and Claude2; extensions include GPT-4, LLaMA-2-70B-chat, and DeepSeekMath.",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought (CoT) / step-by-step prompts",
                "Multi-round discussion with grouped answers",
                "In-context demonstrations of answer-rectifying human explanations (\"convincing samples\")",
                "Agent self-reported confidence estimation with recalibrated confidence-weighted voting",
                "Iterative revision (agents generate revised explanation/answer each round)"
            ],
            "reasoning_methods_description": "Agents are prompted for CoT-style step-by-step explanations. RECONCILE runs R discussion rounds where each agent receives a discussion prompt containing grouped answers/explanations from all agents, each agent's prior round confidence, and convincing human explanation demonstrations for other agents; agents then output revised answer, explanation, and a self-reported confidence. A recalibration function f(p) maps reported confidences to discrete weights and the team answer is chosen by a weighted vote. Convincing samples are chosen as examples where a human explanation rectifies an agent's initial wrong answer and are provided in-context to teach agents how to produce persuading explanations.",
            "diversity_of_methods": "Diverse methods and diverse agents: RECONCILE leverages diversity both at the model-family level (different LLMs produce diverse reasoning styles and knowledge) and at the prompting/multi-round-discussion level (convincing-sample in-context learning plus confidence-weighted voting). The paper demonstrates increased response diversity vs. multi-instance single-model baselines via a BERTScore-based similarity metric.",
            "reasoning_task_name": "StrategyQA, CommonsenseQA (CSQA), GSM8K, AQuA, MATH, Date Understanding, ANLI",
            "reasoning_task_description": "A set of seven reasoning benchmarks spanning open-domain multi-step commonsense (StrategyQA), multiple-choice commonsense (CSQA), mathematical word problems (GSM8K, AQuA, MATH), date-understanding tasks (BIG-bench subset), and adversarial NLI (ANLI).",
            "performance_by_method": "When run with ChatGPT, Bard, and Claude2, RECONCILE achieved (accuracy ± std) StrategyQA 79.0±1.6, CSQA 74.7±0.4, GSM8K 85.3±2.2, AQuA 66.0±0.8, Date 86.7±1.2 (Table 2). On MATH when using a DeepSeekMath agent (7B) plus GPT-4 and Claude2, RECONCILE reached 58.3 (Table 4). On ANLI RECONCILE reached 57.7 (Table 6). RECONCILE also outperformed GPT-4 zero-shot on some commonsense tasks (e.g., StrategyQA and CSQA) and matched or exceeded many baselines across tasks.",
            "comparison_of_methods": "Compared to single-model multi-agent debate baselines (multiple instances of the same LLM), RECONCILE (multi-model) outperformed debate by large margins on multiple tasks (example: date-understanding +11.4% absolute; StrategyQA +7.7% vs the strongest Claude2 debate baseline). Ablations show removing the multi-model component drops StrategyQA accuracy from 79.0% to 72.2% (−6.8%). Removing convincing samples drops to 74.5% (−4.5%); removing grouping or confidence-estimation also reduces performance. Diversity metric (sum of pairwise BERTScore similarities) is lowest (most diverse) for multi-model RECONCILE (0.8739) vs single-model instances (0.9102) and paraphrases (0.9398). RECONCILE reaches consensus faster and achieves higher per-round accuracy vs debate baselines (peak 79.0% vs 71.3%).",
            "key_findings": "Combining diverse LLM agents with discussion prompts that include other agents' grouped answers, confidence estimates, and in-context convincing demonstrations produces more diverse reasoning, faster/better consensus, and higher final accuracy than (a) single-agent methods (CoT, Self-Refine, Self-Consistency) and (b) multi-agent debate systems composed of multiple instances of the same model. Diversity of models is a critical component; convincing examples and confidence-weighted voting materially contribute to gains.",
            "counter_examples_or_negative_results": "Improvements on math tasks were more modest when agents were general-purpose (because some agents like ChatGPT were already strong). Debate with multiple ChatGPT instances sometimes improved math results, showing that single-model multi-agent debate can help in some settings; however debate with multiple Bard or Claude2 instances was ineffective due to insufficient response diversity. Also, accuracy saturates after ~2 rounds of discussion in RECONCILE.",
            "uuid": "e3337.0",
            "source_info": {
                "paper_title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Debate (multi-agent single-model)",
            "name_full": "Multi-agent debate with multiple instances of the same underlying LLM",
            "brief_description": "A multi-agent interaction paradigm where multiple instances (copies) of the same LLM debate/rebut each other's answers to improve reasoning, typically implemented as a baseline in this paper.",
            "citation_title": "Improving factuality and reasoning in language models through multiagent debate",
            "mention_or_use": "use",
            "model_name": "Single-model multi-agent (multiple instances of ChatGPT / Bard / Claude2)",
            "model_description": "Debate baselines instantiate multiple independent sessions of the same LLM (e.g., ChatGPT) and run a debate protocol where agents present arguments and counter-arguments; diversity is induced by sampling (higher temperature) rather than model-family heterogeneity.",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought prompting (CoT) within each agent",
                "Multi-agent debate interaction (argument and counter-argument)",
                "Diversity via sampling (high temperature) across instances"
            ],
            "reasoning_methods_description": "Each instance produces CoT explanations and counter-arguments; diversity arises from different stochastic samples (high decoding temperature) across instances of the same model. Some variants include a separate judge or meta-evaluator.",
            "diversity_of_methods": "Similar styles: limited diversity because all agents share identical architecture and pretraining; diversity relies on sampling variability and prompting, which the paper shows is inferior to multi-model diversity.",
            "reasoning_task_name": "Same seven benchmarks (StrategyQA, CSQA, GSM8K, AQuA, MATH, Date, ANLI)",
            "reasoning_task_description": "Used as a comparison baseline across the same tasks to test whether multiple instances of one model can match multi-model gains.",
            "performance_by_method": "Representative debate baseline (three ChatGPT instances) reported StrategyQA 66.7±3.1, CSQA 62.7±1.2, GSM8K 83.0±2.2, AQuA 65.3±3.1, Date 68.0±1.6 (Table 2). Typical debate+judge gives slightly different numbers (e.g., Debate+Judge ChatGPT×3: StrategyQA 69.7±2.1).",
            "comparison_of_methods": "RECONCILE (multi-model) outperforms debate baselines by substantial margins on several benchmarks (e.g., StrategyQA 79.0% vs debate 66.7% in one comparison; date understanding +11.4% absolute). Ablations show that replacing the multi-model component with same-model agents reduces RECONCILE performance from 79.0% to 72.2% on StrategyQA. Adding convincing human explanations to the debate baseline yields small improvements (debatew/convincing expl 69.5±1.7 vs debate 66.7±3.1) but still remains below RECONCILE.",
            "key_findings": "Debate among multiple instances of the same model can sometimes improve over single-agent baselines (especially on math via sampling diversity), but lacks the complementary knowledge, perspective, and explanation styles that multi-model systems provide, limiting gains and causing echo chambers where incorrect answers persist.",
            "counter_examples_or_negative_results": "Debate with multiple ChatGPT instances can help math tasks (e.g., improving GSM8K), but debate with multiple Bard or Claude2 instances was ineffective due to low diversity. Adding convincing samples to debate yields modest gains but does not bridge the gap to multi-model discussion.",
            "uuid": "e3337.1",
            "source_info": {
                "paper_title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Self-Consistency (SC)",
            "name_full": "Self-Consistency: sampling multiple chain-of-thoughts and taking the most consistent answer",
            "brief_description": "A single-agent technique that samples many independent CoT reasoning traces and returns the most frequent answer (majority vote) to improve chain-of-thought robustness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "Single-agent (e.g., ChatGPT with multiple sampled CoT traces)",
            "model_description": "Self-Consistency runs a single LLM multiple times to generate diverse reasoning paths via sampling; final answer is chosen by majority/consistency among sampled outputs. In the paper, SC is implemented with parity in total LLM calls for fair comparison to RECONCILE.",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought prompting",
                "Diversity via sampling multiple reasoning traces (self-consistency voting)"
            ],
            "reasoning_methods_description": "Generate N sampled CoT traces (stochastic decoding) for a single LLM and choose the answer with highest frequency; can be combined with iterative refinement (SR+SC).",
            "diversity_of_methods": "Diversity is achieved via stochastic sampling of reasoning traces (similar style, same model); the paper contrasts this with cross-model diversity and finds sampling-based diversity insufficient to match multi-model benefits.",
            "reasoning_task_name": "StrategyQA, CSQA, GSM8K, AQuA, Date, etc.",
            "reasoning_task_description": "Used as an advanced single-agent baseline across the same benchmarks; the paper tests 3-way and 9-way SC variants matching RECONCILE's LLM-call budget.",
            "performance_by_method": "Self-Consistency (ChatGPT) reported StrategyQA 73.3±0.5, CSQA 73.0±0.8, GSM8K 82.7±0.5, AQuA 60.3±1.2, Date 69.3±0.4 (Table 2). 9-way SC listed StrategyQA 74.7±0.8 in Table 11.",
            "comparison_of_methods": "RECONCILE (multi-model discussion) outperforms Self-Consistency run with similar total LLM-call budgets on most tasks (e.g., StrategyQA 79.0% RECONCILE vs 74.7% 9-way SC). The paper emphasizes that sampling diversity is less effective than multi-model diversity.",
            "key_findings": "Sampling multiple CoT traces helps but does not substitute for model-family diversity; RECONCILE's cross-model feedback yields larger gains than SC with comparable compute/calls.",
            "counter_examples_or_negative_results": "On certain math tasks (GSM8K), self-consistency with powerful models can be competitive and SC sometimes closes part of the gap; nonetheless RECONCILE still matches or exceeds SC in most reported comparisons.",
            "uuid": "e3337.2",
            "source_info": {
                "paper_title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Self-Refine (SR)",
            "name_full": "Self-Refine: Iterative refinement with self-generated feedback",
            "brief_description": "A single-agent iterative technique where a model generates feedback (critique) about its own outputs and uses that feedback to refine its answer across iterations.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "use",
            "model_name": "Single-agent (e.g., ChatGPT in iterative self-refinement)",
            "model_description": "SR prompts the model to critique and improve its own output across multiple iterations; it can be combined with self-consistency for larger gains.",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought prompting",
                "Iterative self-critique and refinement"
            ],
            "reasoning_methods_description": "The agent generates an answer and self-feedback, then refines its answer conditioned on that feedback. The paper runs SR and SR+SC baselines for fairness.",
            "diversity_of_methods": "Single-method, single-model: the diversity arises only from the model's internal generation across iterations; it lacks external diverse perspectives.",
            "reasoning_task_name": "StrategyQA, CSQA, GSM8K, AQuA, Date",
            "reasoning_task_description": "Used as an advanced single-agent baseline.",
            "performance_by_method": "Self-Refine (ChatGPT) reported StrategyQA 66.7±2.7, CSQA 68.1±1.8, GSM8K 74.3±2.5, AQuA 45.3±2.2, Date 66.3±2.1 (Table 2). Combined SR+SC yields intermediate improvements (e.g., StrategyQA 72.2±1.9).",
            "comparison_of_methods": "RECONCILE outperforms SR and SR+SC across the evaluated benchmarks. The paper notes SR's limitations such as Degeneration-of-Thought (over-confident models failing to generate novel thoughts) and shows external diverse feedback helps overcome those limitations.",
            "key_findings": "Self-refinement can correct some errors but is limited by the model's own biases and overconfidence; multi-model discussion that leverages external corrective demonstrations and cross-model persuasion gives larger gains.",
            "counter_examples_or_negative_results": "SR struggles on many benchmarks relative to RECONCILE; the paper cites Degeneration-of-Thought as a failure mode for SR when the model is overly confident.",
            "uuid": "e3337.3",
            "source_info": {
                "paper_title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-4 (as agent)",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A large proprietary multi-modal-capable LLM from OpenAI; used in the paper both as a baseline single-agent (zero-shot CoT) and as an optional agent inside RECONCILE.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4, evaluated zero-shot with CoT prompting as a strong single-agent baseline and also used as an agent in multi-model RECONCILE variants; exact pretraining details and parameter counts are proprietary / unspecified in the paper.",
            "model_size": null,
            "reasoning_methods": [
                "Zero-shot Chain-of-Thought (CoT)",
                "Used within multi-model RECONCILE discussion in some variants"
            ],
            "reasoning_methods_description": "Zero-shot CoT prompts elicit step-by-step reasoning; when included in RECONCILE, GPT-4 participates in multi-round discussion and receives convincing demonstrations from other agents.",
            "diversity_of_methods": "Single-model default; when combined with other models in RECONCILE it contributes a different model-family viewpoint increasing cross-model diversity.",
            "reasoning_task_name": "StrategyQA, CSQA, GSM8K, AQuA, Date, MATH (via variants)",
            "reasoning_task_description": "Used as a strong single-agent baseline; measured zero-shot CoT performance and compared to RECONCILE team performance.",
            "performance_by_method": "GPT-4 zero-shot CoT reported StrategyQA 75.6±4.7, CSQA 73.3±0.4, GSM8K 90.7±1.7, AQuA 65.7±4.6, Date 89.0±2.2 (Table 2). Including GPT-4 inside RECONCILE (as one of agents) increased StrategyQA team accuracy to 87.7 in one reported variant (Table 3).",
            "comparison_of_methods": "RECONCILE with multi-model agents (including or excluding GPT-4) can outperform GPT-4 zero-shot on several commonsense tasks (e.g., StrategyQA improvements up to +12.1% reported when GPT-4 is in the team). However, GPT-4 remains extremely strong on some math datasets (GSM8K), possibly due to overlaps with pretraining.",
            "key_findings": "Even a strong model like GPT-4 benefits from external feedback from weaker models when assembled into a RECONCILE team; team-level consensus can surpass GPT-4 zero-shot on several benchmarks.",
            "counter_examples_or_negative_results": "On GSM8K GPT-4's zero-shot performance was especially strong (90.7%), making RECONCILE gains limited on that task unless specialized math agents were included.",
            "uuid": "e3337.4",
            "source_info": {
                "paper_title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "DeepSeekMath",
            "name_full": "DeepSeekMath (specialized math model)",
            "brief_description": "A domain-specific open model pre-trained on math-related web corpora that, when included as an agent in RECONCILE, improves team performance on hard math benchmarks.",
            "citation_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
            "mention_or_use": "use",
            "model_name": "DeepSeekMath",
            "model_description": "A 7B parameter specialized model for mathematical reasoning (paper reports a 7B model) trained on math-focused corpora; used as an agent inside RECONCILE to provide domain expertise.",
            "model_size": "7B",
            "reasoning_methods": [
                "Chain-of-Thought style reasoning",
                "Specialized pretraining for math tasks",
                "Participation in RECONCILE multi-round discussions"
            ],
            "reasoning_methods_description": "DeepSeekMath generates CoT explanations for math problems and acts as a domain-specialist agent in RECONCILE; its outputs are combined with other agents' explanations and confidences in the discussion rounds.",
            "diversity_of_methods": "Provides domain-specific reasoning style and evidence/steps different from general-purpose LLMs, increasing model-family diversity in RECONCILE.",
            "reasoning_task_name": "MATH benchmark (Hendrycks et al., 2021) and other math datasets",
            "reasoning_task_description": "A challenging mathematical problem dataset requiring long-form reasoning and domain knowledge.",
            "performance_by_method": "On MATH, representative numbers: GPT-4 zero-shot 44.0, best single-agent (zero-shot) 50.5, best multi-agent (Debate) 48.7, RECONCILE with GPT-4, Claude2, and DeepSeekMath achieved 58.3 (Table 4).",
            "comparison_of_methods": "Including DeepSeekMath in RECONCILE produced large gains on MATH (+7.8% over zero-shot DeepSeekMath and +9.6% over GPT-4-based debate baselines in the reported comparisons), showing that domain-specific models can contribute complementary expertise when combined via the RECONCILE protocol.",
            "key_findings": "Specialized agents substantially boost team performance on domain-specific tasks when assembled into a cross-model discussion framework; RECONCILE effectively leverages such domain knowledge.",
            "counter_examples_or_negative_results": "None explicit in the paper; authors note that when using only general-purpose agents, math improvements were moderate, highlighting the importance of including domain-specialists for large gains.",
            "uuid": "e3337.5",
            "source_info": {
                "paper_title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Examining the inter-consistency of large language models: An in-depth analysis via debate",
            "rating": 1
        }
    ],
    "cost": 0.020527999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RECONCILE: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs</h1>
<p>Justin Chih-Yao Chen Swarnadeep Saha Mohit Bansal<br>UNC Chapel Hill<br>{cychen, swarna, mbansal}@cs.unc.edu</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose RECONCILE, a multi-model multiagent framework designed as a round table conference among diverse LLM agents. RECONCILE enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidenceweighted voting mechanism that leads to a better consensus. In each round, RECONCILE initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answerrectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that RECONCILE significantly improves LLMs' reasoning - both individually and as a team - surpassing prior single-agent and multi-agent baselines by up to $11.4 \%$ and even outperforming GPT-4 on three datasets. RECONCILE also flexibly incorporates different combinations of agents, including API-based, open-source, and domainspecific models, leading to an $8 \%$ improvement on MATH. Finally, we analyze the individual components of RECONCILE, demonstrating that the diversity originating from different models is critical to its superior performance. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>A large body of recent work has focused on improving the reasoning capabilities of Large Language Models (LLMs) by imitating various human cognitive processes (Wang and Zhao, 2023; Park et al., 2023; Sumers et al., 2023; Ye et al., 2023). These include phenomena like reflecting on and critiquing one's own predictions, being receptive to feedback, and learning from feedback. Of note,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>self-reflection is an introspective process that allows the model to improve its outputs by generating feedback from the model itself (Madaan et al., 2023; Shinn et al., 2023). However, self-reflection suffers from Degeneration-of-Thought - when the model is overly confident in its answer, it is unable to generate novel thoughts even after multiple rounds of feedback (Liang et al., 2023).</p>
<p>To promote more diverse thoughts, past work has drawn inspiration from the concept of society of minds in multi-agent systems (Minsky, 1988; Zhuge et al., 2023). It highlights the importance of communication and collaboration between multiple agents for complex decision-making tasks. While such collaborative frameworks like multi-agent debate (Liang et al., 2023; Du et al., 2023) increase the reasoning diversity through the process of a debate, multiple agents have typically been limited to different instances of the same underlying model like ChatGPT (OpenAI, 2022). ${ }^{2}$ This results in an inherent model bias, a restricted knowledge scope, and a lack of external feedback from other models due to identical pre-training data and model architectures across all agents. In general, when multiple agents propose solutions to a problem, the success of such a multi-agent system is fundamentally reliant on (a) the diversity of the solutions, (b) the ability to estimate each agent's confidence, and (c) accordingly, convince other agents (with explanations) to reach a better consensus. This puts forward the question: if multiple diverse LLMs collaboratively solve a task, are they capable of discussing their solutions with each other to reach a better consensus?</p>
<p>We aim to solve reasoning problems by learning from diverse insights and external feedback, originating from agents that belong to different model</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of the main differences between RECONCILE and prior works. While most current self-refine and debating techniques rely on multiple instances of a single model (e.g., ChatGPT), our method incorporates models from different families (e.g., ChatGPT, Bard, and Claude2). Our approach also emphasizes critical elements of effective discussion, including convincing another agent to improve their answers and incorporating the estimated confidence of all agents. For illustrative simplicity, we depict only one agent contemplating how to convince the other two agents.</p>
<p>families. Collaborative processes such as brainstorming, group meetings, and discussions play a pivotal role in reaching a consensus and arriving at more refined solutions to complex problems (Li et al., 2022b). Effective discussion also entails the selection of stances, voting, convincing, exchange of information, and a diversity of opinions. Thus, we propose RECONCILE, a framework of round-table conference for obtaining better consensus among diverse LLM agents. RECONCILE consists of multiple discussion rounds between diverse LLM agents who try to convince<sup>3</sup> each other to either rectify their answers or become more confident of their initial correct answers (see Fig. 1 for a broad overview).</p>
<p>Given a reasoning problem, RECONCILE begins with each agent first generating an answer, its uncertainty, and a corresponding explanation (as a Chain-of-Thought (Wei et al., 2022)) for the answer. Then all agents enter a multi-round discussion phase. Each discussion round consists of all agents generating a revised explanation and answer based on all other agents' explanations and answers from the previous round. In particular, RECONCILE initiates a discussion by designing a discussion prompt for each agent, that lets it condition on (1) grouped answers from all agents, (2) corresponding explanations generated in the previous round, and (3) demonstrations of answer-rectifying human explanations for convincing other agents. We leverage them in an in-context learning framework to teach models to generate their own convincing explanations (see Fig. 3). Even in cases where an agent initially offers an incorrect answer and explanation, it can consider another agent's convincing explanation and amend its response accordingly. In each discussion round, we estimate an agent's uncertainty via a confidence-estimation prompt (Tian et al., 2023; Xiong et al., 2023a). Once all agents converge to the same answer (i.e., a consensus has been reached), we employ these confidences to compute a weighted vote as the team answer.</p>
<p>We primarily develop RECONCILE with three state-of-the-art LLMs: ChatGPT (OpenAI, 2022), Bard (Anil et al., 2023), and Claude2 (Anthropic, 2023). We also demonstrate the flexibility of RECONCILE with variants that employ a much stronger GPT-4 (OpenAI, 2023), an open-source LLaMA-2-70B (Touvron et al., 2023), or a domain-specific DeepSeekMATH (Shao et al., 2024) model as an agent. Across seven benchmarks spanning commonsense reasoning, mathematical reasoning, logical reasoning, and Natural Language Inference (NLI), RECONCILE outperforms prior single-agent (e.g., Self-Refine (Madaan et al., 2023) and Self-consistency (Wang et al., 2023b)) and multi-agent baselines (Debate (Du et al., 2023) and Judge (Liang et al., 2023)) that are built on top of the same underlying models. For example, RECONCILE, (1) on a date understanding task, outperforms the leading multi-agent debate baseline by</p>
<p><sup>3</sup>When we say that an agent tries to convince another agent, we mean that it learns (based on corrective explanations) to defend or argue for its stance while still being receptive to the other agent's argument.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Refine</th>
<th style="text-align: center;">Ensemble</th>
<th style="text-align: center;">Multi-Agent</th>
<th style="text-align: center;">Multi-Model</th>
<th style="text-align: center;">Convincingness</th>
<th style="text-align: center;">Confidence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Self-Refine (SR)</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: left;">Self-Consistency (SC)</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: left;">SR + SC</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: left;">Debate</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square^{*}$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: left;">Judge</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
</tr>
<tr>
<td style="text-align: left;">RECONCILE (Ours)</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary of the main differences between prior work, including Self-Refine (SR, Madaan et al. (2023)); Self-Consistency (SC, <em>Wang et al. (2023b)</em>); Debate (Du et al., 2023) and Judge (Liang et al., 2023). $\square$ means supported and $\square$ means not supported. RECONCILE supports multi-model multi-agent discussion with confidence estimation and convincingness. ${ }^{*}=\mathrm{Du}$ et al. (2023) primarily experiment with multiple instances of ChatGPT as different agents and conduct an initial investigation with 20 samples using ChatGPT and Bard as the two agents.
$11.4 \%$, (2) on StrategyQA, also outperforms GPT- 4 by $3.4 \%$, and (3) on MATH, outperforms both GPT-4 and a specialized DeepSeekMath model by $8 \%$. Moreover, detailed analyses of the individual components of RECONCILE demonstrate that leveraging diverse LLM agents leads to maximum gains, and we further validate their higher response diversity via a BERTScore-based diversity metric (Zhang et al., 2019). Finally, we show that RECONCILE not only leads to better team performance but also enables each agent to improve individually via the discussion process.</p>
<p>In summary, our primary contributions are:</p>
<ul>
<li>We propose RECONCILE, a reasoning framework involving diverse Large Language Models in a Round Table Conference.</li>
<li>We conduct extensive experiments on seven benchmarks to show that RECONCILE outperforms strong baselines (including GPT-4 on some benchmarks) and also generalizes to different combinations of agents.</li>
<li>We study the role of diversity, confidence estimation, and an agent's ability to convince others (by learning from corrective explanations) in multiagent discussion systems.</li>
</ul>
<h2>2 Related Work</h2>
<p>Reasoning with LLMs. Progress in LLMs has led to the development of advanced prompting and fine-tuning techniques for solving reasoning problems. Representative methods include Chain-of-Thought (CoT) (Kojima et al., 2022; Wei et al., 2022; Wang et al., 2023a) and Tree-of-Thought prompting (Yao et al., 2023a), selfconsistency (Wang et al., 2023b), meta-reasoning over multiple paths (Yoran et al., 2023), use of scratchpads (Nye et al., 2021), training veri-
fiers (Cobbe et al., 2021), self-collaboration (Wang et al., 2023c; Schick et al., 2022; Li et al., 2023a; Feng et al., 2024), self-reflection (Shinn et al., 2023; Madaan et al., 2023; Wang and Zhao, 2023; Yao et al., 2023b), improved math reasoning (Yue et al., 2023; Luo et al., 2023) and fine-tuning via bootstrapping models (Zelikman et al., 2022; Lewkowycz et al., 2022; Li et al., 2023b). Eliciting reasoning from a single agent, while promising, is fundamentally limited by a lack of diverse insights.
Reasoning in Multi-Agent Systems. A recent line of work has explored student-teacher frameworks with the goal of distilling reasoning capabilities from a stronger teacher to a weaker student (Magister et al., 2023; Fu et al., 2023; Ho et al., 2023; Saha et al., 2023; Mukherjee et al., 2023). As opposed to a teacher teaching weaker agents, we seek to develop a multi-agent system where different LLM agents have their unique strengths and try to collaboratively improve performance by reaching a better consensus. Notable prior works include multi-agent debating frameworks (Du et al., 2023; Liang et al., 2023; Chan et al., 2023; Xiong et al., 2023a; Khan et al., 2024) but such efforts are still largely limited to multiple instances of the same underlying language model. We argue that relying on a single model limits the potential of complementary benefits from different model families and the advantage of ensemble learning. Moreover, estimating the confidence of each agent and being able to defend or improve one's opinions become more prominent components in such multi-model multi-agent systems because of the individual differences. Overall, Table 1 summarizes RECONCILE's key differences compared to prior single-agent and multi-agent reasoning methods.
Ensembling Large Pretrained Models. Large pre-trained models, by virtue of being trained on</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: Overview of ReConcile with ChatGPT, Bard, and Claude2, consisting of three phases: (1) Initial Response Generation: Each agent generates an initial answer and explanation. (2) Multi-Round Discussion: Each model is presented with a discussion prompt (as illustrated on the left) and subsequently generates an updated answer and explanation. (3) Team answer generation: The team answer is determined by a weighted vote at the end of each round. The left part of the figure shows the discussion prompt for an agent, consisting of (a) grouped answers and explanations of all agents from the previous round, (b) estimated confidence, and (c) demonstrations of convincing samples.</p>
<p>different data and with architectural variations, exhibit distinct capabilities. This has led to the development of ensembles <em>Sagi and Rokach (2018)</em> in multimodal learning <em>Zeng et al. (2023); Li et al. (2022a)</em>. Mixture of Experts, a popular ensemble learning technique, trains multiple smaller specialized models to improve robustness and overall accuracy <em>Jacobs et al. (1991); Shazeer et al. (2017); Du et al. (2022)</em>. Specific to language models, Self-Consistency <em>Wang et al. (2023b)</em> generates diverse reasoning paths using CoT and chooses the most consistent answer as the final output. <em>Jiang et al. (2023)</em> propose LLM-Blender, a method to rank and fuse generations from different models. Different from these, we study communication via explanations between distinct LLM agents and their ability to discuss and convince each other in order to improve collective reasoning.</p>
<h2>3 Problem Setup</h2>
<p>We assume that we are given a test problem $Q$ and there are $n$ agents $\mathcal{A}={A_{i}}<em i="i">{i=1}^{n}$ participating in a round table discussion. Each agent is a distinct LLM, potentially trained with different pre-training data and model architectures. All agents are capable of generating an answer and a corresponding Chain-of-Thought explanation <em>Wei et al. (2022)</em> for the test problem. For each agent $A</em>}}$, we utilize a small number of $k$ demonstrations of convincing samples $C_{i}={c_{j}^{(i)<em j="j">{j=1}^{k}$. Each convincing sample $c</em>$ that helps rectify an agent’s initial incorrect answer (see more details in Sec 4). The objective of ReConcile is to improve the team performance on a given task by holding multiple rounds of discussion between the agents, quantifying the uncertainty associated with each agent, and convincing other agents to reach a better consensus. Note that convincing samples serve as an additional performance enhancer; even when the dataset lacks human explanations, our method can still yield performance gains independent of this (more details below).}^{(i)}=\left(q_{j}^{(i)},a_{j}^{(i)},e_{j}^{(i)}\right)$ for an agent $A_{i}$ is an instance of a question $q_{j}^{(i)}$, gold answer $a_{j}^{(i)}$, and a human explanation $e_{j}^{(i)</p>
<h2>4 ReConcile: A Collaborative Discussion Framework</h2>
<p>ReConcile operates in three phases: initial response generation, multi-round discussion, and team answer generation. The overview of our method is demonstrated in Fig. 2 and Algorithm 1.</p>
<h4>Phase 1: Initial Response Generation.</h4>
<p>ReConcile operates with each agent $A_{i}$ initially generating an answer $a_{i}^{(0)}$, an explanation $e_{i}^{(0)}$, and an associated confidence $p_{i}^{(0)}\in[0,1]$ for the generated answer. Each agent conditions on a zero-shot</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Method for choosing convincing samples for each agent. A convincing sample for ChatGPT consists of a question, a gold answer, and a 'corrective' human explanation that can rectify its initial incorrect answer. Then Bard and Claude2 use it in-context during discussion to convince ChatGPT.</p>
<p>prompt that instructs it to reason about the problem 'step-by-step'. See 'Phase 1' in Fig. 2 and the prompt is shown in Fig. 5 in Appendix A.2.</p>
<h3>Phase 2: Multi-round Discussion. RECONCILE</h3>
<p>Then enters a discussion phase, consisting of <em>R</em> rounds (see 'Phase 2' in Fig. 2). In discussion round <em>r</em>, for each agent <em>A<sub>i</sub></em>, RECONCILE develops a discussion prompt <em>D<sup>(r)</sup><sub>i</sub></em> (as shown in Fig. 5), consisting of the following three components.</p>
<h4>(a) Grouped responses of all agents from the previous round.</h4>
<p><em>D<sup>(r)</sup><sub>i</sub></em> consists of the answers {<em>a<sup>(r−1)</sup><sub>j</sub></em>}<em>n</em> <em>j</em> and explanations {<em>e<sup>(r−1)</sup><sub>j</sub></em>}<em>n</em> of all agents from round (<em>r</em> − 1). To foster better discussions, RECONCILE summarizes this information by grouping the answers into distinct categories and appends all plausible explanations for each answer, as shown in our discussion prompt (Appendix Fig. 5) and on the left side of Fig. 2.</p>
<h4>(b) Confidence associated with the answers.</h4>
<p>All agents are not equally confident in their answers. Hence, an effective discussion should also consider each agent's uncertainty. For all black-box models, we estimate its confidence <em>p<sup>(r)</sup><sub>i</sub></em> in round <em>r</em> by directly prompting the agent to verbally quantify its uncertainty, which in past work has been shown to be effective (Xiong et al., 2023b). See Appendix Fig. 5 for the usage of confidence in discussion.</p>
<h4>(c) Convincing samples from all other agents.</h4>
<p>Finally, the prompt contains convincing samples <em>C<sub>j</sub></em> for all other agents <em>A<sub>j≠i</sub></em>. When an agent tries to reassess its reasoning in light of the reasoning provided by other agents, we hypothesize that it should benefit from conditioning on demonstrations that can convince other agents. In order to obtain such convincing samples for an agent <em>A<sub>j</sub></em>, we select a small number of samples (4 in our experiments) for which the agent's initial answer is wrong but conditioning on the corresponding human explanation, rectifies the answer (see Fig. 3). For datasets that <em>do not</em> come with human explanations (e.g., the date understanding task in our experiments), we develop RECONCILE without using any convincing sample in the discussion prompt and still obtain large improvements (see §6.2 for details).</p>
<p>We now define the discussion prompt <em>D<sup>(r)</sup><sub>i</sub></em> = {<em>a<sup>(r−1)</sup><sub>j</sub></em>, <em>e<sup>(r−1)</sup><sub>j</sub></em>, <em>p<sup>(r−1)</sup><sub>i</sub></em>, <em>C<sub>j≠i</sub></em>}<em>n</em> <em>j</em> for each agent <em>A<sub>i</sub></em> in round <em>r</em>, based on the above three components. The agent conditions on it to generate an updated answer <em>a<sup>(r)</sup><sub>i</sub></em>, explanation <em>e<sup>(r)</sup><sub>i</sub></em>, and confidence <em>p<sup>(r)</sup><sub>i</sub></em>, to be used in the next round. Demonstrations of convincing explanations enable the agent to generate explanations that are more likely to convince other agents to reach a better consensus.</p>
<h3>Phase 3: Team Answer Generation. RECONCILE</h3>
<p>Continues the discussion for a maximum of <em>R</em> rounds or terminates it as soon as a consensus is reached (i.e., all agents agree on the same answer). At the end of any round <em>r</em>, RECONCILE generates the team answer <em>a<sup>(r)</sup></em> for that round using a weighted voting scheme (see the right side of Fig. 2). In particular, we recalibrate each agent's confidence using a function <em>f</em>(·) and then use these as weights to compute the team answer, as follows:</p>
<p>$$
\hat{a}^{(r)} = \arg\max_{a} \sum_{i} f(p_{i}^{(r)})\mathbbm{1}(\hat{a}_{i}^{(r)} = a)
$$</p>
<p>where <em>a</em> is a distinct answer generated by any of the agents, <em>p<sup>(r)</sup><sub>i</sub></em> is the original confidence of agent <em>A<sub>i</sub></em> in round <em>r</em> and <em>f</em>(<em>p<sup>(r)</sup><sub>i</sub></em>) is the corresponding recalibrated confidence. While an unweighted majority vote and uncalibrated confidence-weighted vote also work well in practice, we use the calibrated weighted vote because it not only obtains slightly better results but the same recalibration strategy also works out-of-the-box for all seven tasks that</p>
<p><sup>4</sup>We did not include an agent's own convincing samples in the prompt because an agent is expected to specifically convince <em>other</em> agents. We also verify this empirically – additionally including self-convincing samples in the prompt leads to comparable performance.</p>
<p>we experiment with (see Appendix B.5 for more details of our recalibration function $f(\cdot)$ ).</p>
<h2>5 Experimental Setup</h2>
<p>Agents in RECONCILE. We primarily implement RECONCILE with ChatGPT, Bard, and Claude2 as the three agents, engaging them in up to three rounds of discussion. Later in $\S 6.1$, we also show the generalizability of our RECONCILE framework with different choices of agents, including APIbased (GPT-4), open-source (LLaMA-2-70B), and domain-specific (DeepSeekMath) agents.
Datasets. We evaluate RECONCILE on seven benchmarks, including two commonsense, three math, one logical reasoning, and one NLI task. These are: (1) StrategyQA (Geva et al., 2021), (2) CommonsenseQA (CSQA; (Aggarwal et al., 2021; Talmor et al., 2019)), (3) GSM8K (Cobbe et al., 2021), (4) AQuA (Ling et al., 2017), (5) MATH (Hendrycks et al., 2021), (6) Date Understanding (BIG-bench collaboration, 2023), and (7) ANLI (Nie et al., 2020).
Baselines. We compare RECONCILE to prior works in three categories:</p>
<ul>
<li>Vanilla single-agent methods. In this category, we experiment with (1) zero-shot CoT prompting (Kojima et al., 2022) with one of the interacting LLMs, and (2) eight-shot CoT with Claude2 where the number eight matches the number of convincing samples used in RECONCILE.</li>
<li>Advanced single-agent methods. Next, we compare with (1) Self-Refine (SR) that iteratively generates feedback and refines the output leveraging the model itself (Madaan et al., 2023), (2) Self-Consistency (SC) that samples multiple reasoning paths and generates the most consistent answer (Wang et al., 2023b), and (3) their combination, SR+SC, that first conducts multiple iterations of refinement, followed by a majority vote. Note that in RECONCILE, the number of LLM calls per instance can vary between 3, 6, and 9 based on the number of discussion rounds. Hence, for a fair comparison, we implement SC with the same average number of LLM calls as in RECONCILE. Later in Appendix B.3, we show that RECONCILE even outperforms 9-way SC (that equates to the worst-case LLM calls in RECONCILE).</li>
<li>Multi-agent methods with a single backbone model. Our final baselines are two multi-agent debating methods: a multi-agent debate between
multiple ChatGPT instances (Du et al., 2023) and a debate with judge method (Liang et al., 2023). These methods use multiple instances of the same underlying model (ChatGPT) as different agents.</li>
</ul>
<p>Implementation Details. Owing to the cost associated with API-based models and the limit imposed on the number of API calls, we follow many prior works (Du et al., 2023; Bian et al., 2023; Besta et al., 2023; Yao et al., 2023a) to experiment with a subset of 100 samples (from the validation set for StrategyQA and the test set for all other datasets). Later in Appendix B.1, we also experiment on the full test sets of StrategyQA and Date understanding and find similar trends. We report accuracy and its standard deviation. For each experiment, we conduct at least three runs on the same test samples with the same prompts, primarily accounting for the variance caused by the decoding strategy. Other implementation details can be found in Appendix A.1.</p>
<h2>6 Results</h2>
<h3>6.1 Main Results</h3>
<p>RECONCILE outperforms single-agent and multi-agent baselines. We first evaluate the overall reasoning capabilities of RECONCILE in Table 2 with ChatGPT, Bard, and Claude2 as the three agents. For fair comparisons, all iterative methods go through 3 rounds of iteration and all singlemodel multi-agent baselines are implemented with three agents with a sufficiently high temperature of 1.0 for maximizing diversity. Across all five datasets, RECONCILE outperforms all single-agent and multi-agent baselines that are built on top of the same models (see last row). Notably, without using GPT-4 as an agent, our method outperforms GPT-4 on commonsense tasks like StrategyQA and CSQA and obtains comparable performance to GPT-4 on most other tasks. GPT-4's especially strong results on GSM8K could be attributed in part to the inclusion of some of GSM8K's training samples in GPT-4's pre-training data (OpenAI, 2023). While multi-agent debate with ChatGPT (Du et al., 2023) improves results on math benchmarks, debate with multiple Bard or Claude2 instances is not effective, possibly because the responses (generated from the same model) are not sufficiently diverse. When they team up with ChatGPT in a multi-round discussion, RECONCILE outperforms debate frameworks. It obtains maximum gains of</p>
<table>
<thead>
<tr>
<th>Method Category</th>
<th>Method</th>
<th>Agent</th>
<th>StrategyQA</th>
<th>CSQA</th>
<th>GSM8K</th>
<th>AQuA</th>
<th>Date</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vanilla <br> Single-agent</td>
<td>Zero-shot CoT</td>
<td>GPT-4</td>
<td>75.6$\pm$4.7</td>
<td>73.3$\pm$0.4</td>
<td>90.7$\pm$1.7</td>
<td>65.7$\pm$4.6</td>
<td>89.0$\pm$2.2</td>
</tr>
<tr>
<td></td>
<td>Zero-shot CoT</td>
<td>ChatGPT</td>
<td>67.3$\pm$3.6</td>
<td>66.0$\pm$1.8</td>
<td>73.7$\pm$3.1</td>
<td>44.7$\pm$0.5</td>
<td>67.7$\pm$1.2</td>
</tr>
<tr>
<td></td>
<td>Zero-shot CoT</td>
<td>Bard</td>
<td>69.3$\pm$4.4</td>
<td>56.8$\pm$2.7</td>
<td>58.7$\pm$2.6</td>
<td>33.7$\pm$1.2</td>
<td>50.2$\pm$2.2</td>
</tr>
<tr>
<td></td>
<td>Zero-shot CoT</td>
<td>Claude2</td>
<td>73.7$\pm$3.1</td>
<td>66.7$\pm$2.1</td>
<td>79.3$\pm$3.6</td>
<td>60.3$\pm$1.2</td>
<td>78.7$\pm$2.1</td>
</tr>
<tr>
<td></td>
<td>Eight-shot CoT</td>
<td>Claude2</td>
<td>74.3$\pm$0.8</td>
<td>68.3$\pm$1.7</td>
<td>84.7$\pm$0.9</td>
<td>64.7$\pm$1.2</td>
<td>78.7$\pm$1.7</td>
</tr>
<tr>
<td>Advanced <br> Single-agent</td>
<td>Self-Refine (SR)</td>
<td>ChatGPT</td>
<td>66.7$\pm$2.7</td>
<td>68.1$\pm$1.8</td>
<td>74.3$\pm$2.5</td>
<td>45.3$\pm$2.2</td>
<td>66.3$\pm$2.1</td>
</tr>
<tr>
<td></td>
<td>Self-Consistency (SC)</td>
<td>ChatGPT</td>
<td>73.3$\pm$0.5</td>
<td>73.0$\pm$0.8</td>
<td>82.7$\pm$0.5</td>
<td>60.3$\pm$1.2</td>
<td>69.3$\pm$0.4</td>
</tr>
<tr>
<td></td>
<td>SR + SC</td>
<td>ChatGPT</td>
<td>72.2$\pm$1.9</td>
<td>71.9$\pm$2.1</td>
<td>81.3$\pm$1.7</td>
<td>58.3$\pm$3.7</td>
<td>68.7$\pm$1.2</td>
</tr>
<tr>
<td>Single-model <br> Multi-agent</td>
<td>Debate</td>
<td>$\square \times 3$</td>
<td>66.7$\pm$3.1</td>
<td>62.7$\pm$1.2</td>
<td>83.0$\pm$2.2</td>
<td>65.3$\pm$3.1</td>
<td>68.0$\pm$1.6</td>
</tr>
<tr>
<td></td>
<td>Debate</td>
<td>$\boldsymbol{\Delta} \times 3$</td>
<td>65.3$\pm$2.5</td>
<td>66.3$\pm$2.1</td>
<td>56.3$\pm$1.2</td>
<td>29.3$\pm$4.2</td>
<td>46.0$\pm$2.2</td>
</tr>
<tr>
<td></td>
<td>Debate</td>
<td>$\boldsymbol{\Delta} \times 3$</td>
<td>71.3$\pm$2.2</td>
<td>68.3$\pm$1.7</td>
<td>70.7$\pm$4.8</td>
<td>62.7$\pm$2.6</td>
<td>75.3$\pm$3.3</td>
</tr>
<tr>
<td></td>
<td>Debate+Judge</td>
<td>$\square \times 3$</td>
<td>69.7$\pm$2.1</td>
<td>63.7$\pm$2.5</td>
<td>74.3$\pm$2.9</td>
<td>57.3$\pm$2.1</td>
<td>67.7$\pm$0.5</td>
</tr>
<tr>
<td>Multi-model Multi-agent</td>
<td>RECONCILE</td>
<td>$\square, \diamond, \Delta$</td>
<td>79.0$\pm$1.6</td>
<td>74.7$\pm$0.4</td>
<td>85.3$\pm$2.2</td>
<td>66.0$\pm$0.8</td>
<td>86.7$\pm$1.2</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of RECONCILE (using ChatGPT, Bard, Claude2) with vanilla and advanced single-agent methods and multi-agent debating frameworks. Across all reasoning benchmarks, RECONCILE outperforms all prior single-agent and multi-agent methods. On commonsense tasks (StrategyQA and CSQA), RECONCILE also outperforms GPT-4. All results are on a random subset of 100 samples. The agents are $\square$ GPT-4, $\square$ ChatGPT, $\diamond$ Bard, and $\Delta$ Claude2.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Accuracy</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Best Single-agent (zero-shot)</td>
<td>75.6 ( $\square$ )</td>
<td>73.7 ( )</td>
</tr>
<tr>
<td>Best Multi-agent (Debate)</td>
<td>83.7 ( $\square \times 3$ )</td>
<td>71.3 ( $\square \times 3$ )</td>
</tr>
<tr>
<td>RECONCILE</td>
<td>87.7 ( $\square, \diamond, \Delta$ )</td>
<td>78.0 ( $\square, \Delta, \Delta$ )</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of the best single-agent, best multiagent, and RECONCILE on StrategyQA for a given combination of three agents. RECONCILE flexibly incorporates agents with varying strengths, such as a stronger model like $\square$ GPT-4, or an open-source model like $\square$ LLaMA2-70B.
$11.4 \%(75.3 \% \rightarrow 86.7 \%)$ on date understanding and $7.7 \%(71.3 \% \rightarrow 79.0 \%)$ on StrategyQA when compared to the strongest baseline (multi-agent debate with Claude2). Improvements in the math reasoning tasks are relatively moderate, because of ChatGPT's initial strong performance. However, as demonstrated later in Table 4, integrating a specialized math reasoning model into RECONCILE significantly boosts team performance.</p>
<h2>RECONCILE generalizes to agents of varying</h2>
<p>strengths. Next, we vary the agents in RECONCILE to study its generalization as a multi-agent framework. In particular, we either include (a) a stronger GPT-4 model, or (b) an open-source LLaMA-2-70B-chat model in the discussion. As shown in Table 3, in both these scenarios, RECONCILE outperforms the best single-agent and multiagent baselines, notably even outperforming the zero-shot GPT-4 performance by $12.1 \%(75.6 \% \rightarrow$ $87.7 \%$ ) on StrategyQA. This highlights the potential of a stronger agent to also obtain useful external feedback from comparatively weaker agents.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4 (zero-shot)</td>
<td style="text-align: center;">$44.0(\square)$</td>
</tr>
<tr>
<td style="text-align: left;">Best Single-agent (zero-shot)</td>
<td style="text-align: center;">$50.5(\Delta)$</td>
</tr>
<tr>
<td style="text-align: left;">Best Multi-agent (Debate)</td>
<td style="text-align: center;">$48.7(\square \times 3)$</td>
</tr>
<tr>
<td style="text-align: left;">RECONCILE</td>
<td style="text-align: center;">$\mathbf{5 8 . 3}(\square, \Delta, \Delta)$</td>
</tr>
</tbody>
</table>
<p>Table 4: RECONCILE generalizes to specialized models like $\square$ DeepSeekMath and improves on a challenging mathematical reasoning benchmark, MATH.</p>
<p>RECONCILE generalizes to domain-specific agents. So far, we have experimented with RECONCILE variants that employed general-purpose models like ChatGPT as agents. Our next result in Table 4 shows that even for tasks that require substantial domain knowledge (e.g., the MATH benchmark (Hendrycks et al., 2021)), RECONCILE is flexible enough to utilize and improve upon specialized, domain-specific models. Recently, Shao et al. (2024) proposed DeepSeekMath, a 7B model pre-trained on a large number of mathrelated web corpus and improving over GPT-4. Notably, RECONCILE with GPT-4, Claude2, and DeepSeekMath as agents significantly outperforms zero-shot DeepSeekMath and GPT4-based Debate by $7.8 \%$ and $9.6 \%$ respectively. In summary, RECONCILE shows consistent improvements across a wide range of agent combinations (involving APIbased, open-source, and domain-specific models).</p>
<h2>RECONCILE also improves Natural Language</h2>
<p>Inference. While all our previous results were with reasoning tasks, we also demonstrate RECONCILE's effectiveness on ANLI (Nie et al., 2020),</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Method</th>
<th>Accuracy</th>
<th>D (A1, A2)</th>
<th>D (A1, A3)</th>
<th>D (A2, A3)</th>
<th>D (A1, A2, A3)</th>
</tr>
</thead>
<tbody>
<tr>
<td>BERTScore</td>
<td>RECONCILE ( Paraphrased)</td>
<td>72.2</td>
<td>0.9364</td>
<td>0.9376</td>
<td>0.9453</td>
<td>0.9398</td>
</tr>
<tr>
<td></td>
<td>RECONCILE ( $\square \times 3$ )</td>
<td>72.2</td>
<td>0.9077</td>
<td>0.9181</td>
<td>0.9049</td>
<td>0.9102</td>
</tr>
<tr>
<td></td>
<td>RECONCILE ( $\square, \boldsymbol{\phi}<em i="i">{p}, \boldsymbol{\phi}</em>$ )</td>
<td>79.0</td>
<td>$\mathbf{0 . 8 8 9 1}$</td>
<td>$\mathbf{0 . 8 8 3 3}$</td>
<td>$\mathbf{0 . 8 4 9 3}$</td>
<td>$\mathbf{0 . 8 7 3 9}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of diversity between (a) paraphrased responses (first row) and (b) responses from multiple instances of the same ChatGPT model (second row). RECONCILE with a multi-model component also leads to higher accuracy. Responses from different models in RECONCILE (last row) are most diverse (i.e., less similar).</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Best Single-agent (zero-shot)</td>
<td>51.3 ( )</td>
</tr>
<tr>
<td>Best Multi-agent (Debate)</td>
<td>48.3 ( $\square \times 3$ )</td>
</tr>
<tr>
<td>RECONCILE</td>
<td>$\mathbf{5 7 . 7 ( \ominus , \boldsymbol { \phi } _ { p } , \boldsymbol { \phi } )}$</td>
</tr>
</tbody>
</table>
<p>Table 6: ReConcile improves a challenging NLI benchmark (ANLI), outperforming Debate by $9.4 \%$.
a challenging Natural Language Inference benchmark. Table 6 shows that RECONCILE on ANLI outperforms Debate by a significant $9.4 \%$, pointing to its widespread applicability.</p>
<h3>6.2 Ablations and Analysis of RECONCILE</h3>
<p>Each component of RECONCILE improves reasoning. In Table 7, we evaluate individual components of RECONCILE on StrategyQA. In particular, we compare four variants: (1) w/o Multiple Models: We use ChatGPT as the backbone for all three agents, (2) w/o Grouping: We simply concatenate the responses from different agents without grouping their answers, (3) w/o Convincingness: We remove convincing samples from all prompts, and (4) w/o Confidence Estimation: We do not use any confidence estimates during the discussion and compute majority vote as the team answer. We show that each component has a positive impact on RECONCILE with varying capacities. The effect of different models as agents is particularly significant and we observe a $6.8 \%$ improvement compared to only using ChatGPT as all three agents. This reinforces our hypothesis (and further verified below in 'Diversity Analysis') that diverse LLMs have complementary strengths and when put together in a round table discussion, they can learn from diverse external feedback from other agents and refine their responses to reach a better consensus. Notably, convincing samples lead to a $4.5 \%$ improvement in accuracy. In Appendix B.2, we study the role of convincing samples to show that (1) they also improve other interaction frameworks, and (2) even in the absence of such examples, RECONCILE outperforms debate baselines.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RECONCILE</td>
<td style="text-align: center;">$\mathbf{7 9 . 0}_{2: 1.6}$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Multiple Models</td>
<td style="text-align: center;">$72.2_{3: 2.1}$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Grouping</td>
<td style="text-align: center;">$76.7_{3: 2.5}$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Convincingness</td>
<td style="text-align: center;">$74.5_{3: 1.7}$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Conf Estimation</td>
<td style="text-align: center;">$77.7_{3: 1.3}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablations of RECONCILE on StrategyQA.</p>
<p>Different models enhance response diversity. As was shown in Table 7, RECONCILE obtains the most improvements via its multi-model component. This surpasses RECONCILE with multiple ChatGPT instances, even when the generations sampled from these instances are encouraged to exhibit high diversity with a sufficiently high temperature. To further validate the importance of having multiple models and the diversity brought about by them, we develop a diversity metric. We hypothesize that if explanations from different models are indeed more diverse than those generated from multiple instances of the same model (e.g., in Multiagent Debate), then our diversity metric should capture that. With that goal, we define diversity between multiple agents as the summation of the pairwise diversity between agents: $D\left(A_{1}, A_{2}, A_{3}\right)=$ $D\left(A_{1}, A_{2}\right)+D\left(A_{1}, A_{3}\right)+D\left(A_{2}, A_{3}\right)$, where $A_{1}$, $A_{2}$, and $A_{3}$ are the three agents' initial responses (either belonging to the same underlying model or different models). We then measure pairwise diversity by computing the cosine similarity between the response embeddings with BERTScore (Zhang et al., 2019). Note that lower similarity scores will mean greater diversity. With the diversity metric defined, we compute this metric for three variants: (a) paraphrased responses of a single ChatGPT to serve as a baseline, (b) responses from RECONCILE using three instances of a single ChatGPT model, and (c) responses from RECONCILE with ChatGPT, Bard, and Claude2 as agents. In Table 5, we show that responses from different models exhibit the highest diversity (yielding the lowest similarity score of 0.8739 ) and also the highest accuracy $(79.0 \%)$, followed by the single-model variant</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: RECONCILE achieves better and faster consensus. (a) Comparison of RECONCILE with Debate baselines showing the accuracy after each round. (b) Fraction of samples for which a consensus is reached after each round. (c) Accuracy as a function of consensus.</p>
<table>
<thead>
<tr>
<th>Round</th>
<th>ChatGPT</th>
<th>Bard</th>
<th>Claude2</th>
<th>Team</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>71.0±2.1</td>
<td>71.7±0.9</td>
<td>73.7±1.7</td>
<td>74.3±1.2</td>
</tr>
<tr>
<td>1</td>
<td>71.3±0.9</td>
<td>77.7±1.2</td>
<td>75.3±0.8</td>
<td>77.0±0.9</td>
</tr>
<tr>
<td>2</td>
<td>76.7±0.8</td>
<td>77.3±1.4</td>
<td>77.7±0.9</td>
<td>79.0±0.5</td>
</tr>
<tr>
<td>3</td>
<td>77.0±0.9</td>
<td>76.7±0.8</td>
<td>77.0±1.2</td>
<td>78.7±1.2</td>
</tr>
</tbody>
</table>
<p>Table 8: The round-wise accuracy of ChatGPT, Bard, and Claude2 and their team performance (using weighted vote) on StrategyQA.</p>
<p>(with a similarity score of 0.9102) and the paraphrased variant (with a similarity score of 0.9398). Thus, the higher diversity of (multi-model) RECONCILE means that agents have access to alternate solutions and external feedback, leading to better discussion and reasoning accuracy. We also present a case study in Appendix C.5 to illustrate that the debate baseline sometimes struggles with echo chambers, stemming from a lack of external feedback, supporting the need for external feedback for improving LLMs (Huang et al., 2023).</p>
<p><strong>RECONCILE improves all agents individually.</strong> We showed that the team performance of the agents improves through discussion. Next, in Table 8, we also present the accuracy of each agent after every round, as well as the overall team accuracy for StrategyQA. Evidently, the individual performance of each agent also improves alongside the team's performance.</p>
<p><strong>RECONCILE Reaches Faster and Better Consensus.</strong> RECONCILE terminates the discussion when a consensus is reached. More discussion rounds are costlier due to the increased API calls. Hence, achieving faster consensus while maintaining comparable accuracy gains is more efficient. To study this, in Fig. 4(a), we plot the accuracy trends after each round; in Fig. 4(b), we plot the fraction of samples for which consensus has been reached; and in Fig. 4(c), we analyze accuracy as a function of consensus. From the first plot, we make two important observations: (1) RECONCILE improves accuracy for two rounds, following which the accuracy saturates, (2) Compared to the debate baselines, RECONCILE is not only superior after every round but also peaks at a highest accuracy of 79.0% (vs 71.3% for the baselines). Next, from Fig. 4(b), our observations are also two-fold: (1) In the initial rounds (0 and 1), RECONCILE's consensus percentage is lower because the discussion takes place between diverse LLMs. Diverse agents lead to more differences in opinions initially. (2) However, as the discussion proceeds, RECONCILE establishes consensus for all samples by round 3, while in the baseline, 13% of the samples do not converge even after round 4. Finally, Fig. 4(c) shows that for the samples that enter the discussion phase (i.e., their initial answers did not have a consensus), accuracy is positively correlated with consensus. In other words, as a greater number of samples reach a consensus, accuracy proportionally improves. In summary, RECONCILE reaches <em>faster</em> and <em>better</em> consensus compared to baselines.</p>
<h1>7 Conclusion</h1>
<p>We presented RECONCILE, a multi-agent framework for reasoning with diverse LLM agents, engaged in multiple rounds of discussion via confidence estimation and generating explanations that can correctly convince other agents. RECONCILE demonstrated strong results on multiple reasoning benchmarks, consistently outperforming prior single-agent and multi-agent baselines and even improving upon GPT-4 on some benchmarks.</p>
<h2>Limitations</h2>
<p>For the API-based models used in RECONCILE, we note that we lack complete knowledge of the data that these models have been exposed to, and their scales in terms of parameters. Moreover, due to the API access, we do not possess complete control over their behavior. Depending on API-based models also necessitates the need to prompt these models to estimate their confidence. While this approach proves effective as evidenced by our results, we note that these estimates remain post-hoc in nature. Nevertheless, it is worth highlighting that these limitations could potentially be mitigated in the future should more open-sourced models emerge and demonstrate robust capabilities in adhering to long instructions.</p>
<h2>Acknowledgments</h2>
<p>We thank Peter Hase, Elias Stengel-Eskin, and the anonymous reviewers for useful feedback and suggestions regarding experiments for useful feedback and suggestions regarding experiments. This work was supported by NSF-CAREER Award 1846185, NSF-AI Engage Institute DRL-2112635, DARPA MCS Grant N66001-19-2-4031, Accelerate Foundation Models Research program, and a Google PhD Fellowship. The views contained in this article are those of the authors and not of the funding agency.</p>
<h2>References</h2>
<p>Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. 2021. Explanations for commonsenseqa: New dataset and models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3050-3065.</p>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,</p>
<p>Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report.</p>
<p>Anthropic. 2023. model card and evaluations for claude models.</p>
<p>Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2023. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687.</p>
<p>Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, and Ben He. 2023. Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. arXiv preprint arXiv:2303.16421.</p>
<p>BIG-bench collaboration. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547-5569. PMLR.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate.</p>
<p>Elias Stengel-Eskin and Benjamin Van Durme. 2023. Calibrated interpretation: Confidence estimation in semantic parsing. In TACL.</p>
<p>Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. 2024. Don't hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration. arXiv preprint arXiv:2402.00367.</p>
<p>Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023. Specializing smaller language models towards multi-step reasoning. In ICML.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of modern neural networks. In International conference on machine learning, pages 1321-1330. PMLR.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. NeurIPS.</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023. Large language models are reasoning teachers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14852-14882, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798.</p>
<p>Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation, 3(1):79-87.</p>
<p>Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023. LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14165-14178, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel, and Ethan Perez. 2024. Debating with more persuasive llms leads to more truthful answers. arXiv preprint arXiv:2402.06782.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857.</p>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a. Camel: Communicative agents for "mind" exploration of large scale language model society. arXiv preprint arXiv:2303.17760.</p>
<p>Shuang Li, Yilun Du, Joshua B Tenenbaum, Antonio Torralba, and Igor Mordatch. 2022a. Composing ensembles of pre-trained models via iterative consensus. International Conference on Learning Representations (ICLR).</p>
<p>Yanhong Li, Gang Kou, Guangxu Li, and Yi Peng. 2022b. Consensus reaching process in large-scale group decision making based on bounded confidence and social network. European Journal of Operational Research, 303(2):790-802.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023b. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5315-5333, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language models through multi-agent debate.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. ACL.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dzirri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback.</p>
<p>Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2023. Teaching small language models to reason. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1773-1781, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and YLan Boureau. 2022. Reducing conversational agents' overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857-872.</p>
<p>Marvin Minsky. 1988. Society Of Mind. Simon and Schuster.</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707.</p>
<p>Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. 2015. Obtaining well calibrated probabilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence, volume 29.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885-4901, Online. Association for Computational Linguistics.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>OpenAI. 2022. Chatgpt: Optimizing language models for dialogue.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.</p>
<p>John Platt et al. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61-74.</p>
<p>Omer Sagi and Lior Rokach. 2018. Ensemble learning: A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(4):e1249.</p>
<p>Swarnadeep Saha, Peter Hase, and Mohit Bansal. 2023. Can language models teach weaker agents? teacher explanations improve students via theory of mind. In NeurIPS.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022. Peer: A collaborative language model. arXiv preprint arXiv:2208.11663.</p>
<p>Zhihong Shao, Peiyi Wang, Runxin Xu Qihao Zhu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.</p>
<p>Noam Shazeer, <em>Azalia Mirhoseini, </em>Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations.</p>
<p>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.</p>
<p>Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. 2023. Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,</p>
<p>Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023a. Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2609-2634, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Yuqing Wang and Yun Zhao. 2023. Metacognitive prompting improves understanding in large language models. arXiv preprint arXiv:2308.05342.</p>
<p>Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2023c. Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. arXiv preprint arXiv:2307.05300.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. 2023a. Examining the inter-consistency of large language models: An in-depth analysis via debate.</p>
<p>Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023b. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.</p>
<p>Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2023. Cognitive mirage: A review of hallucinations in large language models. arXiv preprint arXiv:2309.06794.</p>
<p>Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488.</p>
<p>Andy Zeng, Maria Attarian, brian ichter, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. 2023. Socratic models: Composing zero-shot multimodal reasoning with language. In The Eleventh International Conference on Learning Representations.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Róbert Csordás, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. 2023. Mindstorms in natural language-based societies of mind. arXiv preprint arXiv:2305.17066.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>StrategyQA</th>
<th>Date</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT</td>
<td>68.1</td>
<td>69.3</td>
</tr>
<tr>
<td>Bard</td>
<td>70.6</td>
<td>52.8</td>
</tr>
<tr>
<td>Claude2</td>
<td>72.7</td>
<td>77.9</td>
</tr>
<tr>
<td>Multi-agent Debate</td>
<td>71.4</td>
<td>72.4</td>
</tr>
<tr>
<td>ReConcile</td>
<td>78.4</td>
<td>84.5</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison of RECONCILE with baselines on the full test sets of StrategyQA and Date Understanding.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Debate <em>Du et al. (2023)</em></td>
<td>66.7$\pm$3.1</td>
</tr>
<tr>
<td>RC (w/o Convincing Expl)</td>
<td>74.5$\pm$1.7</td>
</tr>
<tr>
<td>RC (w/ Random Expl)</td>
<td>75.0$\pm$2.5</td>
</tr>
<tr>
<td>RC (w/ Convincing Expl)</td>
<td>79.0$\pm$1.6</td>
</tr>
<tr>
<td>Debate (w/ Random Expl)</td>
<td>68.7$\pm$2.2</td>
</tr>
<tr>
<td>Debate (w/ Convincing Expl)</td>
<td>69.5$\pm$1.7</td>
</tr>
</tbody>
</table>
<h2>Appendix A Additional Details of RECONCILE</h2>
<h3>A.1 Implementation Details</h3>
<p>We provide more implementation details of RECONCILE in this section. During decoding, we set the temperature to 0.7 for ChatGPT and Bard and use the default setting for Claude2. All implementations involving ChatGPT are using gpt-3.5-turbo-0613 from Azure OpenAI. We retrieve results from Claude2 by posting requests to their webpage, and for Bard, we use chat-bison-001 from PaLM2 API. For each agent, we use four demonstrations of convincing samples. In addition, we provide the workflow of RECONCILE in Algorithm 1. Required input contains a test problem $Q$, maximum number of discussion rounds $R$, $n$ agents $\mathcal{A}=\left{A_{i}\right}<em i="i">{i=1}^{n}$, and convincing samples $\mathcal{C}=\left{C</em>$. For the open-source models LLaMA2-70B and DeepSeekMath, we use four RTX A6000 GPUs, each with 48GB memory to generate output from them.}\right}_{i=1}^{n}$ for each agent. The output would be the team answer $\hat{a}^{(r)</p>
<p>Figure 5: The prompts used in RECONCILE consist of an initial prompt and a discussion prompt.</p>
<h3>A.2 Initial Prompt and Discussion Prompt</h3>
<p>We show the prompts used in RECONCILE in Fig. 5. The initial prompt encompasses (1) the convincing samples that demonstrate how to convince other agents, (2) the test question, and (3) a requirement for 'step-by-step' reasoning. The prompt also instructs the agent to express their confidence level,</p>
<p>Table 10: Evaluation of the role of convincing samples on StrategyQA. RECONCILE (RC) without convincing samples outperforms multi-agent debate and with it obtains further gains. Convincing samples also boost the debate baseline.</p>
<ul>
<li>Table 9: Comparison of RECONCILE with baselines on the full test sets of StrategyQA and Date Understanding.</li>
</ul>
<table>
<thead>
<tr>
<th>Method</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Debate <em>Du et al. (2023)</em></td>
<td>66.7$\pm$3.1</td>
</tr>
<tr>
<td>RC (w/o Convincing Expl)</td>
<td>74.5$\pm$1.7</td>
</tr>
<tr>
<td>RC (w/ Random Expl)</td>
<td>75.0$\pm$2.5</td>
</tr>
<tr>
<td>RC (w/ Convincing Expl)</td>
<td>79.0$\pm$1.6</td>
</tr>
<tr>
<td>Debate (w/ Random Expl)</td>
<td>68.7$\pm$2.2</td>
</tr>
<tr>
<td>Debate (w/ Convincing Expl)</td>
<td>69.5$\pm$1.7</td>
</tr>
</tbody>
</table>
<p>Table 10: Evaluation of the role of convincing samples on StrategyQA. RECONCILE (RC) without convincing samples outperforms multi-agent debate and with it obtains further gains. Convincing samples also boost the debate baseline.</p>
<p>ranging from 0.0 to 1.0, indicating the likelihood of their answer being correct. The discussion prompt is an extension of the initial prompt, instructing the agent to review and express agreement or disagreement with other agents’ solutions. To facilitate discussions, we design a grouping scheme that aggregates information based on the current opinions at the table. For instance, if two agents affirm that the answer to a given question is 'yes' while the third agent disagrees with a 'no', the designed grouping mechanism in the discussion prompt consolidates this information rather than simply concatenating all responses.</p>
<h2>Appendix B Additional Results</h2>
<h3>B.1 Results on Full Test Sets</h3>
<p>In Table 2, we reported results with 100 test samples following several previous works and due to budget constraints. Upon experimenting on the full test sets of StrategyQA and Date Understanding, we confirm similar trends. Specifically, in Table 9, we compare RECONCILE to all of our major baselines and show that RECONCILE continues to outperform all baselines.</p>
<h3>B.2 Convincing Samples Improve Both RECONCILE and Multi-agent Debate</h3>
<p>Recall that RECONCILE selects a sample as convincing if the corresponding human explanation</p>
<div class="codehilite"><pre><span></span><code>\(\overline{\text { Algorithm 1 RECONCILE: A Group-Discuss-And-Convince Framework }}\)
Require: Test Problem \(Q\), Discussion Rounds \(R\), Agents \(\mathcal{A}=\left\{A_{i}\right\}_{i=1}^{n}\), Convincing Samples \(\mathcal{C}=\left\{C_{i}\right\}_{i=1}^{n}\)
    function \(\operatorname{ReConcile}(Q, R, \mathcal{A}, \mathcal{C})\)
        \(r \leftarrow 0\)
        while \(r \leq R\) and not \(\operatorname{ConSENSUS}\left(Q,\left\{a_{i}^{(r-1)}\right\}_{i=1}^{n}\right)\) do
            \(S \leftarrow[]_{i} P \leftarrow[]\)
            for each \(A_{i} \in \mathcal{A}\) do
                if \(r=0\) then
                    \(P_{I} \leftarrow(Q, \mathcal{C})\)
                    \(a_{i}^{(0)}, c_{i}^{(0)}, p_{i}^{(0)} \leftarrow A_{i}\left(P_{I}\right)\)
            else
                \(P_{D} \leftarrow\left(Q, a_{i}^{(r-1)}, c_{i}^{(r-1)}, p_{i}^{(r-1)}, \mathcal{C}\right)\)
                \(a_{i}^{(r)}, c_{i}^{(r)}, p_{i}^{(r)} \leftarrow A_{i}\left(P_{D}\right)\)
            end if
                \(S \leftarrow S+\left[a_{i}^{(r)}\right], P \leftarrow P+\left[p_{i}^{(r)}\right]\)
            end for
                \(\hat{a}^{(r)} \leftarrow \operatorname{WeIGHTEdVote}(S, P)\)
            end while
            return \(\hat{a}^{(r)}\)
end function
</code></pre></div>

<p>$\triangleright$ Initial prompt consists of question and convincing samples
$\triangleright$ Generate initial answer, explanation, and confidence
$\triangleright$ Discussion prompt
$\triangleright$ Append each agent's answer and confidence
$\triangleright$ Get team answer through a confidence weighted vote
rectifies an agent's incorrect answer. Based on this, Table 7 showed that by collecting only four human explanations, we can obtain significant improvements ('w/o Convincingness' row). Next, we consider a scenario where no human explanations are present. Table 10 shows that even then, RECONCILE outperforms the debate baseline by absolute 7.8 points (second row). If random (i.e., general human explanations that may not necessarily ensure answer rectification) are available (third row), we obtain some small improvements; but our convincing samples that are selected based on our novel answer-rectification criterion (fourth row) improve the results substantially. See Sections C. 3 and C. 4 for illustrative examples. Being able to convince another agent is also a generic concept that can be applied to other multi-agent systems, as demonstrated by improvements in the debate baseline (last row).</p>
<h3>B. 3 Comparison with Other Methods</h3>
<p>In Table 11, we compare RECONCILE to two other single-agent variants. While in our main Table 2, we experimented with a random 8 -shot Claude2 baseline, here we replace the in-context samples with our convincing samples. Even then, RECONCILE exhibits superior performance on all datasets except for GSM8K, again highlighting the importance of collaboration between diverse models. Next, we also report results for 9-way Self-Consistency which in terms of LLM calls represents the worst-case scenario of RECONCILE even for a more open-ended dataset like GSM8K, 9 LLM calls (i.e., 3 discussion rounds) happen in
only $12 \%$ of the samples and an even lesser $9 \%$ on multiple-choice QA dataset like Date understanding. That said, RECONCILE continues to outperform 9-way SC by a large margin on most datasets.</p>
<h2>B. 4 Recalibration Strategy of RECONCILE</h2>
<p>Directly using confidence scores as the voting weights is less effective due to the overconfidence problem of LLMs (Xiong et al., 2023b; Tian et al., 2023; Mielke et al., 2022). Specifically, LLMs tend to produce consistently high confidence scores, which can make it challenging to discern subtle distinctions in confidence levels across different outputs. To address this, we employ a simple yet effective rescaling technique, facilitating better differentiation of confidence levels. This is expressed as:</p>
<p>$$
f\left(p_{i}^{(r)}\right)= \begin{cases}1.0, &amp; \text { if } p_{i}^{(r)}=1.0 \ 0.8, &amp; \text { if } 0.9 \leq p_{i}^{(r)}&lt;1.0 \ 0.5, &amp; \text { if } 0.8 \leq p_{i}^{(r)}&lt;0.9 \ 0.3, &amp; \text { if } 0.6&lt;p_{i}^{(r)}&lt;0.8 \ 0.1, &amp; \text { otherwise }\end{cases}
$$</p>
<p>where $p_{i}^{(r)}$ is the original confidence of agent $A_{i}$ in round $r$ and $f\left(p_{i}^{(r)}\right)$ is the corresponding adjusted score. To decide the optimal weights, we compare with a variety of settings including the majority vote and the uncalibrated confidence-weighted vote. The results are summarized in Table 13. We denote the weight we used in our main experiment as $w^{*}=[1.0,0.8,0.5,0.3,0.1]$ where each value corresponds to the recalibrated confidence score. We further compare with other settings:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>StrategyQA</th>
<th>CSQA</th>
<th>GSM8K</th>
<th>AQuA</th>
<th>Date</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claude2 (w/ 8-shot convincing samples)</td>
<td>74.0$\pm$0.0</td>
<td>69.7$\pm$1.2</td>
<td>85.3$\pm$0.5</td>
<td>64.3$\pm$1.2</td>
<td>81.3$\pm$0.5</td>
</tr>
<tr>
<td>Self-Consistency w/ ChatGPT (9-way)</td>
<td>74.7$\pm$0.8</td>
<td>73.3$\pm$1.2</td>
<td>85.7$\pm$0.4</td>
<td>62.7$\pm$1.2</td>
<td>70.3$\pm$0.9</td>
</tr>
<tr>
<td>RECONCILE</td>
<td>79.0$\pm$1.6</td>
<td>74.7$\pm$0.4</td>
<td>85.3$\pm$2.2</td>
<td>66.0$\pm$0.8</td>
<td>86.7$\pm$1.2</td>
</tr>
</tbody>
</table>
<p>Table 11: Comparison of RECONCILE with Claude2 using 8-shot convincing samples and 9-way Self-Consistency.</p>
<table>
<thead>
<tr>
<th></th>
<th>Max Conf</th>
<th>Majority Vote</th>
<th>Weighted Vote</th>
</tr>
</thead>
<tbody>
<tr>
<td>Accuracy</td>
<td>74.7$\pm$2.1</td>
<td>77.1$\pm$1.3</td>
<td>79.0$\pm$0.5</td>
</tr>
</tbody>
</table>
<p>Table 12: Performance comparison of different voting strategies on StrategyQA. Weighted vote performs the best compared to simple majority vote and choosing the agent’s answer with highest confidence.</p>
<table>
<thead>
<tr>
<th>Voting weight</th>
<th>StrategyQA</th>
<th>GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td>$w_{1}$</td>
<td>0.77</td>
<td>0.84</td>
</tr>
<tr>
<td>$w_{2}$</td>
<td>0.79</td>
<td>0.83</td>
</tr>
<tr>
<td>$w_{3}$</td>
<td>0.78</td>
<td>0.82</td>
</tr>
<tr>
<td>$w_{4}$</td>
<td>0.77</td>
<td>0.83</td>
</tr>
<tr>
<td>Majority</td>
<td>0.76</td>
<td>0.83</td>
</tr>
<tr>
<td>Uncalibrated</td>
<td>0.78</td>
<td>0.84</td>
</tr>
<tr>
<td>$w^{*}$ (Ours)</td>
<td>$\mathbf{0 . 7 9}$</td>
<td>$\mathbf{0 . 8 5}$</td>
</tr>
</tbody>
</table>
<p>Table 13: The robustness of the recalibation weight. We use the same weights $w^{*}$ across all datasets.</p>
<ul>
<li>$w_{1}=[1.0,0.9,0.7,0.5,0.3]$</li>
<li>$w_{2}=[1.0,0.9,0.5,0.3,0.1]$</li>
<li>$w_{3}=[1.0,0.8,0.6,0.4,0.2]$</li>
<li>$w_{4}=[1.0,0.75,0.5,0.25,0.0]$
and the results show that our $w^{<em>}$ works the best across datasets. In our main experiment, we fix the weight using $w^{</em>}$ and it is constantly outperforming majority vote across all seven datasets. In addition, Fig. 9 shows that it helps reduce the Expected Calibration Error (ECE), a popular calibration metric <em>Naeini et al. (2015)</em>. While we note that recalibration can also be achieved through a learned model (e.g., Platt Scaling <em>Platt et al. (1999)</em>), we refrain from using such models because RECONCILE is primarily designed as a few-shot method, and developing a recalibration model would necessitate access to a substantial number of annotated samples. Therefore, we use $f\left(p_{i}^{(r)}\right)$ to perform a weighted vote to generate the team answer.</li>
</ul>
<h3>B.5 Comparison of Different Voting Strategies</h3>
<p>At the end of any round $r$, every agent in RECONCILE generates its answer. Here we explore three voting strategies: (1) maximum confidence vote, where the agent’s answer with the maximum confidence score would be the final team answer,</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>License</th>
</tr>
</thead>
<tbody>
<tr>
<td>StrategyQA</td>
<td>MIT License (License)</td>
</tr>
<tr>
<td>CommonsenseQA</td>
<td>MIT License (License)</td>
</tr>
<tr>
<td>GSM8K</td>
<td>MIT License (License)</td>
</tr>
<tr>
<td>AQuA</td>
<td>Apache 2.0 (License)</td>
</tr>
<tr>
<td>MATH</td>
<td>MIT License (License)</td>
</tr>
<tr>
<td>Date</td>
<td>Apache 2.0 (License)</td>
</tr>
<tr>
<td>ANLI</td>
<td>CC BY-NC 4.0 (License)</td>
</tr>
</tbody>
</table>
<p>Table 14: Dataset licenses
(2) unweighted majority vote, where each vote carries equal weight, irrespective of the confidence score, and (3) weighted vote, where we use the recalibrated confidence scores as the voting weights. As shown in Table 12, weighted vote is the most effective way to aggregate the team answer.</p>
<h2>C Qualitative Examples</h2>
<h3>C.1 Convincing Samples for Each Agent</h3>
<p>Table 15 shows examples of convincing samples on StrategyQA for each agent.</p>
<h3>C.2 Effect of Convincing Samples</h3>
<p>Here, we provide qualitative examples of how convincing samples change the way each agent responds to the question. We compare the initial responses (of each agent) with and without convincing samples in Table 16.</p>
<h3>C.3 RECONCILE w/o Convincing Samples</h3>
<p>We notice that when RECONCILE operates in the absence of convincing samples, the agents tend to maintain their initial opinions more often. As depicted in Fig. 6, all three agents adhere to their original stances throughout the entire discussion and hence never converge to the correct answer.</p>
<h3>C.4 RECONCILE with Convincing Samples</h3>
<p>On the contrary, when convincing samples are present, we show how the explanations of all agents change during the course of a discussion (see Fig. 7). Initially, Bard and Claude2 provide incorrect answers, but as the discussion unfolds, both</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: An illustrative example showing that without demonstrations of convincing samples, agents' opinions do not change much across the discussion rounds. All agents maintain their original stances and hence, never converge to the correct answer.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: An illustrative example showing that with access to demonstrations of convincing samples, agents' opinions and explanations change during the course of multiple discussion rounds.
agents revise their initial predictions, ultimately arriving at the correct answer.</p>
<h2>C. 5 Single-Model Multi-Agent Debate Struggles with Echo Chamber</h2>
<p>In Fig. 8, we provide an illustration of multi-agent debate, implemented with multiple instances of the same underlying ChatGPT model. In this case, an incorrect answer is initially provided, but because external feedback from diverse models is lacking,
all agents persist with the same incorrect response throughout the interaction.</p>
<h2>D Benchmark Licenses</h2>
<p>The licenses of datasets we used are in Table 14.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: An illustrative example of multi-agent debate with multiple instances of ChatGPT. Initially, an incorrect answer is provided, and due to a lack of external feedback from diverse models, all agents persist with the same erroneous response throughout the debate process.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Expected Calibration Error (ECE) (Naeini et al., 2015; Guo et al., 2017; Elias Stengel-Eskin and Benjamin Van Durme, 2023) (a) before and (b) after confidence rescaling in RECONCILE. We observe a significant drop in ECE, showing the effectiveness of our simple method.</p>
<p>QUESTION: Is the tongue part of a creature's head? (Gold answer: yes)
ChatGPT (InitiAL): The tongue is a muscular organ located in the mouth of most vertebrates, including humans. It is involved in functions such as taste, chewing, and speaking. The head, on the other hand, is the anatomical part that contains the brain, eyes, ears, nose, and mouth. Answer: no. Convincing Human Explanation: A creature's tongue is inside its mouth. A creature's mouth is part of its head.
ChatGPT (w/ Convincing Human Explanation): Based on the information provided, the tongue is inside a creature's mouth. The mouth is part of a creature's head. Answer: yes.</p>
<p>QUESTION: Are slime lilies in a different scientific family than asparagus? (Gold answer: no)
BARD (InitiAL): Slime lilies are in the scientific family Araceae while asparagus is in the scientific family Asparagaceae. Answer: yes.
Convincing Human Explanation: Asparagus is a species of plants of the Asparagaceae family. Slime lilies are the common name for the flowering albuca plant. The albuca plant belongs to the scientific family of Asparagaceae.
BARD (w/ Convincing Human Explanation): Slime lilies are in the scientific family of Asparagaceae, while asparagus is in the same family. Answer: no.</p>
<p>QUESTION: Would an astrologer focus on the densest terrestrial planet for a Friday horoscope? (Gold answer: yes)
Claude2 (Initial): Astrologers do not focus on Earth for Friday horoscopes specifically. Astrologers tend to focus on the positions of the Sun, Moon, and planets in the zodiac for horoscopes. The day of the week is not especially relevant. Answer: no.
Convincing Human Explanation: Friday is associated with Venus in astrology Venus is the densest of the terrestrial planets
Claude2 (w/ Convincing Human Explanation): Friday is associated with Venus in astrology, and Venus is the densest of the terrestrial planets (Mercury, Venus, Earth, Mars). So the question is asking if an astrologer would focus on Venus for a Friday horoscope. Answer: yes.</p>
<p>Table 15: Examples of convincing human explanations for each agent (ChatGPT, Bard, and Claude2). A convincing explanation is an instance of a answer-rectifying human explanation. For all three examples, the convincing explanation helps rectify an agent's initial incorrect answer.</p>
<p>QUESTION: Was Lil Jon's top ranked Billboard song a collaboration with a member of The Lox? (Gold answer: no)
ChatGPT (w/O CONVINCING): To determine if Lil Jon's top ranked Billboard song was a collaboration with a member of The Lox, we need to search for Lil Jon's discography and look for his highest-charting song on the Billboard charts. Then, we need to check if this song is a collaboration with a member of The Lox. Answer: no. Confidence: 0.8 .
ChatGPT (w/ Convincing): Lil Jon's top ranked Billboard song is 'Yeah!' which features vocals from Usher and Ludacris. There is no collaboration between Lil Jon and any member of The Lox on this song. Answer: no. Confidence: 0.9 .</p>
<p>QUESTION: Could ten gallons of seawater crush a six year old? (Gold answer: yes)
BARd (w/o CONVincing): Ten gallons of seawater weighs about 82 pounds. The average weight of a six year old is about 44 pounds. Therefore, ten gallons of seawater would be about twice as heavy as a six year old. It is possible that ten gallons of seawater could crush a six year old, but it is also possible that the six year old could survive. The outcome would depend on a number of factors, such as the size and strength of the six year old, the shape of the container holding the seawater, and the amount of time the six year old was submerged in the seawater. Answer: yes. Confidence: 0.7.
BARD (w/ Convincing): The density of seawater is about 1.025 grams per cubic centimeter. Ten gallons of seawater is about 37.85 liters. The volume of a human body is about 60 liters. Therefore, ten gallons of seawater would be about $63 \%$ of the volume of a human body. If ten gallons of seawater were to be poured on a six year old, it would be very difficult for the child to breathe and they would likely drown. Answer: yes. Confidence: 0.95 .</p>
<p>QUESTION: Would the top of Mount Fuji stick out of the Sea of Japan? (Gold answer: yes)
Claude2 (w/o Convincing): To determine if the top of Mount Fuji would stick out of the Sea of Japan, I need to compare the elevation of Mount Fuji to the depth of the Sea of Japan. According to Wikipedia, the elevation of Mount Fuji is 3,776 meters. The average depth of the Sea of Japan is 1,752 meters according to Wikipedia. Since the elevation of Mount Fuji is greater than the average depth of the Sea of Japan, I can conclude that the top of Mount Fuji would stick out above the surface of the Sea of Japan if it was placed there. Answer: yes. Confidence: 0.9.
Claude2 (w/ Convincing): Mount Fuji is the highest mountain in Japan, with a summit elevation of 12,389 feet or 3,776 meters above sea level. The average depth of the Sea of Japan is about 3,742 meters. Since the summit of Mount Fuji is higher than the average depth of the Sea of Japan, the top of Mount Fuji would stick out above the water if it was located in the middle of the Sea of Japan. Answer: yes. Confidence: 0.9 .</p>
<p>Table 16: Comparison of how each agent's reasoning changes with and without convincing samples. We observe that by conditioning on convincing samples, all agents tend to become more confident in their reasoning and generate less uncertain statements (shown in bold), which is also reflected in the actual confidence scores generated by each agent (e.g., goes up from 0.7 to 0.95 for Bard).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Code: https://github.com/dinobby/ReConcile&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ In this work, we refer to multi-agent as multiple instances of the same underlying model (e.g., ChatGPT), whereas multimodel model-agent refers to different models (e.g., ChatGPT, Bard and Claude2) as agents.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
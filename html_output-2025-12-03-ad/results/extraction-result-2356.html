<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2356 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2356</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2356</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-de20b6488e148a19ae6c63defbfca8a6373e4110</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/de20b6488e148a19ae6c63defbfca8a6373e4110" target="_blank">Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction</a></p>
                <p><strong>Paper Venue:</strong> ACS Central Science</p>
                <p><strong>Paper TL;DR:</strong> This work shows that a multihead attention Molecular Transformer model outperforms all algorithms in the literature, achieving a top-1 accuracy above 90% on a common benchmark data set and is able to handle inputs without a reactant–reagent split and including stereochemistry, which makes the method universally applicable.</p>
                <p><strong>Paper Abstract:</strong> Organic synthesis is one of the key stumbling blocks in medicinal chemistry. A necessary yet unsolved step in planning synthesis is solving the forward problem: Given reactants and reagents, predict the products. Similar to other work, we treat reaction prediction as a machine translation problem between simplified molecular-input line-entry system (SMILES) strings (a text-based representation) of reactants, reagents, and the products. We show that a multihead attention Molecular Transformer model outperforms all algorithms in the literature, achieving a top-1 accuracy above 90% on a common benchmark data set. Molecular Transformer makes predictions by inferring the correlations between the presence and absence of chemical motifs in the reactant, reagent, and product present in the data set. Our model requires no handcrafted rules and accurately predicts subtle chemical transformations. Crucially, our model can accurately estimate its own uncertainty, with an uncertainty score that is 89% accurate in terms of classifying whether a prediction is correct. Furthermore, we show that the model is able to handle inputs without a reactant–reagent split and including stereochemistry, which makes our method universally applicable.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2356.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2356.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular Transformer (multi-head attention model for chemical reaction prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully attention-based encoder-decoder transformer adapted to predict chemical reaction products from SMILES strings; provides uncertainty-calibrated, template-free reaction prediction without requiring atom-mapping or a reactant/reagent split.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Organic synthesis / chemical reaction outcome prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given reactants and reagents represented as SMILES, predict the product(s) of the chemical reaction (forward reaction prediction) and provide calibrated uncertainty for predictions to support multistep synthesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Abundant labeled reaction data available from patent-mined datasets (e.g., USPTO_MIT: ~479k reactions; USPTO_STEREO: ~1.0M; non-public Pistachio_2017 test: ~15k). Data are labeled (reactant/reagent -> product), canonicalized with RDKit, sometimes include stereochemistry; public and non-public subsets exist. Data quality variable (noise in stereochemistry, transcription errors), and some datasets filtered for constraints (e.g., LEF).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Text/sequential representation of molecules (SMILES strings) tokenized into tokens; effectively sequence-to-sequence data (discrete token sequences) that implicitly encode graph-structured chemistry; data augmentation used via randomized SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high: combinatorial explosion of possible reactions and products, complex regio-/chemo-/stereoselectivity, multistep dependencies (error compounds over steps), non-linear mappings from reactant SMILES to products; large vocabulary of chemical tokens and long-range dependencies in sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Domain of organic synthesis is mature (well-established chemistry) but computational reaction outcome prediction is an active, rapidly evolving subfield with multiple paradigms (template-based, graph-based, sequence-based); substantial prior work exists but limitations persist.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high — practical use in synthesis planning requires calibrated uncertainty and some interpretability (to prioritize steps/fail fast), though chemically plausible black-box predictions are acceptable for many tasks; mechanistic interpretability remains desirable for validation and for certain reaction classes (e.g., stereospecific transformations).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Transformer-based sequence-to-sequence model (Molecular Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Encoder-decoder transformer (multi-head self-attention) adapted for SMILES-to-SMILES translation: modified from Vaswani et al. with reduced size (4 layers of size 256 instead of 6x512, ~12M parameters), 8 attention heads, positional encodings, trained supervised on tokenized SMILES pairs; uses ADAM optimizer with learning rate schedule and 8000 warmup steps; no recurrent components; SMILES data augmentation (randomized SMILES) doubles training examples; weight averaging of last 20 checkpoints and optional ensembling used to improve performance; product-of-token-probabilities used as an uncertainty/confidence score (ROC AUC 0.89). Label smoothing was evaluated but turned off (0.0) because it degraded uncertainty discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (sequence-to-sequence deep learning / neural machine translation)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable — method directly maps to the sequence-structured SMILES data, requires no handcrafted reaction templates or atom-mapping, handles mixed reactant/reagent inputs and stereochemistry to a degree; modifications (SMILES augmentation, model-size tuning, no label smoothing) improved suitability. Constraints: requires large labeled datasets; performance degrades for noisy/rare/unclassified reaction types.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Top-1 accuracy: 90.4% (USPTO_MIT separated), 88.6% (USPTO_MIT mixed); Top-2: 93.7% (separated); USPTO_STEREO Top-1: 78.1% (separated), 76.2% (mixed); Pistachio_2017 test: 78.0% overall (87.6% for 'classified' subset). Uncertainty calibration: ROC AUC = 0.89 for classifying correct vs incorrect predictions. Training time: ~48 h on a single NVIDIA P100 for best single model; inference/test times reported (e.g., baseline test: 20 m for 40k reactions). Ensembling/weight averaging can raise Top-1 to ~91.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Outperforms prior template-based, graph-based, and earlier seq-2-seq RNN baselines across benchmark datasets and reaction-popularity bins; generalizes better to less-popular templates; predicts subtle chemoselectivity/regioselectivity and some stereochemistry; avoids 'alchemy' (predicting atoms not present) without hard constraints; weaker on certain classes (resolutions, 'unclassified' noisy reactions). Label smoothing increased nominal accuracy slightly but harmed uncertainty discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High: enables accurate, template-free, uncertainty-aware forward reaction prediction; can be used to rank/score reaction pathways and support multistep synthesis planning, reducing experimental cost by prioritizing low-risk steps; deployed in IBM RXN for Chemistry and used by practitioners (>40k predictions reported), indicating practical utility and scalability to real-world workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Direct comparisons in paper show Molecular Transformer outperforms sequence-to-sequence RNN models and multiple graph-based/template-free methods across datasets (see Tables 3 and 4); performance gap increases for less-common templates and for mixed reactant/reagent inputs where other methods often rely on reagent separation or atom-mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key contributors include multi-head attention capturing long-range dependencies (SMILES token locations poorly reflect true chemical adjacency), large labeled datasets from patents, SMILES data augmentation (randomized SMILES), careful training choices (no label smoothing for better uncertainty), checkpoint weight averaging, and architecture choices (replacing RNNs with attention-only transformer). Atom-mapping independence avoided cascading errors tied to mapping heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>A transformer-based sequence-to-sequence model trained on large, tokenized SMILES datasets with data augmentation can learn complex chemical transformations without handcrafted templates and produce well-calibrated uncertainty, making it particularly effective for reaction prediction when ample labeled reaction data are available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2356.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2356.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>seq2seq RNN (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent neural network sequence-to-sequence models with attention for reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Earlier sequence-based models treated reaction prediction as machine translation from reactant/reagent SMILES to product SMILES using RNN encoder-decoder architectures with (single-head) attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Linking the neural machine translation and the prediction of organic chemistry reactions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Organic synthesis / chemical reaction outcome prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict product SMILES from reactant/reagent SMILES sequences using supervised sequence-to-sequence learning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Trained on the same patent-derived SMILES datasets (e.g., USPTO variants), requiring labeled reactant->product pairs; often relied on preprocessing like reactant/reagent separation and reagent tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Tokenized SMILES sequences (sequential text data).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — must capture long-range and non-local chemical dependencies using sequential recurrence; sensitive to SMILES token ordering which does not align with chemical adjacency.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established as a first application of NMT ideas to reaction prediction but superseded by attention-only architectures for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — these models provided attention weights that could be used for some interpretability but remain largely black boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>RNN-based sequence-to-sequence with attention</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Encoder-decoder recurrent networks (LSTM/GRU) with single-head attention between encoder and decoder; earlier Schwaller et al. used reagent tokenization and reactant/reagent separation; produced confidence scores via attention-based measures.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (sequence-to-sequence deep learning)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable but limited: effective for many reaction types but disadvantaged by RNN inductive biases for SMILES sequences; often required reactant/reagent separation and tokenization heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported lower accuracies than Molecular Transformer: e.g., S2S (ref. 28) ~80.3% Top-1 on USPTO_MIT (separated) in Table 4 (paper compares).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Could compete with graph-based methods but underperformed transformer-based models, especially on long-range dependencies and less-common reactions; provided some interpretability via attention weights but limited by single-head attention.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate — demonstrated feasibility of treating reaction prediction as machine translation and informed subsequent work (including the Molecular Transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed earlier template-based methods in some cases but was outperformed by the Molecular Transformer; compared in paper's tables and discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Leveraging sequence-to-sequence NMT analogy enabled end-to-end learning without templates; however, single-head attention and RNN recurrence limited capacity to model non-local chemical relations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>RNN seq2seq models validated the machine-translation framing of reaction prediction but are less effective than multi-head attention transformers for capturing the non-local, graph-like dependencies implicit in SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2356.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2356.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template-based methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template-based reaction prediction methods (rule/ template libraries)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that apply a library of reaction templates/rules (atom/bond neighborhood patterns before/after reaction) to enumerate and score possible product candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Organic synthesis / reaction prediction and retrosynthesis planning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate and rank possible products by applying pre-defined or automatically-extracted reaction templates to reactant structures.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Require datasets to extract templates (often from atom-mapped reaction corpora); template extraction depends on availability of mapped reactions and/or expert rules; template libraries grow as literature grows, making manual upkeep difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Molecular graphs with atom-mapping information; templates are graph-pattern rules.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High combinatorial complexity as number of templates and possible application sites grows; template extraction and matching can be expensive and involve heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Long history and well-established (many older systems), but scalability and maintenance are problematic; entrenched in some retrosynthesis tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — templates encode mechanistic patterns and mapping; users often expect interpretable, rule-based transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Template-based rule application (with ML for template selection in some cases)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Create templates by hand or auto-extract from mapped reaction databases; evaluate applicability and likelihood of templates using classifiers or scoring functions. Some ML methods predict which template applies.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Symbolic/knowledge-based hybrid (sometimes combined with supervised ML for scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable when reliable atom-mapping and comprehensive template libraries exist; limited scalability and can be brittle with noisy or novel reactions.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Varies by implementation; paper reports prior template-based baselines in literature but specific numbers depend on method (not summarized as a single figure here).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Readable and interpretable but not scalable; automatic template extraction still relies on meta-heuristics and atom-mapping tools that themselves use templates — creating circular dependencies and brittleness.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Useful for interpretable retrosynthesis and where expert rules suffice; constrained when facing large, diverse reaction spaces or incomplete atom-mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Template-free methods (graph- and sequence-based) avoid handcrafted rules and often generalize better; this paper argues template-free transformer outperforms template-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Interpretability and explicit mechanistic encoding in templates; success limited by completeness and correctness of template libraries and dependence on atom-mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Template-based approaches are interpretable and historically important but struggle to scale and are vulnerable to errors in atom-mapping and template extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2356.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2356.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WLN / Weisfeiler-Lehman Network</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weisfeiler-Lehman Network for reaction prediction (graph-edit approach by Jin et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based template-free approach that formulates reaction prediction as predicting graph edits (reaction centers) using Weisfeiler-Lehman graph neural networks; candidate products are generated and ranked by a second network.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Organic synthesis / chemical reaction outcome prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict reaction centers and bond changes on molecular graphs to generate product candidates, then rank them to select final product(s).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on atom-mapped reaction datasets to generate ground truth for training; datasets used include USPTO-derived corpora; requires accurate atom-mapping which is non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph-structured molecular inputs (atoms as nodes, bonds as edges) with explicit graph edits as targets (bond changes); multi-step candidate generation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: predicting graph edits requires modeling combinatorial possibilities of bond changes; candidate generation/ranking pipeline increases complexity; often excludes high-change reactions by design unless extended.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Relatively recent and influential graph-based approach with several follow-up improvements; well-cited in reaction prediction literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — approach provides interpretable reaction centers and graph edits corresponding to mechanistic changes, which can aid understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Graph neural network (Weisfeiler-Lehman Network) with candidate generation and ranking</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>First network computes reactivity scores on graph nodes/edges to propose reaction centers; candidate products generated via graph edits (up to a chosen number of bond changes) and a second network ranks candidates; initially used reactant-only or reactant+reagent context depending on version.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (graph neural networks / structured prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable for many reaction types; limitations include dependence on atom-mapped training data and sometimes needing reagents to be excluded or separated for better performance; extensions address multiple bond-change scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported in literature (ref 23) and compared in this paper; WLDN reported lower Top-1 accuracy than Molecular Transformer on USPTO_MIT (e.g., ~79.6% in Table 4 for some variants), though exact numbers depend on dataset/preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provides mechanistically interpretable predictions (reaction centers) but suffers from reliance on atom-mapping and preprocessing choices; earlier versions improved when reagents excluded from reactivity prediction, implying practical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for interpretable, mechanistic-style predictions and candidate generation pipelines, but constrained by atom-mapping and scale when compared to template-free sequence models like the Molecular Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed older template-based systems in some respects; in this paper, Molecular Transformer outperforms WLN across datasets and template-popularity bins, especially on less-common templates.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Structured graph representation and directly modeling bond edits yields mechanistic clarity; success limited by quality of atom-mapping and how reagents/context are handled.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Graph-edit predictive models give interpretable reaction-center predictions but are constrained by the need for reliable atom-mapped training data and certain preprocessing assumptions (e.g., reagent exclusion).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2356.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2356.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-convolutional model (Coley et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-convolutional neural network model for prediction of chemical reactivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-convolutional neural network applied to predict chemical reaction outcomes (reactivity) from molecular graph inputs, representing a recent graph-based ML approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A graph-convolutional neural network model for the prediction of chemical reactivity.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Organic synthesis / chemical reaction outcome prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict reaction products or reactivity scores using graph-based convolutional neural networks trained on reaction datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Trained on atom-mapped reaction corpora (e.g., USPTO-derived datasets); depends on mapped reactions for supervised learning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Molecular graphs (atoms, bonds), possibly with reaction-context features; structured graph inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: must model complex local and non-local chemical interactions on graphs; candidate generation and ranking pipeline may be necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Recent and actively developed; represents a prominent graph-based alternative to sequence models in reaction prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — graph convolutions can provide node/edge-level insights (e.g., reactive sites), offering some mechanistic interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Graph-convolutional neural network</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Uses graph convolution layers to compute node/edge representations and predict reactivity or product structures; may be combined with candidate generation/ranking frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (graph neural networks)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and effective for many reaction classes; some implementations reported constraints such as needing reagent separation or atom-mapped training data; limited handling of stereochemistry in some graph-based versions.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported performance in literature (ref 24) and compared in this paper; specific comparative numbers vary by dataset and preprocessing (paper cites better/worse performance depending on dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Performant on many tasks, but in this paper the Molecular Transformer outperforms the graph-convolutional approach on benchmark datasets and handles stereochemistry better in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for providing graph-native modeling and potential mechanistic insight; can be complementary to sequence models depending on use-case and dataset characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against sequence-based and template-based methods in paper; Molecular Transformer is reported to outperform the graph-convolutional model on standard benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Graph representation aligns with chemical structure allowing localized mechanistic predictions; reliance on high-quality atom-mapped data and preprocessing choices influence success.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Graph-convolutional approaches capture structural chemical information directly, but when ample labeled SMILES-based data and sequence-modeling advances (transformers) exist, sequence models can outperform and more easily handle certain practical preprocessing scenarios (e.g., no reactant/reagent split).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2356.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2356.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ELECTRO / Predicting Electron Paths</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ELECTRO (method framing reaction prediction as electron-path prediction) / Predicting Electron Paths</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that represents reactions as stepwise electron-flow (electron path) predictions, used with graph neural networks to model mechanistic electron movements and predict outcomes; constrained to reactions with linear electron-flow topology in some studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Predicting Electron Paths.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Organic synthesis / mechanistic reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict the stepwise electron flow (mechanistic path) during a reaction to infer product formation; requires reactions that can be represented with linear electron flow (LEF) topology.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Applied to subsets of patent-derived datasets (e.g., USPTO_LEF) filtered to reactions with linear electron-flow; preprocessing reduces dataset size (e.g., 73% of USPTO_MIT retained in one experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph-structured molecular representations with annotated electron-flow steps (sequence of edits or electron movements).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High mechanistic complexity; representing electron flow requires additional mechanistic assumptions and excludes pericyclic and other non-LEF reactions unless specially handled.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Relatively recent; introduces mechanistic modeling (electron flow) into ML reaction prediction but requires restrictive preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — method explicitly models mechanistic electron paths and thus is oriented toward mechanistic interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Mechanism-focused graph-based modeling (electron-path prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Frame reaction prediction as predicting an ordered sequence of electron moves (steps) on molecular graphs; uses graph neural networks and auxiliary preprocessing to select suitable reactions (LEF topology).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (graph-based structured prediction with mechanistic constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable to reactions amenable to LEF representation; not applicable to pericyclic or complex non-linear electron-flow reactions; preprocessing to filter datasets required.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Applied on a filtered subset (USPTO_LEF) with performance reported in referenced work; in this paper, ELECTRO is included among comparison methods but specific numbers vary by implementation and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provides mechanistic, stepwise interpretations for eligible reactions but sacrifices coverage by excluding reactions that lack clear LEF topology; earlier versions required dataset filtering, limiting generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Valuable for mechanistically interpretable predictions in subsets of reactions; limited for broad reaction coverage unless extended.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to graph-edit and sequence-based approaches; Molecular Transformer achieves higher coverage and overall accuracy across unfiltered datasets, while ELECTRO is more mechanistic but narrower in scope.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Explicit mechanistic framing yields interpretability; success limited by applicability to only LEF-topology reactions and dependence on preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Mechanistic electron-flow models offer interpretable predictions for a subset of reactions but are less general than template-free sequence models when applied to large, diverse reaction corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2356.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2356.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GTPN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Transformation Policy Network for Chemical Reaction Prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based policy-network approach that predicts chemical reactions as sequences of graph transformations using learned policies to select edits, aiming to generalize product formation prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph Transformation Policy Network for Chemical Reaction Prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Organic synthesis / chemical reaction outcome prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Model reaction prediction as a sequential decision process of graph transformations; learn policies to choose edits that transform reactants into products.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses atom-mapped reaction datasets for supervised training of graph-edit policies; dataset specifics vary by implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph-structured molecular inputs; target sequences of graph edits/transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: sequential decision space of possible edits is large; requires modeling of combinatorial graph edit sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Recent (ref 26) and part of growing graph-based approaches to reaction prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high — produces explicit edit sequences that can be interpreted in mechanistic terms.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Graph-based reinforcement-style / policy network for graph transformations</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Learn a policy network over graph states to sequentially propose and apply graph transformations (bond changes) to generate products; trained with supervised signals and possibly policy learning components.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning with sequential decision / policy-network formulation (related to reinforcement learning ideas)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable to many reaction types and provides explicit edit sequences; like other graph methods, often depends on atom-mapped data and careful handling of reagents.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported performance in its reference; compared in the paper's aggregate tables (e.g., GTPN reported ~82.4% on some datasets in Table 4), but exact numbers depend on dataset/preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Offers a structured way to model reaction transformations; in this work, Molecular Transformer outperforms GTPN on the standard benchmarks cited.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Promising for interpretable, sequential modeling of reactions and for tasks requiring explicit transformation sequences; scalability and dependence on mapping/data remain considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared in aggregate tables; transformer-based sequence models achieve higher top-1 accuracies on benchmark datasets in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Explicit sequential graph-edit modeling aligns with chemical transformation processes; success depends on training data quality and policy design.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Sequential graph-transformation policies can represent reaction mechanisms explicitly but face scalability and data-dependence challenges relative to large-data sequence models like transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network. <em>(Rating: 2)</em></li>
                <li>A graph-convolutional neural network model for the prediction of chemical reactivity. <em>(Rating: 2)</em></li>
                <li>"Found in Translation": Predicting Outcomes of Complex Organic Chemistry Reactions using Neural Sequence-to-Sequence Models. <em>(Rating: 2)</em></li>
                <li>Linking the neural machine translation and the prediction of organic chemistry reactions. <em>(Rating: 2)</em></li>
                <li>Predicting Electron Paths. <em>(Rating: 2)</em></li>
                <li>Graph Transformation Policy Network for Chemical Reaction Prediction. <em>(Rating: 2)</em></li>
                <li>Neural networks for the prediction of organic chemistry reactions. <em>(Rating: 1)</em></li>
                <li>Neural-Symbolic Machine Learning for Retrosynthesis and Reaction Prediction. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2356",
    "paper_id": "paper-de20b6488e148a19ae6c63defbfca8a6373e4110",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Molecular Transformer",
            "name_full": "Molecular Transformer (multi-head attention model for chemical reaction prediction)",
            "brief_description": "A fully attention-based encoder-decoder transformer adapted to predict chemical reaction products from SMILES strings; provides uncertainty-calibrated, template-free reaction prediction without requiring atom-mapping or a reactant/reagent split.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Organic synthesis / chemical reaction outcome prediction",
            "problem_description": "Given reactants and reagents represented as SMILES, predict the product(s) of the chemical reaction (forward reaction prediction) and provide calibrated uncertainty for predictions to support multistep synthesis planning.",
            "data_availability": "Abundant labeled reaction data available from patent-mined datasets (e.g., USPTO_MIT: ~479k reactions; USPTO_STEREO: ~1.0M; non-public Pistachio_2017 test: ~15k). Data are labeled (reactant/reagent -&gt; product), canonicalized with RDKit, sometimes include stereochemistry; public and non-public subsets exist. Data quality variable (noise in stereochemistry, transcription errors), and some datasets filtered for constraints (e.g., LEF).",
            "data_structure": "Text/sequential representation of molecules (SMILES strings) tokenized into tokens; effectively sequence-to-sequence data (discrete token sequences) that implicitly encode graph-structured chemistry; data augmentation used via randomized SMILES.",
            "problem_complexity": "Very high: combinatorial explosion of possible reactions and products, complex regio-/chemo-/stereoselectivity, multistep dependencies (error compounds over steps), non-linear mappings from reactant SMILES to products; large vocabulary of chemical tokens and long-range dependencies in sequences.",
            "domain_maturity": "Domain of organic synthesis is mature (well-established chemistry) but computational reaction outcome prediction is an active, rapidly evolving subfield with multiple paradigms (template-based, graph-based, sequence-based); substantial prior work exists but limitations persist.",
            "mechanistic_understanding_requirements": "Medium to high — practical use in synthesis planning requires calibrated uncertainty and some interpretability (to prioritize steps/fail fast), though chemically plausible black-box predictions are acceptable for many tasks; mechanistic interpretability remains desirable for validation and for certain reaction classes (e.g., stereospecific transformations).",
            "ai_methodology_name": "Transformer-based sequence-to-sequence model (Molecular Transformer)",
            "ai_methodology_description": "Encoder-decoder transformer (multi-head self-attention) adapted for SMILES-to-SMILES translation: modified from Vaswani et al. with reduced size (4 layers of size 256 instead of 6x512, ~12M parameters), 8 attention heads, positional encodings, trained supervised on tokenized SMILES pairs; uses ADAM optimizer with learning rate schedule and 8000 warmup steps; no recurrent components; SMILES data augmentation (randomized SMILES) doubles training examples; weight averaging of last 20 checkpoints and optional ensembling used to improve performance; product-of-token-probabilities used as an uncertainty/confidence score (ROC AUC 0.89). Label smoothing was evaluated but turned off (0.0) because it degraded uncertainty discrimination.",
            "ai_methodology_category": "Supervised learning (sequence-to-sequence deep learning / neural machine translation)",
            "applicability": "Highly applicable — method directly maps to the sequence-structured SMILES data, requires no handcrafted reaction templates or atom-mapping, handles mixed reactant/reagent inputs and stereochemistry to a degree; modifications (SMILES augmentation, model-size tuning, no label smoothing) improved suitability. Constraints: requires large labeled datasets; performance degrades for noisy/rare/unclassified reaction types.",
            "effectiveness_quantitative": "Top-1 accuracy: 90.4% (USPTO_MIT separated), 88.6% (USPTO_MIT mixed); Top-2: 93.7% (separated); USPTO_STEREO Top-1: 78.1% (separated), 76.2% (mixed); Pistachio_2017 test: 78.0% overall (87.6% for 'classified' subset). Uncertainty calibration: ROC AUC = 0.89 for classifying correct vs incorrect predictions. Training time: ~48 h on a single NVIDIA P100 for best single model; inference/test times reported (e.g., baseline test: 20 m for 40k reactions). Ensembling/weight averaging can raise Top-1 to ~91.0%.",
            "effectiveness_qualitative": "Outperforms prior template-based, graph-based, and earlier seq-2-seq RNN baselines across benchmark datasets and reaction-popularity bins; generalizes better to less-popular templates; predicts subtle chemoselectivity/regioselectivity and some stereochemistry; avoids 'alchemy' (predicting atoms not present) without hard constraints; weaker on certain classes (resolutions, 'unclassified' noisy reactions). Label smoothing increased nominal accuracy slightly but harmed uncertainty discrimination.",
            "impact_potential": "High: enables accurate, template-free, uncertainty-aware forward reaction prediction; can be used to rank/score reaction pathways and support multistep synthesis planning, reducing experimental cost by prioritizing low-risk steps; deployed in IBM RXN for Chemistry and used by practitioners (&gt;40k predictions reported), indicating practical utility and scalability to real-world workflows.",
            "comparison_to_alternatives": "Direct comparisons in paper show Molecular Transformer outperforms sequence-to-sequence RNN models and multiple graph-based/template-free methods across datasets (see Tables 3 and 4); performance gap increases for less-common templates and for mixed reactant/reagent inputs where other methods often rely on reagent separation or atom-mapping.",
            "success_factors": "Key contributors include multi-head attention capturing long-range dependencies (SMILES token locations poorly reflect true chemical adjacency), large labeled datasets from patents, SMILES data augmentation (randomized SMILES), careful training choices (no label smoothing for better uncertainty), checkpoint weight averaging, and architecture choices (replacing RNNs with attention-only transformer). Atom-mapping independence avoided cascading errors tied to mapping heuristics.",
            "key_insight": "A transformer-based sequence-to-sequence model trained on large, tokenized SMILES datasets with data augmentation can learn complex chemical transformations without handcrafted templates and produce well-calibrated uncertainty, making it particularly effective for reaction prediction when ample labeled reaction data are available.",
            "uuid": "e2356.0",
            "source_info": {
                "paper_title": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "seq2seq RNN (prior work)",
            "name_full": "Recurrent neural network sequence-to-sequence models with attention for reaction prediction",
            "brief_description": "Earlier sequence-based models treated reaction prediction as machine translation from reactant/reagent SMILES to product SMILES using RNN encoder-decoder architectures with (single-head) attention.",
            "citation_title": "Linking the neural machine translation and the prediction of organic chemistry reactions.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Organic synthesis / chemical reaction outcome prediction",
            "problem_description": "Predict product SMILES from reactant/reagent SMILES sequences using supervised sequence-to-sequence learning.",
            "data_availability": "Trained on the same patent-derived SMILES datasets (e.g., USPTO variants), requiring labeled reactant-&gt;product pairs; often relied on preprocessing like reactant/reagent separation and reagent tokenization.",
            "data_structure": "Tokenized SMILES sequences (sequential text data).",
            "problem_complexity": "High — must capture long-range and non-local chemical dependencies using sequential recurrence; sensitive to SMILES token ordering which does not align with chemical adjacency.",
            "domain_maturity": "Established as a first application of NMT ideas to reaction prediction but superseded by attention-only architectures for this task.",
            "mechanistic_understanding_requirements": "Medium — these models provided attention weights that could be used for some interpretability but remain largely black boxes.",
            "ai_methodology_name": "RNN-based sequence-to-sequence with attention",
            "ai_methodology_description": "Encoder-decoder recurrent networks (LSTM/GRU) with single-head attention between encoder and decoder; earlier Schwaller et al. used reagent tokenization and reactant/reagent separation; produced confidence scores via attention-based measures.",
            "ai_methodology_category": "Supervised learning (sequence-to-sequence deep learning)",
            "applicability": "Applicable but limited: effective for many reaction types but disadvantaged by RNN inductive biases for SMILES sequences; often required reactant/reagent separation and tokenization heuristics.",
            "effectiveness_quantitative": "Reported lower accuracies than Molecular Transformer: e.g., S2S (ref. 28) ~80.3% Top-1 on USPTO_MIT (separated) in Table 4 (paper compares).",
            "effectiveness_qualitative": "Could compete with graph-based methods but underperformed transformer-based models, especially on long-range dependencies and less-common reactions; provided some interpretability via attention weights but limited by single-head attention.",
            "impact_potential": "Moderate — demonstrated feasibility of treating reaction prediction as machine translation and informed subsequent work (including the Molecular Transformer).",
            "comparison_to_alternatives": "Outperformed earlier template-based methods in some cases but was outperformed by the Molecular Transformer; compared in paper's tables and discussion.",
            "success_factors": "Leveraging sequence-to-sequence NMT analogy enabled end-to-end learning without templates; however, single-head attention and RNN recurrence limited capacity to model non-local chemical relations.",
            "key_insight": "RNN seq2seq models validated the machine-translation framing of reaction prediction but are less effective than multi-head attention transformers for capturing the non-local, graph-like dependencies implicit in SMILES.",
            "uuid": "e2356.1",
            "source_info": {
                "paper_title": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "Template-based methods",
            "name_full": "Template-based reaction prediction methods (rule/ template libraries)",
            "brief_description": "Approaches that apply a library of reaction templates/rules (atom/bond neighborhood patterns before/after reaction) to enumerate and score possible product candidates.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Organic synthesis / reaction prediction and retrosynthesis planning",
            "problem_description": "Generate and rank possible products by applying pre-defined or automatically-extracted reaction templates to reactant structures.",
            "data_availability": "Require datasets to extract templates (often from atom-mapped reaction corpora); template extraction depends on availability of mapped reactions and/or expert rules; template libraries grow as literature grows, making manual upkeep difficult.",
            "data_structure": "Molecular graphs with atom-mapping information; templates are graph-pattern rules.",
            "problem_complexity": "High combinatorial complexity as number of templates and possible application sites grows; template extraction and matching can be expensive and involve heuristics.",
            "domain_maturity": "Long history and well-established (many older systems), but scalability and maintenance are problematic; entrenched in some retrosynthesis tools.",
            "mechanistic_understanding_requirements": "High — templates encode mechanistic patterns and mapping; users often expect interpretable, rule-based transformations.",
            "ai_methodology_name": "Template-based rule application (with ML for template selection in some cases)",
            "ai_methodology_description": "Create templates by hand or auto-extract from mapped reaction databases; evaluate applicability and likelihood of templates using classifiers or scoring functions. Some ML methods predict which template applies.",
            "ai_methodology_category": "Symbolic/knowledge-based hybrid (sometimes combined with supervised ML for scoring)",
            "applicability": "Applicable when reliable atom-mapping and comprehensive template libraries exist; limited scalability and can be brittle with noisy or novel reactions.",
            "effectiveness_quantitative": "Varies by implementation; paper reports prior template-based baselines in literature but specific numbers depend on method (not summarized as a single figure here).",
            "effectiveness_qualitative": "Readable and interpretable but not scalable; automatic template extraction still relies on meta-heuristics and atom-mapping tools that themselves use templates — creating circular dependencies and brittleness.",
            "impact_potential": "Useful for interpretable retrosynthesis and where expert rules suffice; constrained when facing large, diverse reaction spaces or incomplete atom-mapping.",
            "comparison_to_alternatives": "Template-free methods (graph- and sequence-based) avoid handcrafted rules and often generalize better; this paper argues template-free transformer outperforms template-based approaches.",
            "success_factors": "Interpretability and explicit mechanistic encoding in templates; success limited by completeness and correctness of template libraries and dependence on atom-mapping.",
            "key_insight": "Template-based approaches are interpretable and historically important but struggle to scale and are vulnerable to errors in atom-mapping and template extraction.",
            "uuid": "e2356.2",
            "source_info": {
                "paper_title": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "WLN / Weisfeiler-Lehman Network",
            "name_full": "Weisfeiler-Lehman Network for reaction prediction (graph-edit approach by Jin et al.)",
            "brief_description": "A graph-based template-free approach that formulates reaction prediction as predicting graph edits (reaction centers) using Weisfeiler-Lehman graph neural networks; candidate products are generated and ranked by a second network.",
            "citation_title": "Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Organic synthesis / chemical reaction outcome prediction",
            "problem_description": "Predict reaction centers and bond changes on molecular graphs to generate product candidates, then rank them to select final product(s).",
            "data_availability": "Relies on atom-mapped reaction datasets to generate ground truth for training; datasets used include USPTO-derived corpora; requires accurate atom-mapping which is non-trivial.",
            "data_structure": "Graph-structured molecular inputs (atoms as nodes, bonds as edges) with explicit graph edits as targets (bond changes); multi-step candidate generation.",
            "problem_complexity": "High: predicting graph edits requires modeling combinatorial possibilities of bond changes; candidate generation/ranking pipeline increases complexity; often excludes high-change reactions by design unless extended.",
            "domain_maturity": "Relatively recent and influential graph-based approach with several follow-up improvements; well-cited in reaction prediction literature.",
            "mechanistic_understanding_requirements": "Medium — approach provides interpretable reaction centers and graph edits corresponding to mechanistic changes, which can aid understanding.",
            "ai_methodology_name": "Graph neural network (Weisfeiler-Lehman Network) with candidate generation and ranking",
            "ai_methodology_description": "First network computes reactivity scores on graph nodes/edges to propose reaction centers; candidate products generated via graph edits (up to a chosen number of bond changes) and a second network ranks candidates; initially used reactant-only or reactant+reagent context depending on version.",
            "ai_methodology_category": "Supervised learning (graph neural networks / structured prediction)",
            "applicability": "Applicable for many reaction types; limitations include dependence on atom-mapped training data and sometimes needing reagents to be excluded or separated for better performance; extensions address multiple bond-change scenarios.",
            "effectiveness_quantitative": "Reported in literature (ref 23) and compared in this paper; WLDN reported lower Top-1 accuracy than Molecular Transformer on USPTO_MIT (e.g., ~79.6% in Table 4 for some variants), though exact numbers depend on dataset/preprocessing.",
            "effectiveness_qualitative": "Provides mechanistically interpretable predictions (reaction centers) but suffers from reliance on atom-mapping and preprocessing choices; earlier versions improved when reagents excluded from reactivity prediction, implying practical constraints.",
            "impact_potential": "High for interpretable, mechanistic-style predictions and candidate generation pipelines, but constrained by atom-mapping and scale when compared to template-free sequence models like the Molecular Transformer.",
            "comparison_to_alternatives": "Outperformed older template-based systems in some respects; in this paper, Molecular Transformer outperforms WLN across datasets and template-popularity bins, especially on less-common templates.",
            "success_factors": "Structured graph representation and directly modeling bond edits yields mechanistic clarity; success limited by quality of atom-mapping and how reagents/context are handled.",
            "key_insight": "Graph-edit predictive models give interpretable reaction-center predictions but are constrained by the need for reliable atom-mapped training data and certain preprocessing assumptions (e.g., reagent exclusion).",
            "uuid": "e2356.3",
            "source_info": {
                "paper_title": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "Graph-convolutional model (Coley et al.)",
            "name_full": "Graph-convolutional neural network model for prediction of chemical reactivity",
            "brief_description": "A graph-convolutional neural network applied to predict chemical reaction outcomes (reactivity) from molecular graph inputs, representing a recent graph-based ML approach.",
            "citation_title": "A graph-convolutional neural network model for the prediction of chemical reactivity.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Organic synthesis / chemical reaction outcome prediction",
            "problem_description": "Predict reaction products or reactivity scores using graph-based convolutional neural networks trained on reaction datasets.",
            "data_availability": "Trained on atom-mapped reaction corpora (e.g., USPTO-derived datasets); depends on mapped reactions for supervised learning.",
            "data_structure": "Molecular graphs (atoms, bonds), possibly with reaction-context features; structured graph inputs.",
            "problem_complexity": "High: must model complex local and non-local chemical interactions on graphs; candidate generation and ranking pipeline may be necessary.",
            "domain_maturity": "Recent and actively developed; represents a prominent graph-based alternative to sequence models in reaction prediction.",
            "mechanistic_understanding_requirements": "Medium — graph convolutions can provide node/edge-level insights (e.g., reactive sites), offering some mechanistic interpretability.",
            "ai_methodology_name": "Graph-convolutional neural network",
            "ai_methodology_description": "Uses graph convolution layers to compute node/edge representations and predict reactivity or product structures; may be combined with candidate generation/ranking frameworks.",
            "ai_methodology_category": "Supervised learning (graph neural networks)",
            "applicability": "Applicable and effective for many reaction classes; some implementations reported constraints such as needing reagent separation or atom-mapped training data; limited handling of stereochemistry in some graph-based versions.",
            "effectiveness_quantitative": "Reported performance in literature (ref 24) and compared in this paper; specific comparative numbers vary by dataset and preprocessing (paper cites better/worse performance depending on dataset).",
            "effectiveness_qualitative": "Performant on many tasks, but in this paper the Molecular Transformer outperforms the graph-convolutional approach on benchmark datasets and handles stereochemistry better in some cases.",
            "impact_potential": "High for providing graph-native modeling and potential mechanistic insight; can be complementary to sequence models depending on use-case and dataset characteristics.",
            "comparison_to_alternatives": "Compared against sequence-based and template-based methods in paper; Molecular Transformer is reported to outperform the graph-convolutional model on standard benchmarks.",
            "success_factors": "Graph representation aligns with chemical structure allowing localized mechanistic predictions; reliance on high-quality atom-mapped data and preprocessing choices influence success.",
            "key_insight": "Graph-convolutional approaches capture structural chemical information directly, but when ample labeled SMILES-based data and sequence-modeling advances (transformers) exist, sequence models can outperform and more easily handle certain practical preprocessing scenarios (e.g., no reactant/reagent split).",
            "uuid": "e2356.4",
            "source_info": {
                "paper_title": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "ELECTRO / Predicting Electron Paths",
            "name_full": "ELECTRO (method framing reaction prediction as electron-path prediction) / Predicting Electron Paths",
            "brief_description": "An approach that represents reactions as stepwise electron-flow (electron path) predictions, used with graph neural networks to model mechanistic electron movements and predict outcomes; constrained to reactions with linear electron-flow topology in some studies.",
            "citation_title": "Predicting Electron Paths.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Organic synthesis / mechanistic reaction prediction",
            "problem_description": "Predict the stepwise electron flow (mechanistic path) during a reaction to infer product formation; requires reactions that can be represented with linear electron flow (LEF) topology.",
            "data_availability": "Applied to subsets of patent-derived datasets (e.g., USPTO_LEF) filtered to reactions with linear electron-flow; preprocessing reduces dataset size (e.g., 73% of USPTO_MIT retained in one experiment).",
            "data_structure": "Graph-structured molecular representations with annotated electron-flow steps (sequence of edits or electron movements).",
            "problem_complexity": "High mechanistic complexity; representing electron flow requires additional mechanistic assumptions and excludes pericyclic and other non-LEF reactions unless specially handled.",
            "domain_maturity": "Relatively recent; introduces mechanistic modeling (electron flow) into ML reaction prediction but requires restrictive preprocessing.",
            "mechanistic_understanding_requirements": "High — method explicitly models mechanistic electron paths and thus is oriented toward mechanistic interpretability.",
            "ai_methodology_name": "Mechanism-focused graph-based modeling (electron-path prediction)",
            "ai_methodology_description": "Frame reaction prediction as predicting an ordered sequence of electron moves (steps) on molecular graphs; uses graph neural networks and auxiliary preprocessing to select suitable reactions (LEF topology).",
            "ai_methodology_category": "Supervised learning (graph-based structured prediction with mechanistic constraints)",
            "applicability": "Applicable to reactions amenable to LEF representation; not applicable to pericyclic or complex non-linear electron-flow reactions; preprocessing to filter datasets required.",
            "effectiveness_quantitative": "Applied on a filtered subset (USPTO_LEF) with performance reported in referenced work; in this paper, ELECTRO is included among comparison methods but specific numbers vary by implementation and dataset.",
            "effectiveness_qualitative": "Provides mechanistic, stepwise interpretations for eligible reactions but sacrifices coverage by excluding reactions that lack clear LEF topology; earlier versions required dataset filtering, limiting generalizability.",
            "impact_potential": "Valuable for mechanistically interpretable predictions in subsets of reactions; limited for broad reaction coverage unless extended.",
            "comparison_to_alternatives": "Compared to graph-edit and sequence-based approaches; Molecular Transformer achieves higher coverage and overall accuracy across unfiltered datasets, while ELECTRO is more mechanistic but narrower in scope.",
            "success_factors": "Explicit mechanistic framing yields interpretability; success limited by applicability to only LEF-topology reactions and dependence on preprocessing.",
            "key_insight": "Mechanistic electron-flow models offer interpretable predictions for a subset of reactions but are less general than template-free sequence models when applied to large, diverse reaction corpora.",
            "uuid": "e2356.5",
            "source_info": {
                "paper_title": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "GTPN",
            "name_full": "Graph Transformation Policy Network for Chemical Reaction Prediction",
            "brief_description": "A graph-based policy-network approach that predicts chemical reactions as sequences of graph transformations using learned policies to select edits, aiming to generalize product formation prediction.",
            "citation_title": "Graph Transformation Policy Network for Chemical Reaction Prediction.",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Organic synthesis / chemical reaction outcome prediction",
            "problem_description": "Model reaction prediction as a sequential decision process of graph transformations; learn policies to choose edits that transform reactants into products.",
            "data_availability": "Uses atom-mapped reaction datasets for supervised training of graph-edit policies; dataset specifics vary by implementation.",
            "data_structure": "Graph-structured molecular inputs; target sequences of graph edits/transformations.",
            "problem_complexity": "High: sequential decision space of possible edits is large; requires modeling of combinatorial graph edit sequences.",
            "domain_maturity": "Recent (ref 26) and part of growing graph-based approaches to reaction prediction.",
            "mechanistic_understanding_requirements": "Medium to high — produces explicit edit sequences that can be interpreted in mechanistic terms.",
            "ai_methodology_name": "Graph-based reinforcement-style / policy network for graph transformations",
            "ai_methodology_description": "Learn a policy network over graph states to sequentially propose and apply graph transformations (bond changes) to generate products; trained with supervised signals and possibly policy learning components.",
            "ai_methodology_category": "Supervised learning with sequential decision / policy-network formulation (related to reinforcement learning ideas)",
            "applicability": "Applicable to many reaction types and provides explicit edit sequences; like other graph methods, often depends on atom-mapped data and careful handling of reagents.",
            "effectiveness_quantitative": "Reported performance in its reference; compared in the paper's aggregate tables (e.g., GTPN reported ~82.4% on some datasets in Table 4), but exact numbers depend on dataset/preprocessing.",
            "effectiveness_qualitative": "Offers a structured way to model reaction transformations; in this work, Molecular Transformer outperforms GTPN on the standard benchmarks cited.",
            "impact_potential": "Promising for interpretable, sequential modeling of reactions and for tasks requiring explicit transformation sequences; scalability and dependence on mapping/data remain considerations.",
            "comparison_to_alternatives": "Compared in aggregate tables; transformer-based sequence models achieve higher top-1 accuracies on benchmark datasets in this paper.",
            "success_factors": "Explicit sequential graph-edit modeling aligns with chemical transformation processes; success depends on training data quality and policy design.",
            "key_insight": "Sequential graph-transformation policies can represent reaction mechanisms explicitly but face scalability and data-dependence challenges relative to large-data sequence models like transformers.",
            "uuid": "e2356.6",
            "source_info": {
                "paper_title": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction",
                "publication_date_yy_mm": "2018-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network.",
            "rating": 2
        },
        {
            "paper_title": "A graph-convolutional neural network model for the prediction of chemical reactivity.",
            "rating": 2
        },
        {
            "paper_title": "\"Found in Translation\": Predicting Outcomes of Complex Organic Chemistry Reactions using Neural Sequence-to-Sequence Models.",
            "rating": 2
        },
        {
            "paper_title": "Linking the neural machine translation and the prediction of organic chemistry reactions.",
            "rating": 2
        },
        {
            "paper_title": "Predicting Electron Paths.",
            "rating": 2
        },
        {
            "paper_title": "Graph Transformation Policy Network for Chemical Reaction Prediction.",
            "rating": 2
        },
        {
            "paper_title": "Neural networks for the prediction of organic chemistry reactions.",
            "rating": 1
        },
        {
            "paper_title": "Neural-Symbolic Machine Learning for Retrosynthesis and Reaction Prediction.",
            "rating": 1
        }
    ],
    "cost": 0.0196005,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Molecular Transformer - A Model for Uncertainty-Calibrated Chemical Reaction Prediction</h1>
<p>Philippe Schwaller, ${ }^{<em> \ddagger}$, , Teodoro Laino, ${ }^{\dagger}$ Théophile Gaudin, ${ }^{\dagger}$ Peter Bolgar, ${ }^{\text {E }}$ Costas Bekas, ${ }^{\dagger}$ and Alpha A Lee</em>, ${ }^{\ddagger}$<br>†IBM Research GmbH, Zurich, Switzerland<br>$\ddagger$ Department of Physics, University of Cambridge, Cambridge, United Kingdom<br>E-mail: phs@zurich.ibm.ch; aal44@cam.ac.uk</p>
<h4>Abstract</h4>
<p>Organic synthesis is one of the key stumbling blocks in medicinal chemistry. A necessary yet unsolved step in planning synthesis is solving the forward problem: given reactants and reagents, predict the products. Similar to other work, we treat reaction prediction as a machine translation problem between SMILES strings of reactantsreagents and the products. We show that a multi-head attention Molecular Transformer model outperforms all algorithms in the literature, achieving a top-1 accuracy above $90 \%$ on a common benchmark dataset. Our algorithm requires no handcrafted rules, and accurately predicts subtle chemical transformations. Crucially, our model can accurately estimate its own uncertainty, with an uncertainty score that is $89 \%$ accurate in terms of classifying whether a prediction is correct. Furthermore, we show that the model is able to handle inputs without reactant-reagent split and including stereochemistry, which makes our method universally applicable.</p>
<h1>Introduction</h1>
<p>Organic synthesis - the making of complex molecules from simpler building blocks - remains one of the key stumbling blocks in drug discovery. ${ }^{1}$ Although the number of reported molecules has reached 135 million, this still represents only a small proportion of the estimated $10^{60}$ feasible drug-like compounds. ${ }^{2,3}$ The lack of a synthetic route hinders access to potentially fruitful regions of chemical space. Tackling the challenge of organic synthesis with data-driven approaches is particularly timely as generative models in machine learning for molecules are coming of age. ${ }^{4-10}$ These generative models enrich the toolbox of medicinal chemistry by suggesting potentially promising molecules that lie outside of known scaffolds.</p>
<p>There are three salient challenges in predicting chemical reactivity and designing organic synthesis. First, simple combinatorics would suggest that the space of possible reactions is even greater than the already intractable space of possible molecules. As such, strategies that involve handcrafted rules quickly become intractable. Second, reactants seldom contain only one reactive functional group. Designing synthesis requires one to predict which functional group will react with a particular reactant and where a reactant will react within a functional group. Predicting those subtle reactivity differences is challenging because they are often dependent on the what other functional groups are nearby. In addition, for chiral organic molecules, predicting the relative and absolute configuration of chiral centers adds another layer of complexity. Third, organic synthesis is almost always a multistep process where one failed step could invalidate the entire synthesis. For example, the pioneering total synthesis of the antibiotic tetracycline takes 18 steps; ${ }^{11}$ even a hypothetical method that would be correct $80 \%$ of the time would only have a $1 \%$ chance of getting 18 predictions correct in a row (assuming independence). Therefore, tackling the synthesis challenge requires methods that are both accurate and have a good uncertainty estimates. This would crucially allow us to estimate the "risk" of the proposed synthesis path, and put the riskier steps in the beginning of the synthesis, so that one can fail fast and fail cheap.</p>
<p>The long history of computational chemical reaction prediction has been extensively</p>
<p>reviewed in ${ }^{12}$ and. ${ }^{13}$ Methods in the literature may be divided into two different groups, namely, template-based and template-free.</p>
<p>Template-based methods ${ }^{14-16}$ use a library of reaction templates or rules. These templates describe the atoms and their bonds in the neighborhood of the reaction center, before and after the chemical reaction has occurred. Template-based methods then consider all possible reactions centers in a molecule, and enumerate the possible transformations based on the templates together with how likely each transformation is to take place. As such, the key steps in all template-based methods are the construction of templates, and the evaluation of how likely the template is to apply. The focus of the literature has thus far been on the latter question of predicting whether a template applies. ${ }^{15,16}$ However, the problem with the template-based paradigm is that templates themselves are often of questionable validity. Earlier methods generated templates by hand using chemical intuition. ${ }^{17-19}$ Handcrafting is obviously not scalable as the number of reported organic reactions constantly increases and significant time investment is needed to keep up with the literature. Recent machine learning approaches employ template libraries that are automatically extracted from datasets of reactions. ${ }^{15,16}$ Unfortunately, automatic template extraction algorithms still suffer from having to rely on meta-heuristics to define different "classes" of reactions. More problematically, all automatic template extraction algorithms rely on pre-existing atom mapping - a scheme that maps atoms in the reactants to atoms in the product. However, correctly mapping the product back to the reactant atoms is still an unsolved problem ${ }^{20}$ and, more disconcertingly, commonly used tools to find the atom-mapping (e.g. NameRXN ${ }^{21,22}$ ) are themselves based on libraries of expert rules and templates. This creates a vicious circle - atom-mapping is based on templates and templates are based on atom-mapping, and ultimately, seemingly automatic techniques are actually premised on handcrafted and often artisanal chemical rules.</p>
<p>To overcome the limitations of template-based approaches, several template-free methods have emerged over the recent years. Those methods can in turn be categorized into graph-</p>
<p>based and sequence-based. Jin et al. characterize chemical reactions by graph edits that lead from the reactants to the products. ${ }^{23}$ Their reaction prediction is a two-step process. The first network takes a graph representation of the reactants as input and predicts reactivity scores. Based on those reactivity scores, product candidates are generated and then ranked by a second network. An improved version, where candidates with up to 5 bond changes are taken into account and multi-dimensional reactivity matrices are generated, was recently presented. ${ }^{24}$ While an earlier version of the model included both reactants and reagents in the reaction center determination step, the accuracy was significantly improved by excluding the reagents from the reactivity score prediction in the more recent versions. This requires the user to know what are the identity of the reagents, which implicitly means that the user must already know the product as the reagent is defined as chemical species that do not appear in the product! Similarly, Bradshaw et al. ${ }^{25}$ separated reactants and reagents and included the reagents only in a context vector for their gated graph neural network. They represented the reaction prediction problem as a stepwise rearrangement of electrons in the reactant molecules. A side effect of phrasing reaction prediction as predicting electron flow is that a preprocessing step must be applied to eliminate reactions where the electron flow cannot easily be identified - Bradshaw et al. considered only a subset of the USPTO_MIT dataset, containing only $73 \%$ of the reactions with a linear electron flow (LEF) topology, thus by definition excluding pericyclic reactions and other important workhorse organic reactions. A more general version of the algorithm was recently presented in. ${ }^{26}$ Perhaps most intriguingly, all graph-based template-free methods in the literature require atom-mapped datasets to generate the ground truth for training, and atom mapping algorithms make use of reaction templates.</p>
<p>Sequence-based techniques have emerged as an alternative to graph-based methods. The key idea is to use a text representation of the reactants, reagents and products (usually SMILES), and treat reaction prediction as machine translation from one language (reactantsreagents) to another language (products). The idea of applying sequence-based models to</p>
<p>the reaction prediction problem was first explored by Nam \&amp; Kim. ${ }^{27}$ Schwaller et al. ${ }^{28}$ have shown that using analogies between organic chemistry and human languages sequence-tosequence models (seq-2-seq) could compete against graph-based methods. Both previous seq-2 -seq works were based on recurrent neural networks for the encoder and the decoder, with one single-head attention layer in-between. ${ }^{29,30}$ Moreover, both previous seq-2-seq forward prediction works separated reactants and reagents in the inputs using the atom-mapping, and ${ }^{28}$ tokenized the reagent molecules as individual tokens. To increase the interpretability of the model, Schwaller et al. ${ }^{28}$ used attention weight matrices and confidence scores that were generated together with the most likely product.</p>
<p>In this work, we focus on the question of predicting products given reactants and reagent. We show that a fully attention-based model adapted from ${ }^{31}$ with the SMILES ${ }^{32,33}$ representation, the Molecular Transformer, outperforms all previous methods while being completely atom-mapping independent and not requiring splitting the input into reactants and reagents. Our algorithm reaches $90.4 \%$ top-1 accuracy ( $93.7 \%$ top-2 accuracy) on a common benchmark dataset. Importantly, our algorithm does not make use of any handcrafted rules. It can accurately predict subtle and selective chemical transformations, getting the correct chemoselectivity, regioselectivity and, to some extent, stereoselectivity. In addition, our model can estimate its own uncertainty. The uncertainty score predicted by the model has an ROCAUC of 0.89 in terms of classifying whether a reaction is correctly predicted. Our model has been made available since August 2018 in the backend of the IBM RXN for Chemistry, ${ }^{34}$ a free web-based graphical user interface, and has been used by several thousand organic chemists worldwide to perform more than 40,000 predictions so far.</p>
<h1>Data</h1>
<p>Most of the publicly available reaction datasets were derived from the patent mining work of Lowe, ${ }^{35}$ where the chemical reactions were described using a text-based representation</p>
<p>called SMILES. ${ }^{32,33}$ In order to compare to previous work we focus on four datasets. The USPTO_MIT dataset was filtered and split by Jin et al. ${ }^{23}$ This dataset was also used in ${ }^{28}$ and adapted to a smaller subset called USPTO_LEF by Bradshaw et al ${ }^{25}$ to make it compatible with their algorithm. In contrast to the MIT and LEF datasets, USPTO_STEREO ${ }^{28}$ underwent less filtering and the stereochemical information was kept. Up to date, only seq-2seq models were used to predict on USPTO_STEREO. Stereochemistry adds an additional level of complexity because it requires the models not only to predict molecular graph edge changes, but potentially also changes in node labels. Additionally, we used a non-public time-split test set, extracted from the Pistachio database, ${ }^{36}$ to compare the performance on a set containing more diverse reactions against a previous seq-2-seq model. ${ }^{28}$</p>
<p>Table 1: Dataset splits and preprocessing methods used for the experiments</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reactions in</th>
<th style="text-align: center;">train</th>
<th style="text-align: center;">valid</th>
<th style="text-align: center;">test</th>
<th style="text-align: center;">total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">USPTO_MIT set ${ }^{23}$</td>
<td style="text-align: center;">409,035</td>
<td style="text-align: center;">30,000</td>
<td style="text-align: center;">40,000</td>
<td style="text-align: center;">479,035</td>
</tr>
<tr>
<td style="text-align: center;">- No stereochemical information</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">USPTO_LEF ${ }^{25}$</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">29,360</td>
<td style="text-align: center;">349,898</td>
</tr>
<tr>
<td style="text-align: center;">- Non-public subset of USPTO_MIT, without e.g. multi-step reactions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">USPTO_STEREO ${ }^{28}$</td>
<td style="text-align: center;">902,581</td>
<td style="text-align: center;">50,131</td>
<td style="text-align: center;">50,258</td>
<td style="text-align: center;">$1,002,970$</td>
</tr>
<tr>
<td style="text-align: center;">- Patent reactions until Sept. 2016, includes stereochemistry</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Pistachio_2017 ${ }^{28}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">15418</td>
<td style="text-align: center;">15418</td>
</tr>
<tr>
<td style="text-align: center;">- Non-public time split test set, reactions from 2017 taken from Pistachio database ${ }^{36,37}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Preprocessing methods</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- separated</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">source: $\mathrm{COc1c}(\mathrm{C}) \mathrm{c}(\mathrm{C}) \mathrm{c}(\mathrm{OC}) \mathrm{c}(\mathrm{C}(\mathrm{CCCCC} @ \mathrm{CCCO}) \mathrm{c} 2 \mathrm{ccccc} 2) \mathrm{c} 1 \mathrm{C}&gt;\mathrm{C} . \mathrm{CCO} .[\mathrm{Pd}]$ target: $\mathrm{COc1c}(\mathrm{C}) \mathrm{c}(\mathrm{C}) \mathrm{c}(\mathrm{OC}) \mathrm{c}(\mathrm{C}(\mathrm{CCCCCCCO}) \mathrm{c} 2 \mathrm{ccccc} 2) \mathrm{c} 1 \mathrm{C}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- mixed</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">source: C.CCO.COc1c(C)c(C)c(OC)c(C(CCCCC@CCCO)c2ccccc2)c1C.[Pd] target: $\mathrm{COc1c}(\mathrm{C}) \mathrm{c}(\mathrm{C}) \mathrm{c}(\mathrm{OC}) \mathrm{c}(\mathrm{C}(\mathrm{CCCCCCCO}) \mathrm{c} 2 \mathrm{ccccc} 2) \mathrm{c} 1 \mathrm{C}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1 shows an overview of the datasets used in this work and points out the two different preprocessing methods. The separated reagents preprocessing means that the reactants (educts), which contribute atoms to the product, are weakly separated by a $&gt;$ token from the reagents (e.g. solvents and catalysts). Reagents take part in the reaction, but do not contribute any atom to the product. So far, in most of the work, the reagents have been separated from the reactants. Jin et al. ${ }^{23}$ increased their top-1 accuracy by almost</p>
<p>$6 \%$, when they removed the reagents from the first step, where the reaction centers were predicted. In Schwaller et al. ${ }^{28}$ the reagents were represented not as individual atoms, but as separate reagent tokens and only included the 76 most common reagents. ${ }^{38}$ Bradshaw et. al. passed the reagent information as a context vector to their model. In ${ }^{26}$ it was shown that the model performs better when the reagents are tagged as such. Unfortunately, the separation of reactants and reagents is not always obvious. Different tools classify different input molecules as the reactants and hence the reagents will also differ. ${ }^{38}$ For this reason, we decided to train and test on inputs where the reactants and the reagents were mixed and no distinction was made between the two. We called this method of preprocessing mixed. The mixed preprocessing makes the reaction prediction task significantly harder, as the model has to determine the reaction center from a larger number of molecules.</p>
<p>All the reactions used in this work were canonicalized using RDKit. ${ }^{39}$ The inputs for our model were tokenized with the regular expression found in. ${ }^{28}$ In contrast to Schwaller et al., ${ }^{28}$ the reagents were not replaced by reagents tokens, but tokenized in the same way as the reactants.</p>
<h1>The Molecular Transformer</h1>
<p>The model used in this work is based on the transformer architecture. ${ }^{31}$ The model was originally constructed for neural machine translation (NMT) tasks. The main architectural difference compared to seq-2-seq models previously used for reaction prediction, ${ }^{27,28}$ is that the recurrent neural network component was completely removed and it is fully based on the attention mechanism.</p>
<p>The transformer is a step-wise autoregressive encoder-decoder model comprised of a combination of multi-head attention layers and positional feed forward layers. In the encoder, the multi-head attention layers attend the input sequence and encode it into a hidden representation. The decoder consists of two types of multi-head attention layers. The first</p>
<p>is masked and attends only the preceding outputs of the decoder. The second multi-head attention layer attends encoder outputs, as well as the output of the first decoder attention layer. It basically combines the information of the source sequence with the target sequence that has been produced so far. ${ }^{31}$</p>
<p>A multi-head attention layer itself consists of several scaled-dot attention layers running in parallel, which are then concatenated. The scaled-dot attention layers take three inputs: the keys $K$, the values $V$ and the queries $Q$, and computes the attention as follows:</p>
<p>$$
\operatorname{attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$</p>
<p>The dot product of the queries and the keys computes how closely aligned the keys are with the queries. If the query and the key are aligned, their dot product will be large and vice versa. Each key has an associated value vector, which is multiplied with the output of the softmax, through which the dot-products were normalized and the largest components were emphasized. $d_{k}$ is a scaling factor depending on the layer size. The encoder computes interesting features from the input sequence, which are then queried by the decoder depending on its preceding outputs. ${ }^{31}$</p>
<p>One main advantage of the transformer architecture compared to the seq-2-seq models used in ${ }^{27,28}$ is the multi-head attention, which allows the encoder and decoder to peek at different tokens simultaneously.</p>
<p>Since the recurrent component is missing in the transformer architecture, the sequential nature of the data is encoded with positional encodings. ${ }^{40}$ Positional encodings add positiondependent trigonometric signals (see Equations 2) to the token embeddings of size $d_{\text {emb }}$ and allow the network to know where the different tokens are situated in the sequence.</p>
<p>$$
\mathrm{PE}<em _emb="{emb" _text="\text">{(p o s, 2 i)}=\sin \left(\frac{\text { pos }}{10000^{2 i / d</em>}}}}\right), \quad \mathrm{PE<em _emb="{emb" _text="\text">{(p o s, 2 i+1)}=\cos \left(\frac{\text { pos }}{10000^{2 i / d</em>\right)
$$}}}</p>
<p>We based this work on the PyTorch implementation provided by OpenNMT. ${ }^{41}$ All the</p>
<p>components of the transformer model are explained and illustrated graphically on. ${ }^{42}$
While the base transformer model had 65 M parameters, ${ }^{31}$ we decreased the number of trainable weights to 12 M by going from 6 layers of size 512 to 4 layers of size 256 . We experimented with label smoothing ${ }^{43}$ and the number of attention heads. In contrast to the NMT model, ${ }^{31}$ we set the label smoothing parameter to 0.0 . As seen below, a non-zero label smoothing parameter encourages the model to be less confident and therefore negatively affects its ability to discriminate between correct and incorrect predictions. Moreover, we observed that at least 4 attention heads were required to achieve peak accuracies. We however, kept the original 8 attention heads, because this configuration achieved superior validation performance. For the training we use the ADAM optimizer ${ }^{44}$ and vary the learning rate as described in ${ }^{31}$ using 8000 warm up steps, the batch size is set to approximately 4096 tokens, the gradients are accumulated over four batches and normalized by the number of tokens. The model and results can be found online. ${ }^{45}$</p>
<h1>Results \&amp; Discussion</h1>
<p>Table 2: Ablation study of Molecular Transformer on the USPTO_MIT dataset with separated reagents. Train and test time were measured on a single Nvidia P100 GPU. The test set contained 40 k reactions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Top-1 [\%]</th>
<th style="text-align: center;">Top-2 [\%]</th>
<th style="text-align: center;">Top-3 [\%]</th>
<th style="text-align: center;">Top-5 [\%]</th>
<th style="text-align: center;">Training</th>
<th style="text-align: center;">Testing</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Single models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">24 h</td>
<td style="text-align: center;">20 m</td>
</tr>
<tr>
<td style="text-align: left;">Baseline augm.</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">24 h</td>
<td style="text-align: center;">20 m</td>
</tr>
<tr>
<td style="text-align: left;">Baseline augm.</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">48 h</td>
<td style="text-align: center;">20 m</td>
</tr>
<tr>
<td style="text-align: left;">Augm. av. 20</td>
<td style="text-align: center;">$\mathbf{9 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 7}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 6}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 3}$</td>
<td style="text-align: center;">48 h</td>
<td style="text-align: center;">20 m</td>
</tr>
<tr>
<td style="text-align: left;">Ensemble models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Ens. of 5</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">48 h</td>
<td style="text-align: center;">1 h 25 m</td>
</tr>
<tr>
<td style="text-align: left;">Ens. of 10</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">48 h</td>
<td style="text-align: center;">2 h 40 m</td>
</tr>
<tr>
<td style="text-align: left;">Ens. of 20</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">48 h</td>
<td style="text-align: center;">5 h 03 m</td>
</tr>
<tr>
<td style="text-align: left;">Ens. of 2 av. 20 :</td>
<td style="text-align: center;">$\mathbf{9 1 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 8}$</td>
<td style="text-align: center;">2 x 48 h</td>
<td style="text-align: center;">32 m</td>
</tr>
</tbody>
</table>
<p>Table 2 shows the performance of the model as a function of different training variations. SMILES data augmentation ${ }^{46}$ leads to a significant increase in accuracy. We double the training data by generating a copy of every reaction in the training set, where the molecules were replaced by an equivalent random SMILES (augm.) on the range of datasets and preprocessing methods. Results are also improved by averaging the weights over multiple checkpoints, as suggested in. ${ }^{31}$ Our best single models are obtained by training for 48 hours on one GPU (Nvidia P100), saving one checkpoint every 10,000 time steps and averaging the last 20 checkpoints. Ensembling different models is known to increase the performance of NMT models; ${ }^{47}$ however, the performance increase (Ens. of $5 / 10 / 20$ ) is marginal compared to parameter averaging. Nonetheless, ensembling two models which contains the weight average of 20 checkpoints of two independently initialized training runs leads to a top-1 accuracy of $91 \%$. While a higher accuracy and better uncertainty estimation can be obtained by model ensembles, they come at an additional cost in training and/or test time. The top-5 accuracies of our best single models (weight-average of the 20 last checkpoints) on the different datasets are shown in Table 3. The top-2 accuracy is significantly higher than Top-1, reaching over $93 \%$ accuracy.</p>
<p>Table 3: The single model top-k accuracy of the Molecular Transformer</p>
<table>
<thead>
<tr>
<th style="text-align: center;">USPTO*</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Top-1 [\%]</th>
<th style="text-align: center;">Top-2 [\%]</th>
<th style="text-align: center;">Top-3 [\%]</th>
<th style="text-align: center;">Top-5 [\%]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">_MIT</td>
<td style="text-align: center;">separated</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">95.3</td>
</tr>
<tr>
<td style="text-align: center;">_MIT</td>
<td style="text-align: center;">mixed</td>
<td style="text-align: center;">$\mathbf{8 8 . 6}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 4}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 2}$</td>
</tr>
<tr>
<td style="text-align: center;">_STEREO</td>
<td style="text-align: center;">separated</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">87.1</td>
</tr>
<tr>
<td style="text-align: center;">_STEREO</td>
<td style="text-align: center;">mixed</td>
<td style="text-align: center;">$\mathbf{7 6 . 2}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 4}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 3}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 8}$</td>
</tr>
</tbody>
</table>
<h1>Comparison with Previous Work</h1>
<p>As all previous works used single models, we consider only single models trained on the dataaugmented versions of the datasets rather than ensembles for the remainder of this paper in order to have a fair comparison. Table 4 shows that the Molecular Transformer clearly</p>
<p>outperforms all methods in the literature across the different datasets. Crucially, although separating reactant and reagent yields the best model (perhaps unsurprisingly because this separation implies knowledge of the product already), the Molecular Transformer still outperforms the literature when reactant and reagents are mixed. Moreover, our model achieves a reasonable accuracy in the _STEREO dataset, where stereochemical information is taken into account, whereas all prior graph-based methods in the literature cannot account for stereochemistry.</p>
<p>Table 4: Comparison of Top-1 accuracy in [\%] obtained by the different single model methods on the current benchmark datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">USPTO*</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\begin{aligned} &amp; \text { S2S } \ &amp; 28 \end{aligned}$</th>
<th style="text-align: center;">$\begin{aligned} &amp; \text { WLDN } \ &amp; 23 \end{aligned}$</th>
<th style="text-align: center;">$\begin{aligned} &amp; \text { ELECTRO } \ &amp; 25 \end{aligned}$</th>
<th style="text-align: center;">$\begin{aligned} &amp; \text { GTPN } \ &amp; 26 \end{aligned}$</th>
<th style="text-align: center;">$\begin{aligned} &amp; \text { WLDN5 } \ &amp; 24 \end{aligned}$</th>
<th style="text-align: center;">our work</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">_MIT</td>
<td style="text-align: center;">separated</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">90.4</td>
</tr>
<tr>
<td style="text-align: center;">_MIT</td>
<td style="text-align: center;">mixed</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">88.6</td>
</tr>
<tr>
<td style="text-align: center;">LEF</td>
<td style="text-align: center;">separated</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">92.0</td>
</tr>
<tr>
<td style="text-align: center;">LEF</td>
<td style="text-align: center;">mixed</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">90.3</td>
</tr>
<tr>
<td style="text-align: center;">_STEREO</td>
<td style="text-align: center;">separated</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">78.1</td>
</tr>
<tr>
<td style="text-align: center;">_STEREO</td>
<td style="text-align: center;">mixed</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">76.2</td>
</tr>
</tbody>
</table>
<p>Coley et al. ${ }^{24}$ published their predictions performance dividing the reactions of the USPTO_MIT test set into template popularity bins. The template popularity of the test set reactions was computed by counting how many times the corresponding reaction templates were observed in the training set. In Figure 1 we compare the top-1 accuracy of our USPTO_MIT models with the model of Coley et al. ${ }^{24}$ Although Coley et al. had separated the reagents in this experiment, we outperform them across all popularity bins even with our model predicting on a mixed reactants-reagents input. The accuracy gap becomes larger as the template popularity decreases. These findings suggest that the Molecular Transformer overfits common reactions less and requires fewer data points to predict well.</p>
<p>A looming question is how the Molecular Transformer perform by reaction type. Table 5 shows that the weakest predictions of the Molecular Transformer are on resolutions (the</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Top-1 accuracy of our augmented mixed and separated USPTO_MIT single models compared to the model from ${ }^{24}$ on the USPTO_MIT test set, divided into template popularity bins. The dashed lines show the average across all bins.
transformation of absolute configuration of chiral centers, where the reagents are often not recorded in the data), and the ominous label of "unclassified" (where many mis-transcribed reactions will end up). Moreover, the Molecular Transformer outperforms ${ }^{28}$ in virtually every single reaction class. This is because the multi-head attention layer in the Molecular Transformer can process long ranged interactions between tokens, whereas RNN models impose the inductive bias that tokens fare in sequence space are less related. This bias is erroneous as the token location in SMILES space bears no relation to the distance between atoms in 3D space.</p>
<h1>Comparison with human organic chemists</h1>
<p>Coley et al. ${ }^{24}$ conducted a study, where 80 random reactions from 8 different rarity bins were selected from the USPTO_MIT test set and presented to 12 chemists (Graduate students to Professors) to predict the most likely outcome. The predictions of the human chemists were then compared against those of the model. We performed the same test with our</p>
<p>Table 5: Prediction of the augm. mixed STEREO single model on the Pistachio_2017 test set, compared to, ${ }^{28}$ where the reactants and reagents were separated.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Count</th>
<th style="text-align: center;">S2S acc. ${ }^{28}[\%]$</th>
<th style="text-align: center;">Our acc. [\%]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pistachio_2017</td>
<td style="text-align: center;">15418</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">$\mathbf{7 8 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">- Classified</td>
<td style="text-align: center;">11817</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">$\mathbf{8 7 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">- Heteroatom alkylation and arylation</td>
<td style="text-align: center;">2702</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">$\mathbf{8 6 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">- Acylation and related processes</td>
<td style="text-align: center;">2601</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">$\mathbf{9 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">- Deprotections</td>
<td style="text-align: center;">1232</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">$\mathbf{8 8 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">- C-C bond formation</td>
<td style="text-align: center;">329</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">$\mathbf{8 1 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">- Functional group interconversion (FGI)</td>
<td style="text-align: center;">315</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">$\mathbf{9 1 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">- Reductions</td>
<td style="text-align: center;">1996</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">$\mathbf{8 6 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">- Functional group addition (FGA)</td>
<td style="text-align: center;">1090</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">$\mathbf{8 9 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">- Heterocycle formation</td>
<td style="text-align: center;">310</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">$\mathbf{9 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">- Protections</td>
<td style="text-align: center;">868</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">$\mathbf{8 7 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">- Oxidations</td>
<td style="text-align: center;">339</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">$\mathbf{8 5 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">- Resolutions</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">$\mathbf{3 4 . 3}$</td>
<td style="text-align: center;">28.6</td>
</tr>
<tr>
<td style="text-align: left;">- Unrecognized</td>
<td style="text-align: center;">3601</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">$\mathbf{4 6 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">With stereochemistry</td>
<td style="text-align: center;">4103</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">$\mathbf{6 7 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Without stereochemistry</td>
<td style="text-align: center;">11315</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">$\mathbf{8 1 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Invalid Smiles</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">$\mathbf{0 . 5}$</td>
</tr>
</tbody>
</table>
<p>model trained on the mixed USPTO_MIT dataset and achieve a top-1 accuracy of $87.5 \%$, significantly higher than the average of the best human ( $76.5 \%$ ) and the best graph-based model ( $72.5 \%$ ). Additionally, as seen in Figure 2 Molecular Transformer is generalizable and remains accurate even for the less common reactions.</p>
<p>Figure 3 shows the 6 of the 80 reactions for which our model did not output the correct prediction in its top-2 choices. Even though our model does not predict the ground truth, it usually predicts a reasonable most likely outcome: In RXN 14, our model predicts that a primary amine acts as the nucleophile in an amide formation reaction rather than a secondary amine, which is reasonable on the grounds of sterics. In RXN 68, the reaction yielding the reported ground truth is via a nucleophilic substitution of $\mathrm{Cl}^{-}$by $\mathrm{OH}^{-}$by addition-elimination mechanism, followed by lactim-lactam tautomerism. For the reaction to work there must have been a source of hydroxide ions, which is not indicated among the reactants. In the absence of hydroxide ions, the best nucleophile in the reaction mixture is the phenolate ion generated from the phenol by deprotonation by sodium hydride. In RXN</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Top-1 accuracy of our model (mixed, USPTO_MIT) on 80 chemical reactions across 8 reaction popularity bins in comparison with a human study and their graph-based model (WLDN5). ${ }^{24}$</p>
<p>72, the correct product predicted, but the ground truth additionally reports a by-product (which is mechanistically dubious as HCl will react with excess amine to form the ammonium salt). In RXN 76, a carbon atom is clearly missing in the ground truth. In RXN 61, we predict a $\mathrm{SN}_{2}$ where the anion of the alcohol of the beta hydroxy ester acts as nucleophile, whereas the mechanism of the ground truth is presumably ester hydrolysis followed by nucleophilic attack of the carboxylate group. Proton transfers in protic solvents are extremely fast, thus deprotonation of the alcohol OH is much faster than ester hydrolysis. Moreover, the carboxylate anion is a poor nucleophile.</p>
<h1>Uncertainty estimation and reaction pathway scoring</h1>
<p>As organic synthesis is a multistep process, in order for a reaction predictor to be useful it must be able to estimate its own uncertainty. The Molecular Transformer model provides a natural way achieve this - the product of the probabilities of all predicted tokens can be used as a confidence score. Figure 4 plots the receiver operating characteristics (ROC) curve and</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The 6 reactions in the human test set ${ }^{24}$ not predicted within top-2 using our model trained on the augmented mixed USPTO_MIT set.
shows that the AUC-ROC is 0.89 if we use this confidence score as a threshold to predict whether a reaction is mispredicted. Interestingly, Figure 4 reveals that a subtle change in the training method, label smoothing, has a minimal effect on accuracy but a surprisingly significant impact on uncertainty quantification. Label smoothing was introduced by Vaswani et al. ${ }^{31}$ for NMT models. Instead of simply maximizing likelihood of the next target token at a given time step, the network learns a distribution over all possible tokens. Therefore, it is less confident about its predictions. Label smoothing helps to generate higher-scoring</p>
<p>translations in terms of accuracy and BLEU score ${ }^{48}$ for human languages, and also helps in terms of reaching higher top-1 accuracy in reaction prediction. The top-1 accuracy on the validation set (mixed, USPTO_MIT) with the label smoothing parameter set to 0.01 is $87.44 \%$ compared to $87.28 \%$ for no smoothing. However, Figure 4 shows that this small increase in accuracy comes with the cost of not being to able to discriminate between a good and a bad prediction anymore. Therefore, no label smoothing was used during the training of our models. The AUC-ROC of our single mixed USPTO_MIT model measured on the test set was also at 0.89 . The uncertainty estimation metric allows us to estimate the likelihood of a given reactants-product combination, rather only predicting product given reactants, and this could be used as a score to rank reaction pathways. ${ }^{49,50}$
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Receiver operatoring characteristic curve for different label smoothing values for a model trained on the mixed USPTO_MIT dataset, when evaluated on the validation set.</p>
<h1>Chemically Constrained Beam Search</h1>
<p>As no chemical knowledge was integrated into the model, technically, the model could perform "alchemy", e.g. turning a fluoride atom in the reactants into a bromide atom in the products, which was not in the reactants at all. As such, an interesting question is whether the model has learnt to avoid alchemy. To this end, we implemented a constrained beam search, where the probabilities of atomic tokens not observed in the reactants are set to 0.0</p>
<p>and hence not predicted. However, there was no change in accuracy, showing that the model had successfully inferred this constraint from the examples shown during training.</p>
<h1>Conclusion</h1>
<p>We show that a multi-head attention Transformer network, the Molecular Transformer, outperforms all known algorithms in the reaction prediction literature, achieving $90.4 \%$ top-1 accuracy ( $93.7 \%$ top-2 accuracy) on a common benchmark dataset. The model requires no handcrafted rules, and accurately predicts subtle chemical transformations. Moreover, the Molecular Transformer can also accurately estimate its own uncertainty, with an uncertainty score that is $89 \%$ accurate in terms of classifying whether a prediction is correct. The uncertainty score can be used to rank reaction pathways. We point out that previous work have all considered an unrealistically generous setting of separated reactants and reagents. We demonstrate an accuracy of $88.6 \%$ when no distinction is drawn between reactants and reagents in the inputs, a score that outperforms previous work as well. For the more noisy USPTO_STEREO dataset, our top-1 accuracies are $78.1 \%$ (separated) and $76.2 \%$ respectively. The Molecular Transformer has been freely available since August 2018 through a graphical user interface on the IBM RXN for Chemistry platform, ${ }^{34}$ and has so far been used by several thousand organic chemists worldwide for performing more than 40,000 chemical reaction predictions.</p>
<h2>Acknowledgement</h2>
<p>PS and AAL acknowledges the Winton Programme for the Physics of Sustainability for funding. The authors thank G. Landrum, R. Sayle, G. Godin and R. Griffiths for useful feedback and discussions.</p>
<h1>References</h1>
<p>(1) Blakemore, D. C.; Castro, L.; Churcher, I.; Rees, D. C.; Thomas, A. W.; Wilson, D. M.; Wood, A. Organic synthesis provides opportunities to transform drug discovery. Nature chemistry 2018, 10, 383 .
(2) Bohacek, R. S.; McMartin, C.; Guida, W. C. The art and practice of structure-based drug design: a molecular modeling perspective. Medicinal research reviews 1996, 16, $3-50$.
(3) Boström, J.; Brown, D. G.; Young, R. J.; Keserü, G. M. Expanding the medicinal chemistry synthetic toolbox. Nature Reviews Drug Discovery 2018,
(4) Kusner, M. J.; Paige, B.; Hernández-Lobato, J. M. Grammar variational autoencoder. arXiv preprint arXiv:1703.01925 2017,
(5) Gómez-Bombarelli, R.; Wei, J. N.; Duvenaud, D.; Hernández-Lobato, J. M.; SánchezLengeling, B.; Sheberla, D.; Aguilera-Iparraguirre, J.; Hirzel, T. D.; Adams, R. P.; Aspuru-Guzik, A. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science 2018, 4, 268-276.
(6) Griffiths, R.-R.; Hernández-Lobato, J. M. Constrained bayesian optimization for automatic chemical design. arXiv preprint arXiv:1709.05501 2017,
(7) Popova, M.; Isayev, O.; Tropsha, A. Deep reinforcement learning for de novo drug design. Science advances 2018, 4, eaap7885.
(8) Blaschke, T.; Olivecrona, M.; Engkvist, O.; Bajorath, J.; Chen, H. Application of generative autoencoder in de novo molecular design. Molecular informatics 2018, 37, 1700123 .
(9) Jin, W.; Barzilay, R.; Jaakkola, T. Junction Tree Variational Autoencoder for Molecular Graph Generation. arXiv preprint arXiv:1802.04364 2018,</p>
<p>(10) Kang, S.; Cho, K. Conditional molecular design with deep generative models. arXiv preprint arXiv:1805.00108 2018,
(11) Charest, M. G.; Lerner, C. D.; Brubaker, J. D.; Siegel, D. R.; Myers, A. G. A convergent enantioselective route to structurally diverse 6-deoxytetracycline antibiotics. Science 2005, 308, 395-398.
(12) Engkvist, O.; Norrby, P.-O.; Selmi, N.; Lam, Y.-h.; Peng, Z.; Sherer, E. C.; Amberg, W.; Erhard, T.; Smyth, L. A. Computational prediction of chemical reactions: current status and outlook. Drug discovery today 2018,
(13) Coley, C. W.; Green, W. H.; Jensen, K. F. Machine Learning in Computer-Aided Synthesis Planning. Accounts of chemical research 2018, 51, 1281-1289.
(14) Wei, J. N.; Duvenaud, D.; Aspuru-Guzik, A. Neural networks for the prediction of organic chemistry reactions. ACS central science 2016, 2, 725-732.
(15) Coley, C. W.; Barzilay, R.; Jaakkola, T. S.; Green, W. H.; Jensen, K. F. Prediction of organic reaction outcomes using machine learning. ACS central science 2017, 3, $434-443$.
(16) Segler, M. H.; Waller, M. P. Neural-Symbolic Machine Learning for Retrosynthesis and Reaction Prediction. Chemistry-A European Journal 2017, 23, 5966-5971.
(17) Corey, E. J.; Long, A. K.; Rubenstein, S. D. Computer-assisted analysis in organic synthesis. Science 1985, 228, 408-418.
(18) Szymkuć, S.; Gajewska, E. P.; Klucznik, T.; Molga, K.; Dittwald, P.; Startek, M.; Bajczyk, M.; Grzybowski, B. A. Computer-Assisted Synthetic Planning: The End of the Beginning. Angewandte Chemie International Edition 2016, 55, 5904-5937.
(19) Grzybowski, B. A.; Szymkuć, S.; Gajewska, E. P.; Molga, K.; Dittwald, P.; Wołos, A.;</p>
<p>Klucznik, T. Chematica: A Story of Computer Code That Started to Think like a Chemist. Chem 2018, 4, 390-398.
(20) Chen, W. L.; Chen, D. Z.; Taylor, K. T. Automatic reaction mapping and reaction center detection. Wiley Interdisciplinary Reviews: Computational Molecular Science 2013, 3, 560-593.
(21) Nextmove Software NameRXN. http://www.nextmovesoftware.com/namerxn.html.
(22) Schneider, N.; Lowe, D. M.; Sayle, R. A.; Landrum, G. A. Development of a novel fingerprint for chemical reactions and its application to large-scale reaction classification and similarity. Journal of chemical information and modeling 2015, 55, 39-53.
(23) Jin, W.; Coley, C.; Barzilay, R.; Jaakkola, T. Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network. Advances in Neural Information Processing Systems. 2017; pp 2604-2613.
(24) Coley, C. W.; Jin, W.; Rogers, L.; Jamison, T. F.; Jaakkola, T. S.; Green, W. H.; Barzilay, R.; Jensen, K. F. A graph-convolutional neural network model for the prediction of chemical reactivity. Chemical science 2019, 10, 370-377.
(25) Bradshaw, J.; Kusner, M. J.; Paige, B.; Segler, M. H.; Hernández-Lobato, J. M. Predicting Electron Paths. arXiv preprint arXiv:1805.10970 2018,
(26) Do, K.; Tran, T.; Venkatesh, S. Graph Transformation Policy Network for Chemical Reaction Prediction. arXiv preprint arXiv:1812.09441 2018,
(27) Nam, J.; Kim, J. Linking the neural machine translation and the prediction of organic chemistry reactions. arXiv preprint arXiv:1612.09529 2016,
(28) Schwaller, P.; Gaudin, T.; Lanyi, D.; Bekas, C.; Laino, T. "Found in Translation": Predicting Outcomes of Complex Organic Chemistry Reactions using Neural Sequence-to-Sequence Models. Chemical Science 2018,</p>
<p>(29) Bahdanau, D.; Cho, K.; Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 2014,
(30) Luong, M.-T.; Pham, H.; Manning, C. D. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025 2015,
(31) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems. 2017; pp 6000-6010.
(32) Weininger, D. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. Journal of chemical information and computer sciences 1988, 28, 31-36.
(33) Weininger, D.; Weininger, A.; Weininger, J. L. SMILES. 2. Algorithm for generation of unique SMILES notation. Journal of chemical information and computer sciences 1989, 29, 97-101.
(34) IBM RXN for Chemistry. https://rxn.res.ibm.com.
(35) Lowe, D. M. Extraction of chemical structures and reactions from the literature. Ph.D. thesis, University of Cambridge, 2012.
(36) Nextmove Software Pistachio. http://www.nextmovesoftware.com/pistachio.html.
(37) Schneider, N.; Lowe, D. M.; Sayle, R. A.; Tarselli, M. A.; Landrum, G. A. Big data from pharmaceutical patents: a computational analysis of medicinal chemists' bread and butter. J. Med. Chem. 2016, 59, 4385-4402.
(38) Schneider, N.; Stiefl, N.; Landrum, G. A. What's what: The (nearly) definitive guide to reaction role assignment. Journal of chemical information and modeling 2016, 56, $2336-2346$.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7484 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7484</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7484</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-253098632</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2022.emnlp-main.248.pdf" target="_blank">Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</a></p>
                <p><strong>Paper Abstract:</strong> Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allows humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theorybased perspective. We show that one of today’s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measure models’ ability to understand intents and reactions of participants of social interactions, and ToMi (Le, Boureau, and Nickel, 2019), which measures whether models can infer mental states and realities of participants of situations.Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7484.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7484.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOCIALIQA (GPT-3-DAVINCI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SOCIALIQA benchmark evaluated with GPT-3 (DAVINCI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-3 (DAVINCI) on the SOCIALIQA social commonsense / emotional intelligence multiple-choice QA benchmark; measures ability to infer intents, needs, reactions, and effects in everyday social situations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3-DAVINCI</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive large pretrained language model from the GPT-3 family (Brown et al., 2020), probed via API in few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SOCIALIQA</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>social commonsense / social intelligence (theory of mind related)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice QA (3 choices) where each instance gives a social context and asks about participants' intents, reactions, needs, next actions, or effects; drawn from ATOMIC dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>not reported numerically in this paper; paper states GPT-3's 55% accuracy is >30% below human performance (i.e., humans perform >~85%), exact human value not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 55% (best-performing few-shot GPT-3 setup reported)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot prompting (k-shot, k up to 35; best results reported for DAVINCI few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors report GPT-3-DAVINCI achieves ~55% on SOCIALIQA dev set and lags >30% behind humans. Performance gains plateau after ~k=10 examples. Model does better on agent-centric questions than on questions about other participants and exhibits participant-confusion errors. The paper also cites BIG-Bench results showing BIG-G (128B) at 45% and PaLM at 73% on SOCIALIQA (these are reported from BIG-Bench repo, not run by the authors). Models finetuned on SOCIALIQA training data achieve much higher performance (e.g., prior fine-tuned models reported ~83% on the test set per AI2 leaderboard).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7484.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7484.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TOMI-MIND (GPT-3-DAVINCI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TOMI (mental-state questions) evaluated with GPT-3 (DAVINCI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-3 (DAVINCI) on the TOMI QA dataset (Sally‑Ann like stories) focusing on questions that probe participants' mental states (first- and second-order beliefs, true- and false-belief).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3-DAVINCI</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive large pretrained language model from the GPT-3 family (Brown et al., 2020), probed via API in few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>TOMI (MIND questions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>theory of mind / false-belief / mental state reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Two-way QA pairs based on short Sally-Ann-like stories; MIND questions ask where a participant will look for an object (their belief) and include first-order and second-order, true- and false-belief variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>not numerically reported in this paper; authors characterize the reported LLM accuracies as 'well-below-human' but do not give a specific human baseline for TOMI within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy peak ~60% on MIND (mental-state) questions for best GPT-3 setup (DAVINCI); smaller GPT-3 models do not surpass ~55%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot prompting (k ∈ {2,4,8,16,24}; best reported behavior around k=4 for some MIND types)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors highlight a large gap between performance on MIND questions (~55–60%) versus factual questions (~90–100%) for GPT-3-DAVINCI. GPT-3-DAVINCI shows poorer performance on second-order (MIND-2nd) vs first-order (MIND-1st) questions and exhibits recency bias (tendency to default to most recently mentioned object location) that affects MIND-TB behavior; increasing k beyond small values does not substantially improve MIND accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7484.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7484.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TOMI-FACT (GPT-3-DAVINCI/CURIE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TOMI (factual questions) evaluated with GPT-3 (CURIE and DAVINCI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation showing that GPT-3 variants (CURIE and DAVINCI) achieve near-perfect accuracy on TOMI factual memory/real-location questions, contrasting with much lower performance on mental-state questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3-CURIE and GPT-3-DAVINCI</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive pretrained GPT-3 family models probed via few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>TOMI (FACT questions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>memory / factual reasoning (object location memory)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Two-way QA pairs asking about the object's original (FACT-MEM) or final (FACT-REAL) location; these do not require Theory of Mind.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>not provided in this paper for FACT items specifically</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>near-perfect accuracy reported (90–100%) for GPT-3-CURIE and GPT-3-DAVINCI on FACT questions in few-shot settings (k>0)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot prompting (various k; FACT performance high for k>0)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors use this contrast to argue that LLMs can track explicit factual information in short stories but fail to reason about participants' mental states (MIND). GPT-3-ADA, the smallest variant tested, struggled with both FACT and MIND, possibly due to size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7484.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7484.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM on SOCIALIQA (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM (Pathways Language Model) performance on SOCIALIQA as reported from BIG-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported few-shot performance of Google's PaLM (as cited from BIG-Bench repository) on the SOCIALIQA benchmark; included in the paper's extended results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large language model from Google / DeepMind (Chowdhery et al., 2022); reported here via BIG-Bench results.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>353B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SOCIALIQA (few-shot, from BIG-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>social commonsense / social intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Few-shot evaluation on the SOCIALIQA dev set (3-way multiple-choice social commonsense QA).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>not reported numerically here; paper states PaLM still lags behind humans</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>peak accuracy reported ~73% (reported from BIG-Bench repository)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot (BIG-Bench reported up to k=5 for PaLM; authors cite BIG-Bench results)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>PaLM's reported 73% on SOCIALIQA (from BIG-Bench) is included to corroborate that large proprietary models still underperform humans on SOCIALIQA per the paper; authors did not run PaLM themselves but cite BIG-Bench results (accessed 2022-11-10).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7484.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7484.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIG-G on SOCIALIQA (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BIG-G (BIG-Bench 128B) performance on SOCIALIQA as reported from BIG-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported few-shot performance of the BIG-G (128B) model on SOCIALIQA from BIG-Bench; used in the paper to show that scaling alone did not close the gap to humans on SOCIALIQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BIG-G</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large language model included in BIG-Bench evaluations (Srivastava et al., 2022); results cited by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>128B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>SOCIALIQA (few-shot, from BIG-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>social commonsense / social intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Few-shot evaluation on the SOCIALIQA dev set (3-way multiple-choice social commonsense QA).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>not reported numerically in this paper; authors state BIG-G lags behind humans</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>peak accuracy reported ~45% (reported from BIG-Bench repository)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot (BIG-Bench reported up to k=3 for BIG-G; authors cite BIG-Bench results)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Included by the authors to show that even other large models evaluated in BIG-Bench can perform substantially below humans on SOCIALIQA; results cited from BIG-Bench repo (accessed 2022-11-10).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Social iqa: Commonsense reasoning about social interactions <em>(Rating: 2)</em></li>
                <li>Revisiting the evaluation of theory of mind through question answering <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>Pathways language model (PaLM): Scaling to 540 billion parameters for breakthrough performance <em>(Rating: 2)</em></li>
                <li>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models <em>(Rating: 1)</em></li>
                <li>BIG-Bench: A Large-Scale Evaluation Benchmark for Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7484",
    "paper_id": "paper-253098632",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "SOCIALIQA (GPT-3-DAVINCI)",
            "name_full": "SOCIALIQA benchmark evaluated with GPT-3 (DAVINCI)",
            "brief_description": "Evaluation of GPT-3 (DAVINCI) on the SOCIALIQA social commonsense / emotional intelligence multiple-choice QA benchmark; measures ability to infer intents, needs, reactions, and effects in everyday social situations.",
            "citation_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
            "mention_or_use": "use",
            "model_name": "GPT-3-DAVINCI",
            "model_description": "Autoregressive large pretrained language model from the GPT-3 family (Brown et al., 2020), probed via API in few-shot prompting.",
            "model_size": null,
            "test_name": "SOCIALIQA",
            "test_category": "social commonsense / social intelligence (theory of mind related)",
            "test_description": "Multiple-choice QA (3 choices) where each instance gives a social context and asks about participants' intents, reactions, needs, next actions, or effects; drawn from ATOMIC dimensions.",
            "evaluation_metric": "accuracy (percentage correct)",
            "human_performance": "not reported numerically in this paper; paper states GPT-3's 55% accuracy is &gt;30% below human performance (i.e., humans perform &gt;~85%), exact human value not specified here.",
            "llm_performance": "accuracy 55% (best-performing few-shot GPT-3 setup reported)",
            "prompting_method": "few-shot prompting (k-shot, k up to 35; best results reported for DAVINCI few-shot)",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Authors report GPT-3-DAVINCI achieves ~55% on SOCIALIQA dev set and lags &gt;30% behind humans. Performance gains plateau after ~k=10 examples. Model does better on agent-centric questions than on questions about other participants and exhibits participant-confusion errors. The paper also cites BIG-Bench results showing BIG-G (128B) at 45% and PaLM at 73% on SOCIALIQA (these are reported from BIG-Bench repo, not run by the authors). Models finetuned on SOCIALIQA training data achieve much higher performance (e.g., prior fine-tuned models reported ~83% on the test set per AI2 leaderboard).",
            "uuid": "e7484.0",
            "source_info": {
                "paper_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "TOMI-MIND (GPT-3-DAVINCI)",
            "name_full": "TOMI (mental-state questions) evaluated with GPT-3 (DAVINCI)",
            "brief_description": "Evaluation of GPT-3 (DAVINCI) on the TOMI QA dataset (Sally‑Ann like stories) focusing on questions that probe participants' mental states (first- and second-order beliefs, true- and false-belief).",
            "citation_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
            "mention_or_use": "use",
            "model_name": "GPT-3-DAVINCI",
            "model_description": "Autoregressive large pretrained language model from the GPT-3 family (Brown et al., 2020), probed via API in few-shot prompting.",
            "model_size": null,
            "test_name": "TOMI (MIND questions)",
            "test_category": "theory of mind / false-belief / mental state reasoning",
            "test_description": "Two-way QA pairs based on short Sally-Ann-like stories; MIND questions ask where a participant will look for an object (their belief) and include first-order and second-order, true- and false-belief variants.",
            "evaluation_metric": "accuracy (percentage correct)",
            "human_performance": "not numerically reported in this paper; authors characterize the reported LLM accuracies as 'well-below-human' but do not give a specific human baseline for TOMI within this paper.",
            "llm_performance": "accuracy peak ~60% on MIND (mental-state) questions for best GPT-3 setup (DAVINCI); smaller GPT-3 models do not surpass ~55%",
            "prompting_method": "few-shot prompting (k ∈ {2,4,8,16,24}; best reported behavior around k=4 for some MIND types)",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Authors highlight a large gap between performance on MIND questions (~55–60%) versus factual questions (~90–100%) for GPT-3-DAVINCI. GPT-3-DAVINCI shows poorer performance on second-order (MIND-2nd) vs first-order (MIND-1st) questions and exhibits recency bias (tendency to default to most recently mentioned object location) that affects MIND-TB behavior; increasing k beyond small values does not substantially improve MIND accuracy.",
            "uuid": "e7484.1",
            "source_info": {
                "paper_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "TOMI-FACT (GPT-3-DAVINCI/CURIE)",
            "name_full": "TOMI (factual questions) evaluated with GPT-3 (CURIE and DAVINCI)",
            "brief_description": "Evaluation showing that GPT-3 variants (CURIE and DAVINCI) achieve near-perfect accuracy on TOMI factual memory/real-location questions, contrasting with much lower performance on mental-state questions.",
            "citation_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
            "mention_or_use": "use",
            "model_name": "GPT-3-CURIE and GPT-3-DAVINCI",
            "model_description": "Autoregressive pretrained GPT-3 family models probed via few-shot prompting.",
            "model_size": null,
            "test_name": "TOMI (FACT questions)",
            "test_category": "memory / factual reasoning (object location memory)",
            "test_description": "Two-way QA pairs asking about the object's original (FACT-MEM) or final (FACT-REAL) location; these do not require Theory of Mind.",
            "evaluation_metric": "accuracy (percentage correct)",
            "human_performance": "not provided in this paper for FACT items specifically",
            "llm_performance": "near-perfect accuracy reported (90–100%) for GPT-3-CURIE and GPT-3-DAVINCI on FACT questions in few-shot settings (k&gt;0)",
            "prompting_method": "few-shot prompting (various k; FACT performance high for k&gt;0)",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Authors use this contrast to argue that LLMs can track explicit factual information in short stories but fail to reason about participants' mental states (MIND). GPT-3-ADA, the smallest variant tested, struggled with both FACT and MIND, possibly due to size.",
            "uuid": "e7484.2",
            "source_info": {
                "paper_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "PaLM on SOCIALIQA (reported)",
            "name_full": "PaLM (Pathways Language Model) performance on SOCIALIQA as reported from BIG-Bench",
            "brief_description": "Reported few-shot performance of Google's PaLM (as cited from BIG-Bench repository) on the SOCIALIQA benchmark; included in the paper's extended results.",
            "citation_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
            "mention_or_use": "mention",
            "model_name": "PaLM",
            "model_description": "Proprietary large language model from Google / DeepMind (Chowdhery et al., 2022); reported here via BIG-Bench results.",
            "model_size": "353B",
            "test_name": "SOCIALIQA (few-shot, from BIG-Bench)",
            "test_category": "social commonsense / social intelligence",
            "test_description": "Few-shot evaluation on the SOCIALIQA dev set (3-way multiple-choice social commonsense QA).",
            "evaluation_metric": "accuracy (percentage correct)",
            "human_performance": "not reported numerically here; paper states PaLM still lags behind humans",
            "llm_performance": "peak accuracy reported ~73% (reported from BIG-Bench repository)",
            "prompting_method": "few-shot (BIG-Bench reported up to k=5 for PaLM; authors cite BIG-Bench results)",
            "fine_tuned": null,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "PaLM's reported 73% on SOCIALIQA (from BIG-Bench) is included to corroborate that large proprietary models still underperform humans on SOCIALIQA per the paper; authors did not run PaLM themselves but cite BIG-Bench results (accessed 2022-11-10).",
            "uuid": "e7484.3",
            "source_info": {
                "paper_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "BIG-G on SOCIALIQA (reported)",
            "name_full": "BIG-G (BIG-Bench 128B) performance on SOCIALIQA as reported from BIG-Bench",
            "brief_description": "Reported few-shot performance of the BIG-G (128B) model on SOCIALIQA from BIG-Bench; used in the paper to show that scaling alone did not close the gap to humans on SOCIALIQA.",
            "citation_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
            "mention_or_use": "mention",
            "model_name": "BIG-G",
            "model_description": "Proprietary large language model included in BIG-Bench evaluations (Srivastava et al., 2022); results cited by the authors.",
            "model_size": "128B",
            "test_name": "SOCIALIQA (few-shot, from BIG-Bench)",
            "test_category": "social commonsense / social intelligence",
            "test_description": "Few-shot evaluation on the SOCIALIQA dev set (3-way multiple-choice social commonsense QA).",
            "evaluation_metric": "accuracy (percentage correct)",
            "human_performance": "not reported numerically in this paper; authors state BIG-G lags behind humans",
            "llm_performance": "peak accuracy reported ~45% (reported from BIG-Bench repository)",
            "prompting_method": "few-shot (BIG-Bench reported up to k=3 for BIG-G; authors cite BIG-Bench results)",
            "fine_tuned": null,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Included by the authors to show that even other large models evaluated in BIG-Bench can perform substantially below humans on SOCIALIQA; results cited from BIG-Bench repo (accessed 2022-11-10).",
            "uuid": "e7484.4",
            "source_info": {
                "paper_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Social iqa: Commonsense reasoning about social interactions",
            "rating": 2
        },
        {
            "paper_title": "Revisiting the evaluation of theory of mind through question answering",
            "rating": 2
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "Pathways language model (PaLM): Scaling to 540 billion parameters for breakthrough performance",
            "rating": 2
        },
        {
            "paper_title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "rating": 1
        },
        {
            "paper_title": "BIG-Bench: A Large-Scale Evaluation Benchmark for Language Models",
            "rating": 1
        }
    ],
    "cost": 0.0160075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</p>
<p>Maarten Sap maartensap@cmu.edu 
Allen Institute for AI
SeattleWAUSA</p>
<p>Language Technologies Institute
Carnegie Mellon University
PittsburghUSA</p>
<p>Ronan Le Bras 
Allen Institute for AI
SeattleWAUSA</p>
<p>Daniel Fried 
Language Technologies Institute
Carnegie Mellon University
PittsburghUSA</p>
<p>Yejin Choi 
Allen Institute for AI
SeattleWAUSA</p>
<p>Paul G. Allen School of Computer Science
University of Washington
SeattleWAUSA</p>
<p>Tom B Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Amanda Askell 
Sandhini Agarwal 
Ariel Herbert-Voss 
Gretchen Krueger 
Tom Henighan 
Rewon Child 
Aditya Ramesh 
Daniel M Ziegler 
Jeffrey Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Mateusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Sam Mc- Candlish 
Alec Radford 
Ilya Sutskever 
Dario 2020 Amodei 
Mia Xu Chen 
Benjamin N Lee 
Gagan Bansal 
Yuan Cao 
Shuyuan Zhang 
Justin Lu 
Jackie Tsay 
Yi- Nan Wang 
Andrew M Dai 
Zhifeng Chen 
Timothy Sohn 
Yonghui Wu 
Aakanksha Chowdhery 
Sharan Narang 
Jacob Devlin 
Maarten Bosma 
Gaurav Mishra 
Adam Roberts 
Paul Barham 
Hyung Won 
Chung Charles Sutton 
Sebastian Gehrmann 
Parker Schuh 
Kensen Shi 
Sasha Tsvyashchenko 
Joshua Maynez 
Abhishek Rao 
Parker Barnes 
Yi Tay 
Noam Shazeer 
Vin- Odkumar Prabhakaran 
Emily Reif 
Nan Du 
Ben Hutchinson 
Reiner Pope 
James Bradbury 
Jacob Austin 
Michael Isard Guy 
Gur-Ari Pengcheng 
Yin Toju Duke 
Anselm Levskaya 
Sanjay Ghe- Mawat 
Sunipa Dev 
Henryk Michalewski 
Xavier Garcia 
Vedant Misra 
Kevin Robinson 
Liam Fe- Dus 
Denny Zhou 
Daphne Ippolito 
David Luan 
Hyeontaek Lim 
Barret Zoph 
Alexander Spiridonov 
Ryan Sepassi 
David Dohan 
Shivani Agrawal 
Mark Omernick 
Thanumalayan Sankara- Narayana Pillai 
Marie Pellat 
Aitor Lewkowycz 
Erica Moreira 
Oleksandr Polozov 
Katherine Lee 
Zongwei Zhou 
Xuezhi Wang 
Bren- Nan Saeta 
Mark Diaz 
Orhan Firat 
Michele Catasta 
Jason Wei 
Kathy Meier-Hellstern 
Elizabeth Clark 
Tal August 
Sofia Serrano 
Nikita Haduong 
Suchin Gururangan 
Noah A Smith 
Herbert H Clark 
Susan E 1991 Brennan 
Kate Crawford </p>
<p>Douglas Eck
Slav Petrov, and Noah Fiedel
2022Jeff Dean</p>
<p>Using Language. Cambridge University Press
1996</p>
<p>Atlas of AI
Yale University Press</p>
<p>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs
353A0B1B72BA78056EB5C33C2C111A0C
Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effectively navigate and understand everyday social interactions.As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theory-based perspective.We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SOCIALIQA (Sap et al., 2019b), which measures models' ability to understand intents and reactions of participants of social interactions, and TOMI (Le et al., 2019), which measures whether models can infer mental states and realities of participants of situations.Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SOCIALIQA and TOMI, respectively.To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms.Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.</p>
<p>Introduction</p>
<p>With the growing prevalence of AI and NLP systems in everyday social interactions, the need for AI systems with social intelligence and Theory of Mind (TOM), i.e., the ability to infer and reason about the intents, feelings, and mental states of others, becomes increasingly evident (Pereira et al.,</p>
<p>Reasoning about mental states and realities</p>
<p>Social commonsense and emotional intelligence</p>
<p>Although Taylor was older and stronger, they lost to Alex in the wrestling match.</p>
<p>James and Abby are in the bedroom.Abby put the pen in the desk drawer.Abby leaves the bedroom.James moves the pen into the bag.</p>
<p>How would Alex feel as a result?</p>
<p>Where does James think Abby will look for the pen?</p>
<p>ashamed boastful drawer bag</p>
<p>Measuring Neural Theory of Mind</p>
<p>Figure 1: Theory of Mind is the ability for humans to reason about the intents, reactions, and mental states of others.We asses these abilities in LLMs through two question-answering tasks that measure social commonsense and emotional intelligence (SOCIALIQA; top) and reasoning about people's mental states and realities (TOMI; bottom); finding that GPT-3 ( ) struggles on both tasks.We discuss why that may be, drawing from theories of the pragmatics of language.</p>
<p>2016; Langley et al., 2022).For humans, Theory of Mind is a crucial component that enables us to interact and communicate effectively with each other (Premack and Woodruff, 1978;Apperly, 2010).It allows us, for example, to infer that someone likely feels boastful instead of ashamed after winning a wrestling match (Fig. 1; top).In addition, TOM also enables us to reason about people's mental realities, e.g., if someone was out of the room while a pen was moved, she will likely search for the pen where she last saw it instead of where it was moved to (Fig. 1; bottom).</p>
<p>While humans develop it naturally, TOM and social intelligence remain elusive goals for modern AI systems (Choi, 2022), including large neural language models (LLMs).With advances in scaling the sizes of models and datasets, these LLMs have proven very impressive at generating humanlike language for conversational, summarization, or sentence continuation settings, often with zero to few examples to learn from (Brown et al., 2020;Clark et al., 2021;Chowdhery et al., 2022).However, increasing scrutiny has shed light on the shortcomings of these LLMs, showing that they often fall prey to spurious correlational patterns instead of displaying higher-order reasoning (Elkins and Chun, 2020;Dale, 2021;Marcus, 2022).</p>
<p>In line with EMNLP 2022's theme, we examine the open research question of whether and how much LLMs-which are the backbone of most modern NLP systems-exhibit social intelligence and TOM abilities.Using some of the largest English models in existence (GPT-3;Brown et al., 2020), we demonstrate that out-of-the-box LLMs struggle at two types of reasoning abilities that requisites for Theory of Mind (shown in Fig. 1).We argue that these reasoning abilities are necessary but not sufficient for Theory of Mind, and that larger models will likely provide upper bounds on what equivalent-but-smaller models are capable of.</p>
<p>We first assess whether LLMs can reason about social commonsense and emotional intelligence with respect to social interactions ( §3), using the SOCIALIQA benchmark (Sap et al., 2019b) illustrated in Fig. 1 (top).Results show our best performing few-shot GPT-3 setup achieving only 55% accuracy, lagging &gt;30% behind human performance.Furthermore, social reasoning about the protagonists of situations is easier for GPT-3 (5-15% absolute difference) compared to reasoning about other secondary participants.</p>
<p>Second, we measure LLMs' ability to understand other people's mental states and realities in short stories ( §4).We use the TOMI QA benchmark (illustrated in Fig. 1; bottom; Le et al., 2019), which was inspired by the classic Sally-Ann False Belief Theory of Mind test (Baron-Cohen et al., 1985).Here, our results show that GPT-3 models peak at 60% accuracy on questions about participants' mental states, compared to 90-100% on factual questions.</p>
<p>Our novel insights show that reasoning about social situations and false beliefs still presents a significant challenge for large language models, despite their seemingly impressive performance on tasks that could require social intelligence (e.g., story generation, dialogues).In §5, we first examine these shortcomings; drawing on theories of the pragmatics of language, we speculate that the type of texts in LLMs' training datasets could substantially limit learning social intelligence.Then, we outline some possible future directions towards socially aware LLMs, reflecting on the feasibility of interactional data selection, person-centric inductive biases, and interaction-based language learning.Our findings suggest that only increasing the scale of LLMs is likely not the most effective way to create socially aware AI systems, challenging a prevalent narrative in AI research (Narang and Chowdhery, 2022).</p>
<p>Theory of Mind &amp; Large LMs</p>
<p>Why do LLMs need Theory of Mind?Social intelligence, Theory of Mind, and commonsense reasoning have been a longstanding but elusive goal of artificial intelligence for decades (Gunning, 2018;Choi, 2022).These reasoning abilities are becoming increasingly necessary as AI assistants are used in situations that require social intelligence and Theory of Mind in order to operate effectively (Wang et al., 2007;Dhelim et al., 2021;Langley et al., 2022).For example, new technologies are emerging where AI is used to interact and adapt to users (Bickmore and Picard, 2005;Jaques, 2019), e.g., voice assistants, and tutoring systems; or where AI helps enhance communication between multiple users, e.g., email autocomplete (Chen et al., 2019), AI-assisted counseling (Kearns et al., 2020;Allen, 2020;Sharma et al., 2021), or facilitated discussion (Rosé et al., 2014).</p>
<p>As we move beyond just asking single-turn questions to social and interactive AI assistants, higherorder reasoning becomes necessary (McDonald and Pearson, 2019).For example, AI systems should be capable of more nuanced understanding, such as ensuring an alarm is on if someone has a job interview the next morning (Dhelim et al., 2021), knowing to call for help when an elderly person falls (Pollack, 2005), inferring personality and intentions in dialogues (Mairesse et al., 2007;Wang et al., 2019), reasoning about public commitments (Asher and Lascarides, 2013), predicting emotional and affective states (Litman and Forbes-Riley, 2004;Jaques et al., 2020), and incorporating empathy, interlocutor perspective, and social intelligence (Kearns et al., 2020;Sharma et al., 2021).</p>
<p>What is Theory of Mind?Theory of Mind (TOM) describes the ability that we, as humans, have to ascribe and infer the mental states of others, and to predict which likely actions they are going to take (Apperly, 2010). 1 This ability is closely related to (interpersonal) social intelligence (Ganaie and Mudasir, 2015), which allows us to navigate and understand social situations ranging from simple everyday interactions to complex negotiations (Gardner et al., 1995).Interestingly, the development of Theory of Mind and language seem to happen around similar ages in children (Sperber and Wilson, 1986;Wellman, 1992;Miller, 2006;Tauzin and Gergely, 2018). 2 Theories of the pragmatics of language and communication can frame our understanding of this link (Rubio-Fernandez, 2021), positing that one needs to reason about an interlocutor's mental state (TOM) to effectively communicate and understand language (Grice, 1975;Fernández, 2013;Goodman and Frank, 2016;Enrici et al., 2019). 31 While Theory of Mind is well developed in most adults (Ganaie and Mudasir, 2015), reasoning and inference capabilities can be influenced by age, culture, neurodiversity, or developmental disorders (Korkmaz, 2011). 2 The direction of the TOM-language association is still debated (de Villiers, 2007).Some researchers believe language development enables TOM-like abilities (Pyers and Senghas, 2009;Rubio-Fernandez, 2021).On the other hand, some argue that language develops after TOM since preverbal infants already could possess some level of TOM-like abilities (Onishi and Baillargeon, 2005;Southgate and Vernetti, 2014;Poulin-Dubois and Yott, 2018).</p>
<p>3 Most cognitive studies on this subject focus on the English language, which is not representative of the wide variation of 3 SOCIALIQA: Do LLMs have Social Intelligence and Social Commonsense?</p>
<p>A crucial component of Theory-of-Mind is the ability to reason about the intents and reactions of participants of social interactions.To measure this, we use the dev.set of the SOCIALIQA QA benchmark (Sap et al., 2019b), which was designed to probe social and emotional intelligence in various everyday situations.This benchmark covers questions about nine social reasoning dimensions, drawn from the ATOMIC knowledge graph (Sap et al., 2019a).SOCIALIQA instances consist of a context, question, and three answer choices, written in English.Each question relates to a specific reasoning dimension from ATOMIC: six dimensions focus on the pre-and post-conditions of the agent or protagonist of the situation (e.g., needs, intents, reactions, next actions), and three dimensions focus on the post-conditions of other participants involved in the situation (reaction, next action, effect).In total, there are 1954 three-way QA tuples; see Tab. 1 for examples, and Tab. 3 in Appendix A for perdimension counts.</p>
<p>Probing LLMs with SOCIALIQA</p>
<p>To probe our language models, we use a k-shot language probing setup, following Brown et al. (2020).</p>
<p>We select the answer that has the highest likelihood under the language model conditioned on the context and question, as described in Appendix C.</p>
<p>To test the limits of what the models can do, we select k examples that have the same ATOMIC reasoning dimension as the question at hand, varying k language structures, and thus limits the cognitive conclusions one can draw about the link between language and Theory of Mind (Blasi et al., 2022).The people bullied Sasha all her life.But Sasha got revenge on the people.What will the people want to do next?Do whatever Sasha says Others Get even Flee from Sasha (g)</p>
<p>After everyone finished their food they were going to go to a party so Kai decided to finish his food first.What will others want to do next?</p>
<p>Eat their food quickly Others Throw their food away Go back for a second serving Plan a best friends outing with Sasha Others Plan a romantic evening with Sasha Go on a date with Valerie Table 1: Examples of SOCIALIQA questions, which person the questions focus on (Agent, Others), and the human gold answers ( ) and GPT-3-DAVINCI predictions ( ). from 0 to 35 in increments of 5. We use three GPT-3 model sizes: GPT-3-ADA (smallest), and GPT-3-CURIE and GPT-3-DAVINCI (two largest).</p>
<p>SOCIALIQA Results</p>
<p>Shown in Fig. 2, GPT-3 models perform substantially worse than humans (&gt;30% less) on SO-CIALIQA, 4 and also worse than models finetuned on the SOCIALIQA training set (&gt;20%; Lourie et al., 2021). 5Although it is not surprising that GPT-3-DAVINCI reaches higher accuracies than GPT-3-ADA and GPT-3-CURIE, the gains are small, which suggests that increasing model size might not be enough to reach human-level accuracy.These findings are in line with recent BIG-Bench results on SOCIALIQA with the BIG-G (128B parameters; Srivastava et al., 2022)  Focusing on GPT-3-DAVINCI, while increasing the number of examples k improves performance, the differences are marginal after k=10 examples (only 1% increase from 10 to 35 examples).This suggest that performance either plateaus or follows a logarithmic relationship with increasing number of conditioning examples.</p>
<p>Finally, we examine the differences in GPT-3-DAVINCI with respect to which participant is the focus.Shown in Fig. 3, we find that GPT-3-DAVINCI performs consistently better on agentcentric questions, compared to other-oriented questions.Shown in the example predictions in Tab. 1, GPT-3-DAVINCI often confuses which participant is being asked about.In example (e), after Aubrey babysat for Tracy, GPT-3-DAVINCI fails to predict that Tracy will likely want to "let Aubrey know they are appreciated," and instead mistakenly predicts that Tracy will want to "save up for vacation," which is what Aubrey would likely do.GPT-3- DAVINCI displays a similar participant confusion in example (f) in Tab. 1.</p>
<p>TOMI: Can LLMs Reason about Mental States and Realities?</p>
<p>Another key component of Theory of Mind is the ability to reason about mental states and realities of others, recognizing that they may be different than our own mental states.As a measure of this ability in humans, psychologists developed the Sally Ann false-belief test (Wimmer and Perner, 1983), in which two people (Sally and Ann) are together in a room with a ball, a basket, and a box, and while Sally is away, Ann moves the ball from the basket to the box.When asked where Sally will look for her ball, Theory of Mind allows us to infer that Sally will look in the basket (where she left the ball), instead of in the box (where the ball is, unbeknownst to Sally).</p>
<p>To measure the false-belief abilities of LLMs, we use the TOMI QA dataset of English Sally-Annlike stories and questions (Le et al., 2019).6TOMI stories were created using a stochastic rule-based algorithm that samples two participants, an object of interest, and a set of locations or containers, and weaves together a story that involves an object being moved (see Tab. 2).All questions have two possible answers: the original object location, and the final object location.</p>
<p>We investigate how LLMs answer the TOMI story-question pairs, distinguishing between questions about factual object locations (FACT) and questions about where participants think objects are located (i.e., their mental states; MIND).The FACT questions either ask about the object's original (FACT-MEM) or final (FACT-REAL) location.</p>
<p>The MIND questions cover first-order (e.g., "where will Abby look for the object?";MIND-1st) and second-order beliefs (e.g., "where does James think that Abby will look for the object?";MIND-2nd).</p>
<p>We further distinguish the MIND questions between true belief (TB) and false belief (FB), i.e., stories where a participant was present or absent when an object was moved, respectively.Importantly, answering the MIND questions requires Theory of Mind and reasoning about realities and mental states of participants-regardless of the true-or false-belief setting-whereas FACT questions do not require such TOM.There are a total of 1861 two-way QA pairs in our TOMI probe set, with 519 FACT and 1342 MIND questions (see Tab. 4 in Appendix B for more detailed counts).</p>
<p>Probing LLMs with TOMI</p>
<p>We use the k-shot probing setup to test this TOM component in LLMs, with k ∈ {2, 4, 8, 16, 24}.We select k examples of the same reasoning type (i.e., FACT-MEM, MIND-1st, etc.), ensuring a 50-50 split between true-and false-belief examples for the MIND questions.As before, we test GPT-3-ADA, GPT-3-CURIE, and GPT-3-DAVINCI.</p>
<p>TOMI Results</p>
<p>Shown in Fig. 4, our results indicate that GPT-3 models struggle substantially with the TOMI questions related to mental states (MIND), reaching 60% accuracy in the best setup.As expected, the best performance is reached with GPT-3-DAVINCI compared to smaller models which do not surpass 55% accuracy; however, as before, the gains from scaling up GPT-3 are very small.Similarly, increasing the number of few-shot examples beyond k = 4 does not substantially improve performance, corroborating findings on SOCIALIQA.</p>
<p>Further examining GPT-3-DAVINCI with respect to question types, we show that the model struggles substantially more with questions about mental states (55-60% for k &gt; 0) compared to factual questions (90-100% for k &gt; 0; Fig. 5; columns).Furthermore, the difference between performance on MIND-TB and MIND-FB questions shows an interesting pattern when conditioning on an increasing number of examples k (Fig. 5; lines): GPT-3-DAVINCI's MIND-TB accuracy first increases, peaks at k = 4, then decreases.This peak seems to be due to the model defaulting to the most recent object location (i.e., the correct MIND-TB answer), as illustrated in example (e) in Tab. 2. Apparent in Fig. 10 in Appendix B, this recency bias is a phenomenon that has been previously documented in LLMs (O'Connor and Andreas, 2021).In general, GPT-3-DAVINCI's comparably poor performance for MIND-TB and MIND-FB questions at k &gt; 8 suggests that it cannot properly answer questions about participants' mental states and realities.</p>
<p>Discussion: Towards NLP with Neural Theory of Mind</p>
<p>Most humans develop social intelligence and Theory of Mind naturally.However, in this work, we showed that these abilities do not emerge automatically in large-pretrained language models.These shortcomings contrast with the wealth of successes of LLMs at a variety of tasks, including tasks that potentially require social intelligence.For example, GPT-3 has been shown to generate stories with emotional arcs that are virtually indistinguishable from human-written stories (Clark et al., 2021).Additionally, recent work has used GPT-3 to generate social commonsense knowledge related to protagonists of situations (West et al., 2022).While those findings suggest some level of social and emotional intelligence in LLMs, our explorations highlight the limits of these abilities, and raise the open question: how can we create NLP systems with true social intelligence and Theory of Mind?</p>
<p>To begin answering this question, we first discuss the current LLMs training paradigm ( §5.1), drawing from theories of pragmatics to examine why these models are not learning social intelligence efficiently.Then, we outline some possible future directions to bias models towards Theory of Mind ( §5.2), through person-centric neural archi-tectures, data selection, and training objectives.</p>
<p>The Pragmatics of "Static" Text</p>
<p>To understand why LLMs are still struggling with social intelligence, we examine LLMs' training paradigm through the lens of pragmatics.As discussed in §2, pragmatics provides a connection between language development and Theory of Mind (Sperber and Wilson, 1986;Miller, 2006;Tauzin and Gergely, 2018): learning to communicate effectively with language requires reasoning about what our interlocutor knows or does not know (Grice, 1975;Fernández, 2013;Goodman and Frank, 2016;Enrici et al., 2019). 7ne major use of language by people is to communicate about relationships and personal experiences (Clark and Schaefer, 1989;Dunbar, 1993).This is fundamentally different from the training data of LLMs, which consists of language found in what we call static texts: documents that are written for a general audience and are relatively self-contained and topically focused (e.g., news articles, books, Wikipedia articles; Gao et al., 2020;Dodge et al., 2021).Such static text is typically written such that readers only require the language itself as input, which they then combine with their world knowledge and commonsense to understand its meaning (Graesser et al., 1994).</p>
<p>If AI systems are to learn social intelligence and Theory of Mind, we posit that static text has certain limitations, from a pragmatics lens, outlined below.</p>
<p>Reporting bias.Following Grice's maxim of quantity (Grice, 1975), static text often avoids redundancy by omitting content that is known by both the author and the reader (Clark and Brennan, 1991).Also known as reporting bias (Gordon and Van Durme, 2013;Lucy and Gauthier, 2017), this phenomenon likely limits LLMs' ability to learn social commonsense knowledge from static text.</p>
<p>Lack of communicative intent and alternatives.</p>
<p>A corollary to reporting bias, static text does not provide any direct access to communicative intent (why words were used) or to alternatives (which words were not used, and why).This reasoning about intents, alternatives, and their implications is highly predictive of the pragmatic inferences people draw about their interlocutors (Goodman and Frank, 2016) -for example, when someone answers Where does Taylor live? with Somewhere in the U.S., it implies that they likely do not know or do not want to share the exact location, since, if they did, they would have been more specific.This poses a likely limitation that LLMs only learn what words are used, but not which words were not used, and why.</p>
<p>Lack of communicative effects.Language is primarily learned (Wells and Bridges, 1981;Tomasello et al., 2005) and used (Clark, 1996) in collaborative and interactive settings (Clark and Schaefer, 1989), which allow interlocutors to give immediate feedback to each other on whether their language was understood (Clark and Krych, 2004) or should be adjusted (Krauss and Weinheimer, 1966), and observe the perlocutionary effects that their language has on their partners (Austin, 1975).</p>
<p>Since static text has no such feedback, LLMs learn from all texts, as if they were all equally understandable by readers.</p>
<p>Centering theory.At any given time, most text focuses on describing one protagonist and their relation to their surroundings, according to Centering Theory (Grosz et al., 1995).As such, main characters and their mental states are more likely to be described, whereas other participants might only be mentioned.Additionally, main characters or protagonists are more likely to be referred to with pronouns, whereas secondary characters with their names.</p>
<p>Thus, a model trained purely on static text might not learn to reason about social intelligence or mental states and realities of different characters of situations; they might not even inherently learn to resolve coreference for multiple characters (Sakaguchi et al., 2020).In fact, challenges of coreference resolution could explain why GPT-3 models struggle on SOCIALIQA which contains questions with pronouns, and centering theory and main character biases in static text could explain why models find non-protagonist questions more challenging.On the other hand, TOMI does not contain any pronouns, and thus requires social intelligence beyond coreference resolution.</p>
<p>Future directions towards LLMs with Theory of Mind</p>
<p>While there is no one best path towards LLMs with social intelligence and Theory of Mind, it seems likely that progress will require challenging the standard paradigm of training on static text with the language modeling objective.Based on our findings and the limitations we discussed, we reflect on some possible directions forward.</p>
<p>Beyond static text as training data?Perhaps the key is in the data: the knowledge contained in static text might be too limited for models to learn social intelligence, for reasons described in §5.1 Socially grounded text (containing elaborations of communicative intents, character mental states, speaker identities, etc.) could enable more efficient learning of Theory of Mind abilities (Bender and Koller, 2020;Bisk et al., 2020;Hovy and Yang, 2021), similar to how visual groundings can help with learning physical knowledge (Zhang et al., 2022a).Examples of such datasets include "Social Stories," which are devised to help individuals with autism improve their interpersonal skills (Gray, 1995), or the Story Commonsense (Rashkin et al., 2018) and GLUCOSE (Mostafazadeh et al., 2020) commonsense-annotated story datasets.Alternatively, perhaps interactional texts, such as dialogues and other datasets that were explicitly created to require reasoning about mental states, could help with neural Theory of Mind (Bara et al., 2021).Nevertheless, the scale of training datasets seems to be crucial for LLMs (Kaplan et al., 2020;Chowdhery et al., 2022), which poses a challenge: text datasets rich in social intelligence and interactions are not easily found naturally due to reporting biases, and they are costly to create (Rashkin et al., 2018;Mostafazadeh et al., 2020).Promising results on commonsense reasoning suggest a possible hybrid approach: LLMs could be jointly or sequentially trained on static text and commonsense knowledge bases or socially grounded or interactional text (Bosselut et al., 2019;Hwang et al., 2021), first trained on static text and then enhanced for commonsense knowledge via reinforcement learning (Zhou et al., 2021).</p>
<p>Person-centric neural inductive biases?While more socially grounded training data could help, LLMs might also learn social intelligence better if they are designed with person-centric inductive biases and training objectives.Hinting at this, prior work has shown that training entity-centric neural architectures on text with entity coreference information yields more entity-aware LLMs, both in recurrent (Henaff et al., 2017;Ji et al., 2017;Yang et al., 2017;Liu et al., 2019) and Transformerbased models (Févry et al., 2020;De Cao et al., 2020;Rosset et al., 2020;Zhang et al., 2022c).</p>
<p>However, Theory of Mind and social intelligence require much richer social grounding than coreference chains, which is challenging to obtain for supervised settings, especially at the scale that LLMs require.Thus, unsupervised approaches to adding inductive biases to models could be a promising solution.Future work could look to cognitive science and neuroscience research for possible directions (Langley et al., 2022), such as exploring LLMs' equivalents of human concept cells (i.e., sets of neurons that activate for important people or concepts; Bowers, 2017;Calvo Tapia et al., 2020).</p>
<p>Alternatively, examining the internal or latent representations of LLMs could point to future directions towards inductive biases for neural Theory of Mind.As an example, recent work has found evidence of latent representations of grounded semantics in models trained only on static text (Li et al., 2021), which can be tied to real-world grounding with a small amount of additional supervised training (Patel and Pavlick, 2022).Future work might similarly analyze deep learning models for representations of Theory of Mind, toward augmenting the models with structure or objectives that surface and strengthen these representations.</p>
<p>Interactive and experiential grounding?It is possible, nevertheless, that socially grounded data and person-centric inductive biases will not suffice.Some researchers have argued that language understanding could only emerge from interactions and experiences (Bender and Koller, 2020;Bisk et al., 2020).Likely, this applies to Theory of Mind and social intelligence as well, due to lack of communicative intents and alternatives in static text.Future work could explore approaches grounded more explicitly in interaction, intents, and alternatives, e.g., by explicitly predicting possible next steps and learning why predictions were wrong.In fact, promising research has shown that using an interactive learning or multi-agent communication paradigm can enable some Theory of Mind capabilities of models (Hawkins et al., 2019;Lazaridou et al., 2020;Zhu et al., 2021;Wang et al., 2022).</p>
<p>However, there are limits to the types of Theory of Mind that can be learned from interactive simulations, which are often task-specific (e.g., describing objects in an image; Lazaridou et al., 2020;Steinert-Threlkeld et al., 2022).Furthermore, models that were trained in interactive simulation settings often struggle to generalize beyond the simulation environment (Ludwin-Peery et al., 2021;Mu and Goodman, 2021).Based on promising results by Lazaridou et al. (2020); Zhu et al. (2021), future work might create generalizable LLMs with neural Theory of Mind through hybrid approaches that combine pretraining with interactive learning: updating models trained on static text using supervision either from humans (Stiennon et al., 2020;Ouyang et al., 2022;Scheurer et al., 2022) or from proxies for human behavior or social environments (Ammanabrolu et al., 2022a,b) based on broad coverage LLMs (Perez et al., 2022).</p>
<p>Probing and evaluating TOM While neural Theory of Mind and social intelligence may remain an elusive goal for some time, developing measures of those abilities in systems can be done in tandem.We encourage further research in developing benchmarks that measure specific social abilities in LLMs (e.g., Sap et al., 2019b;Zadeh et al., 2019), especially those that minimize annotation artifacts and spurious correlations (Schwartz et al., 2017;Gururangan et al., 2018;Le et al., 2019).Additionally, we encourage further investigations into probing the latent knowledge within LLMs (Tenney et al., 2019;Li et al., 2021) or examining how LLMs handle entities and people (Onoe et al., 2022;Schuster and Linzen, 2022), which could shed light onto better data choices and inductive biases towards neural Theory of Mind and social intelligence.</p>
<p>Conclusion</p>
<p>We explore the open question of whether and how much modern large-scale language models (LLMs) can reason about social intelligence and Theory of Mind.Our results show that out-of-the-box LLMs struggle substantially with these abilities, which we argue are necessary but not sufficient aspects of Theory of Mind.Specifically, GPT-3's social intelligence as measured by SOCIALIQA lags behind humans (&gt;30%), and the model struggles to answer TOMI questions about mental states (55-60%) compared to factual questions (90-100%).In light of these shortcomings, we critically examine the large language model pretraining paradigm from a pragmatics-based perspective, and discuss possible directions towards enabling true social intelligence in NLP systems.</p>
<p>We make our preprocessed datasets available at http://maartensap.com/neuralToM.</p>
<p>Limitations</p>
<p>Our work focuses on investigating the Theory of Mind abilities in large pretrained language models, but we focus on accessing GPT-3 (Brown et al., 2020) through an API, since we do not have access to some of the larger models out there (PaLM;Chowdhery et al., 2022) nor do we have the computational resources to run an open-source version of GPT-3 (OPT; Zhang et al., 2022b).We hypothesize that results would not be drastically different with such models, based on the low accuracy displayed on SOCIALIQA in the recently released BIG-Bench experiments (Srivastava et al., 2022).Nevertheless, we hope developers of larger LLMs will investigate these TOM abilities to confirm or refute our findings.</p>
<p>We measure the ability to answer questions about people's mental states using TOMI, which is an automatically constructed corpus of stories involving people, objects, and locations.The automatic nature of the creation process could induce biases and artifacts, such as objects being in locations that are plausible but not typical (e.g., bananas in a closet), which could influence model's ability to answer questions properly.Based on the near-perfect accuracy on the factual questions, however, this may not be a significant issue.Future work should investigate more naturalistic settings to probe this ability in LLMs.</p>
<p>A potential limitation of our work is that models could latch onto surface patterns and spurious correlations in our two datasets.For example, theoretically, a model prompted with many TOMI examples may be able to reverse-engineer the data creation algorithm to find the solution to each question.However, this would be a bigger limitation if our claims were that LLMs do have social intelligence and Theory of Mind; instead, given that our results show low performance on these tasks even though they are potentially easier due to correlational patterns, this would indicate that LLMs have potentially even less reasoning abilities.</p>
<p>Additionally, while we operationalize our measure of social intelligence and Theory of Mind through two specific tasks, SOCIALIQA and TOMI, these abilities are much broader.As noted earlier, we view these benchmarks as necessary but not sufficient conditions for LLMs to have TOM; solving the benchmarks does not imply that LLMs have TOM, but LLMs with TOM should be able to solve them.We hope that future research will further investigate other aspects of Theory of Mind abilities in large pretrained LMs, drawing on social science research.For example, future work could make use of the "unexpected content" task (Gopnik and Astington, 1988) or the "George Washington University Social Intelligence Test" (Hunt, 1928) to measure the social intelligence of LLMs.</p>
<p>Finally, the focus on English language LLMs and benchmarks for Theory of Mind is another limitation of our work.Echoing recent cognitive science work that argues the need for non-English cognitive science investigations (Blasi et al., 2022).Specifically, false-belief abilities are greatly influenced by language structure and grammar (Boeg Thomsen et al., 2021;Zhang and Zhou, 2022).</p>
<p>Broader Sociotechnical Implications</p>
<p>AI systems are part of a broader sociotechnical system that also involves individual motivations and societal norms (Johnson and Verdicchio, 2017).As such, per a contextualist view of AI (instead of utopian or dystopian; Barbour, 1992), we envision AI systems with social intelligence and Theory of Mind being used in ways that enhance human's lives, autonomy, and agency (Chan, 2022).In parallel, we strongly support the development and research of policy and regulation, to prevent misuses of AI with social intelligence (Wischmeyer and Rademacher, 2020;Crawford, 2021;Reich et al., 2021).LLMs's reasoning abilities along each of these dimensions in further detail.</p>
<p>BIG-Bench and PaLM results on SOCIALIQA.</p>
<p>To further corroborate that LLMs struggle with SOCIALIQA, we show the performance of the nonpublicly available BIG-G (Srivastava et al., 2022) and PaLM (Chowdhery et al., 2022) LLMs, along with the GPT-3 models, in Fig. 7.Both models are proprietary LLMs developed and tested on the 200+ datasets in BIG-Bench by Google / DeepMind.While they are not discussed in the main BIG-Bench paper, the SOCIALIQA results for few-shot settings up to k=3 for BIG-G and k=5 for PaLM can be found on the BIG-Bench github website (accessed on 2022-11-10).Plotted in Fig. 7, both the BIG-G and PaLM LLMs lag behind humans with 45% and 73% peak accuracy, respectively.</p>
<p>B TOMI Details</p>
<p>B.1 Data Preprocessing</p>
<p>We generated TOMI stories using the github repository provided by Le et al. (2019).The code generated 5994 training and 5994 dev.stories.From those, we removed the story-question pairs which wrongly answered TOM-requiring questions from an omniscient perspective (i.e., answered MIND-FB questions from an omniscient perspective instead of the perspective of the character) which we noticed upon manual data inspection.9After this filtering, 5190 training and 5170 dev.stories remained.</p>
<p>For the final TOMI dev.set, we used stratified sampling to obtain similar numbers of storyquestion pairs for all types (FACT-REAL, FACT-MEM, MIND-1st-FB, MIND-1st-TB, MIND-2nd-FB and MIND-2nd-TB).The exact counts are   shown in Tab. 4. We release our final preprocessed TOMI dev.dataset at http://maartensap.com/ neuralToM/ToMi-finalNeuralTOM.csv</p>
<p>B.2 Further TOMI results</p>
<p>Shown in Fig. 8-10, we provide additional results to supplement those in §4.2.</p>
<p>Performance by model size, number of examples, and MIND versus FACT.In Fig. 8, we show the different accuracies that GPT-3 models of various sizes, prompted with various number of examples, for TOMI MIND and FACT questions.This plot shows the same accuracies as Fig. 4, with the addition of the FACT accuracies.These results show that in the few-shot prompting setup, GPT-3-CURIE and GPT-3-DAVINCI can achieve near perfect performance on factual questions about object locations (FACT), but struggle substantially more on questions related to mental states (MIND).Surprisingly, GPT-3-ADA struggles with both factual and mental state questions, possibly due to its smaller size.Performance by question order.In Fig. 9, we break the GPT-3-DAVINCI performance down by TOM order (i.e., MIND-1st, MIND-2nd).Results show that with a number of examples between 2 and 16, GPT-3-DAVINCI performs better on MIND-1st questions (e.g., "Where will Sally look for the ball?") and struggles more with MIND-2nd questions (e.g., "Where does Ann think that Sally will look for the ball?").This difference is somewhat diminished but still present for k=24 few-shot examples.These results somewhat mirror how humans struggle with increasingly higher-order TOM questions (Valle et al., 2015).Recency bias in predictions.We further examine the results from §4.2, looking at GPT-3-DAVINCI's rate of predicting the location where the object was moved to (i.e., FACT-REAL).Shown in Fig. 10, GPT-3-DAVINCI accurately learns to almost always predict the last object location for FACT-FACT-REAL questions, and almost never for FACT-FACT-MEM locations.Interestingly, the rates of selecting the last object location for MIND questions follows a concave pattern.This helps shed light onto the concave accuracy pattern seen in Fig. 5 for MIND-TB (and convex pattern for MIND-FB).Likely, in the few-shot setting with 2 &lt; k &lt; 8, GPT-3-DAVINCI defaults to the most recently mentioned object location due to recency bias, which has been previously documented in LLMs (O'Connor and Andreas, 2021).</p>
<p>C GPT-3 Access and Probing Details</p>
<p>To probe our language models, we use a k-shot language probing setup, following Brown et al. (2020).Specifically, we concatenate the context (c) and question (q) together with proper punctuation, and assign the model prediction to the answer (a i , i ∈ 1, 2, 3) with the highest conditional likelihood under the language model: arg max i p LM (a i | c, q, C k ) where C k denotes the k training examples, for which we provide the context, question, and correct answer concatenated.Note that we explored various probing setups and formats, such as QAoriented formats and normalizing by marginal likelihood of each answer p LM (a) (as also explored in Brown et al., 2020), but found very little difference in performance.We access GPT-3 through the OpenAI API.</p>
<p>Figure 2 :
2
Figure 2: Accuracy on the SOCIALIQA dev.set, broken down by LLM model type and size, as well as number of few-shot examples (k).</p>
<p>Figure 3 :
3
Figure 3: Comparing the accuracy of GPT-3-DAVINCI (35-shot) on SOCIALIQA when the reasoning is about the main agent of the situation versus others.</p>
<p>baby for 9 months and then gave birth to addison.What will happen to Tracy?Throw her baby at the wall Agent Cry Take care of her baby (d) Kai gave Ash some bread so they could make a sandwich.How would Kai feel afterwards?Glad they helped Agent Good they get something to eat Appreciative (e) Aubrey was making extra money by babysitting Tracey's kids for the summer.What will Tracy want to do next?Save up for a vacation Others Let Aubrey know that they are appreciated Pay off her college tuition (f)</p>
<p>(h) Aubrey fed Tracy's kids lunch today when Tracy had to go to work.What will happen to Aubrey?Be grateful Agent Get paid by Tracy Get yelled at by Tracy (i) Sasha was the most popular girl in school when she accepted Jordan's invitation to go on a date.What will Jordan want to do next?</p>
<p>and PaLM (353B parameters; Chowdhery et al., 2022) LLMs, which lag behind humans with 45% and 73% accuracy, respectively (see Fig. 7 in Appendix A.2).</p>
<p>Figure 4 :
4
Figure 4: Accuracy on the TOMI dev.set MIND questions of varying sizes of GPT-3, and with varying number of examples (k).</p>
<p>Figure 5 :
5
Figure 5: Accuracy of GPT-3-DAVINCI by number of examples (k), by reasoning type (FACT vs. MIND; MIND-TB vs. MIND-FB).</p>
<p>study.Noah entered the study.The dress is in the treasure chest.Noah exited the study.Hannah entered the garden.Sophia moved the dress to the box.Where is the dress really?box treasure chest (b) M-1-FB Noah entered the garden.Nathan entered the garden.Evelyn likes the pumpkin.The banana is in the basket.Nathan exited the garden.Noah moved the banana to the suitcase.Lily entered the patio.Aiden is in the patio.Mila entered the patio.Mila hates the radish.The coat is in the box.Aiden moved the coat to the crate.Mila exited the patio.Elizabeth entered the cellar.Carter entered the cellar.The slippers is in the crate.Elizabeth moved the slippers to the container.Carter exited the cellar.Evelyn entered the living room.Jackson entered the playroom.James entered the playroom.The beans are in the treasure chest.James exited the playroom.Jackson moved the beans to the pantry.Jackson exited the playroom.James entered the living room.Isla likes the potato.Ella entered the laundry.Oliver entered the laundry.The slippers are in the box.Ella exited the laundry.Oliver moved the slippers to the basket.Isla entered the office.Where does Ella think that Oliver searches for the slippers?basket box Table 2: Example stories in the TOMI dev.dataset, with GPT-3-DAVINCI predictions (with k=16 examples) and gold answers."Type" denotes reasoning type, M-1 and M-2 denote MIND-1st and MIND-2nd, resp.</p>
<p>Figure 6 :
6
Figure 6: Comparing the accuracy of GPT-3-DAVINCI (35-shot) when over all nine reasoning dimensions.</p>
<p>Figure 7 :
7
Figure 7: Expanded version of Fig. 2, depicting the accuracy on the SOCIALIQA dev.set, broken down by LLM model type and size, as well as number of few-shot examples (k).Here, we also include the accuracy results of the PaLM (Chowdhery et al., 2022) and BIG-G (Srivastava et al., 2022) LLMs, taken from the BIG-Bench github repository on 2022-11-10.</p>
<p>Figure 8 :
8
Figure 8: Examining the accuracy of GPT-3 of different sizes with different number of few-shot examples (k) on TOMI-MIND vs. TOMI-FACT questions.</p>
<p>Figure 9 :
9
Figure 9: Comparing the accuracy of GPT-3-DAVINCI by the question reasoning type, specifically FACT vs. MIND-1st vs. MIND-2nd.</p>
<p>Figure 10 :
10
Figure 10: We plot the proportion of examples for which GPT-3-DAVINCI selects the last object location (i.e., in "reality").</p>
<p>(Chowdhery et al., 2022)y on the SOCIALIQA dev.set, broken down by LLM model type and size, as well as number of few-shot examples (k).Here, we also include the accuracy results of the PaLM(Chowdhery et al., 2022)and BIG-G(Srivastava et al., 2022)LLMs, taken from the BIG-Bench github repository on 2022-11-10.
FACT FACT-MEM FACT-REAL MIND MIND-TB MIND-1st-TB MIND-2nd-TB MIND-FB MIND-1st-FB MIND-2nd-FB519 278 241 1342 778 389 389 564 231 333total1861</p>
<p>Table 4 :
4
TOMI dev.set statistics, broken down by question reasoning type.</p>
<p>We find similar results when using INSTRUCTGPT(Ouyang et al., 2022) instead of GPT-3-DAVINCI.
Lourie et al. (2021) achieves 83% on the test set, as shown on the AI2 SOCIALIQA leaderboard.
TOMI is a more challenging version of the rule-based datasets byNematzadeh et al. (2018) andGrant et al. (2017), as it contains randomly inserted distractor actions that prevent trivial reverse engineering.
Note here that, in contrast to other work(Bender and Koller, 2020;Bisk et al., 2020), we do not focus on whether LLMs "understand" language, instead we examine whether LLMs can answer questions about the emotions and mental states of participants of situations.
http://maartensap.com/social-iqa/data/ socialIQa_v1.4_withDims.tgz
We do not know why these datapoints were generated.
AcknowledgementsWe would like to thank Jack Hessel, Rowan Zellers, Jena D. Hwang, Prithviraj Ammanabrolu for their feedback on preliminary versions of this work, and Anna Jafarpour and Noah Goodman for fruitful cognitive science discussions about the research.We also thank the anonymous reviewers for their thoughtful comments.This research was supported by the Allen Institute for AI and the DARPA MCS program through NIWC Pacific (N66001-19-2-4031).A SOCIALIQA DetailsA.1 Data PreprocessingWe downloaded the SOCIALIQA training and dev.datasets from the publicly available SOCIALIQA website. 8This version of the SOCIALIQA dataset contains the original ATOMIC dimensions that workers were prompted with to create a question, as well as the correspondence between questions and which character they focus on (agent or other).To ensure consistency, for each context, question, and answer, we normalize the casing to start with a capital letter if the text does not already.A.2 Further SOCIALIQA resultsIn addition to results discussed in §3.2, we report further SOCIALIQA results here.SOCIALIQA broken down by reasoning dimension.We break down the best performing GPT-3-DAVINCI (35-shot) setup by reasoning dimension.Shown in Fig.6, we find that GPT-3-DAVINCI struggles most with questions related to what people needed to do before a situation could take place (Need).Conversely, questions related to a situation's agent's intent (Intent) and the effect of the situation on the agent (Effect) are seemingly easier for GPT-3-DAVINCI.Future work should explore
Artificial intelligence and the future of psychiatry. Summer Allen, 10.1109/MPULS.2020.2993657IEEE pulse. 1132020</p>
<p>Situated dialogue learning through procedural environment generation. Prithviraj Ammanabrolu, Renee Jia, Mark Riedl, 10.18653/v1/2022.acl-long.557Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022a1</p>
<p>Aligning to social norms and values in interactive narratives. Prithviraj Ammanabrolu, Liwei Jiang, Maarten Sap, Hannaneh Hajishirzi, Yejin Choi, 10.18653/v1/2022.naacl-main.439Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022b</p>
<p>Mindreaders: the cognitive basis of" theory of mind. Ian Apperly, 2010Psychology Press</p>
<p>Strategic conversation. Nicholas Asher, Alex Lascarides, 2013Semantics and Pragmatics6</p>
<p>John Langshaw, Austin , How to Do Things with Words. Clarendon Press1975</p>
<p>MindCraft: Theory of mind modeling for situated dialogue in collaborative tasks. Cristian-Paul Bara, Ch- Sky, Joyce Wang, Chai, EMNLP. 2021</p>
<p>Ethics in an age of technology. Ian G Barbour, 1992The Gifford lectures</p>
<p>Does the autistic child have a "theory of mind. Simon Baron-Cohen, Alan M Leslie, Uta Frith, Cognition. 2111985</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. M Emily, Alexander Bender, Koller, Proc. of ACL. of ACL2020</p>
<p>Establishing and maintaining long-term humancomputer relationships. Timothy W Bickmore, Rosalind W Picard, ACM Transactions on Computer-Human Interaction. 1222005</p>
<p>Experience grounds language. Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian, EMNLP. Online. Association for Computational Linguistics2020</p>
<p>Overreliance on english hinders cognitive science. Joseph Damián E Blasi, Evangelia Henrich, David Adamou, Asifa Kemmerer, Majid, Trends Cogn. Sci. 2022</p>
<p>Do complement clauses really support false-belief reasoning? a longitudinal study with english-speaking 2-to 3-yearolds. Ditte Boeg Thomsen, Anna Theakston, Birsu Kandemirci, Silke Brandt, Dev. Psychol. 5782021</p>
<p>Gpt-3: What's it good for?. Robert Dale, Natural Language Engineering. 2712021</p>
<p>Autoregressive entity retrieval. Nicola De Cao, Gautier Izacard, Sebastian Riedel, Fabio Petroni, 2020In ICLR</p>
<p>The interface of language and theory of mind. Jill De, Villiers , 10.1016/j.lingua.2006.11.006Revue internationale de linguistique generale. 117112007Lingua</p>
<p>IoT-Enabled social relationships meet artificial social intelligence. Sahraoui Dhelim, Huansheng Ning, Fadi Farha, Liming Chen, Luigi Atzori, Mahmoud Daneshmand, 10.1109/JIOT.2021.3081556IEEE Internet of Things Journal. 8242021</p>
<p>Documenting large webtext corpora: A case study on the colossal clean crawled corpus. Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner, EMNLP. 2021</p>
<p>Coevolution of neocortical size, group size and language in humans. R I M Dunbar, The Behavioral and brain sciences. 1641993</p>
<p>Can gpt-3 pass a writer's turing test. Katherine Elkins, Jon Chun, Journal of Cultural Analytics. 11172122020</p>
<p>Theory of mind, pragmatics and the brain: Converging evidence for the role of intention processing as a core feature ofhuman communication. Ivan Enrici, Bruno G Bara, Mauro Adenzato, 10.1075/pc.19010.enrPragmatics &amp; Cognition. 2612019</p>
<p>Mindful storytellers: Emerging pragmatics and theory of mind development. Camila Fernández, 10.1177/0142723711422633First language. 3312013</p>
<p>Entities as experts: Sparse memory access with entity supervision. Thibault Févry, Baldini Livio, Nicholas Soares, Eunsol Fitzgerald, Tom Choi, Kwiatkowski, EMNLP. 2020</p>
<p>A Study of Social Intelligence &amp; Academic Achievement of College Students of District Srinagar. M Y Ganaie, Hafiz Mudasir, J&amp;K, India. Journal of American Science. 1132015</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.00027The pile: An 800gb dataset of diverse text for language modeling. 2020arXiv preprint</p>
<p>How Are Kids SMART?: Multiple Intelligences in the Classroom. National Professional Resources. Howard Gardner, Robert M Hanson, Steve Hamilton, 1995Incorporated</p>
<p>Pragmatic language interpretation as probabilistic inference. D Noah, Michael C Goodman, Frank, Trends in Cognitive Science. 20112016</p>
<p>Children's understanding of representational change and its relation to the understanding of false belief and the appearance-reality distinction. Child development. Alison Gopnik, Janet W Astington, 1988</p>
<p>Reporting bias and knowledge acquisition. Jonathan Gordon, Benjamin Van Durme, 10.1145/2509558.2509563Proceedings of the 2013 Workshop on Automated Knowledge Base Construction, AKBC '13. the 2013 Workshop on Automated Knowledge Base Construction, AKBC '13New York, NY, USAACM2013</p>
<p>Constructing inferences during narrative text comprehension. M A C Graesser, Singer, Trabasso, Psychological review. 10131994</p>
<p>How can memory-augmented neural networks pass a false-belief task?. Erin Grant, Aida Nematzadeh, Thomas L Griffiths, CogSci. 2017</p>
<p>Teaching children with autism: Strategies to enhance communication and socialization. Carol A Gray, 1995Teaching children with autism to "read" social situations</p>
<p>Logic and conversation. Herbert P Grice, Speech acts. Brill1975</p>
<p>Centering: A framework for modeling the local coherence of discourse. Barbara J Grosz, Aravind K Joshi, Scott Weinstein, Computational Linguistics. 2121995</p>
<p>Machine common sense concept paper. David Gunning, 2018</p>
<p>Annotation artifacts in natural language inference data. Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, Noah A Smith, 10.18653/v1/N18-2017Proceedings of the 2018 Conference of the North American Chapter. Short Papers. the 2018 Conference of the North American ChapterNew Orleans, LouisianaAssociation for Computational Linguistics20182</p>
<p>Continual adaptation for efficient machine communication. Minae Robert D Hawkins, Dorsa Kwon, Noah D Sadigh, Goodman, CoNLL. 2019</p>
<p>Tracking the world state with recurrent entity networks. Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, Yann Lecun, ICLR. 2017</p>
<p>The importance of modeling social factors of language: Theory and practice. Dirk Hovy, Diyi Yang, NAACL. Association for Computational Linguistics2021</p>
<p>The measurement of social intelligence. Thelma Hunt, Journal of Applied Psychology. 1233171928</p>
<p>COMET-)ATOMIC2020: On symbolic and neural commonsense knowledge graphs. Jena D Hwang, Chandra Bhagavatula, Le Ronan, Jeff Bras, Keisuke Da, Antoine Sakaguchi, Yejin Bosselut, Choi, AAAI. 2021</p>
<p>Human-centric dialog training via offline reinforcement learning. Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, Rosalind Picard, 10.18653/v1/2020.emnlp-main.327Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Social and affective machine learning. Natasha M Jaques, 2019Massachusetts Institute of TechnologyPh.D. thesis</p>
<p>Dynamic entity representations in neural language models. Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin Choi, Noah A Smith, EMNLP. Copenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>. G Deborah, Mario Johnson, Verdicchio, 10.1007/s11023-017-9417-6Reframing AI discourse. Minds and Machines. 2742017</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>A Wizard-of-Oz interface and persona-based methodology for collecting health counseling dialog. Neha William R Kearns, Myra Kaura, Cuong Divina, Dong Vo, Teresa Si, Weichao Ward, Yuwen, 10.1145/3334480.33829022020</p>
<p>Revisiting the evaluation of theory of mind through question answering. Matthew Le, Y-Lan Boureau, Maximilian Nickel, 10.18653/v1/D19-1598Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Implicit representations of meaning in neural language models. Belinda Z Li, Maxwell Nye, Jacob Andreas, ACL. 2021</p>
<p>Predicting student emotions in computer-human tutoring dialogues. Diane J Litman, Kate Forbes-Riley, 10.3115/1218955.1219000Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04). the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)Barcelona, Spain2004</p>
<p>The referential reader: A recurrent entity network for anaphora resolution. Fei Liu, Luke Zettlemoyer, Jacob Eisenstein, ACL. Stroudsburg, PA, USAAssociation for Computational Linguistics2019</p>
<p>Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark. Nicholas Lourie, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, AAAI. 2021</p>
<p>Are distributional representations ready for the real world? evaluating word vectors for grounded perceptual meaning. Li Lucy, Jon Gauthier, RoboNLP@ACL. 2017</p>
<p>Limits on simulation approaches in intuitive physics. Ethan Ludwin-Peery, Neil R Bramley, Ernest Davis, Todd M Gureckis, 10.1016/j.cogpsych.2021.101396Cognitive psychology. 1271013962021</p>
<p>Using linguistic cues for the automatic recognition of personality in conversation and text. M A Mairesse, M Walker, R K R Mehl, Moore, Journal of Artificial Intelligence Research. 302007</p>
<p>Deep learning is hitting a wall. Gary Marcus, Nautilus. 2022</p>
<p>Cognitive bots and algorithmic humans: toward a shared understanding of social intelligence. R Kelsey, John M Mcdonald, Pearson, Current Opinion in Behavioral Sciences. 292019</p>
<p>Developmental relationships between language and theory of mind. Carol A Miller, 10.1044/1058-0360(2006/014)American Speech-Language-Hearing Association. 1522006American journal of speech-language pathology</p>
<p>Glucose: Generalized and contextualized story explanations. Nasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon, David Buchanan, Lauren Berkowitz, Or Biran, Jennifer Chu-Carroll, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Emergent communication of generalizations. Jesse Mu, Noah Goodman, 2021In NeurIPS</p>
<p>Pathways language model (PaLM): Scaling to 540 billion parameters for breakthrough performance. Sharan Narang, Aakanksha Chowdhery, 2022</p>
<p>Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, Thomas L Griffiths, arXiv:1808.09352Evaluating theory of mind in question answering. 2018arXiv preprint</p>
<p>What context features can transformer language models use?. O' Joe, Jacob Connor, Andreas, 10.18653/v1/2021.acl-long.70Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Do 15-month-old infants understand false beliefs?. H Kristine, Renée Onishi, Baillargeon, 10.1126/science.1107621Science. 30857192005</p>
<p>Entity cloze by date: What LMs know about unseen entities. Yasumasa Onoe, J Q Michael, Eunsol Zhang, Greg Choi, Durrett, Findings of NAACL. 2022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, arXiv:2203.021552022arXiv preprint</p>
<p>Mapping language models to grounded conceptual spaces. Roma Patel, Ellie Pavlick, International Conference on Learning Representations. 2022</p>
<p>Integrating social power into the decision-making of cognitive agents. Gonçalo Pereira, Rui Prada, Pedro A Santos, Artificial Intelligence. 2412016</p>
<p>. Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat Mcaleese, Geoffrey Irving, 2022Red teaming language models with language models</p>
<p>Intelligent technology for an aging population: The use of ai to assist elders with cognitive impairment. Martha E Pollack, AI magazine. 2622005</p>
<p>Probing the depth of infants' theory of mind: disunity in performance across paradigms. Diane Poulin, - Dubois, Jessica Yott, 10.1111/desc.12600Developmental science. 214e126002018</p>
<p>Does the chimpanzee have a theory of mind?. David Premack, Guy Woodruff, The Behavioral and brain sciences. 141978</p>
<p>Language promotes false-belief understanding: evidence from learners of a new sign language. Jennie E Pyers, Ann Senghas, Psychol. Sci. 2072009</p>
<p>Modeling naive psychology of characters in simple commonsense stories. Antoine Hannah Rashkin, Maarten Bosselut, Kevin Sap, Yejin Knight, Choi, ACL. 2018</p>
<p>Rob Reich, Mehran Sahami, Jeremy M Weinstein, System error: Where big tech went wrong and how we can reboot. Hodder &amp; Stoughton2021</p>
<p>Social factors that contribute to attrition in MOOCs. Carolyn Penstein Rosé, Ryan Carlson, Diyi Yang, Miaomiao Wen, Lauren Resnick, Pam Goldman, Jennifer Sherer, Proceedings of the first ACM conference on Learning @ Scale, L@S '14. the first ACM conference on Learning @ Scale, L@S '14New York, NY, USAAssociation for Computing Machinery2014</p>
<p>Knowledge-Aware language model pretraining. Corby Rosset, Chenyan Xiong, Minh Phan, Xia Song, Paul Bennett, Saurabh Tiwary, 2020In NeurIPS</p>
<p>Pragmatic markers: the missing link between language and theory of mind. Paula Rubio-Fernandez, Synthese. 19912021</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2020</p>
<p>Atomic: An atlas of machine commonsense for if-then reasoning. Maarten Sap, Le Ronan, Emily Bras, Chandra Allaway, Nicholas Bhagavatula, Hannah Lourie, Brendan Rashkin, Noah A Roof, Yejin Smith, Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2019a</p>
<p>Social iqa: Commonsense reasoning about social interactions. Maarten Sap, Derek Hannah Rashkin, Ronan Chen, Yejin Lebras, Choi, EMNLP. 2019b</p>
<p>Training language models with language feedback. Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez, ACL Workshop on Learning with Natural Language Supervision. 2022. 2022</p>
<p>When a sentence does not introduce a discourse entity, transformer-based models still sometimes refer to it. Sebastian Schuster, Tal Linzen, NAACL. 2022</p>
<p>The effect of different writing tasks on linguistic style: A case study of the roc story cloze task. Roy Schwartz, Maarten Sap, Ioannis Konstas, Li Zilles, Yejin Choi, Noah A Smith, CoNLL. 2017</p>
<p>Towards facilitating empathic conversations in online mental health support: A reinforcement learning approach. Ashish Sharma, Inna W Lin, Adam S Miner, David C Atkins, Tim Althoff, Proceedings of the Web Conference 2021. the Web Conference 20212021</p>
<p>Belief-based action prediction in preverbal infants. Victoria Southgate, Angelina Vernetti, 10.1016/j.cognition.2013.08.008Cognition. 13012014</p>
<p>Relevance: Communication and cognition. Dan Sperber, Deirdre Wilson, Citeseer. 1421986</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek B Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Annasaheb Rahane, Anantharaman S Iyer, Anders Johan Andreassen, Andrea Santilli, Andreas Stuhlmuller, Andrew M Dai, Andrew D La, Andrew Kyle Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakacs, Bridget R Roberts, Bao Sheng Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan Ozyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Stephen Howald, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, C Esar, Ferri Ram'irez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Tatiana Ramirez, Clara E Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Daniel H Garrette, Dan Hendrycks, Dan Kilman, Daniel Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Danny Gonz'alez, Danqi Hernandez, Daphne Chen, Dar Ippolito, David Gilboa, D Dohan, David Drakard, Debajyoti Jurgens, Deep Datta, Denis Ganguli, Denis Emelin, Deniz Kleyko, Derek Yuret, Derek Chen, Dieuwke Tam, Diganta Hupkes, Dilyar Misra, Dimitri Coelho Buzan, Diyi Mollo, Dong-Ho Yang, Ekaterina Lee, Ekin Shutova, Elad Dogus Cubuk, Eleanor Segal, Elizabeth Hagerman, Elizabeth P Barnes, Ellie Donoway, Emanuele Pavlick, Emma Fc Rodolà, Eric Lam, Eric Chu, Erkut Tang, Ernie Erdem, Ethan A Chang, Ethan Chi, Ethan Dyer, Ethan Jerzak, Eunice Engefu Kim, Evgenii Manyasi, Fan Zheltonozhskii, Fatemeh Xia, Fernando Siar, Francesca Mart'inez-Plumed, François Happ'e, Frieda Chollet, Gaurav Rong, Genta Mishra, Gerard Indra Winata, Germán De Melo, Giambattista Kruszewski, Giorgio Parascandolo, Gloria Mariani, Gonzalo Wang, Gregor Jaimovitch-L'opez, Guy Betz, Hana Gur-Ari, Galijasevic, Sol Han, Hannah Kim, Hanna Rashkin, Harsh Hajishirzi, Hayden Mehta, Henry Bogar, Hinrich Shevlin, Hiromu Schütze, Hongming Yakura, Hubert Zhang, Ian Wong, Aik-Soon, Isaac Ng, Jaap Noble, Jack Jumelet, John Geissinger, Jacob Kernion, Jaehoon Hilton, Jaime Lee, J Fernández Fisac, James Brooker Simon, James Koppel, James Zheng, Jan Zou, Jana Koco'n, Jared Thompson, Jarema Kaplan, Jascha Radom, Jason Sohl-Dickstein, Jason Phang, Jason Wei, Jekaterina Yosinski, Jelle Novikova, Jenni Bosscher, Jeremy Marsh, Jeroen Kim, Jesse Taal, Jesujoba Engel, Jiacheng Oluwadara Alabi, Jiaming Xu, Jillian Song, Jane W Tang, John Waweru, John Burden, John U Miller, Jonathan Balis, Jorg Berant, Jos Frohberg, José Rozen, Joseph Hernández-Orallo, Joseph Boudeman, Joshua B Jones, Joshua S Tenenbaum, Joyce Rule, Kamil Chua, Karen Kanclerz, Karl Livescu, Karthik Krauth, Katerina Gopalakrishnan, Katja Ignatyeva, Markert, D Kaustubh, Kevin Dhole, Kevin Gimpel, Kory Wallace Ochieng' Omondi, Kristen Mathewson, Ksenia Chiafullo, Kumar Shkaruta, Kyle Shridhar, Kyle Mcdonell, Laria Richardson, Leo Reynolds, Li Gao, Liam Zhang, Lianhui Dugan, Lidia Qin, Louis-Philippe Contreras-Ochando, Luca Morency, Luca Moschella, Lucy Lam, Ludwig Noble, Luheng Schmidt, Luis He, Luke Oliveros Col'on, Lutfi Metz, Maarten Kerem Csenel, Maarten Bosma, Sap, Madotto Maartje Ter Hoeve, Maheen Andrea, Manaal Saleem Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maru, Marie Quintana, Mario Tolkiehn, Martha Giulianelli, Martin Lewis, Matthew Potthast, Matthias Leavitt, M Hagen, Medina Schubert, Melissa Baitemirova, Melvin Andrew Arnaud, Michael A Mcelrath, Michael Yee, Mi Cohen, Michael I Gu, Michael Ivanitskiy, Michael Starritt, Michal Strube, Michele Swkedrowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Monica Suzgun, Mohit Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Gheini, Nanyun Mukundvarma, Nathan Peng, Nayeon Chi, Neta Lee, Oliver Gur-Ari Krakover ; Nuan Wen, Omar Zhang, Omar Agha, Sam Elbaghdadi, Sam Shleifer, Samuel Wiseman, Sam Gruetter, Samuel S Bowman, Sanghyun Schoenholz, Sanjeev Han, Sarah A Kwatra, Sarik Rous, Sayan Ghazarian, Sean Ghosh, Sebastian Casey, Sebastian Bischoff, Sebastian Gehrmann, Sepideh Schuster, Shadi Sadeghi, Sharon Sameh Hamdan, Shashank Zhou, Sherry Srivastava, Shikhar Shi, Shima Singh, Asaadi, Shane Shixiang, Shubh Gu, Shubham Pachchigar, Shyam Toshniwal, Shyamolima Upadhyay, Siamak Debnath, Simon Shakeri, Simone Thormeyer, Siva Melzi, Kumar Reddy, Priscilla Sneha, Makini, Spencer Soo Hwan Lee, Sriharsha Bradley Torene, Stanislas Hatwar, Stefan Dehaene, Stefano Divic, Stella Rose Ermon, Stephanie C Biderman, Stephen Lin, Steven T Prasad, Stuart M Piantadosi, Summer Shieber, Svetlana Misherghi, Swaroop Kiritchenko, William Mishra, William Saunders, Zhang, Xiang Vossen, Ren, F Xiaoyu, Xinyi Tong, Xudong Wu, Yadollah Shen, Yair Yaghoobzadeh, Yang Lakretz, Yasaman Song, Bahri, Ji Ye, Yichi Choi, Yiding Yang, Yifu Hao, Chen ; Yuntao, Zachary Bai, Seid, Zhuoye Zhao Xinran, Zi Zhao, Zijie Jay Fu Wang, Zirui Wang, Wang, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi. Trishala Neeraj, Tushar Khot, Tyler O Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Nicholas Cameron, Nicholas S. Roberts, Nicholas Doiron, Nikita Nangia, Niklas Deckers, Niklas Muennighoff; Omer Levy, Owain Evans, Pablo Casares; Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq A. Ali, Tatsuo Hashimoto; Theo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, T. N. Kornev, Timothy Telleen-Lawton; Tobias Gerstenberg, Trenton ChangTitus TundunyNoah Constant, Noah Fiedel,Vikas Raunak, Vinay V. Ramasesh, Vinay Uday Prabhu. Vishakh Padmakumar, Vivek Srikumar, William Fedus,. Yonatan Belinkov, Yu Hou, Yu Hou,. and Ziyi Wu. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</p>
<p>Emergent communication finetuning (ec-ft) for pretrained language models. Shane Steinert-Threlkeld, Xuhui Zhou, Zeyu Liu, Downey, Emergent Communication Workshop at ICLR 2022. 2022</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Communicative mind-reading in preverbal infants. Tibor Tauzin, György Gergely, 10.1038/s41598-018-27804-4Scientific reports. 8195342018</p>
<p>BERT rediscovers the classical NLP pipeline. Ian Tenney, Dipanjan Das, Ellie Pavlick, ACL. 2019</p>
<p>Understanding and sharing intentions: the origins of cultural cognition. Michael Tomasello, Malinda Carpenter, Josep Call, Tanya Behne, Henrike Moll, Behavioral and Brain Sciences. 2852005</p>
<p>Theory of mind development in adolescence and early adulthood: The growing complexity of recursive thinking ability. Annalisa Valle, Davide Massaro, Ilaria Castelli, Antonella Marchetti, 10.5964/ejop.v11i1.829European journal of psychological assessment: official organ of the European Association of Psychological Assessment. 1112015</p>
<p>Social computing: From social informatics to social intelligence. Fei-Yue Wang, Kathleen M Carley, Daniel Zeng, Wenji Mao, 10.1109/MIS.2007.41IEEE intelligent systems. 2222007</p>
<p>Persuasion for good: Towards a personalized persuasive dialogue system for social good. Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, Zhou Yu, 10.18653/v1/P19-1566Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>ToM2C: Target-oriented multi-agent communication and cooperation with theory of mind. Yuanfei Wang, Fangwei Zhong, Jing Xu, Yizhou Wang, ICLR. arxiv.org2022</p>
<p>The child's theory of mind. The MIT Press series in learning, development, and conceptual change. Henry M Wellman, 1992358</p>
<p>Learning through interaction: volume 1: the study of language development. Gordon Wells, Allayne Bridges, 1981Cambridge University Press1</p>
<p>Symbolic knowledge distillation: from general language models to commonsense models. Peter West, Chandra Bhagavatula, Jack Hessel, Jena D Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi, NAACL. 2022</p>
<p>Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. Heinz Wimmer, Josef Perner, Cognition. 1311983</p>
<p>Regulating Artificial Intelligence. Thomas Wischmeyer, Timo Rademacher, 10.1007/978-3-030-32361-52020SpringerCham</p>
<p>Reference-Aware language models. Zichao Yang, Phil Blunsom, Chris Dyer, Wang Ling, EMNLP. 2017</p>
<p>Socialiq: A question answering benchmark for artificial social intelligence. Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, Louis-Philippe Morency, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Visual commonsense in pretrained unimodal and multimodal models. Chenyu Zhang, Benjamin Van Durme, Zhuowan Li, Elias Stengel-Eskin, 2022a. 2022</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022barXiv preprint</p>
<p>Linguistic cues facilitate children's understanding of beliefreporting sentences. Xiaowen Zhang, Peng Zhou, First Lang. 4212022</p>
<p>A unified Encoder-Decoder framework with entity memory. Zhihan Zhang, Wenhao Yu, Chenguang Zhu, Meng Jiang, EMNLP. 2022c</p>
<p>Pre-training text-to-text transformers for concept-centric common sense. Wangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Selvam, Seyeon Lee, Bill Yuchen Lin, Xiang Ren, ICLR, abs/2011.079562021</p>
<p>Few-shot language coordination by modeling theory of mind. Hao Zhu, Graham Neubig, Yonatan Bisk, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningPMLR2021139of Proceedings of Machine Learning Research</p>            </div>
        </div>

    </div>
</body>
</html>
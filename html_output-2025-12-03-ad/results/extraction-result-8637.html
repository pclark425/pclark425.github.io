<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8637 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8637</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8637</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-265150088</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.06736v3.pdf" target="_blank">Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is a pivotal component in the field of artificial intelligence. Proof planning, particularly in contexts requiring the validation of explanation accuracy, continues to present challenges. The recent advancement of large language models (LLMs) has led to significant progress in natural language proof planning, evolving from one-stage generators to more complex three-stage systems that include additional searchers or verifiers. While these assisted methods improve the quality of generated results, they also introduce increased search efforts and computational costs. Furthermore, the generative process itself remains underexplored. In this study, we propose a stepwise decoding approach augmented by contrastive learning to address two common errors encountered during the LLM generator's decoding process. We fine-tune the language model using both vanilla and enhanced hard negatives to mitigate these decoding errors. Empirical results demonstrate the effectiveness of our strategy. Additionally, our further analysis reveals that even larger LLMs still struggle to generate rigorous logical chains.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8637.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8637.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 Large (instruction-finetuned T5-Large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder transformer (T5-Large) instruction-finetuned with FLAN data, used as the primary generator in the paper and finetuned with the proposed contrastive stepwise decoding (ConDec).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder T5-Large variant instruction-finetuned on FLAN-style instructions; used as the generator and reasoner in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (EntailmentWriter tasks: Task 1/2/3)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Proof-generation / entailment-tree construction: given a hypothesis and a set of facts, produce an entailment tree (leaves, intermediate nodes, and steps) that proves the hypothesis; tasks vary by amount of distractors or full-corpus retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Finetuning with stepwise decoding plus contrastive learning (ConDec); hard negatives (vanilla substitution and enhanced negatives from a reasoner + checker); alternative training of MLE and contrastive loss.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>As the base generator (stepwise finetuned) it attains (Task 2, distractor): Leaves F1 90.7 (AllCorrect 58.8), Steps F1 49.2 (AllCorrect 36.2), Intermediates F1 69.6 (AllCorrect 36.8), Overall AllCorrect 33.5. With ConDec (contrastive + vanilla/enhanced hard negatives, ConDec⋆) performance improves (Task 2): Leaves F1 91.1 (AllCorrect 60.6), Steps F1 50.7 (AllCorrect 37.4), Intermediates F1 70.7 (AllCorrect 38.2), Overall AllCorrect 34.7. Similar gains reported on Task 1/3 in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ConDec-trained Flan-T5-Large outperforms single-shot finetuned baselines (EntailmentWriter) and achieves comparable or better performance than multi-stage methods (MetGen, NLProofs) on Task 2 and Task 3 while being more time-efficient (one-stage inference). Ablations show contrastive loss + hard negatives yields measurable gains over stepwise MLE-only training and over adding only vanilla negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Improvements concentrate on leaves and steps; intermediate-node (semantic conclusion) accuracy is harder to improve. Finetuning with hard negatives slightly degrades Task 1 (no-distractor) in some metrics. Dataset size is limited; Flan-T5-XL (3B) yields similar performance, indicating scale alone does not solve rigorous multi-step proof planning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Stepwise training plus contrastive loss with carefully constructed hard negatives helps the generator discriminate premises and avoid repetition/invalid entailment decoding errors, improving proof generation without expensive verifier/search stages; however, generating rigorous intermediate conclusions remains a core challenge requiring better coverage/curated data or other modeling advances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8637.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8637.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConDec</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive learning based stepwise Decoding (ConDec)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training/decoding technique that augments stepwise finetuning with a contrastive loss using hard negatives (vanilla substitutions and enhanced negatives from a trained reasoner filtered by a checker) to reduce decoding errors in proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ConDec (applied to Flan-T5-Large in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A method (not a standalone pretrained LM): stepwise finetuning of a seq2seq generator combined with a contrastive loss that pulls encoder and decoder latent representations of correct step pairs together and pushes away hard negative generations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>method applied to 0.8B and 3B models in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank / proof generation (Tasks 1/2/3)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as Flan-T5-Large entry — generate entailment trees (multi-step textual entailment chains) from hypothesis and supporting facts, including with distractors or full-corpus retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Stepwise decoding supervision + contrastive loss (Eq. 6–8) using (1) vanilla hard negatives (conclusion substitutions) and (2) enhanced hard negatives produced by a reasoner and filtered by a plausibility checker (Vera); alternative training of MLE and contrastive objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ConDec (vanilla hard negatives) and ConDec⋆ (vanilla + enhanced) improve Leaves/Steps metrics and overall proof accuracy over MLE-only stepwise training; e.g., Task 2 ConDec⋆ on Flan-T5-Large: Leaves F1 91.16, Steps F1 50.73, Intermediates F1 70.74, Overall AllCorrect 34.7 (see paper Table 3/5).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to single-shot finetuning (EntailmentWriter) ConDec yields higher F1 and AllCorrect on Task 2/3; compared to multi-stage systems (MetGen, NLProofs) it achieves comparable or better scores with lower inference cost. Ablations show contrastive loss + hard negatives > contrastive alone > MLE-only stepwise.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>ConDec improves structural aspects (leaves/steps) more than semantic intermediates; on Task 1 (no distractor) adding hard negatives can slightly harm intermediate prediction; generation quality still bounded by dataset coverage and by the reasoner/checker quality used to create enhanced negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Explicitly training the generator to distinguish semantically-plausible but incorrect outputs via contrastive learning reduces common decoding errors (repetition, invalid entailment) and can partially close the gap to multi-stage search/verification methods, while maintaining simpler inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8637.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8637.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 XL (instruction-finetuned T5-XL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger encoder-decoder T5 variant (3B) used in an ablation to test scale effects for stepwise finetuning and ConDec-style training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder transformer (T5-XL) instruction-finetuned on FLAN-style data; evaluated with stepwise finetuning and contrastive/hard-negative training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (Task 2 reported in ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step entailment tree generation with distractors (Task 2).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Stepwise finetuning; compared with Flan-T5-Large baseline and ConDec variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in ablation (Task 2): Leaves F1 90.95 (AllCorrect 57.1), Steps F1 50.2 (AllCorrect 36.5), Intermediates F1 68.8 (AllCorrect 35.9), Overall AllCorrect 33.8 — broadly similar to Flan-T5-Large.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Increasing model size to 3B did not substantially improve proof-planning metrics over 0.8B Flan-T5-Large; stepwise ConDec training on the smaller model matches or exceeds performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Scale alone did not solve intermediate-node accuracy or rigorous proof planning; indicates proof planning requires more than model scale (better training signals, architectures, or objectives).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Proof planning benefits more from targeted training signals (e.g., contrastive hard negatives) than straightforward parameter scaling for these tasks/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8637.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8637.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3.2-1B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.2 1B (decoder-only LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only next-token-prediction LLM evaluated in ablations; found to struggle in stepwise proof-generation due to difficulty distinguishing input and output sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.2-1B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only language model (LLaMA family) focusing on next-token prediction; evaluated both with and without stepwise finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.2B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (Task 2 ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Entailment tree generation with distractors; stepwise decoding setting adds intermediate nodes and prior steps to input.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Stepwise finetuning (and a baseline without stepwise) to generate single-step entailments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Very poor compared to seq2seq baselines: without stepwise training (Task 2 ablation) Leaves F1 19.5 (AllCorrect 5.3), Steps F1 6.4 (AllCorrect 2.9), Intermediates F1 13.9 (AllCorrect 5.3), Overall 2.7; with stepwise finetuning performance dropped further (e.g., Leaves F1 15.6).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Significantly underperforms Flan-T5-Large and other seq2seq models despite being similar in parameter scale; shows ablation gap highlighting architecture effect.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Decoder-only architecture confuses input/output boundaries in stepwise training, tends to generate short outputs and imitate premises (predicting next sentence), and cannot adequately differentiate premises vs. generated steps leading to poor intermediate/hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Natural proof generation is fundamentally sequence-to-sequence; encoder-decoder models (like T5) are better suited than decoder-only LMs for stepwise entailment-tree generation, and architectural choice matters more than naive scaling for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8637.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8637.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-series (prompted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 / GPT-3.5-turbo / GPT-4 and variants (prompting with k-shot, CoT, SI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source large language models evaluated via prompting (5-shot, chain-of-thought, Selection-Inference) on the EntailmentBank proof-generation Task 2 to measure zero-shot/few-shot capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, GPT-3.5-turbo, GPT-4, GPT-4 (CoT/SI), GPT4o-mini, o1-mini, o1-preview (prompted)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Family of autoregressive large language models accessible via API; evaluated via few-shot prompting strategies including vanilla k-shot, chain-of-thought (CoT), and Selection-Inference (SI) decomposition for proof steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>closed-source; sizes not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (Task 2, few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generate stepwise proofs (entailment trees) from hypothesis and distractor-enriched contexts under few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>5-shot in-context learning, Chain-of-Thought prompting, and Selection-Inference (two-stage CoT with premise selection + conclusion inference).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported (Task 2 validation): GPT-3 (5-shot) Leaves F1 64.2 (AllCorrect 15.3), Steps F1 17.6 (AllCorrect 12.3), Intermediates F1 53.6 (AllCorrect 22.3), Overall AllCorrect 12.3. GPT-3.5-turbo (5-shot) slightly lower overall. GPT-4 (5-shot) much stronger: Leaves F1 78.1 (AllCorrect 32.6), Steps F1 30.2 (AllCorrect 22.5), Intermediates F1 63.9 (AllCorrect 30.8), Overall 21.9; GPT-4 (CoT) and GPT-4 (SI) produce modest improvements (CoT: Overall 22.5; SI: mixed: better leaves but more premise-selection errors). Newer small variants (o1-preview) approach GPT-4 but still lag behind finetuned ConDec.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Closed-source LLM prompting (even GPT-4 with CoT) underperforms finetuned ConDec models across many metrics; CoT improves GPT-4 relative to vanilla prompting but gaps remain. ConDec⋆ (finetuned) outperforms all prompted closed-source LLMs in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Prompted LLMs make frequent premise-selection and intermediate-conclusion errors; GPT-3.5 tends to imitate premises producing irrelevant/incorrect steps; GPT-4 better but still makes mistakes (especially under Selection-Inference premise-selection substage), and none match finetuned models on strict AllCorrect metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Few-shot prompting and CoT help but are insufficient for rigorous entailment-tree generation — supervised, stepwise finetuning with contrastive negative examples yields stronger, more reliable proof-generation behavior on EntailmentBank.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8637.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8637.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EntailmentWriter (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EntailmentWriter (Dalvi et al., 2021) / single-shot finetuning baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-shot transformer baseline trained to generate complete entailment trees (one-stage) used as a principal baseline on EntailmentBank.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explaining answers with entailment trees</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EntailmentWriter</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-shot transformer model trained to produce full entailment trees in one generation step; used in original EntailmentBank work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>reported variants include 11B in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (Task 1/2/3)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same entailment-tree / proof-generation benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Single-shot generation (no stepwise correction or auxiliary verifier/search) and reported as baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported (paper Table 3) Task 2 (distractor): Leaves F1 84.3 (AllCorrect 35.6), Steps F1 35.5 (AllCorrect 22.9), Intermediates F1 61.8 (AllCorrect 28.5), Overall AllCorrect 20.9. Larger EntailmentWriter (11B) improves but still below ConDec variants on several metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ConDec (stepwise + contrastive hard negatives) outperforms single-shot EntailmentWriter; multi-stage methods sometimes exceed single-shot baselines but at higher inference cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Single-shot training struggles with stepwise intermediate conclusions, and lacks mechanisms to avoid repetition/invalid entailment errors identified in Dalvi et al. (2021).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Stepwise correction (training or search/verification) is important; single-shot generation achieves high leaves F1 on Task 1 but degrades with distractors (Task 2) and full-corpus retrieval (Task 3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8637.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8637.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>METGEN (module-based entailment tree generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-stage (three-stage) system that decomposes single-step entailment into modules (selection, composition) and uses a controller to select promising steps; used as a strong baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>METGEN: A module-based entailment tree generation framework for answer explanation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetGen</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Module-based multi-stage entailment-tree generator combining forward/backward reasoning modules with a controller selecting steps and assembling proof trees.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (Tasks 1/2/3)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step textual entailment proof generation with search and module orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Multi-module generation (forward/backward) + controller; search over candidate steps; three-stage pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported (Table 3) on Task 2: Leaves F1 82.74 (AllCorrect 46.1), Steps F1 41.3 (AllCorrect 29.6), Intermediates F1 61.3 (AllCorrect 32.4), Overall AllCorrect 27.7. Comparable to or below ConDec on several metrics but uses heavier search.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>MetGen uses heavier search and three-stage processing; ConDec achieves comparable or superior metrics on Task 2/3 with simpler one-stage inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Higher computational and search cost; still challenged on intermediate-node accuracy in full-corpus settings.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Search/module decomposition can improve some aspects of proof planning but at substantial inference cost; targeted generator improvements (ConDec) can match or beat such pipelines in efficiency and many metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8637.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8637.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLProofs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NLProofs (verifier-guided search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-stage method that uses a trained verifier to guide search over expansions and selects a proof tree according to verifier scores; serves as a strong comparison baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating natural language proofs with verifier-guided search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NLProofs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Three-stage entailment-tree generator: generator proposals + verifier scoring + search/selection to build proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (Tasks 1/2/3)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-premise entailment-tree generation with search guided by a verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Separate generator and verifier modules with search over candidate expansions to assemble proof trees.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported (Table 3) Task 2: Leaves F1 90.35 (AllCorrect 58.8), Steps F1 47.23 (AllCorrect 34.4), Intermediates F1 70.24 (AllCorrect 37.8), Overall AllCorrect 33.3. Strong baseline; in some metrics NLProofs matches or slightly trails ConDec⋆.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>NLProofs achieves strong scores but uses explicit verification+search; ConDec provides comparable results with reduced inference cost by improving the generator directly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Higher inference cost due to search & verification; still limited gains on intermediates in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Verifier-guided search is effective for structural correctness, but generator-focused contrastive training offers a cost-effective alternative to raise quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8637.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8637.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RLET (reinforcement learning for entailment trees)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning based one-stage approach that optimizes cumulative rewards across generated entailment trees, reported to have advantages on long-path proofs over noisy retrieval settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rlet: A reinforcement learning based approach for explainable qa with entailment trees</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RLET</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>One-stage generator trained with reinforcement learning to assign rewards to generated steps and optimize full-tree trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBank (Tasks 1/2/3)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Entailment-tree generation with emphasis on long-path proofs and cumulative reward optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Reinforcement learning optimizing cumulative trajectory rewards over full entailment trees (no external search/verifier).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper discusses RLET in context: RLET can have advantage on Task 3 (full-corpus retrieval, long paths) due to training on long paths, but reported performance is substantially lower than top methods on Task 1/2 per discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>RLET underperforms prior methods on Task 1/2 though it benefits in Task 3 relative to some finetuned methods; contrasted with ConDec which is strong on Task 1/2.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Sensitive to noisy retrieval and domain mismatch under zero-shot conditions; poorer performance on Task 1/2 compared to methods trained with exact-match step supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Optimizing long-path trajectory rewards helps in full-corpus retrieval settings but is not a panacea; combining RL signals with stepwise contrastive training could be an avenue for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Explaining answers with entailment trees <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 2)</em></li>
                <li>METGEN: A module-based entailment tree generation framework for answer explanation <em>(Rating: 2)</em></li>
                <li>Generating natural language proofs with verifier-guided search <em>(Rating: 2)</em></li>
                <li>Vera: A general-purpose plausibility estimation model for commonsense statements <em>(Rating: 2)</em></li>
                <li>Rlet: A reinforcement learning based approach for explainable qa with entailment trees <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8637",
    "paper_id": "paper-265150088",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "Flan-T5-Large",
            "name_full": "Flan-T5 Large (instruction-finetuned T5-Large)",
            "brief_description": "An encoder-decoder transformer (T5-Large) instruction-finetuned with FLAN data, used as the primary generator in the paper and finetuned with the proposed contrastive stepwise decoding (ConDec).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5-Large",
            "model_description": "Encoder-decoder T5-Large variant instruction-finetuned on FLAN-style instructions; used as the generator and reasoner in experiments.",
            "model_size": "0.8B",
            "reasoning_task_name": "EntailmentBank (EntailmentWriter tasks: Task 1/2/3)",
            "reasoning_task_description": "Proof-generation / entailment-tree construction: given a hypothesis and a set of facts, produce an entailment tree (leaves, intermediate nodes, and steps) that proves the hypothesis; tasks vary by amount of distractors or full-corpus retrieval.",
            "method_or_approach": "Finetuning with stepwise decoding plus contrastive learning (ConDec); hard negatives (vanilla substitution and enhanced negatives from a reasoner + checker); alternative training of MLE and contrastive loss.",
            "performance": "As the base generator (stepwise finetuned) it attains (Task 2, distractor): Leaves F1 90.7 (AllCorrect 58.8), Steps F1 49.2 (AllCorrect 36.2), Intermediates F1 69.6 (AllCorrect 36.8), Overall AllCorrect 33.5. With ConDec (contrastive + vanilla/enhanced hard negatives, ConDec⋆) performance improves (Task 2): Leaves F1 91.1 (AllCorrect 60.6), Steps F1 50.7 (AllCorrect 37.4), Intermediates F1 70.7 (AllCorrect 38.2), Overall AllCorrect 34.7. Similar gains reported on Task 1/3 in paper tables.",
            "baseline_comparison": "ConDec-trained Flan-T5-Large outperforms single-shot finetuned baselines (EntailmentWriter) and achieves comparable or better performance than multi-stage methods (MetGen, NLProofs) on Task 2 and Task 3 while being more time-efficient (one-stage inference). Ablations show contrastive loss + hard negatives yields measurable gains over stepwise MLE-only training and over adding only vanilla negatives.",
            "limitations_or_failures": "Improvements concentrate on leaves and steps; intermediate-node (semantic conclusion) accuracy is harder to improve. Finetuning with hard negatives slightly degrades Task 1 (no-distractor) in some metrics. Dataset size is limited; Flan-T5-XL (3B) yields similar performance, indicating scale alone does not solve rigorous multi-step proof planning.",
            "insights_or_conclusions": "Stepwise training plus contrastive loss with carefully constructed hard negatives helps the generator discriminate premises and avoid repetition/invalid entailment decoding errors, improving proof generation without expensive verifier/search stages; however, generating rigorous intermediate conclusions remains a core challenge requiring better coverage/curated data or other modeling advances.",
            "uuid": "e8637.0",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ConDec",
            "name_full": "Contrastive learning based stepwise Decoding (ConDec)",
            "brief_description": "A training/decoding technique that augments stepwise finetuning with a contrastive loss using hard negatives (vanilla substitutions and enhanced negatives from a trained reasoner filtered by a checker) to reduce decoding errors in proof generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ConDec (applied to Flan-T5-Large in experiments)",
            "model_description": "A method (not a standalone pretrained LM): stepwise finetuning of a seq2seq generator combined with a contrastive loss that pulls encoder and decoder latent representations of correct step pairs together and pushes away hard negative generations.",
            "model_size": "method applied to 0.8B and 3B models in the paper",
            "reasoning_task_name": "EntailmentBank / proof generation (Tasks 1/2/3)",
            "reasoning_task_description": "Same as Flan-T5-Large entry — generate entailment trees (multi-step textual entailment chains) from hypothesis and supporting facts, including with distractors or full-corpus retrieval.",
            "method_or_approach": "Stepwise decoding supervision + contrastive loss (Eq. 6–8) using (1) vanilla hard negatives (conclusion substitutions) and (2) enhanced hard negatives produced by a reasoner and filtered by a plausibility checker (Vera); alternative training of MLE and contrastive objectives.",
            "performance": "ConDec (vanilla hard negatives) and ConDec⋆ (vanilla + enhanced) improve Leaves/Steps metrics and overall proof accuracy over MLE-only stepwise training; e.g., Task 2 ConDec⋆ on Flan-T5-Large: Leaves F1 91.16, Steps F1 50.73, Intermediates F1 70.74, Overall AllCorrect 34.7 (see paper Table 3/5).",
            "baseline_comparison": "Compared to single-shot finetuning (EntailmentWriter) ConDec yields higher F1 and AllCorrect on Task 2/3; compared to multi-stage systems (MetGen, NLProofs) it achieves comparable or better scores with lower inference cost. Ablations show contrastive loss + hard negatives &gt; contrastive alone &gt; MLE-only stepwise.",
            "limitations_or_failures": "ConDec improves structural aspects (leaves/steps) more than semantic intermediates; on Task 1 (no distractor) adding hard negatives can slightly harm intermediate prediction; generation quality still bounded by dataset coverage and by the reasoner/checker quality used to create enhanced negatives.",
            "insights_or_conclusions": "Explicitly training the generator to distinguish semantically-plausible but incorrect outputs via contrastive learning reduces common decoding errors (repetition, invalid entailment) and can partially close the gap to multi-stage search/verification methods, while maintaining simpler inference.",
            "uuid": "e8637.1",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Flan-T5-XL",
            "name_full": "Flan-T5 XL (instruction-finetuned T5-XL)",
            "brief_description": "A larger encoder-decoder T5 variant (3B) used in an ablation to test scale effects for stepwise finetuning and ConDec-style training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5-XL",
            "model_description": "Encoder-decoder transformer (T5-XL) instruction-finetuned on FLAN-style data; evaluated with stepwise finetuning and contrastive/hard-negative training.",
            "model_size": "3B",
            "reasoning_task_name": "EntailmentBank (Task 2 reported in ablation)",
            "reasoning_task_description": "Multi-step entailment tree generation with distractors (Task 2).",
            "method_or_approach": "Stepwise finetuning; compared with Flan-T5-Large baseline and ConDec variants.",
            "performance": "Reported in ablation (Task 2): Leaves F1 90.95 (AllCorrect 57.1), Steps F1 50.2 (AllCorrect 36.5), Intermediates F1 68.8 (AllCorrect 35.9), Overall AllCorrect 33.8 — broadly similar to Flan-T5-Large.",
            "baseline_comparison": "Increasing model size to 3B did not substantially improve proof-planning metrics over 0.8B Flan-T5-Large; stepwise ConDec training on the smaller model matches or exceeds performance.",
            "limitations_or_failures": "Scale alone did not solve intermediate-node accuracy or rigorous proof planning; indicates proof planning requires more than model scale (better training signals, architectures, or objectives).",
            "insights_or_conclusions": "Proof planning benefits more from targeted training signals (e.g., contrastive hard negatives) than straightforward parameter scaling for these tasks/datasets.",
            "uuid": "e8637.2",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLaMA-3.2-1B",
            "name_full": "LLaMA 3.2 1B (decoder-only LLM)",
            "brief_description": "A decoder-only next-token-prediction LLM evaluated in ablations; found to struggle in stepwise proof-generation due to difficulty distinguishing input and output sequences.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.2-1B",
            "model_description": "Decoder-only language model (LLaMA family) focusing on next-token prediction; evaluated both with and without stepwise finetuning.",
            "model_size": "1.2B",
            "reasoning_task_name": "EntailmentBank (Task 2 ablation)",
            "reasoning_task_description": "Entailment tree generation with distractors; stepwise decoding setting adds intermediate nodes and prior steps to input.",
            "method_or_approach": "Stepwise finetuning (and a baseline without stepwise) to generate single-step entailments.",
            "performance": "Very poor compared to seq2seq baselines: without stepwise training (Task 2 ablation) Leaves F1 19.5 (AllCorrect 5.3), Steps F1 6.4 (AllCorrect 2.9), Intermediates F1 13.9 (AllCorrect 5.3), Overall 2.7; with stepwise finetuning performance dropped further (e.g., Leaves F1 15.6).",
            "baseline_comparison": "Significantly underperforms Flan-T5-Large and other seq2seq models despite being similar in parameter scale; shows ablation gap highlighting architecture effect.",
            "limitations_or_failures": "Decoder-only architecture confuses input/output boundaries in stepwise training, tends to generate short outputs and imitate premises (predicting next sentence), and cannot adequately differentiate premises vs. generated steps leading to poor intermediate/hypothesis generation.",
            "insights_or_conclusions": "Natural proof generation is fundamentally sequence-to-sequence; encoder-decoder models (like T5) are better suited than decoder-only LMs for stepwise entailment-tree generation, and architectural choice matters more than naive scaling for this task.",
            "uuid": "e8637.3",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-series (prompted)",
            "name_full": "GPT-3 / GPT-3.5-turbo / GPT-4 and variants (prompting with k-shot, CoT, SI)",
            "brief_description": "Closed-source large language models evaluated via prompting (5-shot, chain-of-thought, Selection-Inference) on the EntailmentBank proof-generation Task 2 to measure zero-shot/few-shot capabilities.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3, GPT-3.5-turbo, GPT-4, GPT-4 (CoT/SI), GPT4o-mini, o1-mini, o1-preview (prompted)",
            "model_description": "Family of autoregressive large language models accessible via API; evaluated via few-shot prompting strategies including vanilla k-shot, chain-of-thought (CoT), and Selection-Inference (SI) decomposition for proof steps.",
            "model_size": "closed-source; sizes not specified in paper",
            "reasoning_task_name": "EntailmentBank (Task 2, few-shot prompting)",
            "reasoning_task_description": "Generate stepwise proofs (entailment trees) from hypothesis and distractor-enriched contexts under few-shot prompting.",
            "method_or_approach": "5-shot in-context learning, Chain-of-Thought prompting, and Selection-Inference (two-stage CoT with premise selection + conclusion inference).",
            "performance": "Reported (Task 2 validation): GPT-3 (5-shot) Leaves F1 64.2 (AllCorrect 15.3), Steps F1 17.6 (AllCorrect 12.3), Intermediates F1 53.6 (AllCorrect 22.3), Overall AllCorrect 12.3. GPT-3.5-turbo (5-shot) slightly lower overall. GPT-4 (5-shot) much stronger: Leaves F1 78.1 (AllCorrect 32.6), Steps F1 30.2 (AllCorrect 22.5), Intermediates F1 63.9 (AllCorrect 30.8), Overall 21.9; GPT-4 (CoT) and GPT-4 (SI) produce modest improvements (CoT: Overall 22.5; SI: mixed: better leaves but more premise-selection errors). Newer small variants (o1-preview) approach GPT-4 but still lag behind finetuned ConDec.",
            "baseline_comparison": "Closed-source LLM prompting (even GPT-4 with CoT) underperforms finetuned ConDec models across many metrics; CoT improves GPT-4 relative to vanilla prompting but gaps remain. ConDec⋆ (finetuned) outperforms all prompted closed-source LLMs in the paper's comparisons.",
            "limitations_or_failures": "Prompted LLMs make frequent premise-selection and intermediate-conclusion errors; GPT-3.5 tends to imitate premises producing irrelevant/incorrect steps; GPT-4 better but still makes mistakes (especially under Selection-Inference premise-selection substage), and none match finetuned models on strict AllCorrect metrics.",
            "insights_or_conclusions": "Few-shot prompting and CoT help but are insufficient for rigorous entailment-tree generation — supervised, stepwise finetuning with contrastive negative examples yields stronger, more reliable proof-generation behavior on EntailmentBank.",
            "uuid": "e8637.4",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "EntailmentWriter (baseline)",
            "name_full": "EntailmentWriter (Dalvi et al., 2021) / single-shot finetuning baseline",
            "brief_description": "A single-shot transformer baseline trained to generate complete entailment trees (one-stage) used as a principal baseline on EntailmentBank.",
            "citation_title": "Explaining answers with entailment trees",
            "mention_or_use": "mention",
            "model_name": "EntailmentWriter",
            "model_description": "Single-shot transformer model trained to produce full entailment trees in one generation step; used in original EntailmentBank work.",
            "model_size": "reported variants include 11B in comparisons",
            "reasoning_task_name": "EntailmentBank (Task 1/2/3)",
            "reasoning_task_description": "Same entailment-tree / proof-generation benchmark.",
            "method_or_approach": "Single-shot generation (no stepwise correction or auxiliary verifier/search) and reported as baseline in comparisons.",
            "performance": "Reported (paper Table 3) Task 2 (distractor): Leaves F1 84.3 (AllCorrect 35.6), Steps F1 35.5 (AllCorrect 22.9), Intermediates F1 61.8 (AllCorrect 28.5), Overall AllCorrect 20.9. Larger EntailmentWriter (11B) improves but still below ConDec variants on several metrics.",
            "baseline_comparison": "ConDec (stepwise + contrastive hard negatives) outperforms single-shot EntailmentWriter; multi-stage methods sometimes exceed single-shot baselines but at higher inference cost.",
            "limitations_or_failures": "Single-shot training struggles with stepwise intermediate conclusions, and lacks mechanisms to avoid repetition/invalid entailment errors identified in Dalvi et al. (2021).",
            "insights_or_conclusions": "Stepwise correction (training or search/verification) is important; single-shot generation achieves high leaves F1 on Task 1 but degrades with distractors (Task 2) and full-corpus retrieval (Task 3).",
            "uuid": "e8637.5",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MetGen",
            "name_full": "METGEN (module-based entailment tree generation)",
            "brief_description": "A multi-stage (three-stage) system that decomposes single-step entailment into modules (selection, composition) and uses a controller to select promising steps; used as a strong baseline.",
            "citation_title": "METGEN: A module-based entailment tree generation framework for answer explanation",
            "mention_or_use": "mention",
            "model_name": "MetGen",
            "model_description": "Module-based multi-stage entailment-tree generator combining forward/backward reasoning modules with a controller selecting steps and assembling proof trees.",
            "model_size": null,
            "reasoning_task_name": "EntailmentBank (Tasks 1/2/3)",
            "reasoning_task_description": "Multi-step textual entailment proof generation with search and module orchestration.",
            "method_or_approach": "Multi-module generation (forward/backward) + controller; search over candidate steps; three-stage pipeline.",
            "performance": "Reported (Table 3) on Task 2: Leaves F1 82.74 (AllCorrect 46.1), Steps F1 41.3 (AllCorrect 29.6), Intermediates F1 61.3 (AllCorrect 32.4), Overall AllCorrect 27.7. Comparable to or below ConDec on several metrics but uses heavier search.",
            "baseline_comparison": "MetGen uses heavier search and three-stage processing; ConDec achieves comparable or superior metrics on Task 2/3 with simpler one-stage inference.",
            "limitations_or_failures": "Higher computational and search cost; still challenged on intermediate-node accuracy in full-corpus settings.",
            "insights_or_conclusions": "Search/module decomposition can improve some aspects of proof planning but at substantial inference cost; targeted generator improvements (ConDec) can match or beat such pipelines in efficiency and many metrics.",
            "uuid": "e8637.6",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "NLProofs",
            "name_full": "NLProofs (verifier-guided search)",
            "brief_description": "A three-stage method that uses a trained verifier to guide search over expansions and selects a proof tree according to verifier scores; serves as a strong comparison baseline.",
            "citation_title": "Generating natural language proofs with verifier-guided search",
            "mention_or_use": "mention",
            "model_name": "NLProofs",
            "model_description": "Three-stage entailment-tree generator: generator proposals + verifier scoring + search/selection to build proofs.",
            "model_size": null,
            "reasoning_task_name": "EntailmentBank (Tasks 1/2/3)",
            "reasoning_task_description": "Multi-premise entailment-tree generation with search guided by a verifier.",
            "method_or_approach": "Separate generator and verifier modules with search over candidate expansions to assemble proof trees.",
            "performance": "Reported (Table 3) Task 2: Leaves F1 90.35 (AllCorrect 58.8), Steps F1 47.23 (AllCorrect 34.4), Intermediates F1 70.24 (AllCorrect 37.8), Overall AllCorrect 33.3. Strong baseline; in some metrics NLProofs matches or slightly trails ConDec⋆.",
            "baseline_comparison": "NLProofs achieves strong scores but uses explicit verification+search; ConDec provides comparable results with reduced inference cost by improving the generator directly.",
            "limitations_or_failures": "Higher inference cost due to search & verification; still limited gains on intermediates in some tasks.",
            "insights_or_conclusions": "Verifier-guided search is effective for structural correctness, but generator-focused contrastive training offers a cost-effective alternative to raise quality.",
            "uuid": "e8637.7",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RLET",
            "name_full": "RLET (reinforcement learning for entailment trees)",
            "brief_description": "A reinforcement-learning based one-stage approach that optimizes cumulative rewards across generated entailment trees, reported to have advantages on long-path proofs over noisy retrieval settings.",
            "citation_title": "Rlet: A reinforcement learning based approach for explainable qa with entailment trees",
            "mention_or_use": "mention",
            "model_name": "RLET",
            "model_description": "One-stage generator trained with reinforcement learning to assign rewards to generated steps and optimize full-tree trajectories.",
            "model_size": null,
            "reasoning_task_name": "EntailmentBank (Tasks 1/2/3)",
            "reasoning_task_description": "Entailment-tree generation with emphasis on long-path proofs and cumulative reward optimization.",
            "method_or_approach": "Reinforcement learning optimizing cumulative trajectory rewards over full entailment trees (no external search/verifier).",
            "performance": "Paper discusses RLET in context: RLET can have advantage on Task 3 (full-corpus retrieval, long paths) due to training on long paths, but reported performance is substantially lower than top methods on Task 1/2 per discussion.",
            "baseline_comparison": "RLET underperforms prior methods on Task 1/2 though it benefits in Task 3 relative to some finetuned methods; contrasted with ConDec which is strong on Task 1/2.",
            "limitations_or_failures": "Sensitive to noisy retrieval and domain mismatch under zero-shot conditions; poorer performance on Task 1/2 compared to methods trained with exact-match step supervision.",
            "insights_or_conclusions": "Optimizing long-path trajectory rewards helps in full-corpus retrieval settings but is not a panacea; combining RL signals with stepwise contrastive training could be an avenue for future work.",
            "uuid": "e8637.8",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Explaining answers with entailment trees",
            "rating": 2,
            "sanitized_title": "explaining_answers_with_entailment_trees"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 2,
            "sanitized_title": "selectioninference_exploiting_large_language_models_for_interpretable_logical_reasoning"
        },
        {
            "paper_title": "METGEN: A module-based entailment tree generation framework for answer explanation",
            "rating": 2,
            "sanitized_title": "metgen_a_modulebased_entailment_tree_generation_framework_for_answer_explanation"
        },
        {
            "paper_title": "Generating natural language proofs with verifier-guided search",
            "rating": 2,
            "sanitized_title": "generating_natural_language_proofs_with_verifierguided_search"
        },
        {
            "paper_title": "Vera: A general-purpose plausibility estimation model for commonsense statements",
            "rating": 2,
            "sanitized_title": "vera_a_generalpurpose_plausibility_estimation_model_for_commonsense_statements"
        },
        {
            "paper_title": "Rlet: A reinforcement learning based approach for explainable qa with entailment trees",
            "rating": 1,
            "sanitized_title": "rlet_a_reinforcement_learning_based_approach_for_explainable_qa_with_entailment_trees"
        }
    ],
    "cost": 0.019383749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning
30 Oct 2025</p>
<p>Ying Su yingsu@scut.edu.cn 
South China University of Technology</p>
<p>Mingwen Liu 
South China University of Technology</p>
<p>Zhijiang Guo zhijiangguo@hkust-gz.edu.cn 
South China University of Technology</p>
<p>Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning
30 Oct 20258116F5616E4231EA222072000EF82C2BarXiv:2311.06736v3[cs.CL]
Logical reasoning is a pivotal component in the field of artificial intelligence.Proof planning, particularly in contexts requiring the validation of explanation accuracy, continues to present challenges.The recent advancement of large language models (LLMs) has led to significant progress in natural language proof planning, evolving from one-stage generators to more complex three-stage systems that include additional searchers or verifiers.While these assisted methods improve the quality of generated results, they also introduce increased search efforts and computational costs.Furthermore, the generative process itself remains underexplored.In this study, we propose a stepwise decoding approach augmented by contrastive learning to address two common errors encountered during the LLM generator's decoding process.We fine-tune the language model using both vanilla and enhanced hard negatives to mitigate these decoding errors.Empirical results demonstrate the effectiveness of our strategy.Additionally, our further analysis reveals that even larger LLMs still struggle to generate rigorous logical chains.</p>
<p>Introduction</p>
<p>Logical reasoning underpins the comprehension of human cognition and intelligence in machines (Goel et al., 2017).Large Language Models (LLMs) like GPT (Brown et al., 2020;Ouyang et al., 2022) and PaLM (Chowdhery et al., 2022;Anil et al., 2023) have pioneered using natural language as a platform for logical reasoning, complementing the traditional use of formal languages (Kazemi et al., 2022;Creswell et al., 2022).The incorporation of natural language broadens the scope of logical reasoning by allowing flexible querying and tapping into the extensive implicit knowledge encapsulated within LLMs.</p>
<p>In examining the capacity of LLMs for logical reasoning, it is crucial to consider not only the accuracy of their answers but also the correctness of their explanations (Xu et al., 2023).Utilizing prompting methods such as in-context learning (Brown et al., 2020) and chain-of-thought (Wei et al., 2022), LLMs have shown promising results across various deductive reasoning tasks in question-answering formats (Weston et al., 2015;Tafjord et al., 2021;Saparov and He, 2022;Han et al., 2022).These approaches decompose the final task goal by guiding the LLMs through intermediate reasoning steps in a carefully constructed context.However, providing correct explanations, which covers completeness, redundancy, correctness (Xu et al., 2023), emerges as a more daunting challenge.This is particularly evident in tasks that involve generating reasoning chains from premises leading to a conclusion, known as proof generation (Clark et al., 2020;Dalvi et al., 2021).Unfortunately, LLMs often fall short in creating concise and exact proof trees, commonly producing superfluous or imprecise intermediate steps.</p>
<p>Previous studies have utilized LLMs to generate proof trees, employing a range of techniques from holistic approaches (Qu et al., 2022) to incremental steps (Tafjord et al., 2021;Sanyal et al., 2022).Recent methods increasingly rely on postprocessing to enhance the quality of generated results, introducing verification-and search-based systems (Hong et al., 2022;Yang et al., 2022).However, as the methods become more complex, there is a corresponding increase in search efforts and computational costs.Conversely, there has been insufficient focus on refining the generative process itself.Current models exhibit proficiency in selecting relevant premises but struggle to deduce intermediary conclusions, highlighting a deficiency in their understanding of semantic nuances during stepwise deductive reasoning.</p>
<p>Addressing this issue, we introduce a novel strategy dubbed ConDec (Contrastive learning based stepwise Decoding), designed to enhance the generative aspect of LLMs for deductive reasoning tasks.ConDec leverages carefully constructed hard negatives -outputs that are deceivingly similar in form yet differ semantically -to refine generation precision.These hard negatives can be simple sequence alterations or products of an intricate sampling and reasoning process, aided by an external reasoner and checker.Intuitively, the hard negatives are designed to solve decoding errors analyzed in (Dalvi et al., 2021): repetition and invalid entailment.Finetuning with these hard negatives notably advances the LLMs' proficiency in intermediate step and conclusion generation, culminating in overall improved proof accuracy.The main contributions of this study are threefold:</p>
<p>• We introduce ConDec, a stepwise decoding with contrastive learning strategy that enhances stepwise generative quality in proof generation tasks, and devise an automatic method for hard negative generation involving a reasoner and a checker;</p>
<p>• We conduct an extensive empirical analysis on the Entailment benchmark, demonstrating the effectiveness of the proposed method;</p>
<p>• We reveal that LLMs even equipped with chain-of-thought strategies still struggle to perform rigorous logical reasoning in natural language proof generation tasks.</p>
<p>2 Related Work</p>
<p>Logical Reasoning with Natural Language</p>
<p>Logical reasoning is an important ability to realize human-level cognition and intelligence in AI (Nunes, 2012).Early research of logical reasoning uses formal language to represent knowledge and conducts symbolic reasoning (Muggleton and De Raedt, 1994).Recent research uses pretrained language models for logical reasoning in the form of natural language to alleviate the representation challenge with formal language (Musen and Van der Lei, 1988).Among the logical reasoning over natural language (Yang et al., 2023), deductive reasoning covers aspects including hypothesis classification, proof generation, proof generation with incomplete information, and implication enumeration.Several tasks have been proposed to evaluate these reasoning abilities.Specifically, hypothesis classification is conducted over RuleTaker with transformers (Clark et al., 2020).Proof generation providing rationals along with the predicted answer for emulating formal reasoning is further proposed to increase the explanability (Saha et al., 2020).ProofWriter (Tafjord et al., 2021) produces a deductive chain of reasoning over proof generation and implication enumeration with an iterative generating style.To enhance the chain of reasoning for multi-step premises, EntailmentWriter tests transformers' explainability in the form of entailment trees over EntailmentBank (Dalvi et al., 2021).</p>
<p>Proof Generation</p>
<p>Methods for finetuning language models for proof generation vary in the proof direction, inference with or without hypothesis, and whether search or verification is involved.One line of research is inference without a hypothesis available.FaiRR (Sanyal et al., 2022) breaks proof generation into three steps: rule selection, fact selection, and knowledge composition.MetGen (Hong et al., 2022) iteratively generates the entailment tree by conducting a single-step entailment with separate modules and a reasoning controller.SC-Search (Bostrom et al., 2022) decomposes the deductive reasoning task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system's reasoning process.ADGV (Sprague Another line of work is with the hypothesis available.IBR (Qu et al., 2022) enhances the interpretability of reasoning procedures by predicting nodes and edges in the proof tree iteratively backward from the question, as well as increasing efficiency and accuracy by simplifying the intermediate process of reasoning.IRGR (Ribeiro et al., 2022) explains a given hypothesis by iteratively searching for suitable premises, constructing a single entailment step at a time.NLProofs (Yang et al., 2022) trains an independent verifier to check the validity of the proof steps to improve decoding accuracy.Our work follows this line and we focus on stepwise decoding correction on the generator itself.A comparison of our method with other approaches with LLMs is presented in Table 1.</p>
<p>Problem Formulation</p>
<p>Following the task definition in Yang et al. (2022), the proof generation task is to derive a proof tree T given a hypothesis h and a set of supporting facts C = {sent 1 , sent 2 , ..., sent n }.The proof tree T is represented as a tuple T = (h, Ĉ, U, S).Ĉ is a subset of the facts {sent i } ∈ C, denoting leaf nodes on the tree T .U = {int 1 , int 2 , ..., int m } denotes the intermediate nodes on the tree.The intermediate nodes are deduced during the reasoning process.Each intermediate node represents an intermediate conclusion.The intermediate nodes are internal tree nodes.On top of the tree, h is the root node as well as the final conclusion needs to be proved.</p>
<p>The structure of the tree is denoted by reasoning steps S = {step 1 , step 2 , ..., step t }.Each internal tree node corresponds to a reasoning step step i ∈ S with int j ∈ U as the conclusion and its children as premises, i.e., sent
1 &amp; int 1 → int 2 , representing that intermediate node int 2 is the con- clusion of leaf</p>
<p>Approach</p>
<p>Stepwise Decoding with Contrastive Learning</p>
<p>With stepwise training, subtrees are sampled on the original entire proof graph.The decoding goal can be the intermediate node or the final hypothesis of the subtree, depending on the sampling strategy.As analyzed in (Dalvi et al., 2021), there are decoding errors leading to inaccurate proof step generation, finally leading to entire proof generation failure.</p>
<p>An overview is presented in Figure 1.</p>
<p>To address the problem, we adopt a contrastive learning technique to improve the stepwise decoding quality.Learning with contrasting positive and negative pairs can improve the generalization ability of conditional text generations (Lee et al., 2020;An et al., 2022).Inspired by this, we construct negative decoding samples to improve the reasoning ability of the generator.</p>
<p>The goal of the generator is to output a stepwise reasoning step step
(i) j with tokens (s (i) 1 , s(i) 2 , ..., s(i) L ) with length L conditioned on the input text sequence x (i) = (x (i) 1 ,
x(i) 2 , ...).The input text sequence is a concatenation of contexts from hypothesis h, facts C, and previous steps {step
(i) 1 , ..., step (i) j−1 }. i is the index of instances in a batch.
The finetuning loss is to maximize the conditional log-likelihood log p θ (step j |x) for a given
N observations {(x (i) , step (i) j ) N i=1 } as follows: L M LE (θ) = N i=1 log p θ (step (i) j |x (i) ), (1) p θ (s (i) 1 , ..., s(i) L |x (i) ) = L l=1 p θ (s (i) l |s (i) &lt;l , x (i) ),(2)h (i) l = g(s (i) l−1 , M (i) ; θ), M (i) = f (x (i) ; θ),(3)
where f, g denote the encoder and the decoder respectively.M (i) is the concatenation of the hidden representations of the source tokens x (i) .
H (i) is the concatenation of the hidden states [h (i) 1 , ..., h(i)
L ] at the decoder output.With a linear projection layer, the hidden states M (i) and H (i) of the encoder and decoder are mapped onto the latent embedding space:
z (i) x = AvgPool(ReLU(W proj M (i) + b proj )), (4) z (i) s = AvgPool(ReLU(W proj H (i) + b proj )).
(5) The semantic similarity sim between them can be calculated by distance with a dot or cosine function.A contrastive loss maximizes the similarity between the pair of source sequence and target sequence while minimizing the similarity between the negative pairs as follows:
L cont (θ) = N i=1 log exp(sim(z (i) x , z (i) s )/τ ) z (k)
s ∈S exp(sim(z
(i) x , z (k) s )/τ ) , (6
) where S is the set of negative samples in the same batch and τ is the temperature parameter.</p>
<p>Training with Hard Negative</p>
<p>Though stepwise generation improves over a singleshot generation (training over the entire proof tree and the decoding goal is a hypothesis), it still has typical errors (Dalvi et al., 2021): 1) repetition(the entailed conclusion simply repeats one of the input sentences); 2) invalid entailment(the entailed conclusion does not follow from input sentences).To improve the decoding quality, we design two types of hard negatives for these errors and finetune the model with these hard negatives.</p>
<p>The hard negative sequence step (i) j</p>
<p>is constructed based on the gold proof step sequence step (i) j .With the hard negative sequences, the decoding loss becomes:
L cont−hard (θ) = N i=1 log exp(sim(z (i)
x , z
(i) s )/τ ) z (k) s ∈S {z (i)
s } exp(sim(z
(i)
x , z
(i) s )/τ ) ,(7)
where z(i) s is the projected hidden state of hard negatives.The final loss for finetuning is:
L = L M LE + αL cont−hard ,(8)
α is a weighted parameter.</p>
<p>Vanilla Hard Negative</p>
<p>Vanilla hard negatives are constructed based on substitution, which mimics the form of gold stepwise proofs, to address the repetition error.we randomly select one of the premises in a proof step step j and replace the conclusion with its context.For example in Figure 1, the vanilla hard negative is constructed by replacing the gold standard conclusion with the context of input node sent11.</p>
<p>Enhanced Hard Negative</p>
<p>To increase the entailment quality over stepwise proofs, we also propose to construct enhanced hard negatives by exploring unseen proof steps with a reasoner and a checker.The reasoner is first trained with proof steps step j ∈ S in natural language and then utilized to generate a conclusion given an unseen combination of premises in the supporting facts C. Given proof step step j , the premises and conclusion in natural language are denoted as set {p 1 , p 2 , ...} and c.The premises are concatenated as input and the conclusion is output for training the reasoner.
L M LE (ϕ) = log p ϕ (c|p 1 , p 2 , ...),(9)
After training, the reasoner can generate a conclusion given an unseen combination of premises.The premise combinations are first sampled from supporting fact set.The sampled premises {p s 1 , p s 2 , ...} and generated conclusion c s constructs an enhanced hard negative step.Details are presented in Figure 2. In the example, for premises int1 and sent11 in gold proof, randomly sample one premise to substitute one of them and use the reasoner to generate a new conclusion given the recombined premises.</p>
<p>To improve the quality of the hard negatives generated from the reasoner, we further adopt the checker Vera (Liu et al., 2023) to score the hard negatives and filter those with low scores.Vera is finetuned over T5-11B with commonsense datasets.The score from the checker indicates the extent of the reasonableness of the deductive commonsense knowledge generated by the reasoner:
score = sigmoid([p s 1 , p s 2 , ..., c s ]; γ), (10)
where the score is calculated with a sigmoid function after hidden layers γ.</p>
<p>Experiment</p>
<p>Dataset</p>
<p>EntailmentBank (Dalvi et al., 2021): Entailment trees are made up of individual and multi-premise textual entailment steps.EntailmentBank contains 1,840 multi-step entailment trees for accompanying QA pairs.For the proof generation task, only the hypothesis H and context set C are used as inputs.</p>
<p>Each proof tree T contains an average of 6.6 nodes and 2.7 entailment steps.Train/Validation/Test splits are 1,313/187/340 respectively.Entailment-Bank consists of three tasks as follows:</p>
<p>(1) Task 1 (no-distractor): C consists of exactly the leaf nodes of the ground truth proof tree;</p>
<p>(2) Task 2 (distractor): C consists of 15-20 distractor sentences besides the leaf nodes on the ground truth proof tree;</p>
<p>(3) Task 3 (full-corpus): C is a large corpus of 12K sentences derived from WorldTree V2 (Xie et al., 2020), requiring the model to retrieve relevant supporting facts from the corpus.For each hypothesis, 25 supporting facts are retrieved.Following Dalvi et al. (2021), we evaluate the zeroshot performance of the model from Task 2.</p>
<p>Evaluation</p>
<p>Following Dalvi et al. (2021), we use the official tools1 to evaluate the generated entailment tree T = (h, L, E, S) with the golden entailment tree T * = (h, L * , E * , S * ).These metrics evaluate the correctness along 4 dimensions:</p>
<p>(1) Leaves (F1, AllCorrect): F1 measures the precision of leaf nodes of T comparing to gold tree T * .ALLCorrect=1 if F1=1, and ALLCorrect=1 if F1&lt;1.</p>
<p>(2) Steps (F1, AllCorrect): F1 measures the precision of proof steps structurally correctness.Each step contains an internal node u ∈ T * (aligned to v ∈ T ).The predicted step is correct if u and v are perfectly aligned.For each tree, ALLCorrect=1 if F1=1 otherwise 0.</p>
<p>(3) Intermediates (F1, AllCorrect): For the internal node u ∈ T * (aligned to v ∈ T ), the intermediate conclusion is correct if the BLEURT (Sellam et al., 2020) score between u and v is greater than 0.28 (Dalvi et al., 2021).F1 and AllCorrect from all intermediate conclusions in T * and T are calculated.ALLCorrect=1 if F1=1 otherwise 0. (4) Overall (AllCorrect): The metric evaluates whether leaves, steps, and intermediates are all correct, AllCorret = 1 if and only if all the leaves, steps, and intermediates are all correct.</p>
<p>Implementation Details</p>
<p>The optimizer is set as Adam (Kingma and Ba, 2014) for all the training.The average running time is 18 hours for Task 1 and 24 hours for Task 2. Experiments are conducted on A800.</p>
<p>Generator.We use Flan-T5-Large as the generator.Flan-T5-Large is a finetuned version of T5-Large (Raffel et al., 2020) over a collection of FLAN instructions (Chung et al., 2022;Longpre et al., 2023).The generator is trained for 500 epochs on Task1 and 600 epochs on Task2.The learning rate for the first-stage generator is 1e-4 and 5e-5 for Task 1 and Task 2 respectively.</p>
<p>ConDec.The vanilla hard negatives are constructed by substituting the conclusion in the gold proof step with a randomly selected premise node context.For example, for sent11 &amp; sent24 -&gt; int: neptune orbits the sun in the solar system, the hard negatives substitute conclusion neptune orbits the sun in the solar system with the context of sent11 or sent24.The temperature τ is set as 0.05 and α is 0.1.The learning rate of the stepwise decoding stage is the same as finetuning with original proof trees.The MLE loss and contrastive loss are alternatively trained for 10 epochs based on the finetuned generator with MLE loss only.</p>
<p>Enhanced Hard Negatives.We use Flan-T5-Large as an additional reasoner to generate unseen proof steps based on labeled proof trees as hard negatives.To train the reasoner, details of data collection are presented in Appendix A.1.The reasoner is trained for 30 epochs with a learning rate of 1e-4.We further apply Vera (Liu et al., 2023) to filter unreasonable generated proof steps.Details of stepwise proofs sampled for reasoner and enhanced new negatives filtered by the checker are presented in Table 2. To increase diversity, enhanced hard negatives and vanilla hard negatives are jointly sampled for training.</p>
<p>6 Result Analysis</p>
<p>Main Results</p>
<p>The main results are presented in Table 3.By analyzing the results, we can find that: Stepwise correction matters.Stepwise generation methods (MetGen, NLProofs, ConDec) outperform single-shot training (EntailmentWriter), even for EntailmentWriter with a much larger parameter size (11B).When comparing ConDec with threestage generation methods MetGen and NLProofs, ConDec achieves comparable or even better performance on Task 2 and Task 3.This shows that contrastive decoding with hard negatives can improve language models' reasoning ability, demonstrating our methods' effectiveness.While our research focuses on the generator, combining our method with theirs may still improve the final accuracy and it is worth exploring.Enhanced hard negatives facilitate reasoning.With enhanced hard negatives, we can find that the ability of proof planning over three tasks is all improved.Unlike vanilla negative construction, the enhanced hard negatives contain harder or more accurate conclusions given premises.Detailed evaluation of enhanced hard negatives is in Appendix A.1.It further improves the training coverage over reasoning steps, thus leading to better performance ConDec achieves the best performance mostly over leave or step accuracy.The contrastive loss helps the generator discriminate between premises and finds semantically correlated premises to deduce a conclusion.However, deductive reasoning is still challenging as the improvement over intermediates is not as obvious as that on leaves or steps.</p>
<p>Computational Cost Analysis</p>
<p>ConDec's design can reduce the inference cost with a one-stage process during inference as shown in Table 4.We provide a concrete comparison using the validation set for Task 2, with the same model size (Flan-T5-Large), on an A800 GPU.It clearly shows that ConDec can significantly improve time efficiency compared to two and three-stage methods (NLProofs).</p>
<p>Ablation Study</p>
<p>Results of the ablation study on Task 2 test split are presented in Table 5. Backbone model.The results indicate that LLaMA-3.2-1Bfails to adequately differentiate between input premises and output reasoning proof steps.As a decoder model focused on next-word prediction, LLaMA simply predicts proof steps as the next sentence following the premises.With stepwise training, the presence of intermediate or hypothesis nodes can confuse the model regarding when to stop generating output, as a subtree with an intermediate node is a subsequence of the entire tree that includes the hypothesis node.This is why LLaMA tends to produce shorter outputs when trained in a stepwise manner.Moreover, the results for LLaMA are significantly lower than those for Flan-T5-Large, despite the latter's smaller size.Natural proof generation is fundamentally a sequence-to-sequence task.Distinguishing between input and output is crucial, as the</p>
<p>Analysis of Closed-Source LLMs</p>
<p>In-context learning (ICL; Brown et al. 2020) and CoT (Wei et al., 2022) have been widely used in various reasoning tasks (Patel et al., 2021;Cobbe et al., 2021;Fu et al., 2022;Xiong et al., 2023).We apply 5-shot prompting and CoT to evaluate how LLMs perform on the proof generation task.The vanilla prompt method simply adopts a k-shot in-context learning strategy.The CoT decomposes the reasoning process into step-by-step generation.</p>
<p>Based on it, Select-inference(SI) (Creswell et al., 2022) is a two-stage COT that decomposes each reasoning step into a premise selection stage and a conclusion inference state.Details of the CoT is illustrated in Appendix A.2.2.As shown in Table 6.Comparing different LLMs, GPT4 generally outperforms GPT3 and GPT3.5-turbo on all the metrics.One detailed case study of GPT3.5-turbo and GPT4 with k-shot prompting results is in Appendix A.2.1.It shows that the GPT3.5-turbotends to generate more irrelevant and inaccurate steps with simple imitation of premises during inferencing new conclusions.In contrast, GPT4 conducts deductive reasoning and generates correct conclusions based on premises.When accompanied by CoT, GPT4 achieves better performance than the vanilla prompt.While with SI, GPT4 makes more mistakes in the premise selection stage.</p>
<p>Our ConDec ⋆ outperforms all closed-source LLMs in all metrics.Although recent LLMs such as o1-mini and o1-preview show improvements in several metrics compared to their predecessors (GPT-3.5-turboand GPT-4), a substantial performance gap remains when compared to finetuningbased methods.Through stepwise training with curated datasets, these language models can better capture the correlations between premises.</p>
<p>Conclusion</p>
<p>Logical reasoning is both challenging and fundamental in artificial intelligence.Proof generation serves as a measure of the explanatory capabilities of large language models in the context of logical reasoning.To enhance stepwise deductive reasoning, we propose a decoding strategy augmented by contrastive learning.The carefully designed hard negatives address the typical errors encountered during the decoding process.The experimental results across standard benchmarks demonstrate the effectiveness of our method.Additionally, our analysis of larger LLMs reveals that they continue to struggle with proof planning tasks.</p>
<p>Limitations</p>
<p>From the analysis of the paper, natural proof planning ability is still a challenging topic in evaluating LLMs' deductive reasoning ability.The current curated human-annotated dataset in our experiments is of limited size to improve LLMs' deductive reasoning ability.Knowledge from related corpora such as cause and effect, logic reasoning can be further applied to improve the proof generation ability with pre-training or transfer learning.</p>
<p>Threshold score Accuracy(%) 0.7 36 0.8 50 0.9 60</p>
<p>A Appendix</p>
<p>A.1 Details of Enhanced Hard Negatives</p>
<p>Training data of the reasoner is basically all the proof steps in the training set.Each proof step is converted into the form of natural language: 1) premises are concatenated with conjunction "and"; 2) premises and conclusion are linked with "Because" and "Therefore".The reasoner is trained with 8,819 proof steps for both Task 1 and Task 2.</p>
<p>For each proof step step j , substitute one premise from the the supporting fact set C except the ones in current proof step.Recombined proof premises {p s 1 , p s 2 , ...} are converted into natural language form and used as input to the reasoner.The reasoner generates a new conclusion c s based on the premises, forming a hard negative step (i) j .The constructed hard negative step (i) j is then filtered by the checker with a threshold score.To determine the threshold score, three PhD students from the CSE department are assigned with the task to check the quality of 100 randomly selected constructed hard negatives.Each hard negative is first judged by Vera.The score from Vera indicates the reasonableness of the hard negative.By filtering the score with a threshold, we choose the constructed negatives with high quality for further training.The result of human checking over the filtered negatives is shown in the table 7. We find that with a threshold score of 0.9, 60% of the filtered hard negatives are reasonable.</p>
<p>A.2 Prompting Methods</p>
<p>A.2.1 Case study for prompting</p>
<p>Result case from GPT3.5-turbo and GPT4 on Task2 dev split with vanilla prompts is shown in Table 8.</p>
<p>A.2.2 Template for prompting</p>
<p>Prompting template for GPT4 with CoT is in Table 9.</p>
<p>A.3 Baseline Details</p>
<p>MetGen and NLProofs are three-stage methods.MetGen divides single-step entailment into basic logical operations.It reasons in both forward deductive and backward abductive steps.A controller finally selects promising steps among the reasoning results.NLProofs uses a trained verifier to guide the search process of proof generation.The verifier scores the generation expansion steps and finally, a proof tree is selected according to the scores.</p>
<p>A.4 Discussion with RLET RLET (Liu et al., 2022) is trained using cumulative signals across the entire entailment tree, marking the first introduction of reinforcement learning into the entailment tree generation task.It employs a one-stage generation method that does not involve search or verification.RLET flexibly assigns rewards to each generated step and utilizes the overall cumulative reward to optimize training based on the full trajectory.In contrast, our approach and previous methods (as listed in Table 1) rely on exact matches of gold steps as training signals.</p>
<p>In Task 3, the retrieved supporting facts may contain noise and are longer than those in Task 1 and Task 2. RLET's focus on long-path proof training provides it with an advantage over our method (and previous methods listed in Table 1) in this task.However, since the supporting facts are sourced from a relevant corpus, they may deviate from the training setting, particularly under zero-shot performance conditions.As a result, RLET's performance is significantly lower than that of previous methods in Task 1 and Task 2.</p>
<p>A.5 Selection of Enhanced Hard Negatives</p>
<p>Besides randomly selecting premises to construct enhanced hard negatives, we also select according to the top similarity score calculated with BM25 (Robertson et al., 2009).Results are presented in Table 10.The results show that similar distracted premises do not contribute to intermediate node accuracy as much as random premises do.This is because randomly selected premises can introduce more diversity for the constructed hard negatives.</p>
<p>A.6 Case Study</p>
<p>We present a case study comparing the generation capabilities of LLaMA 3.2-1B and Flan-T5-Large to illustrate the differences between encoderdecoder and decoder-only LLMs, as shown in Table 11.The table indicates that LLaMA (without stepwise training) generates longer proofs because it is trained with complete proof chains.In contrast,LLaMA (with stepwise training) typically produces one-step proofs that conclude with either a "hypothesis" or an "intermediate node," reflecting the fact that both are endpoints in the sequences used for stepwise training.Flan-T5-Large (with stepwise training) strikes a balance between generating long and short proof chains due to its encoderdecoder architecture.However, it still faces challenges in accurately selecting premises.Hypothesis: as the distance of the star to earth decreases, the star will appear brighter.</p>
<p>Context: sent1: a star produces light and heat sent2: far is the opposite of close sent3: as the distance from an object increases , the force of gravity on that object will decrease sent4: brightness means amount of light sent5: if two or more things are in a relationship then those things impact each other sent6: feature is synonymous with characteristic ... sent25: moving away from the source increases the distance</p>
<p>Figure 2: Enhanced hard negative construction is implemented by exploring the unseen combination of premises, inferencing with the reasoner, and filtering with a score from the checker.</p>
<p>Gold</p>
<p>Proof: sent1 &amp; sent13 -&gt; int1: a star is a source of light; int1 &amp; sent21 &amp; sent23 -&gt; hypothesis; LLaMA-3.1-1B(w/ostepwise): sent13 &amp; sent1 -&gt; int1: stars are a source of light; int1 &amp; sent21 -&gt; int2: as the distance of a star to earth decreases, the star will appear brighter; int2 &amp; sent7 -&gt; hypothesis; LLaMA-3.1-1B(stepwise):sent21 &amp; sent3 -&gt; hypothesis; Flan-T5-Large(stepwise): sent1 &amp; sent13 -&gt; int1: a star is a source of light and produces light; sent17 &amp; int1 &amp; sent21 -&gt; hypothesis;</p>
<p>Table 1 :
1
Comparison of methods over natural language proof generation.Stepwise Correction means that if stepwise generation is enhanced in training.Stage calculates if the method contains generation, verification, and search.
MethodStepwiseStepwiseDirection Search Verifier Human-authoredStageGenerationCorrectionBenchmarkEntailmentWriter (Dalvi et al., 2021) ✗✗→✗✗✓1IRGR (Ribeiro et al., 2022)✓✗→✓✗✓2SCSearch (Bostrom et al., 2022)✓✗→✓✗✗2MetGen (Hong et al., 2022)✓✗both✓✗✓2ADGV (Sprague et al., 2022)✓✗both✓✓✓3NLProofs (Yang et al., 2022)✓✗→✓✓✓3ConDec✓✓→✗✗✓1</p>
<p>Enhanced hard negative 𝐬𝐭𝐞𝐩 𝟐 : Stepwise output 𝐬𝐭𝐞𝐩 𝟐 : Proof tree
Hypothesis 𝒉:The sun will be the star that appears the brightest to the earth;(𝐓):ℎ: the sun will be the star that appears the brightest to theearthsent1: the sun is the star that is closestFacts 𝑪:to earth sent2: the four planets farthest from the sent3: far means great in distance sun are made of gasint2: as the stars become closer, the light of the stars will appear brightersent1: the sun is the star that is closest to earthsent4: furthest / farthest means greatest / most / highest in distance sent5: to be in the sun means to be inint1: stars are a source of lightsent11: as a source of light becomes closer, the light will appear brighterthe sunlightsent6: appear is similar to apparentsent7: brightness means amount of lightsent13: a source ofsent16: a star……something produces lightproduces lightint1 &amp; sent11 -&gt; int2: as the stars become$hypothesis$=the sun will be thecloser, the light of the stars will appear brighter;star that appears the brightest tothe earth; $facts$=sent1: the sun is the starVanilla hard negative 𝐬𝐭𝐞𝐩 𝟐 :that is closest to earth sent2: thefour …;int1 &amp; sent11 -&gt; int2: as a source of light$partial_proof$=sent13 &amp; sent16becomes closer , the light will appear brighter;-&gt; int1: stars are a source of lightsent11 &amp; sent1 -&gt; int2: the sun will be the starthat appear the brightest to the earth;</p>
<p>Input Output Stepwise Decoding with Contrastive Learning (ConDec) Task Description Encoder Decoder
I𝐧𝐩𝐮𝐭 𝒙 𝐟𝐨𝐫 𝟐𝐧𝐝 𝐬𝐭𝐞𝐩 𝐠𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧:𝐌𝐇Projection layer𝐳 !𝐙 " !𝐳 "𝐙 " !Figure 1: Architecture of the stepwise decoding with contrastive learning over hard negatives. The hard negativesare constructed by vanilla and enhanced strategies. Vanilla strategy means simple conclusion substitution. Theenhanced strategy uses a reasoner and a checker to generate hard negatives.et al., 2022) proposes to abductively infer a premisegiven another premise and a conclusion, as wellas to search over two fingers interleaving deduc-tive (forward-chaining) and abductive (backward-chaining) inferences.</p>
<p>node sent 1 and intermediate node int 1 .The premise of a reasoning step can be from leaf nodes or intermediate nodes.Under the stepwise generation setting, intermediate nodes and proof steps up to the current step are added to given facts as input to generate new intermediate nodes for the next step.</p>
<p>Table 2 :
2
Distribution of samples from training data, constructed enhanced hard negatives and filtered hard negatives on Task 1 and Task 2. In filtering, threshold score is 0.9.
Reasoner Enhanced Negative Filtered Negative8,819Task 1 47,386 104,665 16,371 21,948 Task 2 Task 1 Task 2</p>
<p>Table 3 :
3
Main results on EntailmentBank with finetuning method.Methods with * are three-stage.ConDec denotes finetuning with vanilla hard negatives and ConDec ⋆ denotes finetuning with combination of vanilla and enhanced hard negatives.Best results are boldface and second-best results are underlined.
TaskMethodF1Leaves AllCorrect F1 AllCorrect F1 AllCorrect AllCorrect Steps Intermediates OverallTask 1EntailmentWriter98.786.250.537.767.636.233.5(no-distractor) EntailmentWriter (11B) 99.089.451.538.271.238.535.3MetGen  <em>100.0100.057.741.970.839.236.5NLProofs  </em>97.890.155.642.372.440.638.9ConDec99.998.255.742.172.338.936.2ConDec ⋆99.998.257.343.272.941.537.9Task 2EntailmentWriter84.335.635.522.961.828.520.9(distractor)EntailmentWriter (11B) 89.148.841.427.766.231.525.6MetGen  <em>82.746.141.329.661.432.427.7NLProofs  </em>90.358.847.234.470.237.833.3ConDec91.059.150.236.570.338.234.1ConDec ⋆91.160.650.737.470.738.234.7Task 3EntailmentWriter35.72.96.12.433.47.72.4(full-corpus) EntailmentWriter (11B) 39.93.87.42.935.97.12.9MetGen  <em>34.88.79.88.636.620.48.6NLProofs  </em>43.28.211.26.942.917.36.9ConDec43.38.211.16.543.418.06.5ConDec ⋆44.79.411.77.142.317.77.1over proof planning.ConDec is more efficient with distractors. Specif-ically, contrastive learning with hard negativesachieves obvious performance gain over Task 2and Task 3 while deteriorating the performance onTask 1. For Task 2 and Task 3, there are distractorpremises or full-corpus, which increases the chal-lenges for rigorous reasoning. For Task 1, there isno distractor. Adding hard negatives deviates thegenerator from generating correct steps instead ofintermediate nodes.Predicting intermediate node is still challenging.</p>
<p>Table 4 :
4
Computation time comparison(percentage reduction).Time denotes the inference time.NLProofs requires additional search and verification for prediction.</p>
<p>Table 5 :
5
Ablation study results on EntailmentBank test set in Task 2.
MethodLeaves F1 AllCorrect F1 AllCorrect F1 AllCorrect AllCorrect Steps Intermediates OverallLLaMA-3.2-1B (1.2B, w/o stepwise)19.55.36.42.913.95.32.7LLaMA-3.2-1B (1.2B, stepwise)15.63.54.13.29.95.03.2Flan-T5-Large (0.8B, stepwise)90.758.849.236.269.636.833.5+ contrastive loss90.960.349.535.969.437.132.4+ contrastive loss, vanilla hard91.059.150.236.570.338.234.1+ contrastive loss, vanilla and enhanced hard 91.160.650.737.470.738.234.7Flan-T5-XL (3B, stepwise)90.957.150.236.568.835.933.8MethodF1Leaves AllCorrect F1Steps AllCorrect F1Intermediates AllCorrect AllCorrect OverallConDec ⋆92.463.155.345.572.843.941.2GPT3 (5-shot)64.215.317.612.353.622.312.3GPT3.5-turbo (5-shot)61.99.016.94.351.915.13.74GPT4 (5-shot)78.132.630.222.563.930.821.9GPT4 (SI)79.130.024.314.065.432.113.9GPT4 (CoT)79.033.730.922.564.731.622.5GPT4o-mini (5-shot)63.118.220.013.954.524.113.9o1-mini (5-shot)72.527.328.613.950.918.711.8o1-preview (5-shot)84.043.338.728.067.833.721.9</p>
<p>Table 6 :
6
Results on EntailmentBank validation set in Task 2 with finetuning and prompting methods.GPT3.5 is GPT3.5-turbo-0613.GPT4 is GPT4-0613.GPT4o-mini is gpt-4o-mini-2024-07-18.o1-mini is o1-mini-2024-09-12.o1-preview is o1-preview-2024-09-12.
encoder-decoder architecture of Flan-T5 processesdifferent texts. A case study comparing LLaMAand Flan-T5 is presented in the Appendix A.6.Model size. For the model size, we also try Flan-T5-XL (3B). Stepwise training over Flan-T5-Largeprovides a strong baseline. Though much largerin model size, Flan-T5-XL achieves similar per-formance over the metrics. It shows that proofplanning requires a higher level of understandingof deductive reasoning over multi-hops.Negative sample. ConDec without hard negativesimproves the leave accuracy while decreasing thestep accuracy, leading to a slight drop in the overallperformance. Adding vanilla or enhanced hard neg-atives can generally improve over all the metrics.</p>
<p>Table 7 :
7
Human check accuracy on enhanced hard negatives from reasoner and checker with different threshold scores.</p>
<p>Table 10 :
10
Ablation study of enhanced hard negative construction on EntailmentBank test set on Task 2.
MethodLeaves F1 AllCorrect F1 AllCorrect F1 AllCorrect AllCorrect Steps Intermediates OverallRandom 91.060.650.737.470.738.234.7BM2590.860.350.336.868.837.934.7</p>
<p>Table 11 :
11
Case study of LLaMA and Flan-T5.</p>
<p>https://github.com/allenai/entailment_bank</p>
<p>Cont: Contrastive neural text generation. Chenxin An, Jiangtao Feng, Kai Lv, Lingpeng Kong, Xipeng Qiu, Xuanjing Huang, Advances in Neural Information Processing Systems. 202235</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Natural language deduction through search over statement compositions. Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, Greg Durrett, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, 10.24963/ijcai.2020/537Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. the Twenty-Ninth International Joint Conference on Artificial Intelligence20202020</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, arXiv:2205.097122022arXiv preprint</p>
<p>Explaining answers with entailment trees. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, Peter Clark, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, arXiv:2210.00720Complexity-based prompting for multi-step reasoning. 2022arXiv preprint</p>
<p>Vinod Goel, Gorka Navarrete, Ira A Noveck, Jérôme Prado, The reasoning brain: the interplay between cognitive neuroscience and theories of reasoning. 2017</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Folio: Natural language reasoning with firstorder logic. 2022arXiv preprint</p>
<p>METGEN: A modulebased entailment tree generation framework for answer explanation. Ruixin Hong, Hongming Zhang, Xintong Yu, Changshui Zhang, 10.18653/v1/2022.findings-naacl.145Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Najoung Seyed Mehran Kazemi, Deepti Kim, Xin Bhatia, Deepak Xu, Ramachandran, arXiv:2212.13894Lambada: Backward chaining for automated reasoning in natural language. 2022arXiv preprint</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Contrastive learning with adversarial perturbations for conditional text generation. Seanie Lee, Dong Bok Lee, Sung Ju Hwang, International Conference on Learning Representations. 2020</p>
<p>Jiacheng Liu, Wenya Wang, Dianzhuo Wang, Noah A Smith, Yejin Choi, Hannaneh Hajishirzi, arXiv:2305.03695Vera: A general-purpose plausibility estimation model for commonsense statements. 2023arXiv preprint</p>
<p>Rlet: A reinforcement learning based approach for explainable qa with entailment trees. Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, arXiv:2301.13688The flan collection: Designing data and methods for effective instruction tuning. 2023arXiv preprint</p>
<p>Inductive logic programming: Theory and methods. Stephen Muggleton, Luc De, Raedt , The Journal of Logic Programming. 191994</p>
<p>Of brittleness and bottlenecks: Challenges in the creation of pattern-recognition and expert-system models. A Mark, Johan Musen, Van Der Lei, Machine intelligence and pattern recognition. Elsevier19887</p>
<p>Logical reasoning and learning. Terezinha Nunes, 2012</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Arkil Patel, Satwik Bhattamishra, Navin Goyal, arXiv:2103.07191Are nlp models really able to solve simple math word problems?. 2021arXiv preprint</p>
<p>Interpretable proof generation via iterative backward reasoning. Hanhao Qu, Yu Cao, Jun Gao, Liang Ding, Ruifeng Xu, 10.18653/v1/2022.naacl-main.216Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United States2022Association for Computational Linguistics</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Entailment tree explanations via iterative retrieval-generation reasoner. Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Rui Dong, Xiaokai Wei, Henghui Zhu, Xinchi Chen, Peng Xu, Zhiheng Huang, Andrew Arnold, Findings of the Association for Computational Linguistics: NAACL 2022. 2022</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Prover: Proof generation for interpretable reasoning over rules. Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, Mohit Bansal, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>FaiRR: Faithful and robust deductive reasoning over natural language. Soumya Sanyal, Harman Singh, Xiang Ren, 10.18653/v1/2022.acl-long.77Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland20221Association for Computational Linguistics</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, arXiv:2210.012402022arXiv preprint</p>
<p>Bleurt: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur Parikh, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Natural language deduction with incomplete information. Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022In NeurIPS</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov, arXiv:1502.05698Towards ai-complete question answering: A set of prerequisite toy tasks. 2015arXiv preprint</p>
<p>Worldtree v2: A corpus of sciencedomain structured explanations and inference patterns supporting multi-hop inference. Zhengnan Xie, Sebastian Thiem, Jaycie Martin, Elizabeth Wainwright, Steven Marmorstein, Peter Jansen, Proceedings of the Twelfth Language Resources and Evaluation Conference. the Twelfth Language Resources and Evaluation Conference2020</p>
<p>Dq-lore: Dual queries with low rank approximation re-ranking for in-context learning. Jiong Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang, ArXiv, abs/2310.029542023</p>
<p>Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, arXiv:2306.09841Are large language models really good logical reasoners? a comprehensive evaluation from deductive, inductive and abductive views. Jun Liu, and Erik Cambria. 2023arXiv preprint</p>
<p>Generating natural language proofs with verifier-guided search. Kaiyu Yang, Jia Deng, Danqi Chen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>k / f / c sent5: united states is located in the northern hemisphere ... sent9: when the seasons change from summer to fall / from summer to winter , the amount of daylight will decrease ... sent14: new york / new york state is a state located in the united states of america sent15: winter has the least sunlight ... sent22: the earth being tilted on its rotating axis causes seasons sent23: december is during the winter in the northern hemisphere. Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria, arXiv:2303.12023373 / 212 / 100Logical reasoning over natural language as knowledge representation: A survey. 2023arXiv preprintGold Proof: sent14 &amp; sent5 -&gt; int1: new york state is located in the northern hemisphere. int1 &amp; sent23 -&gt; int2: december is during the winter for new york state; int2 &amp; sent15 -&gt; hypothesis</p>
<p>&amp; sent22 -&gt; int1: December is during the winter in the northern hemisphere; int1 &amp; sent9 -&gt; int2: the amount of daylight will decrease during December; int2 &amp; sent15 -&gt; int3: December has the least sunlight. GPT3.5-turbo: sent23int3 &amp; sent14 -&gt; hypothesis</p>
<p>Context: sent1: identifying is similar to determining. sent2: if a fossil is of an organism that cannot be identified then that organism is probably extinct. sent3: discovering something usually requires seeing that something. sent4: a dinosaur is a kind of extinct animal. sent5: fossils can be used as evidence for the ancient environment. sent6: dead means not alive. ... sent25: fossils can be used to study the history of organisms and environments on earth Reasoning step by step and finally output the proof: From sent13 &amp; sent24. GPT4: sent14 &amp; sent5 -&gt; int1: New York state is located in the northern hemisphere; int1 &amp; sent23 -&gt; int2: December is during the winter in New York state; int2 &amp; sent15 -&gt; hypothesis. Table 8: Case study of vanilla prompt with GPT3.5-turbo and GPT4. Hypothesis: if a fossil of a bird cannot be identified then that kind of bird is probably extinct. we can infer that a bird is a kind of organism. int1</p>
<p>From int1 &amp; sent2, we can infer that if a fossil of a bird cannot be identified then that kind of bird is probably extinct. hypothesis</p>
<p>Proof: sent13 &amp; sent24 -&gt; int1: a bird is a kind of organism; int1 &amp; sent2 -&gt; hypothesis; Table 9: Prompting template of stepwise proof generation for GPT4 with COT. </p>            </div>
        </div>

    </div>
</body>
</html>
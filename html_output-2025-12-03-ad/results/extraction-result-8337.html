<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8337 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8337</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8337</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-277313478</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.19281v1.pdf" target="_blank">CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model</a></p>
                <p><strong>Paper Abstract:</strong> Proving Rubik's Cube theorems at the high level represents a notable milestone in human-level spatial imagination and logic thinking and reasoning. Traditional Rubik's Cube robots, relying on complex vision systems and fixed algorithms, often struggle to adapt to complex and dynamic scenarios. To overcome this limitation, we introduce CubeRobot, a novel vision-language model (VLM) tailored for solving 3x3 Rubik's Cubes, empowering embodied agents with multimodal understanding and execution capabilities. We used the CubeCoT image dataset, which contains multiple-level tasks (43 subtasks in total) that humans are unable to handle, encompassing various cube states. We incorporate a dual-loop VisionCoT architecture and Memory Stream, a paradigm for extracting task-related features from VLM-generated planning queries, thus enabling CubeRobot to independent planning, decision-making, reflection and separate management of high- and low-level Rubik's Cube tasks. Furthermore, in low-level Rubik's Cube restoration tasks, CubeRobot achieved a high accuracy rate of 100%, similar to 100% in medium-level tasks, and achieved an accuracy rate of 80% in high-level tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8337.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8337.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CubeRobot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CubeRobot (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language model and embodied-control framework introduced in this paper for solving 3x3 Rubik's Cube tasks via a dual-loop Vision-Chain-of-Thought architecture, a Memory Stream, and an Embodied-Projector to produce executable plans for a robot arm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CubeRobot</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>End-to-end multimodal VLM developed in this paper; uses a frozen language model connected to visual features through an Embodied-Projector, dual-loop (outer/inner) Vision-CoT for high- and low-level planning, and a Memory Stream for storing natural-language memories and embeddings to retrieve task-relevant information. Outputs embodied plans that are converted to robot executable actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Rubik's Cube (3x3)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>3D spatial manipulation puzzle requiring planning of face rotations and multi-step spatial transformations</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Input: simulator-captured state image of a 3x3 Rubik's Cube plus natural-language prompt. Tasks split into three difficulty levels (low: 15 single-step tasks; medium: 18 tasks requiring 9–12 moves; high: 10 tasks requiring 19–31 moves). CubeRobot is fine-tuned on the CubeCoT dataset and runs a dual-loop process: outer-loop decomposes tasks and schedules subtasks; inner-loop executes subtasks, producing Thought / Reasoning / Reflection outputs recorded in the Memory Stream. Outputs are embodied plans that are converted to RAPID scripts for an LR Mate 200iD robot in simulation and fed back via images to the inner-loop for reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Dual-loop Vision-CoT (outer-loop high-level planner, inner-loop executor with Thought/Reasoning/Reflection), Memory Stream (natural-language memory objects with timestamping and embedding-based retrieval using cosine similarity), Embodied-Projector to convert image features to soft visual prompts for the language model, iterative plan refinement and reflection, conversion to robot joint/cartesian commands for execution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Achieved 100% accuracy on low-difficulty tasks, 100% on medium-difficulty tasks, and 80% on high-difficulty tasks (task-level accuracy as reported in paper's Table 1 and Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High accuracy on medium and many high-level tasks indicates effective handling of multi-step 3D manipulations. Ablation study shows removing Dual-loop CoT or Memory Stream substantially reduces medium/high accuracy (Dual-loop CoT only: medium 38.89%, high 30%; Memory Stream ablation: medium 33.33%, high 20%; VLM-only: medium 27.78%, high 10%), providing experimental evidence that the planned multi-step symbolic/linguistic reasoning plus memory retrieval supports spatial reasoning and multi-step manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to multiple off-the-shelf VLMs in the paper's experiments (Table 1); CubeRobot outperformed all listed baseline VLMs, achieving 100/100/80 (low/medium/high) versus substantially lower results for other fine-tuned models and 0% for many un-fine-tuned models. Ablation comparisons show clear performance drops when removing dual-loop or memory components.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Did not achieve 100% on high-level tasks (80% success). Paper notes permission constraints prevented fine-tuning of some VLMs, and that some baseline models exhibited deadlocks in medium/high tasks. CubeRobot's remaining failures at high difficulty indicate limits in handling the most complex 19–31 move configurations; the paper does not provide per-instance failure diagnostics beyond aggregate accuracy. Real-world transfer beyond simulation (physical robot) is shown in simulation; no reported tested physical-robot success rates in real hardware in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8337.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8337.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniGPT-V2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniGPT-V2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language model described in prior work and evaluated (after fine-tuning) in this paper as a baseline for Rubik's Cube solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MiniGPT-V2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A VLM that uses a large language model as a unified interface for vision-language multi-task learning (reference provided in paper); in this work it was fine-tuned on the CubeCoT dataset for Rubik's Cube tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Rubik's Cube (3x3)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>3D spatial manipulation puzzle requiring multi-step rotation plans</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Fine-tuned on CubeCoT; evaluated by providing cube state images and prompts; outputs interpreted as solving steps and measured for task-level accuracy across low/medium/high difficulty splits.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard VLM decoding (as implemented in MiniGPT-V2) with fine-tuning on CubeCoT. No dual-loop Vision-CoT or Memory Stream (those are specific to CubeRobot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1 / text as showing improved low-difficulty performance compared to some other baselines; specific numbers mapped from table: low-level ≈ 73.33%, medium-level ≈ 22.22%, high-level ≈ 10%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Partial: improved low-level performance after fine-tuning indicates the model can map image states to single-step instructions but substantially lower medium/high accuracy (and deadlocks reported in medium/high) suggest limited multi-step spatial planning capability relative to CubeRobot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly to CubeRobot and other VLM baselines in Table 1; outperformed LLaVA(7b) and MiniGPT-4 on low-strategy tasks per text, but underperformed CubeRobot on all levels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Observed deadlocks in medium and high tasks during solution process; very low success on multi-step (medium/high) tasks despite fine-tuning (≈22.22% medium, 10% high), indicating failures in long-horizon planning for 3D manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8337.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8337.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLM baseline (an advanced VLM integrating strong language models) that was fine-tuned and evaluated on CubeCoT in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MiniGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A recent vision-language model that augment LLMs with visual encoders to perform multimodal tasks; in this paper it was fine-tuned on CubeCoT for Rubik's Cube solving evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Rubik's Cube (3x3)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>3D spatial manipulation puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Fine-tuned variant evaluated on simulator images and prompts across low/medium/high tasks; measured by task-level accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard VLM reasoning via fine-tuned sequence generation; no dual-loop CoT or Memory Stream components described for MiniGPT-4 in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported approximate performance (from Table 1): low-level ≈ 61.11%, medium-level ≈ 22.22%, high-level ≈ 10%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performs reasonably on low single-step tasks after fine-tuning but fails (low accuracy) on medium/high multi-step tasks, suggesting limited multi-step spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms CubeRobot; compared with MiniGPT-V2 and LLaVA(7b) in paper where MiniGPT-V2 is described as better for low tasks and all three (MiniGPT-V2, MiniGPT-4, LLaVA(7b)) deadlocked more often during medium/high tasks compared to CubeRobot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Deadlocks and low success rates on medium/high tasks; tends to produce overly complex solutions on single-step low tasks (reported for some un-fine-tuned systems), and even after fine-tuning still low multi-step performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8337.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8337.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA (7B variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language assistant (LLaVA) model evaluated in multiple sizes in this paper; the 7B variant was fine-tuned and evaluated on CubeCoT as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal VLM family (LLaVA) used as a visual-language baseline; the paper evaluates multiple size variants (7B, 13B, 34B) though only the 7B variant was fine-tuned for the CubeCoT evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Rubik's Cube (3x3)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>3D spatial manipulation puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Evaluated both unfine-tuned (other size variants) and fine-tuned (7B) versions on CubeCoT using simulator-captured cube images and prompts, measured by task-level accuracy across difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard VLM sequence generation; when fine-tuned, operates as a direct mapping from visual input + prompt to solving steps; no specialized dual-loop or memory components.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported approximate performance for the 7B fine-tuned variant: low-level ≈ 66.67%, medium-level ≈ 22.22%, high-level ≈ 10% (from Table 1 mapping). Larger unfine-tuned LLaVA variants (13B, 34B) achieved 0% in the paper's evaluations when not fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Fine-tuned 7B variant can solve some low-level tasks but fails on most medium/high tasks, indicating only limited spatial reasoning capacity for multi-step Rubik's Cube solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to MiniGPT-V2 and MiniGPT-4; all three fine-tuned baselines (MiniGPT-V2/4 and LLaVA(7B)) achieved nonzero low-level performance but much lower medium/high performance than CubeRobot; unfine-tuned larger LLaVA variants performed poorly (0%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Un-fine-tuned LLaVA variants (13B, 34B) produced 0% success. The fine-tuned 7B variant still achieved low success on multi-step tasks and experienced deadlocks in medium/high difficulty tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8337.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8337.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Un-fine-tuned VLMs (grouped)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Un-fine-tuned Vision-Language Models (examples: LLaVA 13B/34B, MiniGPT-4-Video, BLIP-2 variants, Gemini, GPT-4V, GIT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of publicly available vision-language models that the paper evaluated without fine-tuning on the CubeCoT Rubik's Cube tasks and found they largely failed under the experimental setup.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various un-fine-tuned VLMs (LLaVA 13B/34B, MiniGPT-4-Video, BLIP-2 (flan-t5-xl/xxl), Gemini ultra, GPT-4V, GIT, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Off-the-shelf vision-language models referenced and evaluated by the authors in their original (un-fine-tuned) form; architectures vary (e.g., large language model decoders with frozen encoders, gated cross-attention, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Rubik's Cube (3x3)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>3D spatial manipulation puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Provided cube state images and prompts, evaluated zero-shot (no fine-tuning) on CubeCoT tasks across difficulty levels; outputs interpreted as solving steps and assessed for accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Their standard pretrained VLM inference pipelines (no task-specific dual-loop CoT, memory, or CubeCoT fine-tuning applied in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as 0% accuracy across low, medium, and high difficulty when evaluated without fine-tuning (the paper explicitly states un-fine-tuned VLMs performed poorly, achieving 0% across all three difficulty levels).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No positive evidence; the paper reports that these un-fine-tuned VLMs failed to provide correct solutions and often produced overly complex or irrelevant outputs, indicating insufficient spatial reasoning for this specific multi-step 3D puzzle without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Contrasted with fine-tuned baselines (MiniGPT-V2/4, LLaVA(7B)) and CubeRobot; un-fine-tuned models uniformly underperformed (0%), showing the importance of task-specific fine-tuning and specialized architectures (dual-loop CoT + memory) for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Produced 0% success on CubeCoT tasks when not fine-tuned. Reported failure modes include producing overly complex solutions for single-step tasks, failing to respond usefully to prompts, and inability to perform multi-step planning required for cube restoration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning. <em>(Rating: 2)</em></li>
                <li>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. <em>(Rating: 2)</em></li>
                <li>Flamingo: a Visual Language Model for Few-Shot Learning. <em>(Rating: 1)</em></li>
                <li>The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision). <em>(Rating: 1)</em></li>
                <li>PaLM-E: An Embodied Multimodal Language Model. <em>(Rating: 2)</em></li>
                <li>Solving Rubik's Cube with a Robot Hand. <em>(Rating: 2)</em></li>
                <li>Benchmarking Robot Manipulation With the Rubik's Cube. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8337",
    "paper_id": "paper-277313478",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "CubeRobot",
            "name_full": "CubeRobot (this work)",
            "brief_description": "A vision-language model and embodied-control framework introduced in this paper for solving 3x3 Rubik's Cube tasks via a dual-loop Vision-Chain-of-Thought architecture, a Memory Stream, and an Embodied-Projector to produce executable plans for a robot arm.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CubeRobot",
            "model_description": "End-to-end multimodal VLM developed in this paper; uses a frozen language model connected to visual features through an Embodied-Projector, dual-loop (outer/inner) Vision-CoT for high- and low-level planning, and a Memory Stream for storing natural-language memories and embeddings to retrieve task-relevant information. Outputs embodied plans that are converted to robot executable actions.",
            "model_size": null,
            "puzzle_name": "Rubik's Cube (3x3)",
            "puzzle_type": "3D spatial manipulation puzzle requiring planning of face rotations and multi-step spatial transformations",
            "task_setup": "Input: simulator-captured state image of a 3x3 Rubik's Cube plus natural-language prompt. Tasks split into three difficulty levels (low: 15 single-step tasks; medium: 18 tasks requiring 9–12 moves; high: 10 tasks requiring 19–31 moves). CubeRobot is fine-tuned on the CubeCoT dataset and runs a dual-loop process: outer-loop decomposes tasks and schedules subtasks; inner-loop executes subtasks, producing Thought / Reasoning / Reflection outputs recorded in the Memory Stream. Outputs are embodied plans that are converted to RAPID scripts for an LR Mate 200iD robot in simulation and fed back via images to the inner-loop for reflection.",
            "mechanisms_or_strategies": "Dual-loop Vision-CoT (outer-loop high-level planner, inner-loop executor with Thought/Reasoning/Reflection), Memory Stream (natural-language memory objects with timestamping and embedding-based retrieval using cosine similarity), Embodied-Projector to convert image features to soft visual prompts for the language model, iterative plan refinement and reflection, conversion to robot joint/cartesian commands for execution.",
            "performance_metrics": "Achieved 100% accuracy on low-difficulty tasks, 100% on medium-difficulty tasks, and 80% on high-difficulty tasks (task-level accuracy as reported in paper's Table 1 and Table 2).",
            "evidence_of_spatial_reasoning": "High accuracy on medium and many high-level tasks indicates effective handling of multi-step 3D manipulations. Ablation study shows removing Dual-loop CoT or Memory Stream substantially reduces medium/high accuracy (Dual-loop CoT only: medium 38.89%, high 30%; Memory Stream ablation: medium 33.33%, high 20%; VLM-only: medium 27.78%, high 10%), providing experimental evidence that the planned multi-step symbolic/linguistic reasoning plus memory retrieval supports spatial reasoning and multi-step manipulation.",
            "comparisons": "Directly compared to multiple off-the-shelf VLMs in the paper's experiments (Table 1); CubeRobot outperformed all listed baseline VLMs, achieving 100/100/80 (low/medium/high) versus substantially lower results for other fine-tuned models and 0% for many un-fine-tuned models. Ablation comparisons show clear performance drops when removing dual-loop or memory components.",
            "limitations_or_failure_cases": "Did not achieve 100% on high-level tasks (80% success). Paper notes permission constraints prevented fine-tuning of some VLMs, and that some baseline models exhibited deadlocks in medium/high tasks. CubeRobot's remaining failures at high difficulty indicate limits in handling the most complex 19–31 move configurations; the paper does not provide per-instance failure diagnostics beyond aggregate accuracy. Real-world transfer beyond simulation (physical robot) is shown in simulation; no reported tested physical-robot success rates in real hardware in this paper.",
            "uuid": "e8337.0",
            "source_info": {
                "paper_title": "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MiniGPT-V2",
            "name_full": "MiniGPT-V2",
            "brief_description": "A vision-language model described in prior work and evaluated (after fine-tuning) in this paper as a baseline for Rubik's Cube solving.",
            "citation_title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning.",
            "mention_or_use": "use",
            "model_name": "MiniGPT-V2",
            "model_description": "A VLM that uses a large language model as a unified interface for vision-language multi-task learning (reference provided in paper); in this work it was fine-tuned on the CubeCoT dataset for Rubik's Cube tasks.",
            "model_size": null,
            "puzzle_name": "Rubik's Cube (3x3)",
            "puzzle_type": "3D spatial manipulation puzzle requiring multi-step rotation plans",
            "task_setup": "Fine-tuned on CubeCoT; evaluated by providing cube state images and prompts; outputs interpreted as solving steps and measured for task-level accuracy across low/medium/high difficulty splits.",
            "mechanisms_or_strategies": "Standard VLM decoding (as implemented in MiniGPT-V2) with fine-tuning on CubeCoT. No dual-loop Vision-CoT or Memory Stream (those are specific to CubeRobot).",
            "performance_metrics": "Reported in Table 1 / text as showing improved low-difficulty performance compared to some other baselines; specific numbers mapped from table: low-level ≈ 73.33%, medium-level ≈ 22.22%, high-level ≈ 10%.",
            "evidence_of_spatial_reasoning": "Partial: improved low-level performance after fine-tuning indicates the model can map image states to single-step instructions but substantially lower medium/high accuracy (and deadlocks reported in medium/high) suggest limited multi-step spatial planning capability relative to CubeRobot.",
            "comparisons": "Compared directly to CubeRobot and other VLM baselines in Table 1; outperformed LLaVA(7b) and MiniGPT-4 on low-strategy tasks per text, but underperformed CubeRobot on all levels.",
            "limitations_or_failure_cases": "Observed deadlocks in medium and high tasks during solution process; very low success on multi-step (medium/high) tasks despite fine-tuning (≈22.22% medium, 10% high), indicating failures in long-horizon planning for 3D manipulation.",
            "uuid": "e8337.1",
            "source_info": {
                "paper_title": "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MiniGPT-4",
            "name_full": "MiniGPT-4",
            "brief_description": "A VLM baseline (an advanced VLM integrating strong language models) that was fine-tuned and evaluated on CubeCoT in this paper.",
            "citation_title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.",
            "mention_or_use": "use",
            "model_name": "MiniGPT-4",
            "model_description": "A recent vision-language model that augment LLMs with visual encoders to perform multimodal tasks; in this paper it was fine-tuned on CubeCoT for Rubik's Cube solving evaluation.",
            "model_size": null,
            "puzzle_name": "Rubik's Cube (3x3)",
            "puzzle_type": "3D spatial manipulation puzzle",
            "task_setup": "Fine-tuned variant evaluated on simulator images and prompts across low/medium/high tasks; measured by task-level accuracy.",
            "mechanisms_or_strategies": "Standard VLM reasoning via fine-tuned sequence generation; no dual-loop CoT or Memory Stream components described for MiniGPT-4 in this work.",
            "performance_metrics": "Reported approximate performance (from Table 1): low-level ≈ 61.11%, medium-level ≈ 22.22%, high-level ≈ 10%.",
            "evidence_of_spatial_reasoning": "Performs reasonably on low single-step tasks after fine-tuning but fails (low accuracy) on medium/high multi-step tasks, suggesting limited multi-step spatial reasoning.",
            "comparisons": "Underperforms CubeRobot; compared with MiniGPT-V2 and LLaVA(7b) in paper where MiniGPT-V2 is described as better for low tasks and all three (MiniGPT-V2, MiniGPT-4, LLaVA(7b)) deadlocked more often during medium/high tasks compared to CubeRobot.",
            "limitations_or_failure_cases": "Deadlocks and low success rates on medium/high tasks; tends to produce overly complex solutions on single-step low tasks (reported for some un-fine-tuned systems), and even after fine-tuning still low multi-step performance.",
            "uuid": "e8337.2",
            "source_info": {
                "paper_title": "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLaVA (7B)",
            "name_full": "LLaVA (7B variant)",
            "brief_description": "A vision-language assistant (LLaVA) model evaluated in multiple sizes in this paper; the 7B variant was fine-tuned and evaluated on CubeCoT as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA (7B)",
            "model_description": "A multimodal VLM family (LLaVA) used as a visual-language baseline; the paper evaluates multiple size variants (7B, 13B, 34B) though only the 7B variant was fine-tuned for the CubeCoT evaluation.",
            "model_size": "7B",
            "puzzle_name": "Rubik's Cube (3x3)",
            "puzzle_type": "3D spatial manipulation puzzle",
            "task_setup": "Evaluated both unfine-tuned (other size variants) and fine-tuned (7B) versions on CubeCoT using simulator-captured cube images and prompts, measured by task-level accuracy across difficulty levels.",
            "mechanisms_or_strategies": "Standard VLM sequence generation; when fine-tuned, operates as a direct mapping from visual input + prompt to solving steps; no specialized dual-loop or memory components.",
            "performance_metrics": "Reported approximate performance for the 7B fine-tuned variant: low-level ≈ 66.67%, medium-level ≈ 22.22%, high-level ≈ 10% (from Table 1 mapping). Larger unfine-tuned LLaVA variants (13B, 34B) achieved 0% in the paper's evaluations when not fine-tuned.",
            "evidence_of_spatial_reasoning": "Fine-tuned 7B variant can solve some low-level tasks but fails on most medium/high tasks, indicating only limited spatial reasoning capacity for multi-step Rubik's Cube solutions.",
            "comparisons": "Compared to MiniGPT-V2 and MiniGPT-4; all three fine-tuned baselines (MiniGPT-V2/4 and LLaVA(7B)) achieved nonzero low-level performance but much lower medium/high performance than CubeRobot; unfine-tuned larger LLaVA variants performed poorly (0%).",
            "limitations_or_failure_cases": "Un-fine-tuned LLaVA variants (13B, 34B) produced 0% success. The fine-tuned 7B variant still achieved low success on multi-step tasks and experienced deadlocks in medium/high difficulty tasks.",
            "uuid": "e8337.3",
            "source_info": {
                "paper_title": "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Un-fine-tuned VLMs (grouped)",
            "name_full": "Un-fine-tuned Vision-Language Models (examples: LLaVA 13B/34B, MiniGPT-4-Video, BLIP-2 variants, Gemini, GPT-4V, GIT)",
            "brief_description": "A set of publicly available vision-language models that the paper evaluated without fine-tuning on the CubeCoT Rubik's Cube tasks and found they largely failed under the experimental setup.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Various un-fine-tuned VLMs (LLaVA 13B/34B, MiniGPT-4-Video, BLIP-2 (flan-t5-xl/xxl), Gemini ultra, GPT-4V, GIT, etc.)",
            "model_description": "Off-the-shelf vision-language models referenced and evaluated by the authors in their original (un-fine-tuned) form; architectures vary (e.g., large language model decoders with frozen encoders, gated cross-attention, etc.).",
            "model_size": null,
            "puzzle_name": "Rubik's Cube (3x3)",
            "puzzle_type": "3D spatial manipulation puzzle",
            "task_setup": "Provided cube state images and prompts, evaluated zero-shot (no fine-tuning) on CubeCoT tasks across difficulty levels; outputs interpreted as solving steps and assessed for accuracy.",
            "mechanisms_or_strategies": "Their standard pretrained VLM inference pipelines (no task-specific dual-loop CoT, memory, or CubeCoT fine-tuning applied in the paper).",
            "performance_metrics": "Reported as 0% accuracy across low, medium, and high difficulty when evaluated without fine-tuning (the paper explicitly states un-fine-tuned VLMs performed poorly, achieving 0% across all three difficulty levels).",
            "evidence_of_spatial_reasoning": "No positive evidence; the paper reports that these un-fine-tuned VLMs failed to provide correct solutions and often produced overly complex or irrelevant outputs, indicating insufficient spatial reasoning for this specific multi-step 3D puzzle without task-specific fine-tuning.",
            "comparisons": "Contrasted with fine-tuned baselines (MiniGPT-V2/4, LLaVA(7B)) and CubeRobot; un-fine-tuned models uniformly underperformed (0%), showing the importance of task-specific fine-tuning and specialized architectures (dual-loop CoT + memory) for these tasks.",
            "limitations_or_failure_cases": "Produced 0% success on CubeCoT tasks when not fine-tuned. Reported failure modes include producing overly complex solutions for single-step tasks, failing to respond usefully to prompts, and inability to perform multi-step planning required for cube restoration.",
            "uuid": "e8337.4",
            "source_info": {
                "paper_title": "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning.",
            "rating": 2,
            "sanitized_title": "minigptv2_large_language_model_as_a_unified_interface_for_visionlanguage_multitask_learning"
        },
        {
            "paper_title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.",
            "rating": 2,
            "sanitized_title": "minigpt4_enhancing_visionlanguage_understanding_with_advanced_large_language_models"
        },
        {
            "paper_title": "Flamingo: a Visual Language Model for Few-Shot Learning.",
            "rating": 1,
            "sanitized_title": "flamingo_a_visual_language_model_for_fewshot_learning"
        },
        {
            "paper_title": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision).",
            "rating": 1,
            "sanitized_title": "the_dawn_of_lmms_preliminary_explorations_with_gpt4vision"
        },
        {
            "paper_title": "PaLM-E: An Embodied Multimodal Language Model.",
            "rating": 2,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "Solving Rubik's Cube with a Robot Hand.",
            "rating": 2,
            "sanitized_title": "solving_rubiks_cube_with_a_robot_hand"
        },
        {
            "paper_title": "Benchmarking Robot Manipulation With the Rubik's Cube.",
            "rating": 2,
            "sanitized_title": "benchmarking_robot_manipulation_with_the_rubiks_cube"
        }
    ],
    "cost": 0.01614025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model
25 Mar 2025</p>
<p>Feiyang Wang 
Xiaomin Yu 
Wangyu Wu wangyu.wu@liverpool.ac.uk </p>
<p>Central South University Changsha
HunanChina</p>
<p>Great Bay University Guangdong
China</p>
<p>The University of Liverpool Liverpool
UK</p>
<p>CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model
25 Mar 20251C772228E95F428BD1F34146F5530DAC10.1145/3701716.3717565arXiv:2503.19281v1[cs.RO]Vision-Language ModelMultimodalityEmbodied AIRobotics and AI
Proving Rubik's Cube theorems at the high level represents a notable milestone in human-level spatial imagination and logic thinking and reasoning.Traditional Rubik's Cube robots, relying on complex vision systems and fixed algorithms, often struggle to adapt to complex and dynamic scenarios.To overcome this limitation, we introduce CubeRobot, a novel vision-language model (VLM) tailored for solving 3x3 Rubik's Cubes, empowering embodied agents with multimodal understanding and execution capabilities.We used the CubeCoT image dataset, which contains multiple-level tasks (43 subtasks in total) that humans are unable to handle, encompassing various cube states.We incorporate a dual-loop Vision-CoT architecture and Memory Stream, a paradigm for extracting task-related features from VLM-generated planning queries, thus enabling CubeRobot to independent planning, decision-making, reflection and separate management of high-and low-level Rubik's Cube tasks.Furthermore, in low-level Rubik's Cube restoration tasks, CubeRobot achieved a high accuracy rate of 100%, similar to 100% in medium-level tasks, and achieved an accuracy rate of 80% in high-level tasks.CCS Concepts• Computer systems organization → External interfaces for robotics.</p>
<p>Introduction</p>
<p>As VLMs have demonstrated remarkable performance across a wide range of natural language processing tasks [15-17, 22, 29, 46, 49, 53], we aim to explore their potential in more complex downstream applications, such as Rubik's Cube solving [10,32,58].The Rubik's Cube, as a 3D puzzle, serves as a comprehensive showcase of human intelligence, patience, and spatial reasoning skills.Beyond this, our primary objective is to emulate and comprehend the human approach to problem-solving, thereby gaining insights to enhance our algorithms and models.This pursuit transcends mere technological advancement; it signifies a profound appreciation and reverence for the intricacies of human thought processes.Existing multi-modal large language models (e.g., LLaVA [28], Flamingo [1], BLIP-2 [23], PaLM-E [14]) excel in processing language and image data.However, they still face challenges in comprehending complex 3D environments, exhibiting limitations in depth perception, object relationships, and spatial reasoning(Figure .1).Moreover, while current language models have demonstrated some long-chain reasoning capabilities, they still fall short of the capacity to independently solve highly complex tasks such as Rubik's Cube restoration.</p>
<p>1.We introduce CubeRobot, a Vision-Language model(VLM) tailored for solving the 3x3 Rubik's Cube.</p>
<ol>
<li>
<p>We concentrate on solving the 3x3 Rubik's Cube and have compiled a specialized dataset CubeCoT for this task.Additionally, we've established high, medium, and low-difficulty tasks to evaluate the model's solving capabilities.</p>
</li>
<li>
<p>We present a dual-loop process chain architecture, consisting of an outer-loop and an inner-loop, to enhance efficiency in handling multidimensional tasks.</p>
</li>
<li>
<p>We introduce Memory Stream, a novel memory system that logs natural language descriptions and timestamps, optimizing robots' decision-making and boosting CubeRobot's efficiency in solving Rubik's Cubes.</p>
</li>
</ol>
<p>Related Works</p>
<p>Large language models</p>
<p>Recent advancements in large language models (LLMs) have been fueled by increased training data and the expansion of model parameters.Initial models like BERT [12], GPT-2 [35], and T5 [36] laid the groundwork for this progress.The release of GPT-3 [3], with its impressive 175 billion parameters, marked a pivotal moment, achieving remarkable performance across a broad spectrum of language tasks.This milestone catalyzed the development of several other large-scale models, such as Megatron-Turing NLG [38], Chinchilla [18], PaLM [8], OPT [63], BLOOM [37], and LLaMA [44].Moreover, Wei et al. [47] identified certain emergent abilities that manifest only in large-scale models, emphasizing the critical role of scale in the evolution of these systems.Further advancements have come with the alignment of GPT-3 through human feedback and instructions, giving rise to InstructGPT [33] and ChatGPT [30], which enable interactive, human-like conversations and answer a wide variety of complex queries.Recently, models like Alpaca [40] and Vicuna [7], built on LLaMA [44], have been open-sourced and exhibit comparable performance levels.</p>
<p>LLMs in Vision-Language Tasks</p>
<p>In recent years, there has been a growing trend of utilizing autoregressive language models as decoders for vision-language tasks [19,21,26,27,43,50,[54][55][56], a strategy that has garnered significant attention [1, 4, 13, 20, 24, 25, 49, 51-53, 57, 61, 62].This method leverages cross-modal transfer, enabling shared knowledge between the language and multimodal domains.Early work, such as Visual-GPT [4] and Frozen [45], illustrated the advantages of using a pretrained language model as a vision-language decoder.Flamingo [1] introduced a novel approach by combining a pre-trained vision encoder with a language model through gated cross-attention, training it on billions of image-text pairs, and demonstrating impressive few-shot learning capabilities in context.BLIP-2 [24] later improved on this by integrating a Flan-T5 [9] model with a Q-Former to more efficiently align visual features with the language model.More recently, PaLM-E [13], with a staggering 562 billion parameters, advanced the integration of real-world sensory data into LLMs, bridging the gap between real-world perceptions and human language.Furthermore, the release of  has enhanced visual understanding and reasoning, demonstrating superior performance after training on a vast dataset of aligned image-text pairs.Large language models (LLMs), like ChatGPT, have significantly improved the performance of vision-language tasks when combined with specialized models.Visual ChatGPT [48] and MM-REACT [60] show how ChatGPT can act as an integrator, coordinating various visual foundation models to address more complex problems.In a similar vein, ChatCaptioner [64] uses ChatGPT as an interrogator, generating diverse questions that BLIP-2 answers.Through multiple conversation rounds, ChatGPT extracts visual details from BLIP-2 and summarizes the image content effectively.The Video ChatCaptioner [5] extends this idea, applying it to the spatiotemporal understanding of videos.ViperGPT [39] highlights the capability of combining an LLM with multiple vision models to solve intricate visual queries programmatically.In contrast, MiniGPT-4 directly integrates visual data with the language model, enabling it to perform various vision-language tasks without relying on external vision models.</p>
<p>Methodology</p>
<p>To tackle complex environmental perception and interaction tasks, we introduce a system integrating vision-language processing, a dual-loop Chain-of-Thought (CoT) architecture, and a memory stream mechanism.This system efficiently converts visual inputs into executable instructions and enables high-level control for specific tasks, supporting effective operation and decision-making in complex environments.</p>
<p>CubeRobot: Framework</p>
<p>Dual-loop VisionCoT</p>
<p>We believe that one has to consider the problem's unique components as well as its general viewpoint in order to tackle multidimensional projects efficiently [42].CubeRobot employs a dual-loop process chain architecture, as illustrated in Figure .3, with an outer-loop process is used for high-level job management and an inner-loop process is used for low-level task execution.The outer-loop process enables the CubeRobot to recognize and segment huge assignments into smaller, more manageable sections.On the other hand, the inner-loop process acts as the painstaking executor, focusing on the particulars of the assigned responsibilities.</p>
<p>Initial Action Generation: The CubeRobot starts by creating an initial plan that establishes the fundamental tactics for carrying out the assignment.This entails dividing a given difficult job  into more manageable, smaller subtasks.A CubeRobot is used to break down a large task into a number of smaller tasks, { 1 , • • • ,  }:</p>
<p>Iterative Action Refinements: CubeRobot advances by removing the first subtask from the task queue following the initial planning.The inner-loop is then given this subtask.CubeRobot keeps an eye on the state and progress of tasks all the time.The inner-loop retrieves    from the Memory Store and reports it following the completion of each subtask.CubeRobot initiates the proper handling procedures, such adjusting the plan or moving on to the next subtask, based on the reflection.The latest subtask's Memory Stream provides   .Until there are no more subtasks in the queue, the outer loop is completed.</p>
<p>Outer-Loop.The outer-loop serves as the high-level planner and the primary orchestrator of tasks, acting as a supervisor for the entire problem-solving sequence.Its responsibilities can be broken down as follows:</p>
<p>Inner-Loop.The inner-loop is pivotal for executing the individual subtasks assigned by the outer-loop.Given a subtask   , an appropriate action is designated, ensuring   reaches its intended outcome.For every action   , we integrate CubeRobot's reasoning process and the actual action into a single function call, meaning that both the action to be performed and the reasoning trail are passed as parameters of this function.</p>
<p>Thought: A brief summary of the CubeRobot's main realization of the circumstances.</p>
<p>Reasoning: Follows the CubeRobot's logical path to the formation of its notion.</p>
<p>Reflection: Serves as a reflection loop by recording the CubeRobot's introspection on its behaviors.It draws attention to any mistakes or possible areas for development.</p>
<p>Memory Stream</p>
<p>The memory stream maintains a comprehensive record of the robot's experience.It is a list of memory objects, where each object contains a natural language description, a creation timestamp, and a most recent access timestamp.The most basic element of the memory stream is an observation, which is an event directly perceived by a robot.Common observations include behaviors performed by the robots themselves or behaviors that robots perceive as being performed by other robots or non-robot objects [34].CubeRobot assigns a higher score to memory objects that were recently accessed so that events from a moment ago or this morning are likely to There are many possible implementations of an importance score; we find that directly asking the language model to output an integer score is effective.</p>
<p>In our implementation, we use the language model to generate an embedding vector of the text description of each memory.Then, we calculate relevance as the cosine similarity between the memory's embedding vector and the query memory's embedding vector.</p>
<p>EXPERIMENT</p>
<p>We obtained fundamental data from the website [11], and adopted a set of 15 simple basic instructions for solving Rubik's cubes.We outlined 28 distinct Rubik's cube-solving steps and used a Rubik's cube simulator to capture images of the cube in various states.</p>
<p>In our study, Rubik's cube-solving tasks are divided into three difficulty levels: low, medium, and high.At the low difficulty level, there are 15 independent tasks, each solvable in a single step by executing one of our defined basic instructions.The medium difficulty level includes 18 independent tasks that require between 9 and 12 precise moves to complete cube restoration.The high difficulty level consists of 10 extremely challenging independent tasks, ranging from a minimum of 19 to a potential maximum of 31 complex moves to successfully solve the Rubik's cube.</p>
<p>CubeRobot Manipulation Results</p>
<p>The experiments involve assessing the ability of various VLM models to solve a Rubik's cube using three tasks of increasing difficulty.By inputting the state image of the Rubik's cube along with a prompt, We obtained the accuracy of the various models.(Table.1).Due to permission constraints, some Visual Language Models  (VLMs) could not be fine-tuned.Notably, un-fine-tuned VLMs performed poorly in downstream Rubik's cube solving tasks, achieving 0% accuracy across all three levels of difficulty.Specifically, in lowlevel tasks, models like LLaVA(34b) [28] and MiniGPT-4Video [2] tended to provide overly complex solutions for problems that could be solved a single step, leading to 0% accuracy.On the other hand, models such as Gemini [41], blip2 [23], and GPT-4V [59] failed to provide effective responses based on the given prompts.After fine-tuning, MiniGPT-V2 [6] showed significantly improved performance in low-strategy tasks compared to LLaVA(7b) [28] and MiniGPT-4 [65].In contrast, CubeRobot excelled in low-strategy tasks, achieving a 100% accuracy.In more complex medium-level and high-level Rubik's Cube challenges, MiniGPT-V2 [6], LLaVA(7b) [28], and MiniGPT-4 [65] encountered deadlocks during the solution process, leading to lower accuracy.However, CubeRobot maintained its outstanding performance, achieving 100% accuracy in the medium-level task and an impressive 80% accuracy in the high-level task.</p>
<p>Virtual Environment Simulation of CubeRobot</p>
<p>We conduct simulation experiments in a virtual environment, with the assistance of the LR Mate 200iD robotic arm.Figure .4 illustrates the process of CubeRobot to complete high-level tasks.</p>
<p>We developed an interface that translates the output of CubeRobot into a script written in the RAPID language supported by RoboGuide.This script contains specific commands and actions required for the robotic arm to solve each step of the Rubik's cube puzzle.The translation process involves converting the abstract steps generated by CubeRobot into joint angles and Cartesian coordinates that the LR Mate 200iD robotic arm can understand and execute.To monitor progress and verify the correctness of the solution, we capture real-time images after CubeRobot completes each subtask.We set up a virtual camera within RoboGuide, positioning it optimally to observe both the robotic arm and the Rubik's cube.Captured images are fed back into CubeRobot's Inner-Loop, which retrieves the "reflection" from the memory store and reports it to CubeRobot.</p>
<p>Ablation study</p>
<p>We conduct ablation studies to analyze the effectiveness of the inner and outer loops in the Dual-loop CoT architecture, as well as the necessity of the memory stream for model performance.</p>
<p>As shown in Table .2, the experimental results indicate that the introduction of the memory mechanism significantly improves the accuracy of the Rubik's Cube solving task compared to the Dualloop CoT-Only model.Among the memory retrieval mechanisms, the recency, importance, and relevance scores play crucial roles in enabling the model to perform complex tasks.Moreover, the interaction between the inner and outer loops of the Dual-loop CoT architecture is essential for task decomposition and integration, which is critical for managing task execution.</p>
<p>CONCLUSION</p>
<p>In this work, we present CubeRobot, an end-to-end multimodal embedded AI foundation model that enables robots to achieve the separation of high-level planning and low-level task execution.To this end, we create a Rubik's Cube dataset named CubeCoT and introduce the Dual-loop CoT , which plays a crucial role in task decomposition and integration through the interaction process between inner and outer loops.Furthermore, this innovative approach not only streamlines task processing but also significantly improves the robot's ability to learn from and adjust to new challenges.</p>
<p>Figure 1 :
1
Figure 1: Comparison of Rubik's Cube Solving Performance.</p>
<p>Figure. 2
2
Figure.2 above shows the framework of CubeRobot.The Embodied-Projector, represented by the symbol  (•), functions as an information bottleneck that connects the frozen language model with the visual input   .It does this by supplying the language model with the most pertinent visual data.The Embodied-Projector is composed of two sub-modules:   :   →   , which extracts features from the picture input, and   :   →   , which extracts features from the text input.In order to interact with   through cross-attention layers and with   through self-attention layers, we use  learnable Action query embeddings    as the input of .A mapping function represented as  :  →  ′ , carries out this transformation using a linear projection across a fully connected (FC) layer.By acting as "soft visual prompts for the language model, " the projected embeddings,  ′ , decouple the whole interaction into visual-query and query-text interactions.Using  ′ and the text prompt as input, the language model infers the final embodied planning.The embodied plan   is utilized as the input text for Embodied-Projector to query the task-relevant instance level characteristics   =  (  ,   ,    ) for high-level control, which seeks to create actions to interact with the environment.A pre-trained CubeRobot model on ViT is used to infer the global context.The output of the policy network consists of specific executable actions, such as positions and velocities in the Cartesian coordinate system.</p>
<p>Figure 2 :
2
Figure2: Framework of CubeRobot.The orange arrow shows the vision-language planning process, while the gray arrow represents that we leverage the queried language plans for better policy learning in Rubik's Cube Manipulation tasks.</p>
<p>Figure 3 :
3
Figure 3: Dual-loop CoT.The outer-loop manages high-level tasks, including initial action planning and iterative refinements, while tracking task progress.The inner-loop executes specific sub-tasks assigned by the outer-loop, employing thought, reasoning, and reflection.remain in the robot's attentional sphere.For instance, a mundane event, such as observing all sides of the Cube yield a high score.There are many possible implementations of an importance score; we find that directly asking the language model to output an integer score is effective.In our implementation, we use the language model to generate an embedding vector of the text description of each memory.Then, we calculate relevance as the cosine similarity between the memory's embedding vector and the query memory's embedding vector.</p>
<p>Figure 4 :
4
Figure 4: Visualization results of CubeRobot.We accurately emulated the movements of the CubeRobot equipped with LR Mate 200iD robotic arm.</p>
<p>Table 1 :
1
Accuracy of various Visual Language Models for Rubik's Cube solving tasks
llava(13b) llava(34b)MiniGPT4-Videoblip2-flan -t5-xlblip2-flan -t5-xxlGemini ultraGPT-4V GIT MiniGPT-V2 MiniGPT-4 llava(7b)CubeRobot (ours)Task finetune✖✖✖✖✖✖✖✖✔✔✔✔low-level0000000073.33%61.11% 66.67%100%medium-level0000000022.22%22.22% 22.22%100%high-level0000000010%10%10%80%</p>
<p>Table 2 :
2
Ablation Analysis on Rubik's Cube Tasks
VersionLow-level medium-level high-levelCubeRobot100%100%80%-Dual-loop CoT100%38.89%30%-Memory Stream100%33.33%20%VLM only100%27.78%10%</p>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan, arXiv:2204.14198[cs.CV]Flamingo: a Visual Language Model for Few-Shot Learning. 2022</p>
<p>Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, Mohamed Elhoseiny, arXiv:2404.03413[cs.CVMiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens. 2024</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Visualgpt: Data-efficient adaptation of pretrained language models for image captioning. Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, Mohamed Elhoseiny, arXiv:2304.04227Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions. 2023. 2023arXiv preprint</p>
<p>Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny, arXiv:2310.09478[cs.CV]MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning. 2023</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. </p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022. 2022arXiv preprint</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022. 2022arXiv preprint</p>
<p>Solving Rubik's cube via quantum mechanics and deep reinforcement learning. Sebastiano Corli, Lorenzo Moro, Davide E Galli, Enrico Prati, 10.1088/1751-8121/ac2596Journal of Physics A: Mathematical and Theoretical. 544253022021. Sept. 2021</p>
<p>Ruwix Twisty Puzzle Wiki, More Rubik's Patterns. Denes Ferenc, 2012. 2024-5-22</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018. 2018arXiv preprint</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378Palm-e: An embodied multimodal language model. 2023. 2023arXiv preprint</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Yevgen Huang, Pierre Chebotar, Daniel Sermanet, Sergey Duckworth, Vincent Levine, Karol Vanhoucke, Marc Hausman, Klaus Toussaint, Andy Greff, Igor Zeng, Pete Mordatch, Florence, arXiv:2303.03378[cs.LG]PaLM-E: An Embodied Multimodal Language Model. 2023</p>
<p>Zhizhao Duan, Hao Cheng, Duo Xu, Xi Wu, Xiangxie Zhang, Xi Ye, Zhen Xie, arXiv:2405.03194[cs.CV]CityLLaVA: Efficient Fine-Tuning for VLMs in City Scenario. 2024</p>
<p>Dual-Hybrid Attention Network for Specular Highlight Removal. Xiaojiao Guo, Xuhang Chen, Shenghong Luo, Shuqiang Wang, Chi-Man Pun, ACM MM. 2024</p>
<p>Underwater Image Restoration Through a Prior Guided Hybrid Sense Approach and Extensive Benchmark Analysis. Xiaojiao Guo, Xuhang Chen, Shuqiang Wang, Chi-Man Pun, IEEE TCSVT. 2025. 2025</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556Training compute-optimal large language models. 2022. 2022arXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, arXiv:2302.140452023. 2023arXiv preprint</p>
<p>EDSF: Fast and accurate ellipse detection via disjoint-set forest. Jingen Jiang, Mingyang Zhao, Zeyu Shen, Dong-Ming Yan, 2022 IEEE International Conference on Multimedia and Expo (ICME). IEEE2022</p>
<p>Structure-aware surface reconstruction via primitive assembly. Jingen Jiang, Mingyang Zhao, Shiqing Xin, Yanchao Yang, Hanxiao Wang, Xiaohong Jia, Dong-Ming Yan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Harnessing the Power of Large Vision Language Models for Synthetic Image Detection. Mamadou Keita, Wassim Hamidouche, Hassen Bougueffa, Abdenour Hadid, Abdelmalik Taleb-Ahmed, arXiv:2404.02726[cs.CV]2024</p>
<p>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, ICML. 2023</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.125972023. 2023arXiv preprint</p>
<p>Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International Conference on Machine Learning. PMLR2022</p>
<p>Real-time idling vehicles detection using combined audio-visual deep learning. Xiwen Li, Tristalee Mangin, Surojit Saha, Rehman Mohammed, Evan Blanchard, Dillon Tang, Henry Poppe, Ouk Choi, Kerry Kelly, Ross Whitaker, Emerging Cutting-Edge Developments in Intelligent Traffic and Transportation Systems. IOS Press2024</p>
<p>Joint audio-visual idling vehicle detection with streamlined input dependencies. Xiwen Li, Rehman Mohammed, Tristalee Mangin, Surojit Saha, Ross T Whitaker, Kerry E Kelly, Tolga Tasdizen, arXiv:2410.211702024. 2024arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, arXiv:2310.03744[cs.CV]Improved Baselines with Visual Instruction Tuning. 2023</p>
<p>GameVLM: A Decision-making Framework for Robotic Task Planning Based on Visual Language Models and Zero-sum Games. Aoran Mei, Jianhua Wang, Guo-Niu Zhu, Zhongxue Gan, arXiv:2405.13751[cs.RO]2024</p>
<p>Introducing ChatGPT. 2022. 2022OpenAI</p>
<p>Solving Rubik's Cube with a Robot Hand. Ilge Openai, Marcin Akkaya, Maciek Andrychowicz, Mateusz Chociej, Bob Litwin, Arthur Mcgrew, Alex Petron, Matthias Paino, Glenn Plappert, Raphael Powell, Jonas Ribas, Nikolas Schneider, Jerry Tezak, Peter Tworek, Lilian Welinder, Qiming Weng, Wojciech Yuan, Lei Zaremba, Zhang, arXiv:1910.07113[cs.LG]2019</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Sung Joon, Joseph C Park, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442[cs.HC]Generative Agents: Interactive Simulacra of Human Behavior. 2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 192019. 2019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020. 2020</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, arXiv:2211.05100Bloom: A 176b-parameter open-access multilingual language model. 2022. 2022arXiv preprint</p>
<p>Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, arXiv:2201.119902022. 2022arXiv preprint</p>
<p>ViperGPT: Visual Inference via Python Execution for Reasoning. Dídac Surís, Sachit Menon, Carl Vondrick, arXiv:2303.081282023. 2023arXiv preprint</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford Alpaca: An Instruction-following LLaMA model. 2023</p>
<p>Gemini Team, arXiv:2312.11805[cs.CLGemini: A Family of Highly Capable Multimodal Models. 2024</p>
<p>XAgent: An Autonomous Agent for Complex Task Solving. Xagent Team, 2023</p>
<p>Plug-and-play vqa: Zero-shot vqa by conjoining large pretrained models with zero training. Anthony Meng, Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, Steven Ch Hoi, arXiv:2210.087732022. 2022arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Multimodal few-shot learning with frozen language models. Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, Oriol Eslami, Felix Vinyals, Hill, Advances in Neural Information Processing Systems. 342021. 2021</p>
<p>Beyond Human Vision: The Role of Large Vision Language Models in Microscope Image Analysis. Prateek Verma, Minh-Hao Van, Xintao Wu, arXiv:2405.00876[cs.CV]2024</p>
<p>Emergent Abilities of Large Language Models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Transactions on Machine Learning Research. 2022. 2022</p>
<p>Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.04671Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models. 2023. 2023arXiv preprint</p>
<p>Adaptive Patch Contrast for Weakly Supervised Semantic Segmentation. Wangyu Wu, Tianhong Dai, Zhenhong Chen, Xiaowei Huang, Jimin Xiao, Fei Ma, Renrong Ouyang, EAAI. 1391096262025. 2025</p>
<p>Image Augmentation with Controlled Diffusion for Weakly-Supervised Semantic Segmentation. Wangyu Wu, Tianhong Dai, Xiaowei Huang, Fei Ma, Jimin Xiao, ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2024</p>
<p>Image Augmentation with Controlled Diffusion for Weakly-Supervised Semantic Segmentation. Wangyu Wu, Tianhong Dai, Xiaowei Huang, Fei Ma, Jimin Xiao, ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2024</p>
<p>Top-k pooling with patch contrastive learning for weakly-supervised semantic segmentation. Wangyu Wu, Tianhong Dai, Xiaowei Huang, Fei Ma, Jimin Xiao, 2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC). IEEE2024</p>
<p>Prompt Categories Cluster for Weakly Supervised Semantic Segmentation. Wangyu Wu, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao, arXiv:2412.138232024. 2024arXiv preprint</p>
<p>Are people located in the places they mention in their tweets? a multimodal approach. Zhaomin Xiao, Eduardo Blanco, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022</p>
<p>Short interest trend prediction with large language models. Zhaomin Xiao, Zhelu Mai, Yachen Cui, Zhuoer Xu, Jiancheng Li, Proceedings of the 2024 International Conference on Innovation in Artificial Intelligence. the 2024 International Conference on Innovation in Artificial Intelligence2024</p>
<p>Corporate event predictions using large language models. Zhaomin Xiao, Zhelu Mai, Zhuoer Xu, Yachen Cui, Jiancheng Li, 2023 10th International Conference on Soft Computing &amp; Machine Intelligence (ISCMI). IEEE2023</p>
<p>Zero-shot video question answering via frozen bidirectional language models. Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid, arXiv:2206.081552022. 2022arXiv preprint</p>
<p>Benchmarking Robot Manipulation With the Rubik's Cube. Boling Yang, Patrick E Lancaster, Siddhartha S Srinivasa, Joshua R Smith, 10.1109/lra.2020.2969912IEEE Robotics and Automation Letters. 522020. April 2020</p>
<p>Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, arXiv:2309.17421[cs.CV]The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision). 2023</p>
<p>MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action. Zhengyuan Yang, * , Linjie Li, * , Jianfeng Wang, * , Kevin Lin, * , Ehsan Azarnasab, * , Faisal Ahmed, * , Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, 2023. 2023</p>
<p>Jiawei Yao, Chuming Li, Canran Xiao, arXiv:2410.05578Swift Sampler: Efficient Learning of Sampler by 10 Parameters. 2024. 2024arXiv preprint</p>
<p>Augdmc: Data augmentation guided deep multiple clustering. Jiawei Yao, Enbei Liu, Maham Rashid, Juhua Hu, Procedia Computer Science. 2222023. 2023</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022. 2022arXiv preprint</p>
<p>Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, Mohamed Elhoseiny, arXiv:2303.06594ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions. 2023. 2023arXiv preprint</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592[cs.CV]MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
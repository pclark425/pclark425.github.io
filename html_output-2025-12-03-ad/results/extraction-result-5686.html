<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5686 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5686</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5686</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-eecb4dbf218d08d43a727c7e79f86a296502f117</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/eecb4dbf218d08d43a727c7e79f86a296502f117" target="_blank">Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</a></p>
                <p><strong>Paper Venue:</strong> Journal of Medical Internet Research</p>
                <p><strong>Paper TL;DR:</strong> This paper summarizes the current state of research about prompt engineering and aims at providing practical recommendations for the wide range of health care professionals to improve their interactions with LLMs.</p>
                <p><strong>Paper Abstract:</strong> Prompt engineering is a relatively new field of research that refers to the practice of designing, refining, and implementing prompts or instructions that guide the output of large language models (LLMs) to help in various tasks. With the emergence of LLMs, the most popular one being ChatGPT that has attracted the attention of over a 100 million users in only 2 months, artificial intelligence (AI), especially generative AI, has become accessible for the masses. This is an unprecedented paradigm shift not only because of the use of AI becoming more widespread but also due to the possible implications of LLMs in health care. As more patients and medical professionals use AI-based tools, LLMs being the most popular representatives of that group, it seems inevitable to address the challenge to improve this skill. This paper summarizes the current state of research about prompt engineering and, at the same time, aims at providing practical recommendations for the wide range of health care professionals to improve their interactions with LLMs.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5686.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5686.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt specificity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Be as Specific as Possible (Prompt specificity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recommendation that more specific prompts yield more accurate and focused responses from LLMs; illustrated by contrasting a vague prompt with a narrowly scoped question.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Medical information retrieval / question answering (illustrative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Example tasks where users ask the model for information about medical topics (e.g., 'heart disease' vs 'risk factors for coronary artery disease').</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Comparison of a vague/general prompt (e.g., 'Tell me about heart disease') versus a specific/focused prompt (e.g., 'What are the most common risk factors for coronary artery disease?'), i.e., increasing prompt specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Less specific (vague) prompt vs more specific (focused) prompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper states that more specific prompts lead to more accurate and focused responses because they constrain the model's output space and clarify the user's intent.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5686.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context/setting provision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Describe the Setting and Provide Context</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recommendation to include situational context in prompts (e.g., purpose, audience, role of user) so the LLM can tailor responses appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Contextualized content generation (e.g., article writing, medical advice phrasing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where output depends on the user's situation or intent (e.g., producing tips for healthcare professionals writing about prompt engineering).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Providing explicit context/setting in the prompt (e.g., 'I'm writing an article about tips... Can you list those tips with examples?') versus omitting context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>No context vs provided context/setting</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Including context helps the model produce responses that are better aligned with the user's needs because the model can condition its output on the provided scenario and constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5686.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experiment with Different Prompt Styles (Direct question / List / Summary / Process)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper recommends varying prompt style (direct question, list request, summary, step-by-step process) because style influences the structure and content of model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Information elicitation and structuring (e.g., listing symptoms, summarizing disease progression, procedural steps)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where the user requests information in different organizational formats: enumerated lists, concise summaries, or procedural steps.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Different prompt formulations: direct question ('What are the symptoms?'), request for list ('List all potential symptoms'), request for summary ('Summarize key symptoms and progression'), request for process ('Provide a step-by-step process of diagnosing...').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct question vs list vs summary vs step-by-step process</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (format-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper suggests that the chosen format guides the model's output organization and level of detail; e.g., asking explicitly for a list yields enumerated items, while a process prompt encourages sequential steps.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5686.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goal-oriented prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Identify the Overall Goal of the Prompt First</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recommendation to state the desired output type and purpose (e.g., creative ideas, technical description) to help the model produce more relevant responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Output-targeted generation (e.g., ideation for presentations, explanations at a target depth)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where the form of the desired output matters (e.g., 'list of 5 ideas for a presentation').</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt explicitly stating the overall goal (e.g., 'I'd like a list of 5 ideas for a presentation...') versus leaving goal implicit.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Implicit goal vs explicit goal specification</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Explicitly defining the goal narrows the model's response space and helps it focus on the appropriate style, scope, and level of detail.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5686.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Role prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ask it to Play Roles (Role prompting / persona)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Technique to ask the model to assume a professional role (e.g., 'Act as a Data Scientist') so responses are framed with that expertise and style.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Role-specific explanations and advice (e.g., tutoring, nutrition advice, data science explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where adopting a specific persona helps tailor tone, depth, and content (e.g., 'Act as my nutritionist...').</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instruction to adopt a role/persona before asking the question.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>No role specification vs explicit role specification</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Role prompts provide an implicit instruction set (tone, domain knowledge level) which constrains output and yields responses better suited to the intended audience or task.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5686.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterate and Refine (Iterative prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Advice to refine prompts iteratively and request modifications based on previous outputs to progressively improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Progressive refinement of generated content (general use)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Workflows where the user asks for an initial output and then refines it via follow-up prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multi-turn iterative prompting: initial prompt followed by edits, clarifications, or refinement requests.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Single-shot one-off prompt vs iterative multi-turn refinement</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Iterative refinement leverages the conversational context to correct, narrow, or expand outputs; models rarely produce an ideal answer on first try, so iteration improves alignment with user needs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5686.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Use threads / conversation history</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use the Threads (Conversation history retention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recommendation to use persistent conversation threads so the model can build on previously provided details and feedback without repeating context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-turn workflows requiring context carryover (e.g., multi-step patient scenario discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that benefit from retaining previous exchanges so subsequent prompts can assume earlier context.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Leveraging thread history instead of re-providing all context each turn.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Stateless single-prompt interactions vs thread-based multi-turn interactions</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Using threads preserves context and prior feedback, reducing the need to restate details and enabling the model to produce more coherent, context-aware outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5686.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open vs closed questioning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ask Open-Ended Questions (Open vs Closed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recommendation that open-ended prompts (e.g., 'How do you feel?') elicit broader, more informative responses than closed yes/no prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Patient elicitation / exploratory information gathering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that aim to gather richer subjective or diagnostic information from users or simulated patients.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Open-ended question formulation versus closed yes/no question.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Closed question vs open-ended question</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (richer output)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Open-ended prompts allow the model to draw on broader training data and produce more comprehensive and potentially creative responses; closed prompts constrain output and may miss important information.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5686.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Request examples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Request Examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recommendation to ask the model for specific examples to clarify concepts and show practical applications of abstract ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clarification and applied examples (educational / explanatory tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where examples help demonstrate application of rules or procedures (e.g., clinical scenarios illustrating a concept).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Explicitly request illustrative examples as part of the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>No examples requested vs explicit example requests</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Examples ground abstract responses in concrete situations and help the user assess correctness and applicability of model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5686.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temporal awareness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal Awareness (Time-scoped prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recommendation to include explicit time references in prompts (e.g., 'first six weeks after knee surgery') because the model's responses should be time-contextualized.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Time-sensitive medical information (e.g., postsurgical recovery expectations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where the timing or temporal scope of information matters for accuracy and relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompts with explicit temporal framing versus prompts without time references.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>No time reference vs explicit time-scoped prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Specifying a temporal frame narrows the expected content (e.g., acute vs long-term phases) and improves relevance of generated information.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e5686.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Set realistic expectations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Set Realistic Expectations (Acknowledge model constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Advice to craft prompts that respect known LLM limitations (e.g., knowledge cutoff) to avoid unrealistic requests (e.g., asking for post-2021 research).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (with knowledge cutoff Nov 2021) / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-retrieval tasks requiring up-to-date information</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where users might expect current literature or guidelines beyond the model's training cutoff.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Realistic prompt that requests information up to the model's cutoff vs unrealistic prompt requesting the latest (post-cutoff) research.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Unrealistic post-cutoff information request vs realistic pre-cutoff scope</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (if unrealistic expectations) / improved (if expectations matched to model capabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Matching prompt content to known model limitations prevents misleading or incorrect outputs; unrealistic prompts can produce erroneous or fabricated answers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e5686.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>One-shot / Few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-shot / Few-shot Prompting Methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recommendation to provide one or a few examples in the prompt to guide LLM output (e.g., giving example names when asking for new names).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Controlled generation (e.g., naming, style transfer, format conformity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where a few examples can steer the model's output distribution toward user-preferred styles or content.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>One-shot: provide a single example; Few-shot: provide multiple example inputs/outputs within the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot (no examples) vs one-shot vs few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (guides output style)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing examples helps the model infer the desired format, tone, or constraints, thereby increasing likelihood of outputs matching expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e5686.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting for prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting for Prompts (Meta-prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Technique of asking the model to generate or improve prompts for the user (e.g., 'What prompt could I use right now to get a better output?').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / general LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Meta-prompt generation and optimization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Using the model to suggest high-quality prompts or refine existing prompts to improve future outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Asking the LLM to propose or optimize prompts for a given task or thread.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>User-crafted prompts vs model-suggested prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (practical aid in prompt design)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Models can propose formulations that capture effective constraints or phrasings the user may not consider, accelerating prompt optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e5686.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (author use)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pre-trained Transformer 4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large language model from OpenAI; the author reports using GPT-4 during the ideation process to test prompt engineering recommendations through imaginary scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Ideation and internal testing of prompt engineering recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Author used GPT-4 to explore and validate prompt engineering suggestions by running imagined scenarios and prompts during paper preparation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Author-tested prompt engineering recommendations via imaginary scenarios and prompts; no formal experimental protocol reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Author states GPT-4 was used to ensure the paper covers prompt engineering suggestions of value and to test recommendations through imaginary scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5686.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e5686.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI conversational LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conversational large language model frequently referenced in the paper as the canonical example of LLMs used by medical professionals and the public; used illustratively for prompt examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General-purpose conversational tasks and medical Q&A (illustrative examples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various tasks exemplified throughout the paper: diagnosis aid, administrative assistance, patient engagement, research assistance, education.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Various prompt formats illustrated (specificity, context, role-play, one/few-shot, etc.); ChatGPT is used as the exemplar LLM in examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (format-dependent, per recommendations)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper uses ChatGPT as an example to show how prompt wording and structure influence outputs; emphasizes knowledge cutoff and limitations as part of realistic expectation setting.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Cataloging Prompt Patterns to Enhance the Discipline of Prompt Engineering <em>(Rating: 2)</em></li>
                <li>Prompt engineering for healthcare: Methodologies and applications <em>(Rating: 2)</em></li>
                <li>Artificial intelligence for health message generation: an empirical study using a large language model (LLM) and prompt engineering <em>(Rating: 2)</em></li>
                <li>Prompt engineering with ChatGPT: a guide for academic writers <em>(Rating: 1)</em></li>
                <li>ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5686",
    "paper_id": "paper-eecb4dbf218d08d43a727c7e79f86a296502f117",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Prompt specificity",
            "name_full": "Be as Specific as Possible (Prompt specificity)",
            "brief_description": "Recommendation that more specific prompts yield more accurate and focused responses from LLMs; illustrated by contrasting a vague prompt with a narrowly scoped question.",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / general LLMs",
            "model_size": null,
            "task_name": "Medical information retrieval / question answering (illustrative)",
            "task_description": "Example tasks where users ask the model for information about medical topics (e.g., 'heart disease' vs 'risk factors for coronary artery disease').",
            "problem_format": "Comparison of a vague/general prompt (e.g., 'Tell me about heart disease') versus a specific/focused prompt (e.g., 'What are the most common risk factors for coronary artery disease?'), i.e., increasing prompt specificity.",
            "comparison_format": "Less specific (vague) prompt vs more specific (focused) prompt",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "The paper states that more specific prompts lead to more accurate and focused responses because they constrain the model's output space and clarify the user's intent.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.0",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Context/setting provision",
            "name_full": "Describe the Setting and Provide Context",
            "brief_description": "Recommendation to include situational context in prompts (e.g., purpose, audience, role of user) so the LLM can tailor responses appropriately.",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / general LLMs",
            "model_size": null,
            "task_name": "Contextualized content generation (e.g., article writing, medical advice phrasing)",
            "task_description": "Tasks where output depends on the user's situation or intent (e.g., producing tips for healthcare professionals writing about prompt engineering).",
            "problem_format": "Providing explicit context/setting in the prompt (e.g., 'I'm writing an article about tips... Can you list those tips with examples?') versus omitting context.",
            "comparison_format": "No context vs provided context/setting",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Including context helps the model produce responses that are better aligned with the user's needs because the model can condition its output on the provided scenario and constraints.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.1",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Prompt style",
            "name_full": "Experiment with Different Prompt Styles (Direct question / List / Summary / Process)",
            "brief_description": "Paper recommends varying prompt style (direct question, list request, summary, step-by-step process) because style influences the structure and content of model outputs.",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / general LLMs",
            "model_size": null,
            "task_name": "Information elicitation and structuring (e.g., listing symptoms, summarizing disease progression, procedural steps)",
            "task_description": "Tasks where the user requests information in different organizational formats: enumerated lists, concise summaries, or procedural steps.",
            "problem_format": "Different prompt formulations: direct question ('What are the symptoms?'), request for list ('List all potential symptoms'), request for summary ('Summarize key symptoms and progression'), request for process ('Provide a step-by-step process of diagnosing...').",
            "comparison_format": "Direct question vs list vs summary vs step-by-step process",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (format-dependent)",
            "explanation_or_hypothesis": "The paper suggests that the chosen format guides the model's output organization and level of detail; e.g., asking explicitly for a list yields enumerated items, while a process prompt encourages sequential steps.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.2",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Goal-oriented prompting",
            "name_full": "Identify the Overall Goal of the Prompt First",
            "brief_description": "Recommendation to state the desired output type and purpose (e.g., creative ideas, technical description) to help the model produce more relevant responses.",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / general LLMs",
            "model_size": null,
            "task_name": "Output-targeted generation (e.g., ideation for presentations, explanations at a target depth)",
            "task_description": "Tasks where the form of the desired output matters (e.g., 'list of 5 ideas for a presentation').",
            "problem_format": "Prompt explicitly stating the overall goal (e.g., 'I'd like a list of 5 ideas for a presentation...') versus leaving goal implicit.",
            "comparison_format": "Implicit goal vs explicit goal specification",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Explicitly defining the goal narrows the model's response space and helps it focus on the appropriate style, scope, and level of detail.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.3",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Role prompting",
            "name_full": "Ask it to Play Roles (Role prompting / persona)",
            "brief_description": "Technique to ask the model to assume a professional role (e.g., 'Act as a Data Scientist') so responses are framed with that expertise and style.",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / general LLMs",
            "model_size": null,
            "task_name": "Role-specific explanations and advice (e.g., tutoring, nutrition advice, data science explanations)",
            "task_description": "Tasks where adopting a specific persona helps tailor tone, depth, and content (e.g., 'Act as my nutritionist...').",
            "problem_format": "Instruction to adopt a role/persona before asking the question.",
            "comparison_format": "No role specification vs explicit role specification",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Role prompts provide an implicit instruction set (tone, domain knowledge level) which constrains output and yields responses better suited to the intended audience or task.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.4",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Iterative prompting",
            "name_full": "Iterate and Refine (Iterative prompting)",
            "brief_description": "Advice to refine prompts iteratively and request modifications based on previous outputs to progressively improve results.",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / general LLMs",
            "model_size": null,
            "task_name": "Progressive refinement of generated content (general use)",
            "task_description": "Workflows where the user asks for an initial output and then refines it via follow-up prompts.",
            "problem_format": "Multi-turn iterative prompting: initial prompt followed by edits, clarifications, or refinement requests.",
            "comparison_format": "Single-shot one-off prompt vs iterative multi-turn refinement",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Iterative refinement leverages the conversational context to correct, narrow, or expand outputs; models rarely produce an ideal answer on first try, so iteration improves alignment with user needs.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.5",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Use threads / conversation history",
            "name_full": "Use the Threads (Conversation history retention)",
            "brief_description": "Recommendation to use persistent conversation threads so the model can build on previously provided details and feedback without repeating context.",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / general LLMs",
            "model_size": null,
            "task_name": "Multi-turn workflows requiring context carryover (e.g., multi-step patient scenario discussion)",
            "task_description": "Tasks that benefit from retaining previous exchanges so subsequent prompts can assume earlier context.",
            "problem_format": "Leveraging thread history instead of re-providing all context each turn.",
            "comparison_format": "Stateless single-prompt interactions vs thread-based multi-turn interactions",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Using threads preserves context and prior feedback, reducing the need to restate details and enabling the model to produce more coherent, context-aware outputs.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.6",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Open vs closed questioning",
            "name_full": "Ask Open-Ended Questions (Open vs Closed)",
            "brief_description": "Recommendation that open-ended prompts (e.g., 'How do you feel?') elicit broader, more informative responses than closed yes/no prompts.",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / general LLMs",
            "model_size": null,
            "task_name": "Patient elicitation / exploratory information gathering",
            "task_description": "Tasks that aim to gather richer subjective or diagnostic information from users or simulated patients.",
            "problem_format": "Open-ended question formulation versus closed yes/no question.",
            "comparison_format": "Closed question vs open-ended question",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (richer output)",
            "explanation_or_hypothesis": "Open-ended prompts allow the model to draw on broader training data and produce more comprehensive and potentially creative responses; closed prompts constrain output and may miss important information.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.7",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Request examples",
            "name_full": "Request Examples",
            "brief_description": "Recommendation to ask the model for specific examples to clarify concepts and show practical applications of abstract ideas.",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / general LLMs",
            "model_size": null,
            "task_name": "Clarification and applied examples (educational / explanatory tasks)",
            "task_description": "Tasks where examples help demonstrate application of rules or procedures (e.g., clinical scenarios illustrating a concept).",
            "problem_format": "Explicitly request illustrative examples as part of the prompt.",
            "comparison_format": "No examples requested vs explicit example requests",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Examples ground abstract responses in concrete situations and help the user assess correctness and applicability of model outputs.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.8",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Temporal awareness",
            "name_full": "Temporal Awareness (Time-scoped prompts)",
            "brief_description": "Recommendation to include explicit time references in prompts (e.g., 'first six weeks after knee surgery') because the model's responses should be time-contextualized.",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / general LLMs",
            "model_size": null,
            "task_name": "Time-sensitive medical information (e.g., postsurgical recovery expectations)",
            "task_description": "Tasks where the timing or temporal scope of information matters for accuracy and relevance.",
            "problem_format": "Prompts with explicit temporal framing versus prompts without time references.",
            "comparison_format": "No time reference vs explicit time-scoped prompts",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Specifying a temporal frame narrows the expected content (e.g., acute vs long-term phases) and improves relevance of generated information.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.9",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Set realistic expectations",
            "name_full": "Set Realistic Expectations (Acknowledge model constraints)",
            "brief_description": "Advice to craft prompts that respect known LLM limitations (e.g., knowledge cutoff) to avoid unrealistic requests (e.g., asking for post-2021 research).",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (with knowledge cutoff Nov 2021) / general LLMs",
            "model_size": null,
            "task_name": "Knowledge-retrieval tasks requiring up-to-date information",
            "task_description": "Tasks where users might expect current literature or guidelines beyond the model's training cutoff.",
            "problem_format": "Realistic prompt that requests information up to the model's cutoff vs unrealistic prompt requesting the latest (post-cutoff) research.",
            "comparison_format": "Unrealistic post-cutoff information request vs realistic pre-cutoff scope",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced (if unrealistic expectations) / improved (if expectations matched to model capabilities)",
            "explanation_or_hypothesis": "Matching prompt content to known model limitations prevents misleading or incorrect outputs; unrealistic prompts can produce erroneous or fabricated answers.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.10",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "One-shot / Few-shot prompting",
            "name_full": "One-shot / Few-shot Prompting Methods",
            "brief_description": "Recommendation to provide one or a few examples in the prompt to guide LLM output (e.g., giving example names when asking for new names).",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / general LLMs",
            "model_size": null,
            "task_name": "Controlled generation (e.g., naming, style transfer, format conformity)",
            "task_description": "Tasks where a few examples can steer the model's output distribution toward user-preferred styles or content.",
            "problem_format": "One-shot: provide a single example; Few-shot: provide multiple example inputs/outputs within the prompt.",
            "comparison_format": "Zero-shot (no examples) vs one-shot vs few-shot",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (guides output style)",
            "explanation_or_hypothesis": "Providing examples helps the model infer the desired format, tone, or constraints, thereby increasing likelihood of outputs matching expectations.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.11",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Prompting for prompts",
            "name_full": "Prompting for Prompts (Meta-prompting)",
            "brief_description": "Technique of asking the model to generate or improve prompts for the user (e.g., 'What prompt could I use right now to get a better output?').",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / general LLMs",
            "model_size": null,
            "task_name": "Meta-prompt generation and optimization",
            "task_description": "Using the model to suggest high-quality prompts or refine existing prompts to improve future outputs.",
            "problem_format": "Asking the LLM to propose or optimize prompts for a given task or thread.",
            "comparison_format": "User-crafted prompts vs model-suggested prompts",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (practical aid in prompt design)",
            "explanation_or_hypothesis": "Models can propose formulations that capture effective constraints or phrasings the user may not consider, accelerating prompt optimization.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.12",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "GPT-4 (author use)",
            "name_full": "GPT-4 (Generative Pre-trained Transformer 4)",
            "brief_description": "Large language model from OpenAI; the author reports using GPT-4 during the ideation process to test prompt engineering recommendations through imaginary scenarios.",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "Ideation and internal testing of prompt engineering recommendations",
            "task_description": "Author used GPT-4 to explore and validate prompt engineering suggestions by running imagined scenarios and prompts during paper preparation.",
            "problem_format": "Author-tested prompt engineering recommendations via imaginary scenarios and prompts; no formal experimental protocol reported.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": null,
            "explanation_or_hypothesis": "Author states GPT-4 was used to ensure the paper covers prompt engineering suggestions of value and to test recommendations through imaginary scenarios.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.13",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "ChatGPT (mention)",
            "name_full": "ChatGPT (OpenAI conversational LLM)",
            "brief_description": "Conversational large language model frequently referenced in the paper as the canonical example of LLMs used by medical professionals and the public; used illustratively for prompt examples.",
            "citation_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "mention_or_use": "mention",
            "model_name": "ChatGPT",
            "model_size": null,
            "task_name": "General-purpose conversational tasks and medical Q&A (illustrative examples)",
            "task_description": "Various tasks exemplified throughout the paper: diagnosis aid, administrative assistance, patient engagement, research assistance, education.",
            "problem_format": "Various prompt formats illustrated (specificity, context, role-play, one/few-shot, etc.); ChatGPT is used as the exemplar LLM in examples.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (format-dependent, per recommendations)",
            "explanation_or_hypothesis": "Paper uses ChatGPT as an example to show how prompt wording and structure influence outputs; emphasizes knowledge cutoff and limitations as part of realistic expectation setting.",
            "counterexample_or_null_result": null,
            "uuid": "e5686.14",
            "source_info": {
                "paper_title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Cataloging Prompt Patterns to Enhance the Discipline of Prompt Engineering",
            "rating": 2,
            "sanitized_title": "cataloging_prompt_patterns_to_enhance_the_discipline_of_prompt_engineering"
        },
        {
            "paper_title": "Prompt engineering for healthcare: Methodologies and applications",
            "rating": 2,
            "sanitized_title": "prompt_engineering_for_healthcare_methodologies_and_applications"
        },
        {
            "paper_title": "Artificial intelligence for health message generation: an empirical study using a large language model (LLM) and prompt engineering",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_for_health_message_generation_an_empirical_study_using_a_large_language_model_llm_and_prompt_engineering"
        },
        {
            "paper_title": "Prompt engineering with ChatGPT: a guide for academic writers",
            "rating": 1,
            "sanitized_title": "prompt_engineering_with_chatgpt_a_guide_for_academic_writers"
        },
        {
            "paper_title": "ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns",
            "rating": 1,
            "sanitized_title": "chatgpt_utility_in_healthcare_education_research_and_practice_systematic_review_on_the_promising_perspectives_and_valid_concerns"
        }
    ],
    "cost": 0.01391575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial</h1>
<p>Bertalan Mesk, MD, PhD<br>The Medical Futurist Institute, Budapest, Hungary</p>
<p>Corresponding Author:<br>Bertalan Mesk, MD, PhD<br>The Medical Futurist Institute<br>Povl Bang-Jensen u. 2/B1. 4/1.<br>Budapest, 1118<br>Hungary<br>Phone: 36703807260<br>Email: berci@medicalfuturist.com</p>
<h4>Abstract</h4>
<p>Prompt engineering is a relatively new field of research that refers to the practice of designing, refining, and implementing prompts or instructions that guide the output of large language models (LLMs) to help in various tasks. With the emergence of LLMs, the most popular one being ChatGPT that has attracted the attention of over a 100 million users in only 2 months, artificial intelligence (AI), especially generative AI, has become accessible for the masses. This is an unprecedented paradigm shift not only because of the use of AI becoming more widespread but also due to the possible implications of LLMs in health care. As more patients and medical professionals use AI-based tools, LLMs being the most popular representatives of that group, it seems inevitable to address the challenge to improve this skill. This paper summarizes the current state of research about prompt engineering and, at the same time, aims at providing practical recommendations for the wide range of health care professionals to improve their interactions with LLMs.</p>
<p>(J Med Internet Res 2023;25:e50638) doi: $\underline{10.2196 / 50638}$</p>
<h2>KEYWORDS</h2>
<p>artificial intelligence; AI; digital health; future; technology; ChatGPT; GPT-4; large language models; language model; LLM; prompt; prompts; prompt engineering; AI tool; engineering; healthcare professional; decision-making; LLMs; chatbot; chatbots; conversational agent; conversational agents; NLP; natural language processing</p>
<h2>The Emergence of Large Language Models and Prompt Engineering</h2>
<p>With the emergence of large language models (LLMs), with the most popular one being ChatGPT that has attracted the attention of over a 100 million users in only 2 months, artificial intelligence (AI), especially generative AI has become accessible for the masses [1]. This is an unprecedented paradigm shift not only because of the use of AI becoming more widespread but also due to the possible implications of LLMs in health care [2].</p>
<p>Numerous studies have shown what medical tasks and health care processes LLMs can contribute to in order to ease the burden on medical professionals, increase efficiency, and decrease costs [3].</p>
<p>Health care institutions have started investing in generative AI, medical companies have started integrating LLMs into their
businesses, medical associations have released guidelines about the use of these models, and medical curricula have also started covering this novel technology [4-6]. Thus, a new, essential skill has emerged: prompt engineering.</p>
<p>Prompt engineering is a relatively new field of research that refers to the practice of designing, refining, and implementing prompts or instructions that guide the output of LLMs to help in various tasks. It is essentially the practice of effectively interacting with AI systems to optimize their benefits.</p>
<p>In the context of medical professionals and health care in general, this could encompass the following:</p>
<ul>
<li>
<p>Decision support: medical professionals can use prompt engineering to optimize AI systems to aid in decision-making processes, such as diagnosis, treatment selection, or risk assessment.</p>
</li>
<li>
<p>Administrative assistance: prompts can be engineered to facilitate administrative tasks, such as patient scheduling, record keeping, or billing, thereby increasing efficiency.</p>
</li>
<li>Patient engagement: prompt engineering can be used to improve communication between health care providers and patients. For example, AI systems can be designed to send prompts for medication reminders, appointment scheduling, or lifestyle advice.</li>
<li>Research and development: in research scenarios, prompts can be crafted to assist in tasks such as literature reviews, data analysis, and generating hypotheses.</li>
<li>Training and education: prompts can be engineered to facilitate the education of medical professionals, including ongoing training in the latest treatments and procedures.</li>
<li>Public health: on a larger scale, prompt engineering can assist in public health initiatives by helping analyze population health data, predict disease trends, or educate the public.</li>
</ul>
<p>Prompt engineering, therefore, has the potential to improve the efficiency, accuracy, and effectiveness of health care delivery, making it an increasingly important skill for medical professionals.</p>
<p>This paper summarizes the current state of research on prompt engineering and, at the same time, aims at providing practical recommendations for the wide range of health care professionals to improve their interactions with LLMs.</p>
<h2>The State of Prompt Engineering</h2>
<p>The use of LLMs, especially ChatGPT, comes with major limitations and risks. First, since ChatGPT is not updated in real time and its training data only include information up to November 2021, it may lack crucial, up-to-date medical research or changes in clinical guidelines, potentially impacting the quality and relevance of its responses. Furthermore, ChatGPT cannot access or process individual user data or context, which limits its ability to provide personalized medical advice and increases the risk of data misinterpretation.</p>
<p>There is also a crucial need for users to verify every single response from ChatGPT with a qualified health care professional, as the model's answers are generated on the basis of patterns in the data it was trained on and may not be accurate or safe.</p>
<p>The model's inability to empathize or deliver sensitive information may also result in a subpar patient experience. Importantly, potential breaches of patient confidentiality could violate privacy laws such as the Health Insurance Portability and Accountability Act of 1996 in the United States. Despite its potential as an assistive tool, these limitations necessitate careful consideration of its application in health care [7].</p>
<p>While these risks are significant, the potential outcomes can outweigh them; therefore, the need for improving at designing
better prompts has grown extensively since the launch of ChatGPT.</p>
<p>There have been attempts at addressing this issue. One study aimed at designing a catalogue of prompt engineering techniques, presented in pattern form, which have been applied to solve common problems when conversing with LLMs [8]. Another study provided a summary of the latest advances in prompt engineering for a very specific audience, researchers working in natural language processing for the medical domain, or academic writers [9,10]. One study introduced the potential of an AI system to generate health awareness messages through prompt engineering [11].</p>
<p>While there is research in the field, it is clear that there have been no comprehensive, yet practical guides for medical professionals. This is the gap that this paper aims to fill.</p>
<h2>How to Improve at Prompt Engineering</h2>
<p>As in the case of any essential skill, becoming better at prompt engineering would involve an improved understanding of the fundamental principles of the technology, gaining practical exposure to systems using the technology, and continually refining and iterating the skill based on feedback.</p>
<p>The following are some concrete steps that a health care professional can take to improve their skills in prompt engineering:</p>
<ul>
<li>Understanding the underlying principles of how AI and machine learning models work can provide a foundation on which to build prompt engineering skills. As shown, it is possible to gain that understanding without any prior technical or coding knowledge [12].</li>
<li>Familiarizing themselves with the LLMs they are working with as each system has its own set of capabilities and limitations. Understanding both can help craft more effective prompts.</li>
<li>Practice makes perfect; therefore, attempting to interact with LLMs regularly and make a note of the prompts that yield the most helpful and accurate results can have benefits.</li>
</ul>
<p>It is also important to constantly test prompts in real-world scenarios as their effectiveness is best evaluated in practical application.</p>
<h2>Specific Recommendations for Better LLM Prompts</h2>
<p>Besides these general approaches, here is a summary of specific recommendations with practical examples that a health care professional might want to consider to improve their skills in prompt engineering. Figure 1 summarizes these recommendations, their examples with ChatGPT's key terms, limitations, and the most popular plugins.</p>
<p>Figure 1. A cheat sheet of prompt engineering recommendations for health care professionals with examples for each: ChatGPT's key terms and their explanations, its limitations, and its most popular plugins. A high resolution version is attached as Multimedia Appendix 1.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h2>Be as Specific as Possible</h2>
<p>The more specific the prompt, the more accurate and focused the response is likely to be. The following is an example prompt:</p>
<ul>
<li>Less specific: "Tell me about heart disease."</li>
<li>More specific: "What are the most common risk factors for coronary artery disease?"</li>
</ul>
<h2>Describe the Setting and Provide the Context Around the Question</h2>
<p>One must consider the discussion one is having with ChatGPT as a discussion one would have with a person they just met, who might still be able to answer their questions and address one's challenges.</p>
<p>The following is an example prompt: "I'm writing an article about tips and tricks for ChatGPT prompt engineering for people working in healthcare. Can you please list a few of those tips and tricks with some specific prompt examples?"</p>
<h2>Experiment With Different Prompt Styles</h2>
<p>The style of one's prompt can significantly impact the answer. One can try different formats such as asking ChatGPT to generate a list about their brief or to provide a summary of the topic. The following is an example:</p>
<ul>
<li>Direct question: "What are the symptoms of COVID-19?"</li>
<li>Request for a list: "List all the potential symptoms of COVID-19."</li>
<li>Request for a summary: "Summarize the key symptoms and progression of COVID-19."</li>
<li>Process: "Provide a step-by-step process of diagnosing COVID-19."</li>
</ul>
<h2>Identify the Overall Goal of the Prompt First</h2>
<p>Describe exactly what kind of output is being sought. Whether it would be getting creative ideas for an article, asking for a specific description of an advanced scientific topic, or providing a list of examples around questions, defining it helps ChatGPT come up with more relevant answers. The following is an example: "I'd like to get a list of 5 ideas for a presentation at a scientific event to make my research findings more easily understandable."</p>
<h2>Ask it to Play Roles</h2>
<p>This can help streamline the desired process of obtaining the information or input one was looking for in a specific setting. With new topics without prior knowledge, it is prudent to obtain only a basic description; in addition, one can also ask ChatGPT to act as a tutor and help dive into a detailed topic step-by-step. The following are a couple of examples:</p>
<ul>
<li>"Act as a Data Scientist and explain Prompt Engineering to a physician."</li>
<li>"Act as my nutritionist and give me tips about a balanced Mediterranean diet."</li>
</ul>
<h2>Iterate and Refine</h2>
<p>Even if one's skills in prompt engineering are advanced, LLMs change so dynamically that one rarely get the best response on was looking for after the first prompt attempt. Constantly iterating prompts is something with which we should get accustomed. Users of LLMs are also encouraged to ask the LLM to modify the output based on feedback on its previous response.</p>
<h2>Use the Threads</h2>
<p>One can navigate back to a specific discussion by clicking on the specific thread in the left column on ChatGPT's dashboard. This way, one can build upon the details and responses one has already received in a previous thread. This can save a lot of time as there is no need to describe the same situation and all the feedback ChatGPT has received on its responses.</p>
<h2>Ask Open-Ended Questions</h2>
<p>Open-ended questions can provide a broader, more comprehensive understanding of the user's situation. For
instance, asking "How do you feel?" rather than "Do you feel pain?" allows for a wider array of responses that can potentially provide more insight into the patient's mental, emotional, or physical state. Open-ended questions can also help to generate a larger data set for training AI models, making them more effective. Lastly, asking open-ended questions allows ChatGPT to display its potential better by leveraging its training on a diverse range of topics. This can lead to more unexpected and creative solutions or ideas that a health care professional might not have thought of. The following is an example:</p>
<ul>
<li>Closed question: "Is exercise important for patients with osteoporosis?"</li>
<li>Open question: "How does regular physical activity benefit patients with osteoporosis?"</li>
</ul>
<h2>Request Examples</h2>
<p>Asking for specific examples can help to clarify the meaning of a concept or idea, making it easier to understand. Especially with complex medical terminology or procedures, examples can provide a practical context that aids comprehension. Also, examples often help in visualizing abstract or complicated ideas. When ChatGPT provides examples, it can showcase how a certain concept or rule is applied in different scenarios. This can be beneficial in health care, where theoretical knowledge needs to be connected to real-world applications.</p>
<h2>Temporal Awareness</h2>
<p>This refers to the model's understanding of time-related concepts and its ability to generate contextually relevant responses based on time. Therefore, describing what time line the prompt and the desired output would be related to helps LLMs provide a more useful answer. The following is an example:</p>
<ul>
<li>Without a time reference: "Describe the healing process after knee surgery."</li>
<li>With a time reference: "What can a patient typically expect during the first six weeks of healing after knee surgery?"</li>
</ul>
<h2>Set Realistic Expectations</h2>
<p>Knowing the limitations of AI tools such as ChatGPT is crucial, as it helps set realistic expectations about the output. For instance, ChatGPT cannot access any data or information after November, 2021; it cannot provide personalized medical advice or replace a professional's judgement. The following is an example:</p>
<ul>
<li>Unrealistic prompt: "What's the latest research published this month about Alzheimer's?"</li>
<li>Realistic prompt: "What were some of the major research breakthroughs in Alzheimer's treatment up until 2021?"</li>
</ul>
<h2>Use the One-Shot/Few-Shot Prompting Method</h2>
<p>The one-shot prompting method is one in which ChatGPT can generate an answer based on a single example or piece of context provided by the user. The following is an example:</p>
<ul>
<li>Generate 10 possible names for a new digital stethoscope device.</li>
<li>A name that I like is DigSteth.</li>
</ul>
<p>With the few-shot strategy, ChatGPT can generate an answer based on a few examples or pieces of context provided by the user. The following is an example:</p>
<ul>
<li>Generate 10 possible names for a new digital stethoscope device.</li>
<li>Names that I like include:</li>
<li>Digital</li>
<li>Steth</li>
<li>Stethoscope</li>
</ul>
<h2>Prompting for Prompts</h2>
<p>One of the easiest ways of improving at prompt engineering is asking ChatGPT to get involved in the process and design prompts for the user. The following is an example: "What prompt could I use right now to get a better output from you in this thread/task?"</p>
<h2>Conclusions</h2>
<p>As the skill of prompt engineering has gained significant interest worldwide, especially in the health care setting, it would be
important to include teaching the practical methods this paper described in the medical curriculum and postgraduate education. While the technical details and background of generative AI will probably be included in future curricula, it would be useful for medical students to learn the most practical tips of using LLMs even before that happens.</p>
<p>The general message for every LLM user should be that they could use such AI tools to expand their knowledge, capabilities, and ideas instead of solving things on their behalf. Ideally, this approach and mindset would stem from trained medical professionals who could share it with their patients.</p>
<p>In summary, as more patients and medical professionals use AI-based tools-LLMs being the most popular representatives of that group-it seems inevitable to address the challenge to improve at this skill. Furthermore, as doing so does not require any technical knowledge or prior programming expertise, prompt engineering alone can be considered an essential emerging skill that helps leverage the full potential of AI in medicine and health care.</p>
<h1>Acknowledgments</h1>
<p>I used the generative AI tool GPT-4 (OpenAI) [1] during the ideation process to make sure the paper covers every possible prompt engineering suggestion of value. During that process, I tested the prompt engineering recommendations I made in the paper through imaginary scenarios.</p>
<h2>Conflicts of Interest</h2>
<p>None declared.</p>
<h2>Multimedia Appendix 1</h2>
<p>Higher resolution version of Figure 1.
[PNG File, 661 KB-Multimedia Appendix 1]</p>
<h2>References</h2>
<ol>
<li>Introducing ChatGPT. OpenAI. URL: https://openai.com/blog/chatgpt [accessed 2023-09-25]</li>
<li>Mesko B. The ChatGPT (generative artificial intelligence) revolution has made artificial intelligence approachable for medical professionals. J Med Internet Res 2023 Jun 22;25:e48392 [FREE Full text] [doi: 10.2196/48392] [Medline: 37347508]</li>
<li>Sallam M. ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns. Healthcare (Basel) 2023 Mar 19;11(6) [FREE Full text] [doi: 10.3390/healthcare11060887] [Medline: 36981544]</li>
<li>Adams K. Google Cloud, Mayo Clinic Strike Generative AI Partnership. MedCity News. 2023. URL: https://medcitynews. com/2023/06/google-cloud-mayo-clinic-generative-ai-llm-healthcare/ [accessed 2023-09-25]</li>
<li>Lunden I. Nabla, a digital health startup, launches Copilot, using GPT-3 to turn patient conversations into action. TechCrunch. 2023. URL: https://techcrunch.com/2023/03/14/
nabla-a-french-digital-health-startup-launches-copilot-using-gpt-3-to-turn-patient-conversations-into-actionable-items/ [accessed 2023-09-25]</li>
<li>Taulli T. Generative AI: How ChatGPT and Other AI Tools Will Revolutionize Business. Berkeley, CA: Apress; 2023.</li>
<li>Mesk B, Topol EJ. The imperative for regulatory oversight of large language models (or generative AI) in healthcare. NPJ Digit Med 2023 Jul 06;6(1):120 [FREE Full text] [doi: 10.1038/s41746-023-00873-0] [Medline: 37414860]</li>
<li>Schmidt DC, Spencer-Smith J, Fu Q, White J. Cataloging Prompt Patterns to Enhance the Discipline of Prompt Engineering. URL: https://www.dre.vanderbilt.edu/ schmidt/PDF/ADA_Europe_Position_Paper.pdf [accessed 2023-09-25]</li>
<li>
<p>Wang J, Shi E, Yu S, Wu Z, Ma C, Dai H, et al. Prompt engineering for healthcare: Methodologies and applications. arXiv. Preprint posted online April 28, 2023</p>
</li>
<li>
<p>Giray L. Prompt engineering with ChatGPT: a guide for academic writers. Ann Biomed Eng 2023 Jun 07 [doi: 10.1007/s10439-023-03272-4] [Medline: 37284994]</p>
</li>
<li>Lim S, Schmlzle R. Artificial intelligence for health message generation: an empirical study using a large language model (LLM) and prompt engineering. Front Commun 2023 May 26;8 [doi: 10.3389/fcomm.2023.1129082]</li>
<li>Mesk B, Grg M. A short guide for medical professionals in the era of artificial intelligence. NPJ Digit Med 2020 Sep 24;3(1):126 [FREE Full text] [doi: 10.1038/s41746-020-00333-z] [Medline: 33043150]</li>
</ol>
<h1>Abbreviations</h1>
<p>AI: artificial intelligence
LLM: large language model</p>
<p>Edited by A Mavragani; submitted 07.07.23; peer-reviewed by O Tamburis, A Zavar; comments to author 06.09.23; revised version received 14.09.23; accepted 19.09.23; published 04.10.23</p>
<p>Please cite as:
Mesk B
Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial
J Med Internet Res 2023;25:e50638
URL: https://www.jmir.org/2023/1/e50638
doi: $\underline{10.2196 / 50638}$
PMID: 37792434
(C)Bertalan Mesk. Originally published in the Journal of Medical Internet Research (https://www.jmir.org), 04.10.2023. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on https://www.jmir.org/, as well as this copyright and license information must be included.</p>            </div>
        </div>

    </div>
</body>
</html>
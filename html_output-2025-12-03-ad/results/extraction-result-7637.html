<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7637 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7637</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7637</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-274131526</p>
                <p><strong>Paper Title:</strong> An exploration of prompting LLMs to generate energy-efficient code</p>
                <p><strong>Paper Abstract:</strong> The increasing electricity demands of personal computers, communication networks, and data centers contribute to higher atmospheric greenhouse gas emissions, which in turn lead to global warming and climate change. Therefore the energy consumption of code must be minimised. Large language models can generate code, so we study the influence of prompting for energy-efficient code by examining the energy consumption of the generated code. We use three different Python code problems of varying difficulty levels. Prompt modification is done by adding the sentence "Give me an energy-optimised solution for this problem" or by providing two Python coding best practices. The large language models used are Code Llama-70b, Code Llama-70b-Instruct, Code Llama-70b-Python, DeepSeek-Coder-33b-base, and DeepSeek-Coder-33b-instruct. We find a decrease in energy consumption for a specific combination of prompt optimisation, LLM, and Python code problem. However, no single optimisation prompt consistently decreases energy consumption for the same LLM across the different Python code problems.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7637",
    "paper_id": "paper-274131526",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00367875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An exploration of prompting LLMs to generate energy-efficient code</p>
<p>Tom Cappendijk tom.cappendijk@student.uva.nl 
Ana Oprescu a.m.oprescu@uva.nl </p>
<p>Informatics Institute Universiteit van Amsterdam Amsterdam
The Netherlands</p>
<p>Pepijn de Reus Informatics Institute Universiteit van Amsterdam Amsterdam
The Netherlands</p>
<p>Informatics Institute Universiteit van Amsterdam Amsterdam
The Netherlands</p>
<p>An exploration of prompting LLMs to generate energy-efficient code
BCE18B90D80CB70CF16902976BE2A86F10.1109/GREENS66463.2025.00010
The increasing electricity demands of personal computers, communication networks, and data centers contribute to higher atmospheric greenhouse gas emissions, which in turn lead to global warming and climate change.Therefore the energy consumption of code must be minimised.Large language models can generate code, so we study the influence of prompting for energy-efficient code by examining the energy consumption of the generated code.We use three different Python code problems of varying difficulty levels.Prompt modification is done by adding the sentence "Give me an energy-optimised solution for this problem" or by providing two Python coding best practices.The large language models used are Code Llama-70b, Code Llama-70b-Instruct, Code Llama-70b-Python, DeepSeek-Coder-33b-base, and DeepSeek-Coder-33b-instruct.We find a decrease in energy consumption for a specific combination of prompt optimisation, LLM, and Python code problem.However, no single optimisation prompt consistently decreases energy consumption for the same LLM across the different Python code problems.Index Terms-energy efficient code, green software, green AI, prompting for energy efficiency 31</p>
<p>I. INTRODUCTION</p>
<p>The increase in atmospheric greenhouse gasses (GHG) emission has led to global warming [1].Zero emission of GHG is mandatory to stall climate change.While the European Union's Green Deal sets the zero-emission goal for 2050 [2], the electricity demand for IT infrastructure increases [3].Reducing the energy consumption of computing systems reduces the emission of GHG [4], and can, amongst others, be achieved by optimising the software used in these systems.This study focuses on software, in particular, code that is generated by Large Language Models (LLMs).The ability to generate correct code by a LLM is not positively correlated to the ability to generate efficient code [5].So for a user, it is important to judge the LLM not solely on the correctness of code but also on the code's energy consumption.</p>
<p>Lately, a growing number of studies examine the energy consumption and carbon emissions associated with LLMs [6], [7].E.g. by investigating strategies to lower the energy consumption of these models during training [8], [9].However, during usage, a LLM requires computation as well.As such, another research branch focuses on the energy usage during inference and provides solutions to lower energy consumption [10], [11].To position our research within the domain of energy consumption of LLMs, we do not focus on energy consumption during training or inference.Instead, we focus on the energy consumption of the code produced by LLMs.Our work relates to very recent research which assesses the sustainability awareness of LLMs by reviewing the code they produce [12], [13], and differs in that we focus on the ability of LLMs to generate code with a lower energy consumption.We conduct experiments by prompting energyefficient code solutions and collect the statistics, including energy consumption, of the code generated by LLMs.</p>
<p>To understand to what extent can prompts trigger a LLM to produce code that consumes less energy, we formulate the following two research questions:</p>
<p>(1) Which prompt optimisation leads to generating code that is consuming less energy for a given LLM? (2) To what extent does the code problem influence the impact of the prompt optimisation on energy consumption?We use the programming language Python and three difficulty levels.Prompts are modified by adding the sentence "Give me an energy-optimised solution for this problem" or by using two Python coding best green practices [14].We use Code Llama-70b, Code Llama-70b-Instruct, Code Llama-70b-Python [15], DeepSeek-Coder-33b-base, and DeepSeek-Coder-33b-instruct [16] and find a decrease in energy consumption for a specific combination of prompt optimisation, LLM, and Python code problems.However, no single optimisation prompt consistently decreases energy consumption for the same LLM across the different Python code problems.</p>
<p>Scope of this paper We do not incorporate all the available LLMs.There is a continuous release of new models.Some of the LLMs, such as ChatGPT-3, are behind a paywall and fall out of our scope.We also did not set out to implement open-source LLMs.Our focus is on prompt modification, showing how prompts can trigger a LLM to produce code that consumes less energy.Additionally, fine-tuning the implementation of a specific LLM can be complex due to numerous parameters and training options.Delving into such fine-tuning is not within the scope of this study.</p>
<p>II. BACKGROUND AND RELATED WORK</p>
<p>A language model is a model that assigns probabilities to upcoming words or sequences of words [17].A LLM is the product of a pretrained language model, pretraining enables the LLM to gain knowledge about the language and its context based on large amounts of text.After pretraining, a LLM is fine-tuned for tasks such as questioning and answering.</p>
<p>A. An Empirical Study on LLM-based Green Code Generation</p>
<p>This study [12] examines whether generative AI models can generate code that meets sustainability objectives, focusing on metrics like runtime, memory usage, energy consumption, and floating-point operations (FLOPs).Results show that AIgenerated code does improve sustainability metrics upon optimisation requests but typically falls short of human-coded solutions from LeetCode.Our work uses the same metrics but instead we report the metrics directly whereas Vartziotis et al. merge them to 'Green capacity'.Also, we use DeepSeek-Coder and Code Llama instead of GitHub Copilot [18], Amazon CodeWhisperer [19], and OpenAI ChatGPT-3 [20].</p>
<p>B. Energy Efficiency of the Code Generated by Code Llama</p>
<p>This work [13] is similar to our setup and measures the energy efficiency of code generated by Code Llama.The paper shows that the performance is highly dependant on the coding language and problem.Generally speaking, human-generated code is more energy-efficient than Code Llama's output, even when prompted for energy-efficient code.Our work differs as we do not examine the regular output but rather prompt for green coding principles, next to that we extend this work by including the model DeepSeek-Coder.</p>
<p>III. METHODOLOGY</p>
<p>Apart from energy consumption, to investigate the performance of the code generated by LLMs, we use the same metrics as Vartziotis et al. (2024) [12]: runtime, peak memory usage, and the number of floating-point operations (flops).Though we report all these metrics for compatibility with related work, we focus on energy consumption to answer our research questions.We also describe the selection of LLMs, code prompts, and best practices to test prompt modification.</p>
<p>We use Perf and GNU's Not Unix (GNU) time command to measure energy consumption and resource management.</p>
<p>Perf is not primarily an energy profiler, but it offers energy profiling functionalities.Perf supports a list of measurable events, that vary with each processor type and model [21].Sources of the events are kernel counters, the processor, and the processor's Performance Monitoring Unit (PMU).Perf is capable of measuring energy-related events.On Intel-based systems, data of the energy events are provided with the interface Running Average Power Limit (RAPL) [22].</p>
<p>We use Perf because it works independently of programming languages as it measures the statistics per single command.Perf allows repeated measurements of the same event and captures the event's output together with the elapsed time.We use the event power/energy-pkg/ to gather the energy consumption of a command.This Perf event captures the energy consumption, runtime and flops of the whole CPU package and returns the energy data in Joules [23].</p>
<p>To measure the peak memory usage of each command, we use the GNU tool time.This tool runs a command and returns a summary of used system resources.This includes the peak memory usage of the command.We use time because it measures a command's peak memory consumption and is programming language independent.</p>
<p>A. Selecting Large Language Models</p>
<p>Of the LLMs used by Vartziotis et al. [12], only ChatGPT-3 is supported with an API at the time of this writing.ChatGPT-3 can be integrated via a paid API and is therefore unavailable for this study.Amazon CodeWhisperer and Github Copilot are both an extension of an integrated development environment.This study has access to public LLMs, a list of available LLMs can be found on GitHub 1 .We select DeepSeek-Coder-Base and DeepSeek-Coder-Instruct due to their strong performance.Because of its high performance in previous work [16] we choose the 33B parameter model.The Code Llama-base models rank second-best [16].While only the 33B parameter version was considered in these results, there is evidence that the 70B model outperforms the 33B model in both the HumanEval and MBPP benchmarks [15].Thus, we use the 70B versions of the Code Llama model: Code Llama 70B, Code Llama -Instruct 70B, and Code Llama -Python 70B.</p>
<p>B. Selecting code problems</p>
<p>LeetCode 2 is a platform that provides code problems, categorised by difficulty levels: easy, medium, and hard.We choose this platform for compatibility with related work [12].Each problem is provided with a starting code snippet stating, if applicable for the programming language, the class function name and inputs of the function.To create a code prompt we copy the code problem text description and corresponding starting code snippet.We chose Python as the programming language for the code problems, due to its usage in a wide variety of domains and popularity [24].We select one problem for each difficulty level, whereas Vartziotis et al [12] pick 6 problem sets.Each problem is provided with constraints, so we have tested the solutions with test cases that are as large as possible while still adhering to the constraints.We test the code problems with large test cases to increase the runtime and with that, we increased the granularity, since short energy measurements can be inaccurate [23].Table II shows an overview of the code problems.</p>
<p>C. Prompt modification</p>
<p>Previous work [12] modifies prompts to generate code that consumes less energy.Their method involves adding a sentence at the beginning of the code prompt, specifying that the solution must be energy-optimised.A LLM can provide an optimised solution if it understands how to enhance the code to consume less energy.This assumes that the LLM understands the problem and knows how to improve the outcome.From a user perspective, this approach is straightforward and does not require any knowledge of code optimisation.We use this method as a baseline for testing prompt modifications.In addition to this approach, we examine best practices for writing energy-efficient Python code.[14] examines programs that solve the same problem with varying energy consumption using the same programming language.The study identifies techniques for writing more energy-efficient code.The study shows that in Python, the use of for-loops is more efficient than while-loops.The study also shows that using libraries instead of writing the code yourself reduces energy consumption.We incorporate these findings into our prompt modifications.Each finding is tested separately by adding a sentence to the beginning of the code prompt.The sentences together with their labels are shown in Table III.</p>
<p>Category Description</p>
<p>Energy Give me an energy-optimised solution for this problem Libraries Use library functions in the following problem For loop Use a for-loop instead of a while-loop in the following problem</p>
<p>D. Code similarities</p>
<p>We examine the similarity between the generated code for non-optimised and optimised code prompts addressing the same problem using pycode-similar, a plagiarism detector for Python code.The tool compares the abstract syntax tree (AST) of the different code solutions [25], by normalising the AST to remove unhelpful attributes, such as print statements, and then using difflib to determine the differences between the ASTs.</p>
<p>IV. EXPERIMENTAL SETUP AND RESULTS</p>
<p>The LLMs are hosted on an Oracle server, as the models require significant hardware resources.We access the Oracle server via a command line interface and transfer the output to our local machine.The measurements are conducted on a laptop with an 11th Gen Intel Core i5-1135G7, Ubuntu 22.04.4LTS and kernel version 6.5.0-35-generic.Nota bene, because we analyse the energy consumption of the code generated by the code LLMs, not the models themselves, we do not use a GPU in our experimental setup.The Python programs used in our study are available on GitHub.</p>
<p>To minimise background tasks, we use a terminal and prevent the system from updating by putting the device in 'airplane mode' during the experiments.Energy consumption and runtime are measured 50 times for each prompt.The peak memory usage measurement is repeated 50 times to account for variations caused by cache effects and cold starts [26].The energy consumed by main memory depends on the access pattern of the program.As total memory consumption does not consider this, we use peak memory in alignment with related work [12].The total number of floating point operations is not influenced by the Python process, even when floating point operations are executed, thus we test and verify this once.Output Modification.This is a manual step required because none of the LLM code outputs can be used directly due to one or more of the following issues, which results in incorrect Python code: repetition of the code prompt in the output, usage of libraries without importing them, irrelevant code after the code solution, and omission of the initial code snippet provided in the code prompt.Here, the "code snippet" refers to the starting point of the prompt, which serves as the basis for the LLM's continuation.</p>
<p>We solely adjust the LLMs' code output when it does not influence the scope of the class or method relevant to the code problem.This ensures that modifications do not affect the solution itself.LLMs are probabilistic models, so we examine their outputs across multiple runs and find no difference in code output across different runs (see Git).</p>
<p>V. RESULTS</p>
<p>Each configuration consists of one LLM together with one prompt.The prompts are derived from three different code problems and four different prompt types (default + three optimisation sentences), resulting in a total of sixty configurations (twelve prompts × five LLMs).We apply the Mann-Whitney U test and compare configurations without optimisation against those with optimisation for the same code problem and LLM, as seen in various previous work [13], [14].This comparison is based on differences in energy consumption, runtime, the total number of floating-point operations, and peak memory usage.</p>
<p>We use the Mann-Whitney U test because it is independent of the underlying distributions of our data, assuming that these distributions are similarly shaped [27].The null hypothesis of the Mann-Whitney U test is that energy dataset x belongs to the same energy population as energy dataset y [27].The alternative hypothesis is determined by the variant of the Mann-Whitney U test.The two-sided Mann-Whitney U test has an alternative hypothesis that energy dataset x does not belong to the same population as energy dataset y.The alternative hypothesis of the one-sided Mann Whitney is that energy dataset x is stochastically larger than energy dataset y.We use the one-sided variant twice because we wish to say whether energy dataset x is stochastically larger or smaller than energy dataset y.The first application has the null hypothesis that energy dataset x belongs to the same energy population as energy dataset y.The alternative hypothesis is that energy dataset x is stochastically larger than energy dataset y.We test this with an alpha-value of 0.01 because we wish to reduce the chance of falsely rejecting the null hypothesis.If we cannot reject the null hypothesis, the p-value is greater than the alpha-value, we cannot say that energy dataset x is stochastically greater than energy dataset y and test the opposite.We use the null hypothesis that energy dataset y is from the same population as energy dataset x.The alternative hypothesis is that energy dataset y is stochastically larger than energy dataset x.If the null hypothesis cannot be rejected in the second test either, we deem the results inconclusive.</p>
<p>We calculate a score for every combination to get an overview of the energy consumption of the different configurations across the code problems.A value of one is assigned if the configuration in the row consumes more energy than the configuration in the column and a value of minus one is assigned if the opposite holds.No value is assigned otherwise, given we have three code problems the numbers may add up to -3 or 3. Figure 1 shows the results, and Figure 2 shows occurrence of unknown comparison results.Green shows that the configuration on the row consumes less energy than the configuration on the column for all the problems, red, the opposite.</p>
<p>Base Prompt Versus Optimised Prompt: We assess the difference between the base-and optimised prompt.The metrics used are similarity, energy, memory, flops, and runtime.We are not interested in the value itself, but rather in the difference between the base-and optimised prompt.The similarity tool computes this difference at code level.For the other metrics, we report the difference as a percentage of the base value:
optimised − base base × 100(1)
The total number of floating point operations is measured once so we use the result directly.The result for the metrics energy, runtime, and peak memory consumption is a set of fifty data points.To calculate the percentage difference, we use the mean of each dataset.The library functions and for-loop optimisation prompts give an exact instruction on what the LLM must apply to generate code with lower energy consumption.We looked at the code generated with these optimisation prompts and made an overview of the configurations in which the optimisation is applied.This overview is shown in Table IV       is present in the base solution, this is the code prompt without an added optimisation sentence.The value 0 indicates that the optimisation is not present in the code output.
+ + + + + - - - \ - - - + - -Unk - - -Unk Unk - -Unk - Unk - - + \ Unk Unk + Unk Unk + + + Unk + + - - + Unk + - - + Unk \ Unk + Unk Unk + + + Unk + + - - + Unk + - - + Unk Unk \ + Unk Unk + + + Unk + + - - + Unk - - - - - - - \ - - - - - - - - - - - - + - - + Unk Unk Unk + \ Unk + + + Unk + + - - + + + - - + Unk Unk Unk + Unk \ + + + Unk + + - - + + - - -Unk - - - + - - \ -Unk - + Unk - -Unk - Unk - - + - - - + - - + \ Unk - + + - - + - - - - + - - - + - -Unk Unk \ - + + - -Unk - + - - + Unk Unk Unk + Unk Unk + + + \ + + - - + Unk - - -Unk - - - + - - - - - - \ Unk - -Unk - - - -Unk - - - + - -Unk - - -Unk \ - -+ + - + + N/A + + + N/A + Unk + + + - - - - - \ Unk - + + N/A + + + N/A + - + + + - - - - -Unk \ - + + N/A + + + N/A + -Unk + + + + + + + + + \ + + N/A + + + N/A + + + + + - - - - - - - - \ + N/A + Unk + N/A + - - + + - - - - - - - - - \ N/A + - + N/A + - - + + N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A \ N/A N/A N/A N/A N/A N/A N/A N/A N/A - - - - - - - - - -N/A \ - -N/A Unk - - + + - - - - - - - -Unk + N/A + \ + N/A + - - + + - - - - - - - - - -N/A + - \ N/A + - - + + N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A \ N/A N/A N/A N/A N/A - - - - - - - - - -N/A Unk - -N/A \ - - + + - -Unk -Unk + + - + + N/A + + + N/A + \ + + + - - - - - -Unk - + + N/A + + + N/A + - \ + + - - - - - - - - - -N/A - - -N/A - - - \ + - - - - - - - - - -N/A - - -N/A - - - - \
Mann Whitney U test for Sort List</p>
<p>VI. DISCUSSION</p>
<p>When analysing the results of prompt modification, we find something unexpected.Table V shows that the Code Llama-70b-Python model has no difference in code output for the Median of Two Sorted Arrays code problem as the similarity is 100.There is however a difference in the mean value of the en-
DC-33B DC-33B-E DC-33B-LF DC-33B-FL DC-33B-I DC-33B-I-E DC-33B-I-LF DC-33B-I-FL CL-70B CL-70B-E CL-70B-LF CL-70B-FL CL-70B-I CL-70B-I-E CL-70B-I-LF CL-70B-I-FL CL-70B-P CL-70B-P-E CL-70B-P-LF CL-70B-P-FL Models DC-33B DC-33B-E DC-33B-LF DC-33B-FL DC-33B-I DC-33B-I-E DC-33B-I-LF DC-33B-I-FL CL-70B CL-70B-E CL-70B-LF CL-70B-FL CL-70B-I CL-70B-I-E CL-70B-I-LF CL-70B-I-FL CL-70B-P CL-70B-P-E CL-70B-P-LF CL-70B-P-FL Models \ N/A + - - + - + Unk + N/A Unk Unk + N/A + Unk + Unk + N/A \ N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A -N/A \ - - + - + -Unk N/A -Unk Unk N/A Unk -Unk -Unk + N/A + \ + + + + + + N/A + + + N/A + + + + + + N/A + - \ + Unk + + + N/A + + + N/A + + + + + -N/A - - - \ -Unk - -N/A - - -N/A - - - - - + N/A + -Unk + \ + Unk + N/A + + + N/A + + + + + -N/A - - -Unk - \ - -N/A - - -N/A - - - - - Unk N/A + - - + Unk + \ + N/A Unk + + N/A + + + Unk + -N/A Unk - - + - + - \ N/A Unk Unk + N/A + Unk + - + N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A \ N/A N/A N/A N/A N/A N/A N/A N/A N/A Unk N/A + - - + - + Unk Unk N/A \ Unk + N/A + Unk + Unk + Unk N/A Unk - - + - + -Unk N/A Unk \ + N/A + Unk + - + -N/A Unk - - + - + - -N/A - - \ N/A Unk -Unk -Unk N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A \ N/A N/A N/A N/A N/A -N/A Unk - - + - + - -N/A - -Unk N/A \ -Unk -Unk Unk N/A + - - + - + -Unk N/A Unk Unk + N/A + \ + Unk + -N/A Unk - - + - + - -N/A - -Unk N/A Unk - \ -Unk Unk N/A + - - + - + Unk + N/A Unk + + N/A + Unk + \ + -N/A Unk - - + - + - -N/A - -Unk N/A Unk -Unk - \
Mann Whitney U test for Median of Two Sorted Arrays ergy datasets.This difference is 4.4%, 0.3% and 4.9% between the base prompt and the energy, library functions, and for-loop optimisation prompt respectively.As the code output is the same for all prompts, we think this difference in energy consumption is caused by the noise of background processes, which we discuss in our limitations.</p>
<p>Comparison for the same model and code problem in Figure 5 we find that the Mann Whitney U test assigns a plus (+) to the base prompt compared with the energy or the for-loop optimisation.The correct value for this comparison is unknown (Unk) because there should not be a significant difference in energy consumption between two similar code solutions.This example illustrates that we cannot draw conclusions with the Mann Whitney U test about the energy consumption difference of two solutions.</p>
<p>A. Energy Consumption</p>
<p>When comparing the mean value of the optimisation prompt energy dataset against the base prompt energy dataset, we see some notable differences.We look at percentage differences larger than 15% and try to find their cause.The code solution corresponding to the base prompt and the optimisations prompts for all LLMs are available in our Git repository 3 .We start with the results for the Sort list code problem, collected in Table V.We find that the for-loop opti-misation prompt consumes 21.7%, 21.6%, and respectively 59.7% less energy compared to the base prompt for the LLMs Code Llama-70b, Code Llama-70b-Instruct, and Code Llama-70b-Python.The optimised solutions convert the linked list to an array and use a library function to sort the array.The resulting sorted array must be converted back to a linked list.For this Code Llama-70b and Code Llama-70b-Instruct use a for-loop, the Code Llama-70b-Python uses a while-loop.The computationally intensive aspect of the solution is list sorting.Reconstructing the list into a linked list via a loop contributes to a minor portion of execution time and thus has small impact on the energy consumption compared to sorting using a library function or Python code. Figure 4 shows that the base prompt for Code Llama-70B-Python uses more energy than the base prompts for Code Llama-70B and is a difference in the code output between the base and the optimised code solutions.We find that the base solution merges the two input lists.This input list is sorted and based on the length of this list, the median value is retrieved.For the for-loop and energy optimisation, the input arrays are not merged and sorted.This solution searches the middle point of the two input arrays, as if they were combined, and determines the median based on the values around this center.This code solution does not contain a for-loop, instead containing a while-loop.</p>
<p>B. Total Number Of Floating Points</p>
<p>We measure the percentage difference in the total number of floating point operations between the base solution and an optimised solution.Across all three code problems, this percentage difference is zero.This means that we cannot use the total number of floating point operations to asses a difference between configurations.</p>
<p>C. Peak Memory Consumption</p>
<p>The difference in peak memory consumption between the base solution and the optimised solution is between -0.5% and 1.3% with two exceptions.These exceptions are 24.1% and 24.2%, indicating that the base prompt consumes less peak memory than the optimised prompt.The configurations that achieved these exception values are the LLMs Code Llama-70b-Instruct and Code Llama-70b with the for-loop optimisation prompt for the Sort List code problem.Because of the minimal difference, we cannot use peak memory consumption to asses a difference between configurations.</p>
<p>D. Answering the Research Questions</p>
<p>Regarding RQ (1), we find that the for-loop optimisation prompt most frequently triggers the LLMs to generate solutions that consume less energy compared to the base solution.These solutions do not have to include a for-loop, they may perform better due to the use of library functions.However, the for-loop optimisation does not consistently show a decrease in energy consumption across all LLMs or for the same LLM across all code problems.There is a case where the for-loop optimisation led to a 465.5% increase in energy consumption.We find that the base solution merges the two input lists with a + operation and sorts the result with a library function.The optimised solution merges and sorts the two input arrays in native Python code and uses three while loops.Therefore, we cannot conclude whether an optimisation prompt would consistently decrease energy consumption in all LLMs and code problems.This means we cannot determine with 100% confidence which prompt optimisation consumes less energy for a given LLM.</p>
<p>Regarding RQ (2), the Sort List code problem most frequently shows a decrease in energy consumption of more than 15% with prompt optimisation.However, this code problem does not show this decrease in all three optimisation prompts.</p>
<p>The answer to RQ (1) is that a specific combination of prompt optimisation, LLM and code problems can lead to a decrease in energy consumption.However, no single optimisation prompt consistently decreases the energy consumption for the same LLM across the different code problems.We have a similar answer for RQ (2).Based on the experimental results, we cannot determine the relation between coding problems and prompt optimisation for energy consumption.</p>
<p>E. LLM Code Output Similarity</p>
<p>We find that the similarity is 100% across all runs, with one exception.We investigated by checking the parameters of the LLMs.The Deepseek-coder LLMs are implemented via HuggingFace, which has a do_sample parameter which is by default false, we did not adjust this value, meaning that the LLM uses greedy decoding [28].This decoding strategy picks the token with the highest probability as the next token and the result is a deterministic output [29].The Code Llama LLMs use a parameter called temperature, which regulates the degree of randomness in the output of a LLM by adjusting the probability distribution of tokens [30].A temperature greater than 1 reduces the likelihood of highly probable tokens and enhances the likelihood of less probable tokens.A temperature less than 1 increases the likelihood of highly probable tokens while decreasing the likelihood of less probable tokens.The temperature ranges between 0 and 2, with zero representing a greedy decoding strategy.The default temperature for the Code Llama LLMs is set at 0.2.We did not adjust this value, which clarifies how the output of the Code Llama LLMs is the same across the 10 different runs.</p>
<p>F. Limitations</p>
<p>Our work uses prompts two LLMs on coding problems to analyse the effect on energy consumption.Our AI-generated code did not consistently outperform the human-generated code, as seen in related work [13].Our setup is naive, with one prompt and no hyperparameter tuning.A limitation is that we cannot rule out that different settings would lead to the same results.Next to that, we used a laptop for our measurements.A different hardware setup could lead to different conclusions, e.g.we might see a more distinct deviation if we use a GPU cluster instead.This hardware limitation is reflected in the deviations in Table V where the same outcome (similarity 100) sometimes leads to lower or higher energy consumption.Even though we tried to kill background processes and run the device in 'airplane' mode, we could not rule out noise between -4.9% and 0.5% in the mean energy measurements.</p>
<p>G. Future Work</p>
<p>RD1: Hardware energy measurement to measure more accurately.We also want to test the energy consumption of code on other hardware systems to find out if these have less noise, i.e. microcontrollers or embedded systems.RD2: Expand scope to include LLMs that are now unavailable such as ChatGPT.Next to that, extend the scope with more programming languages such as C, Java, PHP etc.</p>
<p>VII. CONCLUSIONS</p>
<p>This study sets out to explore to what extent LLMs can generates code with lower energy consumption than humanwritten code.We conduct experiments to examine the impact of prompt modification on the energy consumption of code generated by LLMs.We use several code-level energy-efficient best practices, popular code LLMs and code problems.</p>
<p>The answer to our main research question To what extent can prompts trigger a LLM to produce code that consumes less energy? is quite nuanced.We find that the for-loop prompt optimisation often results in a lower energy consumption compared to the base solution.However, the instructions provided by the for-loop prompt optimisation are not always implemented correctly.Some LLM-generated solutions outperform the base ones due to the replacement of native Python code with more efficient library functions.A poignant counterexample consists of a for-loop optimisation leading to a 465.5% increase in energy consumption compared to the base solution.The for-loop optimisation also does not consistently reduce energy consumption across all LLMs.</p>
<p>Fig. 1 .
1
Fig. 1.Comparison of the different LLMs and their prompts for all the problems.We abbreviate DeepSeek Coder to DC and Code Llama to CL. B represents the Base prompt, E represents the prompt for Energy efficiency, LF the prompt for Library Functions and finally FL the use of For Loops.Green shows that the configuration on the row consumes less energy than the configuration on the column for all the problems, red, the opposite.</p>
<p>Fig. 2 .
2
Fig. 2. Comparison of the different LLMs and their (optimised) prompt for all the problems.We abbreviate DeepSeek Coder to DC and Code Llama to CL. B represents the Base prompt, E represents the prompt for Energy efficiency, LF the prompt for Library Functions and finally FL the use of For Loops.The values indicate the number of times the unknown value occurred between the two configurations.</p>
<p>MODEL ABBREVIATIONS: DC = DEEPSEEK-CODER-33B, CL = CODELLAMA-70B.DC-33B DC-33B-E DC-33B-LF DC-33B-FL DC-33B-I DC-33B-I-E DC-33B-I-LF DC-33B-I-FL CL-70B CL-70B-E CL-70B-LF CL-70B-FL CL-70B-I CL-70B-I-E CL-70B-I-LF CL-70B-I-FL CL-70B-P CL-70B-P-E CL-70B-P-LF CL-70B-P-FL Models DC-33B DC-33B-E DC-33B-LF DC-33B-FL DC-33B-I DC-33B-I-E DC-33B-I-LF DC-33B-I-FL CL-70B CL-70B-E CL-70B-LF CL-70B-FL CL-70B-I CL-70B-I-E CL-70B-I-LF CL-70B-I-FL CL-70B-P CL-70B-P-E CL-70B-P-LF CL-70B-P-FL</p>
<p>Fig. 3 .
3
Fig. 3. Comparison of the LLMs and their (optimised) prompt for the Assign Cookies problem.A + means that the configuration on the row consumes more energy than the configuration on the column, a -is the opposite and Unk stands for Unknown meaning that both null hypotheses cannot be rejected.Model abbreviations: DC = deepseek-coder-33b, CL = CodeLlama-70b, I = instruction-tuned, Py = Python-version, E = prompt for energy-effiency, LF = library function &amp; FL = For-Loop.</p>
<p>-I-E CL-70B-I-LF CL-70B-I-FL CL-70B-P CL-70B-P-E CL-70B-P-LF CL-70B-P-FL</p>
<p>Fig. 4 .
4
Fig. 4. Comparison of the LLMs and their (optimised) prompt for the Sort List problem.A + means that the configuration on the row consumes more energy than the configuration on the column, a -is the opposite and Unk stands for Unknown meaning that both null hypotheses cannot be rejected.Model abbreviations: DC = deepseek-coder-33b, CL = CodeLlama-70b, I = instruction-tuned, Py = Python-version, E = prompt for energy-effiency, LF = library function &amp; FL = For-Loop.</p>
<p>Fig. 5 .
5
Fig. 5. Comparison of the LLMs and their (optimised) prompt for the Median of Two Sorted Arrays.A + means that the configuration on the row consumes more energy than the configuration on the column, a -is the opposite and Unk stands for Unknown meaning that both null hypotheses cannot be rejected.Model abbreviations: DC = deepseek-coder-33b, CL = CodeLlama-70b, I = instruction-tuned, Py = Python-version, E = prompt for energy-effiency, LF = library function &amp; FL = For-Loop.</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.
LevelEasyMediumHardCode Problem Assign Cookies Sort List Median of Two Sorted ArraysTABLE IIOVERVIEW OF THE DIFFERENT CODE PROBLEMS1 https://github.com/eugeneyan/open-llms, accessed on 27-03-2024.2 https://leetcode.com/problemset/, accessed on 02-03-2024.32</p>
<p>TABLE III OVERVIEW
III
OF THE PROMPT OPTIMISATION SENTENCE TYPES.</p>
<p>. The value 1 indicates that the optimisation is present in the code solution.This means that a minimum of one instance is present in the solution.The * indicates that the optimisation 34 Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.
ModelPromptAssign CookiesSort ListMedian of Two Sorted ArraysSim. Energy Mem. FLOPs Runtime Sim. Energy Mem. FLOPs Runtime Sim. Energy Mem. FLOPs Runtimeenergy 90.6 13.4 -0.1 0.010.6 66.9 1.70.00.00.9/////DC-33b-base lib func 90.6 8.80.10.011.0 98.6 -0.3 0.00.00.1 100.0 -3.1 0.00.00.0for-loop 100.0 -3.8 0.10.00.474.1 -0.2 -0.1 0.0-1.591.2 465.5 1.00.0496.6energy 100.0 0.20.10.0-0.492.2 -0.8 0.00.00.845.7 -17.3 1.30.0-19.1DC-33b-inst lib func 100.0 0.30.00.0-0.3 100.0 -1.3 -0.3 0.0-0.6 100.0 0.50.00.0-0.1for-loop 64.5 -26.8 -8.3 0.0-26.9 93.0 2.9 -0.2 0.03.545.7 -17.4 1.10.0-19.3energy 96.9 4.0 -0.1 0.00.292.9 -2.0 0.10.0-1.288.2 -3.3 -0.5 0.0-0.3CL-70b-inst lib func 96.9 -2.1 0.20.00.497.3/////////for-loop 96.9 -1.7 0.00.00.377.7 -21.6 24.1 0.0-19.1 88.2 -1.8 -0.3 0.00.0energy 96.9 -0.5 -0.2 0.0-0.292.9 -0.9 0.30.0-1.288.2 -3.2 0.20.00.0CL-70blib func 96.9 -6.1 -0.2 0.0-0.497.3/////////for-loop 96.9 -4.2 -0.2 0.01.077.7 -21.7 24.2 0.0-19.3 88.2 -2.</p>
<p>TABLE V THE
V
BASE CODE PROMPT SOLUTION COMPARED TO THE OPTIMISED PROMPT SOLUTIONS ACROSS CODING PROBLEMS.NEGATIVE PERCENTAGES INDICATE THAT THE BASE CODE PROMPT SOLUTION HAS A HIGHER VALUE AND VICE-VERSA FOR POSITIVE VALUES.THE MEAN VALUE OF 50 SAMPLES IS TAKEN FOR THE METRICS ENERGY, PEAK MEMORY CONSUMPTION, AND RUNTIME.WE WRITE A MISSING VALUE "/" IF THE MODEL DOES NOT OUTPUT FOR THE PROMPT.</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
https://github.com/tcappendijk/Greencode framework
ACKNOWLEDGMENTSWe thank Oracle, in particular Bas Oudejans and Marc Ordelman for their support.This research was supported by GreenDIGIT, an European Union project funded under the grant agreement 101131207, and the Universiteit van Amsterdam Master Artificial Intelligence.Code Llama-70B-Instruct.Next to that, the for-loop optimised solution of Code Llama-70B-Python uses less energy than the for-loop-optimised solutions for Code Llama-70B and Code Llama-70B-Instruct.This means the for-loop optimisation for Code Llama-70B-Python has the largest decrease in energy consumption, as its base prompt uses the most energy, but its optimised prompt uses the least.The library functions prompt optimisation consumes 59.4% less energy compared to the base prompt for the Code Llama-70b-Python model.We find that the difference on a code level between the base on the optimisation is the same as for the Code Llama-70b-Python LLM with the for-loop optimisation.The two DeepSeek-Coder LLMs do not improve.Their base code solution sorts the linked list inPython code, the optimised solutions do not change to sorting with a library function.For the Assign cookies code problem, we find that the DeepSeek-coder-33b-instruct LLM with the for-loop prompt optimisation consumes 26.8% less energy compared to the base prompt.On a code level we find that the base version updates a counter, and indexes with this counter in both input lists.The for-loop optimised solution, does not loop through the sorted input lists.Instead, it uses the -1 index notation to get the last element of the list.The break condition of the loop is when both input lists are empty.This optimised solution does not contain a for-loop.Instead, the difference between the base and the optimised code solution is within using the library function pop() and index method.For the Median of Two Sorted Arrays code problem, we look at Table V.For the DeepSeek-Coder-33b-base LLM, we find that the for-loop optimisation has an increase in energy consumption of 465.5% compared to the base prompt.We find that the base solution merges the two input lists with a + operation and sorts the result with a library function.The optimised solution is merging and sorting the two input arrays in native Python code and uses three while-loops for this.The DeepSeek-Coder-33b-instruct LLM compared with the base prompt shows a decrease of 17.3% and 17.4% in energy consumption for the energy and for-loop optimisation prompts respectively.The code solutions for the energy and for-loop optimisation prompts are the same.There
Greenhouse effect: greenhouse gases and their impact on global warming. D W Kweku, O Bismark, A Maxwell, K A Desmond, K B Danso, E A Oti-Mensah, A T Quachie, B B Adormaa, Journal of Scientific research and reports. 1762018</p>
<p>European green deal. European Council, March 2024</p>
<p>Energy, climate: which virtual words for which real world. 202373The Shift Project</p>
<p>Green coding: Reduce your carbon footprint. R Radersma, Ethical Software Engineering and Ethically Aligned Design. 2022</p>
<p>On evaluating the efficiency of source code generated by llms. C Niu, T Zhang, C Li, B Luo, V Ng, Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering. the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering2024</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and TransparencyAssociation for Computing Machinery2021</p>
<p>D Patterson, J Gonzalez, Q Le, C Liang, L.-M Munguia, D Rothchild, D So, M Texier, J Dean, arXiv:2104.10350Carbon emissions and large neural network training. 2021arXiv preprint</p>
<p>Great power, great responsibility: Recommendations for reducing energy for training language models. J Mcdonald, B Li, N Frey, D Tiwari, V Gadepally, S Samsi, Findings of the Association for Computational Linguistics: NAACL 2022. 2022</p>
<p>Hyperparameter power impact in transformer language model training. L H P De Chavannes, M G K Kongsbak, T Rantzau, L Derczynski, Proceedings of the second workshop on simple and efficient natural language processing. the second workshop on simple and efficient natural language processing2021</p>
<p>From words to watts: Benchmarking the energy costs of large language model inference. S Samsi, D Zhao, J Mcdonald, B Li, A Michaleas, M Jones, W Bergeron, J Kepner, D Tiwari, V Gadepally, 2023 IEEE High Performance Extreme Computing Conference (HPEC). IEEE2023</p>
<p>Energy-efficient inference service of transformer-based deep learning models on gpus. Y Wang, Q Wang, X Chu, 2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics). IEEE2020</p>
<p>Learn to code sustainably: An empirical study on green code generation. T Vartziotis, I Dellatolas, G Dasoulas, M Schmidt, F Schneider, T Hoffmann, S Kotsopoulos, M Keckeisen, Proceedings of the 1st International Workshop on Large Language Models for Code, ser. LLM4Code '24. the 1st International Workshop on Large Language Models for Code, ser. LLM4Code '24Association for Computing Machinery2024</p>
<p>A controlled experiment on the energy efficiency of the source code generated by code llama. V.-A Cursaru, L Duits, J Milligan, D Ural, B R Sanchez, V Stoico, I Malavolta, International Conference on the Quality of Information and Communications Technology. Springer2024</p>
<p>Finding significant differences in the energy consumption when comparing programming languages and programs. L Koedijk, A Oprescu, 2022 International Conference on ICT for Sustainability (ICT4S). IEEE2022</p>
<p>Code llama: Open foundation models for code. B Roziere, J Gehring, F Gloeckle, S Sootla, I Gat, X E Tan, Y Adi, J Liu, T Remez, J Rapin, arXiv:2308.129502023arXiv preprint</p>
<p>Deepseek-coder: When the large language model meets programming-the rise of code intelligence. D Guo, Q Zhu, D Yang, Z Xie, K Dong, W Zhang, G Chen, X Bi, Y Wu, Y Li, arXiv:2401.141962024arXiv preprint</p>
<p>D Jurafsky, J H Martin, Speech and Language Processing: An Introduction to Natural Language Processing. 2024Computational Linguistics, and Speech Recognition. Stanford University</p>
<p>Github copilot is generally available to all developers. T Dohmke, July. 2022252023</p>
<p>Codewhisperer. Web Amazon, Services, 2022</p>
<p>Introducing ChatGPT. Openai, Nov. 2022</p>
<p>Tutorial: Linux kernel profiling with perf. May 2024</p>
<p>Running average power limit energy reporting. Intel, 26 April 2024</p>
<p>Measuring energy consumption for short code paths using rapl. M Hähnel, B Döbel, M Völp, H Härtig, ACM SIGMETRICS Performance Evaluation Review. 4032012</p>
<p>Python-the fastest growing programming language. K Srinath, International Research Journal of Engineering and Technology. 4122017</p>
<p>pycode similar: Python code comparison tool. Fyrestone, 2024. May 2024</p>
<p>Energy efficiency across programming languages: how do energy, time, and memory relate?. R Pereira, M Couto, F Ribeiro, R Rua, J Cunha, J P Fernandes, J Saraiva, Proceedings of the 10th ACM SIGPLAN international conference on software language engineering. the 10th ACM SIGPLAN international conference on software language engineering2017</p>
<p>The mann-whitney u: A test for assessing whether two independent samples come from the same distribution. N Nachar, Tutorials in Quantitative Methods for Psychology. 20084</p>
<p>Documentation. June 2024Hugging Face</p>
<p>Text generation strategies. Huggingface, June 2024</p>
<p>Is temperature the creativity parameter of large language models. M Peeperkorn, T Kouwenhoven, D Brown, A Jordanous, arXiv:2405.004922024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
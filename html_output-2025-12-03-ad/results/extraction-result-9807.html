<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9807 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9807</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9807</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-5f86c548675f54299a0a1f7abe2b6ca9d3a3296c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5f86c548675f54299a0a1f7abe2b6ca9d3a3296c" target="_blank">TeleQnA: A Benchmark Dataset to Assess Large Language Models Telecommunications Knowledge</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The automated question generation framework responsible for creating TeleQnA, the first benchmark dataset designed to evaluate the knowledge of Large Language Models (LLMs) in telecommunications, is outlined, illustrating that LLMs can rival the performance of active professionals in telecom knowledge, thanks to their capacity to process vast amounts of information.</p>
                <p><strong>Paper Abstract:</strong> We introduce TeleQnA, the first benchmark dataset designed to evaluate the knowledge of Large Language Models (LLMs) in telecommunications. Comprising 10,000 questions and answers, this dataset draws from diverse sources, including standards and research articles. This paper outlines the automated question generation framework responsible for creating this dataset, along with how human input was integrated at various stages to ensure the quality of the questions. Afterwards, using the provided dataset, an evaluation is conducted to assess the capabilities of LLMs, including GPT-3.5 and GPT-4. The results highlight that these models struggle with complex standards related questions but exhibit proficiency in addressing general telecom-related inquiries. Additionally, our results showcase how incorporating telecom knowledge context significantly enhances their performance, thus shedding light on the need for a specialized telecom foundation model. Finally, the dataset is shared with active telecom professionals, whose performance is subsequently benchmarked against that of the LLMs. The findings illustrate that LLMs can rival the performance of active professionals in telecom knowledge, thanks to their capacity to process vast amounts of information, underscoring the potential of LLMs within this domain. The dataset has been made publicly accessible on GitHub.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9807",
    "paper_id": "paper-5f86c548675f54299a0a1f7abe2b6ca9d3a3296c",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0034644999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TeleQnA: A Benchmark Dataset to Assess Large Language Models Telecommunications Knowledge</h1>
<p>Ali Maatouk ${ }^{<em> \dagger}$, Fadhel Ayed ${ }^{</em> \dagger}$, Nicola Piovesan ${ }^{\dagger}$, Antonio De Domenico ${ }^{\dagger}$, Merouane Debbah ${ }^{\ddagger}$, Zhi-Quan Luo ${ }^{\S}$<br>${ }^{\dagger}$ Paris Research Center, Huawei Technologies, Boulogne-Billancourt, France<br>${ }^{\ddagger}$ Khalifa University of Science and Technology, Abu Dhabi, UAE<br>${ }^{\S}$ The Chinese University of Hong Kong, Shenzhen, China</p>
<h4>Abstract</h4>
<p>We introduce TeleQnA ${ }^{1}$, the first benchmark dataset designed to evaluate the knowledge of Large Language Models (LLMs) in telecommunications. Comprising 10,000 questions and answers, this dataset draws from diverse sources, including standards and research articles. This paper outlines the automated question generation framework responsible for creating this dataset, along with how human input was integrated at various stages to ensure the quality of the questions. Afterwards, using the provided dataset, an evaluation is conducted to assess the capabilities of LLMs, including GPT-3.5 and GPT-4. The results highlight that these models struggle with complex standardsrelated questions but exhibit proficiency in addressing general telecom-related inquiries. Additionally, our results showcase how incorporating telecom knowledge context significantly enhances their performance, thus shedding light on the need for a specialized telecom foundation model. Finally, the dataset is shared with active telecom professionals, whose performance is subsequently benchmarked against that of the LLMs. The findings illustrate that LLMs can rival the performance of active professionals in telecom knowledge, thanks to their capacity to process vast amounts of information, underscoring the potential of LLMs within this domain. The dataset has been made publicly accessible on GitHub ${ }^{2}$.</p>
<h2>I. INTRODUCTION</h2>
<p>LLMs have recently sparked a revolution in the realm of Natural Language Processing (NLP), elevating automatic text generation and interaction to high levels of performance. Currently, the advent of LLMs has begun to impact many other fields beyond NLP, such as medicine [1] and finance [2]. Much like these fields, the telecom industry stands on the brink of experiencing the potential impact LLMs could have on its landscape. Indeed, it is foreseen that LLMs may greatly facilitate a wide range of tasks, including troubleshooting anomalies, comprehending standards, and providing optimization recommendations for network enhancement [3]-[5].</p>
<p>The success of LLMs depends critically on benchmark datasets designed to assess their proficiency in specific domains. These datasets also play a pivotal role in determining the optimal architectural design for LLMs and guiding the pretraining procedure in these specialized fields. In the context of NLP applications, notable examples include the TriviaQA [1], HelloSwag [6], and SIQA [7] datasets, tailored to evaluate LLMs capabilities in reading comprehension, commonsense reasoning, and social intelligence, respectively. The same can</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>be observed in other domains, such as medicine and finance, where benchmark datasets like MultiMedQA [1] and FLUE [8] have been introduced to assess the proficiency of LLMs in these fields.</p>
<p>As LLMs find their way into the telecommunications industry, a clear and pressing issue arises-there is a notable absence of a benchmark dataset designed to evaluate these models' proficiency in telecom. Consequently, there is an urgent need for such a dataset, as highlighted in various prior research (e.g., [9]). This paper aims to bridge this gap by introducing TeleQnA, the first benchmark dataset tailored specifically for evaluating LLMs' telecom knowledge. The contributions of our paper are fourfold:</p>
<ul>
<li>First, we curate a comprehensive dataset that encompasses a vast corpus, spanning various disciplines within the telecommunications domain and taking on diverse forms, such as standards and research articles.</li>
<li>Second, we introduce an automated Question and Answer (QnA) generation process based on the gathered material, involving two LLMs communicating with one another. We show how this architecture, complemented by human input at different stages, not only ensures the quality of the generated dataset but also facilitates its scalability to accommodate a wide array of multidisciplinary topics.</li>
<li>Third, we proceed to assess the telecom knowledge of prominent LLMs: GPT-3.5 and GPT-4 [10]. We illustrate their proficiency in responding to general telecom inquiries and highlight their deficiencies in addressing intricate questions related to standards specifications. Additionally, we demonstrate how the incorporation of contextual information significantly enhances these models' performance, particularly in areas where they typically face difficulties. All of these findings underscore the necessity for a specialized telecom foundation model to fully harness the potential of LLMs within the industry.</li>
<li>Lastly, we assess the performance of professionals actively engaged in this field in comparison to that of LLMs. Our findings demonstrate that LLMs can compete with active professionals in telecom knowledge. This potential stems from their ability to process extensive volumes of information, highlighting the valuable role LLMs are poised to play in shaping the future of this domain.</li>
</ul>
<h2>II. DATASET SOURCES</h2>
<p>When curating the dataset sources, our primary objective was to assemble a diverse and comprehensive collection of</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Distribution of the QnA dataset among the categories of the collected source materials.</p>
<p>Open-access data related to the telecom industry. This involved a broad spectrum of content, encompassing standards and scholarly research materials. In total, our collection comprised around 25,000 pages, containing approximately 6 million words. This approach guarantees that the resultant QnA dataset offers an in-depth and well-rounded depiction of the telecom industry's multifaceted aspects. Furthermore, this diversity serves as a valuable resource for evaluating the strengths of an LLM in various aspects, including standard specifications and general telecom expertise. The allocation of the QnA dataset among the three specified categories – research, standards, and lexicon – is illustrated in Fig. 1.</p>
<h3>A. Standards Documents</h3>
<p>Standards play a pivotal role in the field of telecommunications by ensuring that different technologies from various vendors can work together cohesively. These standards are established and maintained by recognized standardization bodies, including but not limited to 3GPP, IEEE, and ITU. Incorporating these documents when creating the QnA dataset is immensely valuable for capturing the intricacies of the telecom industry. Particularly, it allows delving deep into the technical facets and the present-day implementation of telecom technologies. With this in mind, we have integrated two distinct document sets into our raw data, all closely associated with standard activities:</p>
<ul>
<li><strong>Technical Specifications</strong>: Technical specifications are detailed documents that define the technical standards for telecommunications systems. Given the sheer number of these documents, we have uniformly sampled a portion of them to ensure a non-biased representative selection of these documents across multiple standardization bodies.</li>
<li><strong>Standards Overview</strong>: To offer a more comprehensive perspective on standards, moving beyond the intricate technical specifications, we have gathered materials from various sources, encompassing standards summaries, review publications, and standards-related white papers.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Word cloud illustrating the prominent keywords extracted from the collected research materials.</p>
<h3>B. Research Material</h3>
<p>The landscape of publishing venues within the telecommunications field is vast. It encompasses a wide array of publishers, and is presented in diverse formats such as journals and conferences, with each repository housing a plethora of valuable information. The sheer abundance and diversity of material available pose a considerable challenge when attempting to create a QnA dataset based on these scholarly works. Additionally, the varying levels of citation and peer review standards, coupled with our commitment to exclusively use open-access material, introduce another layer of complexity to the task of curating a reliable and representative dataset. To address these challenges, we have categorized our sources into two groups: <em>Research publications</em> and <em>Research overview</em>. The former category primarily comprises technical details sourced from research articles and in-depth technical open-access books, while the second includes content from review and survey venues. Within each of these categories, we have adhered to the subsequent set of guiding principles:</p>
<ul>
<li><strong>Diversity</strong>: Our first guiding principle was the diversity of material. In particular, we considered a broad spectrum of outlets, encompassing well-established venues hosted by various publishers.</li>
<li><strong>Relevance</strong>: Within these outlets, we have chosen to identify the most influential material by considering their citation counts over the past 15 years. Our selection of this time frame is deliberate, as it allows us to incorporate the most recent and pertinent findings. On the other hand, the number of citations serves as an indicator of a manuscript popularity and impact over time. Furthermore, to infuse the dataset with the insights of industry experts, we have added material that telecom professionals have.</li>
</ul>
<p>identified as pivotal contributions to this collection.</p>
<ul>
<li>Bias Minimization: To mitigate potential biases within the dataset, we have also taken a deliberate approach to uniformly sample a number of manuscripts from these venues. This strategy enhances the dataset diversity, encompassing not only popular topics but also those that, while less prominent, remain highly pertinent to the field.</li>
<li>Multi-disciplinarity: We have ensured that our sources extend beyond the realm of pure communications and encompass topics relevant to the field. This expanded scope includes material from various domains, such as machine learning and optimization. This approach stems from the recognition that an LLM tailored for the telecom sector should possess a degree of proficiency in interconnected domains. Although the majority of the sources consist of communication-oriented material, this diverse and multidisciplinary knowledge enriches the dataset, fortifying its comprehensive and interdisciplinary nature. A word cloud representing the keywords extracted from the considered research material is illustrated in Fig. 2.</li>
</ul>
<h2>C. Telecom Lexicon</h2>
<p>The telecommunications industry possesses a distinctive lexicon, characterized by an extensive array of technical terms, each holding significance in understanding the intricacies of communication networks. The incorporation of this lexicon into the generation of the QnA dataset holds significant value, as it provides a perfect avenue to evaluate the proficiency of an LLM in comprehending telecommunications terminology. To procure this source of data, we conducted a thorough examination of the standards documents and research papers we have collected and created a comprehensive dictionary of the technical terminology used therein.</p>
<h2>III. DATASET CHARACTERISTICS</h2>
<h2>A. Question Type</h2>
<p>With the source material prepared, our first course of action involved adopting a question type, and after careful consideration, we opted for a multiple-choice approach. This decision was made for several reasons:</p>
<ul>
<li>Precision Assessment: The multiple-choice format offers a straightforward means of gauging the accuracy of an LLM when presented with the dataset.</li>
<li>Complex Decision-Making: The multiple-choice format allows us to probe deeper into the LLM's decisionmaking capabilities. It enables us to assess whether the LLM can accurately discern situations where multiple choices may be correct, rather than just one.</li>
<li>Nuanced Discrimination: By adopting a multiple-choice framework, we can examine the LLM's ability to select the correct answer when presented with options that resemble the correct choice but are ultimately incorrect. For the reasons mentioned above, multiple-choice approaches have gained popularity in the realm of machine learning benchmarks. For instance, they have been prominently utilized in datasets like MCTest and SWAG [11], [12].</li>
</ul>
<h2>B. Dataset Format</h2>
<p>To maintain consistency across all questions, we have implemented a standardized format. Each question is represented in JSON format, comprising five distinct fields:</p>
<ul>
<li>Question: This field consists of a string that presents the question associated with a specific concept within the telecommunications domain.</li>
<li>Options: This field comprises a set of strings representing the various answer options.</li>
<li>Answer: This field contains a string that adheres to the format "option ID: Answer" and presents the correct response to the question. A single option is correct; however, options may include choices like "All of the Above" or "Both options 1 and 2".</li>
<li>Explanation: This field encompasses a string that clarifies the reasoning behind the correct answer.</li>
<li>Category: This field includes a label identifying the source category depicted in Fig. 1.</li>
</ul>
<h2>IV. DataSET CREATION</h2>
<p>To construct a comprehensive QnA dataset covering the multifaceted domain of telecommunications, a substantial number of questions is required. This task is further complicated by the specialized nature of telecom knowledge, demanding expertise to craft pertinent questions, answers, and explanations. Moreover, the telecom documents we collected often contain highly intricate information, making it infeasible for a team of human experts to generate questions and answers that comprehensively cover the diverse range of telecom subdomains. Adding to the complexity, we require these questions to be challenging to answer, and asking human experts to craft incorrect options that are also challenging across a wide array of subdomains is a task far from trivial. Given these challenges, adopting a crowdsourcing approach, similar to the one employed for the MCTest dataset [11], becomes impractical. To address these challenges, we leveraged OpenAI's GPT3.5's API ${ }^{3}$ to facilitate the generation process. This involved developing an automated framework based on LLMs, which will be elaborated on in this section. An overview of the entire process can be found in Figure 3.</p>
<h2>A. Preprocessing</h2>
<p>Our initial step involved cleaning and formatting the collected data sources for seamless integration with the utilized LLMs. In parallel, we created acronyms extractors that identify the acronyms found in these documents and their corresponding definitions, allowing us to grasp the terminology employed within these materials.</p>
<h2>B. LLM Agents</h2>
<p>Our LLM-based framework revolves around two GPT-3.5 LLM agents: a generator and a validator, each with distinct roles as outlined below.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A high-level overview of the QnA generation process.</p>
<ul>
<li>
<p><strong>Generator:</strong> The generator is an LLM instance responsible for producing multiple-choice questions. To accomplish this, it is supplied with a segment of a telecommunications document, selected from those discussed in Section II. Subsequently, it is instructed to generate a set of multiple-choice questions based on the given context, adhering to the formatting guidelines. Additionally, it is directed to ensure that the questions are self-contained. Furthermore, it is prompted to provide a rationale for the correct answer, typically by referencing sentences within the document to substantiate the choice of the answer.</p>
</li>
<li>
<p><strong>Validator:</strong> The questions and options generated by the generator, together with the contextual information (i.e., the relevant segment of the document), are given as input to another GPT-3.5 LLM, referred to as the validator. To prevent any biasing, the answer and the rationale behind the correct answer are intentionally omitted when providing input to the validator. The validator's primary function is to select an option among those provided solely based on its knowledge and the contextual information provided. If the validator picks the option designated as correct by the generator, the question is retained. However, if the validator selects an incorrect option, the question is discarded. This automated layer serves to enhance the overall accuracy and mitigate any potential instances of misinformation or fabrication that the generator may introduce.</p>
</li>
</ul>
<h3><em>C. Post-Processing</em></h3>
<p>Moving forward in the process, we transition into the post-processing stage. Following the initial validation step, this phase focuses on the following key aspects:</p>
<ul>
<li>
<p><strong>Filtering:</strong> We developed a tool designed to identify and filter out questions that are not self-contained, meaning they refer to the text from which are taken from by including particular terms such as "figure" and "fig".</p>
</li>
<li>
<p><strong>Shuffling:</strong> Our experiments have revealed a tendency of GPT-3.5 to place the correct answer in option 1. To mitigate this bias, we shuffled the options to ensure equal likelihood for all choices.</p>
</li>
<li>
<p><strong>Acronyms:</strong> We designed acronym detectors that identify the use of acronyms in the questions and map them to their respective definitions.</p>
</li>
</ul>
<h3><em>D. Human Validation - First Stage</em></h3>
<p>Having validated the questions using the validator and completed the necessary post-processing steps, we now introduce the first stage that incorporates human intervention. Specifically, a telecom expert is assigned the task of carefully reviewing each question, the provided answer options, the explanation given by the question generator for the answer, and the contextual information used. The objective is to make an informed decision regarding whether or not to discard a question. The primary objectives of this human-in-the-loop process are twofold:</p>
<ul>
<li>
<p><strong>Ensure question correctness:</strong> The expert's role is to verify the accuracy of the questions, their associated options, and the designated correct answer, effectively eliminating improperly generated questions.</p>
</li>
<li>
<p><strong>Ensure that the questions are self-contained:</strong> The questions should not revolve around content that is of localized interest, such as experimental results specific to a proposed algorithm.</p>
</li>
</ul>
<h3><em>E. Refinement</em></h3>
<p>The collected telecom sources inherently exhibit overlaps across the various venues. This overlap gives rise to redundancy within the QnA dataset, a challenge that necessitates a refinement of the dataset. To address this issue systematically, we employed Open AI's Ada v2 text embeddings model to compute embeddings for all the questions in the QnA dataset along with their corresponding explanations. These embeddings serve as compact numerical representations, encapsulating the essence of phrases, thus enabling us to gauge the semantic similarity between any two questions by measuring the proximity of their respective embeddings in this high dimensional space. Having computed these embeddings, we then applied the K-Means clustering algorithm to group questions into clusters, in order to streamline the subsequent human intervention step. In fact, the concluding phase encompasses a secondary human validation that serves a dual purpose:</p>
<ul>
<li>
<p><strong>Eliminating redundant questions:</strong> The primary goal of the intervention is to eliminate duplicate or semantically-redundant questions within each cluster.</p>
</li>
<li>
<p><strong>Final iteration of quality checking:</strong> The second objective entails conducting a final round of verification, similar to the first human validation stage.</p>
</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>Run 1</th>
<th>Run 2</th>
<th>Run 3</th>
<th>Run 4</th>
<th>Mean</th>
<th>Std</th>
</tr>
</thead>
<tbody>
<tr>
<td>50 questions</td>
<td>63.35</td>
<td>65.63</td>
<td>65.10</td>
<td>63.79</td>
<td>64.46</td>
<td>0.92</td>
</tr>
<tr>
<td>25 questions</td>
<td>65.19</td>
<td>67.56</td>
<td>66.90</td>
<td>66.30</td>
<td>66.48</td>
<td>0.87</td>
</tr>
<tr>
<td>10 questions</td>
<td>67.64</td>
<td>64.32</td>
<td>66.94</td>
<td>67.20</td>
<td>66.52</td>
<td>1.29</td>
</tr>
<tr>
<td>5 questions</td>
<td>65.45</td>
<td>67.55</td>
<td>68.52</td>
<td>66.85</td>
<td>67.09</td>
<td>1.11</td>
</tr>
<tr>
<td>1 question</td>
<td>66.43</td>
<td>67.48</td>
<td>68.10</td>
<td>66.46</td>
<td>67.11</td>
<td>0.70</td>
</tr>
</tbody>
</table>
<p>Table I: Illustration of GPT-3.5’s accuracy (%) across multiple runs and question batch sizes.</p>
<h2>V Performance Evaluation</h2>
<p>In this section, we conduct a comprehensive evaluation of GPT-3.5 and GPT-4 using the TeleQnA dataset. Our objective is to assess their proficiency in telecom knowledge while also offering insights into their strengths and weaknesses. Additionally, we compare the performance of telecom professionals to these language models, establishing a benchmark for reference. As a performance measure, we define the accuracy as the percentage of questions for which the entity at hand selected the option marked as correct in the dataset.</p>
<h3>A. Batch Size and Variance Study</h3>
<p>Our numerical analysis begins by examining the variance of the accuracy achieved by the LLMs when responding to the TeleQnA dataset. Specifically, we are interested in tracking how the accuracy evolves with each iteration of LLM questioning. Additionally, we investigate the impact of sending questions in random batches of varying sizes ($B=1,5,10,25$, and $50$) to the LLM. The former investigation aims to show the number of iterations required to obtain a statistically reliable accuracy score for the LLM’s telecom knowledge. Meanwhile, the latter investigation seeks to identify trends when querying the LLM’s API with larger question batches, potentially streamlining the process by sending questions in batches rather than one by one. To obtain these findings, we uniformly sampled 1000 questions and conducted accuracy assessments across multiple runs and with different batch sizes. The results of these experiments are presented in Table I for GPT-3.5^{4}.</p>
<p>The results demonstrate the consistency of accuracy across multiple runs, with a standard deviation of approximately 1%. Given that the standard deviation decreases with the number of questions considered, it can be inferred that this deviation is even smaller when applied to the entire dataset. Therefore, we can conclude that conducting a single run is sufficiently reliable for assessing the proficiency of the LLM. Conversely, we observe a decline in mean accuracy when questions are submitted in larger batches. This decrease may be attributed to the inherent diversity of questions stemming from various sub-domains within telecommunications. When questions are batched together, the LLM encounters a broader spectrum of topics, which may result in less specialized responses and, therefore, a potential drop in accuracy. However, we note that this decline in performance remains modest when transitioning from a batch size of $B=1$ to $B=5$. Thus, we have chosen to</p>
<ul>
<li>GPT-4 exhibited similar trends, and therefore, we have opted to exclude the specific details.
<img alt="img-3.jpeg" src="img-3.jpeg" /></li>
</ul>
<p>Figure 4: Accuracy (%) comparison among GPT-3.5, GPT-4, and active professionals in all five categories.</p>
<p>utilize this batch size in the rest of our experiments to reduce the number of queries needed for assessment.</p>
<h3>B. Performance Benchmarks</h3>
<p>We evaluate the performance of GPT-3.5 and GPT-4 across the various categories of the TeleQnA dataset. The results are reported in Fig. 4, and the details can be found in Appendix B. As anticipated, GPT-4 consistently outperforms GPT-3.5, demonstrating around 7% improvement across all categories. Notably, LLMs exhibit exceptional performance in the lexicon category, which encompasses general telecom knowledge and terminology, achieving approximately 87% accuracy for GPT-4. Conversely, these models face challenges when confronted with more intricate questions related to standards, with the highest performing model, GPT-4, achieving a modest 64% accuracy in this domain. In summary, GPT-3.5 averaged an accuracy of 67%, while GPT-4 achieved an accuracy of 74%. These results demonstrate that these models possess a solid foundation in general telecom expertise. However, to attain higher accuracy in responding to complex inquiries, further adaptations to the telecom domain are necessary.</p>
<p>In our final step, we conducted a performance benchmark comparing active professionals to LLMs. To accomplish this, we designed surveys consisting of ten questions, with two questions representing each of the five categories. These surveys were then distributed to thirty active professionals in the telecom field, working in various domains such as standardization, signal processing, optical networks, etc. These professionals were explicitly instructed not to utilize search engines or external references, relying solely on their personal knowledge. The results reveal that LLMs and active professionals exhibit comparable performance in general telecom knowledge. However, in the case of intricate questions related to research and standards, LLMs demonstrate the capability to rival these professionals. This is attributed to LLMs’ ability to digest and memorize complex and intricate documents.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Accuracy (\%) comparison among GPT-3.5, GPT-4, and GPT-3.5 with context in the standards specifications category.</p>
<p>Furthermore, it is crucial to recognize the challenge faced by professionals when responding to these questions, as they encompass a broad range of telecom subdomains that these individuals may not be necessarily actively engaged with in their work. Considering all factors, our results underscore the significant promise that LLMs hold within this domain, as demonstrated by their competitiveness within this extensive and comprehensive dataset.</p>
<h2>C. Influence of Context</h2>
<p>Until this stage of our benchmarking process, we have been querying the LLMs without accompanying contextual information for the questions. Nevertheless, in this subsection, our goal is to investigate how supplementing questions with additional context affects the accuracy of these models.</p>
<p>To accomplish this, we have focused on the standards specifications sources, encompassing thousands of technical standards pages. Our selection of this category is driven by the fact that this is where LLMs have exhibited the lowest performance. With this in mind, we segmented these pages into approximately 500-word segments before generating embeddings for each segment using OpenAI's Ada v2 text embeddings. Moreover, we employed the same OpenAI model to create embeddings for the questions and corresponding options belonging to this category. Following that, we constructed a distance matrix between the embeddings of each question-options pair and those of each segment. The next step involved querying the LLM by supplying batches of five questions-options pairs, and additionally, as context, the top3 closest segments to the five question-option pairs based on the distance matrix. The outcomes of these experiments are illustrated in Fig. 5.</p>
<p>The $22.5 \%$ relative accuracy gain highlights the significant enhancement in performance achieved by incorporating contextual information, demonstrating how even less advanced models like GPT-3.5 can match the performance of state-of-the-art GPT-4 model. This underscores the necessity for a specialized telecom language models, fine-tuned or trained specifically on telecom-related data. Developing such a foundation model has the potential to push the boundaries of LLMs performance in the telecom domain, paving the way for a wide range of use cases that demand telecom knowledge and expertise.</p>
<h2>VI. CONCLUSIONS, LIMITATIONS, AND FUTURE DIRECTIONS</h2>
<h2>A. Conclusions</h2>
<p>In this paper, we introduced TeleQnA, the first open-source benchmark dataset tailored specifically to the telecom industry. With a collection of 10,000 questions and answers drawn from various sources in the field, TeleQnA serves as an evaluation tool for assessing the knowledge of LLMs in the telecommunications domain. Throughout this paper, we have detailed the development process of TeleQnA and conducted an extensive evaluation of GPT-3.5 and GPT-4 using TeleQnA. The results have revealed interesting insights: although these models excel in addressing general telecom-related questions, they face challenges when tackling complex inquiries. Importantly, our findings emphasize the critical need for specialized telecom foundation models.</p>
<h2>B. Limitations</h2>
<p>While our framework's design is aimed at minimizing errors in the questions, it remains dependent on human judgment to determine whether a generated question should be retained or discarded. This introduces an element of subjectivity, particularly in cases where questions offer multiple valid options, albeit with varying degrees of correctness. In such instances, human subjectivity comes into play. For instance, let us consider the following question:</p>
<p>Question 1: What advantages does a data center-enabled HAP (High Altitude Platform) offer from an energy perspective?</p>
<ul>
<li>Option 1: Saves cooling energy and harvests solar energy</li>
<li>Option 2: Uses renewable energy and offsets carbon emissions</li>
<li>Option 3: Deploys large-scale solar panels and batteries</li>
<li>Option 4: Requires less energy for communication links</li>
</ul>
<p>Answer: Option 1: Saves cooling energy and harvests solar energy
Explanation: The data center-enabled HAP saves cooling energy due to its location at the naturally low temperature stratosphere and harvests solar energy using large solar panels.
Category: Research publications
An expert might discern that Option 1 is more accurate than Option 2. In fact, although Option 2 is indeed an advantage offered by data center-enabled HAP, Option 1 provides more insights into the fact that high-altitude platforms are deployed in the low temperature stratosphere, while also considering the fact that solar energy is used (i.e., a renewable energy). We eliminated some of these questions to ensure the utmost objectivity and correctness, but deliberately retained others to challenge the LLM and assess its capability to discern between options exhibiting differing degrees of correctness.</p>
<p>Another important consideration is that a substantial portion of the generated questions draw from research papers to encompass a wide range of subjects. While this enriches the dataset, it introduces an element of subjectivity, as authors in</p>
<p>their papers may present their own perspectives and interpretations of phenomena, which can be inherently subjective. While our aim has been to filter out such questions, it is important to acknowledge that, in the end, the process involves an element of human subjectivity. All of these factors collectively indicate that, despite our efforts to optimize the process extensively, we must acknowledge that the dataset generation is not flawless, as it continues to rely on human input.</p>
<h2>C. Future Directions</h2>
<p>In light of these limitations, one thing becomes abundantly clear: the development and advancement of extensive and comprehensive telecom knowledge datasets for evaluating LLMs necessitate a collaborative effort spanning the entire industry. We are receptive to all forms of feedback, especially in cases where certain questions may elicit nuanced perspectives from active community members regarding their correctness. In the end, we view TeleQnA as an initial step toward such a collaborative endeavor. Researchers also have the potential to create their own datasets, and by combining these diverse datasets with TeleQnA, one can see a future where an expansive and all-encompassing dataset, akin to those successfully crafted by the machine learning community for their applications of interest, can be created. This, in turn, paves the way for the widespread adoption of high-performing LLMs tailored specifically for telecom applications.</p>
<h2>VII. ACKNOWLEDGMENTS</h2>
<p>We express our gratitude to the active telecom professionals for their generous contribution of time and participation in our surveys designed to evaluate telecom professionals' knowledge using TeleQnA.</p>
<h2>REFERENCES</h2>
<p>[1] K. Singhal et al., "Large language models encode clinical knowledge," Nature, vol. 620, no. 7972, pp. 172-180, Aug 2023. [Online]. Available: https://doi.org/10.1038/s41586-023-06291-2
[2] S. Wu et al., "BloombergGPT: A Large Language Model for Finance," arXiv preprint arXiv:2303.17564, May 2023.
[3] A. Maatouk et al., "Large language models for telecom: Forthcoming impact on the industry," arXiv preprint arXiv:2308.06013, August 2023.
[4] Altman Solon, "The opportunity in generative AI for Telecom," White Paper, September 2023. [Online]. Available: https://pages.awscloud.com/ GLOBAL-other-DL-generative-ai-for-telecom-whitepaper-2023-learn. html
[5] L. Bariah et al., "Understanding Telecom Language Through Large Language Models," arXiv preprint arXiv:2306.07933, 2023.
[6] R. Zellers et al., "HellaSwag: Can a machine really finish your sentence?" in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Jul. 2019, pp. 4791-4800.
[7] M. Sap et al., "Social IQa: Commonsense reasoning about social interactions," in EMNLP-IJCNLP 2019, Nov. 2019, pp. 4463-4473.
[8] R. Shah et al., "When FLUE meets FLANG: Benchmarks and large pretrained language model for financial domain," in EMNLP 2022, Dec. 2022, pp. 2322-2335.
[9] M. Kotaru, "Adapting foundation models for information synthesis of wireless communication specifications," arXiv preprint arXiv:2308.04033, August 2023.
[10] OpenAI, "GPT-4 Technical Report," arXiv preprint arXiv:2303.08774, March 2023.
[11] M. Richardson, "Mctest: A challenge dataset for the open-domain machine comprehension of text," in EMNLP 2013, October 2013.
[12] R. Zellers et al., "Swag: A large-scale adversarial dataset for grounded commonsense inference," in EMNLP 2018, October 2018.</p>
<p>Ali Maatouk (Member, IEEE) is a Researcher with Huawei Technologies, France. His research interests include the mathematical modeling and optimization of communication systems, machine learning, and the notion of information freshness.</p>
<p>Fadhel Ayed is a Senior Researcher with Huawei Technologies, France. His research interests include resource-efficient machine learning, stochastic processes, and mathematical foundations of deep learning.</p>
<p>Nicola Piovesan (Member, IEEE) is a Senior Researcher with Huawei Technologies, France. His research interests include energy sustainability, energy efficiency optimization, and machine learning in wireless communication systems.</p>
<p>Antonio De Domenico (Member, IEEE) is a Senior Researcher with Huawei Technologies, France. His research interests include heterogeneous wireless networks, machine learning, and green communications.</p>
<p>Mérouane Debbah (Fellow, IEEE) is a Professor at Khalifa University of Science and Technology in Abu Dhabi. His research interests include Large Language Models, distributed AI systems for networks and semantic communications.</p>
<p>Zhi-Quan Luo (Fellow, IEEE) is is Vice President (Academic) of the Chinese University of Hong Kong (Shenzhen). His research interests lie in optimization algorithms for signal processing, wireless communication and data analytics.</p>
<h2>APPENDIX A</h2>
<h2>SAMPLES OF TELEQNA</h2>
<p>Question 1: What does EIRP stand for?</p>
<ul>
<li>Option 1: Effective Isotropic Radio Power</li>
<li>Option 2: Equivalent Isotropic Radiated Power</li>
<li>Option 3: Efficient Isotropic Radiation Propagation</li>
<li>Option 4: Effective Infrared Radiation Power</li>
<li>Option 5: Equivalent Infrared Radiated Power</li>
</ul>
<p>Answer: Option 2: Equivalent Isotropic Radiated Power
Explanation: EIRP stands for Equivalent Isotropic Radiated Power.
Category: Lexicon
Question 2: What is a Heterogeneous Network?</p>
<ul>
<li>Option 1: A network consisting of multiple cells with different characteristics</li>
<li>Option 2: A network that provides services to the user in a managed way</li>
<li>Option 3: A network that changes the radio access mode or system used for bearer services</li>
<li>Option 4: A network that broadcasts a specific indicator and identity</li>
<li>Option 5: A network where the subscriber's personal service environment is controlled
Answer: Option 1: A network consisting of multiple cells with different characteristics
Explanation: A heterogeneous network is a 3GPP access network with multiple cells having different characteristics.
Category: Lexicon
Question 3: Which communication technique uses Light Diodes as transmitters?</li>
<li>Option 1: Large-Scale Antenna Arrays</li>
<li>Option 2: Free Space Optical Communications</li>
<li>Option 3: Heterogeneous Networks</li>
<li>Option 4: Cooperative Relay Communications</li>
<li>Option 5: Cognitive Radio Communications</li>
</ul>
<p>Answer: Option 2: Free Space Optical Communications
Explanation: Free Space Optical Communications use Light Diodes as transmitters.
Category: Research overview
Question 4: What is the main advantage of using SDR (software-defined radio) techniques?</p>
<ul>
<li>Option 1: They provide higher computational power for signal processing.</li>
<li>Option 2: They are more cost-effective compared to traditional communication systems.</li>
<li>Option 3: They offer flexibility to update receiver and transmitter functionalities by modifying software code.</li>
<li>Option 4: They result in lower latency in signal processing.</li>
<li>Option 5: None of the above.</li>
</ul>
<p>Answer: Option 3: They offer flexibility to update receiver and transmitter functionalities by modifying software code.
Explanation: SDR platforms are designed to be highly flexible, where all the receiver and transmitter functionalities
can be updated by a simple modification of the software code.
Category: Research overview
Question 5: What is the maximum number of eigenmodes that the MIMO channel can support? (nt is the number of transmit antennas, nr is the number of receive antennas)</p>
<ul>
<li>Option 1: nt</li>
<li>Option 2: nr</li>
<li>Option 3: min(nt, nr)</li>
<li>Option 4: max(nt, nr)</li>
</ul>
<p>Answer: Option 3: min(nt, nr)
Explanation: The maximum number of eigenmodes that the MIMO channel can support is min(nt, nr).
Category: Research publications
Question 6: What are the parameters governing the wireless channel propagation?</p>
<ul>
<li>Option 1: Positions of the transmitter and receiver antenna elements</li>
<li>Option 2: Carrier frequency</li>
<li>Option 3: Nature and positions of scattering objects in the environment</li>
<li>Option 4: All of the above</li>
</ul>
<p>Answer: Option 4: All of the above
Explanation: The parameters governing the wireless channel propagation include the positions of the transmitter (Tx) and receiver (Rx) antenna elements, the carrier frequency, as well as the nature and positions of scattering objects in the environment.
Category: Research publications
Question 7: What frequency band does Bluetooth use? [Bluetooth - Overview]</p>
<ul>
<li>Option 1: 2.4 GHz</li>
<li>Option 2: 5 GHz</li>
<li>Option 3: 40 MHz</li>
<li>Option 4: 1 MHz</li>
</ul>
<p>Answer: Option 1: 2.4 GHz
Explanation: Bluetooth uses the 2.4 GHz licence-free ISM band for transmission.
Category: Standards overview
Question 8: Which pairs of wires are used in 10/100Base-T? [IEEE 802.3]</p>
<ul>
<li>Option 1: Pair 1 and pair 2</li>
<li>Option 2: Pair 2 and pair 3</li>
<li>Option 3: Pair 3 and pair 4</li>
<li>Option 4: Pair 4 and pair 1</li>
</ul>
<p>Answer: Option 2: Pair 2 and pair 3
Explanation: In 10/100Base-T, pair 2 (orange/white and orange) and pair 3 (green/white and green) are used.
Category: Standards overview
Question 9: What is the RRC buffer size for a UE? [3GPP Release 17]</p>
<ul>
<li>Option 1: 45 MB</li>
<li>
<p>Option 2: 45 KB</p>
</li>
<li>
<p>Option 3: 45 GB</p>
</li>
<li>Option 4: 45 TB</li>
<li>Option 4: 4500 KB</li>
</ul>
<p>Answer: Option 2: 45 KB
Explanation: The RRC buffer size for a UE is 45 Kbytes.
Category: Standards specifications
Question 10: What are the required inputs for the Nadrf_MLModelManagement_Delete service operation? [3GPP Release 18]</p>
<ul>
<li>Option 1: Storage Transaction Identifier</li>
<li>Option 2: Unique ML Model identifier(s)</li>
<li>Option 3: Model file address(es)</li>
<li>Option 4: Storage Transaction Identifier or Unique ML Model identifier(s)</li>
<li>Option 4: Storage Transaction Identifier and Unique ML Model identifier(s)
Answer: option 4: Storage Transaction Identifier or Unique ML Model identifier(s)
Explanation: The required inputs for the Nadrf_MLModelManagement_Delete service operation are either the Storage Transaction Identifier or the Unique ML Model identifier(s).
Category: Standards specifications</li>
</ul>
<h1>APPENDIX B <br> Detailed Benchmarking Results</h1>
<p>In this appendix, we report the results obtained from our experiments involving GPT-3.5 and GPT-4 queries using TeleQnA. These results were obtained by randomly sampling batches of five questions drawn from TeleQnA, which were then provided to the large language model, and subsequently compared to the actual, correct answers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;">Humans</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lexicon (500 questions)</td>
<td style="text-align: center;">82.20</td>
<td style="text-align: center;">86.80</td>
<td style="text-align: center;">80.33</td>
</tr>
<tr>
<td style="text-align: left;">Research overview (2000 questions)</td>
<td style="text-align: center;">68.50</td>
<td style="text-align: center;">76.25</td>
<td style="text-align: center;">63.66</td>
</tr>
<tr>
<td style="text-align: left;">Research publications (4500 questions)</td>
<td style="text-align: center;">70.42</td>
<td style="text-align: center;">77.62</td>
<td style="text-align: center;">68.33</td>
</tr>
<tr>
<td style="text-align: left;">Standards overview (1000 questions)</td>
<td style="text-align: center;">64.00</td>
<td style="text-align: center;">74.40</td>
<td style="text-align: center;">61.66</td>
</tr>
<tr>
<td style="text-align: left;">Standards specifications (2000 questions)</td>
<td style="text-align: center;">56.97</td>
<td style="text-align: center;">64.78</td>
<td style="text-align: center;">56.33</td>
</tr>
<tr>
<td style="text-align: left;">Overall accuracy (10000 questions)</td>
<td style="text-align: center;">67.29</td>
<td style="text-align: center;">74.91</td>
<td style="text-align: center;">64.86</td>
</tr>
</tbody>
</table>
<p>Table II: Illustration of GPT-3.5, GPT-4, and active professionals accuracy (\%) across the various TeleQnA categories.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ The quality of the generation process was comparable between GPT-3.5 and GPT-4, which led us to favor the latter due to its lower cost.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
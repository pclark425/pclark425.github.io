<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8632 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8632</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8632</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-279299104</p>
                <p><strong>Paper Title:</strong> Large Language Models in Healthcare and Medical Applications: A Review</p>
                <p><strong>Paper Abstract:</strong> This paper provides a systematic and in-depth examination of large language models (LLMs) in the healthcare domain, addressing their significant potential to transform medical practice through advanced natural language processing capabilities. Current implementations demonstrate LLMs’ promising applications across clinical decision support, medical education, diagnostics, and patient care, while highlighting critical challenges in privacy, ethical deployment, and factual accuracy that require resolution for responsible integration into healthcare systems. This paper provides a comprehensive understanding of the background of healthcare LLMs, the evolution and architectural foundation, and the multimodal capabilities. Key methodological aspects—such as domain-specific data acquisition, large-scale pre-training, supervised fine-tuning, prompt engineering, and in-context learning—are explored in the context of healthcare use cases. The paper highlights the trends and categorizes prominent application areas in medicine. Additionally, it critically examines the prevailing technical and social challenges of healthcare LLMs, including issues of model bias, interpretability, ethics, governance, fairness, equity, data privacy, and regulatory compliance. The survey concludes with an outlook on emerging research directions and strategic recommendations for the development and deployment of healthcare LLMs.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8632.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8632.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemical language models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemical language models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based language models adapted to chemical and biochemical sequence/representation data that can perform de novo molecular generation, property prediction, and optimization for drug discovery and related molecular design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>chemical language models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer / biochemical language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery / pharmaceutical lead design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>de novo molecular generation and optimization via generative language-model techniques, often warm-started from pre-trained biochemical models and/or fine-tuned for target properties</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Described qualitatively as capable of proposing novel molecular configurations and de novo compounds; no quantitative novelty metrics are reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Achieved by warm-starting from pre-trained biochemical models and fine-tuning or conditioning for desired properties (e.g., pharmacokinetics, toxicity), though the review gives only conceptual descriptions without implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported/expected metrics include predicted efficacy/safety profiles, pharmacokinetic properties, drug-likeness and synthesizability; the review does not provide numeric thresholds or benchmark values.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The review states chemical language models have shown notable achievements in de novo drug design and that warm-started models initialized with pre-trained biochemical LLMs outperform baselines in generating high-quality candidate compounds, but provides no detailed quantitative results within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Qualitatively reported to outperform baseline approaches when warm-started from biochemical pre-training; explicit numerical comparisons to traditional computational chemistry methods are not provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Limitations noted include lack of reproducible reporting across studies, concerns about synthesizability and real-world validation, potential bias from training data, and need for experimental validation; the review highlights a general paucity of quantitative, reproducible evaluation in published work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Healthcare and Medical Applications: A Review', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8632.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8632.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-style generative model applied to molecular representations (mentioned as an example of models used for molecule-to-molecule translation and de novo molecule generation in the literature surveyed).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-style transformer (generative molecular language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery / mapping molecules to indications / de novo molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>language-model generation on molecular representations (e.g., SMILES or equivalent) and molecule-to-indication translation workflows as discussed in cited literature</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Paper reports MolGPT as used in literature for generating or translating molecules and indications; no concrete novelty statistics or similarity metrics provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Described uses include conditioning or translating molecules to indications; the review does not provide specific details of conditioning strategies or target-specific binding optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in the review for MolGPT; referenced literature typically evaluates drug-likeness, diversity, and mapping accuracy, but no concrete numbers are given here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Included in survey tables as one of the generative models used in drug discovery and molecule–indication translation studies; the review itself does not report experimental outcomes for MolGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned alongside other generative and predictive models; no direct performance comparisons are given in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Same general limitations for chemical LLM applications are noted (reproducibility, validation, synthesizability, potential bias), but no MolGPT-specific failure cases are described.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Healthcare and Medical Applications: A Review', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8632.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8632.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemLLM / ChemLLM (ChemLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemLLM (chemical large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A named example of chemical-domain large language models cited in the survey literature for applications in de novo design and molecule–indication translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer / domain-specific chemical LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery, de novo molecular generation, molecule–indication translation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>generative transformer-based molecular generation and translation (literature-cited use); details not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>The review reports that models such as ChemLLM are used to generate new compounds; no numerical measures of novelty are given in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Reported to be applied for target-oriented tasks in cited works (e.g., mapping molecules to indications), but the review does not provide implementation specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this review; typical metrics in cited literature would include drug-likeness and mapping accuracy, but no values are given here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChemLLM is listed among models reported in the literature for accelerating drug discovery and indication mapping; this review does not present primary experimental results for ChemLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited in the context of generative AI approaches that can accelerate lead discovery relative to traditional approaches; no direct comparative metrics are provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>General concerns noted include lack of reproducible reporting and need for wet-lab validation; no ChemLLM-specific limitations are detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Healthcare and Medical Applications: A Review', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8632.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8632.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProtGPT2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProtGPT2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A protein-focused GPT-style language model referenced in the survey literature as an example of generative models applied to biomolecular sequence design (protein generation/inference) relevant to molecular biology and drug discovery pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ProtGPT2</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-style transformer (protein language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>protein sequence generation / molecular biology; supporting drug development workflows (e.g., protein design rather than small-molecule design)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>sequence-generation via autoregressive language modeling on protein sequence data (cited in literature summarized by the review)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>ProtGPT2 is protein-focused; the review notes its role in molecular biology and R&D rather than providing novelty metrics for small molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Used for protein design and sequence generation; review indicates relevance to drug development pipelines but gives no specifics on target-binding optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in this review for ProtGPT2; typical literature evaluates sequence plausibility, structural compatibility, and function prediction, but no numbers are given here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned in table entries and citations as part of the landscape of generative models used in pharmaceutical R&D and molecular biology; no primary results presented in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Listed among generative and structure models used in R&D (e.g., compared conceptually to AlphaFold/ESMFold workflows), but no quantitative comparisons are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>General limitations applicable to biomolecular generative models (need for experimental validation, functional testing, and reproducibility) are noted; no model-specific failures are included.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Healthcare and Medical Applications: A Review', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8632.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8632.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold / ESMFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold and ESMFold (structure-prediction models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deep-learning models for protein structure prediction (AlphaFold) and fast transformer-based structure prediction (ESMFold) mentioned in the review as part of AI toolchains enabling molecular and drug development, often used alongside generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AlphaFold / ESMFold</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>deep learning / transformer-based structure prediction models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>structural biology to support drug discovery and molecular design (protein structure prediction that informs design decisions)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>not generative for small molecules but used to predict protein structures to inform molecule/protein design workflows; often integrated with generative models in pipelines referenced by the review</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>These models themselves do not generate small molecules; their use increases confidence in protein-target-informed design but the review gives no metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Used to provide structural context for target binding and downstream molecule design; review mentions their use in R&D pipelines conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this review (other literature reports metrics like TM-score / RMSD for structure prediction), but no such numbers are reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as foundational tools in modern AI-driven pharmaceutical R&D; the review notes their inclusion in cited works that combine structure prediction and generative chemistry but does not provide experimental results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented as part of a suite of AI tools transforming pharma (alongside GPT-style generative models); the review does not supply direct performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>The review notes general needs for integration, experimental follow-up, and validation when using such models in drug discovery; model-specific limitations are not elaborated here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Healthcare and Medical Applications: A Review', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8632.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8632.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (drug discovery mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pre-trained Transformer 4) as applied in drug discovery contexts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose large language model referenced in the review literature as being used or cited in the pharmaceutical R&D and drug discovery context, often alongside domain-specific models and structure predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT / transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery, pharmaceutical R&D, hypothesis generation and translational mapping between molecules and indications</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompt-based generation and language-model-assisted hypothesis generation; cited literature uses GPT-4 alongside other tools for ideation and candidate proposal.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>GPT-4 is listed among models contributing to idea generation and R&D acceleration; the review does not quantify novelty of chemical outputs produced by GPT-4 in the cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>When used in drug-discovery contexts, GPT-4 is typically combined with domain adaptation or used as part of multi-model pipelines, but the review provides only high-level descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in the review for GPT-4's chemical-design outputs; cited works discuss improvements in productivity and ideation rather than concrete chemical evaluation metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 is mentioned in multiple cited entries as part of generative-AI toolchains that accelerate drug discovery and R&D; this review does not present original experimental outcomes for GPT-4 in chemical design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Discussed qualitatively as part of generative AI approaches that complement structure predictors and domain models; no detailed performance comparisons are reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Same general limitations apply: lack of reproducible reporting, need for wet-lab validation of model-proposed compounds, concerns about hallucination and bias, and resource/validation challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models in Healthcare and Medical Applications: A Review', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emerging opportunities of using large language models for translation between drug molecules and indications <em>(Rating: 2)</em></li>
                <li>Generative artificial intelligence in drug discovery: Basic framework, recent advances, challenges, and opportunities <em>(Rating: 2)</em></li>
                <li>Large language models facilitating modern molecular biology and novel drug development <em>(Rating: 2)</em></li>
                <li>New Horizons: Pioneering Pharmaceutical R&D with Generative AI from lab to the clinic-an industry perspective <em>(Rating: 2)</em></li>
                <li>A review of transformers in drug discovery and beyond <em>(Rating: 2)</em></li>
                <li>Accelerating drug discovery, development, and clinical trials by artificial intelligence <em>(Rating: 1)</em></li>
                <li>Driving productivity and scientific breakthroughs in pharmaceutical R&D <em>(Rating: 1)</em></li>
                <li>Using LLMs for translation between drug molecules and indications <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8632",
    "paper_id": "paper-279299104",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "Chemical language models",
            "name_full": "Chemical language models (general)",
            "brief_description": "Transformer-based language models adapted to chemical and biochemical sequence/representation data that can perform de novo molecular generation, property prediction, and optimization for drug discovery and related molecular design tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "chemical language models (general)",
            "model_type": "transformer / biochemical language model",
            "model_size": null,
            "training_data": null,
            "application_domain": "drug discovery / pharmaceutical lead design",
            "generation_method": "de novo molecular generation and optimization via generative language-model techniques, often warm-started from pre-trained biochemical models and/or fine-tuned for target properties",
            "novelty_of_chemicals": "Described qualitatively as capable of proposing novel molecular configurations and de novo compounds; no quantitative novelty metrics are reported in this review.",
            "application_specificity": "Achieved by warm-starting from pre-trained biochemical models and fine-tuning or conditioning for desired properties (e.g., pharmacokinetics, toxicity), though the review gives only conceptual descriptions without implementation details.",
            "evaluation_metrics": "Reported/expected metrics include predicted efficacy/safety profiles, pharmacokinetic properties, drug-likeness and synthesizability; the review does not provide numeric thresholds or benchmark values.",
            "results_summary": "The review states chemical language models have shown notable achievements in de novo drug design and that warm-started models initialized with pre-trained biochemical LLMs outperform baselines in generating high-quality candidate compounds, but provides no detailed quantitative results within this paper.",
            "comparison_to_other_methods": "Qualitatively reported to outperform baseline approaches when warm-started from biochemical pre-training; explicit numerical comparisons to traditional computational chemistry methods are not provided in the review.",
            "limitations_and_challenges": "Limitations noted include lack of reproducible reporting across studies, concerns about synthesizability and real-world validation, potential bias from training data, and need for experimental validation; the review highlights a general paucity of quantitative, reproducible evaluation in published work.",
            "uuid": "e8632.0",
            "source_info": {
                "paper_title": "Large Language Models in Healthcare and Medical Applications: A Review",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "MolGPT",
            "name_full": "MolGPT",
            "brief_description": "A GPT-style generative model applied to molecular representations (mentioned as an example of models used for molecule-to-molecule translation and de novo molecule generation in the literature surveyed).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MolGPT",
            "model_type": "GPT-style transformer (generative molecular language model)",
            "model_size": null,
            "training_data": null,
            "application_domain": "drug discovery / mapping molecules to indications / de novo molecule generation",
            "generation_method": "language-model generation on molecular representations (e.g., SMILES or equivalent) and molecule-to-indication translation workflows as discussed in cited literature",
            "novelty_of_chemicals": "Paper reports MolGPT as used in literature for generating or translating molecules and indications; no concrete novelty statistics or similarity metrics provided in the review.",
            "application_specificity": "Described uses include conditioning or translating molecules to indications; the review does not provide specific details of conditioning strategies or target-specific binding optimization.",
            "evaluation_metrics": "Not specified in the review for MolGPT; referenced literature typically evaluates drug-likeness, diversity, and mapping accuracy, but no concrete numbers are given here.",
            "results_summary": "Included in survey tables as one of the generative models used in drug discovery and molecule–indication translation studies; the review itself does not report experimental outcomes for MolGPT.",
            "comparison_to_other_methods": "Mentioned alongside other generative and predictive models; no direct performance comparisons are given in this review.",
            "limitations_and_challenges": "Same general limitations for chemical LLM applications are noted (reproducibility, validation, synthesizability, potential bias), but no MolGPT-specific failure cases are described.",
            "uuid": "e8632.1",
            "source_info": {
                "paper_title": "Large Language Models in Healthcare and Medical Applications: A Review",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ChemLLM / ChemLLM (ChemLLM)",
            "name_full": "ChemLLM (chemical large language model)",
            "brief_description": "A named example of chemical-domain large language models cited in the survey literature for applications in de novo design and molecule–indication translation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChemLLM",
            "model_type": "transformer / domain-specific chemical LLM",
            "model_size": null,
            "training_data": null,
            "application_domain": "drug discovery, de novo molecular generation, molecule–indication translation",
            "generation_method": "generative transformer-based molecular generation and translation (literature-cited use); details not provided in this review.",
            "novelty_of_chemicals": "The review reports that models such as ChemLLM are used to generate new compounds; no numerical measures of novelty are given in the paper.",
            "application_specificity": "Reported to be applied for target-oriented tasks in cited works (e.g., mapping molecules to indications), but the review does not provide implementation specifics.",
            "evaluation_metrics": "Not specified in this review; typical metrics in cited literature would include drug-likeness and mapping accuracy, but no values are given here.",
            "results_summary": "ChemLLM is listed among models reported in the literature for accelerating drug discovery and indication mapping; this review does not present primary experimental results for ChemLLM.",
            "comparison_to_other_methods": "Cited in the context of generative AI approaches that can accelerate lead discovery relative to traditional approaches; no direct comparative metrics are provided in the review.",
            "limitations_and_challenges": "General concerns noted include lack of reproducible reporting and need for wet-lab validation; no ChemLLM-specific limitations are detailed.",
            "uuid": "e8632.2",
            "source_info": {
                "paper_title": "Large Language Models in Healthcare and Medical Applications: A Review",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ProtGPT2",
            "name_full": "ProtGPT2",
            "brief_description": "A protein-focused GPT-style language model referenced in the survey literature as an example of generative models applied to biomolecular sequence design (protein generation/inference) relevant to molecular biology and drug discovery pipelines.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ProtGPT2",
            "model_type": "GPT-style transformer (protein language model)",
            "model_size": null,
            "training_data": null,
            "application_domain": "protein sequence generation / molecular biology; supporting drug development workflows (e.g., protein design rather than small-molecule design)",
            "generation_method": "sequence-generation via autoregressive language modeling on protein sequence data (cited in literature summarized by the review)",
            "novelty_of_chemicals": "ProtGPT2 is protein-focused; the review notes its role in molecular biology and R&D rather than providing novelty metrics for small molecules.",
            "application_specificity": "Used for protein design and sequence generation; review indicates relevance to drug development pipelines but gives no specifics on target-binding optimization.",
            "evaluation_metrics": "Not provided in this review for ProtGPT2; typical literature evaluates sequence plausibility, structural compatibility, and function prediction, but no numbers are given here.",
            "results_summary": "Mentioned in table entries and citations as part of the landscape of generative models used in pharmaceutical R&D and molecular biology; no primary results presented in this review.",
            "comparison_to_other_methods": "Listed among generative and structure models used in R&D (e.g., compared conceptually to AlphaFold/ESMFold workflows), but no quantitative comparisons are provided.",
            "limitations_and_challenges": "General limitations applicable to biomolecular generative models (need for experimental validation, functional testing, and reproducibility) are noted; no model-specific failures are included.",
            "uuid": "e8632.3",
            "source_info": {
                "paper_title": "Large Language Models in Healthcare and Medical Applications: A Review",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "AlphaFold / ESMFold",
            "name_full": "AlphaFold and ESMFold (structure-prediction models)",
            "brief_description": "Deep-learning models for protein structure prediction (AlphaFold) and fast transformer-based structure prediction (ESMFold) mentioned in the review as part of AI toolchains enabling molecular and drug development, often used alongside generative models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "AlphaFold / ESMFold",
            "model_type": "deep learning / transformer-based structure prediction models",
            "model_size": null,
            "training_data": null,
            "application_domain": "structural biology to support drug discovery and molecular design (protein structure prediction that informs design decisions)",
            "generation_method": "not generative for small molecules but used to predict protein structures to inform molecule/protein design workflows; often integrated with generative models in pipelines referenced by the review",
            "novelty_of_chemicals": "These models themselves do not generate small molecules; their use increases confidence in protein-target-informed design but the review gives no metrics.",
            "application_specificity": "Used to provide structural context for target binding and downstream molecule design; review mentions their use in R&D pipelines conceptually.",
            "evaluation_metrics": "Not detailed in this review (other literature reports metrics like TM-score / RMSD for structure prediction), but no such numbers are reproduced here.",
            "results_summary": "Cited as foundational tools in modern AI-driven pharmaceutical R&D; the review notes their inclusion in cited works that combine structure prediction and generative chemistry but does not provide experimental results.",
            "comparison_to_other_methods": "Presented as part of a suite of AI tools transforming pharma (alongside GPT-style generative models); the review does not supply direct performance comparisons.",
            "limitations_and_challenges": "The review notes general needs for integration, experimental follow-up, and validation when using such models in drug discovery; model-specific limitations are not elaborated here.",
            "uuid": "e8632.4",
            "source_info": {
                "paper_title": "Large Language Models in Healthcare and Medical Applications: A Review",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4 (drug discovery mentions)",
            "name_full": "GPT-4 (Generative Pre-trained Transformer 4) as applied in drug discovery contexts",
            "brief_description": "A general-purpose large language model referenced in the review literature as being used or cited in the pharmaceutical R&D and drug discovery context, often alongside domain-specific models and structure predictors.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_type": "GPT / transformer",
            "model_size": null,
            "training_data": null,
            "application_domain": "drug discovery, pharmaceutical R&D, hypothesis generation and translational mapping between molecules and indications",
            "generation_method": "prompt-based generation and language-model-assisted hypothesis generation; cited literature uses GPT-4 alongside other tools for ideation and candidate proposal.",
            "novelty_of_chemicals": "GPT-4 is listed among models contributing to idea generation and R&D acceleration; the review does not quantify novelty of chemical outputs produced by GPT-4 in the cited works.",
            "application_specificity": "When used in drug-discovery contexts, GPT-4 is typically combined with domain adaptation or used as part of multi-model pipelines, but the review provides only high-level descriptions.",
            "evaluation_metrics": "Not provided in the review for GPT-4's chemical-design outputs; cited works discuss improvements in productivity and ideation rather than concrete chemical evaluation metrics in this paper.",
            "results_summary": "GPT-4 is mentioned in multiple cited entries as part of generative-AI toolchains that accelerate drug discovery and R&D; this review does not present original experimental outcomes for GPT-4 in chemical design.",
            "comparison_to_other_methods": "Discussed qualitatively as part of generative AI approaches that complement structure predictors and domain models; no detailed performance comparisons are reported in the review.",
            "limitations_and_challenges": "Same general limitations apply: lack of reproducible reporting, need for wet-lab validation of model-proposed compounds, concerns about hallucination and bias, and resource/validation challenges.",
            "uuid": "e8632.5",
            "source_info": {
                "paper_title": "Large Language Models in Healthcare and Medical Applications: A Review",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emerging opportunities of using large language models for translation between drug molecules and indications",
            "rating": 2,
            "sanitized_title": "emerging_opportunities_of_using_large_language_models_for_translation_between_drug_molecules_and_indications"
        },
        {
            "paper_title": "Generative artificial intelligence in drug discovery: Basic framework, recent advances, challenges, and opportunities",
            "rating": 2,
            "sanitized_title": "generative_artificial_intelligence_in_drug_discovery_basic_framework_recent_advances_challenges_and_opportunities"
        },
        {
            "paper_title": "Large language models facilitating modern molecular biology and novel drug development",
            "rating": 2,
            "sanitized_title": "large_language_models_facilitating_modern_molecular_biology_and_novel_drug_development"
        },
        {
            "paper_title": "New Horizons: Pioneering Pharmaceutical R&D with Generative AI from lab to the clinic-an industry perspective",
            "rating": 2,
            "sanitized_title": "new_horizons_pioneering_pharmaceutical_rd_with_generative_ai_from_lab_to_the_clinican_industry_perspective"
        },
        {
            "paper_title": "A review of transformers in drug discovery and beyond",
            "rating": 2,
            "sanitized_title": "a_review_of_transformers_in_drug_discovery_and_beyond"
        },
        {
            "paper_title": "Accelerating drug discovery, development, and clinical trials by artificial intelligence",
            "rating": 1,
            "sanitized_title": "accelerating_drug_discovery_development_and_clinical_trials_by_artificial_intelligence"
        },
        {
            "paper_title": "Driving productivity and scientific breakthroughs in pharmaceutical R&D",
            "rating": 1,
            "sanitized_title": "driving_productivity_and_scientific_breakthroughs_in_pharmaceutical_rd"
        },
        {
            "paper_title": "Using LLMs for translation between drug molecules and indications",
            "rating": 1,
            "sanitized_title": "using_llms_for_translation_between_drug_molecules_and_indications"
        }
    ],
    "cost": 0.013334,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models in Healthcare and Medical Applications: A Review
10 June 2025</p>
<p>Essam A Rashed 
Walayat Hussain 
Ghada Khoriba 
Subhankar Maity 0009-0001-1358-9534
Biomedical Sensors &amp; Systems Lab
University of Memphis
38152MemphisTNUSA</p>
<p>Electrical and Computer Engineering Department
University of Memphis
38152MemphisTNUSA</p>
<p>Manob Jyoti Saikia msaikia@memphis.edu 0000-0001-6656-4333
Biomedical Sensors &amp; Systems Lab
University of Memphis
38152MemphisTNUSA</p>
<p>Electrical and Computer Engineering Department
University of Memphis
38152MemphisTNUSA</p>
<p>Large Language Models in Healthcare and Medical Applications: A Review
10 June 20254223A11166BFD06139D337AE7D65B37510.3390/bioengineering12060631Received: 7 May 2025 Revised: 2 June 2025 Accepted: 8 June 2025healthcarelarge language modelsLLMsmedicalpatientreview
This paper provides a systematic and in-depth examination of large language models (LLMs) in the healthcare domain, addressing their significant potential to transform medical practice through advanced natural language processing capabilities.Current implementations demonstrate LLMs' promising applications across clinical decision support, medical education, diagnostics, and patient care, while highlighting critical challenges in privacy, ethical deployment, and factual accuracy that require resolution for responsible integration into healthcare systems.This paper provides a comprehensive understanding of the background of healthcare LLMs, the evolution and architectural foundation, and the multimodal capabilities.Key methodological aspects-such as domain-specific data acquisition, large-scale pre-training, supervised fine-tuning, prompt engineering, and in-context learning-are explored in the context of healthcare use cases.The paper highlights the trends and categorizes prominent application areas in medicine.Additionally, it critically examines the prevailing technical and social challenges of healthcare LLMs, including issues of model bias, interpretability, ethics, governance, fairness, equity, data privacy, and regulatory compliance.The survey concludes with an outlook on emerging research directions and strategic recommendations for the development and deployment of healthcare LLMs.</p>
<p>Introduction</p>
<p>Large language models (LLMs) represent a revolutionary advancement in artificial intelligence, demonstrating unprecedented capabilities in understanding and generating human-like text [1].These models, developed on deep learning and natural language processing technologies, have rapidly integrated into diverse sectors, including healthcare, where they have begun to transform various aspects of medical practice [2].LLMs excel in processing extensive textual data, deriving insights, and producing high-quality outputs, leading to innovations across clinical decision-making, patient care, medical education, and research.</p>
<p>The healthcare sector, traditionally characterized by large amounts of textual data in the form of medical records, research literature, clinical guidelines, and patient communications, presents a particularly fertile ground for LLM applications [3].By analyzing and interpreting these complex medical texts, LLMs offer the promise of enhancing diagnostic accuracy, streamlining clinical workflows, improving patient-provider communication, and accelerating medical discoveries [2,4].The ability of these models to process and synthesize information at unprecedented scales could potentially address critical challenges in healthcare, including information overload, documentation burden, and the need for personalized care [5].</p>
<p>However, the integration of LLMs into healthcare also raises significant concerns regarding data privacy, ethical considerations, factual accuracy, and the potential for bias [4,5].The high-stakes nature of medical decision-making demands careful validation, responsible deployment, and thoughtful governance of these powerful technologies.As such, understanding both the capabilities and limitations of LLMs in healthcare becomes paramount for researchers, clinicians, policymakers, and technology developers [6].</p>
<p>This comprehensive survey examines the current landscape of LLMs in healthcare, exploring their foundational technologies, methodologies, applications, evaluation frameworks, and challenges.Drawing from recent research across medical informatics, artificial intelligence, and clinical practice, this paper aims to provide a holistic understanding of how LLMs are reshaping healthcare and what future developments may hold.By synthesizing insights from diverse studies and perspectives, this survey seeks to contribute to the responsible and effective integration of LLMs into healthcare systems, ultimately advancing the goal of improved patient outcomes and healthcare delivery [2,5].</p>
<p>Methodology</p>
<p>Review Design and Rationale</p>
<p>To ensure methodological rigor and transparency, this review was conducted following the Arksey and O'Malley framework [7] for scoping reviews, as refined by Levac et al. [8].This approach was selected to comprehensively map the breadth of literature on LLMs in healthcare, accommodate heterogeneous study designs, and systematically identify research gaps and future directions.The scoping review methodology is particularly suited to emerging fields such as LLMs in medicine, where the evidence base is rapidly expanding and diverse in scope.</p>
<p>Literature Search Strategy</p>
<p>A systematic literature search was performed across five major databases: PubMed, MEDLINE, IEEE Xplore, ACM Digital Library, Google Scholar, and arXiv.The search covered the period from January 2015 to April 2025, capturing the evolution of transformerbased LLMs and their applications in healthcare.The following Boolean search string was employed: ("large language model" OR "LLM" OR "GPT" OR "transformer model") AND ("healthcare" OR "medicine" OR "clinical" OR "patient" OR "diagnosis" OR "treatment" OR "medical education").Searches were limited to English-language publications.Additional records were identified through manual review of reference lists and relevant preprints.</p>
<p>Study</p>
<p>Data Extraction and Synthesis</p>
<p>A standardized data extraction form was used to capture key information from each included study.The extracted information encompassed the following elements:</p>
<p>• Source: Refers to the original study or publication cited in the review, providing the foundational reference for the findings presented.• LLMs Used: Specifies the large language models or AI systems analyzed or implemented in the respective studies (e.g., GPT-4, Med-PaLM, BioGPT, LLaMA, etc.).• Highlight: Summarizes the core focus or contributions of the study, such as advancements in clinical decision support, diagnostic reasoning, or medical education.• Application Area: Indicates the specific domain or healthcare context in which the LLMs were applied, such as diagnostics, patient care, clinical decision support, medical education, or drug discovery.</p>
<p>Section 5 provides a detailed summary of key LLMs and their applications in healthcare.</p>
<p>Limitations of the Review</p>
<p>Several limitations should be acknowledged.First, the rapid pace of LLM development means that some recent advances may not be captured.Second, the exclusion of non-English literature may introduce language bias.Third, the diversity of study designs and metrics precluded formal meta-analysis.Finally, most of the included studies originated in high-resource settings, which can limit the generalizability of the findings to global healthcare contexts.</p>
<p>Background on Large Language Models</p>
<p>Evolution and Architectural Foundations</p>
<p>Large language models have evolved significantly from their predecessors, marking a paradigm shift in natural language processing approaches (Figure 1).Traditional models relied on rule-based systems or statistical methods with limited contextual understanding, whereas modern LLMs leverage deep neural networks, particularly transformer architectures, to process and generate text with remarkable sophistication [4,9].The transformer architecture, introduced in 2017, revolutionized language modeling through its attention mechanism, enabling models to weigh the importance of different words in context and capture long-range dependencies in text [4].The evolution of LLMs has been characterized by increasing scale, both in terms of parameter count and training data size.Contemporary models like GPT-4 and LLaMA incorporate billions of parameters, allowing them to capture intricate patterns in language and domain-specific knowledge, including medical terminology and concepts [4].This scaling has proven crucial for achieving emergent capabilities that smaller models simply cannot manifest, including complex reasoning, nuanced understanding of medical scenarios, and generation of contextually appropriate responses to healthcare queries [5,9].</p>
<p>Traditional</p>
<p>Foundational Models and Healthcare Adaptation</p>
<p>LLMs in healthcare typically fall into two categories: general-purpose models finetuned for medical applications and specialized models developed specifically for healthcare contexts (Figure 2).General-purpose models such as ChatGPT have demonstrated surprising proficiency in medical knowledge despite not being explicitly trained for healthcare applications [6,10].Meanwhile, specialized models like Med-PaLM, PMC-LLaMA, and GatorTronGPT are designed from the ground up with medical applications in mind, often pre-trained on vast corpora of biomedical literature, clinical notes, and healthcare datasets [4,11].</p>
<p>The adaptation of LLMs for healthcare typically involves domain-specific pre-training on medical corpora, followed by fine-tuning for particular tasks or specialties.This process allows the models to develop robust representations of medical knowledge while calibrating their outputs to align with clinical standards and practices [11].Furthermore, techniques such as reinforcement learning from human feedback (RLHF) have emerged as critical methods for aligning model outputs with human preferences and medical guidelines, enhancing the relevance and safety of generated content for healthcare applications.</p>
<p>FOUNDATIONAL MODELS AND HEALTHCARE ADAPTION</p>
<p>General-Purpose LLMs GPT-4, ChatGPT</p>
<p>Fine-Tuning</p>
<p>Specialty areas or clinical tasks</p>
<p>Reinforcement Learning from Human Feedback (RLHF)</p>
<p>Align outputs with clinical standards, human preferences, and safety Figure 2. Foundational models and healthcare adaptation of large language models (LLMs).</p>
<p>Multimodal Capabilities</p>
<p>Recent advancements have extended LLMs beyond text to incorporate multimodal capabilities, enabling the processing and generation of diverse data types such as images, audio, and structured clinical data [4,5].This development is particularly significant for healthcare, where diagnostic and treatment decisions often rely on the integration of multiple data modalities, including imaging studies, vital signs, laboratory results, and clinical narratives [5].</p>
<p>Multimodal LLMs in healthcare can process and interpret various inputs, from radiological images to patient-reported symptoms, providing more comprehensive and contextually informed outputs (Figure 3).For instance, models like Med-Flamingo and LLaVA-Med demonstrate the ability to analyze medical images in conjunction with textual information, potentially enhancing diagnostic accuracy and clinical decision support [11].This multimodal integration represents a significant advancement toward more holistic and patient-centered applications of artificial intelligence in healthcare settings [5].The development of effective LLMs for healthcare applications begins with the crucial step of data acquisition.Medical LLMs require vast and diverse datasets encompassing clinical records, medical literature, healthcare guidelines, and domain-specific knowledge resources [11].These datasets form the foundation upon which models develop their understanding of medical terminology, concepts, and relationships (Figure 4).The quality, diversity, and representativeness of these training data significantly influence the models' performance and applicability across different healthcare contexts and populations [9,11].</p>
<p>MULTIMODAL FOUNDATIONAL MODELS</p>
<p>Med</p>
<p>Methodology of LLMs in Healthcare</p>
<p>Pre-training involves exposing models to extensive corpora of medical texts to develop general language capabilities and domain-specific understanding.Datasets commonly used for pre-training medical LLMs include PubMed abstracts, PMC Open Access, MIMIC (Medical Information Mart for Intensive Care), and various electronic health record collections [11].Through pre-training, models learn to predict tokens in context, developing representations of medical language that capture semantic relationships, clinical reasoning patterns, and domain-specific knowledge structures [4].</p>
<p>The scale and diversity of pre-training data present both opportunities and challenges.Although larger and more diverse datasets can improve model performance and generalizability, they also raise concerns regarding data quality, privacy, and the potential incorporation of biases present in historical medical records and literature [4,6].Consequently, careful curation and preprocessing of training data have emerged as essential considerations in the development of responsible and effective healthcare LLMs [5,11].</p>
<p>METHODOLOGY OF LLMs IN HEALTHCARE</p>
<p>Fine-Tuning and Adaptation</p>
<p>Following pre-training, LLMs undergo fine-tuning to adapt their capabilities to specific healthcare tasks, specialties, or use cases.Fine-tuning typically involves additional training on smaller, task-specific datasets with direct supervision for the target application [4].This process allows models to specialize their outputs for particular clinical contexts while maintaining the broad knowledge base acquired during pre-training.</p>
<p>Task-specific fine-tuning has been applied to adapt LLMs for various healthcare applications, including medical question answering, clinical note summarization, diagnostic assistance, and treatment recommendation [4,11].For instance, models like Med-PaLM undergo fine-tuning on medical examination questions and clinical vignettes to enhance their performance in diagnostic reasoning and medical knowledge application [11].Similarly, specialized fine-tuning enables models to generate patient-friendly explanations, summarize clinical literature, or assist in documentation tasks with greater precision and relevance.</p>
<p>Innovative approaches to fine-tuning include instruction tuning, where models learn to follow specific directions relevant to healthcare contexts, and alignment techniques such as reinforcement learning from human feedback (RLHF), which optimize models based on expert evaluations of their outputs.These methods help bridge the gap between the general capabilities of LLMs and the specific requirements of healthcare applications, where precision, safety, and adherence to clinical standards are paramount [4,5].</p>
<p>Prompt Engineering and In-Context Learning</p>
<p>Prompt engineering has emerged as a key methodology for effectively using LLMs in healthcare without requiring extensive model retraining.This approach involves carefully crafting input prompts to guide the model toward generating appropriate, accurate, and clinically relevant outputs [4].In healthcare contexts, prompts can include patient information, clinical questions, or specific instructions that frame the interaction in ways that leverage the model's existing knowledge while constraining its responses to align with medical best practices [6].</p>
<p>Advanced prompt engineering techniques, such as chain-of-thought prompting, encourage models to articulate their reasoning processes step by step, mirroring clinical decision-making and making their conclusions more transparent and verifiable [12].Similarly, few-shot learning through exemplars embedded in prompts allows models to adapt to specific medical tasks by providing demonstrations of desired outputs, enhancing their performance without additional training [4].</p>
<p>In-context learning represents another powerful methodology for healthcare applications, enabling LLMs to adapt to new tasks or domains based on examples provided within the input context [4].This capability allows healthcare professionals to customize model behavior for specific clinical scenarios, patient populations, or medical specialties by including relevant examples or guidelines in their prompts.The flexibility provided by prompt engineering and in-context learning has facilitated the rapid adaptation of LLMs to various healthcare applications, from the generation of patient education materials to the assistance of complex diagnostic reasoning [12].</p>
<p>Applications of LLMs in Healthcare</p>
<p>Clinical Decision Support and Diagnostics</p>
<p>Large language models have demonstrated promising capabilities in augmenting clinical decision-making processes across various medical specialties [13,14].By analyzing patient symptoms, medical records, and relevant data, LLMs can assist healthcare providers in identifying potential diagnoses, suggesting appropriate tests, and recommending evidence-based treatments [4,[15][16][17][18][19].Studies have shown that LLMs can achieve consid-erable accuracy in diagnostic reasoning, particularly when provided with comprehensive clinical information and well-structured instructions [2,6,15,16].Specific implementations include systems that analyze symptoms for preliminary diagnosis, models that interpret laboratory results in the clinical context, and tools that identify potential drug interactions or adverse events [5,20].For instance, Stanford University researchers have employed LLMs to suggest potential treatments for cardiac conditions, while the National Institutes of Health (NIH)'s GatoTron LLM examines electronic health records to detect potential drug interactions [21].These applications demonstrate how LLMs can serve as cognitive assistants for clinicians, helping them synthesize complex information and consider diverse diagnostic possibilities [10].</p>
<p>Healthcare providers have expressed particular comfort with LLMs functioning in assistive roles, similar to physician extenders or trainees, where they can enhance clinical reasoning while maintaining human oversight of final decisions [10,14].This alignment with clinician preferences suggests a path for integration that preserves the critical role of human judgment while leveraging the information-processing capabilities of LLMs to enhance diagnostic precision and treatment optimization [2,10,[22][23][24].</p>
<p>Medical Education and Training</p>
<p>LLMs are revolutionizing medical education by delivering personalized learning experiences and creating immersive simulations of real-world clinical scenarios.These models can generate customized educational content, adapt to individual learning styles, and provide immediate feedback, enhancing the efficiency and effectiveness of medical training programs [2,[25][26][27][28][29].The ability of LLMs to simulate patient encounters with diverse presentations and complexities offers valuable opportunities for students to develop clinical reasoning skills in a safe and controlled environment [12,30].Beyond student education, LLMs are proving valuable for continuing professional development, helping practicing clinicians stay up to date with rapidly evolving medical knowledge and guidelines [31,32].By analyzing the latest research literature and clinical trials, these models can provide concise summaries of emerging evidence, facilitating the integration of new findings into clinical practice [5,28,33].Furthermore, LLMs have been employed to develop interdisciplinary programs that combine medicine, AI, data analytics, and leadership skills, preparing healthcare professionals for an increasingly digital healthcare landscape [31].</p>
<p>The implementation of LLMs in medical education addresses one of the major challenges in the field: the overwhelming cognitive load faced by students and practitioners [29,34].By managing the mechanical aspects of information processing, these technologies enable learners to focus on critical thinking, problem-solving, and the humanistic dimensions of medicine, potentially enhancing both technical competence and compassionate care [2].</p>
<p>Patient Care and Communication</p>
<p>LLMs are enhancing patient care through various applications designed to improve provider efficiency, patient engagement, and healthcare accessibility [23,[35][36][37].Virtual medical assistants powered by LLMs can facilitate patient triage, symptom assessment, and care navigation, as exemplified by systems like the NHS's Florence Chatbot and Babylon Health Chatbot.These tools help direct patients to appropriate levels of care while providing basic health information and addressing common concerns.</p>
<p>The conversational capabilities of LLMs present significant opportunities for bridging patient-provider communication gaps and addressing barriers related to health literacy, language differences, and complex medical terminology [5,38,39].By translating medical jargon into accessible language, LLMs can help patients better understand their conditions, treatment options, and care plans, potentially improving adherence and outcomes [4,40].Additionally, these models can assist in obtaining comprehensive patient histories through natural conversation, ensuring thorough documentation while reducing the burden on healthcare providers.</p>
<p>Healthcare providers have recognized the potential of LLMs to enhance patient care through more personalized and efficient service delivery [2,6].By analyzing individual patient data, medical literature, and clinical guidelines, LLMs can offer tailored insights for diagnosis, treatment planning, and ongoing monitoring, potentially improving patient outcomes, reducing errors, and boosting satisfaction.This personalized approach aligns with the broader movement toward precision medicine, where interventions are customized to individual patient characteristics and preferences [5,6].</p>
<p>Medical Literature Analysis and Research Support</p>
<p>The exponential growth of medical literature presents significant challenges for clinicians and researchers attempting to stay current with the latest evidence [41][42][43].LLMs are addressing this challenge by efficiently analyzing and summarizing vast volumes of medical literature, helping healthcare professionals maintain awareness of emerging developments and evidence-based practices [4,44].This capability supports informed clinical decision-making while reducing the time burden associated with literature reviews [4,12].</p>
<p>In research contexts, LLMs are accelerating discovery by analyzing large datasets from medical records, clinical trials, and scientific literature.This analytical power aids in identifying potential new treatments, developing effective therapies, and understanding disease mechanisms through pattern recognition and hypothesis generation [5].The ability to process and synthesize information across diverse sources enables researchers to identify connections and insights that might otherwise remain obscure [4].</p>
<p>Furthermore, LLMs are streamlining various aspects of the research process, from literature reviews and hypothesis formulation to experimental design and manuscript preparation [2].By automating routine aspects of research documentation and analysis, these models allow investigators to focus on creative and interpretive aspects of scientific inquiry [5].This efficiency gain has the potential to accelerate the pace of medical discovery and innovation, ultimately translating to improved patient care and outcomes.</p>
<p>Drug Discovery and Development</p>
<p>The application of LLMs in drug discovery represents a transformative approach to pharmaceutical research, offering the potential to significantly reduce development timelines and costs [4,[45][46][47][48].These models demonstrate remarkable capabilities in analyzing complex molecular structures, identifying promising compounds with therapeutic potential, and predicting the efficacy and safety profiles of candidate drugs [4,[49][50][51][52].By leveraging their pattern recognition abilities, LLMs can suggest novel molecular configurations that might address specific therapeutic targets, potentially expanding the range of treatment options for various conditions [4,[51][52][53][54].Chemical language models specifically designed for pharmaceutical applications have shown notable achievements in de novo drug design [4,53,54].These specialized models can generate molecular structures with desired properties, predict compound behaviors in biological systems, and optimize candidates for improved pharmacokinetics and reduced side effects [4].Studies have demonstrated that warm-started models, initialized with pre-trained biochemical language models, outperform baseline approaches in generating high-quality compounds with promising therapeutic potential [4,55].The integration of LLMs into drug discovery pipelines illustrates the potential for artificial intelligence to transform traditional research and development processes in pharmaceuticals [5,52].By accelerating the identification and optimization of lead compounds, these models may help address unmet medical needs more rapidly and efficiently, potentially benefiting patients with conditions that currently lack effective treatments [4].</p>
<p>Radiology and Medical Imaging</p>
<p>The integration of LLMs with medical imaging represents a significant advancement in diagnostic capabilities, particularly through multimodal models that can process both visual and textual information [4,11,[56][57][58].By analyzing radiological images in conjunction with clinical data, these systems can assist in the early identification of abnormalities and contribute to more precise diagnostic interpretations [4,[58][59][60].Models such as Med-Flamingo and LLaVA-Med demonstrate the capacity to understand and analyze medical images within their clinical context, potentially enhancing both the efficiency and accuracy of diagnostic processes [11,61,62].</p>
<p>Beyond image interpretation, LLMs are revolutionizing radiological workflow through automated report generation [4,63].This application addresses the time-consuming and potentially error-prone task of creating detailed radiological reports, especially in highvolume clinical environments [4].Automated medical report generation from imaging data streamlines the reporting process while maintaining accuracy and comprehensiveness, allowing radiologists to focus on complex cases requiring specialized expertise [4].Systems like ChatCAD have shown promising results in generating high-quality radiological reports that maintain consistency with human expertise while incorporating relevant clinical information [4].</p>
<p>The advancement of LLMs in radiology could potentially address workforce shortages and improve access to specialized imaging services, particularly in underserved areas [5].By augmenting the capabilities of radiologists through efficient image analysis and reporting, these technologies may help extend the reach of diagnostic imaging services while maintaining quality and accuracy [4,5].This application exemplifies how LLMs can enhance existing medical practices rather than replacing human expertise, supporting healthcare providers in delivering more efficient and accessible care [5,10].</p>
<p>Clinical Documentation and Administrative Support</p>
<p>The documentation burden in healthcare represents a significant challenge, with clinicians spending substantial time on administrative tasks that detract from direct patient care [64].LLMs offer promising solutions by assisting with various aspects of clinical documentation, from generating initial drafts of medical notes to organizing and summarizing patient information [2,65,66].This capability addresses the dual challenges of ensuring comprehensive documentation while minimizing the time impact on healthcare providers.</p>
<p>Specific applications include automated generation of clinical notes from doctorpatient conversations, standardization of medical notes for improved natural language processing, and organization of clinical data for enhanced accessibility and utility [4].By streamlining these processes, LLMs can help reduce clinician burnout, improve documentation quality, and allow healthcare providers to dedicate more attention to direct patient interaction [2].Additionally, these models can assist with coding and billing processes, potentially enhancing revenue cycle management while ensuring compliance with documentation requirements [2].</p>
<p>The efficiency gains offered by LLMs in clinical documentation may have broader implications for healthcare delivery and quality.By reducing the administrative burden on healthcare providers, these technologies could potentially address workforce shortages, improve provider satisfaction, and enhance the overall patient experience through more engaged and attentive care.Furthermore, standardized and comprehensive documenta-tion facilitated by LLMs may support improved clinical research, quality improvement initiatives, and population health management [5].</p>
<p>A summary of the application of LLMs used in healthcare is provided in Tables 1 and 2.</p>
<p>Evaluation Frameworks and Benchmarks</p>
<p>Performance Metrics and Assessment Approaches</p>
<p>Evaluating the performance of LLMs in healthcare contexts requires specialized metrics and methodologies that reflect the unique requirements and high-stakes nature of medical applications.Traditional natural language processing metrics such as BLEU and ROUGE are commonly applied to assess the quality of generated text, while task-specific metrics, including accuracy, precision, recall, and AUC, are employed for classification and prediction tasks [4,11].Additionally, specialized evaluation frameworks like MultiMedQA have been developed to assess the capabilities of LLMs in answering medical questions across various formats, testing both factual accuracy and medical reasoning abilities [11].</p>
<p>Healthcare LLMs are frequently evaluated using established medical benchmarks such as the USMLE (United States Medical Licensing Examination), PubMedQA, and MedMCQA, which provide standardized measures of medical knowledge and reasoning [11].Performance on these benchmarks serves as an indicator of a model's capacity to understand and apply medical concepts in clinically relevant contexts [4,11].More recent evaluation approaches include MMedBench, which covers 21 medical fields and assesses performance across multiple languages, providing deeper insights into model capabilities across diverse healthcare domains and linguistic contexts [11].</p>
<p>The evaluation of multimodal LLMs in healthcare introduces additional complexity, requiring metrics that assess performance across different data types.For image-related tasks, models like PMC-CLIP are evaluated using Recall@K for image-text retrieval and AUC for image classification [11].Similarly, models like LLaVA-Med and Med-Flamingo undergo evaluation on specialized visual question answering datasets such as VQA-RAD, SLAKE, and Path-VQA to measure their performance in medical imaging applications [11].</p>
<p>Human-Centered Evaluation</p>
<p>While automated metrics provide valuable quantitative assessments, human-centered evaluation approaches play a crucial role in determining the clinical utility and safety of healthcare LLMs [6,10].Mixed-methods surveys of clinicians have revealed varying levels of comfort with LLM applications in healthcare, with greater acceptance for assistive roles that support rather than replace human decision-making [6].These findings highlight the importance of incorporating healthcare provider perspectives in the evaluation and implementation of LLMs in clinical settings [6,10].</p>
<p>Expert validation of LLM outputs represents another essential component of evaluation, particularly for applications involving diagnosis, treatment recommendations, or patient communication [6,10].Studies have employed methodologies ranging from direct comparisons with clinician judgments to more sophisticated approaches where experts assess the quality, accuracy, and safety of model-generated content across different healthcare scenarios [10].These evaluations provide critical insights into the alignment between LLM outputs and clinical standards, identifying areas where models may require further refinement or human oversight [6,10].</p>
<p>The integration of automated metrics with human judgment offers a more comprehensive evaluation framework for healthcare LLMs [10,11].For instance, GPT-4 has been used as a reference model for evaluating other LLMs, with its assessments validated against human expert judgments to establish reliability [11].Similarly, doctors have been involved in comparing responses from different models, providing qualitative assessments that complement quantitative performance metrics [11].This multifaceted approach to evaluation acknowledges both the technical performance and practical utility of LLMs in healthcare contexts [6,10].</p>
<p>Reproducibility and Validation Challenges</p>
<p>A significant challenge in evaluating healthcare LLMs involves ensuring the reproducibility of results and validating performance across diverse clinical scenarios [6,67].A systematic review of studies on LLM-based chatbot health advice services revealed considerable variation in reporting quality, with many studies providing insufficient information to identify the specific model being evaluated [67].This lack of transparency complicates efforts to reproduce findings or compare performance across different studies and implementations [67].The evaluation of closed-source models presents particular challenges, as researchers often have limited visibility into model architecture, training data, or optimization methods [6,67].The systematic review found that 99.3% of studies assessed closed-source models without providing adequate information for identification, limiting the scientific value and practical applicability of the evaluations [67].This opacity in model reporting undermines the ability to build upon existing research or establish reliable benchmarks for healthcare LLM performance [67].</p>
<p>Another validation challenge concerns the ground truth used to define successful performance.The same review found that 64.5% of studies relied on subjective means as the ground truth for evaluating LLM performance, potentially introducing inconsistency and bias into the assessment process [67].Less than a third of studies addressed the ethical, regulatory, and patient safety implications of clinically integrating LLMs, highlighting a critical gap in comprehensive evaluation frameworks [67].Addressing these challenges requires more rigorous reporting standards, transparent evaluation methodologies, and greater attention to the practical and ethical dimensions of healthcare LLM implementation [6,67].</p>
<p>Empirical Evaluation and Benchmarking</p>
<p>Recent years have witnessed a surge in quantitative studies evaluating LLMs using established medical benchmarks.For example, Med-PaLM 2 achieved state-of-the-art results on the MultiMedQA suite, including the MedQA (USMLE) benchmark, with accuracy improvements exceeding 19% over previous models.GPT-4 has demonstrated an impressive 93.1% accuracy on MedQA, while models such as BioGPT and Meditron have excelled in biomedical question answering and domain-specific tasks [4,69].Beyond static benchmarks, real-world validation frameworks such as RWE-LLM have engaged thousands of clinicians across diverse settings, processing over 300,000 clinical interactions and demonstrating robust error detection and safety validation in live environments [70].These empirical results underscore the growing maturity of LLMs for clinical deployment, while also revealing persistent challenges in generalizability and reliability.</p>
<p>User-Centered and Clinician-Involved Studies</p>
<p>Recognizing that successful integration of LLMs in healthcare requires more than technical excellence, recent research has increasingly incorporated feedback from clinicians and end-users.Human-centered evaluation frameworks, such as the QUEST protocol, involve physicians, nurses, and pharmacists in the systematic assessment of LLM outputs for accuracy, comprehensiveness, bias, and harm in real clinical scenarios [71].Large-scale studies have shown that clinicians generally view LLMs as valuable adjuncts: improving diagnostic confidence, supporting decision making, and improving workflow efficiency, especially when the models are positioned as assistive tools rather than autonomous decision makers [72][73][74].For instance, in the RWE-LLM framework, over 6200 licensed clinicians participated in multi-tiered safety validation, providing critical feedback that informed iterative model improvement and ensured alignment with clinical standards [70].This participatory approach not only enhances model safety but also fosters user trust and acceptance.</p>
<p>Challenges and Limitations</p>
<p>Data Diversity and Heterogeneity in Healthcare</p>
<p>A central challenge in deploying LLMs in healthcare is the heterogeneity of data across languages, demographic groups, healthcare systems, and data quality.Healthcare data are inherently diverse, encompassing structured records, free-text clinical notes, imaging, and patient-generated information, each with unique formats and standards.This diversity is further complicated by regional differences in language, medical terminology, cultural practices, and health system workflows.For instance, an LLM trained predominantly on English-language data from high-resource environments may underperform in rural or low-resource regions, where local languages, dialects, and unique healthcare practices prevail.Such disparities hinder the generalizability and equity of AI-driven solutions, risking the exacerbation of existing health disparities [75].</p>
<p>Data heterogeneity also includes demographic diversity, age, sex, ethnicity, and socioeconomic status.Models trained on non-representative datasets may perpetuate biases, resulting in less accurate or potentially unsafe recommendations for underrepresented groups [4,75].Additionally, healthcare data often suffer from varying quality, including missing information, inconsistent coding, and documentation errors, all of which can degrade model reliability and trustworthiness [4,75].To address these challenges, several strategies are actively pursued:</p>
<p>• Curated and Representative Datasets: Building and utilizing datasets that reflect multiple languages, cultures, and demographic groups is essential.This includes collecting multilingual medical corpora and integrating data from varied healthcare environments [76].• Domain Adaptation and Fine-Tuning: LLMs can be fine-tuned on region-or institutionspecific data to capture local nuances in language and practice, improving model relevance and accuracy for specific settings [76].[4,77].</p>
<p>Proactively addressing data heterogeneity is vital for building LLMs that are robust, generalizable, and equitable, ultimately supporting improved health outcomes for all patient groups [4,75].</p>
<p>Technical Challenges and Model Limitations</p>
<p>Despite their impressive capabilities, LLMs in healthcare face significant technical challenges that limit their immediate clinical utility.Figure 5 presents these challenges and limitations of healthcare LLMs.A primary concern involves hallucination instances where models generate plausible but factually incorrect information-which can have serious consequences in medical contexts where accuracy is paramount [5,6].This tendency to produce fabricated content, particularly when facing uncertainty or gaps in knowledge, raises concerns about reliability in high-stakes healthcare applications [6].Limited contextual understanding represents another technical challenge, as current models may struggle to fully comprehend complex medical scenarios or integrate information across different time points or data sources [2,4].While LLMs can process vast amounts of text, they may miss subtle clinical nuances or fail to appropriately weigh the relevance of different information elements, potentially leading to incorrect or incomplete analyses [2,5].Additionally, these models typically have knowledge cutoffs based on their training data, limiting their awareness of recent medical developments or emerging health threats [4,6].</p>
<p>CHALLENGES AND LIMITATIONS OF LLMs IN HEALTHCARE</p>
<p>The computational requirements of large language models present practical implementation challenges, particularly in resource-constrained healthcare settings [5].The hardware needed to run sophisticated LLMs may be prohibitively expensive for many healthcare organizations, while the energy consumption associated with model inference raises concerns about environmental sustainability [4,5].Furthermore, the latency of model responses may be problematic in time-sensitive clinical scenarios where immediate decision support is required [4,5].</p>
<p>Ethical Considerations and Governance</p>
<p>The deployment of LLMs in healthcare raises profound ethical questions that demand careful consideration and robust governance frameworks.Patient privacy represents a fundamental concern, as these models may inadvertently memorize or leak sensitive health information from their training data [6,68].Ensuring compliance with healthcare privacy regulations such as HIPAA, while leveraging the capabilities of LLMs requires sophisticated data protection measures and appropriate limitations on model inputs and outputs [5,6].</p>
<p>Informed consent and transparency present additional ethical challenges, particularly regarding patient awareness of AI involvement in their care [6].Patients have the right to understand when LLMs are being used to influence their diagnosis, treatment, or health information, yet conveying this information in an accessible manner without overwhelming individuals remains challenging [6,67].The potential for patients to develop inappropriate trust in or resistance to LLM-influenced care further complicates these considerations [6].</p>
<p>Accountability and responsibility constitute critical governance concerns, as the distributed nature of LLM development and deployment can obscure lines of liability when errors occur [5,6].Healthcare organizations, technology developers, regulatory bodies, and individual providers all play roles in ensuring safe and appropriate LLM use, necessitating clear frameworks for oversight, incident reporting, and continuous quality improvement [6,67].Less than a third of studies address these ethical, regulatory, and patient safety implications, highlighting a significant gap in the current approach to healthcare LLM implementation [67].</p>
<p>Explainability and Interpretability of LLM Outputs</p>
<p>A major barrier to the adoption of LLMs in healthcare is the need for explainability; clinicians and stakeholders must be able to interpret and trust model outputs, especially given the high-stakes nature of medical decision-making [4,78,79].Transparent and interpretable models foster clinician trust, support shared decision-making, and ensure accountability in patient care [4,77,80,81].Several methods enhance the interpretability of LLM outputs in healthcare: Explainable AI is not only a technical challenge but also an ethical imperative.As LLMs become more integrated into clinical workflows, ongoing research and development in explainability will be essential for responsible and effective deployment [78][79][80][81].</p>
<p>Bias, Fairness, and Health Equity</p>
<p>LLMs trained on historical medical data risk perpetuating or amplifying existing biases in healthcare, potentially exacerbating health disparities among different demographic groups [4,5].Studies have demonstrated that certain models exhibit racial bias in patient diagnosis, disproportionately affecting minority groups [5].This algorithmic bias may result from the underrepresentation of certain populations in training data or the presence of biased clinical practices in the historical record from which models learn [4,6].Addressing bias requires comprehensive approaches that include diverse and representative training data, careful model evaluation across different demographic groups, and ongoing monitoring for disparate impacts [5,6].Additionally, the development of debiasing techniques specifically designed for healthcare applications can help mitigate these concerns, though complete elimination of bias remains challenging given the complex socio-historical factors influencing medical data and practice [4,6].</p>
<p>Ensuring equitable access to the benefits of LLM technologies represents another dimension of fairness concerns [5,6].The digital divide in healthcare, wherein technological advancements disproportionately benefit well-resourced settings and populations, may be exacerbated by LLM integration if deployment primarily occurs in affluent healthcare systems or requires resources unavailable in underserved areas [5].Thoughtful implementation strategies that prioritize health equity and actively address access disparities are essential for ensuring that LLMs contribute to reducing rather than widening healthcare inequalities [5,6].</p>
<p>Integration with Clinical Workflow</p>
<p>The successful implementation of LLMs in healthcare requires seamless integration with existing clinical workflows, technological infrastructure, and human processes [12].Disrupting established routines or adding complexity to already busy clinical environments may limit adoption regardless of the potential benefits offered by these technologies [12].Understanding and adapting to the practical realities of diverse healthcare settings-from large academic medical centers to small community practices-presents a significant implementation challenge [12].Interoperability with existing electronic health record systems and other healthcare technologies represents a crucial technical aspect of workflow integration [12].LLMs must be able to access relevant clinical data in real time while maintaining security and privacy, often requiring sophisticated integration solutions that may be difficult to implement across heterogeneous IT environments [5,12].Furthermore, the outputs of these models need to be presented in formats that align with clinical information needs and decision-making processes, avoiding information overload while providing actionable insights [12].</p>
<p>Healthcare provider acceptance and adaptation constitute human factors that significantly influence successful integration [10,12].Clinicians may experience resistance to technology-driven changes in practice, particularly when they perceive potential threats to autonomy, increases in workload, or risks to patient care quality [10].Addressing these concerns through collaborative design approaches, comprehensive training programs, and clear communication about the supportive rather than the replacement role of LLMs can enhance acceptance and appropriate utilization [10,12].</p>
<p>Future Directions</p>
<p>Multimodal and Domain-Specific Advancements</p>
<p>The future of LLMs in healthcare will likely be characterized by increasingly sophisticated multimodal capabilities, enabling models to process and integrate diverse data types, including text, images, audio, and structured clinical information [4,5].These advance-ments will support a more comprehensive analysis of patient cases, potentially enhancing diagnostic accuracy and treatment optimization through holistic data interpretation [5,11].The development of specialized architectures designed explicitly for multimodal medical data integration represents a promising research direction with significant clinical implications [4,12].Domain-specific models tailored to particular medical specialties or healthcare contexts will likely proliferate, offering enhanced performance for specialized applications [11,12].Models designed specifically for fields such as radiology, pathology, mental health, or emergency medicine can incorporate domain-specific knowledge and reasoning patterns, potentially outperforming general-purpose models in these specialized contexts [11,12].This trend toward specialization may be accompanied by increased transparency in model development and evaluation, addressing current limitations in reproducibility and validation [11,68].</p>
<p>Architectural innovations focused on enhancing the reliability, efficiency, and interpretability of healthcare LLMs represent another important direction for future research [4,5].Developments may include improved mechanisms for uncertainty quantification, allowing models to express confidence levels in their outputs and appropriately defer to human judgment when facing ambiguity [4,12].Similarly, advancements in computational efficiency may reduce the resource requirements for deployment, potentially expanding access to these technologies across diverse healthcare settings [4,5].</p>
<p>Human-AI Collaboration Models</p>
<p>The evolution of effective collaboration models between healthcare professionals and LLMs represents a critical area for future development [10,12].Rather than viewing AI as a replacement for human expertise, research increasingly focuses on creating synergistic relationships where each party contributes complementary strengths [10,12].These collaborative frameworks may involve dynamic task allocation based on relative capabilities, shared decision-making protocols, and adaptive interfaces that adjust to different clinical scenarios and user preferences [10,12].Educational approaches for preparing healthcare professionals to work effectively with LLMs constitute another important direction for development [12].Future medical education may increasingly incorporate training on appropriate AI utilization, critical evaluation of model outputs, and recognition of situations where human judgment should prevail.This educational evolution will help ensure that healthcare providers can leverage the capabilities of LLMs while maintaining the human-centered aspects of care that remain essential to quality healthcare delivery [10].</p>
<p>Implementation science research focused on optimizing the integration of LLMs into clinical practice represents another promising direction [12].Studies exploring factors that influence successful adoption, identifying best practices for implementation across diverse healthcare settings, and developing metrics for evaluating real-world impact will provide valuable guidance for healthcare organizations seeking to leverage these technologies effectively [12].This research may help bridge the gap between promising pilot projects and scalable, sustainable implementations that meaningfully improve healthcare delivery.</p>
<p>Regulatory Frameworks and Standard Development</p>
<p>The development of comprehensive regulatory frameworks specifically addressing LLMs in healthcare represents an essential direction for ensuring responsible deployment and patient safety [5,6].Current regulatory approaches often struggle to address the unique characteristics of these models, including their probabilistic outputs, potential for emergent behaviors, and continuous evolution through additional training or fine-tuning [6,67].Future regulatory frameworks may incorporate novel approaches such as continuous monitoring requirements, performance thresholds for specific clinical applications, and mandatory reporting of adverse events associated with LLM use [5,6].</p>
<p>Standardization efforts around evaluation methodologies, reporting requirements, and implementation guidelines will likely accelerate as LLMs become more prevalent in healthcare [6,67].These standards may address current gaps in reproducibility and transparency, potentially requiring more detailed documentation of model characteristics, training data, evaluation procedures, and performance limitations [67].Additionally, standards for user interfaces, clinical decision support integration, and appropriate disclaimers may help ensure consistent and responsible implementation across different healthcare contexts [6,67].International coordination on governance approaches represents another important direction, as healthcare LLMs increasingly cross national boundaries through cloud-based deployment models [5,6].Harmonizing regulatory requirements, ethical standards, and data governance practices across different jurisdictions may help facilitate innovation while maintaining appropriate safeguards [5,6].This coordination could potentially accelerate the responsible development and deployment of healthcare LLMs by reducing regulatory uncertainty and establishing consistent expectations for developers and implementers [5,6].</p>
<p>Patient-Centered Design and Participatory Approaches</p>
<p>Future developments in healthcare LLMs will likely place greater emphasis on patientcentered design, incorporating patient perspectives, preferences, and needs throughout the development and implementation process [5,6].Patient involvement in defining use cases, establishing evaluation criteria, and assessing real-world impact can help ensure that these technologies address genuine healthcare needs while respecting patient autonomy and values [6].This participatory approach may lead to applications that more effectively support shared decision-making, enhance health literacy, and improve patient engagement [5,6].</p>
<p>Personalization capabilities represent another promising direction for patient-centered LLM development [5,6].Future models may increasingly adapt their interactions based on individual patient characteristics, preferences, and health literacy levels, providing customized information and support that resonates with diverse populations [5].This personalization could potentially enhance patient understanding, adherence to treatment recommendations, and overall satisfaction with healthcare experiences [5,6].Accessibility considerations will likely receive greater attention in future healthcare LLM development, ensuring that these technologies benefit populations with diverse needs and abilities [5,6].This may include multilingual capabilities to serve linguistically diverse communities, interfaces designed for individuals with different levels of digital literacy or physical abilities, and deployment strategies that prioritize underserved populations and healthcare settings [5,6].Through these patient-centered approaches, healthcare LLMs may contribute to more equitable and inclusive healthcare systems that better serve all populations [5,6].</p>
<p>Conclusions</p>
<p>This comprehensive survey has examined the rapidly evolving landscape of large language models in healthcare, highlighting their transformative potential while acknowledging the significant challenges that must be addressed for responsible implementation.The application of LLMs across diverse healthcare domains-from clinical decision support and medical education to research acceleration and patient care-demonstrates the remarkable versatility and utility of these technologies in addressing complex healthcare needs [2,4].At the same time, concerns regarding factual accuracy, privacy, bias, and appropriate integration into clinical workflows underscore the necessity of thoughtful development and governance approaches [2,5,6].</p>
<p>The technical evolution of healthcare LLMs continues at a rapid pace, with advancements in multimodal capabilities, domain specialization, and computational efficiency expanding the range of possible applications [4,11,12].These developments are complemented by growing attention to evaluation methodologies, regulatory frameworks, and implementation strategies that can help translate promising research into tangible healthcare improvements [6,67].The emphasis on human-AI collaboration rather than replacement reflects an emerging consensus about the optimal role of these technologies in supporting rather than supplanting human expertise and judgment [10,12].</p>
<p>Looking forward, the responsible integration of LLMs into healthcare systems will require concerted efforts from diverse stakeholders, including technology developers, healthcare providers, regulatory bodies, patient advocates, and researchers [5,6,67].By addressing current limitations while thoughtfully leveraging the capabilities of these powerful models, the healthcare community has an opportunity to enhance clinical decision-making, improve operational efficiency, accelerate medical discoveries, and ultimately deliver more personalized and effective care to patients [5,6].As this field continues to evolve, maintaining a balance between innovation and caution will be essential for realizing the full potential of large language models to transform healthcare while upholding the fundamental principles of safety, equity, and patient-centeredness that define quality healthcare delivery [5,6,67].</p>
<p>Figure 1 .
1
Figure 1.Evolution and architectural foundation of large language models (LLMs).</p>
<ol>
<li>1 .
1
Data Acquisition and Pre-Training</li>
</ol>
<p>Figure 4 .
4
Figure 4. Methodology of LLMs in healthcare: data acquisition and pre-training, fine-tuning and adaptation, prompt engineering, and in-context learning.</p>
<p>Figure 5 .
5
Figure 5. Challenges and limitations of healthcare large language models (LLMs).</p>
<p>Model Reasoning OUTPUTS &amp; APPLICATIONS • Comprehensive Clinical Insights • Diagnostic Supports • Patient-Centered Report • Decision Support Tools More Accurate Diagnosis Benefits Enhanced Contextual Relevant Improved Care Recommendati ons Holistic Patient Understanding
Figure 3. Multimodal capabilities of healthcare large language models in processing and generationof diverse data types such as images, audio, and structured clinical data.
• Multimodal Fusion Layer • Contextual Fusion Layer • Contextual Embedding Layer • Cross-</p>
<p>Table 1 .
1
Survey of large language models in healthcare applications.
SourceLLMs UsedHighlightApplication AreaOverview of foundationalM. Johnsen [1]GPT-4, LLaMALLM conceptsGeneral overviewand applicationsComprehensive scopingX. Meng et al. [2]ChatGPT, GPT-4, Med-PaLMreview of LLM applicationsMedical applicationsin medicineReview of applications,D. Wang and S. Zhang [3]ChatGPT, GPT-3.5, GPT-4advances, and challenges ofMedical and healthcare fieldsLLMs in healthcareZ. A. Nazi and W. Peng [4]ChatGPT, GPT-4, Med-PaLMComprehensive review of LLMs in healthcare and medicineHealthcare and medical domainK. Zhang et al. [5]ChatGPT, GPT-4, Med-PaLMAnalysis of transformative impact of LLMs on healthcareMedicine and healthcare transformationSystematic review of LLMF. Busch et al. [6]ChatGPT, GPT-4, Claudeapplications and challenges inPatient carepatient careK. He et al. [9]ChatGPT, GPT-4, Med-PaLM, Llama-2Survey focusing on data, technology, applications, accountability and ethicsHealthcare ethics and accountabilityM. Spotnitz et al. [10]ChatGPT, GPT-4, ClaudeSurvey of clinicians' perspectives on LLM utilityClinical utility assessmentD. Zhang et al. [11]ChatGPT, Med-PaLM, GatorTronSurvey of medical datasets for training and evaluating LLMsMedical datasetsW. Wang et al. [12]ChatGPT, GPT-4, Claude, BioMistralSurvey of LLM-based agents in medicineMedical agentsJ. Li et al. [13]ChatGPT, Med-PaLM, Med-GeminiAnalysis of whether LLMs human expertise enhance or replaceClinical decision supportM. Yuan et al. [14]ChatGPT, GPT-4, ClaudeProgressive pathway towards AI healthcare assistantsHealthcare assistantsE. Jussupow et al. [15]Early GPT models, BERTInvestigation of physicians' decision-making with AIMedical diagnosisA. Bojesomo et al. [16]ChatGPT, GPT-4, Med-PaLMSystematic review of LLMs for disease diagnosisDisease diagnosisP. Karttunen [17]ChatGPT, GPT-4, Llama-2Analysis of LLMs for healthcare decision supportHealthcare decision supportI. Almubark [18]ChatGPT, Med-PaLM, Med-GeminiImpact of LLMs on disease diagnosisDisease diagnosisJ. C. L. Ong et al. [19]ChatGPT, GPT-4, ClaudeDevelopment of LLM-based CDSS for medication safetyMedication safetyC. Castaneda et al. [20]Pre-LLM AI systemsCDSS for diagnostic accuracy and precision medicineDiagnostic accuracyDevelopment of LLMX. Yang et al. [21]GatorTronspecifically for electronicElectronic health recordshealth records</p>
<p>Table 1 .
1
Cont.
SourceLLMs UsedHighlightApplication AreaX. Yang et al. [22]ChatGPT, Med-PaLM, Med-GeminiApplications of LLMs in diagnosis and treatmentDisease diagnosis and treatmentK. Holley and M. Mathur [23]ChatGPT, GPT-4, Claude, GeminiExploration of LLMs and in healthcare generative AI as next frontierHealthcare innovationLLM-empowered diagnosticB. Yang et al. [24]DrHouse systemreasoning system using sensorDiagnostic reasoningdata and expert knowledgeB. Santhosh and K. Viswanath [25]GPT-3.5, GPT-4, BERTIntegration of ML and DL in medical educationMedical educationA. Abd-Alrazaq et al. [26]ChatGPT, GPT-4, BardOpportunities and challenges of LLMs in medical educationMedical educationC. W. Safranek et al. [27]ChatGPT, GPT-4, ClaudeApplications and implications of LLMs in medical educationMedical educationSystematic review of LLMsH. C. Lucas et al. [28]ChatGPT, GPT-4, Med-PaLMand implications forMedical educationmedical educationT. M. Benítez et al. [29]ChatGPT, GPT-4, ClaudePromise and pitfalls of LLMs in medical educationMedical educationAccelerating integration ofD. Q. Wang et al. [30]ChatGPT, Bard, LLaMAChatGPT intoBiomedical researchbiomedical researchM. Almansour and F. M. Alfhaid [31]GPT-4, Claude, GeminiPersonalization of health generative AI professional education usingHealth professional educationW. Qian [55]Early GPT models, BERTMachine learning applications for drug discoveryDrug discovery</p>
<p>Table 2 .
2
Continued: Survey of large language models in healthcare applications.
SourceLLMs UsedHighlightApplication AreaD. Domrös-Zoungrana et al. [32]ChatGPT, GPT-4, Med-PaLMConsiderations for integrating AI in medical educationMedical educationL. Zhui et al. [33]ChatGPT, GPT-4, ClaudeEthical considerations of LLMs in medical educationMedical education ethicsK. g Lema [34]GPT-4, Claude, GeminiAGI applications for medical education and trainingMedical education and trainingS. Tripathi et al. [35]ChatGPT, GPT-4, ClaudeOptimizing clinical workflow and patient care with LLMsClinical workflow optimizationDevelopment, applications,R. Yang et al. [36]ChatGPT, GPT-4, Llama-2and challenges of LLMs inHealthcare applicationshealthcareM. ZareiNejad and P. Tavana [37]ChatGPT, GPT-4, ClaudeApplications of generative AI for patient engagementPatient engagementZ. Yang et al. [38]Talk2CareFacilitating asynchronous patient-provider communication using LLMsPatient-provider communication</p>
<p>Table 2 .
2
Cont.
SourceLLMs UsedHighlightApplication AreaR. Mohammad et al. [39]Arabic-adapted ChatGPT, Arabic GPTOptimizing LLMs for Arabic healthcare communicationMultilingual healthcare communicationN. Mannhardt [40]ChatGPT, GPT-4, Llama-2Enhancing readability of clinical notes using LLMsClinical note comprehensionPerformance of ChatGPT forN. L. Rane et al. [44]ChatGPT, GPT-4, Bardscientific and researchMedical researchadvancementsB. Huo et al. [67]ChatGPT, GPT-4, ClaudeSystematic review of LLM chatbots for health adviceHealth adviceM. Nydén and D. Bika [45]ChatGPT, GPT-4, AlphaFoldMedicine design and development in the AI eraDrug developmentG. Doron et al. [46]GPT-4, AlphaFold, ESMFoldPioneering pharmaceutical R&amp;D with generative AIPharmaceutical R&amp;DAI driving digitalS. Harrer et al. [47]ChatGPT, GPT-4, AlphaFoldtransformation inPharma transformationpharmaceutical industryY. Zhang et al. [48]ChatGPT, AlphaFold, ESMFoldAccelerating drug discovery and clinical trials with AIDrug discoveryDriving productivity inG. Doron et al. [49]GPT-4, AlphaFold, Chromapharmaceutical R&amp;D withPharmaceutical productivitygenerative AIJ. Jiang et al. [50]GPT-4, BERT, RoBERTaReview of transformer models in drug discoveryDrug discoveryFramework, advances,A. Gangwal et al. [51]GPT-4, ProtGPT2, AlphaFoldchallenges of generative AI inDrug discoverydrug discoveryK. Zhang et al. [52]ChatGPT, GPT-4, AlphaFoldAI applications in drug developmentDrug developmentX.h. Liu et al. [53]ChatGPT, AlphaFold, ESMFoldLLMs facilitating molecular biology and drug developmentMolecular biology and drug developmentUsing LLMs for translationD. Oniani et al. [54]GPT-4, MolGPT, ChemLLMbetween drug moleculesDrug indication mappingand indicationsApplications and challengesR. AlSaad et al. [56]GPT-4V, Gemini Pro, Claude 3of multimodal LLMsMultimodal healthcarein healthcareR. Agbareia et al. [57]GPT-4V, Med-Flamingo, LLaVA-MedQuantitative analysis of LLMs for diagnosis visual-textual integration inMedical diagnosisR. Guo et al. [58]Med-Flamingo, LLaVA-Med, PMC-CLIPSurvey of image-text in biomedicine multimodal modelsBiomedical imagingD. Tian et al. [59]Med-Flamingo, LLaVA-Med, RadBERTRole of LLMs in medical image processingMedical image processingAI applications for boneM. Kutbi [60]GPT-4V, RadBERT, PandaGPTfracture detection inBone fracture detectionmedical images</p>
<p>Table 2 .
2
Cont.
SourceLLMs UsedHighlightApplication AreaVision-language models forM. Ayaz et al. [61]MedVLMmedical applications inMedical image understandingconsumer devicesFoundation modelsC. Liu et al. [62]Med-Flamingo, LLaVA-Med, BioMedCLIPcombining visual and language capabilities forMedical multimodalitymedicineN. Soni et al. [63]ChatCAD, RadGPT, Radiology-GPTOpportunities and challenges of LLMs in radiologyRadiologySecurity and privacyM. A. Rahman [68]GPT-4V, Gemini Pro, Claude 3considerations for multimodalHealthcare securityLLMs in healthcare
Data Availability Statement:No new data were created or analyzed in this study.Conflicts of Interest:The authors declare no conflicts of interest.Author Contributions: Conceptualization, S.M. and M.J.S.; methodology, S.M. and M.J.S.; software, S.M.; validation, M.J.S.; formal analysis, S.M.; investigation, M.J.S.; resources, M.J.S.; data curation, S.M.; writing-original draft preparation, S.M.; writing-review and editing, M.J.S.; visualization, M.J.S.; supervision, M.J.S.; project administration, M.J.S.; funding acquisition, M.J.S.All authors have read and agreed to the published version of the manuscript.
Large Language Models. M Johnsen, LLMs. </p>
<p>. Maria Johnsen, 2024Trondheim, Norway</p>
<p>The application of large language models in medicine: A scoping review. X Meng, X Yan, K Zhang, D Liu, X Cui, Y Yang, M Zhang, C Cao, J Wang, X Wang, 10.1016/j.isci.2024.1097132024. 10971327</p>
<p>Large language models in medical and healthcare fields: Applications, advances, and challenges. D Wang, S Zhang, 10.1007/s10462-024-10921-0Artif. Intell. Rev. 572992024</p>
<p>Large Language Models in Healthcare and Medical Domain: A Review. Z A Nazi, W Peng, 10.3390/informatics11030057Informatics. 2024</p>
<p>Revolutionizing health care: The transformative impact of large language models in medicine. K Zhang, X Meng, X Yan, J Ji, J Liu, H Xu, H Zhang, D Liu, J Wang, X Wang, 10.2196/59069J. Med. Internet Res. 272025e59069. [CrossRef</p>
<p>Current applications and challenges in large language models for patient care: A systematic review. F Busch, L Hoffmann, C Rueger, E H Van Dijk, R Kader, E Ortiz-Prado, M R Makowski, L Saba, M Hadamitzky, J N Kather, 10.1038/s43856-024-00717-2Commun. Med. 2025, 5, 26</p>
<p>Scoping studies: Towards a methodological framework. H Arksey, L O'malley, Int. J. Soc. Res. Methodol. 82005</p>
<p>Scoping studies: Advancing the methodology. D Levac, H Colquhoun, K K O'brien, 10.1186/1748-5908-5-69Implement. Sci. 52010</p>
<p>A survey of large language models for healthcare: From data, technology, and applications to accountability and ethics. K He, R Mao, Q Lin, Y Ruan, X Lan, M Feng, E Cambria, 10.1016/j.inffus.2025.102963Inf. Fusion. 1182025. 102963</p>
<p>A survey of clinicians' views of the utility of large language models. M Spotnitz, B Idnay, E R Gordon, R Shyu, G Zhang, C Liu, J J Cimino, C Weng, 10.1055/a-2281-7092Appl. Clin. Inf. 152024</p>
<p>A survey of datasets in medicine for large language models. D Zhang, X Xue, P Gao, Z Jin, M Hu, Y Wu, X Ying, 10.20517/ir.2024.27Intell. Robot. 2024</p>
<p>W Wang, Z Ma, Z Wang, C Wu, W Chen, X Li, Y Yuan, arXiv:2502.11211A Survey of LLM-based Agents in Medicine: How far are we from Baymax? arXiv 2025. </p>
<p>Large language models-powered clinical decision support: Enhancing or replacing human expertise?. J Li, Z Zhou, H Lyu, Z Wang, 10.1016/j.imed.2025.01.001Intell. Med. 52025</p>
<p>Large language models illuminate a progressive pathway to artificial intelligent healthcare assistant. M Yuan, P Bao, J Yuan, Y Shen, Z Chen, Y Xie, J Zhao, Q Li, Y Chen, L Zhang, 10.1016/j.medp.2024.100030Med. Plus 2024, 1, 100030. [CrossRef</p>
<p>Augmenting medical diagnosis decisions? An investigation into physicians' decision-making process with artificial intelligence. E Jussupow, K Spohrer, A Heinzl, J Gawlitza, 10.1287/isre.2020.0980Inf. Syst. Res. 322021</p>
<p>Revolutionizing Disease Diagnosis with Large Language Models: A Systematic Review. A Bojesomo, M Seghier, L Hadjileontiadis, A Alshehhi, 10.21203/rs.3.rs-5704278/v1Res. Sq. 2024</p>
<p>Large Language Models in Healthcare Decision Support. P Karttunen, 2023Tampere, FinlandTampere UniversityBachelor's Thesis</p>
<p>Exploring the Impact of Large Language Models on Disease Diagnosis. I Almubark, 10.1109/ACCESS.2025.3527025IEEE Access. 132025</p>
<p>Development and testing of a novel large language model-based clinical decision support systems for medication safety in 12 clinical specialties. J C L Ong, L Jin, K Elangovan, G Y S Lim, D Y Z Lim, G G R Sng, Y Ke, J Y M Tung, R J Zhong, C M Y Koh, arXiv:2402.017412024</p>
<p>Clinical decision support systems for improving diagnostic accuracy and achieving precision medicine. C Castaneda, K Nalley, C Mannion, P Bhattacharyya, P Blake, A Pecora, A Goy, K S Suh, 10.1186/s13336-015-0019-3J. Clin. Bioinform. 52015</p>
<p>A large language model for electronic health records. npj Digit. X Yang, A Chen, N Pournejatian, H C Shin, K E Smith, C Parisien, C Compas, C Martin, A B Costa, M G Flores, 10.1038/s41746-022-00742-2Med. 2022, 5, 194. [CrossRef</p>
<p>Application of large language models in disease diagnosis and treatment. X Yang, T Li, Q Su, Y Liu, C Kang, Y Lyu, L Zhao, Y Nie, Y Pan, 10.1097/CM9.0000000000003456Chin. Med. J. 1382025</p>
<p>The Next Frontier. K Holley, M Mathur, Generative Ai For Llms, Healthcare, 2024O'Reilly Media, IncSebastopol, CA, USA</p>
<p>Drhouse: An llm-empowered diagnostic reasoning system through harnessing outcomes from sensor data and expert knowledge. B Yang, S Jiang, L Xu, K Liu, H Li, G Xing, H Chen, X Jiang, Z Yan, 10.1145/3699765Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 82024</p>
<p>Integration of machine learning and deep learning in medical and healthcare education. B Santhosh, K Viswanath, Applications of Parallel Data Processing for Biomedical Imaging. Hershey, PA, USA2024IGI Global</p>
<p>Large language models in medical education: Opportunities, challenges, and future directions. A Abd-Alrazaq, R Alsaad, D Alhuwail, A Ahmed, P M Healy, S Latifi, S Aziz, R Damseh, S A Alrazak, J Sheikh, 10.2196/48291JMIR Med. Educ. 92023</p>
<p>The role of large language models in medical education: Applications and implications. C W Safranek, A E Sidamon-Eristoff, A Gilson, D Chartash, 10.2196/50945JMIR Med. Educ. 92023e50945. [CrossRef</p>
<p>A systematic review of large language models and their implications in medical education. H C Lucas, J S Upperman, J R Robinson, 10.1111/medu.15402Med. Educ. 582024</p>
<p>Harnessing the potential of large language models in medical education: Promise and pitfalls. T M Benítez, Y Xu, J D Boudreau, A W C Kow, F Bello, L Van Phuoc, X Wang, X Sun, G K K Leung, Y Lan, 10.1093/jamia/ocad252J. Am. Med. Inf. Assoc. 312024</p>
<p>Accelerating the integration of ChatGPT and other large-scale AI models into biomedical research and healthcare. D Q Wang, L Y Feng, J G Ye, J G Zou, Y F Zheng, 10.1002/mef2.43MedComm Future Med. 2023, 2, e43. [CrossRef</p>
<p>Generative artificial intelligence and the personalization of health professional education: A narrative review. M Almansour, F M Alfhaid, 10.1097/MD.0000000000038955Medicine. 1032024e38955. [CrossRef</p>
<p>Medical Education: Considerations for a Successful Integration of Learning with and Learning about AI. D Domrös-Zoungrana, N Rajaeean, S Boie, E Fröling, C Lenz, 10.1177/23821205241284719J. Med. Educ. Curric. Dev. 2024, 11, 23821205241284719</p>
<p>Ethical considerations and fundamental principles of large language models in medical education. Z Li, F Li, X Wang, Q Fu, W Ren, J. Med. Internet Res. 26e600832024</p>
<p>Artificial General Intelligence (AGI) for Medical Education and Training. K G Lema, 10.31730/osf.io/xytzm2023</p>
<p>Efficient healthcare with large language models: Optimizing clinical workflow and enhancing patient care. S Tripathi, R Sukumaran, T S Cook, 10.1093/jamia/ocad258J. Am. Med. Inf. Assoc. 312024</p>
<p>Large language models in health care: Development, applications, and challenges. R Yang, T F Tan, W Lu, A J Thirunavukarasu, D S W Ting, N Liu, 10.1002/hcs2.61Health Care Sci. 22023</p>
<p>Application of Generative AI in Patient Engagement. M Zareinejad, P Tavana, Application of Generative AI in Healthcare Systems. Cham, SwitzerlandSpringer Nature2025119</p>
<p>Z Yang, X Xu, B Yao, S Zhang, E Rogers, S Intille, N Shara, G G Gao, D Wang, Talk2care, arXiv:2309.09357Facilitating asynchronous patient-provider communication with large-language-model. arXiv 2023. </p>
<p>Optimizing Large Language Models for Arabic Healthcare Communication: A Focus on Patient-Centered NLP Applications. Big Data Cogn. R Mohammad, O S Alkhnbashi, M Hammoudeh, 10.3390/bdcc8110157Comput. 2024, 8, 157. [CrossRef</p>
<p>Improving Patient Access and Comprehension of Clinical Notes: Leveraging Large Language Models to Enhance Readability and Understanding. N Mannhardt, 2023Cambridge, MA, USAMassachusetts Institute of TechnologyPh.D. Thesis</p>
<p>Progress in evidence-based medicine: A quarter century on. B Djulbegovic, G H Guyatt, 10.1016/S0140-6736(16)31592-6Lancet. 3902017</p>
<p>Evidence based medicine: An approach to clinical problem-solving. W Rosenberg, A Donald, BMJ. 3101995</p>
<p>Evidence-based treatment and practice: New opportunities to bridge clinical research and practice, enhance the knowledge base, and improve patient care. A E Kazdin, 10.1037/0003-066X.63.3.146Am. Psychol. 632008</p>
<p>Contribution and performance of ChatGPT and other Large Language Models (LLM) for scientific and research advancements: A double-edged sword. N L Rane, A Tawde, S P Choudhary, J Rane, Int. Res. J. Mod. Eng. Technol. Sci. 2023</p>
<p>New Medicines Design, Development and Commercialization in the Era of AI. M Nydén, D Bika, Proceedings of the LMDE Conference. the LMDE ConferenceAthens, Greece; Berlin/Heidelberg, GermanySpringer19-20 June 2023. 2023</p>
<p>New Horizons: Pioneering Pharmaceutical R&amp;D with Generative AI from lab to the clinic-an industry perspective. G Doron, S Genway, M Roberts, S Jasti, arXiv:2312.124822023</p>
<p>Artificial intelligence drives the digital transformation of pharma. S Harrer, J Menard, M Rivers, D V Green, J Karpiak, J R Jeliazkov, M V Shapovalov, D Del Alamo, M C Sternke, Artificial Intelligence in Clinical Practice. The NetherlandsAmsterdam2024</p>
<p>Accelerating drug discovery, development, and clinical trials by artificial intelligence. Y Zhang, M Mastouri, Y Zhang, 10.1016/j.medj.2024.07.02620245</p>
<p>Driving productivity and scientific breakthroughs in pharmaceutical R&amp;D. G Doron, S Genway, M Roberts, S Jasti, A I Generative, Drug Discov. Today. 302024. 104272</p>
<p>A review of transformers in drug discovery and beyond. J Jiang, L Chen, L Ke, B Dou, C Zhang, H Feng, Y Zhu, H Qiu, B Zhang, G Wei, 10.1016/j.jpha.2024.101081J. Pharm. Anal. 2024. 101081</p>
<p>Generative artificial intelligence in drug discovery: Basic framework, recent advances, challenges, and opportunities. A Gangwal, A Ansari, I Ahmad, A K Azad, V Kumarasamy, V Subramaniyan, L S Wong, Front. Pharmacol. 13310622024</p>
<p>Artificial intelligence in drug development. K Zhang, X Yang, Y Wang, Y Yu, N Huang, G Li, X Li, J C Wu, S Yang, 10.1038/s41591-024-03434-4Nat. Med. 312025</p>
<p>Large language models facilitating modern molecular biology and novel drug development. X H Liu, Z H Lu, T Wang, F Liu, 10.3389/fphar.2024.1458739Front. Pharmacol. 152024. 1458739</p>
<p>Emerging opportunities of using large language models for translation between drug molecules and indications. D Oniani, J Hilsman, C Zang, J Wang, L Cai, J Zawala, Y Wang, 10.1038/s41598-024-61124-0Sci. Rep. 2024, 14, 10738. [CrossRef</p>
<p>W Qian, Machine Learning for Drug Discovery and Beyond. Champaign, IL, USA2022University of Illinois at Urbana-ChampaignPh.D. Thesis</p>
<p>Multimodal large language models in health care: Applications, challenges, and future outlook. R Alsaad, A Abd-Alrazaq, S Boughorbel, A Ahmed, M A Renault, R Damseh, J Sheikh, 10.2196/59505J. Med. Internet Res. 262024e59505. [CrossRef</p>
<p>Visual-textual integration in LLMs for medical diagnosis: A preliminary quantitative analysis. R Agbareia, M Omar, S Soffer, B S Glicksberg, G N Nadkarni, E Klang, 10.1016/j.csbj.2024.12.019Comput. Struct. Biotechnol. J. 272024</p>
<p>A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations. R Guo, J Wei, L Sun, B Yu, G Chang, D Liu, S Zhang, Z Yao, M Xu, L Bu, 10.1016/j.compbiomed.2024.108709Comput. Biol. Med. 1782024. 108709</p>
<p>The role of large language models in medical image processing: A narrative review. D Tian, S Jiang, L Zhang, X Lu, Y Xu, 10.21037/qims-23-892Quant. Imaging Med. Surg. 142023. 1108</p>
<p>Artificial intelligence-based applications for bone fracture detection using medical images: A systematic review. M Kutbi, 10.3390/diagnostics14171879Diagnostics. 142024. 1879</p>
<p>Medical Vision-Language Model for Consumer Devices. M Ayaz, M Khan, M Saqib, A Khelifi, M Sajjad, A Elsaddik, Medvlm, IEEE Consumer Electronics Magazine; IEEE: Piscataway. NJ, USA2024</p>
<p>Visual-language foundation models in medicine. C Liu, Y Jin, Z Guan, T Li, Y Qin, B Qian, Z Jiang, Y Wu, X Wang, Y F Zheng, 10.1007/s00371-024-03579-wVis. Comput. 412024</p>
<p>A Review of The Opportunities and Challenges with Large Language Models in Radiology: The Road Ahead. N Soni, M Ora, A Agarwal, T Yang, G Bathla, 10.3174/ajnr.A8589Am. J. Neuroradiol. 2024</p>
<p>Medical Informatics Committee of the American College of Physicians. Clinical documentation in the 21st century: Executive summary of a policy position paper from the American College of Physicians. T Kuhn, P Basch, M Barr, T Yackel, 10.7326/M14-2128Ann. Intern. Med. 1622015</p>
<p>A scoping review of using large language models (LLMs) to investigate electronic health records (EHRs). L Li, J Zhou, Z Gao, W Hua, L Fan, H Yu, L Hagen, Y Zhang, T L Assimes, L Hemphill, arXiv:2405.03066</p>
<p>Enhancing Clinical Documentation with AI: Reducing Errors, Improving Interoperability, and Supporting Real-Time Note-Taking. S Saadat, M Khalilizad Darounkolaei, M Qorbani, A Hemmat, S Hariri, 10.61186/ist.202502.01.01Infosci. Trends. 22025</p>
<p>Large Language Models for Chatbot Health Advice Studies: A Systematic Review. B Huo, A Boyle, N Marfo, W Tangamornsuksan, J P Steen, T Mckechnie, Y Lee, J Mayol, S A Antoniou, A J Thirunavukarasu, JAMA Netw. Open. 8e24578792025</p>
<p>A Survey on Security and Privacy of Multimodal LLMs-Connected Healthcare Perspective. M A Rahman, 10.1109/GCWkshps58843.2023.10465035Proceedings of the 2023 IEEE Globecom Workshops (GC Wkshps). the 2023 IEEE Globecom Workshops (GC Wkshps)Kuala Lumpur, Malaysia4-8 December 2023</p>
<p>Toward expert-level medical question answering with large language models. K Singhal, T Tu, J Gottweis, R Sayres, E Wulczyn, M Amin, L Hou, K Clark, S R Pfohl, H Cole-Lewis, 10.1038/s41591-024-03423-7Nat. Med. 312025</p>
<p>Real-World Evaluation of Large Language Models in Healthcare (RWE-LLM): A New Realm of AI Safety &amp; Validation. M Bhimani, A Miller, J D Agnew, M S Ausin, M Raglow-Defranco, H Mangat, M Voisard, M Taylor, S Bierman-Lytle, V Parikh, 10.1101/2025.03.17.253241572025</p>
<p>A framework for human evaluation of large language models in healthcare derived from literature review. npj Digit. T Y C Tam, S Sivarajkumar, S Kapoor, A V Stolyar, K Polanska, K R Mccarthy, H Osterhoudt, X Wu, S Visweswaran, S Fu, 10.1038/s41746-024-01258-7Med. 2024, 7, 258. [CrossRef</p>
<p>Human-centered design and evaluation of AI-empowered clinical decision support systems: A systematic review. L Wang, Z Zhang, D Wang, W Cao, X Zhou, P Zhang, J Liu, X Fan, F Tian, 10.3389/fcomp.2023.1187299Front. Comput. Sci. 2023, 5, 1187299</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, 10.1038/s41591-023-02448-8Nat. Med. 292023. 1930-1940</p>
<p>M Al-Garadi, T Mungle, A Ahmed, A Sarker, Z Miao, M E Matheny, arXiv:2503.04748Large Language Models in Healthcare. arXiv 2025. </p>
<p>On the impact of data heterogeneity in federated learning environments with application to healthcare networks. U Milasheuski, L Barbieri, B C Tedeschini, M Nicoli, S Savazzi, Proceedings of the 2024 IEEE Conference on Artificial Intelligence (CAI). the 2024 IEEE Conference on Artificial Intelligence (CAI)Singapore; Piscataway, NJ, USAJune 2024. 2024IEEE</p>
<p>Challenges and barriers of using large language models (LLM) such as ChatGPT for diagnostic medicine with a focus on digital pathology-A recent scoping review. E Ullah, A Parwani, M M Baig, R Singh, 10.1186/s13000-024-01464-7Diagn. Pathol. 19432024</p>
<p>Evaluating large language models as agents in the clinic. npj Digit. N Mehandru, B Y Miao, E R Almaraz, M Sushil, A J Butte, A Alaa, 10.1038/s41746-024-01083-yMed. 2024, 7, 84. [CrossRef</p>
<p>Explainable AI for healthcare 5.0: Opportunities and challenges. D Saraswat, P Bhattacharya, A Verma, V K Prasad, S Tanwar, G Sharma, P N Bokoro, R Sharma, 10.1109/ACCESS.2022.3197671IEEE Access. 102022</p>
<p>Survey of explainable AI techniques in healthcare. A Chaddad, J Peng, J Xu, A Bouridane, 10.3390/s23020634Sensors. 232023</p>
<p>Application of explainable artificial intelligence for healthcare: A systematic review of the last decade. H W Loh, C P Ooi, S Seoni, P D Barua, F Molinari, U R Acharya, 10.1016/j.cmpb.2022.107161Comput. Methods Programs Biomed. 2261071612011-2022. 2022</p>
<p>From blackbox to explainable AI in healthcare: Existing tools and case studies. P N Srinivasu, N Sandhya, R H Jhaveri, R Raut, 10.1155/2022/8167821Mob. Inf. Syst. 2022, 2022, 8167821</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>
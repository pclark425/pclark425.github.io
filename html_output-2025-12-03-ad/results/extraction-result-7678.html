<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7678 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7678</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7678</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-271227251</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.10210v1.pdf" target="_blank">A Survey on Symbolic Knowledge Distillation of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> This survey article delves into the emerging and critical area of symbolic knowledge distillation in large language models (LLMs). As LLMs such as generative pretrained transformer-3 (GPT-3) and bidirectional encoder representations from transformers (BERT) continue to expand in scale and complexity, the challenge of effectively harnessing their extensive knowledge becomes paramount. This survey concentrates on the process of distilling the intricate, often implicit knowledge contained within these models into a more symbolic, explicit form. This transformation is crucial for enhancing the interpretability, efficiency, and applicability of LLMs. We categorize the existing research based on methodologies and applications, focusing on how symbolic knowledge distillation can be used to improve the transparency and functionality of smaller, more efficient artificial intelligence (AI) models. The survey discusses the core challenges, including maintaining the depth of knowledge in a comprehensible format, and explores the various approaches and techniques that have been developed in this field. We identify gaps in current research and potential opportunities for future advancements. This survey aims to provide a comprehensive overview of symbolic knowledge distillation in LLMs, spotlighting its significance in the progression toward more accessible and efficient AI systems.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Formal mathematics statement curriculum learning <em>(Rating: 2)</em></li>
                <li>Scaling laws for neural language models <em>(Rating: 1)</em></li>
                <li>Language models as knowledge bases <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7678",
    "paper_id": "paper-271227251",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "Formal mathematics statement curriculum learning",
            "rating": 2,
            "sanitized_title": "formal_mathematics_statement_curriculum_learning"
        },
        {
            "paper_title": "Scaling laws for neural language models",
            "rating": 1,
            "sanitized_title": "scaling_laws_for_neural_language_models"
        },
        {
            "paper_title": "Language models as knowledge bases",
            "rating": 1,
            "sanitized_title": "language_models_as_knowledge_bases"
        }
    ],
    "cost": 0.010409,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Symbolic Knowledge Distillation of Large Language Models</p>
<p>Graduate Student Member, IEEEKamal Acharya kamala2@umbc.edu 0000-0002-9712-0265
Department of Information Systems
University of Maryland
Baltimore County
21250BaltimoreMDUSA</p>
<p>Member, IEEEAlvaro Velasquez alvaro.velasquez@colorado.edu 0000-0001-6757-105X
Department of Information Systems
University of Maryland
Baltimore County
21250BaltimoreMDUSA</p>
<p>Department of Computer Science
University of Colorado
80309BoulderCOUSA</p>
<p>Fellow, IEEEHoubing Herbert Song h.song@ieee.org 0000-0003-2631-9223
Department of Information Systems
University of Maryland
Baltimore County
21250BaltimoreMDUSA</p>
<p>A Survey on Symbolic Knowledge Distillation of Large Language Models
975F509F5D41926C94C7944410209A4DLarge Language ModelsSymbolic KnowledgeSymbolic Knowledge Distillation
This survey paper delves into the emerging and critical area of symbolic knowledge distillation in Large Language Models (LLMs).As LLMs like Generative Pre-trained Transformer-3 (GPT-3) and Bidirectional Encoder Representations from Transformers (BERT) continue to expand in scale and complexity, the challenge of effectively harnessing their extensive knowledge becomes paramount.This survey concentrates on the process of distilling the intricate, often implicit knowledge contained within these models into a more symbolic, explicit form.This transformation is crucial for enhancing the interpretability, efficiency, and applicability of LLMs.We categorize the existing research based on methodologies and applications, focusing on how symbolic knowledge distillation can be used to improve the transparency and functionality of smaller, more efficient Artificial Intelligence (AI) models.The survey discusses the core challenges, including maintaining the depth of knowledge in a comprehensible format, and explores the various approaches and techniques that have been developed in this field.We identify gaps in current research and potential opportunities for future advancements.This survey aims to provide a comprehensive overview of symbolic knowledge distillation in LLMs, spotlighting its significance in the progression towards more accessible and efficient AI systems.Impact Statement-There is burgeoning interest in the potential of symbolic knowledge to enhance the interpretability, efficiency, and application scope of LLMs, transforming them into more robust, understandable, and versatile tools.Despite the recognition of its importance, there remains a notable dearth of comprehensive research that thoroughly examines and evaluates the process and implications of this integration.Existing literature predominantly focuses on either the advancements in LLMs or content of the knowledge in the LLMs , with less emphasis on the symbolic knowledge distillation of LLMs.This survey aims to fill this critical gap by offering an extensive review of the current state of symbolic knowledge disitllation in LLMs by highlighting the methodologies, challenges, and advancements in this field.</p>
<p>including websites, research papers, and books, LLMs encapsulate knowledge within their numerous parameters.They can serve as knowledge bases [1], from which information can be extracted and formatted for various purposes, such as finetuning other models for specific tasks [2], validating actions [3], or generating larger and more accurate datasets [4].However, the knowledge embedded in LLMs is not immediately accessible and requires careful extraction and efficient utilization to yield effective results.</p>
<p>The knowledge within LLMs, stored in the weights of their parameters, can be converted into a more interpretable symbolic form through the process of symbolic knowledge distillation.The core challenge here lies in translating the implicit, distributed knowledge encoded in the neural networks of LLMs into explicit, symbolic representations.This transformation is essential for several reasons: to improve the transparency and interpretability of the models, to facilitate knowledge transfer to smaller, more efficient models, and to enable more robust and explainable AI systems.By converting the knowledge into symbolic form, it becomes possible to understand the reasoning behind the model's decisions.This is crucial for applications where understanding the 'why' behind predictions or recommendations is as important as the outcomes themselves.The process is fraught with complexities, including preserving the nuance and depth of the learned knowledge while making it comprehensible and utilizable in a symbolic format.</p>
<p>In this paper, we introduce a detailed framework dedicated to symbolic knowledge distillation of LLMs, initiating our discussion with a historical overview of symbolic knowledge distillation and its evolutionary path to its current state.Following this, we delve into an analysis of various traditional knowledge distillation methods and their comparison with symbolic knowledge distillation approaches.We further explore LLM architectures, including their training and fine-tuning mechanisms.We classify symbolic knowledge distillation techniques into three distinct categories: Direct, Multilevel, and Distillation via Reinforcement Learning.Additionally, we have compiled research papers focused on symbolic knowledge, as well as those specifically addressing symbolic knowledge distillation of LLMs.Our survey provides a thorough examination of the latest developments in symbolic knowledge distillation of LLMs, highlighting the methodologies, challenges, and progress in the field, thereby offering valuable insights for the research community interested in further exploration of this domain.</p>
<p>The rapid expansion of LLMs has led to the production arXiv:2408.10210v1[cs.CL] 12 Jul 2024 of numerous survey papers.All the previous survey papers on LLMs cover different aspects except for the symbolic knowledge.Further exploring we find that no survey paper has been published related to the symbolic knowledge distillation.The focus areas of existing survey papers on LLMs include:</p>
<p>• Comprehensive overviews of LLMs [5], [6], [7] • Evaluation of LLMs [8] • Code generation [9] • LLMs in education [10] • LLM as Knowledge Base [11], [12] • Reasoning Knowledge in LLMs [13] • Explainability in LLMs [14] • Aligning LLMs with human [15] • Instruction tuning for LLM [16] • Model Compression in LLM [17] • Trustworthiness evaluation of LLM [18] • LLM for software engineering [19] • Hallucination in LLM [20] • Multimodal LLM [21] • LLMs for Robotics [22] • LLMs for Information Retrieval [23] Our work stands in contrast to existing approaches in several key aspects.While traditional methods primarily focus on either the performance enhancement of smaller models or the interpretability aspect of knowledge distillation, our framework synergizes these objectives.</p>
<p>The remainder of this paper is structured as follows: Section II reviews the milestones in knowledge distillation and LLM, establishing the context and background for our work.Section III details the preliminaries about symbolic knowledge distillation and LLM, followed by Section IV, which presents a thorough process of symbolic knowledge distillation in LLM.Section V discusses the related research work that has been carried out.In Section VI, we discuss opportunities that have emerged from Symbolic Knowledge Distillation.Section VII is devoted to the challenges of implementing proposed Symbolic knowledge distillation applications.We identify the obstacles and challenges that may arise.Section VIII highlights the Lesson Learned and Key Takeaways and finally, in Section IX, we offer concluding remarks on our survey paper.</p>
<p>II. MILESTONES IN KNOWLEDGE DISTILLATION AND LARGE LANGUAGE MODELS</p>
<p>Over the last seven decades, language technology has advanced significantly.The Turing Test [24], conducted in 1950, was one of the earliest milestones in this field, which laid the foundation for the concept that machines can perform at the level of humans and demonstrate the intelligence.In the same year Shannon used concept of entropy and provided the way of prediction of the next letter when the preceding text is known [25].In 1964, ELIZA [26] was introduced as a Natural Language Processing (NLP) computer program which was designed to mimic the conversational style of a psychotherapist.SHRDLU [27], introduced in 1968, was an early example of an interactive natural language understanding system which can understand and respond to natural language commands related to a simplified world of objects.Following year was the dominance of the Statistical Language Model(SLM).Notable works that lead the way were "Introduction of Stochastic Approach for Parsing" [28] in 1986 and "Statistical Approach to machine translation" [29] in 1990.Due to the problem like Brittleness Across Domains, False Independence Assumption and Shannon-Style Experiments, there was downfall of the SLMs [30].</p>
<p>With the introduction of Long Short-Term Memory(LSTM) [31] in 1997, we entered into the era of Neural Language Model(NLM).These models helped in language processing by capturing the long term dependencies and successfully handling the vanishing gradients.In 2001, the first neural language model was introduced which can be trained using Stochastic Gradient Descent(SGD) algorithm and proved to be computationally efficient and scalable to larger dataset.[32].Neural Networks not only increased in scope and functionality but also in terms of the size [33].The concept of model compression [34] was introduced in 2006.Model compression and acceleration techniques was divided into four different approaches [35]: parameter pruning and sharing [36][37] [38][39] [40], low-rank factorization [41] [42], transferred/compact convolutional layers [43] and knowledge distillation [44].</p>
<p>In 2011, IBM Watson made significant strides in language processing by winning a Jeopardy game against human competitors [45].Two years later, in 2013, the Word2Vec algorithm [46] was introduced, which enabled computers to understand the context of a word and its relationship with other words using dense vector representation where similar words are located close to each other.In 2014, seq2seq [47] was introduced which used encoder to represent variable length input sequence into fixed length vector and decoder to generate output sequence.In the same year, Global Vectors for Word Representation(GloVe) [48] was introduced, which used cooccurance matrix to capture relationship between the words in corpus and was successful in capturing the local and global context informaiton.Knowledge distillation is a model compression technique introduced in 2015 that transfers knowledge from a high-capacity teacher model to a more compact student model.Later in that year FitNets [49] was introduced that add an additional term along with the knowledge distillation loss.In 2016, study [50] instead of utilizing representations from a specific point in the network, employed attention maps as hints, comparing the mean squared error (MSE) between the attention maps of the student and teacher models.In same year, SQuAD (Standford Question Answering Dataset) [51] was introduced, which facilitated the development of questionanswering systems by being benchmark dataset for evaluating machine reading comprehension.</p>
<p>In 2017, the Transformer [52] model was introduced, which enabled the development of advanced language models that can learn relationships between words in a sentence more efficiently by using the concept of self-attention.In the following year, 2017 [53] employed a similar approach.However, instead of utilizing representations or attention maps, they provided hints by using Gram matrices.In 2018, a supplementary module called the paraphraser [54] is incorporated into the model.In same year, ELMo (Embedding from Language Model) [55], context dependent representation of word was introduced which uses different embeddings for same word in different context.Universal Sentence Encoder [56] was also introduced in same year, which further enhanced language processing by introducing embeddings for sentence representations and can handle multiple languages.</p>
<p>General Language Understanding Evaluation(GLUE) [57], a benchmark to evaluate the performance of NLP models on a range of language understanding tasks, became a standard evaluation framework for comparing different language models.Bidirectional Encoder Representations from Transformers(BERT) [58] and Generative Pre-Training-1(GPT-1) [59] were introduced in the same year, 2018 which begin the era of Pre-trained Language Model(PLM).In 2019, GPT-2 [60] became the first language model to touch a billion scale of parameters.Later that year, T5 [61] became the first language model to touch the 10 billion parameter scale.According to [62] published in 2019, the current approach of extracting hints may not be optimal due to the loss of information caused by the ReLU transformation.To address this, they introduced a modified activation function called marginReLU.In [63] published in 2020, the student model learns from the intermediate representations of the teacher model by employing a contrastive loss over these representations.As like the way human way of learning, knowledge distillation was applied in the model; self-learning [64], mutual learning [65], teacher student learning [44], teacher assistant [66] and continual learning [67].Moreover, the application of knowledge distillation extends beyond transferring knowledge between models.It can also be utilized in various other tasks, including adversarial attacks [68], data augmentation [69][70], data privacy and security [71], as well as dataset distillation [72] [73].Between 2010 and 2020, the domain of transfer learning experienced significant expansion, with numerous transfer learning models achieving state-of-the-art results across various disciplines [74].</p>
<p>Google Shard (GShard) [75], introduced in 2020, became the first language model to touch the 100 billion parameter scale.</p>
<p>And in 2021, the Generalist Language Model (GLaM) [76] became the first language model to touch the trillion parameter scale.Concept of symbolic knowledge distillation [2] was introduced in the same year which is a technique for training smaller models using larger models as teachers and involves distilling knowledge symbolically.Since then symbolic knowledge distillation has been used in various areas such as reference free sentence summarization [3], comparative knowledge acquisition [77].The scaling laws for neural language models [78], reveal that model performance improves predictably with increases in model size, dataset size, and computational resources, following a power-law relationship.This means that larger models are significantly more efficient in learning from data.In 2022 and 2023, this trend persisted, with various industry leaders introducing new large-scale language models that leveraged these principles to achieve enhanced performance, demonstrating the continued advancement and efficacy of scaling up model size and computational power in the development of language models.Major technology companies are investing heavily in developing their own LLMs because they recognize the immense potential of these systems to revolutionize various industries, such as healthcare, finance, and customer service.Also, LLMs can help these companies maintain their position as leaders in the field of AI and keep up with competitors.Given the swift advancements in this field, there is a pressing need to steer AI towards paths that prioritize safety and responsibility 1 .</p>
<p>The study [79] concludes that for compute-optimal training, both the model size and the number of training tokens should be scaled equally; specifically, each doubling of the model size should be accompanied by a doubling of the number of training tokens.Conversely, study [80] suggest that the supply of high-quality language data will likely be depleted by 2026.In contrast, low-quality language data and image data are projected to be exhausted between 2030 and 2050 for low-quality language data, and between 2030 and 2060 for image data.The current trajectory of rapidly increasing the parameters of LLMs, which depend on vast datasets, may decelerate unless there are significant improvements in data efficiency or new data sources are discovered.These findings have influenced the development of next-generation LLMs towards models capable of generating their own training data for self-improvement.Furthermore, LLMs will need to incorporate self-fact-checking capabilities.These scenarios underscore the importance of symbolic knowledge distillation and suggest a potential shift of LLMs towards this approach.</p>
<p>It has been utilized for labeling [81][82], where the teacher model generates outputs based on the provided input, and for expansion [83] [84], where the teacher model produces samples akin to given demonstrations through in-context learning.For data generation [85] which involves synthesizing data according to specific meta-information, such as a topic or entity, feedback [86] which involves providing guidance on the student's outputs, encompassing preferences, corrections, and expansions of challenging samples.Finally, for self-checking [87] which entails the student model generating outputs, which are subsequently filtered for high quality or self-evaluated by the student model.</p>
<p>III. BACKGROUND AND PRELIMINARIES</p>
<p>For understanding the process of symbolic knowledge distillation of LLMs, we need to dive deeper into the two different technical theory of knowledge distillation followed by LLMs.Following sub-section will focus on that part.</p>
<p>A. Knowledge Distillation</p>
<p>Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) with the goal of retaining much of the teacher model's performance [117].This process is crucial in scenarios where computational resources are limited or where deployment requires lightweight models.There are various types of traditional knowledge distillation techniques: response-based, feature-based and relation-based and one modern symbolic knowledge distillation, each with its unique approach and area of application:</p>
<p>1) Response-based Knowledge Distillation: Responsebased knowledge distillation involves transferring knowledge from the teacher model's final output layer to the student model, aiming to mimic the teacher's final predictions.This approach is straightforward and has proven effective across various tasks, employing a loss function based on the divergence between the teacher's and student's logits.It's widely applied in model compression and has been adapted for different types of model predictions, including object detection and human pose estimation, where the teacher's output may include additional information like bounding box offsets [118] or heatmaps for landmarks [119].A key application of responsebased knowledge distillation is in image classification [44], where "soft targets" -the probabilities assigned to each class by the teacher model -play a crucial role.These probabilities are adjusted using a temperature factor to control the softness of the targets, allowing the transfer of knowledge from the teacher to the student.The distillation process typically employs the Kullback-Leibler divergence loss to optimize the similarity between the teacher's and student's probability distributions.</p>
<p>This method is praised for its simplicity and effectiveness, particularly in leveraging knowledge for training.However, its reliance on the final layer's output means it may not fully utilize intermediate-level supervision from the teacher, an aspect crucial for representation learning in deep neural networks.</p>
<p>2) Feature-based Knowledge Distillation: Feature-based knowledge distillation taps into the strength of deep neural networks to learn hierarchical feature representations, a process central to representation learning [120].Unlike responsebased knowledge distillation, which focuses on the outputs of the last layer, feature-based distillation utilizes the outputs from intermediate layers, or feature maps, to guide the student model.This approach is particularly beneficial for training models that are both narrower and deeper, as it provides a richer set of training signals.</p>
<p>The concept was first introduced with Fitnets [49], aiming to improve student model training by matching feature activations between the teacher and student directly.Following this, several methodologies have been developed to facilitate this matching process, either directly or indirectly [121].Notable contributions include the derivation of "attention maps" to express the use of neuron selectivity transfer [122], matching probability distributions in feature space [123], and introducing "factors" for more interpretable intermediate representations [54].Techniques like route constrained hint learning [124] and the use of activation boundaries [125] have been proposed to minimize the performance gap between teacher and student models, alongside innovative strategies like cross-layer knowledge distillation [121] which adaptively matches teacher and student layers.</p>
<p>Despite the effectiveness of feature-based knowledge transfer in enriching the student model's learning, challenges remain in selecting appropriate layers for hints and guidance due to the size discrepancies between teacher and student models.This necessitates further exploration into how best to match the feature representations between teacher and student models effectively.</p>
<p>3) Relation-based Knowledge Distillation: Relation-based knowledge distillation goes beyond the scope of responsebased and feature-based methods by examining the relationships between different layers or data samples within the teacher model.This approach delves into the dynamics between feature maps, layers, and even the relationships between different teachers or data samples, offering a more nuanced form of knowledge transfer.</p>
<p>Flow of solution process (FSP) [53] utilizes the Gram matrix between two layers to encapsulate the relationships between pairs of feature maps through inner product calculations.Knowledge distillation via singular value decomposition [126] distill essential information from these relationships.[127] explored multi-teacher scenarios by constructing graphs based on logits and features from each teacher, modeling their importance and relationships.[128] proposed a multi-head graph-based distillation technique that leverages intra-data relations between feature maps through a multi-head attention network.[129] focused on pairwise hint information, allowing the student model to mimic mutual information flows from pairs of hint layers in the teacher model.</p>
<p>The distillation loss in relation-based knowledge distillation is formulated based on the similarity and correlation functions between the feature representations of teacher and student models, aiming to capture and transfer the intricate relationships present in the teacher's architecture.Relationbased knowledge can also encompass structured knowledge of data, privileged information about input features, and various other categories, each represented by different loss functions like Earth Mover distance, Huber loss, Angle-wise loss, and Frobenius norm.While recent advancements have introduced several types of relation-based knowledge, the challenge remains in effectively modeling the relational information from feature maps or data samples for knowledge transfer.This area continues to be ripe for further research and exploration to enhance the efficacy of knowledge distillation techniques.4) Symbolic Knowledge Distillation: Contrary to the methods discussed earlier, symbolic knowledge distillation is centered on the distillation and transmission of knowledge in a symbolic format, including rules, logic, or symbolic representations.This method integrates structured knowledge bases and rules with machine learning models to boost their performance and clarity.It encodes intricate, structured information in a manner that allows for manipulation in reasoning, inference, and decision-making processes.The importance of this approach lies in its alignment with human methods of interpreting and reasoning with knowledge, thus providing enhanced transparency and interpretability.</p>
<p>Symbolic knowledge distillation represents a technique within machine learning where knowledge is extracted from a complex, typically less transparent model (like a deep neural network) and converted into a symbolic, more understandable format.This methodology merges the principles of conventional knowledge distillation with those of symbolic AI, aiming to improve the interpretability, transparency, and possibly the efficiency of machine learning models.It serves as a bridge between the often "black box" nature of deep learning models and the necessity for models that can be comprehended and trusted by humans.Such a requirement is especially critical in sectors demanding high levels of responsibility and explainability, including healthcare, finance, and autonomous driving.Although the specific mathematical model employed may vary based on the approach and the symbolic representation chosen, the overall process typically includes several defined steps.</p>
<p>Training the Teacher Model: A complex model (teacher) is trained on a dataset to achieve high performance.This model can be a deep neural network, and its architecture and training process depend on the specific task (e.g., image recognition, NLP).</p>
<p>Extracting Knowledge: The subsequent phase involves deriving insights from the teacher model, achievable through multiple approaches, including: examining the neuron activation patterns within the network; employing methods like Layer-wise Relevance Propagation (LRP) [130] or SHapley Additive exPlanations(SHAP) [131] to assess the significance of various inputs in the network's decision-making process; and identifying rules or patterns based on the decision boundaries established by the network.</p>
<p>Symbolic Representation: The gathered knowledge is subsequently converted into a symbolic representation.This process includes: developing decision trees or compiling sets of logical rules that mimic the neural network's behavior, and utilizing graphical models or alternative structured forms to Training the Student Model: Following the translation of extracted knowledge into a symbolic form, a simpler and more interpretable 'student' model is trained to mimic this symbolic representation.The training process involves two key strategies.The symbolic representation may be used directly as a comprehensive set of rules for decision-making, allowing the student model to replicate decision processes based on predefined logical rules or the student model is trained to approximate the symbolic representation itself.This approach often incorporates conventional supervised learning techniques, with the significant distinction that the symbolic knowledge extracted from the teacher model acts as a guide or target.</p>
<p>Evaluation and Refinement: Once the student model has been trained to mimic the symbolic representation, it under-goes evaluation to verify that it retains the critical knowledge and performance attributes of the teacher model.This assessment might reveal the need for adjustments either to the symbolic representation itself or to the training methodology of the student model.Such refinements are crucial for ensuring that the student not only approximates the teacher's performance but does so in a way that is both interpretable and transparent.This emphasis on interpretability and transparency is key, as it aims to produce a student model that not only performs well but also provides insights into its decision-making processes, making it more understandable and trustworthy to users.</p>
<p>B. Large Language Models</p>
<p>LLMs are the foundation model for the language and has been the hot topic for past few years.Alot of opportunities has been created in one hand and due to ineffective use, it has also created some kind of fear among the users.In this section we will focus on the architecture of LLM followed by the training process.</p>
<p>1) Architecture: Transformer [52] architecture is the backbone of all the LLMs.Due to its features like parallelizable computation, attention based mechanism it has been able to reduced reliance in hand-crafted features and also improved the performance in NLP tasks.All the LLMs are directly or in-directly has the root the in the transformer architecture.Existing all the LLMs can be found to be belonging into one of the following architecture:</p>
<p>Encoder-Decoder Architecture: The underlying principle of this architecture involves transforming the input sequence into a fixed-length vector form, and subsequently, transforming this representation into the output sequence.The architecture is composed of two sets of Transformer blocks: one serving as the encoder and the other as the decoder.The encoder is tasked with processing the input sequence, utilizing a series of multi-head self-attention layers to convert it into latent representations.These representations are then leveraged by the decoder, which, through an autoregressive process, generates the output sequence by employing cross-attention mechanisms to focus on the latent representations provided by the encoder.PLM like T5 [61], BART [132] and Flan-T5 [94] uses this architecture.</p>
<p>Casual Decoder Architecture: The causal decoder architecture is a type of decoder-only architecture used in language modeling, where the input and output tokens are processed in the same fashion through the decoder.This architecture incorporates a unidirectional attention mask, which ensures that each input token can only attend to past tokens and itself by masking all future attentions to zeros.The GPTseries models, including GPT-1 [59], GPT-2 [60], and GPT-3 [96], are representative language models of this architecture.Many other LLMs, such as OPT [108], BLOOM [133], and Gopher [104], have also adopted the causal decoder architecture.</p>
<p>Prefix Decoder Architecture: The prefix decoder architecture, also known as a non-causal decoder, is another type of decoder-only architecture which revises the masking mechanism of causal decoders to enable bidirectional attention over  Pre-trainning:Pre-training LLMs involves training on extensive unlabeled text datasets to learn general language patterns and insights.The success of pre-training hinges on both the scale and quality of the training corpus, with large, diverse datasets allowing models to capture a wide array of language patterns and generalize effectively to new data.</p>
<p>The pre-training process unfolds in phases, starting with data collection, which is divided into general and specialized data sources.General data encompasses a wide range of text, including webpages, conversations, Q&amp;A portals, and books, while specialized data targets more niche content like research papers, code, and multilingual texts.The second phase, data pre-processing, focuses on refining the dataset by eliminating noisy, redundant, and irrelevant content.Techniques employed include quality filtering, deduplication (at sentence, document, and dataset levels), privacy protection (removing personal information), and tokenization (splitting text into manageable units for the model).Given that LLMs are not typically retrained frequently, the pre-training phase must be approached with precision, prioritizing a balanced mix of source materials [104], and ensuring both the quantity [110] and quality [136] of the data are optimal.Pre-training tasks may involve language modeling [95], favored by decoder-only architectures for predicting subsequent tokens, or de-noising autoencoding [132], which focuses on correcting or replacing corrupted tokens.</p>
<p>Fine tuning or Adaptive tuning: The fine-tuning stage is crucial for adapting pre-trained LLMs to specific domains or tasks, leveraging labeled examples or reinforcement learning to refine the model's understanding and predictive capabilities.It encompasses two main strategies: instruction tuning and alignment tuning.</p>
<p>Instruction tuning entails the fine-tuning of a language model by incorporating explicit instructions or demonstrations during training.This approach is designed to direct the model towards desired behaviors and outcomes, facilitating a more targeted response to tasks.The instructions for this tuning can be derived from existing datasets reformatted to include clear directives or crafted to reflect specific human needs.Alignment tuning, on the other hand, aims to adjust the LLM's outputs to match human expectations accurately, a process that may involve a trade-off known as the alignment tax [106].This concept refers to potential compromises in the model's capabilities as it is fine-tuned to prioritize outputs that are deemed more acceptable or beneficial from a human perspective.The most commonly used alignment criterias are helpfulness, honesty, and harmlessness [106] [99].Few other criteria are also mentioned like behavior, intent, incentive, and inner aspects [137].</p>
<p>IV. SYMBOLIC KNOWLEDGE DISTILLATION OF LARGE LANGUAGE MODELS</p>
<p>Symbolic Knowledge Distillation of LLMs aimed at distilling the extensive knowledge encapsulated within LLMs into more interpretable and efficient forms.It's central methodology revolves around transforming the latent knowledge of models like GPT-3 into symbolic or rule-based representations.It involves a sophisticated process designed to transform the latent, complex knowledge within these models into explicit, structured, and interpretable forms.This process begins with the careful crafting of customised prompts that guide LLMs to generate outputs rich in specific knowledge types.Following this, NLP techniques like Named Entity Recognition (NER), Part-Of-Speech (POS) tagging, and dependency parsing, are employed to analyze and structure the responses.This step extract meaningful information and identify patterns within the text, which are then transformed into structured knowledge formats such as logic rules, knowledge graphs, or semantic frames.It derives explicit rules and patterns from the LLMs' responses, thereby facilitating the encoding of this information into symbolic representations that can be easily understood and manipulated.</p>
<p>The subsequent phase of this process involves the refinement and validation of the generated symbolic representations to preserve depth of knowledge and to ensure their accuracy, consistency, and practical utility.This includes refining the symbolic knowledge using the human experts or using the trained models to classify the generated knowledge on the basis of quality.The refined symbolic knowledge base undergoes validation against established benchmarks, allowing for the assessment of enhancements and ensuring the symbolic representations meet the required standards of quality and utility.</p>
<p>The creation of a high-quality knowledge base facilitates the training of smaller models, demonstrating that a quality dataset can significantly improve the performance of models that are 100 times smaller than their teacher counterparts [2].This highlights the efficacy of integrating symbolic knowledge into language models, presenting a viable alternative to scaling up LLMs.Symbolic knowledge distillation generates smaller, yet more efficient models, making them suitable for deployment in everyday practical applications, offering a more resourceefficient pathway to achieving high-quality outputs in language models.</p>
<p>Various approaches that are used to distill the symbolic knowledge of LLMs can be categorised as:</p>
<p>A. Direct Distillation</p>
<p>The distillation of symbolic knowledge from LLMs like GPT-3 begins with the construction of a specific prompt.This prompt is designed to elicit responses that encapsulate commonsense or factual understanding.It could involve scenarios, questions, or statements that require the application of general knowledge about the world.The effectiveness of this step hinges on the ability to craft prompts that are both clear and contextually rich enough to guide the LLM towards producing relevant and insightful outputs.Upon receiving the prompt, the LLM generates a response based on its training and the intricacies of the provided context.These models, have been exposed to extensive and varied textual data, encompassing a wide array of commonsense situations and factual knowledge.This extensive training enables them to generate responses that are not only contextually appropriate but also rich in commonsense and factual knowledge.The model's response is a complex interplay of its learned patterns, linguistic understanding, and the implicit knowledge embedded within its training corpus.This step translates the implicit knowledge within the model into explicit textual responses that can be further analyzed and utilized for knowledge extraction.</p>
<p>The generated text is then analyzed to extract knowledge.This can be in the form of statements, inferences, or relationships that are implicitly or explicitly expressed in the text.The extraction process might involve additional processing steps like parsing the text to identify relevant information or using templates to format the knowledge in a structured way.The knowledge base derived from this process can be further improved with the assistance of critics, who may be human evaluators providing feedback on the quality and acceptability of the generated content.Once a substantial volume of high-quality generated data has been accumulated, this data can be utilized to train a critic model like RoBERTa, which can be used to evaluate the generated text for accuracy, relevance, and coherence.The critic model can filter out lowerquality outputs, ensuring that only high-quality commonsense knowledge is retained.The high-quality knowledge can then be distilled into structured formats like knowledge graphs or further trained into specialized models.This process involves organizing the knowledge in a way that can be easily utilized by other systems or models.</p>
<p>B. Multilevel distillation of symbolic knowledge</p>
<p>This approach iteratively refines the knowledge transfer from a larger, pre-trained teacher model to a smaller, more efficient student model.The process begins with the teacher model, typically a LLM like GPT-3, generating initial knowledge base.The generated knowledge base is then filtered for quality, focusing on aspects like accuracy and length.The smaller student model, such as GPT2-Large, is initially trained on this filtered dataset.Subsequently, the student model generates new knowledge base, which are again filtered to enhance quality.This cycle of generation and refining through filtering is repeated iteratively, with each iteration aiming to improve fidelity and succinctness of the distilled knowledge.</p>
<p>During each iteration, various filters are applied to ensure the quality which are fidelity filter, length filter or contextual filter.The Fidelity Filter ensures a true representation of the input sentence, verified using an off-the-shelf Natural Language Inference (NLI) model.The Length Filter controls the length to fit within a predefined compression ratio, gradually guiding the model to produce increasingly concise output.A Contextual Filter is used in some cases, focusing on the coherence in the larger context of the text.The process results in the development of increasingly efficient student models that inherit the distillation ability of the teacher model but with enhanced control over quality.This method allows for the creation of high-quality, succinct dataset with diverse compression ratios, without relying on pre-existing annotated datasets.</p>
<p>C. Distillation using Reinforcement Learning policy</p>
<p>The approach refines the policy of a LLM through a twostep iterative process: generating and filtering data.The first step, involves using the current LLM policy to generate a range of output predictions for given contexts, effectively augmenting the training dataset.Initially, this policy might be based on a supervised learning model, and the generated outputs may not be perfectly aligned with human preferences.However, this step is essential for creating a diverse set of potential outputs for further refinement.The generated data forms the basis for the next critical phase of the process.</p>
<p>In the second step, the data produced is ranked and filtered using a filters like scoring function, typically a learned reward model trained on human preferences.This step is pivotal in selecting the best outputs that align with the desired human outcomes, as determined by the scores from the reward model.The filtering threshold can be incrementally increased in subsequent iterations, ensuring that only the top-performing outputs are selected for further training.The language model is then fine-tuned on this curated dataset with an offline RL objective, adjusting its policy to produce outputs that are more likely to receive high scores.This process of generating and filtering, repeated iteratively, serves as a feedback loop, continuously refining the model's policy towards outputs increasingly aligned with human preferences.</p>
<p>All three techniques mentioned have been successfully applied to various research areas, including commonsense reasoning [2], translation [4], summarisation [3] , and mathematical reasoning [138], among others, yielding significant results.F ig.7 provides an overview of all the areas explored so far, with detailed discussions presented in the related works section.T able.III offers insights into each research area, categorizing them based on the techniques discussed above.</p>
<p>V. RELATED WORKS</p>
<p>In this segment, we begin by exploring the foundational work that positions LLMs as a knowledge base and then delve into research focused on analyzing the knowledge contained within LLMs.Lastly, we review efforts aimed at distilling this knowledge into a symbolic form.An overview of this concept is presented in F ig.7.</p>
<p>A. Knowledge Base of LLM</p>
<p>LLM can act as a knowledge base or oracle that performs well on open-domain question answering without fine-Fig.6. Overview of Distillation process using RL tuning [1].LLM can also function as the domain-specific KBs in biomedical field however they are highly influenced by prompt bias and synonym variance [139].It rapidly and stably acquires linguistic knowledge, including syntax, grammar, and parts of speech, predominantly in the early stages of pretraining, showing little variation across different domains.In contrast, the assimilation of factual and commonsense knowledge is slower, more sensitive to the domain of the training data, and exhibits a more gradual progression throughout the pre-training period [140].</p>
<p>B. Consistency of Knowledge In LM</p>
<p>The research [141] sheds light on the consistency of knowledge in PLMs like BERT and RoBERTa.Their findings reveal a concerning lack of consistency in these models, particularly when responding to paraphrased queries with factual content.The study [142] adds another layer of complexity to this issue by highlighting the challenges PLMs face in accurately processing negated facts and their susceptibility to being misled by contextually irrelevant or misleading information.</p>
<p>C. Editing the Knowledge in LLM</p>
<p>Editing knowledge in LLMs has become a prominent area of research with several innovative approaches proposed to address this challenge.Constrained layer-wise finetuning [143] formulates knowledge modification as a constrained optimization problem and allows for fine-tuning specific layers to update knowledge while retaining existing information.[144] introduced the concept of Knowledge Neurons, enabling pinpointing specific components responsible for factual knowledge within LLMs and providing the means to manipulate them for altering model output.The KNOWLEDGEEDITOR [145] offers an efficient way to update factual knowledge in pre-trained LLMs without extensive retraining.The paper [146] introduces methods for detecting, updating, and visualizing beliefs in LLM by using the Sequential Local and Generalizing (SLAG) update objective.Model Editor Networks with Gradient Decomposition (MEND) [147] efficiently edit large-scale pre-trained models by transforming gradients during fine-tuning.Continual Knowledge Learning (CKL) [148] addresses the challenge of updating and maintaining the relevancy of world knowledge in LLMs.</p>
<p>D. Reasoning with Knowledge in LLM</p>
<p>The research landscape concerning reasoning abilities in PLMs and transformers, has seen significant exploration and development.The paper [149] found that while BERT could learn simpler one-hop rules, it struggled with more complex two-hop rules and distinguishing between symmetric and nonsymmetric relations.[150] demonstrates that transformers can effectively emulate reasoning over language, achieving high accuracy on various synthetic datasets that require different depths of inference and can act as limited "soft theorem provers".PROVER [151] extended [150] to answer binary questions over rule-bases while generating corresponding proofs for enhanced interpretability.ProofWriter [152] stands out for its ability to produce implications and corresponding natural language proofs from given theories, using the T5 transformer architecture.The paper [153] explores the capability of Transformer Language Models (TLMs) in logical reasoning with natural language focusing on first-order logic proofs.The paper [154] explore the capacity of transformer models to perform deductive reasoning on logical theories expressed in natural language by introducing a method for generating challenging reasoning datasets whereas the paper [155] enhance the deductive reasoning abilities of PLMs using soft Horn rules and achieved high performance on unseen logical rules and showed improved understanding of logical properties like negation and symmetry.The paper [156] introduces a novel dataset to evaluate the mathematical reasoning capabilities of neural networks, focusing on problems across arithmetic, algebra, probability, and calculus.</p>
<p>The paper [157] integrates commonsense reasoning on natural language question-answering tasks by employing smaller language models,and demonstrate competitive performance against large PLMs.RICA (Robust Inference using Commonsense Axioms) [158],found that PLMs are vulnerable to perturbation attacks, where minor changes in input data drastically alter their conclusions.The paper [159] presents the Common Sense Explanations (CoS-E) dataset and the Commonsense Auto-Generated Explanation (CAGE) framework, which leverages natural language explanations(human-like explanations) to improve model's reasoning capabilities.</p>
<p>E. Interpreting the Knowledge of LLM</p>
<p>Interpreting the knowledge encoded in LLMs has been advanced through various studies, each contributing unique insights into how these models capture and process linguistic information.[160] argue that attention weights often don't align with other feature importance measures and can produce similar predictions despite different attention distributions.This view is nuanced by [161], who suggest that attention can serve as an explanation, but its validity depends on the context and testing methods.[162] also investigate attention in text classification, finding that while there is some correlation between attention weights and model predictions, attention weights alone are not definitive indicators of input importance and propose that gradient-based attention weight rankings provide a deeper understanding.</p>
<p>The study [163] include method for quantifying non-linearity in transformers, particularly in feed-forward networks.They reveal a non-distinct feature extraction process in BERT layers, influenced by skip connections.[164] demonstrate that transformer layers function as key-value memories, capturing textual patterns and inducing distributions over the output vocabulary, with lower layers focusing on shallow patterns and upper layers on semantic ones.[165] show that factual associations in GPT models are tied to localized computations, particularly in middle-layer feed-forward modules.</p>
<p>F. Explainability in LLM</p>
<p>The study [166] investigates the application of Influence Functions (IFs) to identify artifacts in models, comparing their effectiveness with that of common word-saliency methods.Researchers in study [167] compare IFs with simpler retrievalbased methods and suggest that despite the complexity of IFs, simpler methods can achieve comparable performance.Exploring further in study [168], they introduce Trainingfeature attribution (TFA), which synergizes saliency maps and instance attribution to effectively uncover artifacts.Researcher in [169] propose Human In the Loop Debugging using Influence Functions (HILDIF), a pipeline that employs influence functions for debugging deep text classifiers, allowing human involvement in enhancing model performance.</p>
<p>In a different approach, study [170] presents a novel method for training language models to generate natural text explanations alongside their predictions, utilizing the text-to-text framework [61].Addressing the challenge of inconsistency in natural language explanations, [171] introduces an adversarial framework to identify and measure these inconsistencies.The Proto-Trex model [172] uses prototypical examples to explain model predictions, thus mitigating the opacity often associated with complex models.Research [173] enhances interpretability by extracting key text segments, termed "rationales", serving as justifications for model predictions.Study [174] works on improving commonsense reasoning by employing contrastive explanations generated through specialized prompts, aligning model reasoning more closely with human cognitive patterns.</p>
<p>G. Symbolic Knowledge Distillation</p>
<p>The conducted research works in this area can be categorised as follows:</p>
<p>1) Commonsense Knowledge: The study [2] introduces a transformative shift in the conventional practice, transitioning from the traditional 'from-human-to-corpus-to-machine' approach to an innovative 'from-machine-to-corpus-to-machine' paradigm through the introduction of symbolic knowledge distillation.In their research, the authors not only succeed in creating a substantially larger common-sense dataset from ATOMIC resource [175], approximately ten times larger than previously manually synthesized datasets, but also enhance its diversity and quality.Their novel approach involves training the common-sense model using this newly generated knowledge graph.Despite being only 1/100th of its predecessor model, it outperforms the previous model, showcasing the effectiveness of their approach.The paper [176] introduces NOVACOMET, an innovative open commonsense knowledge model that merges the strengths of both knowledge and general task models.This model, built upon symbolic knowledge distilled from proprietary models like GPT-3, creates an auditable discrete knowledge graph, NOVATOMIC, which facilitates open-format training and application to a wide array of reasoning tasks.It demonstrates superior performance in commonsense reasoning, outperforming comparable models in various benchmarks.The model's training involves novel techniques like commonsense field masking for enhanced flexibility in knowledge handling.Iterative Imitation and Decoding for Distillation(I2D2) [177] framework employs a fourstage process that includes prompt construction, constrained decoding using NeuroLogic Decoding, critic filtering, and selfimitation learning, where the model is iteratively refined based on its own high-quality outputs.A new corpus, Gen-A-tomic, was created to provide diverse and accurate commonsense knowledge.I2D2 demonstrated superior performance in accuracy and precision over larger models like GPT-3, with GPT-2 XL showing significant improvements through self-imitation learning iterations.</p>
<p>2) Translation: Reinforced Self-Training (ReST) [4] is a method to align LLMs with human preferences in the realm of machine translation.This approach incorporates reinforcement learning from human feedback (RLHF) to enhance the output quality.ReST initiates by generating a dataset through sampling from the initial LLM policy, followed by the application of offline reinforcement learning algorithms to refine the policy.This method is identified as more efficient than traditional online RLHF techniques, primarily because it facilitates the creation of the training dataset in an offline manner, promoting the reuse of data.The effectiveness of ReST is demonstrated through significant improvements in translation quality, validated by both automated metrics and human evaluations across various machine translation benchmarks.</p>
<p>3) Summarisation: REFEREE [3] is a framework for reference-free sentence summarization that allows for direct control of compression ratio.It uses Symbolic Knowledge Distillation to distill latent knowledge from PLMs, resulting in smaller but better summarizers with sharper controllability.The framework employs iterative distillation of knowledge, where student models from previous iterations serve as teacher models in the next iteration.This iterative process also generates a high-quality dataset of sentence-summary pairs with varying compression ratios.The final student models outperform the larger GPT3-Instruct model in terms of compression ratio controllability without compromising the quality of the summarization.</p>
<p>4) Mathematical Proof and Reasoning:</p>
<p>The paper [138] presents a method called expert iteration, which combines proof search with learning to improve language modeling in formal mathematics.The method involves finding new original proofs for the same statements and closing marginally harder statements at each iteration, which in turn provides more useful training data for the next iteration.By interleaving proof search with learning, expert iteration is able to dramatically outperform proof search only.The paper demonstrates the effectiveness of expert iteration on a manually curated set of problem statements and achieves state-of-the-art results on the miniF2F benchmark, a set of formalized statements of mathematical problems from various competitions.The paper [178] explores the concept of distilling abilities from LLMs into smaller ones, specifically for enhancing their performance in multistep math reasoning tasks.The process begins with generating a dataset using a larger model (like GPT-3.5)employing chainof-thought reasoning, where the model details the steps leading to a solution.This dataset is then used to fine-tune a smaller T5 model, with the aim of specializing its abilities in the specific area of multi-step reasoning.This fine-tuning process allows the smaller model to learn the complex reasoning patterns demonstrated by the larger model.</p>
<p>5) Visual</p>
<p>Commonsense: Localized Symbolic Knowledge Distillation (LSKD) [179] enhances vision-language models by focusing on localized regions within images.This method addresses a significant limitation in existing models, which interpret images as a whole, by introducing Localized Visual Commonsense models that can specify and reason about multiple distinct regions in an image.The authors develop a scalable framework for generating localized visual commonsense statements and establish the Localized Commonsense Knowledge Corpus, which aids in expanding the capabilities of vision+language models to include references-as-input.The paper highlights the state-of-the-art zero-shot performance of these models on three localized visual reasoning tasks and showcases the superiority of the student model over the teacher model through human evaluation.</p>
<p>6) Instruction Generation: Traditional instruction-tuned models, reliant on human-written instruction data, often lack diversity and creativity, constraining the generality of the model.SELF-INSTRUCT [180] mitigates this by enabling models to generate their own instructions, inputs, and outputs, which are then used for fine-tuning.This process involves generating task instructions, classifying them, creating instances via input-first or output-first approaches, and filtering out low-quality data.The approach significantly reduces the need for human-labeled data, fostering a broader and more creative instructional capability in LMs.The performance evaluation shows that the GPT3SELF-INST model, finetuned on this self-generated data, substantially outperforms the vanilla GPT-3 in instruction-following tasks and closely matches the performance of models like InstructGPT001.Alpaca [181] enhance the SELF-INSTRUCT data generation pipeline by employing the more advanced text-davinci-003 model for instruction data generation that explicitly defines the requirements for instruction generation, aiming for more focused and relevant outputs.The adoption of aggressive batch decoding, producing 20 instructions simultaneously, significantly reduces data generation costs and simplifying the pipeline by eliminating the distinction between classification and non-classification instructions and generating only a single instance per instruction, instead of 2 to 3, streamlines the process.Evol-Instruct [182] is a novel method that uses LLMs to automatically generate a vast array of complex instructional data.This approach begins with simple initial instructions and employs the LLM to evolve these into more sophisticated and diverse instructions through in-depth and in-breadth evolution processes.It enhances instructions by adding constraints, increasing reasoning complexity, and diversifying topics, thus creating a rich dataset for fine-tuning LLMs.This dataset is used to train the LLaMA model, resulting in WizardLM, a model demonstrating superior performance in following complex instructions compared to human-generated datasets and existing models like ChatGPT.</p>
<p>7) Handling queries: Vicuna-13B [183] is an open-source chatbot developed by fine-tuning the LLaMA model with around 70,000 user-shared ChatGPT conversations from ShareGPT.It demonstrates superior performance, achieving over 90% of ChatGPT's quality, and surpassing other models like LLaMA and Stanford Alpaca.The training, which cost approximately $300, utilized advanced techniques for handling multi-turn conversations.Despite its advancements, Vicuna-13B shares common LLM limitations, such as challenges in reasoning or math tasks, and has potential issues with factual accuracy and safety.Koala [184], a chatbot model developed by fine-tuning Meta's LLaMA with web-sourced dialogue data, including interactions with large models like ChatGPT.Koala demonstrates competitive performance against established models such as ChatGPT and Stanford's Alpaca, particularly in handling real user queries.ASK ME ANYTHING PROMPTING (AMA) [185] is a prompting method for improving the performance of LLMs like GPT-3.AMA leverages multiple effective but imperfect prompts, aggregating them using weak supervision to enhance prediction quality.This method primarily utilizes open-ended question-answering formats, which are found to be more effective than restrictive prompts.AMA's recursive use of the LLM to transform task inputs into these formats, combined with the aggregation of diverse prompts, demonstrates significant improvements in LLM predictions.QAMELEON [186] is an innovative approach to multilingual question answering (QA) systems, leveraging PLMs within a few-shot learning framework.PLMs generate QA pairs in multiple languages, significantly reducing the need for extensive, language-specific training datasets.By requiring only a minimal number of examples (as few as five per language), QAMELEON efficiently fine-tunes QA models, overcoming traditional constraints of resource-intensive data annotation.This approach not only simplifies and accelerates the development of multilingual QA systems but also achieves superior accuracy and efficiency, demonstrating its potential as a scalable and effective solution in NLP.</p>
<p>8) Labeling Data: The research paper [81] examines the efficacy of using GPT-3 for data labeling in NLP tasks, highlighting its cost-effectiveness compared to traditional human labeling.The study reveals that GPT-3 can reduce labeling costs by 50% to 96% across various tasks, including sentiment analysis, text classification, and summarization.The paper introduces a novel framework that combines GPT-3 generated pseudo labels with human labels, improving performance under limited budgets.Furthermore, an active labeling strategy is explored, where low-confidence labels by GPT-3 are reannotated by humans, enhancing label quality.Despite these benefits, the paper notes that GPT-3 is more suited for lowstakes labeling tasks, as its reliability in high-stakes scenarios remains limited.The research [82] presents a novel method for utilizing PLMs in tasks with scarce labeled training data.This technique involves prompting the LM with multiple queries about an example, and the model's responses are then interpreted as votes for specific labels or as abstentions.This process, integrated within a weak supervision framework, leverages the capabilities of the LM as a labeling function.The Snorkel system is subsequently employed to clean and refine these noisy label sources, culminating in the creation of enhanced training data for an end classifier.9) Task Specific Small Models: The method, "Distilling step-by-step" [187], involves extracting rationales from LLMs alongside output labels.These rationales, serving as detailed explanations for model predictions, are then used in a multi-task learning framework to train smaller models on both label and rationale prediction tasks.This technique significantly reduces the data and model size required, enabling smaller models to surpass the performance of LLMs more efficiently.The paper demonstrates the effectiveness of this approach across multiple datasets and tasks, showcasing it as a resourceefficient alternative to standard finetuning and traditional distillation methods.</p>
<p>10) Complex Reasoning: Orca [188] is designed to enhance the capabilities of smaller models through imitation learning from large foundation models (LFMs).Traditional methods faced issues like limited imitation signals, smallscale homogeneous training data, and inadequate evaluation, leading to an overestimation of the small models' capabilities.These models often imitated the style but not the reasoning process of LFMs.Orca addresses these challenges by learning from GPT-4's rich signals, including explanation traces, stepby-step thought processes, and complex instructions, with guidance from ChatGPT as a teacher.This approach enables progressive learning through large-scale and diverse imitation data.Orca significantly outperforms state-of-the-art instruction-tuned models like Vicuna-13B in complex zeroshot reasoning benchmarks, achieving more than a 100% improvement in Big-Bench Hard (BBH) and a 42% improvement in AGIEval.Orca reaches parity with ChatGPT in BBH and exhibits competitive performance in professional and academic exams like the SAT, LSAT, GRE, and GMAT, in zero-shot settings without Chain of Thought (CoT), though it still trails behind GPT-4.Orca 2 [189] builds upon the Orca project, focusing on enhancing smaller LMs' reasoning abilities.Orca 2 continues exploration, particularly addressing the limitations of imitation learning, which had been the primary method for training small LMs.This method, while effective in replicating the output of larger models, often fell short in reasoning and comprehension skills.It introduces various reasoning techniques (e.g., step-by-step processing, recall-then-generate, recall-reason-generate, extract-generate, direct-answer methods) and focuses on teaching small LMs to choose the most effective reasoning strategy for a given task.This approach aims to enable small LMs to perform at their best, regardless of their size, by utilizing more nuanced data and training strategies.The system is described as a "Cautious Reasoner," learning to execute specific reasoning steps and strategize at a higher level how to approach particular tasks.</p>
<p>VI. OPPORTUNITIES</p>
<p>Symbolic Knowledge distillation of LLM has been one of the heated topics and has been gaining rapid popularity.Among the various areas, the most prominent areas where it can be applied are:</p>
<p>A. Creation of larger, diversified and qualitative dataset It offers significant potential in enhancing dataset quality and diversity.This process involves extracting structured knowledge from LLMs to create datasets that are not only larger in scale but also exhibit a broader range of qualities and characteristics.These enriched datasets can be pivotal in Direct Commonsense Reasoning [3] Multi-level Summarisation [4] RL based Translation [176] Direct Commonsense Reasoning [177] Direct Commonsense Reasoning [138] Direct Mathematical Proof and Reasoning [178] Direct Mathematical Proof and Reasoning [179] Direct Visual Commonsense Reasoning [180] Direct Instruction Generation [181] Direct Instruction Generation [182] Direct Instruction Generation [183] Direct Handling Queries [184] Direct Handling Queries [185] Direct Handling Queries [186] Direct Handling Queries [81] Direct Labeling Data [82] Direct Labeling Data [187] Direct Generating Task Specific Small Models [188] Direct Complex Reasoning [189] Direct Complex Reasoning training more robust and efficient machine learning models, leading to advancements in various domains such as NLP, image recognition, and beyond.The ability to generate highquality datasets from LLMs accelerates the development of more sophisticated AI systems, contributing to advances in both academic research and practical applications.</p>
<p>B. Reduction in the cost by utilising machines in the low level task under guidance on humans</p>
<p>Implementing symbolic knowledge distillation in low-level tasks allows for the effective delegation of routine and repetitive tasks to machines, significantly reducing operational costs.By leveraging the distilled knowledge from LLMs, machines can perform these tasks with a high degree of accuracy and efficiency, under the supervision of human experts.This collaboration between human intelligence and machine capabilities leads to optimized resource utilization, where humans focus on more complex, creative, or decision-making tasks while machines handle the routine aspects, thereby enhancing overall productivity and cost-effectiveness.</p>
<p>C. Smaller and more powerful models than LLMs for summarization, translation, common sense etc Distilling knowledge from LLMs into smaller models presents a promising avenue for creating compact yet powerful AI tools.These distilled models retain the core capabilities of their larger counterparts but with reduced computational requirements.This makes them particularly suitable for applications like text summarization, language translation, and common sense reasoning, where efficiency and speed are crucial.These smaller models offer the dual benefits of lower resource consumption and faster processing times, making them ideal for deployment in environments with limited computational resources or for applications requiring real-time responses.</p>
<p>D. Instruction tuning</p>
<p>Instruction tuning, in the context of symbolic knowledge distillation from LLMs, refers to the process of refining and optimizing AI models to better understand and execute specific instructions.This approach enhances the model's ability to interpret and act upon user commands accurately, leading to more intuitive and user-friendly AI systems.Instruction tuning is particularly relevant in applications where user interaction is key, such as virtual assistants, educational tools, and interactive AI systems.By focusing on instruction tuning, developers can create AI models that are not only powerful in their capabilities but also align closely with user expectations and needs, facilitating more effective and seamless human-AI interactions.</p>
<p>E. Novel Algorithm and Evaluation Benchmark</p>
<p>Size alone does not determine the quality of language generation.Innovative approaches, such as those seen in I2D2 [177], present a viable option, particularly in scenarios where utilizing massive models like GPT-3 is impractical.Given that this field is in its infancy, the evaluation benchmarks are quite intricate and require significant refinement.Current evaluation techniques are from traditional knowledge distillation benchmarks and must be updated to fit this novel area of study.Symbolic Knowledge Distillation of LLMs involves two components: the neural aspect (LLMs) and the symbolic aspect (distilled symbolic knowledge).Together, these form a Neurosymbolic model, which necessitates the development of new benchmarks for evaluation, testing, and validation [190].</p>
<p>G. Self Improvement of LLMs</p>
<p>Reinforcement Learning from Human Feedback (RLHF) has emerged as a prevalent method for refining LLMs.However, the involvement of human input inherently constrains its efficacy and outcomes to the limits of human capabilities.Upon undergoing fine-tuning, LLMs can surpass human performance levels.Leveraging these enhanced models to autonomously fine-tune themselves, either via rewards [87] or prompt tuning or alternative mechanisms, presents a viable strategy for eliminating the limitations imposed by human intervention opening the gateway for Superintelligence.When employing Reinforcement Learning (RL) for fine-tuning LLMs by themselves, opting for Neurosymbolic RL approaches is often more advantageous.This is because Neurosymbolic RL not only aids in the tuning process but also enhances the model with the ability to interpret and explain its decision-making process comprehensively [191].</p>
<p>H. Cross-domain Symbiosis</p>
<p>Symbolic Knowledge extracted from LLMs extends its utility beyond the linguistic domain.Studies, such [179], demonstrate that textual knowledge can augment visual models by offering explanations and enhancing efficiency.This interdisciplinary application can be further leveraged in diverse fields such as medical imaging, autonomous driving, and surveillance, serving not only to elucidate model outputs but also to improve transfer from one domain to another(simulation to real) by providing the semantic anchors [192].This crossdomain synergy highlights the potential of Symbolic Knowledge in broadening the applicability and understanding of complex AI systems.</p>
<p>I. Industrial Applications</p>
<p>Symbolic knowledge distillation reveals a critical insight: the effectiveness of LLMs is significantly influenced not only by their size (number of parameters) but more importantly by the quality of the datasets on which they are trained.It highlights the significant role of symbolic knowledge distillation in enhancing domain-specific AI applications by fine-tuning LLMs with specialized corpora and instruction-following data.Notable implementations include LawyerLLaMA [193] and LawGPT [194] for legal services, HuatuoGPT [195] and ChatDoctor [196] for medical applications, XuanYuan [197] for finance, DARWIN Series [198] and SciGLM [199] for scientific research.These tailored models demonstrate substantial improvements in accuracy, efficiency, and usability, showcasing the transformative potential of symbolic knowledge distillation in various industries.</p>
<p>VII. CHALLENGES A. Ensuring Data Quality and Diversity in Datasets</p>
<p>While symbolic knowledge distillation from LLMs promises to enhance dataset quality, a major challenge is ensuring the high quality and representativeness of the generated data.The datasets derived from LLMs may inherit biases or inaccuracies present in the original training data of these models.This can lead to the propagation of errors and skewed perspectives in the new datasets, affecting the reliability and fairness of AI systems trained on them.Ensuring data quality requires rigorous validation processes and mechanisms to identify and mitigate biases, which can be resource-intensive, complex, is still an not so explored area.</p>
<p>Balancing Automation and Human Oversight in Dataset Generation</p>
<p>While utilizing machines under human guidance can reduce costs, achieving the right balance between automation and human oversight is challenging.Over-reliance on automation may lead to oversight of nuanced or exceptional cases that require human judgment.Conversely, excessive human intervention can negate the efficiency gains from automation.Establishing effective protocols and systems for humanmachine collaboration, where machines handle routine tasks while humans oversee and intervene as needed, is crucial but difficult to optimize.</p>
<p>C. Developing Compact Models Without Compromising Performance</p>
<p>Creating smaller models from LLMs that maintain high performance levels is a significant challenge.There are research efforts to quantize LLMs to ultra-low bit sizes, their performance has been found lacking and does not meet the standards required for industrial applications [200] [201].Symbolic Knowledge Distillation has shown promise in specific, narrower fields such as translation, summarization, and commonsense reasoning.However, it must evolve into a comprehensive symbolic knowledge base capable of generalizing across all domains.Developing these compact models requires sophisticated techniques to compress and optimize the knowledge transfer without losing the nuances and depth of the original model.</p>
<p>D. Effective Instruction Tuning for Diverse Applications</p>
<p>Instruction tuning in AI models poses the challenge of adapting to a wide range of instructions and use cases.Models must be versatile enough to understand and execute a variety of commands accurately across different domains and contexts.This requires extensive training and fine-tuning, which can be resource-intensive.Moreover, ensuring that the models remain adaptable and up-to-date with evolving user needs and language usage is an ongoing challenge, necessitating continuous monitoring and updates.</p>
<p>E. Adaptability and Continuous Learning</p>
<p>Ensuring that distilled models can adapt to new information and evolving data landscapes is challenging.Continuous learning mechanisms that allow models to update their knowledge without compromising efficiency or requiring complete retraining are essential for keeping distilled models relevant and effective.</p>
<p>VIII. LESSON LEARNED AND KEY TAKEAWAYS</p>
<p>A. Efficiency Through Distillation</p>
<p>Symbolic knowledge distillation demonstrates a powerful method to enhance the efficiency of LLMs.By distilling complex, large-scale models into smaller, more manageable versions without significant loss in performance, researchers can achieve remarkable efficiency gains.This approach not only reduces computational requirements but also makes advanced AI capabilities more accessible for applications with limited resources.</p>
<p>B. Advancement in Commonsense Reasoning</p>
<p>The transition to a 'from-machine-to-corpus-to-machine' paradigm marks a significant advancement in commonsense reasoning.This innovative approach, through the creation of extensive and diverse datasets like ATOMIC and models like NOVACOMET, underscores the potential of machinegenerated knowledge in improving AI's understanding and application of commonsense knowledge.</p>
<p>C. Innovation in Data Generation and Use by Collaborating Human Intelligence and Machine Capabilities</p>
<p>LLMs has the potential in generating high-quality, diverse datasets.These datasets serve as a foundation for training more robust models, emphasizing the importance of data quality, diversity, and the innovative use of symbolic knowledge in dataset creation.The effective collaboration between human oversight and automated processes in dataset generation and task execution highlights the synergistic potential of combining human intelligence with machine efficiency.This collaboration is key to overcoming current limitations and unlocking new capabilities in AI systems.</p>
<p>D. Cross-Domain Applications</p>
<p>The applications of symbolic knowledge distillation extend beyond NLP into areas such as visual commonsense reasoning and mathematical proof solving.This cross-domain applicability showcases the versatility of distilled models and their potential to revolutionize various fields by enhancing model performance and understanding.</p>
<p>E. Instruction Tuning and Generation</p>
<p>The development and refinement of techniques for instruction tuning and generation signify a leap towards creating more user-friendly and intuitive AI systems.Models capable of generating their own instructions or being finely tuned to understand and execute specific commands can lead to more natural and effective human-AI interactions.</p>
<p>F. Challenges and Opportunities</p>
<p>While the advancements are notable, they also underscore challenges such as ensuring data quality, balancing automation with human oversight, and developing compact models without compromising performance.Addressing these challenges presents opportunities for further research and innovation in model training, dataset creation, and the development of algorithms for enhanced capabilities and benchmark for the evaluation.</p>
<p>To address the identified gaps in current research on symbolic knowledge distillation, it is crucial to first ensure the quality and diversity of datasets through rigorous validation to identify and mitigate biases inherited from LLMs, ensuring the trustworthy knowledge distillation.Balancing automation and human oversight is also essential; effective protocols for human-machine collaboration can optimize efficiency while ensuring nuanced cases are handled appropriately.Though the size of data required for efficient distillation is still unknown, research [202] propose that only 1000 high quality human curated data is enough.Another challenge is developing compact models without compromising performance, which requires sophisticated techniques to compress and optimize knowledge transfer while maintaining the depth of the original models.Effective instruction tuning for diverse applications demands extensive training and fine-tuning to ensure models can accurately execute various commands across domains.Ensuring adaptability and continuous learning in distilled models is vital, necessitating mechanisms for ongoing updates without compromising efficiency.Addressing these areas will advance symbolic knowledge distillation towards more reliable and practical applications.</p>
<p>IX. CONCLUSION</p>
<p>This survey paper has explored the emerging and crucial domain of symbolic knowledge distillation in LLMs.As LLMs continue to grow in scale and complexity, the need to effectively extract and represent their extensive knowledge becomes paramount.By categorizing existing research based on methodologies and applications, we have highlighted how symbolic knowledge distillation can enhance the transparency and functionality of smaller, more efficient AI models.This comprehensive overview underscores the significance of symbolic knowledge distillation in advancing more accessible and efficient AI systems.While there is a notable lack of comprehensive research in this area, our survey paper fills this crucial gap by offering an extensive review of the current state of symbolic knowledge distillation in LLMs, shedding light on methodologies, challenges, and advancements in this field.</p>
<p>Fig. 1 .
1
Fig. 1.Milestones in history of LLM and Knowledge Distillation</p>
<p>Fig. 2 .
2
Fig. 2. Types of Traditional Knowledge Distillation (a) Response-based, (b) Feature-based and (c) Relation-based</p>
<p>Fig. 3 .
3
Fig. 3. Symbolic Knowledge Distillation</p>
<p>Fig. 4 .
4
Fig. 4. Overview of Direct Distillation process LLMs</p>
<p>Fig. 5 .
5
Fig. 5. Overview of Multilevel Distillation process LLMs</p>
<p>Fig. 7 .
7
Fig. 7. Overview of Related Works</p>
<p>F</p>
<p>. Creation of Open source data and open model The concept of symbolic distillation presents an intriguing avenue for creating open source data and models within the realm of LLMs.Currently, many LLMs are proprietary and trained on closed-source data, limiting accessibility and transparency.Symbolic distillation involves extracting symbolic knowledge and representations from LLMs, which can then be used to generate open source data.This open data can serve as the foundation for training new models that are open source, thereby democratizing access to advanced language models.By transitioning from closed source to open source, we can promote transparency, collaboration, and innovation in the field of NLP, aligning with the principles of open science and open AI.</p>
<p>TABLE II COMPARISON
II
OF TRADITIONAL AND SYMBOLIC KNOWLEDGE DISTILLATION PROCESS
ParametersTraditional Knowledge DistillationSymbolic Knowledge DistillationNature of Knowledge TransferSoft outputs or logits which represent the teacher's learned probability distributionHuman-readable representations such as logical rules, decision trees, or graphical modelsInterpretability and TransparencyStudent model remains a black-box neural networkStudent model, guided by symbolic representations offer insights into the decision-making processMethods Used for DistillationTechniques such as temperature scaling are used to soften the teacher's outputsInvolve methods like Layer-wise Relevance Propagation (LRP) or SHAPStudent ModelMimic the teacher modelCan be tune to behave differently than teacher modelData GenerationNoYesLayerwise DependencyDiffernet layers have different influencesNo such dependency
[135]refix tokens, while maintaining unidirectional attention only on generated tokens.This allows the prefix decoders to bidirectionally encode the prefix sequence and predict the output tokens autoregressively, where the same parameters are shared during encoding and decoding.Unlike the causal decoder architecture, the prefix decoder architecture can incorporate bidirectional information into the decoding process, making it more suitable for tasks that require understanding the context of the entire input sequence.Existing representative LLMs based on prefix decoders include GLM-130B[134]and U-PaLM[135].2) Training Process of Large Language Models: The whole training process of LLM can be divided into two phases:</p>
<p>TABLE IV RELATED
IV
WORKS IN SYMBOLIC KNOWLEDGE DISTILLATION WITH THEIR MAJOR COMPONENTS
ResearchTeacherStudentDataset GeneratedSize of Dataset[2]GPT-3(175B)COM ET distil (1.5B)Commonsense Knowledge Graph6.5M[3]GPT-3REFEREE-CONTROLSentence-summary pairs100K[4]Encoder-Decoder ArchitectureTeacher ItselfTranslation DatasetN/A[176]GPT-3NOVACOMETNOVATOMIC2.2M[177]GPT-3GPT-2Gen-A-tomic7M[138]Decoder Only ArchitectureTeacher ItselfTactic DatasetN/A[178]GPT-3.5FlanT5Math ReasoningN/A[179]ChatGPTBLIP-2Localized Commonsense Knowledge1M[180]GPT-3Teacher ItselfInstruction Dataset82K[181]GPT-3.57B LLaMAInstruction Dataset52K[182]ChatGPTWizardLMInstruction Dataset250K[183]ChatGPTVicuna-13BConversational Dataset70K[184]ChatGPTKoala-13BConversational DatasetN/A[185]GPT3-175BGPT-J-6BPrompt DatasetN/A[186]PaLM-540BmT5-XLMultilingual QA47173[81]GPT-3RoBERTaLabeled Data5.1K[82]GPT-3T0++Labeled DataN/A[187]540B PaLM770M T5RationalesN/A[188]GPT-4Orca(13B)Zero shot queries5M[189]GPT-4Orca-2Progressive queries817K
https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/3 0/executive-order-on-the-safe-secure-and-trustworthy-development-and-use -of-artificial-intelligence/(last accessed on: [28/02/2024])
Manuscript received January 6, 2024.This work was supported in part by the U.S. National Science Foundation under Grant No. 2309760 and Grant No. 2317117.K. Acharya and H. Song are with the Security and Optimization for Networked Globe Laboratory (SONG Lab),
Language models as knowledge bases. F Petroni, T Rocktäschel, P Lewis, A Bakhtin, Y Wu, A H Miller, S Riedel, arXiv:1909.010662019arXiv preprint</p>
<p>Symbolic knowledge distillation: from general language models to commonsense models. P West, C Bhagavatula, J Hessel, J D Hwang, L Jiang, R L Bras, X Lu, S Welleck, Y Choi, arXiv:2110.071782021arXiv preprint</p>
<p>Referee: Reference-free sentence summarization with sharper controllability through symbolic knowledge distillation. M Sclar, P West, S Kumar, Y Tsvetkov, Y Choi, arXiv:2210.138002022arXiv preprint</p>
<p>Reinforced self-training (rest) for language modeling. C Gulcehre, T L Paine, S Srinivasan, K Konyushkova, L Weerts, A Sharma, A Siddhant, A Ahern, M Wang, C Gu, arXiv:2308.089982023arXiv preprint</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Recent advances in natural language processing via large pre-trained language models: A survey. B Min, H Ross, E Sulem, A P B Veyseh, T H Nguyen, O Sainz, E Agirre, I Heintz, D Roth, ACM Computing Surveys. 5622023</p>
<p>Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects. M U Hadi, R Qureshi, A Shah, M Irfan, A Zafar, M B Shaikh, N Akhtar, J Wu, S Mirjalili, 2023Authorea Preprints</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, K Zhu, H Chen, L Yang, X Yi, C Wang, Y Wang, arXiv:2307.031092023arXiv preprint</p>
<p>Large language models meet nl2code: A survey. D Zan, B Chen, F Zhang, D Lu, B Wu, B Guan, W Yongji, J.-G Lou, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. E Kasneci, K Sessler, S Küchemann, M Bannert, D Dementieva, F Fischer, U Gasser, G Groh, S Günnemann, E Hüllermeier, S Krusche, G Kutyniok, T Michaeli, C Nerdel, J Pfeffer, O Poquet, M Sailer, A Schmidt, T Seidel, M Stadler, J Weller, J Kuhn, G Kasneci, Learning and Individual Differences. 1031022742023</p>
<p>A review on language models as knowledge bases. B Alkhamissi, M Li, A Celikyilmaz, M Diab, M Ghazvininejad, 2022</p>
<p>Language models as or for knowledge bases. S Razniewski, A Yates, N Kassner, G Weikum, arXiv:2110.048882021arXiv preprint</p>
<p>Towards reasoning in large language models: A survey. J Huang, K C , -C Chang, arXiv:2212.104032022arXiv preprint</p>
<p>Explainability for large language models: A survey. H Zhao, H Chen, F Yang, N Liu, H Deng, H Cai, S Wang, D Yin, M Du, arXiv:2309.010292023arXiv preprint</p>
<p>Aligning large language models with human: A survey. Y Wang, W Zhong, L Li, F Mi, X Zeng, W Huang, L Shang, X Jiang, Q Liu, arXiv:2307.129662023arXiv preprint</p>
<p>Instruction tuning for large language models: A survey. S Zhang, L Dong, X Li, S Zhang, X Sun, S Wang, J Li, R Hu, T Zhang, F Wu, arXiv:2308.107922023arXiv preprint</p>
<p>A survey on model compression for large language models. X Zhu, J Li, Y Liu, C Ma, W Wang, arXiv:2308.076332023arXiv preprint</p>
<p>Trustworthy llms: a survey and guideline for evaluating large language models' alignment. Y Liu, Y Yao, J.-F Ton, X Zhang, R G H Cheng, Y Klochkov, M F Taufiq, H Li, arXiv:2308.053742023arXiv preprint</p>
<p>Large language models for software engineering: Survey and open problems. A Fan, B Gokkaya, M Harman, M Lyubarskiy, S Sengupta, S Yoo, J M Zhang, arXiv:2310.035332023arXiv preprint</p>
<p>Siren's song in the ai ocean: A survey on hallucination in large language models. Y Zhang, Y Li, L Cui, D Cai, L Liu, T Fu, X Huang, E Zhao, Y Zhang, Y Chen, arXiv:2309.012192023arXiv preprint</p>
<p>Multimodal large language models: A survey. J Wu, W Gan, Z Chen, S Wan, P S Yu, arXiv:2311.131652023arXiv preprint</p>
<p>Large language models for robotics: A survey. F Zeng, W Gan, Y Wang, N Liu, P S Yu, arXiv:2311.072262023arXiv preprint</p>
<p>Large language models for information retrieval: A survey. Y Zhu, H Yuan, S Wang, J Liu, W Liu, C Deng, Z Dou, J.-R Wen, arXiv:2308.071072023arXiv preprint</p>
<p>Computing machinery and intelligence. A M Turing, 2009Springer</p>
<p>Prediction and entropy of printed english. C E Shannon, Bell system technical journal. 3011951</p>
<p>Eliza-a computer program for the study of natural language communication between man and machine. J Weizenbaum, Communications of the ACM. 911966</p>
<p>Procedures as a representation for data in a computer program for understanding natural language. T Winograd, 1971MASSACHUSETTS INST OF TECH CAMBRIDGE PROJECT MAC, Tech. Rep</p>
<p>A stochastic approach to parsing. G Sampson, The 11th International Conference on Computational Linguistics. 1986. 19861</p>
<p>A statistical approach to machine translation. P F Brown, J Cocke, S A Della Pietra, V J Della Pietra, F Jelinek, J Lafferty, R L Mercer, P S Roossin, Computational linguistics. 1621990</p>
<p>Two decades of statistical language modeling: Where do we go from here. R Rosenfeld, Proceedings of the IEEE. the IEEE200088</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 981997</p>
<p>A neural probabilistic language model. Y Bengio, R Ducharme, P Vincent, Advances in neural information processing systems. 200013</p>
<p>Lc: A flexible, extensible open-source toolkit for model compression. Y Idelbayev, M Á Carreira-Perpiñán, Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management. the 30th ACM International Conference on Information &amp; Knowledge Management2021</p>
<p>Model compression. C Buciluǎ, R Caruana, A Niculescu-Mizil, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. the 12th ACM SIGKDD international conference on Knowledge discovery and data mining2006</p>
<p>Model compression and acceleration for deep neural networks: The principles, progress, and challenges. Y Cheng, D Wang, P Zhou, T Zhang, IEEE Signal Processing Magazine. 3512018</p>
<p>Quantized convolutional neural networks for mobile devices. J Wu, C Leng, Y Wang, Q Hu, J Cheng, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Binaryconnect: Training deep neural networks with binary weights during propagations. M Courbariaux, Y Bengio, J.-P David, Advances in neural information processing systems. 201528</p>
<p>Structured transforms for small-footprint deep learning. V Sindhwani, T Sainath, S Kumar, Advances in Neural Information Processing Systems. 201528</p>
<p>Learning both weights and connections for efficient neural network. S Han, J Pool, J Tran, W Dally, Advances in neural information processing systems. 201528</p>
<p>Packing convolutional neural networks in the frequency domain. Y Wang, C Xu, C Xu, D Tao, IEEE transactions on pattern analysis and machine intelligence. 201841</p>
<p>On compressing deep models by low rank and sparse decomposition. X Yu, T Liu, X Wang, D Tao, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Exploiting linear structure within convolutional networks for efficient evaluation. E L Denton, W Zaremba, J Bruna, Y Lecun, R Fergus, Advances in neural information processing systems. 201427</p>
<p>A survey of model compression and acceleration for deep neural networks. Y Cheng, D Wang, P Zhou, T Zhang, arXiv:1710.092822017arXiv preprint</p>
<p>Distilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, arXiv:1503.025312015arXiv preprint</p>
<p>The era of cognitive systems: An inside look at ibm watson and how it works. R High, IBM Corporation, Redbooks. 1162012</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.37812013arXiv preprint</p>
<p>Sequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V Le, Advances in neural information processing systems. 201427</p>
<p>GloVe: Global vectors for word representation. J Pennington, R Socher, C Manning, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsOct. 2014</p>
<p>A Romero, N Ballas, S E Kahou, A Chassang, C Gatta, Y Bengio, arXiv:1412.6550Fitnets: Hints for thin deep nets. 2014arXiv preprint</p>
<p>Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. S Zagoruyko, N Komodakis, arXiv:1612.039282016arXiv preprint</p>
<p>Squad: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, arXiv:1606.052502016arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. J Yim, D Joo, J Bae, J Kim, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Paraphrasing complex network: Network compression via factor transfer. J Kim, S Park, N Kwak, Advances in neural information processing systems. 201831</p>
<p>Deep contextualized word representations. M E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational LinguisticsJun. 20181</p>
<p>Universal sentence encoder. D Cer, Y Yang, S -Y. Kong, N Hua, N Limtiaco, R S John, N Constant, M Guajardo-Cespedes, S Yuan, C Tar, arXiv:1803.111752018arXiv preprint</p>
<p>Glue: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, arXiv:1804.074612018arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>I Solaiman, M Brundage, J Clark, A Askell, A Herbert-Voss, J Wu, A Radford, G Krueger, J W Kim, S Kreps, arXiv:1908.09203Release strategies and the social impacts of language models. 2019arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>A comprehensive overhaul of feature distillation. B Heo, J Kim, S Yun, H Park, N Kwak, J Y Choi, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Contrastive representation distillation. Y Tian, D Krishnan, P Isola, arXiv:1910.106992019arXiv preprint</p>
<p>Revisit knowledge distillation: a teacher-free framework. L Yuan, F E Tay, G Li, T Wang, J Feng, 2019</p>
<p>Deep mutual learning. Y Zhang, T Xiang, T M Hospedales, H Lu, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Improved knowledge distillation via teacher assistant. S I Mirzadeh, M Farajtabar, A Li, N Levine, A Matsukawa, H Ghasemzadeh, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Lifelong gan: Continual learning for conditional image generation. M Zhai, L Chen, F Tung, J He, M Nawhal, G Mori, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Distillation as a defense to adversarial perturbations against deep neural networks. N Papernot, P Mcdaniel, X Wu, S Jha, A Swami, 2016 IEEE symposium on security and privacy (SP. IEEE2016</p>
<p>Self-supervised label augmentation via input transformations. H Lee, S J Hwang, J Shin, International Conference on Machine Learning. PMLR2020</p>
<p>Explaining sequence-level knowledge distillation as data-augmentation for neural machine translation. M A Gordon, K Duh, arXiv:1912.033342019arXiv preprint</p>
<p>Private model compression via knowledge distillation. J Wang, W Bao, L Sun, X Zhu, B Cao, S Y Philip, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Dataset distillation. T Wang, J.-Y Zhu, A Torralba, A A Efros, arXiv:1811.109592018arXiv preprint</p>
<p>Flexible dataset distillation: Learn labels instead of images. O Bohdal, Y Yang, T Hospedales, arXiv:2006.085722020arXiv preprint</p>
<p>A decade survey of transfer learning. S Niu, Y Liu, J Wang, H Song, IEEE Transactions on Artificial Intelligence. 122010-2020. 2020</p>
<p>Gshard: Scaling giant models with conditional computation and automatic sharding. D Lepikhin, H Lee, Y Xu, D Chen, O Firat, Y Huang, M Krikun, N Shazeer, Z Chen, arXiv:2006.166682020arXiv preprint</p>
<p>Glam: Efficient scaling of language models with mixture-of-experts. N Du, Y Huang, A M Dai, S Tong, D Lepikhin, Y Xu, M Krikun, Y Zhou, A W Yu, O Firat, International Conference on Machine Learning. PMLR2022</p>
<p>Neurocomparatives: Neuro-symbolic distillation of comparative knowledge. P Howard, J Wang, V Lal, G Singer, Y Choi, S Swayamdipta, arXiv:2305.049782023arXiv preprint</p>
<p>Scaling laws for neural language models. J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.083612020arXiv preprint</p>
<p>Training compute-optimal large language models. J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D D L Casas, L A Hendricks, J Welbl, A Clark, arXiv:2203.155562022arXiv preprint</p>
<p>Will we run out of data? an analysis of the limits of scaling datasets in machine learning. P Villalobos, J Sevilla, L Heim, T Besiroglu, M Hobbhahn, A Ho, arXiv:2211.043252022arXiv preprint</p>
<p>Want to reduce labeling cost? gpt-3 can help. S Wang, Y Liu, Y Xu, C Zhu, M Zeng, arXiv:2108.134872021arXiv preprint</p>
<p>Language models in the loop: Incorporating prompting into weak supervision. R Smith, J A Fries, B Hancock, S H Bach, arXiv:2205.023182022arXiv preprint</p>
<p>Code alpaca: An instruction-following llama model for code generation. S Chaudhary, 2023Code alpaca: An instruction-following llama model for code generation</p>
<p>Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. H Luo, Q Sun, C Xu, P Zhao, J Lou, C Tao, X Geng, Q Lin, S Chen, D Zhang, arXiv:2308.095832023arXiv preprint</p>
<p>Enhancing chat language models by scaling high-quality instructional conversations. N Ding, Y Chen, B Xu, Y Qin, Z Zheng, S Hu, Z Liu, M Sun, B Zhou, arXiv:2305.142332023arXiv preprint</p>
<p>Lion: Adversarial distillation of closed-source large language model. Y Jiang, C Chan, M Chen, W Wang, arXiv:2305.128702023arXiv preprint</p>
<p>Self-rewarding language models. W Yuan, R Y Pang, K Cho, S Sukhbaatar, J Xu, J Weston, arXiv:2401.100202024arXiv preprint</p>
<p>mt5: A massively multilingual pre-trained text-to-text transformer. L Xue, N Constant, A Roberts, M Kale, R Al-Rfou, A Siddhant, A Barua, C Raffel, arXiv:2010.119342020arXiv preprint</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.016522021arXiv preprint</p>
<p>Lamda: Language models for dialog applications. R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, arXiv:2201.082392022arXiv preprint</p>
<p>Solving quantitative reasoning problems with language models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>Ul2: Unifying language learning paradigms. Y Tay, M Dehghani, V Q Tran, X Garcia, J Wei, X Wang, H W Chung, D Bahri, T Schuster, S Zheng, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, Y Li, X Wang, M Dehghani, S Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Webgpt: Browserassisted question-answering with human feedback. R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.093322021arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>View in Article. R Openai, arxiv 2303.08774Gpt-4 technical report. 2023213</p>
<p>Gpt-j-6b: A 6 billion parameter autoregressive language model. B Wang, A Komatsuzaki, 2021</p>
<p>GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. S Black, L Gao, P Wang, C Leahy, S Biderman, 10.5281/zenodo.5297715Mar. 2021If you use this software, please cite it using these metadata</p>
<p>S Black, S Biderman, E Hallahan, Q Anthony, L Gao, L Golding, H He, C Leahy, K Mcdonell, J Phang, arXiv:2204.06745Gpt-neox-20b: An open-source autoregressive language model. 2022arXiv preprint</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. J W Rae, S Borgeaud, T Cai, K Millican, J Hoffmann, F Song, J Aslanides, S Henderson, R Ring, S Young, arXiv:2112.114462021arXiv preprint</p>
<p>Competitionlevel code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, T Eccles, J Keeling, F Gimeno, A Lago, Science. 37866242022</p>
<p>Improving alignment of dialogue agents via targeted human judgements. A Glaese, N Mcaleese, M Trębacz, J Aslanides, V Firoiu, T Ewalds, M Rauh, L Weidinger, M Chadwick, P Thacker, arXiv:2209.143752022arXiv preprint</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Opt: Open pre-trained transformer language models. S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, arXiv:2205.010682022arXiv preprint</p>
<p>Opt-iml: Scaling language model instruction meta learning through the lens of generalization. S Iyer, X V Lin, R Pasunuru, T Mihaylov, D Simig, P Yu, K Shuster, T Wang, Q Liu, P S Koura, arXiv:2212.120172022arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.139712023arXiv preprint</p>
<p>Multitask prompted training enables zero-shot task generalization. V Sanh, A Webson, C Raffel, S H Bach, L Sutawika, Z Alyafeai, A Chaffin, A Stiegler, T L Scao, A Raja, M Dey, M S Bari, C Xu, U Thakker, S Sharma, E Szczechla, T Kim, G Chhablani, N V Nayak, D Datta, J Chang, M T Jiang, H Wang, M Manica, S Shen, Z X Yong, H Pandey, R Bawden, T Wang, T Neeraj, J Rozen, A Sharma, A Santilli, T Févry, J A Fries, R Teehan, S Biderman, L Gao, T Bers, T Wolf, A M Rush, abs/2110.082072021CoRR</p>
<p>Bloom: A 176b-parameter open-access multilingual language model. B Workshop, T L Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow, R Castagné, A S Luccioni, F Yvon, arXiv:2211.051002022arXiv preprint</p>
<p>Crosslingual generalization through multitask finetuning. N Muennighoff, T Wang, L Sutawika, A Roberts, S Biderman, T L Scao, M S Bari, S Shen, Z.-X Yong, H Schoelkopf, arXiv:2211.017862022arXiv preprint</p>
<p>ERNIE 2.0: A continual pre-training framework for language understanding. Y Sun, S Wang, Y Li, S Feng, H Tian, H Wu, H Wang, CoRR. 1907.12412. 2019</p>
<p>Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. Y Sun, S Wang, S Feng, S Ding, C Pang, J Shang, J Liu, X Chen, Y Zhao, Y Lu, arXiv:2107.021372021arXiv preprint</p>
<p>Ernie 3.0 titan: Exploring largerscale knowledge enhanced pre-training for language understanding and generation. S Wang, Y Sun, Y Xiang, Z Wu, S Ding, W Gong, S Feng, J Shang, Y Zhao, C Pang, arXiv:2112.127312021arXiv preprint</p>
<p>Knowledge distillation: A survey. J Gou, B Yu, S J Maybank, D Tao, International Journal of Computer Vision. 1292021</p>
<p>Learning efficient object detection models with knowledge distillation. G Chen, W Choi, X Yu, T Han, M Chandraker, Advances in neural information processing systems. 201730</p>
<p>Fast human pose estimation. F Zhang, X Zhu, M Ye, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Representation learning: A review and new perspectives. Y Bengio, A Courville, P Vincent, IEEE transactions on pattern analysis and machine intelligence. 201335</p>
<p>Cross-layer distillation with semantic calibration. D Chen, J.-P Mei, Y Zhang, C Wang, Z Wang, Y Feng, C Chen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Like what you like: Knowledge distill via neuron selectivity transfer. Z Huang, N Wang, arXiv:1707.012192017arXiv preprint</p>
<p>Learning deep representations with probabilistic knowledge transfer. N Passalis, A Tefas, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018</p>
<p>Knowledge distillation via route constrained optimization. X Jin, B Peng, Y Wu, Y Liu, J Liu, D Liang, J Yan, X Hu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Knowledge transfer via distillation of activation boundaries formed by hidden neurons. B Heo, M Lee, S Yun, J Y Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Self-supervised knowledge distillation using singular value decomposition. S H Lee, D H Kim, B C Song, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>Better and faster: knowledge transfer from multiple self-supervised learning tasks via graph distillation for video classification. C Zhang, Y Peng, arXiv:1804.100692018arXiv preprint</p>
<p>Graph-based knowledge distillation by multihead attention network. S Lee, B C Song, arXiv:1907.022262019arXiv preprint</p>
<p>Heterogeneous knowledge distillation using information flow modeling. N Passalis, M Tzelepi, A Tefas, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Explainable AI: interpreting, explaining and visualizing deep learning. G Montavon, A Binder, S Lapuschkin, W Samek, K.-R Müller, 2019Layer-wise relevance propagation: an overview</p>
<p>A unified approach to interpreting model predictions. S M Lundberg, S.-I Lee, Advances in neural information processing systems. 201730</p>
<p>Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, arXiv:1910.134612019BartarXiv preprint</p>
<p>Bloom: A 176b-parameter open-access multilingual language model. T L Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow, R Castagné, A S Luccioni, F Yvon, M Gallé, arXiv:2211.051002022arXiv preprint</p>
<p>Glm-130b: An open bilingual pre-trained model. A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, arXiv:2210.024142022arXiv preprint</p>
<p>Transcending scaling laws with 0.1% extra compute. Y Tay, J Wei, H W Chung, V Q Tran, D R So, S Shakeri, X Garcia, H S Zheng, J Rao, A Chowdhery, arXiv:2210.113992022arXiv preprint</p>
<p>Scaling laws and interpretability of learning from repeated data. D Hernandez, T Brown, T Conerly, N Dassarma, D Drain, S El-Showk, N Elhage, Z Hatfield-Dodds, T Henighan, T Hume, arXiv:2205.104872022arXiv preprint</p>
<p>Alignment of language agents. Z Kenton, T Everitt, L Weidinger, I Gabriel, V Mikulik, G Irving, arXiv:2103.146592021arXiv preprint</p>
<p>Formal mathematics statement curriculum learning. S Polu, J M Han, K Zheng, M Baksys, I Babuschkin, I Sutskever, arXiv:2202.013442022arXiv preprint</p>
<p>Can language models be biomedical knowledge bases. M Sung, J Lee, S Yi, M Jeon, S Kim, J Kang, arXiv:2109.071542021arXiv preprint</p>
<p>Probing across time: What does roberta know and when. L Z Liu, Y Wang, J Kasai, H Hajishirzi, N A Smith, arXiv:2104.078852021arXiv preprint</p>
<p>Measuring and improving consistency in pretrained language models. Y Elazar, N Kassner, S Ravfogel, A Ravichander, E Hovy, H Schütze, Y Goldberg, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. N Kassner, H Schütze, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJul. 2020</p>
<p>Modifying memories in transformer models. C Zhu, A S Rawat, M Zaheer, S Bhojanapalli, D Li, F Yu, S Kumar, arXiv:2012.003632020arXiv preprint</p>
<p>Knowledge neurons in pretrained transformers. D Dai, L Dong, Y Hao, Z Sui, B Chang, F Wei, arXiv:2104.086962021arXiv preprint</p>
<p>Editing factual knowledge in language models. N De Cao, W Aziz, I Titov, arXiv:2104.081642021arXiv preprint</p>
<p>P Hase, M Diab, A Celikyilmaz, X Li, Z Kozareva, V Stoyanov, M Bansal, S Iyer, arXiv:2111.13654Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs. 2021arXiv preprint</p>
<p>Fast model editing at scale. E Mitchell, C Lin, A Bosselut, C Finn, C D Manning, arXiv:2110.113092021arXiv preprint</p>
<p>Towards continual knowledge learning of language models. J Jang, S Ye, S Yang, J Shin, J Han, G Kim, S J Choi, M Seo, arXiv:2110.032152021arXiv preprint</p>
<p>Are pretrained language models symbolic reasoners over knowledge. N Kassner, B Krojer, H Schütze, arXiv:2006.104132020arXiv preprint</p>
<p>Transformers as soft reasoners over language. P Clark, O Tafjord, K Richardson, arXiv:2002.058672020arXiv preprint</p>
<p>Prover: Proof generation for interpretable reasoning over rules. S Saha, S Ghosh, S Srivastava, M Bansal, arXiv:2010.028302020arXiv preprint</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. O Tafjord, B D Mishra, P Clark, arXiv:2012.130482020arXiv preprint</p>
<p>Measuring systematic generalization in neural proof generation with transformers. N Gontier, K Sinha, S Reddy, C Pal, Advances in Neural Information Processing Systems. 202033242</p>
<p>Pushing the limits of rule reasoning in transformers through natural language satisfiability. K Richardson, A Sabharwal, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236219</p>
<p>Rulebert: Teaching soft rules to pre-trained language models. M Saeed, N Ahmadi, P Nakov, P Papotti, arXiv:2109.130062021arXiv preprint</p>
<p>Analysing mathematical reasoning abilities of neural models. D Saxton, E Grefenstette, F Hill, P Kohli, arXiv:1904.015572019arXiv preprint</p>
<p>Commonsense reasoning with implicit knowledge in natural language. P Banerjee, S Mishra, K K Pal, A Mitra, C Baral, 20213rd Conference on Automated Knowledge Base Construction</p>
<p>P Zhou, R Khanna, S Lee, B Y Lin, D Ho, J Pujara, X Ren, arXiv:2005.00782Rica: Evaluating robust inference capabilities based on commonsense axioms. 2020arXiv preprint</p>
<p>Explain yourself! leveraging language models for commonsense reasoning. N F Rajani, B Mccann, C Xiong, R Socher, arXiv:1906.023612019arXiv preprint</p>
<p>Attention is not explanation. S Jain, B C Wallace, arXiv:1902.101862019arXiv preprint</p>
<p>Attention is not not explanation. S Wiegreffe, Y Pinter, arXiv:1908.046262019arXiv preprint</p>
<p>Is attention interpretable?. S Serrano, N A Smith, arXiv:1906.037312019arXiv preprint</p>
<p>Of non-linearity and commutativity in bert. S Zhao, D Pascual, G Brunner, R Wattenhofer, 2021 International Joint Conference on Neural Networks (IJCNN). IEEE2021</p>
<p>Transformer feed-forward layers are key-value memories. M Geva, R Schuster, J Berant, O Levy, arXiv:2012.149132020arXiv preprint</p>
<p>Locating and editing factual associations in gpt. K Meng, D Bau, A Andonian, Y Belinkov, Advances in Neural Information Processing Systems. 202235372</p>
<p>Explaining black box predictions and unveiling data artifacts through influence functions. X Han, B C Wallace, Y Tsvetkov, arXiv:2005.066762020arXiv preprint</p>
<p>An empirical comparison of instance attribution methods for nlp. P Pezeshkpour, S Jain, B C Wallace, S Singh, arXiv:2104.041282021arXiv preprint</p>
<p>Combining feature and instance attribution to detect artifacts. P Pezeshkpour, S Jain, S Singh, B C Wallace, arXiv:2107.003232021arXiv preprint</p>
<p>Hildif: Interactive debugging of nli models using influence functions. H Zylberajch, P Lertvittayakumjorn, F Toni, Proceedings of the First Workshop on Interactive Learning for Natural Language Processing. the First Workshop on Interactive Learning for Natural Language Processing2021</p>
<p>Wt5?! training text-to-text models to explain their predictions. S Narang, C Raffel, K Lee, A Roberts, N Fiedel, K Malkan, arXiv:2004.145462020arXiv preprint</p>
<p>Make up your mind! adversarial generation of inconsistent natural language explanations. O.-M Camburu, B Shillingford, P Minervini, T Lukasiewicz, P Blunsom, arXiv:1910.030652019arXiv preprint</p>
<p>Interactively providing explanations for transformer language models. F Friedrich, P Schramowski, C Tauchmann, K Kersting, arXiv:2110.020582021arXiv preprint</p>
<p>Rationalizing neural predictions. T Lei, R Barzilay, T Jaakkola, arXiv:1606.041552016arXiv preprint</p>
<p>Prompting contrastive explanations for commonsense reasoning tasks. B Paranjape, J Michael, M Ghazvininejad, L Zettlemoyer, H Hajishirzi, arXiv:2106.068232021arXiv preprint</p>
<p>Atomic: An atlas of machine commonsense for if-then reasoning. M Sap, R Le Bras, E Allaway, C Bhagavatula, N Lourie, H Rashkin, B Roof, N A Smith, Y Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>Novacomet: Open commonsense foundation models with symbolic knowledge distillation. P West, R L Bras, T Sorensen, B Y Lin, L Jiang, X Lu, K Chandu, J Hessel, A Baheti, C Bhagavatula, arXiv:2312.059792023arXiv preprint</p>
<p>I2d2: Inductive knowledge distillation with neurologic and self-imitation. C Bhagavatula, J D Hwang, D Downey, R L Bras, X Lu, K Sakaguchi, S Swayamdipta, P West, Y Choi, arXiv:2212.092462022arXiv preprint</p>
<p>Specializing smaller language models towards multi-step reasoning. Y Fu, H Peng, L Ou, A Sabharwal, T Khot, arXiv:2301.127262023arXiv preprint</p>
<p>Localized symbolic knowledge distillation for visual commonsense models. J S Park, J Hessel, K R Chandu, P P Liang, X Lu, P West, Y Yu, Q Huang, J Gao, A Farhadi, arXiv:2312.048372023arXiv preprint</p>
<p>Self-instruct: Aligning language model with self generated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Stanford alpaca: An instruction-following llama model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, 2023</p>
<p>Wizardlm: Empowering large language models to follow complex instructions. C Xu, Q Sun, K Zheng, X Geng, P Zhao, J Feng, C Tao, D Jiang, arXiv:2304.122442023arXiv preprint</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, March 2023</p>
<p>Koala: A dialogue model for academic research. X Geng, A Gudibande, H Liu, E Wallace, P Abbeel, S Levine, D Song, April 2023Blog post</p>
<p>Ask me anything: A simple strategy for prompting language models. S Arora, A Narayan, M F Chen, L Orr, N Guha, K Bhatia, I Chami, F Sala, C Ré, arXiv:2210.024412022arXiv preprint</p>
<p>Qameleon: Multilingual qa with only 5 examples. P Agrawal, C Alberti, F Huot, J Maynez, J Ma, S Ruder, K Ganchev, D Das, M Lapata, arXiv:2211.082642022arXiv preprint</p>
<p>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. C.-Y Hsieh, C.-L Li, C.-K Yeh, H Nakhost, Y Fujii, A Ratner, R Krishna, C.-Y Lee, T Pfister, arXiv:2305.023012023arXiv preprint</p>
<p>Orca: Progressive learning from complex explanation traces of gpt-4. S Mukherjee, A Mitra, G Jawahar, S Agarwal, H Palangi, A Awadallah, arXiv:2306.027072023arXiv preprint</p>
<p>Orca 2: Teaching small language models how to reason. A Mitra, L Del Corro, S Mahajan, A Codas, C Simoes, S Agrawal, X Chen, A Razdaibiedina, E Jones, K , arXiv:2311.110452023arXiv preprint</p>
<p>A survey on verification and validation, testing and evaluations of neurosymbolic artificial intelligence. J Renkhoff, K Feng, M Meier-Doernberg, A Velasquez, H H Song, IEEE Transactions on Artificial Intelligence. 2024</p>
<p>Neurosymbolic reinforcement learning and planning: A survey. K Acharya, W Raza, C Dourado, A Velasquez, H H Song, IEEE Transactions on Artificial Intelligence. 2023</p>
<p>Transfer from imprecise and abstract models to autonomous technologies (tiamat). A Velasquez, 2023Defense Advanced Research Projects Agency (DARPA) Program Solicitation</p>
<p>Q Huang, M Tao, C Zhang, Z An, C Jiang, Z Chen, Z Wu, Y Feng, arXiv:2305.15062Lawyer llama technical report. 2023arXiv preprint</p>
<p>Chatlaw: Open-source legal large language model with integrated external knowledge bases. J Cui, Z Li, Y Yan, B Chen, L Yuan, arXiv:2306.160922023arXiv preprint</p>
<p>H Zhang, J Chen, F Jiang, F Yu, Z Chen, J Li, G Chen, X Wu, Z Zhang, Q Xiao, arXiv:2305.15075Huatuogpt, towards taming language model to be a doctor. 2023arXiv preprint</p>
<p>Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge. Y Li, Z Li, K Zhang, R Dan, S Jiang, Y Zhang, Cureus. 1562023</p>
<p>Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters. X Zhang, Q Yang, Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. the 32nd ACM International Conference on Information and Knowledge Management2023</p>
<p>Darwin series: Domain specific large language models for natural science. T Xie, Y Wan, W Huang, Z Yin, Y Liu, S Wang, Q Linghu, C Kit, C Grazian, W Zhang, arXiv:2308.135652023arXiv preprint</p>
<p>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang, Y Yue, Y Dong, J Tang, arXiv:2401.079502024arXiv preprint</p>
<p>Omniquant: Omnidirectionally calibrated quantization for large language models. W Shao, M Chen, Z Zhang, P Xu, L Zhao, Z Li, K Zhang, P Gao, Y Qiao, P Luo, arXiv:2308.131372023arXiv preprint</p>
<p>Y Shang, Z Yuan, Q Wu, Z Dong, arXiv:2310.00034Pb-llm: Partially binarized large language models. 2023arXiv preprint</p>
<p>Song has been a Highly Cited Researcher identified by Web of Science since 2021. He is an ACM Distinguished Speaker (2020-present), an IEEE Computer Society Distinguished Visitor (2024-present), an IEEE Communications Society (ComSoc) Distinguished Lecturer (2024-present), an IEEE Intelligent Transportation Systems Society (ITSS) Distinguished Lecturer (2024-present), an IEEE Vehicular Technology Society (VTS) Distinguished Lecturer (2023-present) and an IEEE Systems Council Distinguished Lecturer (2023-present). Dr. Song received Research.com Rising Star of Science Award in 2022, 2021 Harry Rowe Mimno Award bestowed by IEEE Aerospace and Electronic Systems Society, and 10+ Best Paper Awards from major international conferences. C Zhou, P Liu, P Xu, S Iyer, J Sun, Y Mao, X Ma, A Efrat, P Yu, L Yu, The Washington Times, and New Atlas. Dr. Song is an IEEE Fellow, an Asia-Pacific Artificial Intelligence Association (AAIA) Fellow, an ACM Distinguished Member, and a Full Member of Sigma Xi. Dr. J-MASS; Security Magazine, CXOTech Magazine, Fox News2024. 2017-2020362020He is the editor of ten books, the author of more than 100 articles and the inventor of 2 patents. His research interests include AI/machine learning/big data analytics, cyber-physical systems/internet of things, and cybersecurity and privacy. His research has been sponsored by federal agencies (including National Science Foundation, National Aeronautics and Space Administration, US Department of Transportation, and Federal Aviation Administration, among others) and industry. His research has been featured on popular news media outlets, including IEEE Spectrum, IEEE GlobalSpec's Engineering360, IEEE Transmitter, insideBIGDATA, Association for Uncrewed Vehicle Systems International (AUVSI). including IEEE CPSCom-2019, IEEE ICII 2019, IEEE/AIAA ICNS 2019, IEEE CBDCom 2020, WASA 2020, AIAA/ IEEE DASC 2021, IEEE GLOBECOM 2021 and IEEE INFOCOM 2022. He has been an IEEE Impact Creator since 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7242 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7242</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7242</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-268041828</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.18409v3.pdf" target="_blank">A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the “Cookie Theft” task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics 1 . It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7242.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7242.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructBLIP-7B (Description Recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructBLIP (blip2vicuna-instruct) 7B - Description Recognition Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recognition performance of InstructBLIP-7B on CogBench image description task, measured as recall of annotated entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructBLIP-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model built on BLIP-2 with a frozen image encoder, a Query Transformer (Q-Former) and an LLM (Vicuna-style); only Q-Former updated in instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Image Description (Recognition Score)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual recognition / description</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Generative image description task where model must describe image; Recognition Score = recall of annotated [Entities] present in model-generated description (entity-level recall using embedding similarity threshold).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recognition Score (recall of annotated entities)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>0.94 (Recognition Score; based on annotated reference descriptions in CogBench)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>0.50 (Recognition Score as reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt: "Describe this image in detail."</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Annotated [Description] in CogBench (paper's annotations used as human upper-limit estimate)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>InstructBLIP-7B shows substantially lower recognition than human baseline (0.50 vs 0.94); authors note GPT-4V performs best on recognition overall.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7242.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-VL-Chat (Description Recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-VL-Chat - Description Recognition Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recognition performance of Qwen-VL-Chat on CogBench image description task, measured as recall of annotated entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language chat model with vision-language adapters designed for multimodal understanding and dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not explicitly stated (paper tests Qwen-VL-Chat variant)</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Image Description (Recognition Score)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual recognition / description</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Generative image description task; Recognition Score = recall of annotated entities extracted from model descriptions using noun extraction and embedding similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recognition Score (recall)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>0.94 (Recognition Score; CogBench annotated descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>0.54 (Recognition Score as reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt: "Describe this image in detail."</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Annotated [Description] in CogBench</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Qwen-VL-Chat is the best-performing open-source model on cognition per the paper narrative, but recognition remains far below human level.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7242.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-v1.5-7B (Description Recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA v1.5 - 7B - Description Recognition Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recognition performance of LLaVA-v1.5-7B on CogBench image description task (entity recall).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-v1.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model connecting a vision encoder to an LLM with instruction tuning; improved over LLaVA with additional VQA and academic-oriented data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Image Description (Recognition Score)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual recognition / description</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Generative image description evaluated by entity recall (Recognition Score).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recognition Score (recall)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>0.94 (Recognition Score; CogBench annotated descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>0.51 (Recognition Score as reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Annotated [Description] in CogBench</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Per-case analysis shows LLaVA recognizes fewer entities than GPT-4V and often fails at high-level story inference despite sometimes inferring relationships (e.g., 'woman is mother').</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7242.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-v1.5-13B (Description Recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA v1.5 - 13B - Description Recognition Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recognition performance of LLaVA-v1.5-13B on CogBench (entity recall for image descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-v1.5-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaVA v1.5 architecture scaled to a 13B LLM, instruction-tuned on language-image instruction data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Image Description (Recognition Score)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual recognition / description</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Generative image description evaluated by entity recall.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recognition Score (recall)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>0.94 (Recognition Score; CogBench annotated descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>0.52 (Recognition Score as reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Annotated [Description] in CogBench</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>13B LLaVA improves slightly over the 7B variant but remains well below human recognition performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7242.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mPLUG-Owl-2 (Description Recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>mPLUG-Owl-2 (mplug-owl2-llama2-7b) - Description Recognition Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recognition performance of mPLUG-Owl-2 (LLama2-based 7B variant) on CogBench image description task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mPLUG-Owl-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model with a vision encoder, visual abstractor and language decoder, trained with a two-stage (pretraining + visual instruction tuning) strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Image Description (Recognition Score)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual recognition / description</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Entity-level recall of annotated items in generated descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recognition Score (recall)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>0.94 (Recognition Score; CogBench annotated descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>0.48 (Recognition Score as reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Annotated [Description] in CogBench</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>mPLUG-Owl-2's recognition is slightly below other 7B models; authors highlight differences in design/training as possible causes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7242.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ShareGPT4V-7B (Description Recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ShareGPT4V - 7B - Description Recognition Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recognition performance of ShareGPT4V-7B on CogBench (entity recall in generated descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ShareGPT4V-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model following LLaVA v1.5 design, pretraining/supervised fine-tuning with a large resource of highly descriptive captions; 7B variant tested.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Image Description (Recognition Score)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual recognition / description</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Recall of annotated entities in generated description.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recognition Score (recall)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>0.94 (Recognition Score; CogBench annotated descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>0.57 (Recognition Score as reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Annotated [Description] in CogBench</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>ShareGPT4V-7B shows improved recognition vs many open-source models, possibly due to training on highly descriptive captions; cognition improvements are not commensurate with recognition gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7242.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ShareGPT4V-13B (Description Recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ShareGPT4V - 13B - Description Recognition Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recognition performance of ShareGPT4V-13B on CogBench (entity recall).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ShareGPT4V-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B variant of ShareGPT4V trained with descriptive caption resources and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Image Description (Recognition Score)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual recognition / description</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Entity recall score from generated descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recognition Score (recall)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>0.94 (Recognition Score; CogBench annotated descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>0.60 (Recognition Score as reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Annotated [Description] in CogBench</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>13B ShareGPT4V further improves recognition compared to 7B variant but remains below human level.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7242.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V (Description Recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (gpt-4-visionpreview) - Description Recognition Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recognition performance of GPT-4V on CogBench image description task, measured as recall of annotated entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's multimodal GPT-4 variant with visual understanding capabilities (vision-enabled LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not disclosed</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Image Description (Recognition Score)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual recognition / description</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Entity recall from model-generated descriptions compared to CogBench annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recognition Score (recall)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>0.94 (Recognition Score; CogBench annotated descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>0.77 (Recognition Score as reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Annotated [Description] in CogBench</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4V achieves the best recognition among evaluated LVLMs but still misses many entities vs human reference; cognition (high-level reasoning) gap is larger than recognition gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7242.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructBLIP-7B (VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructBLIP (blip2vicuna-instruct) 7B - VQA (CogBench Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple-choice VQA accuracy of InstructBLIP-7B across CogBench reasoning categories and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructBLIP-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BLIP-2 based VLM with instruction tuning of Q-Former; used for multiple-choice VQA evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Visual Question Answering (Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual reasoning / cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice VQA on CoR-derived questions across eight cognitive reasoning categories; accuracy reported per category and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (proportion correct; chance = 0.25)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Overall accuracy 0.96; per-category: Time 0.98, Location 0.96, Character 0.99, Character Relationship 0.96, Event 0.97, Event Relationship 0.98, Next Moment Event 0.96, Mental State 0.95 (as reported in Table 4; human responses from a healthy 24-year-old male with a Bachelor's degree)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy 0.43; per-category: Time 0.34, Location 0.57, Character 0.47, Character Relationship 0.52, Event 0.36, Event Relationship 0.44, Next Moment Event 0.39, Mental State 0.37 (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot multiple-choice instruction prompt (VQA formatting)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Single human assessor: 'responses of a healthy 24-year-old male with a Bachelor's degree' (as stated in paper for Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>InstructBLIP-7B performs worst among evaluated LVLMs on VQA; large gap versus human performance across all categories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7242.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-VL-Chat (VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-VL-Chat - VQA (CogBench Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple-choice VQA accuracy of Qwen-VL-Chat across CogBench reasoning categories and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language chat model evaluated on CogBench multiple-choice VQA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not explicitly stated in paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Visual Question Answering (Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual reasoning / cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice VQA accuracy across eight reasoning categories.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Overall accuracy 0.96; per-category values as in Table 4 (human single-subject baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy 0.55; per-category: Time 0.64, Location 0.84, Character 0.63, Character Relationship 0.55, Event 0.54, Event Relationship 0.40, Next Moment Event 0.50, Mental State 0.53 (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot multiple-choice instruction</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Single healthy 24-year-old male (paper's described human baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Qwen-VL-Chat is among the stronger open-source models on VQA but still lags far behind the human baseline, particularly on event-related reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7242.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-v1.5-7B (VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA v1.5 - 7B - VQA (CogBench Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple-choice VQA accuracy of LLaVA-v1.5-7B across CogBench reasoning categories and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-v1.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned vision-language model connecting a vision encoder and an LLM; 7B variant evaluated on CogBench VQA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Visual Question Answering (Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual reasoning / cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice VQA across eight cognitive reasoning categories; accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Overall accuracy 0.96 (single-subject human baseline), per-category values in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy 0.52; per-category: Time 0.55, Location 0.80, Character 0.54, Character Relationship 0.54, Event 0.51, Event Relationship 0.37, Next Moment Event 0.47, Mental State 0.53 (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot multiple-choice instruction</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Single healthy 24-year-old male baseline (as reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>LLaVA-v1.5-7B shows moderate performance on lower-level location reasoning but struggles on event-relationship and next-moment event reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7242.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-v1.5-13B (VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA v1.5 - 13B - VQA (CogBench Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple-choice VQA accuracy of LLaVA-v1.5-13B across CogBench categories and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-v1.5-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B LLaVA v1.5 variant instruction-tuned for vision-language tasks; evaluated on CogBench VQA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Visual Question Answering (Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual reasoning / cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice VQA across eight reasoning categories; accuracy reported per category and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Overall accuracy 0.96 (single-subject human baseline); per-category in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy 0.59; per-category: Time 0.68, Location 0.85, Character 0.60, Character Relationship 0.62, Event 0.48, Event Relationship 0.54, Next Moment Event 0.68, Mental State 0.56 (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot multiple-choice instruction</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Single healthy 24-year-old male baseline (paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>13B variant substantially outperforms 7B in many categories, illustrating effect of LLM size on VQA performance, yet still far from human.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7242.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mPLUG-Owl-2 (VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>mPLUG-Owl-2 - VQA (CogBench Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple-choice VQA accuracy of mPLUG-Owl-2 (LLama2-7B variant) across CogBench categories and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mPLUG-Owl-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model that uses modality-collaborative components with a two-stage training approach; 7B variant evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Visual Question Answering (Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual reasoning / cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice VQA across eight cognitive reasoning categories with accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Overall accuracy 0.96 (single-subject human baseline); per-category listed in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy 0.55; per-category: Time 0.57, Location 0.82, Character 0.54, Character Relationship 0.53, Event 0.51, Event Relationship 0.46, Next Moment Event 0.53, Mental State 0.56 (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot multiple-choice instruction</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Single healthy 24-year-old male baseline (paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>mPLUG-Owl-2 performs similarly to other 7B open-source models; location reasoning easier than event-related reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7242.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ShareGPT4V-7B (VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ShareGPT4V - 7B - VQA (CogBench Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple-choice VQA accuracy of ShareGPT4V-7B across CogBench reasoning categories and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ShareGPT4V-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ShareGPT4V 7B variant following LLaVA v1.5 design, trained with descriptive caption datasets, evaluated on CogBench VQA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Visual Question Answering (Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual reasoning / cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice VQA across eight cognitive reasoning categories; accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Overall accuracy 0.96 (single-subject human baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy 0.54; per-category: Time 0.62, Location 0.78, Character 0.63, Character Relationship 0.59, Event 0.50, Event Relationship 0.36, Next Moment Event 0.53, Mental State 0.56 (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot multiple-choice instruction</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Single healthy 24-year-old male baseline (paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>ShareGPT4V-7B shows reasonable recognition-derived benefits for VQA but still trails humans, especially on event-relationship reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7242.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ShareGPT4V-13B (VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ShareGPT4V - 13B - VQA (CogBench Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple-choice VQA accuracy of ShareGPT4V-13B across CogBench categories and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ShareGPT4V-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B ShareGPT4V variant evaluated on CogBench VQA; leverages large-scale descriptive caption data in training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Visual Question Answering (Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual reasoning / cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice VQA across eight cognitive reasoning categories; accuracy per category and overall provided.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Overall accuracy 0.96 (single-subject baseline); per-category in Table 4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy 0.58; per-category: Time 0.70, Location 0.85, Character 0.59, Character Relationship 0.56, Event 0.50, Event Relationship 0.50, Next Moment Event 0.72, Mental State 0.58 (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot multiple-choice instruction</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Single healthy 24-year-old male baseline (paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>13B ShareGPT4V shows marked improvement on Next Moment Event reasoning vs 7B variant, but a clear gap remains to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7242.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e7242.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V (VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (gpt-4-visionpreview) - VQA (CogBench Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple-choice VQA accuracy of GPT-4V across CogBench reasoning categories and overall; achieves the best performance among evaluated LVLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's multimodal GPT-4 variant with visual understanding and strong reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not disclosed</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Visual Question Answering (Multiple-Choice)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>visual reasoning / cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice VQA on CoR-derived questions across eight cognitive reasoning categories; accuracy reported per category and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Overall accuracy 0.96; per-category: Time 0.98, Location 0.96, Character 0.99, Character Relationship 0.96, Event 0.97, Event Relationship 0.98, Next Moment Event 0.96, Mental State 0.95 (single-subject human baseline reported in Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy 0.71; per-category: Time 0.72, Location 0.88, Character 0.79, Character Relationship 0.69, Event 0.68, Event Relationship 0.65, Next Moment Event 0.72, Mental State 0.71 (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot multiple-choice instruction</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Responses of a healthy 24-year-old male with a Bachelor's degree (as stated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4V is the top-performing LVLM in CogBench VQA but still shows a sizable gap to human accuracy (0.71 vs 0.96 overall), particularly on event/event-relationship reasoning which remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>From recognition to cognition: Visual commonsense reasoning <em>(Rating: 2)</em></li>
                <li>VisualCOMET: Reasoning about the dynamic context of a still image <em>(Rating: 2)</em></li>
                <li>MM-Bench: Is your multi-modal model an all-around player? <em>(Rating: 1)</em></li>
                <li>MME: A comprehensive evaluation benchmark for multimodal large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7242",
    "paper_id": "paper-268041828",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "InstructBLIP-7B (Description Recognition)",
            "name_full": "InstructBLIP (blip2vicuna-instruct) 7B - Description Recognition Score",
            "brief_description": "Recognition performance of InstructBLIP-7B on CogBench image description task, measured as recall of annotated entities.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InstructBLIP-7B",
            "model_description": "Vision-language model built on BLIP-2 with a frozen image encoder, a Query Transformer (Q-Former) and an LLM (Vicuna-style); only Q-Former updated in instruction tuning.",
            "model_size": "7B",
            "test_name": "Image Description (Recognition Score)",
            "test_category": "visual recognition / description",
            "test_description": "Generative image description task where model must describe image; Recognition Score = recall of annotated [Entities] present in model-generated description (entity-level recall using embedding similarity threshold).",
            "evaluation_metric": "Recognition Score (recall of annotated entities)",
            "human_performance": "0.94 (Recognition Score; based on annotated reference descriptions in CogBench)",
            "llm_performance": "0.50 (Recognition Score as reported in Table 2)",
            "prompting_method": "Zero-shot instruction prompt: \"Describe this image in detail.\"",
            "fine_tuned": false,
            "human_data_source": "Annotated [Description] in CogBench (paper's annotations used as human upper-limit estimate)",
            "statistical_significance": null,
            "notes": "InstructBLIP-7B shows substantially lower recognition than human baseline (0.50 vs 0.94); authors note GPT-4V performs best on recognition overall.",
            "uuid": "e7242.0",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Qwen-VL-Chat (Description Recognition)",
            "name_full": "Qwen-VL-Chat - Description Recognition Score",
            "brief_description": "Recognition performance of Qwen-VL-Chat on CogBench image description task, measured as recall of annotated entities.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-VL-Chat",
            "model_description": "Vision-language chat model with vision-language adapters designed for multimodal understanding and dialogue.",
            "model_size": "not explicitly stated (paper tests Qwen-VL-Chat variant)",
            "test_name": "Image Description (Recognition Score)",
            "test_category": "visual recognition / description",
            "test_description": "Generative image description task; Recognition Score = recall of annotated entities extracted from model descriptions using noun extraction and embedding similarity.",
            "evaluation_metric": "Recognition Score (recall)",
            "human_performance": "0.94 (Recognition Score; CogBench annotated descriptions)",
            "llm_performance": "0.54 (Recognition Score as reported in Table 2)",
            "prompting_method": "Zero-shot instruction prompt: \"Describe this image in detail.\"",
            "fine_tuned": false,
            "human_data_source": "Annotated [Description] in CogBench",
            "statistical_significance": null,
            "notes": "Qwen-VL-Chat is the best-performing open-source model on cognition per the paper narrative, but recognition remains far below human level.",
            "uuid": "e7242.1",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaVA-v1.5-7B (Description Recognition)",
            "name_full": "LLaVA v1.5 - 7B - Description Recognition Score",
            "brief_description": "Recognition performance of LLaVA-v1.5-7B on CogBench image description task (entity recall).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-v1.5-7B",
            "model_description": "Vision-language model connecting a vision encoder to an LLM with instruction tuning; improved over LLaVA with additional VQA and academic-oriented data.",
            "model_size": "7B",
            "test_name": "Image Description (Recognition Score)",
            "test_category": "visual recognition / description",
            "test_description": "Generative image description evaluated by entity recall (Recognition Score).",
            "evaluation_metric": "Recognition Score (recall)",
            "human_performance": "0.94 (Recognition Score; CogBench annotated descriptions)",
            "llm_performance": "0.51 (Recognition Score as reported in Table 2)",
            "prompting_method": "Zero-shot instruction prompt",
            "fine_tuned": false,
            "human_data_source": "Annotated [Description] in CogBench",
            "statistical_significance": null,
            "notes": "Per-case analysis shows LLaVA recognizes fewer entities than GPT-4V and often fails at high-level story inference despite sometimes inferring relationships (e.g., 'woman is mother').",
            "uuid": "e7242.2",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaVA-v1.5-13B (Description Recognition)",
            "name_full": "LLaVA v1.5 - 13B - Description Recognition Score",
            "brief_description": "Recognition performance of LLaVA-v1.5-13B on CogBench (entity recall for image descriptions).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-v1.5-13B",
            "model_description": "LLaVA v1.5 architecture scaled to a 13B LLM, instruction-tuned on language-image instruction data.",
            "model_size": "13B",
            "test_name": "Image Description (Recognition Score)",
            "test_category": "visual recognition / description",
            "test_description": "Generative image description evaluated by entity recall.",
            "evaluation_metric": "Recognition Score (recall)",
            "human_performance": "0.94 (Recognition Score; CogBench annotated descriptions)",
            "llm_performance": "0.52 (Recognition Score as reported in Table 2)",
            "prompting_method": "Zero-shot instruction prompt",
            "fine_tuned": false,
            "human_data_source": "Annotated [Description] in CogBench",
            "statistical_significance": null,
            "notes": "13B LLaVA improves slightly over the 7B variant but remains well below human recognition performance.",
            "uuid": "e7242.3",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "mPLUG-Owl-2 (Description Recognition)",
            "name_full": "mPLUG-Owl-2 (mplug-owl2-llama2-7b) - Description Recognition Score",
            "brief_description": "Recognition performance of mPLUG-Owl-2 (LLama2-based 7B variant) on CogBench image description task.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "mPLUG-Owl-2",
            "model_description": "Vision-language model with a vision encoder, visual abstractor and language decoder, trained with a two-stage (pretraining + visual instruction tuning) strategy.",
            "model_size": "7B",
            "test_name": "Image Description (Recognition Score)",
            "test_category": "visual recognition / description",
            "test_description": "Entity-level recall of annotated items in generated descriptions.",
            "evaluation_metric": "Recognition Score (recall)",
            "human_performance": "0.94 (Recognition Score; CogBench annotated descriptions)",
            "llm_performance": "0.48 (Recognition Score as reported in Table 2)",
            "prompting_method": "Zero-shot instruction prompt",
            "fine_tuned": false,
            "human_data_source": "Annotated [Description] in CogBench",
            "statistical_significance": null,
            "notes": "mPLUG-Owl-2's recognition is slightly below other 7B models; authors highlight differences in design/training as possible causes.",
            "uuid": "e7242.4",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ShareGPT4V-7B (Description Recognition)",
            "name_full": "ShareGPT4V - 7B - Description Recognition Score",
            "brief_description": "Recognition performance of ShareGPT4V-7B on CogBench (entity recall in generated descriptions).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ShareGPT4V-7B",
            "model_description": "Model following LLaVA v1.5 design, pretraining/supervised fine-tuning with a large resource of highly descriptive captions; 7B variant tested.",
            "model_size": "7B",
            "test_name": "Image Description (Recognition Score)",
            "test_category": "visual recognition / description",
            "test_description": "Recall of annotated entities in generated description.",
            "evaluation_metric": "Recognition Score (recall)",
            "human_performance": "0.94 (Recognition Score; CogBench annotated descriptions)",
            "llm_performance": "0.57 (Recognition Score as reported in Table 2)",
            "prompting_method": "Zero-shot instruction prompt",
            "fine_tuned": false,
            "human_data_source": "Annotated [Description] in CogBench",
            "statistical_significance": null,
            "notes": "ShareGPT4V-7B shows improved recognition vs many open-source models, possibly due to training on highly descriptive captions; cognition improvements are not commensurate with recognition gains.",
            "uuid": "e7242.5",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ShareGPT4V-13B (Description Recognition)",
            "name_full": "ShareGPT4V - 13B - Description Recognition Score",
            "brief_description": "Recognition performance of ShareGPT4V-13B on CogBench (entity recall).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ShareGPT4V-13B",
            "model_description": "13B variant of ShareGPT4V trained with descriptive caption resources and instruction tuning.",
            "model_size": "13B",
            "test_name": "Image Description (Recognition Score)",
            "test_category": "visual recognition / description",
            "test_description": "Entity recall score from generated descriptions.",
            "evaluation_metric": "Recognition Score (recall)",
            "human_performance": "0.94 (Recognition Score; CogBench annotated descriptions)",
            "llm_performance": "0.60 (Recognition Score as reported in Table 2)",
            "prompting_method": "Zero-shot instruction prompt",
            "fine_tuned": false,
            "human_data_source": "Annotated [Description] in CogBench",
            "statistical_significance": null,
            "notes": "13B ShareGPT4V further improves recognition compared to 7B variant but remains below human level.",
            "uuid": "e7242.6",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4V (Description Recognition)",
            "name_full": "GPT-4V (gpt-4-visionpreview) - Description Recognition Score",
            "brief_description": "Recognition performance of GPT-4V on CogBench image description task, measured as recall of annotated entities.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_description": "OpenAI's multimodal GPT-4 variant with visual understanding capabilities (vision-enabled LLM).",
            "model_size": "not disclosed",
            "test_name": "Image Description (Recognition Score)",
            "test_category": "visual recognition / description",
            "test_description": "Entity recall from model-generated descriptions compared to CogBench annotations.",
            "evaluation_metric": "Recognition Score (recall)",
            "human_performance": "0.94 (Recognition Score; CogBench annotated descriptions)",
            "llm_performance": "0.77 (Recognition Score as reported in Table 2)",
            "prompting_method": "Zero-shot instruction prompt",
            "fine_tuned": false,
            "human_data_source": "Annotated [Description] in CogBench",
            "statistical_significance": null,
            "notes": "GPT-4V achieves the best recognition among evaluated LVLMs but still misses many entities vs human reference; cognition (high-level reasoning) gap is larger than recognition gap.",
            "uuid": "e7242.7",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "InstructBLIP-7B (VQA)",
            "name_full": "InstructBLIP (blip2vicuna-instruct) 7B - VQA (CogBench Multiple-Choice)",
            "brief_description": "Multiple-choice VQA accuracy of InstructBLIP-7B across CogBench reasoning categories and overall.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InstructBLIP-7B",
            "model_description": "BLIP-2 based VLM with instruction tuning of Q-Former; used for multiple-choice VQA evaluation.",
            "model_size": "7B",
            "test_name": "Visual Question Answering (Multiple-Choice)",
            "test_category": "visual reasoning / cognition",
            "test_description": "Multiple-choice VQA on CoR-derived questions across eight cognitive reasoning categories; accuracy reported per category and overall.",
            "evaluation_metric": "Accuracy (proportion correct; chance = 0.25)",
            "human_performance": "Overall accuracy 0.96; per-category: Time 0.98, Location 0.96, Character 0.99, Character Relationship 0.96, Event 0.97, Event Relationship 0.98, Next Moment Event 0.96, Mental State 0.95 (as reported in Table 4; human responses from a healthy 24-year-old male with a Bachelor's degree)",
            "llm_performance": "Overall accuracy 0.43; per-category: Time 0.34, Location 0.57, Character 0.47, Character Relationship 0.52, Event 0.36, Event Relationship 0.44, Next Moment Event 0.39, Mental State 0.37 (Table 4)",
            "prompting_method": "Zero-shot multiple-choice instruction prompt (VQA formatting)",
            "fine_tuned": false,
            "human_data_source": "Single human assessor: 'responses of a healthy 24-year-old male with a Bachelor's degree' (as stated in paper for Table 4)",
            "statistical_significance": null,
            "notes": "InstructBLIP-7B performs worst among evaluated LVLMs on VQA; large gap versus human performance across all categories.",
            "uuid": "e7242.8",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Qwen-VL-Chat (VQA)",
            "name_full": "Qwen-VL-Chat - VQA (CogBench Multiple-Choice)",
            "brief_description": "Multiple-choice VQA accuracy of Qwen-VL-Chat across CogBench reasoning categories and overall.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-VL-Chat",
            "model_description": "Vision-language chat model evaluated on CogBench multiple-choice VQA.",
            "model_size": "not explicitly stated in paper",
            "test_name": "Visual Question Answering (Multiple-Choice)",
            "test_category": "visual reasoning / cognition",
            "test_description": "Multiple-choice VQA accuracy across eight reasoning categories.",
            "evaluation_metric": "Accuracy",
            "human_performance": "Overall accuracy 0.96; per-category values as in Table 4 (human single-subject baseline).",
            "llm_performance": "Overall accuracy 0.55; per-category: Time 0.64, Location 0.84, Character 0.63, Character Relationship 0.55, Event 0.54, Event Relationship 0.40, Next Moment Event 0.50, Mental State 0.53 (Table 4)",
            "prompting_method": "Zero-shot multiple-choice instruction",
            "fine_tuned": false,
            "human_data_source": "Single healthy 24-year-old male (paper's described human baseline)",
            "statistical_significance": null,
            "notes": "Qwen-VL-Chat is among the stronger open-source models on VQA but still lags far behind the human baseline, particularly on event-related reasoning.",
            "uuid": "e7242.9",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaVA-v1.5-7B (VQA)",
            "name_full": "LLaVA v1.5 - 7B - VQA (CogBench Multiple-Choice)",
            "brief_description": "Multiple-choice VQA accuracy of LLaVA-v1.5-7B across CogBench reasoning categories and overall.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-v1.5-7B",
            "model_description": "Instruction-tuned vision-language model connecting a vision encoder and an LLM; 7B variant evaluated on CogBench VQA.",
            "model_size": "7B",
            "test_name": "Visual Question Answering (Multiple-Choice)",
            "test_category": "visual reasoning / cognition",
            "test_description": "Multiple-choice VQA across eight cognitive reasoning categories; accuracy reported.",
            "evaluation_metric": "Accuracy",
            "human_performance": "Overall accuracy 0.96 (single-subject human baseline), per-category values in Table 4.",
            "llm_performance": "Overall accuracy 0.52; per-category: Time 0.55, Location 0.80, Character 0.54, Character Relationship 0.54, Event 0.51, Event Relationship 0.37, Next Moment Event 0.47, Mental State 0.53 (Table 4)",
            "prompting_method": "Zero-shot multiple-choice instruction",
            "fine_tuned": false,
            "human_data_source": "Single healthy 24-year-old male baseline (as reported in paper)",
            "statistical_significance": null,
            "notes": "LLaVA-v1.5-7B shows moderate performance on lower-level location reasoning but struggles on event-relationship and next-moment event reasoning.",
            "uuid": "e7242.10",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaVA-v1.5-13B (VQA)",
            "name_full": "LLaVA v1.5 - 13B - VQA (CogBench Multiple-Choice)",
            "brief_description": "Multiple-choice VQA accuracy of LLaVA-v1.5-13B across CogBench categories and overall.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-v1.5-13B",
            "model_description": "13B LLaVA v1.5 variant instruction-tuned for vision-language tasks; evaluated on CogBench VQA.",
            "model_size": "13B",
            "test_name": "Visual Question Answering (Multiple-Choice)",
            "test_category": "visual reasoning / cognition",
            "test_description": "Multiple-choice VQA across eight reasoning categories; accuracy reported per category and overall.",
            "evaluation_metric": "Accuracy",
            "human_performance": "Overall accuracy 0.96 (single-subject human baseline); per-category in Table 4.",
            "llm_performance": "Overall accuracy 0.59; per-category: Time 0.68, Location 0.85, Character 0.60, Character Relationship 0.62, Event 0.48, Event Relationship 0.54, Next Moment Event 0.68, Mental State 0.56 (Table 4)",
            "prompting_method": "Zero-shot multiple-choice instruction",
            "fine_tuned": false,
            "human_data_source": "Single healthy 24-year-old male baseline (paper)",
            "statistical_significance": null,
            "notes": "13B variant substantially outperforms 7B in many categories, illustrating effect of LLM size on VQA performance, yet still far from human.",
            "uuid": "e7242.11",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "mPLUG-Owl-2 (VQA)",
            "name_full": "mPLUG-Owl-2 - VQA (CogBench Multiple-Choice)",
            "brief_description": "Multiple-choice VQA accuracy of mPLUG-Owl-2 (LLama2-7B variant) across CogBench categories and overall.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "mPLUG-Owl-2",
            "model_description": "Vision-language model that uses modality-collaborative components with a two-stage training approach; 7B variant evaluated.",
            "model_size": "7B",
            "test_name": "Visual Question Answering (Multiple-Choice)",
            "test_category": "visual reasoning / cognition",
            "test_description": "Multiple-choice VQA across eight cognitive reasoning categories with accuracy reported.",
            "evaluation_metric": "Accuracy",
            "human_performance": "Overall accuracy 0.96 (single-subject human baseline); per-category listed in Table 4.",
            "llm_performance": "Overall accuracy 0.55; per-category: Time 0.57, Location 0.82, Character 0.54, Character Relationship 0.53, Event 0.51, Event Relationship 0.46, Next Moment Event 0.53, Mental State 0.56 (Table 4)",
            "prompting_method": "Zero-shot multiple-choice instruction",
            "fine_tuned": false,
            "human_data_source": "Single healthy 24-year-old male baseline (paper)",
            "statistical_significance": null,
            "notes": "mPLUG-Owl-2 performs similarly to other 7B open-source models; location reasoning easier than event-related reasoning.",
            "uuid": "e7242.12",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ShareGPT4V-7B (VQA)",
            "name_full": "ShareGPT4V - 7B - VQA (CogBench Multiple-Choice)",
            "brief_description": "Multiple-choice VQA accuracy of ShareGPT4V-7B across CogBench reasoning categories and overall.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ShareGPT4V-7B",
            "model_description": "ShareGPT4V 7B variant following LLaVA v1.5 design, trained with descriptive caption datasets, evaluated on CogBench VQA.",
            "model_size": "7B",
            "test_name": "Visual Question Answering (Multiple-Choice)",
            "test_category": "visual reasoning / cognition",
            "test_description": "Multiple-choice VQA across eight cognitive reasoning categories; accuracy reported.",
            "evaluation_metric": "Accuracy",
            "human_performance": "Overall accuracy 0.96 (single-subject human baseline)",
            "llm_performance": "Overall accuracy 0.54; per-category: Time 0.62, Location 0.78, Character 0.63, Character Relationship 0.59, Event 0.50, Event Relationship 0.36, Next Moment Event 0.53, Mental State 0.56 (Table 4)",
            "prompting_method": "Zero-shot multiple-choice instruction",
            "fine_tuned": false,
            "human_data_source": "Single healthy 24-year-old male baseline (paper)",
            "statistical_significance": null,
            "notes": "ShareGPT4V-7B shows reasonable recognition-derived benefits for VQA but still trails humans, especially on event-relationship reasoning.",
            "uuid": "e7242.13",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ShareGPT4V-13B (VQA)",
            "name_full": "ShareGPT4V - 13B - VQA (CogBench Multiple-Choice)",
            "brief_description": "Multiple-choice VQA accuracy of ShareGPT4V-13B across CogBench categories and overall.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ShareGPT4V-13B",
            "model_description": "13B ShareGPT4V variant evaluated on CogBench VQA; leverages large-scale descriptive caption data in training.",
            "model_size": "13B",
            "test_name": "Visual Question Answering (Multiple-Choice)",
            "test_category": "visual reasoning / cognition",
            "test_description": "Multiple-choice VQA across eight cognitive reasoning categories; accuracy per category and overall provided.",
            "evaluation_metric": "Accuracy",
            "human_performance": "Overall accuracy 0.96 (single-subject baseline); per-category in Table 4",
            "llm_performance": "Overall accuracy 0.58; per-category: Time 0.70, Location 0.85, Character 0.59, Character Relationship 0.56, Event 0.50, Event Relationship 0.50, Next Moment Event 0.72, Mental State 0.58 (Table 4)",
            "prompting_method": "Zero-shot multiple-choice instruction",
            "fine_tuned": false,
            "human_data_source": "Single healthy 24-year-old male baseline (paper)",
            "statistical_significance": null,
            "notes": "13B ShareGPT4V shows marked improvement on Next Moment Event reasoning vs 7B variant, but a clear gap remains to human performance.",
            "uuid": "e7242.14",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4V (VQA)",
            "name_full": "GPT-4V (gpt-4-visionpreview) - VQA (CogBench Multiple-Choice)",
            "brief_description": "Multiple-choice VQA accuracy of GPT-4V across CogBench reasoning categories and overall; achieves the best performance among evaluated LVLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_description": "OpenAI's multimodal GPT-4 variant with visual understanding and strong reasoning capabilities.",
            "model_size": "not disclosed",
            "test_name": "Visual Question Answering (Multiple-Choice)",
            "test_category": "visual reasoning / cognition",
            "test_description": "Multiple-choice VQA on CoR-derived questions across eight cognitive reasoning categories; accuracy reported per category and overall.",
            "evaluation_metric": "Accuracy",
            "human_performance": "Overall accuracy 0.96; per-category: Time 0.98, Location 0.96, Character 0.99, Character Relationship 0.96, Event 0.97, Event Relationship 0.98, Next Moment Event 0.96, Mental State 0.95 (single-subject human baseline reported in Table 4)",
            "llm_performance": "Overall accuracy 0.71; per-category: Time 0.72, Location 0.88, Character 0.79, Character Relationship 0.69, Event 0.68, Event Relationship 0.65, Next Moment Event 0.72, Mental State 0.71 (Table 4)",
            "prompting_method": "Zero-shot multiple-choice instruction",
            "fine_tuned": false,
            "human_data_source": "Responses of a healthy 24-year-old male with a Bachelor's degree (as stated in paper)",
            "statistical_significance": null,
            "notes": "GPT-4V is the top-performing LVLM in CogBench VQA but still shows a sizable gap to human accuracy (0.71 vs 0.96 overall), particularly on event/event-relationship reasoning which remain challenging.",
            "uuid": "e7242.15",
            "source_info": {
                "paper_title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "From recognition to cognition: Visual commonsense reasoning",
            "rating": 2,
            "sanitized_title": "from_recognition_to_cognition_visual_commonsense_reasoning"
        },
        {
            "paper_title": "VisualCOMET: Reasoning about the dynamic context of a still image",
            "rating": 2,
            "sanitized_title": "visualcomet_reasoning_about_the_dynamic_context_of_a_still_image"
        },
        {
            "paper_title": "MM-Bench: Is your multi-modal model an all-around player?",
            "rating": 1,
            "sanitized_title": "mmbench_is_your_multimodal_model_an_allaround_player"
        },
        {
            "paper_title": "MME: A comprehensive evaluation benchmark for multimodal large language models",
            "rating": 1,
            "sanitized_title": "mme_a_comprehensive_evaluation_benchmark_for_multimodal_large_language_models"
        }
    ],
    "cost": 0.01924075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models
29 Feb 2024</p>
<p>Xiujie Song 
Mengyue Wu 
Kenny Q Zhu kenny.zhu@uta.edu 
University of Texas at Arlington
ArlingtonTexasUSA</p>
<p>Chunhao Zhang 
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Yanyi Chen yanyi.chen@rutgers.edu 
Rutgers University
New BrunswickNew JerseyUSA</p>
<p>A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models
29 Feb 20249563CE39BF42AE9C025FB24BC66BE459arXiv:2402.18409v2[cs.AI]
Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities.Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate highlevel cognitive ability of LVLMs using images with rich semantics 1 .It defines eight reasoning capabilities and consists of an image description task and a visual question answering task.Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.</p>
<p>Introduction</p>
<p>Recently, with the emergence of Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023), the cognitive abilities of language models have reached a new level (Zhuang et al., 2023).They demonstrated remarkable performance in many tasks (Bubeck et al., 2023).In Vision Language (VL), several researchers (Zhu et al., 2023b;Liu et al., 2023b;Ye et al., 2023) endeavor to boost Vision Language Pre-trained Models (VLPMs) by integrating powerful LLMs (Touvron et al., 2023;Chiang et al., 2023), referred to as Large Vision-Language Models (LVLMs).With LLM as the "brain", the cognitive abilities of LVLMs are also improved and more challenging tasks, such as table, chart, and document reasoning etc, can be solved (Yang et al., 2023).Some state-of-the-art LVLMs, such as GPT-4V (OpenAI, 2023), are progressing towards human-level cognitive abilities.Thus, there's an increasing interset in evaluating the cognitive abilities of LVLMs.Though some LVLM evaluation benchmarks, such as MME (Fu et al., 2023), MMBench (Liu et al., 2023c), SEED Bench (Li et al., 2023a), etc., also consider cognitive reasoning ability as one aspect of their evaluation, they do not provide a comprehensive evaluation of higher-level reasoning ability via a concentrated task and most of the images they use require relatively little reasoning to understand.However with humans, we have such tasks like Cookie Theft picture description, which is broadly used to assess one's cognitive function.</p>
<p>Among different picture description tasks, the Cookie Theft picture description task (Figure 1) from the Boston Diagnostic Aphasia Examination (Goodglass et al., 2001) has dominated work in speech-language pathology (Cummings, 2019).Surprisingly, this picture has been developed and utilized for more than 50 years and continues to be applied and discussed today.The success of Cookie Theft has sparked our curiosity to investigate the magic behind this picture and explore ways to apply this human-oriented cognitive ability assessment to evaluate cognitive abilities of LVLMs.Cummings (2019) analyzed the reasons for the success of of Cookie Theft.It was found that it contains information of varying degrees of salience and contains more semantic categories.It was also found that during describing Cookie Theft, compared to individuals with cognitive impairments, healthy people tend to demonstrate their coginitive ability by reasoning.For instance, in Figure 1, by comparing the descriptions produced by the healthy man and the woman with probable Alzheimer's dementia, we can identify the following differences:</p>
<p>• The description produced by a healthy man uses "mother" instead of "lady", indicating the reasoning about character relationship.</p>
<p>• The healthy man used "stealing cookies" in-Uh the boy is stealing cookies out of the jar.And he's on a stool that's falling down.And the girl is laughing at him.And the mother is washing dishes but she's preoccupied.And the water is flowing over the sink.And uh there's trouble galore.The wind is blowing the uh curtains.Um ... she is not paying any attention to her kids.And um the boy in addition to stealing he's trying to give the girl a a cookie.And uh the whole thing is going to collapse.Uh but aside from that they seem to be a fairly happy family.</p>
<p>Healthy Probable Alzheimer's Dementia</p>
<p>They're baking.Making a mess out of the place.By not putting by not no neatness.There's a few accidents.The little boy is sta standing on a chair which is crooked.The lady has water running out of her sink.And towel in her hand which is dangerous.Cookies are coming down the cookie jar is coming down.And the place is very pretty.stead of "taking cookies", indicating his reasoning about this high-level event.The description produced by the patient even did not mention this event at all.</p>
<p>• The healthy man also used "the mother is preoccupied" and "happy" to describe people's mental state.</p>
<p>• The description also reflects the causal relationships between events.Because "the mother is preoccupied" and "not paying attention to her kids", "kids are stealing cookies."</p>
<p>Through these reasoning processes, the difference of cognitive abilities between the two individuals is reflected in their descriptions.Such a picture that can evaluate cognitive function is carefully designed and crafted.Tasnim et al. (2022) introduced guidelines for drawing pictures similar to Cookie Theft, which is consistent with findings mentioned above.Generally speaking, compared to typical images, Cookie Theft-like images feature a prominent story theme, richer content, display relationships between components and require stronger cognitive abilities to understand and describe.</p>
<p>The above studies provided us with directions for collecting Cookie Theft-like images and developing assessment criteria to construct a cognitive evaluation benchmark, so that the idea of this human cognition assessment can be leveraged to evaluate cognition of LVLMs comprehensively.To fill this research gap, we propose to construct a Cognitive Evaluation Benchmark, named as Cog-Bench, which consists of high quality Cookie Theftlike images to evaluate the cognitive reasoning ability of LVLMs.CogBench defines eight core cognitive reasoning capabilities, including reasoning about special time, location, character, character relationship, event, event relationship, next moment event and mental state.Both a generative Image Description task and a discriminative Visual Question Answering (VQA) task are designed.</p>
<p>Our main contributions are as follows:</p>
<p>• To the best of our knowledge, this is the firstof-its-kind attempt to incorporate the cognitive test Cookie-Theft picture description task designed for humans into the evaluation of LVLMs.</p>
<p>• We created the first ever and the largest to-date VL dataset with Cookie Theft-like images to evaluate LVLM's cognitive abilities.</p>
<p>• Our evaluation on existing well-known LVLMs shows that there is still a large gap between the cognitive ability of LVLMs and human beings, indicating CogBench is a challenging evaluation benchmark.</p>
<p>Dataset Construction</p>
<p>In this section, we will introduce the construction of CogBench.We will first introduce image collection, annotation and the statistics of CogBench.Then, we will introduce tasks in CogBench.</p>
<p>Image Collection</p>
<p>Based on findings of previous studies (Cummings, 2019;Tasnim et al., 2022), we set the following image collection criteria:</p>
<p>• Rule 1: Stroy-telling The image depicts an interesting story clearly.For instance, the Cookie Theft picture tells the story of a mother busy washing dishes while two kids takes the opportunity to stand on a stool and sneakily steal cookies.</p>
<p>• Rule 2: Rich Chain-of-Reasonings Images should display rich Chain-of-Reasonings (CoRs) in a scene.A CoR connects low-level observations in an image to produce a highlevel reasoning conclusion or connects the cause and effect of events.For example, "The mother is busy washing dishes.+ The boy is standing on the stool behind the mother.+ The girl standing by the boy is shushing him.+ The boy is fetching cookies from the jar in the carbinet.→ The boy and girl are stealing cookies." is a CoR about high-level event "stealing cookies".Note that the story is actually constructed via these CoRs.</p>
<p>• Rule 3: Content Complexity Restriction Images should contain rich content but not be overly complex.The number of subjects should be limited for better emphasis on the key points of the story, facilitating description by humans or models.</p>
<p>Images in CogBench (Ours)
(a) (c) (d) (e) (f)
Images for visual reasoning from previous datasets.With the above criteria, we aim to select highquality images for cognition evaluation of LVLMs.Most of the images are manully collected from Pinterest 3 , and the Cookie Theft is also included in CogBench.Figure 2 illustrates the differences between our images and those from other datasets.</p>
<p>Image Annotation</p>
<p>Human annotators, mostly undergraduate or graduate students aged 18-28, are hired to annotate the 3 https://www.pinterest.com/collected images.As shown in Figure 3, the annotation includes three parts: [Entities], [CoRs] and [Description].</p>
<p>By annotating [Entities] and [CoRs], we aim to evaluate the low-level recognition ability and high-level cognitive reasoning ability of models respectively based on description.[Description] is annotated as the reference description for the image.The three parts are annotated in a sequential order.</p>
<p>Entity Annotation We ask annotators to list as many entities in the image as possible and entities that are difficult to recognize should be omitted.</p>
<p>CoR Annotation</p>
<p>In order to evaluate model cognition in a fine-grained manner, the following eight reasoning capabilities are annotated with CoRs:</p>
<p>• Special Time Reasoning: reasoning about the special time of the story in the image, e.g.festivals, seasons etc.</p>
<p>• Location Reasoning: reasoning about the location of the story in the image, e.g.near the school etc.</p>
<p>• Character Reasoning: reasoning about the character of the subjects in the image, e.g.police officer etc.</p>
<p>• Character Relationship Reasoning: reasoning about relationship between characters in the image, e.g."the woman is the mother of the kids."</p>
<p>• Event Reasoning: reasoning about high-level events happened in the current moment and previous moments in the image based on clues in the picture.The difference between highlevel and low-level lies in how much semantic information the event contains, e.g."stealing cookies" is a higher-level event compared to "taking cookies" as it additionally conveys the semantic of "taking advantage without permission or knowledge."</p>
<p>• Event Relationship Reasoning: reasoning about causal and temporal relationship between different events in the image.For instance, "the sink is overflowing because the mother left the tap on."</p>
<p>• Next Moment Event Reasoning: reasoning about the event that will happen in the next moment.For example, "The police officer will reprimand the boy who violates the rules."</p>
<p>[Description]:</p>
<p>In a house, a woman is wearing a dress and holding a handbag and man is wearing a suit and holding a hat.They are walking outward.They are likely a couple and about to attend a formal event.A boy extends both hands and runs towards his mother.He wants to hug his mother.He is chasing his parents and wants to go out with his mom.The mother is bending down and placing her hands on the boy's forehead.She is preventing the boy from following them outside.The boy is probably disappointed.His sister is sitting on a sofa in the living room and the TV is on, indicating she was watching TV.Perhaps curious about what's happening, she is looking at the boy.Next, the boy will stay at home with his sister.</p>
<p>[Location Reasoning]: … [Character Relationship Reasoning]:</p>
<p>A man, a woman, a boy and a girl are in a house.→ The woman is the mother …</p>
<p>… [Event Reasoning]:</p>
<p>The woman is wearing ... + The man is wearing ... + They are walking outward.+ The boy extends both hands and runs towards his mother.+ The mother is bending down and placing her hands on the boy's forehead.→ Mom is preventing the boy from following them outside.</p>
<p>… [Event Relationship Reasoning]:</p>
<p>The woman is wearing ... + … + They are walking outward.+ The boy extends both hands ... + The parents had no intention of taking the boy out.→ The mother is bending down and placing her hands on the boy's forehead.</p>
<p>… [Next Moment Event Reasoning]:</p>
<p>The woman is wearing ... + ... + They are walking outward.→ The man and the woman are about to attend a formal event.Considering different people may have different understanding about some images, we ask three annotators to annotate each image.Then, we manually merge the three annotations as one final annotation mainly based on the principle of majority vote.For the [Description], if two or more annotators understand the story in the image in the same way, we accept one of the best descriptions as the final description.For [Entities] and [CoRs], we first accept most of the entities and CoRs that are annotated by at least two annotators.Other entities and CoRs are also included if they are reasonable.</p>
<p>CogBench Statistics</p>
<p>To build CogBench, we have gathered 95 images with 1041 entities, 881 CoRs, 95 descriptions and 1091 questions.Table 1 shows the distribution of these CoRs and questions.Though the number of images in CogBench is not large, the content contained in each picture is very rich.The number of CoRs in event-related reasoning and [Mental State Reasoning] is large, which is a manifestation of the rich interesting stories in the images.</p>
<p>Tasks in CogBench</p>
<p>To comprehensively evaluate cognitive abilities of LVLMs, we designed a generative Image Description task and a discriminative Multiple-Choice Question Answering task in CogBench.</p>
<p>Image Description Task</p>
<p>Inspired by Cookie Theft picture description task, we propose to assess the cognitive abilities of LVLMs through an Image Description task using Cookie Theft-like images.Recently, some researchers (Xie et al., 2022;Zhu et al., 2023a;Zhuge et al., 2023) are also trying to improve model performance on image description task.The difference between our description task and existing image description task is that we expect LVLMs understand and describe the story in the image through high-level cognitive reasoning.For instance, in Figure 3, the description of the image should not only include what is in the picture but also focus on elucidating the story of "parents are going out to attend a formal event, and the mother is refusing the boy to accompany them" through a series of reasonings.</p>
<p>Visual Question Answering Task</p>
<p>All questions in VQA task are Multiple-Choice Questions with four options, which simplifies evaluation.Similar to Description task, the questions in VQA task are also related to high-level cognitive reasoning.For example, in Figure 4, we can ask question about the event "What is the woman doing?" or question about the cause of the event "Why is the mom placing her hands on the boy's forehead?",which require models to answer with  reasoning.</p>
<p>We use a semi-automatic GPT-assisted approach to generate questions for images based on [CoRs] we annotated in the previous section, referred to as CoR-based GPT-assisted Question Generation.The main idea of the question generation is that for each CoR, we can ask questions about both the conclusion (right part of →) and reasons to draw the conclusion (left part of →), as shown in Fiure 4. The process can be divided into two stages.</p>
<p>In the first stage, we use GPT-4 (OpenAI, 2023) to generate questions for images in CogBench.For each category of reasoning capability, we design a sepcific prompt for GPT-4 to generate questions, so that questions related to different types of CoRs can be generated.For Question Generation, the key point is to generate high quality distractors.Thus, we encourage GPT-4 to hallucinate to generate more perplexing distractors in our prompt.Appendix B shows an example of prompt we use for GPT-4 to generate questions.</p>
<p>Though GPT-4 is powerful, it is still difficult for it to generate 100% satisfying questions for all images.Thus, in the second stage, we manually select and modify the questions generated by GPT-4.The principle of selection and modification is that the question cannot apparently point to the correct option and distractors should be as closely related to the question and the correct option as possible, thereby possessing a certain level of misleadingness.During selection, ChatGPT is also utilized to help detect those simple questions that can be easily answered without having to accept the image as input.</p>
<p>Evaluation Strategy of CogBench</p>
<p>In this section, we will introduce the evaluation strategy of tasks in CogBench.</p>
<p>Evaluation for Description Task</p>
<p>For the evluation of Description task, we consider model performance comprehensively on two levels: low-level Recognition ability and high-level Cognition ability.Evaluation metrics for both levels are calculated based on recall score, referred to as Recognition Score and Cognition Score, respectively.</p>
<p>The Recognition Score is calculated as the ratio of the number of recognized [Entities] to the number of annotated [Entities] in all images.</p>
<p>First, we use SpaCy4 to extract nouns from the model generated description, and then calculate cosine similariy between embeddings5 of annotated [Entities] and extracted nouns.For each entity, if the cosine similarity score between the entity and any noun is greater than a threshold (0.5 in this paper), we consider the entity is recognized by the model.</p>
<p>For Cognition Score, we first calculate a score for each of the eight cognitive reasoning abilities, and then compute an overall score.</p>
<p>We use GPT-4 (OpenAI, 2023) to help calculate the Cognition Score.To avoid the interpretability issues of GPT-based evaluation, instead of using a subjective evaluation method of directly comparing two descriptions, we use GPT-4 to perform a easier, more objective and fine-grained binary classification task, that is, judging whether the generated description contains the semantics of annotated CoRs.Details and prompts of this GPT-based evaluation are shown in Appendix C.</p>
<p>After obtaining the score of each CoR, we calculate a recall score for each reasoning capabilities.The overall Cognition Score is calculated by summing up the scores of all CoRs and dividing by the total number of CoRs.The effectiveness analysis of GPT-based cognition evaluation is shown in Appendix F.</p>
<p>Evaluation for VQA Task</p>
<p>For Multiple-Choice Questions in VQA task, we use accuracy as the evaluation metric.As questions are generated based on CoRs, we can also calculate both the accuracy for each reasoning capability and the overall accuracy.</p>
<p>Experiments</p>
<p>In this section, we will introduce selected LVLMs for evaluation, experiment results and analysis.</p>
<p>Large Vision Language Models</p>
<p>InstructBLIP-7B (Dai et al., 2023), Qwen-VL-Chat (Bai et al., 2023), LLaVA-v1.5-7Band LLaVA-v1.5-13B(Liu et al., 2023a), mPLUG-Owl-2 (Ye et al., 2023), ShareGPT4V-7B and ShareGPT4V-13B (Chen et al., 2023a) and GPT-4V (OpenAI, 2023) are selected in this paper to be evaluated using CogBench.A brief introduction of these models is shown in Appendix D.</p>
<p>Results of Description Task</p>
<p>We prompt the selected LVLMs with the following instruction to obtain detailed descriptions about images in CogBench.</p>
<p>Describe this image in detail.</p>
<p>Then, we evaluate the performance of LVLMs on Description task in terms of both recognition and cognition ability.</p>
<p>As a reference, we also calculated traditional image captioning evaluation metrics by camparing annotated reference description and model-generated description, and details are shown in Appendix E. for training, which makes it describe more entities shown in images.Though GPT-4V achieves the best performance, there are still a lot of entities that are not recognized by GPT-4V, indicating a room for improvement.</p>
<p>Recognition</p>
<p>Cognition</p>
<p>Case Study of Description Task</p>
<p>Figure 5 shows a case of Description task.In terms of recognition, GPT-4V shows a good performance by recognizing most annotated entities ("boy", "girl", "man", "woman", "table", "dress", "hat", "suit") and only fails to recognize the "sofa", "television", "handbag".LLaVA-v1.5-13Bobviously recognizes fewer entities than GPT-4V, it only recognizes "boy", "woman", and "dress".However, GPT-4V fails to understand the story in the image and gets a 0 in terms of cognition.This is because it neither clarifies the location nor the relationships between the characters, nor does it accurately depict the events occurring in the image, etc. LLaVA-v1.5-13B is slightly better and successfully inferred that the woman is the mother.</p>
<p>The case shows CogBench can reflect that current LVLMs fail at aspects like recognition and cognition and the cognitive abilities exhibited in the description still have some gap with the level of humans.</p>
<p>Results of VQA Task</p>
<p>Table 4 shows the performance of LVLMs on VQA task.Consistent with results in previous sections, GPT-4V achieves the best performance and there is a performance gap between GPT-4V and open-source models.For open-source models, ShareGPT4V-13B and LLaVA-v1.5-13Bachieves better performance than other LVLMs based on 7B LLMs, which shows the importance of LLM size to LVLMs.As for LVLMs based on 7B LLMs, the performance of InstructBLIP is the worst and there is also a gap between InstructBLIP and other models.One possible reason is that it only finetunes the Q-former for instruction-tuning as Q-former has limited capacity compared with LLMs.The performance of other models are similar.Consistent with previous findings, reasoning about location is also the easiest for LVLMs and event-related reasoning are more difficult.There is also a large gap between the performance of LVLMs and human beings.Note that the accuracy of Human in Table 4 is calculated based on the responses of a healthy 24-year-old male with a Bachelor's degree.</p>
<p>Related Work</p>
<p>Evaluation Benchmark for LVLMs.To better understand emerging capabilities of LVLMs, many scholars are dedicated to proposing different evaluation benchmarks.LVLM-eHub (Xu et al., 2023) evaluates LVLMs' multimodal capabilities across six categories using various publicly available computer vision datasets.MME (Fu et al., 2023), MM-Bench (Liu et al., 2023c) and SEED Bench (Li et al., 2023a) use True/False Questions or Multiple-Choice Questions to evaluate different abilities of LVLMs.MM-VET (Yu et al., 2023) evaluates LVLMs in terms of their integrated VL capabilities via open-ended questions.Different from them, CogBench mainly focus on high-level cognition evaluation of LVLMs.Though some of them also consider cognition as one of the evaluation dimensions, most images they use can only be used to evaluate limited aspects of reasoning.</p>
<p>Image Captioning.Image Captioning is a classical VL task.As model capabilities advance, researchers strive to enhance their ability to describe images in detail.Krause et al. (2017) propose Image Paragraph Captioning, tasking models with generating a descriptive paragraph for an image.Recently, some researchers (Xie et al., 2022;Zhu et al., 2023a;Zhuge et al., 2023;Chen et al., 2023a) are trying to leveraging the ability of LLMs to generate more detailed image descriptions.None of This image features an illustration of four characters in a vintage style, reminiscent of the mid-20th century.On the left side, a young girl in a pink dress with a bow in her hair is seated at what appears to be a table or booth, observing the scene.She seems somewhat disengaged from the drama unfolding next to her.The central figure is a woman dressed in a fancy, white, full-skirted dress with a black top that leaves her shoulders bare.She's bending forward with a surprised look on her face as if she's trying to avoid something.Her right arm is extended outward with an open hand, gesturing in refusal or to keep her balance.In her left hand, she holds a pair of white gloves, and she is wearing yellow high-heeled shoes.The woman has short blonde hair styled in a typical fashion of the 1950s era.To her right, a little boy in red shorts and a horizontally striped t-shirt, with a bandage on his right knee, is reaching out toward her with his arms in an expressive, perhaps pleading gesture.His mouth is open as if he's saying something, and his expression conveys earnestness or longing.On the far right stands a man in a formal black suit and bowtie, holding a green top hat and white gloves in his right hand, while his left hand is in his pocket.He appears bemused by the scene, looking down at the boy with a slight smirk on his face.His posture is relaxed and he seems to be distanced from the immediate action between the woman and the boy.The setting includes a blue carpeted floor, a light blue wall, and a yellow draped curtain on the left side.There's also a small plant and a picture hanging on the wall in the background.The artwork is rendered in a realistic style with attention to detail and a vibrant color palette.It has a signature in the bottom right corner which suggests it was painted by an artist named "John."The scene depicted has a playful, dramatic quality and seems to suggest a narrative, perhaps from a magazine or advertising illustration of that time.The illustration style and clothing hint at Americana and lifestyle from the 1950s.</p>
<p>The image is a painting or drawing of a woman in a white dress, reaching down to help a young boy.The woman appears to be a motherly figure, assisting the child in a caring manner.The boy is wearing a red shirt, and the woman is holding his hand.In the background, there are two other people present, one on the left side and another on the right side of the image.A chair is located on the left side of the scene, and a potted plant can be seen on the right side.The overall scene conveys a sense of warmth and togetherness.these tasks consider to evaluate the high-level cognitive ability of models through description.HL dataset (Cafagna et al., 2023) requires models to generate high-level captions, but it only considers three aspects (scene, action, rationale).The content of most images in existing datasets does not reach the level of a story.This reveals the need for higher-level datasets like CogBench.</p>
<p>Recognition</p>
<p>Visual Reasoning.Visual Reasoning is closely related to the cognitive ability of models.VCR (Zellers et al., 2019) tasks models with answering visual questions using commonsense reasoning and justifying their answers.VisualCOMET (Park et al., 2020) is a framework of visual commonsense reasoning tasks to predict past, future events, and present intents.Hessel et al. (2022) utilize images from VCR and Visual Genome (Krishna et al., 2017) to evaluate the ability of models to perform abductive reasoning.Fu et al. (2022) propose a task to identify the time and location of a given image.CURE (Chen et al., 2023b) is proposed to measure both the zero-shot reasoning performance and consistency of VLMs.Similarly, compared CogBench, these tasks considers less kinds of reasoning and CogBench can be seen as the next step of these efforts.</p>
<p>Conclusion</p>
<p>In this paper, we incorporated the idea of Cookie Theft picture description task into the evaluation of the high-level cognitive abilities of LVLMs and designed a novel evaluation benchmark called Cog-Bench.Images in CogBench are of high quality and contain rich cognitive reasoning, which makes it different from existing image datasets.Experiments show that there is still a large gap between the cognitive ability of LVLMs and human beings, indicating CogBench is a challenging benchmark.</p>
<p>Limitations</p>
<p>Compared to some large-scale benchmarks, Cog-Bench is a relatively light-weight benchmark with fewer images due to our strict image collection criteria.We aim to substitute quantity with highquality images and annotations containing rich semantic information.In the future, we will keep updating CogBench to include more high-quality images and annotations.</p>
<p>Ethical Considerations</p>
<p>Most images in CogBench are manually collected from Pinterest.We follow the terms of service of Pinterest to collect these images.The images are used as fair use for research purposes only.We will share the data with other researchers who will follow the ethical considerations as established in this study.As for annotation, we pay our annotators a fair wage and will answer any questions they have during their annotation process promptly.They are free to take breaks or quit the annotation task at any time.</p>
<p>A Image Annotation Instruction</p>
<p>Figure 6 shows the image annotation instruction for annotators of CogBench.</p>
<p>B Prompt of CoR-based GPT-assisted Question Generation</p>
<p>C Prompt of GPT-based Description Cognition Evaluation</p>
<p>Figure 8 and Figure 9 shows prompts of ChatGPT or GPT-4 for cognition evaluation of Description task.</p>
<p>For CoR types other than [Event Relationship Reasoning], we task GPT-4 with determining whether the conclusion in each CoR is mentioned in the description.The prompt is shown in Figure 8.For [Event Relationship Reasoning], we task GPT-4 with determining whether the causal relationship between events (i.e. the whole CoR), as annotated, is present in the description.The prompt is shown in Figure 9.</p>
<p>D Introduction to Selected LVLMs</p>
<p>• InstructBLIP (Dai et al., 2023) builds upon BLIP-2 (Li et al., 2023b).It consists of an image encoder, a LLM, and a Query Transformer (Q-Former).During instruction tuning, only the Q-Former is updated.We use "blip2vicuna-instruct" for testing.</p>
<p>• Qwen-VL-Chat (Bai et al., 2023)   vision-language adapter.We test "Qwen-VL-Chat".</p>
<p>• LLaVA v1.5 (Liu et al., 2023a) is an upgraded version of LLaVA (Liu et al., 2023b).</p>
<p>LLaVA connects a vision encoder and LLM for general-purpose visual and language understanding.It is instruction-tuned on the language-image instruction-following data generated by language-only GPT-4 (OpenAI, 2023).By using CLIP-ViT-L-336px with an MLP projection and adding academic-taskoriented VQA data with simple response formatting prompts, LLaVA v1.5 achieves better performance."llava-v1.5-7b"and "llava-v1.5-13b"are tested.</p>
<p>• mPLUG-Owl-2 (Ye et al., 2023) mainly comprises a fundamental vision encoder, a visual abstractor, and a language decoder.It also adopts a two-stage training strategy, comprising pre-training and visual instruction tuning.We test "mplug-owl2-llama2-7b".</p>
<p>• ShareGPT4V (Chen et al., 2023a) follows the design of LLaVA v1.5.They incorporate a large-scale resource featuring highly descriptive captions into both the pretraining and supervised fine-tuning phases of ShareGPT4V model.We test"ShareGPT4V-7B" and "ShareGPT4V-13B".</p>
<p>• GPT-4V (OpenAI, 2023) is one of the most powerful LVLMs in the world developed by OpenAI.The version of "gpt-4-visionpreview" is tested.</p>
<p>Table 5 shows an overview of the design of different LVLMs.</p>
<p>You are going to see some pictures.Each picture tells a story and requires different kinds of reasoning to fully understand the story behind it.You will be first asked to identify the entities and reasoning processes in the picture.Then, you will need to describe the story of the picture based on your identified entities and reasoning processes.</p>
<p>First, you will be asked to identify the entities in the picture.The annotation format is [A, B, C], where A, B, C are entities.</p>
<p>[Entities]: Please list all entities appearing in the picture, including people, animals, objects etc.You are encouraged to list as many entities as possible.Note that these entities need to be in your picture description afterwards.For entities that are difficult to recognize, please do not list them here or describe them.</p>
<p>Then, you will be asked to identify different reasoning processes in the picture.The annotation format should follows the structure [A1 + A2 -&gt; B], where A1 and A2 are premises and B is the conclusion.Note that if you write a conclusion, there must be at least one premise.Do not write a conclusion only, like [B].Please write one conclusion at a time, do not write a reasoning process like [A1-&gt;B-&gt;C], which should be split into two.Each picture does not necessarily requires all the reasoning.Please write None, if a picture does not involve a specific kind of reasoning or it is not important in the picture.</p>
<p>[Special Time Reasoning] Please write your reasoning processes about the special time of the story in the picture, e.g.festivals, seasons etc.The special time is usually relevant to the story of the picture.For instance, if it is daytime in a picture, it is easily recognized and requires no reasoning and there is nothing special, you can write None.However, if there is a lamp on or a clock indicating a specific time, you can write down your reasoning about time.</p>
<p>[Location Reasoning] Please write your reasoning processes about the location of the story in the picture, e.g.near the school.</p>
<p>[Character Reasoning]: Please write your reasoning processes about the character of the subjects in the picture, e.g.teacher, doctor etc.</p>
<p>[Character Relationship Reasoning]: Please write your reasoning processes about relationship between characters in the picture, e.g.friendship.</p>
<p>[Event Reasoning]: Please write your reasoning processes about events happened in the current moment and previous moments in the picture based on clues in the picture.Note that you only need to annotate those high-level events and you can ignore those low-level events.For instance, "the woman is looking at the man" is a low level event and you can ignore its reasoning process.Differently, the reasoning process [A mother is busy cooking.+ A boy is fetching cookies behind the mom.+ A girl is shushing the boy.-&gt; The boy is stealing cookies.] is a reasoning about high-level event stealing and you should write it down.</p>
<p>[Event Relationship Reasoning]: Please write your reasoning processes about relationship between different events in the picture.These events are usually linked through causal and temporal relations.Note that events in this part do not necessarily appears in the [Event Reasoning] part as some events here are low-level events.</p>
<p>[Next Moment Event Reasoning]: Please write your reasoning processes about the event that will happen in the next moment.Note that you only need to write down events that have a very high probability of happening, instead of guessing what might happen next.</p>
<p>[Mental State Reasoning]: Please write your reasoning processes about mental state of subjects in the picture, e.g.daydreaming, happy etc.You need to reason as best you can about the mental states of all the subjects in the picture, unless they are not showing obvious emotions.</p>
<p>Finally, you will be asked to describe the picture in as much detail as you can.</p>
<p>[Description]: Please describe all you see in the picture in a paragraph based on the entities and reasoning processes you identified above, ensuring that all of them are included in your description.Each picture has a story behind it and you need to tell that story through your description.</p>
<p>Figure 6: Image annotation instruction for annotators.</p>
<p>We have a description of an image and the description tells a detailed story unfolding in the image.In the process of describing an image, it is often necessary to engage in reasoning about events based on the clues within the image, leading to certain conclusions.For example, when we see the wind is blowing outside, and a man is reading a newspaper in the telephone booth, we can infer that he is actually hiding from the wind in the telephone booth.Therefore, in this task, in addition to the image description, the reasoning processes about event within the image description have also been extracted.For each reasoning process, we use A1+A2+...-&gt;B to represent it, where A1, A2, ... are clues we observed in the picture and B represents the conclusion about event we inferred.Thus, given an image description and the reasoning processes about event, our task is: 1) Generate a question based on reasoning processes about event.</p>
<p>2) Generate four options: A, B, C, and D. There is only one correct answer among the four options, which is consistent with the description and reasoning processes provided.The correct answer option should be randomly chosen from A, B, C, and D. For those incorrect options (distractors), you are encouraged to hallucinate some clues that are highly relevant to the question and the description but do not actually consistent with the description.That is, you can distort the facts in the description and reasoning processes using elements related to the question to generate some easily selectable distractors.It would be better if you can generate some distractors that are similar to but different from the correct option.Please avoid situations where the correct option is significantly longer or shorter than the distractors.-For example, if the description is "There are some snow on the ground and it is windy, ... We can see it is cold.Inside a phone booth, a man is smiling while looking at a newspaper.He is sheltering from the cold wind in the phone booth..." and the question is "Why can we tell that the man is seeking shelter for warmth?",you can use "newsstand", which is related to "seeking shelter for warmth" in the question, to distort the fact in description "in a phone booth."Then you can get "the man is in the newsstand."Similarly, you can hallucinate a question related distractor "it is raining and a man is smiling and reading a newspaper in a phone booth," which is similar to the correct option "it is windy and a man is smiling and reading a newspaper in a phone booth," but different from it and inconsistent with the description.</p>
<p>-3) Generate the the letter corresponding to the correct answer, that is A, B, C, or D.</p>
<p>Here are some examples: ----[Description]:</p>
<p>There are some snow on the ground and it is windy, indicating it is winter.There are two men and two women standing on the roadside.There is a sign that says "NO STANDING BUS STOP", indicating it is near a bus stop.A man is standing on the road side, wrapping his coat tightly around himself, and peering out onto the road.They are probably waiting for a bus here.We can see it is cold.Inside a phone booth, a man is smiling while looking at a newspaper.He is sheltering from the cold wind in the phone booth.He looks happy, because it is warm there.Two women are also wrapping their coats tightly and looking at the man in the phone booth.They are probably friends and standing together.They are unhappy with the man.There are some buildings by the road.</p>
<p>[Event Reasoning]: It is windy and cold.+ A man is standing in a phone booth reading newspaper.-&gt; The man is sheltering from the cold wind in the phone booth.</p>
<p>[Generated Multiple-Choice Questions]: What is the man doing in the phone booth? A. Making a phone call.B. Reading a book.C. Avoiding someone he doesn't want to see.D. Sheltering from the cold wind.Correct Answer: [D] Why can we tell that the man is seeking shelter for warmth? A. It is windy and a man is smiling and reading a newspaper in a newsstand.B. It is raining and a man is smiling and reading a newspaper in a newsstand.C. It is windy and a man is smiling and reading a newspaper in a phone booth.D. It is raining and a man is smiling and reading a newspaper in a phone booth.Correct Answer: [C] ----Please: 1).Generate at least one question for each reasoning process.2).Generate more diverse questions, try to generat questions from different perspectives or angles and don't limit yourself to the question templates provided in the examples.3).Avoid generating repetitive questions with similar meanings.</p>
<p>Figure 1 :
1
Figure1: Cookie Theft picture description task.The descriptions in the green frame and the orange frame were respectively produced by a 75-year-old healthy man and a 66-year-old woman with probable Alzheimer's dementia 2 .</p>
<p>Figure 2 :
2
Figure 2: The comparison between our images and those from the previous visual reasoning tasks.Compared to our images, image (a) has fewer entities and CoRs, image (b) and (c) have some entities, but fewer CoRs.</p>
<p>Figure 3 :
3
Figure 3: An example of Description task from CogBench.</p>
<p>Figure 4 :
4
Figure 4: An example of VQA task from CogBench.</p>
<p>Figure 5 :
5
Figure 5: Case study of the Description task.A representative open-source LVLM LLaVA-v1.5-13B(red frame) and GPT-4V (green frame) are selected for analysis.Recognized entities are marked in blue, and CoRs are marked in green.</p>
<p>Figure 7
7
Figure 7 shows an example prompt of CoR-based GPT-assisted Question Generation for GPT-4.This prompt is used to generate questions based on [Event Reasoning] CoRs.Prompts for other CoR types are similar to this one.</p>
<p>Figure 7 :
7
Figure 7: An example prompt of CoR-based GPT-assisted Question Generation for GPT-4 to generate questions based on [Event Reasoning].</p>
<p>Table 1 :
1
Distribution of CoRs and questions in CogBench.
Time Location CharacterCharacter RelationshipEventEvent RelationshipNext Moment EventMental StateCoR 23794210224517256162QA47947015925816874221
What is the woman doing? A. The woman is measuring whether the boy has a fever.B.The woman is going to hug his son.C.The woman is preventing the boy from going out with them.D.The woman is touching the boy's forehead, expressing love for him.Correct Answer: [C] [Event Reasoning]:The woman is wearing ... + ... + The boy extends both hands and runs towards his mother.+ The mother is bending down and placing her hands on the boy's forehead.→ Mom is preventing the boy from following them outside.</p>
<p>Table 2 :
2
Table 2 shows the Recognition Score of models on Description task.GPT-4V achieves the best performance in terms of recognition, which means it can recognize and describe more entities.Besides, it is significantly better than other open-source LVLMs, indicating open-source LVLMs still have some room for development before reaching the recognition capability of GPT-4V.ShareGPT4V-7B and ShareGPT4V-13B achieves better performance than other open-source LVLMs.As it follows the design of LLaVA v1.5, one possible reason is that ShareGPT4V uses a high-quality imagetext dataset featuring highly descriptive captions Recognition score of LVLMs on Description task.For reference, the Recognition Score of Human is calculated based on the annotated [Description] in CogBench as an estimate of an upper limit score.
ModelRecognition ScoreInstructBLIP-7B0.50Qwen-VL-Chat0.54LLaVA-v1.5-7B0.51LLaVA-v1.5-13B0.52mPLUG-Owl-20.48ShareGPT4V-7B0.57ShareGPT4V-13B0.60GPT-4V0.77Human0.94</p>
<p>Table 3
3shows Cognition Scores of LVLMs onDescription task. Similarly, GPT-4V achievesthe best performance and there is a large per-formance gap between GPT-4V and other open-source models. For open-source models, Qwen-VL-Chat achieves the best performance. ThoughShareGPT4V achieves better recognition perfor-mance than other open-source LVLMs, it's cog-nition performance does not show a significantimprovement compared with other models. Interms of different reasoning capabilities, all of theLVLMs show better performance on [Location Rea-soning] than others, which is probably because itis a kind of relatively lower-level reasoning. Differ-ently, for [Event Reasoning], [Event RelationshipReasoning], and [Next Moment Event Reasoning],all of the open-source LVLMs show very low per-formance, indicating they almost do not understandthe story in the images at all. The significantly largeperformance gap on these three kinds of reasoningtypes between GPT-4V and open-source LVLMscould be a manifestation of the capabilities emerg-ing in GPT-4V. Though GPT-4V achieves the bestperformance, there is also a huge gap between thecognitive ability of LVLMs and human beings, andthe gap is obviously larger than in the aspect ofrecognition. This indicates that LVLMs still have alot of room for development in terms of cognitiveabilities.</p>
<p>Table 3 :
3
Cognition Scores of LVLMs on Description task evaluated by GPT-4.For reference, the Cognition Score of Human is calculated based on the annotated [Description] CogBench as an estimate of an upper limit score.</p>
<p>Table 4 :
4
Model performance on VQA task.Each QA contains four options, with a chance rate of 25%.
ModelTime Location CharacterCharacter RelationshipEventEvent RelationshipNext Moment EventMental StateOverallInstructBLIP-7B 0.340.570.470.520.360.440.390.370.43Qwen-VL-Chat0.640.840.630.550.540.400.500.530.55LLaVA-V1.5-7B 0.550.800.540.540.510.370.470.530.52LLaVA-V1.5-13B 0.680.850.600.620.480.540.680.560.59mPLUG-Owl-20.570.820.540.530.510.460.530.560.55ShareGPT4V-7B 0.620.780.630.590.500.360.530.560.54ShareGPT4V-13B 0.700.850.590.560.500.500.720.580.58GPT-4V0.720.880.790.690.680.650.720.710.71Human0.980.960.990.960.970.980.960.950.96</p>
<p>Table 5 :
5
LVLMs evaluated in this paper.</p>
<p>Code and data are available at https://github.com/xiujiesong/Cog-Bench.
The two samples are extracted from DementiaBank (https://dementia.talkbank.org/), which records description transcripts from both healthy subjects and dementia patients.
https://spacy.io/
Implemented with sentence-transformers package (https://www.sbert.net) and all-mpnet-base-v2 is adopted as the model to encode [Entities] and nouns.
Given a <DESCRIPTION> and some <EVENT RELATIONSHIP>s, please tell me whether this <DESCRIPTION> clearly depicts the cause-andeffect relationships between events.The format of a <EVENT RELATIONSHIP> follows the structure "A1 + A2 + ... + An -&gt; B", where A1, A2, ..., An and B are events.Events A1, A2, ..., An are the causes of event B, and event B is the result caused by events A1, A2, ..., An.The criteria for judgment lie in whether the <DESCRIPTION> mentions these events and clearly depicts the causal relationships between them.Assign a score of 0 or 1 to each <EVENT RE-LATIONSHIP>, where 0 represents NO and 1 represents YES.E Evaluation Results of Description Task based on Traditional Image Captioning MetricsTable6shows the performance of models on traditional image captioning evaluation metrics.FollowingKrause et al. (2017), we use METEOR (Banerjee and Lavie, 2005), CIDEr(Vedantam et al., 2015), BLEU-1, BLEU-2, BLEU-3, and BLEU-4(Papineni et al., 2002)F Effectiveness Analysis of GPT-based Cognition Evaluation of Description TaskTo prove the effectiveness of GPT-based evaluation, we manually annotated a subset by assigning 0/1 to CoRs of 20 images and use the subset to evaluate the performance of different evluation methods.F.1 Implementation of Non-GPT-based Evaluation MethodsApart from ChatGPT or GPT-4, some other evaluation methods are implemented to perform this task, as shown in Table7.For methods based on ROUGE(Lin, 2004), BERTScore(Zhang et al., 2020), and BLEURT(Sellam et al., 2020), we first split the description into sentences, then use a CoR as a reference to calculate the (recall) score for each sentence compared to the CoR.Then, the highest score among all calculated scores is taken as the score of the CoR corresponding to the description.Finally, the score is converted into 0/1 using a threshold.We also tried Natural Language Inference (NLI) models to perform the task.First, we use DeBERTa(He et al., 2021)to perform sentence-level NLI task similar to methods mentioned above.If there is at least one "Entailment" for all the sentences, the score of the CoR will be 1.The model we adopted is mDeBERTa-v3-base-xnli-multilingual-nli-2mil7.The second NLI model we tried is DocNLI(Yin et al., 2021), which can directly take the description and CoR as input and do the classification task.Table7shows the accuracy of different evaluation methods on the subset.It can be seen that GPT-4 achieves the best performance, which indicates that GPT-based evaluation is generally consistent with human evaluation and thus effective for evaluating the performance of LVLMs on Description task.F.2 Result Analysis
Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, 2023and beyond</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, Michigan2005Association for Computational Linguistics</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Hl dataset: visually-grounded description of scenes, actions and rationales. Michele Cafagna, Kees Van Deemter, Albert Gatt, Proceedings of the 16th International Natural Language Generation Conference. the 16th International Natural Language Generation Conference2023</p>
<p>Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, Dahua Lin, arXiv:2311.12793Sharegpt4v: Improving large multimodal models with better captions. 2023aarXiv preprint</p>
<p>Measuring and improving chain-of-thought reasoning in vision-language models. Yangyi Chen, Karan Sikka, Michael Cogswell, Ji Heng, Ajay Divakaran, arXiv:2309.044612023barXiv preprint</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, 2023. April 202314</p>
<p>Describing the cookie theft picture: Sources of breakdown in alzheimer's dementia. Louise Cummings, 10.1075/ps.17011.cum2019Pragmatics and Society10</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, 2023</p>
<p>Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, arXiv:2306.13394Mme: A comprehensive evaluation benchmark for multimodal large language models. 2023arXiv preprint</p>
<p>There's a Time and Place for Reasoning Beyond the Image. Xingyu Fu, Ben Zhou, Ishaan Preetam Chandratreya, Carl Vondrick, Dan Roth, Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL). of the Annual Meeting of the Association for Computational Linguistics (ACL)2022</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. Harold Goodglass, Edith Kaplan, Sandra Weintraub ; Philadelphia, Pa Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, International Conference on Learning Representations. Lippincott Williams &amp; Wilkins2001. 2021BDAE: The Boston Diagnostic Aphasia Examination</p>
<p>The abduction of sherlock holmes: A dataset for visual abductive reasoning. Jack Hessel, Jena D Hwang, Jae , Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate Saenko, Yejin Choi, European Conference on Computer Vision. Springer2022</p>
<p>A hierarchical approach for generating descriptive image paragraphs. Jonathan Krause, Justin Johnson, Ranjay Krishna, Li Fei-Fei, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 1232017</p>
<p>Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan, arXiv:2307.16125Seed-bench: Benchmarking multimodal llms with generative comprehension. 2023aarXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.125972023barXiv preprint</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, 2023a</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, 2023bIn NeurIPS</p>
<p>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, arXiv:2307.06281Mmbench: Is your multi-modal model an all-around player?. 2023carXiv preprint</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Visualcomet: Reasoning about the dynamic context of a still image. Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, Yejin Choi, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020</p>
<p>Bleurt: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur P Parikh, Proceedings of ACL. ACL2020</p>
<p>DEPAC: a corpus for depression and anxiety detection from speech. Mashrura Tasnim, Malikeh Ehghaghi, Brian Diep, Jekaterina Novikova, 10.18653/v1/2022.clpsych-1.1Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology. the Eighth Workshop on Computational Linguistics and Clinical PsychologySeattle, USAAssociation for Computational Linguistics2022</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Cider: Consensus-based image description evaluation. Ramakrishna Vedantam, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Visual clues: Bridging vision and language foundations for image paragraph captioning. Yujia Xie, Luowei Zhou, Xiyang Dai, Lu Yuan, Nguyen Bach, Ce Liu, Michael Zeng, Advances in Neural Information Processing Systems. 202235</p>
<p>Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, Ping Luo, arXiv:2306.09265Lvlm-ehub: A comprehensive evaluation benchmark for large visionlanguage models. 2023arXiv preprint</p>
<p>Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, arXiv:2309.17421The dawn of lmms: Preliminary explorations with gpt-4v (ision). 202391arXiv preprint</p>
<p>Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou, arXiv:2311.04257mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. 2023arXiv preprint</p>
<p>DocNLI: A large-scale dataset for documentlevel natural language inference. Wenpeng Yin, Dragomir Radev, Caiming Xiong, 10.18653/v1/2021.findings-acl.435Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang, arXiv:2308.02490Mm-vet: Evaluating large multimodal models for integrated capabilities. 2023arXiv preprint</p>
<p>From recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2019</p>
<p>Bertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020</p>
<p>Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, Mohamed Elhoseiny, arXiv:2303.06594Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. 2023aarXiv preprint</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592Minigpt-4: Enhancing vision-language understanding with advanced large language models. 2023barXiv preprint</p>
<p>Yan Zhuang, Qi Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang, Guanhao Zhao, Zheng Zhang, Qingyang Mao, Shijin Wang, arXiv:2306.10512Efficiently measuring the cognitive ability of llms: An adaptive testing perspective. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4415 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4415</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4415</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-267760070</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.12928v4.pdf" target="_blank">A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence</a></p>
                <p><strong>Paper Abstract:</strong> —By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers. In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews. Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords. Second, based on these indicators, the study presents comparative analyses of different reviews, unveiling the characteristics of publications across various fields, periods, and journals. The newly emerging AI-generated literature reviews are also appraised, and the observed differences suggest that most AI-generated reviews still lag behind human-authored reviews in several aspects. Third, we briefly provide a subjective evaluation of representative PAMI reviews and introduce a paper structure-based typology of literature reviews. This typology may improve the clarity and effectiveness for scholars in reading and writing reviews, while also serving as a guide for AI systems in generating well-organized reviews. Finally, this Analysis offers insights into the current challenges of literature reviews and envisions future directions for their development.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4415.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4415.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>aTNCSI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>adaptive Topic Normalized Citation Success Index (aTNCSI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-augmented, article-level, field-normalized bibliometric indicator that uses an LLM to generate a topic keyword for a paper, retrieves topic papers, fits a citation distribution, and returns a 0–1 impact probability (TNCSI) for that paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>aTNCSI (ChatGPT-augmented TNCSI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline introduced in this paper: (1) use ChatGPT (few-shot prompt) to generate a representative topic keyword from a paper's title+abstract; (2) retrieve up to the API limit of related papers (Semantic Scholar) for that topic; (3) compute the discrete citation frequency distribution and fit an exponential decay PDF via maximum likelihood estimation; (4) integrate the fitted PDF over [0, citeNum] to obtain the TNCSI value (0–1). The 'adaptive' prefix denotes that topic selection is automated by the LLM rather than fixed keywords.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo by default, as used in the paper's examples)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based topic keyword generation (prompted/few-shot) combined with API retrieval (Semantic Scholar) of papers and their citation counts</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>statistical synthesis: fitting citation distribution (exponential) using MLE and integrating to compute a normalized impact probability; no textual synthesis across papers is performed by the indicator itself</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>up to 1000 retrieved per topic (limited by Semantic Scholar API, as noted)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>applied to reviews and papers in PAMI (pattern analysis & machine intelligence); method is generalizable to topic-specific paper collections</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>numeric article-level impact indicator (TNCSI value between 0 and 1)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>interpretability (probability semantics), comparisons to existing article-level metrics (FWCI, RCR), and qualitative checks; the paper also uses the indicator in comparative analyses</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Qualitative/interpretative results reported: TNCSI has meaning (e.g., 0.5 implies more citations than half of same-topic papers); no large-scale automated accuracy numbers vs. ground truth provided, but used to compare papers and fields in the RiPAMI database</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Conventional bibliometric indicators such as citation counts, Field-Weighted Citation Impact (FWCI), Relative Citation Ratio (RCR), and FNCSI</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitatively, aTNCSI provides article-level and topic-normalized assessment without requiring a pre-defined field; the paper argues it mitigates some limitations of FWCI/RCR which require field definitions, but no direct numeric superiority claims are empirically established in the text</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automating topic generation with an LLM enables topic-adaptive, article-level normalization of citation impact; the indicator is interpretable (probability) and useful for cross-topic comparisons when coupled with LLM-derived topic keywords</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality depends on LLM-generated topic keywords (prompt sensitivity and ambiguity); dependence on external APIs (Semantic Scholar) and their limits (e.g., 1000-result cap); potential biases from retrieval sources and LLM misclassification of topic granularity</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Limited by the retrieval API (practical cap ~1000 papers); TNCSI computation scales with retrieved sample size but the paper does not provide numerical scaling experiments relating sample size to indicator variance</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4415.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4415.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-topic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT-based topic keyword generation (few-shot prompt engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method using ChatGPT with carefully designed prompts and few-shot examples to extract a concise, retrieval-appropriate topic keyword from a paper's title and abstract to support downstream bibliometric retrieval and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT topic keyword generator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The authors design system/user/assistant role prompts and few-shot examples to instruct ChatGPT to return a single, specific keyword suitable for retrieving related literature. They evaluated multiple prompt designs on a manually annotated dataset of 201 papers and tuned prompts by measuring normalized edit distance (NED) against manual keywords.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo by default in the paper's workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>structured natural-language prompting (few-shot) to extract a single topic/search keyword from title + abstract</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>no synthesis across papers — produced keyword is used to guide subsequent retrieval and downstream analyses</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>method evaluated on a 201-paper annotated dataset for prompt selection; applied per-paper to guide retrieval of topic corpora (up to 1000 papers)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>PAMI literature reviews and other scientific papers evaluated in the RiPAMI dataset</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>single concise topic/search keyword (string)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Normalized Edit Distance (NED) between LLM-generated keyword and manual annotation; prompt quality compared across variants (Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Some prompt variants achieved substantially lower NEDs; best observed NED reported in the paper's results (example best NED ≈ 0.28 in the presented table), indicating reasonable agreement with manual keywords for tuned prompts</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Manual human-annotated topic keywords</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LLM-generated keywords with tuned few-shot prompts approach human annotations on the 201-sample test set (measured by NED), but performance varies with prompt and topic ambiguity</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt design and few-shot examples materially improve the quality of LLM-derived topic keywords; providing context about expected granularity reduces ambiguity</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Prompt sensitivity: minor prompt changes can cause divergent keywords; ambiguity in paper focus (algorithm vs. application) can mislead LLM without contextual guidance</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4415.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4415.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-lit-pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Three-stage AI-generated literature review pipeline (retrieval → synthesis → report)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common architecture for AI literature-review systems: automated knowledge retrieval (keyword/date/citation filters), LLM-based synthesis of retrieved content (summarization, paraphrasing, Q&A), and report formatting into a literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Three-stage literature-review pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>High-level approach described in the paper: (1) Knowledge retrieval — select candidate papers via keywords, date ranges, citations, crawlers or APIs; (2) Synthesis — use LLMs with prompts to summarize, paraphrase, and combine extracted content; (3) Report generation — format synthesized outputs into a cohesive review (usually plain text), optionally sectioned. The paper notes many existing tools follow this pipeline and highlights weaknesses (e.g., inability to extract figures/tables, limited appraisal).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Various LLMs are mentioned in examples (ChatGPT, Google Bard, LLaMA), typically unspecified versions</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>keyword/date-based retrieval, crawler or API-based collection (e.g., Google Scholar crawling, Semantic Scholar API), embedding/cosine-similarity filtering in some systems</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-driven summarization, paraphrasing, and question-answering; often sequential per-document processing or section-wise generation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>varies by implementation — examples range from very small curated sets (e.g., ~5 references used by a PaperDigest example) up to hundreds/limited-by-API (hundreds to ~1000)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Applied widely across PAMI and general scientific literature in discussed examples</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>narrative literature reviews (plain-text), sometimes with organized sections; generally not multimodal (figures/tables often omitted)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human expert appraisal, the paper's Reference Quality Measurement (RQM) metric for quality, and anecdotal comparisons to human reviews</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Qualitative result: AI-generated reviews are fast (seconds) but generally lower quality than human-authored reviews in retrieval breadth, critical appraisal, and inclusion of multimodal content. Some systems achieved reasonable RQM in specific small-case examples (e.g., one PaperDigest instance), but overall they lag human reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human-authored literature reviews (and manual appraisal by experts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Lower in coverage, synthesis depth, and critical evaluation; faster in generation speed but prone to missing recent work or multimodal information</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The retrieval step is crucial — poor retrieval constrains synthesis quality; LLMs are effective at fluent synthesis but struggle with critical appraisal and extracting tables/figures; prompt engineering and retrieval breadth are primary levers for improvement</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Hallucination and factual errors from LLMs, limited multimodal extraction (tables/figures), dependency on up-to-dateness of retrieval sources, ethical concerns and publishers' policies on AI-generated content</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance depends on retrieval coverage and LLM context windows; the paper notes API limits and dataset sizes (e.g., Semantic Scholar 1000 cap) as practical scaling constraints but does not report quantitative scaling curves</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4415.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4415.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatPaper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatPaper (GitHub tool for automated paper retrieval and ChatGPT-based summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A community tool that crawls Google Scholar for user keywords, filters retrieved papers via embedding cosine similarity, and sequentially processes selected papers through ChatGPT to produce concise narratives per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatPaper</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Retrieves papers based on user-defined keyword using a Google Scholar crawler, applies cosine-similarity-based filtering on embeddings to select relevant papers, and then processes the selected papers sequentially with ChatGPT to produce brief narrative descriptions summarizing each selected paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (unspecified edition in the paper's citation of ChatPaper)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>crawler-based retrieval (Google Scholar), embedding-based cosine similarity filtering, then LLM per-document summarization</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>sequential per-paper summarization and concatenation into a narrative overview (no explicit multi-document hierarchical synthesis described)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>not specified in the paper (depends on user query and filtering thresholds)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>general academic papers (applied by users to various domains)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>short narrative descriptions / summary per selected paper; can be used as the basis for a literature overview</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>RQM and other indicators from this paper were applied to evaluate outputs in the case study, but ChatPaper itself does not publish standard metrics in the cited description</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper reports ChatPaper outputs are brief and concise; in the authors' case study ChatPaper-generated reviews lacked appraisal steps and scored lower quality compared to human reviews</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human-authored literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Inferior in depth, appraisal, and multimodal inclusion; faster generation</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining embedding-based filtering with LLM summarization produces concise outputs but lacks automated evaluation or critical synthesis of references</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on Google Scholar crawling (legal/robustness concerns), no integrated appraisal of reference quality, inability to extract figures/tables from PDFs</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4415.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4415.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperDigest</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paper Digest (AI-powered research platform for automated review generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial/online platform that generates automated literature summaries or mini-reviews by retrieving papers via keywords and date constraints and producing narrative text via LLM-based summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperDigest</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Users specify keywords and optional date constraints; the platform retrieves relevant literature, applies internal filtering, and uses LLMs to synthesize narrative plain-text descriptions constituting an automated review. In the paper's case study, an evaluated PaperDigest output used only 5 references.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>unspecified LLM(s) (platform proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>keyword/date-based retrieval and internal filtering (details proprietary/unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-driven narrative generation and summarization</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>varies; one evaluated example in the paper used 5 references</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>general scientific topics</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>plain-text literature review / summary</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>The authors computed the RQM on produced outputs; PaperDigest does not publicly report evaluation metrics in the citation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In the paper's case study, a PaperDigest-generated review achieved relatively high RQM but relied on a very small reference set (5 refs), suggesting limited coverage despite favorable metric</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human-authored literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>While some automated outputs can score highly on single metrics in narrow cases, overall they remain inferior in coverage and critical synthesis compared to human reviews</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automated platforms can be fast and sometimes produce high-scoring outputs by some metrics, but restricted retrieval breadth undermines comprehensiveness</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Small/biased reference sets, inability to extract multimodal content (tables/figures), potential metric gaming</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4415.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4415.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JenniAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jenni AI (interactive AI writing assistant used for semi-automated review generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semi-automated writing assistant where the user issues 'AI Commands' to guide generation; supports customization of literature-review content via interactive prompts rather than full automation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Jenni AI</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Human-in-the-loop system: users interact via explicit AI commands to request sections, rewrites, or summaries; the system assists composition and customization rather than fully automated retrieval/appraisal. The paper notes it as an example of semi-automated review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>unspecified LLM(s) (proprietary / platform-specific)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>user-guided extraction and summarization (no detailed crawler/automatic retrieval described in the paper's mention)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>interactive LLM-assisted composition controlled by user commands</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>general writing and literature synthesis tasks</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>customized narrative literature-review text and sections</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>none reported in the cited mention</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Described as semi-automated; enables tailored outputs but relies on user input to shape retrieval and appraisal</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human-authored reviews</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Provides customization advantages but is not a full replacement for manual literature appraisal</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interactive AI tools can increase control and customization of generated reviews, reducing pure automation risks</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires human involvement; does not automate retrieval/appraisal end-to-end</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4415.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4415.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aydin2022-2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aydın et al. experiments using ChatGPT / Google Bard to generate literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical examples where authors used ChatGPT and Google Bard to paraphrase recent paper abstracts and then queried the LLMs with structured prompts to generate formatted literature reviews on topics such as digital twin in healthcare and metaverse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-assisted paraphrase-and-QA review generation (Aydın et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The reported workflow: collect recent paper abstracts (e.g., last 3 years), use LLM (ChatGPT/Google Bard) to paraphrase abstracts, then design several question prompts (e.g., 'What is X?') to query the LLMs and rearrange their answers into a formatted review. This is a human-mediated pipeline where LLMs do paraphrasing and answer structured prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Google Bard and ChatGPT (unspecified versions, used in respective studies cited)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>paraphrasing of abstracts by LLMs and question-answering prompts to extract synthesized content</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM Q&A-based synthesis with manual reorganization into review structure</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>applied on sets of recent papers (authors limited to last 3 years in cited examples); exact counts vary per study</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>digital twin in healthcare; metaverse (domain-specific case studies cited by Aydın et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>formatted literature reviews (textual), assembled from LLM-paraphrased abstracts and prompt responses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>qualitative appraisal reported in those works; the surveyed paper notes limitations and ethical concerns</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Generated reviews are feasible but often lack deep critical analysis; the surveyed paper cites these works as demonstrating both capability and current limitations (fabrication risk, insufficient critical discernment)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human-authored reviews</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Inferior in critical appraisal and factual reliability despite being faster to generate</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can be used to paraphrase and synthesize recent abstracts to produce a draft review rapidly, but output quality and factual reliability are concerns</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Fabrication/factual inaccuracies, lack of critical synthesis, and ethical/publishing concerns about AI-authored content</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Openai chatgpt generated literature review: Digital twin in healthcare <em>(Rating: 2)</em></li>
                <li>Google bard generated literature review: metaverse <em>(Rating: 2)</em></li>
                <li>ChatPaper <em>(Rating: 2)</em></li>
                <li>Paper Digest, Paper digest ai-powered research platform. <em>(Rating: 2)</em></li>
                <li>Seamless -ai literature review tool for scientific research <em>(Rating: 1)</em></li>
                <li>Ai-powered literature review generator. O Blocktechnology, 2023. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4415",
    "paper_id": "paper-267760070",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "aTNCSI",
            "name_full": "adaptive Topic Normalized Citation Success Index (aTNCSI)",
            "brief_description": "An LLM-augmented, article-level, field-normalized bibliometric indicator that uses an LLM to generate a topic keyword for a paper, retrieves topic papers, fits a citation distribution, and returns a 0–1 impact probability (TNCSI) for that paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "aTNCSI (ChatGPT-augmented TNCSI)",
            "system_description": "Pipeline introduced in this paper: (1) use ChatGPT (few-shot prompt) to generate a representative topic keyword from a paper's title+abstract; (2) retrieve up to the API limit of related papers (Semantic Scholar) for that topic; (3) compute the discrete citation frequency distribution and fit an exponential decay PDF via maximum likelihood estimation; (4) integrate the fitted PDF over [0, citeNum] to obtain the TNCSI value (0–1). The 'adaptive' prefix denotes that topic selection is automated by the LLM rather than fixed keywords.",
            "llm_model_used": "ChatGPT (gpt-3.5-turbo by default, as used in the paper's examples)",
            "extraction_technique": "LLM-based topic keyword generation (prompted/few-shot) combined with API retrieval (Semantic Scholar) of papers and their citation counts",
            "synthesis_technique": "statistical synthesis: fitting citation distribution (exponential) using MLE and integrating to compute a normalized impact probability; no textual synthesis across papers is performed by the indicator itself",
            "number_of_papers": "up to 1000 retrieved per topic (limited by Semantic Scholar API, as noted)",
            "domain_or_topic": "applied to reviews and papers in PAMI (pattern analysis & machine intelligence); method is generalizable to topic-specific paper collections",
            "output_type": "numeric article-level impact indicator (TNCSI value between 0 and 1)",
            "evaluation_metrics": "interpretability (probability semantics), comparisons to existing article-level metrics (FWCI, RCR), and qualitative checks; the paper also uses the indicator in comparative analyses",
            "performance_results": "Qualitative/interpretative results reported: TNCSI has meaning (e.g., 0.5 implies more citations than half of same-topic papers); no large-scale automated accuracy numbers vs. ground truth provided, but used to compare papers and fields in the RiPAMI database",
            "comparison_baseline": "Conventional bibliometric indicators such as citation counts, Field-Weighted Citation Impact (FWCI), Relative Citation Ratio (RCR), and FNCSI",
            "performance_vs_baseline": "Qualitatively, aTNCSI provides article-level and topic-normalized assessment without requiring a pre-defined field; the paper argues it mitigates some limitations of FWCI/RCR which require field definitions, but no direct numeric superiority claims are empirically established in the text",
            "key_findings": "Automating topic generation with an LLM enables topic-adaptive, article-level normalization of citation impact; the indicator is interpretable (probability) and useful for cross-topic comparisons when coupled with LLM-derived topic keywords",
            "limitations_challenges": "Quality depends on LLM-generated topic keywords (prompt sensitivity and ambiguity); dependence on external APIs (Semantic Scholar) and their limits (e.g., 1000-result cap); potential biases from retrieval sources and LLM misclassification of topic granularity",
            "scaling_behavior": "Limited by the retrieval API (practical cap ~1000 papers); TNCSI computation scales with retrieved sample size but the paper does not provide numerical scaling experiments relating sample size to indicator variance",
            "uuid": "e4415.0",
            "source_info": {
                "paper_title": "A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ChatGPT-topic",
            "name_full": "ChatGPT-based topic keyword generation (few-shot prompt engineering)",
            "brief_description": "A method using ChatGPT with carefully designed prompts and few-shot examples to extract a concise, retrieval-appropriate topic keyword from a paper's title and abstract to support downstream bibliometric retrieval and evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ChatGPT topic keyword generator",
            "system_description": "The authors design system/user/assistant role prompts and few-shot examples to instruct ChatGPT to return a single, specific keyword suitable for retrieving related literature. They evaluated multiple prompt designs on a manually annotated dataset of 201 papers and tuned prompts by measuring normalized edit distance (NED) against manual keywords.",
            "llm_model_used": "ChatGPT (gpt-3.5-turbo by default in the paper's workflows)",
            "extraction_technique": "structured natural-language prompting (few-shot) to extract a single topic/search keyword from title + abstract",
            "synthesis_technique": "no synthesis across papers — produced keyword is used to guide subsequent retrieval and downstream analyses",
            "number_of_papers": "method evaluated on a 201-paper annotated dataset for prompt selection; applied per-paper to guide retrieval of topic corpora (up to 1000 papers)",
            "domain_or_topic": "PAMI literature reviews and other scientific papers evaluated in the RiPAMI dataset",
            "output_type": "single concise topic/search keyword (string)",
            "evaluation_metrics": "Normalized Edit Distance (NED) between LLM-generated keyword and manual annotation; prompt quality compared across variants (Table 3)",
            "performance_results": "Some prompt variants achieved substantially lower NEDs; best observed NED reported in the paper's results (example best NED ≈ 0.28 in the presented table), indicating reasonable agreement with manual keywords for tuned prompts",
            "comparison_baseline": "Manual human-annotated topic keywords",
            "performance_vs_baseline": "LLM-generated keywords with tuned few-shot prompts approach human annotations on the 201-sample test set (measured by NED), but performance varies with prompt and topic ambiguity",
            "key_findings": "Prompt design and few-shot examples materially improve the quality of LLM-derived topic keywords; providing context about expected granularity reduces ambiguity",
            "limitations_challenges": "Prompt sensitivity: minor prompt changes can cause divergent keywords; ambiguity in paper focus (algorithm vs. application) can mislead LLM without contextual guidance",
            "uuid": "e4415.1",
            "source_info": {
                "paper_title": "A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "AI-lit-pipeline",
            "name_full": "Three-stage AI-generated literature review pipeline (retrieval → synthesis → report)",
            "brief_description": "A common architecture for AI literature-review systems: automated knowledge retrieval (keyword/date/citation filters), LLM-based synthesis of retrieved content (summarization, paraphrasing, Q&A), and report formatting into a literature review.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Three-stage literature-review pipeline",
            "system_description": "High-level approach described in the paper: (1) Knowledge retrieval — select candidate papers via keywords, date ranges, citations, crawlers or APIs; (2) Synthesis — use LLMs with prompts to summarize, paraphrase, and combine extracted content; (3) Report generation — format synthesized outputs into a cohesive review (usually plain text), optionally sectioned. The paper notes many existing tools follow this pipeline and highlights weaknesses (e.g., inability to extract figures/tables, limited appraisal).",
            "llm_model_used": "Various LLMs are mentioned in examples (ChatGPT, Google Bard, LLaMA), typically unspecified versions",
            "extraction_technique": "keyword/date-based retrieval, crawler or API-based collection (e.g., Google Scholar crawling, Semantic Scholar API), embedding/cosine-similarity filtering in some systems",
            "synthesis_technique": "LLM-driven summarization, paraphrasing, and question-answering; often sequential per-document processing or section-wise generation",
            "number_of_papers": "varies by implementation — examples range from very small curated sets (e.g., ~5 references used by a PaperDigest example) up to hundreds/limited-by-API (hundreds to ~1000)",
            "domain_or_topic": "Applied widely across PAMI and general scientific literature in discussed examples",
            "output_type": "narrative literature reviews (plain-text), sometimes with organized sections; generally not multimodal (figures/tables often omitted)",
            "evaluation_metrics": "Human expert appraisal, the paper's Reference Quality Measurement (RQM) metric for quality, and anecdotal comparisons to human reviews",
            "performance_results": "Qualitative result: AI-generated reviews are fast (seconds) but generally lower quality than human-authored reviews in retrieval breadth, critical appraisal, and inclusion of multimodal content. Some systems achieved reasonable RQM in specific small-case examples (e.g., one PaperDigest instance), but overall they lag human reviews.",
            "comparison_baseline": "Human-authored literature reviews (and manual appraisal by experts)",
            "performance_vs_baseline": "Lower in coverage, synthesis depth, and critical evaluation; faster in generation speed but prone to missing recent work or multimodal information",
            "key_findings": "The retrieval step is crucial — poor retrieval constrains synthesis quality; LLMs are effective at fluent synthesis but struggle with critical appraisal and extracting tables/figures; prompt engineering and retrieval breadth are primary levers for improvement",
            "limitations_challenges": "Hallucination and factual errors from LLMs, limited multimodal extraction (tables/figures), dependency on up-to-dateness of retrieval sources, ethical concerns and publishers' policies on AI-generated content",
            "scaling_behavior": "Performance depends on retrieval coverage and LLM context windows; the paper notes API limits and dataset sizes (e.g., Semantic Scholar 1000 cap) as practical scaling constraints but does not report quantitative scaling curves",
            "uuid": "e4415.2",
            "source_info": {
                "paper_title": "A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ChatPaper",
            "name_full": "ChatPaper (GitHub tool for automated paper retrieval and ChatGPT-based summarization)",
            "brief_description": "A community tool that crawls Google Scholar for user keywords, filters retrieved papers via embedding cosine similarity, and sequentially processes selected papers through ChatGPT to produce concise narratives per paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "ChatPaper",
            "system_description": "Retrieves papers based on user-defined keyword using a Google Scholar crawler, applies cosine-similarity-based filtering on embeddings to select relevant papers, and then processes the selected papers sequentially with ChatGPT to produce brief narrative descriptions summarizing each selected paper.",
            "llm_model_used": "ChatGPT (unspecified edition in the paper's citation of ChatPaper)",
            "extraction_technique": "crawler-based retrieval (Google Scholar), embedding-based cosine similarity filtering, then LLM per-document summarization",
            "synthesis_technique": "sequential per-paper summarization and concatenation into a narrative overview (no explicit multi-document hierarchical synthesis described)",
            "number_of_papers": "not specified in the paper (depends on user query and filtering thresholds)",
            "domain_or_topic": "general academic papers (applied by users to various domains)",
            "output_type": "short narrative descriptions / summary per selected paper; can be used as the basis for a literature overview",
            "evaluation_metrics": "RQM and other indicators from this paper were applied to evaluate outputs in the case study, but ChatPaper itself does not publish standard metrics in the cited description",
            "performance_results": "Paper reports ChatPaper outputs are brief and concise; in the authors' case study ChatPaper-generated reviews lacked appraisal steps and scored lower quality compared to human reviews",
            "comparison_baseline": "Human-authored literature reviews",
            "performance_vs_baseline": "Inferior in depth, appraisal, and multimodal inclusion; faster generation",
            "key_findings": "Combining embedding-based filtering with LLM summarization produces concise outputs but lacks automated evaluation or critical synthesis of references",
            "limitations_challenges": "Relies on Google Scholar crawling (legal/robustness concerns), no integrated appraisal of reference quality, inability to extract figures/tables from PDFs",
            "uuid": "e4415.3",
            "source_info": {
                "paper_title": "A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PaperDigest",
            "name_full": "Paper Digest (AI-powered research platform for automated review generation)",
            "brief_description": "A commercial/online platform that generates automated literature summaries or mini-reviews by retrieving papers via keywords and date constraints and producing narrative text via LLM-based summarization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "PaperDigest",
            "system_description": "Users specify keywords and optional date constraints; the platform retrieves relevant literature, applies internal filtering, and uses LLMs to synthesize narrative plain-text descriptions constituting an automated review. In the paper's case study, an evaluated PaperDigest output used only 5 references.",
            "llm_model_used": "unspecified LLM(s) (platform proprietary)",
            "extraction_technique": "keyword/date-based retrieval and internal filtering (details proprietary/unspecified)",
            "synthesis_technique": "LLM-driven narrative generation and summarization",
            "number_of_papers": "varies; one evaluated example in the paper used 5 references",
            "domain_or_topic": "general scientific topics",
            "output_type": "plain-text literature review / summary",
            "evaluation_metrics": "The authors computed the RQM on produced outputs; PaperDigest does not publicly report evaluation metrics in the citation",
            "performance_results": "In the paper's case study, a PaperDigest-generated review achieved relatively high RQM but relied on a very small reference set (5 refs), suggesting limited coverage despite favorable metric",
            "comparison_baseline": "Human-authored literature reviews",
            "performance_vs_baseline": "While some automated outputs can score highly on single metrics in narrow cases, overall they remain inferior in coverage and critical synthesis compared to human reviews",
            "key_findings": "Automated platforms can be fast and sometimes produce high-scoring outputs by some metrics, but restricted retrieval breadth undermines comprehensiveness",
            "limitations_challenges": "Small/biased reference sets, inability to extract multimodal content (tables/figures), potential metric gaming",
            "uuid": "e4415.4",
            "source_info": {
                "paper_title": "A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "JenniAI",
            "name_full": "Jenni AI (interactive AI writing assistant used for semi-automated review generation)",
            "brief_description": "A semi-automated writing assistant where the user issues 'AI Commands' to guide generation; supports customization of literature-review content via interactive prompts rather than full automation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Jenni AI",
            "system_description": "Human-in-the-loop system: users interact via explicit AI commands to request sections, rewrites, or summaries; the system assists composition and customization rather than fully automated retrieval/appraisal. The paper notes it as an example of semi-automated review generation.",
            "llm_model_used": "unspecified LLM(s) (proprietary / platform-specific)",
            "extraction_technique": "user-guided extraction and summarization (no detailed crawler/automatic retrieval described in the paper's mention)",
            "synthesis_technique": "interactive LLM-assisted composition controlled by user commands",
            "number_of_papers": "not specified",
            "domain_or_topic": "general writing and literature synthesis tasks",
            "output_type": "customized narrative literature-review text and sections",
            "evaluation_metrics": "none reported in the cited mention",
            "performance_results": "Described as semi-automated; enables tailored outputs but relies on user input to shape retrieval and appraisal",
            "comparison_baseline": "Human-authored reviews",
            "performance_vs_baseline": "Provides customization advantages but is not a full replacement for manual literature appraisal",
            "key_findings": "Interactive AI tools can increase control and customization of generated reviews, reducing pure automation risks",
            "limitations_challenges": "Requires human involvement; does not automate retrieval/appraisal end-to-end",
            "uuid": "e4415.5",
            "source_info": {
                "paper_title": "A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Aydin2022-2023",
            "name_full": "Aydın et al. experiments using ChatGPT / Google Bard to generate literature reviews",
            "brief_description": "Empirical examples where authors used ChatGPT and Google Bard to paraphrase recent paper abstracts and then queried the LLMs with structured prompts to generate formatted literature reviews on topics such as digital twin in healthcare and metaverse.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM-assisted paraphrase-and-QA review generation (Aydın et al.)",
            "system_description": "The reported workflow: collect recent paper abstracts (e.g., last 3 years), use LLM (ChatGPT/Google Bard) to paraphrase abstracts, then design several question prompts (e.g., 'What is X?') to query the LLMs and rearrange their answers into a formatted review. This is a human-mediated pipeline where LLMs do paraphrasing and answer structured prompts.",
            "llm_model_used": "Google Bard and ChatGPT (unspecified versions, used in respective studies cited)",
            "extraction_technique": "paraphrasing of abstracts by LLMs and question-answering prompts to extract synthesized content",
            "synthesis_technique": "LLM Q&A-based synthesis with manual reorganization into review structure",
            "number_of_papers": "applied on sets of recent papers (authors limited to last 3 years in cited examples); exact counts vary per study",
            "domain_or_topic": "digital twin in healthcare; metaverse (domain-specific case studies cited by Aydın et al.)",
            "output_type": "formatted literature reviews (textual), assembled from LLM-paraphrased abstracts and prompt responses",
            "evaluation_metrics": "qualitative appraisal reported in those works; the surveyed paper notes limitations and ethical concerns",
            "performance_results": "Generated reviews are feasible but often lack deep critical analysis; the surveyed paper cites these works as demonstrating both capability and current limitations (fabrication risk, insufficient critical discernment)",
            "comparison_baseline": "Human-authored reviews",
            "performance_vs_baseline": "Inferior in critical appraisal and factual reliability despite being faster to generate",
            "key_findings": "LLMs can be used to paraphrase and synthesize recent abstracts to produce a draft review rapidly, but output quality and factual reliability are concerns",
            "limitations_challenges": "Fabrication/factual inaccuracies, lack of critical synthesis, and ethical/publishing concerns about AI-authored content",
            "uuid": "e4415.6",
            "source_info": {
                "paper_title": "A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Openai chatgpt generated literature review: Digital twin in healthcare",
            "rating": 2,
            "sanitized_title": "openai_chatgpt_generated_literature_review_digital_twin_in_healthcare"
        },
        {
            "paper_title": "Google bard generated literature review: metaverse",
            "rating": 2,
            "sanitized_title": "google_bard_generated_literature_review_metaverse"
        },
        {
            "paper_title": "ChatPaper",
            "rating": 2
        },
        {
            "paper_title": "Paper Digest, Paper digest ai-powered research platform.",
            "rating": 2,
            "sanitized_title": "paper_digest_paper_digest_aipowered_research_platform"
        },
        {
            "paper_title": "Seamless -ai literature review tool for scientific research",
            "rating": 1,
            "sanitized_title": "seamless_ai_literature_review_tool_for_scientific_research"
        },
        {
            "paper_title": "Ai-powered literature review generator. O Blocktechnology, 2023.",
            "rating": 1,
            "sanitized_title": "aipowered_literature_review_generator_o_blocktechnology_2023"
        }
    ],
    "cost": 0.0231255,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence
23 Feb 2024</p>
<p>Penghai Zhao 
Xin Zhang 
Ming-Ming Cheng 
Jian Yang 
Xiang Li 
A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence
23 Feb 2024BE866CD6A0799D022A8A5244D4216D3AarXiv:2402.12928v2[cs.DL]literature reviewpattern analysismachine intelligencebibliometricAI-generated scholar content
By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic.However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers.In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives.First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically.To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews.Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords.Second, based on these indicators, the study presents comparative analyses of different reviews, unveiling the characteristics of publications across various fields, periods, and journals.The newly emerging AI-generated literature reviews are also appraised, and the observed differences suggest that most AI-generated reviews still lag behind human-authored reviews in several aspects.Third, we briefly provide a subjective evaluation of representative PAMI reviews and introduce a paper structure-based typology of literature reviews.This typology may improve the clarity and effectiveness for scholars in reading and writing reviews, while also serving as a guide for AI systems in generating well-organized reviews.Finally, this Analysis offers insights into the current challenges of literature reviews and envisions future directions for their development.</p>
<p>INTRODUCTION</p>
<p>"The entropy of the universe tends to a maximum."</p>
<p>-Rudolf Clausius T HE entropy of almost all natural and artificial systems in the universe exhibits a continuous increase.This principle also applies to the knowledge system of humanity, which similarly undergoes a perpetual escalation in entropy.Such escalating disorder within the knowledge system can lead to redundant endeavors, adversely affecting the generation of new knowledge.Analogous to the role of gravity in shaping the early universe filled with diverse particles, literature review plays a vital role in the knowledge system by consolidating scattered knowledge.Essentially, a literature review is a scholarly composition that not only demonstrates an understanding of the academic literature on a given topic but also situates this knowledge within a broader context.It compiles the most relevant and significant publications related to a specific research area, thereby offering a comprehensive overview of that field.</p>
<p>Almost all the field boast their own literature reviews, especially for the rapidly developing field of pattern analysis and machine intelligence, including image classification [20], [61], [62], [64], image segmentation [35], [67], [70], [91], object detection [21], [47], [81], [116], natural language processing [36], [63], [69], etc.As reported by the AI Index Report [65], there has been a striking surge in artificial intelligence publications, soaring • P. Zhao, X. Zhang, M. Cheng, J. Yang  from 200,000 in 2010 to nearly 500,000 by 2021.This exponential increase has subsequently led to a proliferation of related literature reviews.This trend is clearly illustrated in Fig. 1, which shows a marked increase in the annual publication of reviews, underscoring the growing prevalence of literature reviews in the field.While comprehensive literature reviews are valuable, excessive reviews may lead to information overload and redundant effort.These issues emerge when multiple reviews address the same topic, creating significant redundancy and presenting readers with the same studies and ideas repeatedly.Additionally, writing and peer-reviewing literature reviews imposes a considerable burden on both authors and reviewers, as it involves a broader and more thorough examination of the literature than standard research papers typically demand.Additionally, with the advent of AI-generated literature review systems, the landscape is further complicated.The differences between AI-generated and humanauthored reviews remain an area for exploration.</p>
<p>Up to now, far too little attention has been paid to the abovementioned issues.This paper concentrates on reviews within the field of PAMI, with three distinct objectives: (1) to endeavor an automated evaluation of these literature reviews in a relatively scientific manner; (2) to offer a thorough review of existing human-authored and AI-generated literature reviews from diverse perspectives; (3) to pinpoint limitations while providing future insights for literature review.</p>
<p>Scope</p>
<p>In this study, we exclusively examine review articles.Grant's comprehensive review [31] concluded that there are at least fourteen review types characterized by methods used, which highlights the fact that different reviews serve different purposes.For instance, a critical review strives to exhibit that the author has carried out extensive research on literature and evaluated it critically.</p>
<p>A systematic review aims to systematically explore, assess and integrate research evidence, usually in adherence with review guidelines.However, these types of reviews are relatively rare in the field of PAMI.Given the predominance of literature reviews in this area, we entitle this paper "A Literature Review of Literature Reviews".Although the quantity of reviews is considerably smaller compared to that of normal papers, it remains impractical to analyze reviews within every field.Therefore, this paper will focus only on reviews in the PAMI field.The reviews being analyzed in this paper are sourced from various publication venues, including conferences, journals, and pre-prints.Reviews from various sources exhibit a diverse range of writing styles, article lengths, and citation counts, providing a representative cross-section of the PAMI field.Considering the recent emergence of deep learning in PAMI, most reviews being investigated were published in the last decade.However, for a comprehensive understanding, we also analyze a few reviews published over ten years ago.</p>
<p>Contribution</p>
<p>In summary, the main contributions of this survey are as follows:</p>
<p>• We conduct a comprehensive and detailed review of literature reviews in the field of PAMI.To the best of our knowledge, there have been no scholarly attempts to systematically review the literature reviews.• A biography database dubbed RiPAMI is built and released.</p>
<p>This database contains meta-data for 2904 literature reviews, including titles, authors, citations and references detail, etc.In addition, we construct a topic key phrase dataset comprising 201 different domain reviews through manual annotation.This dataset can be utilized to evaluate the prompt effectiveness in identifying the paper topic or to conduct further analyses for the community.• Subjective evaluations of seminal literature reviews in PAMI are presented.Alongside this, we introduce an organizational structure-based review typology designed to guide both human authors and AI systems in the methodical crafting of literature reviews.• We propose the impact and quality indicators for quantitatively evaluating literature reviews.Diverging from tradi-tional, our approaches offer real-time, article-level, and fieldnormalized quantitative assessment of literature reviews.These automated indicators may help address concerns arising from excessive reviews and provide support during the appraisal stage of AI-generated review systems.• Based on the proposed evaluation indicators, we thoroughly assess the quality of the selected human-authored literature reviews in the PAMI field.Furthermore, a comparison between human-authored and AI-generated reviews is carried out, which highlights the enduring strengths of humanauthored literature reviews and uncovers the limitations of existing AI-generated review systems.• We briefly discuss the challenges and provide a preliminary analysis of future directions for literature reviews in the PAMI field, highlighting potential areas for improvement.All the data and code framework used in this paper are publicly available at https://sway.cloud.microsoft/2TXEuPuNlDKEmC9p.</p>
<p>Organization of the Paper</p>
<p>The remainder of the paper is organized as follows.Section 2 briefs the evolution of the literature review and details the database constructed for this paper.Section 3 introduces the typology of reviews and provides subjective comments on selected reviews.Section 4 presents the quantitative measures used to gauge the impact and quality of the publications respectively, and compares literature reviews from various journals and fields based on these evaluation indicators.The characteristics of human-authored and AI-generated literature reviews are discussed in Section 5 and a case study is carried out to elucidate what are the advantages and shortcomings of these two types of literature reviews.In Section 6, We further discuss the challenges and future of the literature review.Finally, this paper is concluded in section 7.</p>
<p>BACKGROUND AND SETTING</p>
<p>Background of Literature Review</p>
<p>Given the historical evolution and disciplinary variations of literature reviews, it is challenging to discern the very first peerreviewed literature review in history.However, there is limited research to suggest that contemporary literature reviews can be traced back to the 17th and 18th centuries [37] when the scientific method and the concept of peer review began to take shape.Initially, literature reviews were focused more on assimilating existing knowledge as opposed to following a formalized structure.Scholars used prior works to direct their research and grounded their contributions in an established understanding of the subject.Literature reviews during this period served to prevent duplicated efforts and build upon the work of predecessors.</p>
<p>As scholarly communication advanced and the scientific method became more rigorous, the literature review experienced a notable transformation in both methodology and purpose.Nowadays, a well-executed literature review is more than a basic summary of references; it goes beyond that by organizing and combining the information through analysis and synthesis.Reading a good literature review may benefit both novice and seasoned researchers.For novice researchers, literature review is one of the most important ways to gain knowledge in a specific field.They can quickly learn the basics, grasp fundamental concepts, and gain a comprehensive understanding of major theories.For seasoned researchers, literature reviews may help them keep up with the latest research, identify gaps in the current research field, and avoid duplicating work that has already been done.</p>
<p>Database</p>
<p>Literature reviews usually cite regular papers in a particular field as references; however, our literature review investigates literature reviews in various fields as the material to be analyzed and synthesized.This section describes how the reviews are selected and provides further details on the construction of the database.</p>
<p>Data Source</p>
<p>Reliable data sources for analyzing extensive reviews are fundamentally important.Based on the means of data acquisition and storage, existing scientific scholar data sources may be classified into two main categories: web-based and snapshot-based sources.Web-based source data refers to the meta-data that can be retrieved from the data provider in real-time with the use of the web crawler or the API (e.g.Semantic Scholar, arXiv, CrossRef).Such an approach would only consume a small amount of storage on the local machine but needs to query meta-data every single time.On the contrary, the offline snapshot consumes a larger amount of storage space but eliminates the need for frequent API queries.Since the snapshot is a mirror image of relevant papers before a specific time, its data remains unchanged over time compared to the API-based sources.As a result, the snapshot ensures a consistent dataset in all of the experiments, avoiding issues of irreproducibility caused by changes in the provided web retrieving service.</p>
<p>Tab. 1 compares various most commonly used data sources, where "Counts only" in the citations column means that the source only records the citation counts, while "Complete" signifies that the data source provides a complete list of citations.Unfortunately, none of these sources is perfect.For example, arXiv is a free distribution service and an open-access archive for millions of scholarly articles in various fields.Users can utilize arXiv's APIs to access all of the paper meta-data stored in arXiv databases.Meta-data retrieved from arXiv contains valuable information but fails to query the publication venue, citations, and references.Semantic Scholar seems promising, but it suffers a lower update rate than arXiv and a narrower search scope than Google Scholar.Google Scholar is an online search engine that indexes scholarly literature from a wide range of disciplines and publication formats.It employs automated programs to retrieve files for inclusion in the search results (which are not limited to academic papers, but also include patents, books, etc.).Despite its widespread use, Google Scholar still encounters challenges.Beel [11], [12], [13] argues that Google Scholar places a high weight on citation counts in its ranking algorithm and has therefore been criticized for exacerbating the Matthew effect.Moreover, the citation counts displayed on Google Scholar are subject to manipulation by complete nonsense articles indexed on Google Scholar (e.g.citations from AI-generated pre-print papers published on arXiv should have been ignored).Therefore, a promising engineering solution is to leverage the strengths of the different approaches to overcome the weaknesses of each approach, as demonstrated in this paper.</p>
<p>Database construction</p>
<p>We first detail the database construction approach used for this paper (as illustrated in Fig. 2).To avoid potential copyright and licensing issues, papers are retrieved and downloaded using the arXiv's API.Calls to the API are made by means of HTTP requests to a certain URL.The responses will be cached and stored in an SQL-based database.</p>
<p>We employed a simple yet effective technique to ensure that the type of paper retrieved is review paper and highly relevant to the field of pattern recognition.First, we identify 106 keywords based on the scopes of related journals and conferences.As can be seen in Fig. 2, these selected keywords include, but are not limited to, speech recognition, optical character recognition, and self-supervised learning.Next, we conduct a primary retrieving and filter the results following a clear rule, i.e. the title of the paper must contain "survey" or "review", and the keyword should be included in the abstract.Additionally, we filter out noisy data through both ChatGPT-based and manual double-checking processes.</p>
<p>As mentioned in Sec.2.2.1, we suggest enriching the metadata of papers by leveraging a combination of disparate data sources.Considering the potential legal risks of crawling to obtain academic data from Google Scholar, Semantic Scholar API was employed to obtain additional meta-data, such as citation and reference details which are not provided by the arXiv API.</p>
<p>Finally, a total of 2904 eligible papers with meta-data are retrieved and collected.To ensure reproducible experiments and prevent overburdening the server, we construct an SQL-based database dubbed RiPAMI (Reviews in Pattern Analysis and Machine Intelligence).This database stores information related to the paper such as title, abstract, date of publication, venue, citation counts, and reference details, etc.</p>
<p>It should be noted that this paper employs the arXiv API for retrieving literature reviews.As such, there might be potential biases and issues related to incomplete retrieval.That is, papers that are not published on arXiv but meet the criteria will not be included in the database.However, we believe that such a problem is unavoidable.On the one hand, it is difficult for any researcher to guarantee that he or she is able to retrieve the entire relevant literature.On the other hand, most researchers tend to select high-level articles as references to ensure the quality of the literature review, which also leads to the bias issue to some extent.Considering that most articles published on arXiv will also be published subsequently in different conferences and journals, the arXiv API-based retrieval can be regarded as a sample of the full set of literature reviews.We expect that the dataset constructed in this manner shares similar statistical characteristics with the full set of literature reviews.</p>
<p>Given that citation counts vary over time, the date for retrieving citation details is January 1, 2024.</p>
<p>Database Statistics</p>
<p>The database consists of more than 2900 literature reviews from a variety of sources, publication years, and fields.To elucidate the characteristics of the RiPAMI database, we conduct a statistical analysis and plot the result in Fig. 3.</p>
<p>Years of Publication Figure 3 (a) illustrates the distribution of publication years of literature reviews contained in the RiPAMI database.What can be clearly seen in this figure is a growing trend in the number of reviews.This trend has similar characteristics to the one obtained in Fig. 1, e.g. both are steadily increasing and showing a fairly significant increase between 2019 and 2020.This to some extent reflects the consistency between our sample data and the original data in terms of statistics.</p>
<p>Database</p>
<p>Title &amp; Authors Venue Abstract Citations References Source Types Charge
arXiv ✓ × ✓ × × API-based × CrossRef ✓ ✓ ✓ Counts Only ✓ API-based × Google Scholar ✓ ✓ ✓ Complete × Crawler-based × IEEE Xplore ✓ ✓ ✓ Counts Only × API-based × Semantic Scholar ✓ ✓ ✓ Complete ✓ API-based × Web of Science ✓ ✓ ✓ Complete ✓ API-based ✓ Scopus ✓ ✓ ✓ Complete ✓ API-based ✓ arXiv Data File ✓ × ✓ × × Snapshots × CrossRef Data File ✓ ✓ ✓ Counts Only ✓ Snapshots × RiPAMI(Ours) ✓ ✓ ✓ Counts Only ✓ Snapshots ×</p>
<p>SQL-based Snapshot</p>
<p>Primary Retrieving Filtering Ensemble Querying Retrieving Fig. 2. Schematic representation of the database construction process.From keywords to a SQL-based snapshot RiPAMI, there are three major steps to ensure that the data in RiPAMI is clean, accurate, and reliable.For visual clarity, the polar figure displays only a part of retrieving keywords.</p>
<p>Number of the References</p>
<p>The number of references in a survey paper may influence its credibility and reliability.As shown in 3 (b), the distribution of literature review references in the RiPAMI database follows a log-normal pattern.The average number of cited references is approximately 140, while the median number is 123.</p>
<p>Number of the Authors A review paper typically aims to provide a comprehensive overview of a specific topic.Involving multiple authors with diverse expertise could enhance the depth and breadth of the review.Fig. 3 (c) indicates that the majority of reviews are written by fewer than 10 authors.</p>
<p>Number of the Citations We count the citations of papers in RiPAMI and plot them in Fig. 3 (d).A power law distribution of received citations could be found.This phenomenon where a small subset of papers receives the majority of citations is sometimes referred to as the "Matthew Effect" or the "Pareto principle", as reported in [17], [68].</p>
<p>A SUBJECTIVE EVALUATION OF LITERATURE REVIEWS</p>
<p>Popular Paper Structure</p>
<p>The review's structure is alternatively referred to as the framework of the review.It is the outline that authors consider when starting to conduct the survey.A well-designed structure is believed to enhance the paper's readability and facilitate the reader's comprehension of the paper's concepts and knowledge.Typically, this framework encompasses foundational sections such as the introduction, methodology, discussion, and conclusion, each fulfilling a specific function.</p>
<p>Similar to the research article, the introduction section of the literature review usually includes contextualizing the research topic, identifying the knowledge gap, defining the scope and objectives, and laying the foundation knowledge for wider audiences."Introduction" usually lies at the very beginning of the paper.Most reviews first introduce the definition of the topic to acquaint readers with a basic understanding of the field.For instance, the term "Named Entity Recognition" may be unfamiliar to many scholars outside the field of natural language processing.A clever introduction might provide the origins of the terminology and inform the readers what is named entity recognition, as has been done in Ref [73].A brief definition of the field in the introduction is also welcomed for some relatively popular fields, e.g., the literature review [84] examines the concept of image categorization in the first sentence.</p>
<p>In addition to contextualizing the research topic, many introductions also highlight the existing research and identify gaps or challenges in the current understanding of the topic.It sets the stage for the literature review by explaining why the research being reviewed is necessary and what gaps are expected to be filled.Wang et al. [103] emphasize that while there are comprehensive surveys of self-supervised learning in computer vision, there is a lack of a similar overview specifically tailored to the remote sensing community.</p>
<p>Readers need to evaluate if the article is worth reading before investing more time in it.A statement of the scope or contributions of the article is a way for readers to quickly assess its relevance.In the first section, Wangkhade et al. [104] provide us with several important contributions including analyzing wellknown technologies, proposing taxonomies of approaches, and summarizing benefits and challenges of sentiment analysis.</p>
<p>Preliminaries and problem formulations are also popular with readers, as they both provide profound background knowledge.Given that the Gumbel-max involves considerable mathematical concepts and calculations, a "Preliminaries" section in the review of the Gumbel-max trick [39] serves the purpose of providing background information and basic understanding related to the Gumbel-max trick.</p>
<p>The middle part of the survey paper, also known as the review part, presents a detailed examination of the relevant research studies, methodologies, findings, and theories related to the chosen theme of the review.Beyond that, The middle part synthesizes the information from related studies to provide a cohesive and integrated understanding of the topic.This synthesis not only aligns the disparate studies but also evaluates their contributions and the interrelations among them.It often includes comparative analyses, highlighting similarities and differences in approaches, and may identify limitations in the existing research that warrant further investigation.This section is instrumental in demonstrating how individual studies collectively advance understanding of the subject matter.</p>
<p>The manner of organization and synthesis in the review part is subject to the choice of topic.Based on the organization of the review part, we propose a typology for literature reviews which includes three major types: method clustering-based, challengeoriented, and hybrid literature review.More details about the typology are presented in Section 3.2.</p>
<p>The subsequent ending part serves as a succinct conclusion to the reviewed studies.Within this section, the authors prefer to sum up the key findings to answer the question in the beginning part.It usually highlights both the advantages and disadvantages of reviewed literature to conclude research gaps and suggest potential avenues for future research.</p>
<p>One of the main objectives of the concluding part is to provide a relatively concise summary of the main insights derived from the reviewed studies and highlight their significance and relevance to the research topic.Some literature reviews conduct comprehensive analysis and synthesis, which in turn run up to a considerable number of pages.The paper [51] consists of 51 double-column pages in the main body, which poses a challenge for readers who may not have the time to peruse it thoroughly.Fortunately, this paper also includes a concise summary which provides readers with an efficient understanding of the content and allows them to navigate to relevant sections of interest.</p>
<p>When discussing further in the ending part, the authors may attempt to identify gaps and point out future directions through the analysis of existing literature.Some of the papers present the gaps and future directions separately.Hassain et al. [40] first discussed major challenges of multi-view video summarization such as lack of synchronization, instability of camera, and crowded scenes individually.Where there are challenges, there are future research directions.In addition to challenges, the authors also provide recommendations and future directions from various perspectives including models, benchmark datasets, and agents-based MVS, etc. Conversely, some papers choose to combine discussions of current issues with emerging research trends, as seen in the 'Future Directions' section of Ref [110].</p>
<p>For literature reviews delving into pragmatic methodologies, the inclusion of an applications section is quite fitting.Ref [48] offers a comprehensive review of various surveys on convolutional neural networks, dedicating a distinct section to the typical applications of 1-D, 2-D, and multidimensional CNNs.Similarly, Ref [59] illustrates the deployment of few-shot learning techniques across disciplines like computer vision, natural language processing, and reinforcement learning.</p>
<p>Typology of the Literature Review</p>
<p>To investigate more comprehensively how papers are organized and synthesized, we propose a typology for the literature review based on the organization of the middle part.Note that the terms typology and taxonomy are often used interchangeably, but there is a subtle difference.According to Ref. [93], typology creates categories based on conceptual dimensions and idealized types, providing a systematic basis for comparison.Conversely, taxonomy categorizes items based on observable and measurable characteristics.Hence, we opt for "typology" to categorize literature reviews in this context, considering their content and structure.</p>
<p>The typologies for review have been well developed [23], [24], [31], [78].Most of these articles mainly focus on categorizing literature reviews based on their purpose or the analytical framework used, such as the SALSA framework (Search, Appraisal, Synthesis, and Analysis).For example, literature reviews can be categorized as narrative reviews, critical reviews, etc., depending on their purpose.However, few studies have investigated how the review part of a literature review is organized in a systematic way.Investigating the organization of the review part may enhance the understanding of the literature review, thereby offering valuable guidance for researchers and AI-generated review systems.Based on the writing style and paper structure, we categorized existing literature reviews into three main types: method clustering-based, challenge-oriented, and hybrid literature review.</p>
<p>Method Clustering-based Literature Review</p>
<p>Method Clustering-based literature review refers to grouping methods according to their technical characteristics and presenting them in separate sections or subsections.Authors need to identify different sections depending on the topic of the article and arrange the same type of methods as closely as possible.</p>
<p>Featuring a clear and well-organized article structure, method clustering-based literature reviews are widely favored by researchers.The reader can gain a comprehensive understanding of the typology, details, and advancements of technology within a specific field which facilitates a foundational comprehension of the research field.However, such a type of literature review is less tailored for readers who are completely unfamiliar with the subject.This is because readers who lack basic knowledge of the field may not understand the connection between the methods mentioned in different sections, or are even confused to the select a proper algorithm for a certain task.</p>
<p>By way of illustration, Minaee et al. conducted a comprehensive survey on deep learning-based image segmentation models [70] in a method clustering-based manner.They grouped the models into 10 categories based on the adopted model architectures, such as fully convolutional models, encoder-decoder-based models, and attention-based models.In this way, the reader can quickly gain an understanding of the typologies of deep learningbased image segmentation models and how they differ from each other.</p>
<p>Challenge-oriented Literature Review</p>
<p>Known for its practicality, challenge-oriented literature reviews focus on presenting research methods and findings that address a specific challenge or task.In this type of review, each section or subsection focuses on a specific challenge or task and centers the discussion on how to address that challenge.The references cited within each section are primarily directly related to addressing that challenge to provide support and evidence.</p>
<p>Challenge-oriented literature reviews are well-suited to application-oriented fields, where readers at different levels of expertise can easily and quickly seek out appropriate solutions within the literature to address the problem they are facing.Despite the advantages, challenge-oriented literature reviews may fail to provide a comprehensive elaboration of fundamentals and methods details given the limited paper length.</p>
<p>Bandini et al. divided their survey about egocentric vision hand analysis into several sections including hand segmentation, hand detection, and hand identification [10].Each section contains multiple subsections, and each of the subsections investigates a certain challenge.(e.g.subsections entitled "Robustness to Illumination Changes" and "Lack of Pixel-Level Annotations" were arranged in the "Hand Segmentation" section ).</p>
<p>Hybrid Literature Review</p>
<p>By integrating characteristics of method clustering-based and challenge-oriented review, a hybrid literature review strikes a fair balance between comprehensiveness and practicality.</p>
<p>Typically, the hybrid literature review refers to a challengeoriented framework incorporating a method clustering-based subframework.One advantage of a hybrid review is that it simultaneously offers multiple solutions to a certain problem along with the classification of the respective methodologies.Such a hybrid appears to be a perfect framework for review parts, but unfortunately, not all topics are suitable for the hybrids, particularly regarding those more theoretical fields.Therefore, it is advisable to adopt different frameworks for different topics.</p>
<p>An example of this is the study carried out by Guo et al. [33] in which authors divide sections by tasks and introduce similar approaches group by group.</p>
<p>Review of Selected Reviews</p>
<p>In this section, we present a subjective evaluation of selected reviews in different areas.In contrast to the previous surveys, the literature reviews investigated in this section tend to be representative.This means that these surveys are of fairly high quality and popular with a wide range of researchers, evident in the number of citations they received.</p>
<p>Computer Vision</p>
<p>Computer vision is one of the most popular sub-fields of pattern recognition and machine intelligence.This section will focus on the literature reviews within the realm of computer vision.</p>
<p>Image Classification refers to the task of assigning a label or a category to an input image, which is one of the most renowned tasks in the field of computer vision.A survey by Rawat [84] explores the development and advancements of deep convolutional neural networks (CNNs) in the field of image classification.The paper covers the historical context, their role in the deep learning renaissance, and the notable contributions and challenges faced in recent years.It highlights the remarkable progress of CNNs in image classification, while also acknowledging the ongoing research efforts to address challenges and provide recommendations for future exploration.Schmarje et al. [85] provides a comprehensive survey on semi-, self-, and unsupervised learning methods for image classification.The survey compares and analyzes 34 different methods based on their performance and commonly used ideas, highlighting the trends and research opportunities in the field.Through comprehensive analysis, the authors reveal the potential of semi-supervised methods for real-world applications and identify challenges such as class imbalance and noisy labels.Furthermore, the paper emphasizes the importance of combining different techniques from various training strategies to improve overall performance.In addition to CNNs, there exist alternative techniques for image classification.The paper [19] presents a comprehensive analysis of Support Vector Machines (SVM) in image classification.It discusses various techniques that can enhance classification accuracy and highlights its advancements.Liu et al. [57] investigate more than 100 different visual Transformers comprehensively in three fundamental CV tasks including classification, detection, and segmentation.They also propose a taxonomy to categorize various transformers into six groups.</p>
<p>Object Detection entails identifying and localizing objects of interest within an image or video.Liu et al. [51] offers a comprehensive survey on the advancements in deep learning-based generic object detection.This paper discusses an extensive range of issues, including detection frameworks, taxonomies, feature depiction, training strategies, and evaluation metrics.Though there have been significant advancements in generic object detection, the detection of small objects, which focuses on identifying objects with a small size, still presents challenges.The review conducted by Cheng et al. [21] investigates 181 literature, constructs two large-scale datasets (SODA-D and SODA-A), and evaluates the performance of mainstream small object detection methods.Object detection demonstrates the utility and effectiveness across multiple domains.Li et al. [47] and Litjens et al. [49] investigate numerous methods and applications of object detection in remote sensing and medical image analysis respectively, showing that these methods have the flexibility to be applied in various scenarios and meet different needs.</p>
<p>Image Segmentation is the process of dividing an image into meaningful and distinct regions to facilitate analysis and understanding.This work referenced 196 papers and received 1900 citations.As described earlier, Minaee et al. [70] proposed a taxonomy for image segmentation methods which divides models into 11 categories.In addition to the taxonomy, the authors evaluate the quantitative performances of various methods on popular benchmarks.The paper also identifies open challenges and proposes promising research directions for future advancements in deep-learning-based image segmentation.Given that most image segmentation algorithms heavily rely on expensive pixel-level annotations, interest in weakly supervised image segmentation methods has increased.Ref [89] surveys label-efficient deep image segmentation methods.According to the paper, weakly supervised segmentation approaches can be categorized into four hierarchical types, ranging from no supervision to inaccurate supervision.The authors investigated each of these four methods in separate sections, highlighting the strategies used to bridge the gap between weak supervision and dense prediction.Image segmentation techniques have a wide range of applications in the field of medical image processing, as introduced in [60], [83], [91], [108].</p>
<p>Natural Language Processing</p>
<p>Acclaimed as the jewel of the artificial intelligence crown, natural language processing (NLP) stands as a pivotal domain within the field of PAMI.Here, we provide a further discussion on several popular NLP research directions.</p>
<p>Named Entity Recognition (NER) involves identifying and classifying named entities in text, such as person names, organizations, locations, and dates.The survey by Li et al. [46] begins by introducing NER resources, including tagged NER corpora and off-the-shelf NER tools.Then, authors categorize existing works based on a taxonomy that considers distributed representations for input, context encoder, and tag decoder.The paper surveys representative methods for applying deep learning in various NER tasks, and provides a valuable reference for designing deep learning-based NER models.NER serves as the foundation technique for various natural language applications, such as relation extraction [74], knowledge graph [2], etc. Due to the linguistic variance of different languages, NER methods may also vary from language to language.Surveys about various language-specific NER could be found in [52], [82], [105].</p>
<p>Sentiment Analysis focuses on determining the sentiment or emotion expressed in text, such as positive, negative, or neutral.Yadav et al. introduce the process of gathering and analyzing people's opinions and sentiments from various sources such as social media platforms and blogs in their paper [109].The paper evaluates and compares different approaches used in sentiment analysis, with a focus on supervised machine learning methods like Naive Bayes and SVM algorithms.The common application areas of sentiment analysis and the challenges involved in accurately interpreting sentiments are also reported.The paper by Yue [114] categorizes and compares a large number of techniques and methods from three different perspectives: taskoriented, granularity-oriented, and methodology-oriented.It also explores different types of data and advanced tools for research, highlighting their strengths and limitations.</p>
<p>Language Modeling involves training models to understand and generate human language.As early attempts, recurrent neural networks achieved desirable performance and wide application at that time, despite some shortcomings.The paper [113] specifically focuses on RNNs and long short-term memory (LSTM) cells.The authors highlight the limitations of traditional RNNs and emphasize the significance of LSTM in handling long-term dependencies.They discuss various LSTM cell variants and their performance on different characteristics and tasks.Furthermore, the paper also categorizes LSTM networks into two major types: LSTM-dominated networks which optimize connections between inner LSTM cells, and integrated LSTM networks which incorporate advantageous features from various components.Recently, Large Language Models (LLMs) have drawn widespread attention.LLMs demonstrate significant performance improvements and unique abilities such as in-context learning, setting them apart from smaller-scale models.By investigating more than 600 works, Zhao et al. [121] conduct a comprehensive review of the recent advancements in LLMs.The authors discuss the evolution of language modeling techniques, from statistical models to neural models, and highlight the emergence of pre-trained language models as a powerful approach in NLP tasks.The survey focuses on LLMs with a parameter scale exceeding 10 billion and explores four key aspects: pre-training, adaptation tuning, utilization, and capacity evaluation.The paper also presents available resources for developing LLMs and discusses important implementation guidelines.Overall, this survey serves as an up-to-date and valuable reference for researchers and engineers interested in the field of LLMs.</p>
<p>Others</p>
<p>Reviews in other popular sub-fields will be investigated in this section.</p>
<p>The paper by Zhou et al. [123] covers the evolution of pretrained foundation models from BERT to ChatGPT and highlights their significance as parameter initializations for downstream tasks.The survey explores popular pre-trained foundation models in text, image, and graph modalities, discussing their components, pre-training methods, and advancements thoroughly.The paper also addresses topics including model efficiency, compression, security, and privacy, while offering valuable insights into scalability, logical reasoning ability, and cross-domain learning.Another survey paper [112] presents a comprehensive review of the stateof-the-art in self-supervised recommendation (SSR).The paper proposes an exclusive definition of SSR and develops a taxonomy that categorizes existing SSR methods into four categories: contrastive, generative, predictive, and hybrid.It further introduces an open-source library called SELFRec, which incorporates a wide range of SSR models and benchmark datasets.Through rigorous experiments and empirical comparison, the paper derives significant findings related to the selection of self-supervised signals for enhancing recommendation.The conclusion highlights the limitations and outlines future research directions in the field of self-supervised recommendation.</p>
<p>AN OBJECTIVE EVALUATION OF LITERATURE REVIEWS</p>
<p>Objective evaluation of literature reviews, or any other academic work, is a crucial aspect of academic research, as it enables the quantitative assessment of a certain academic publication in a relatively fair, transparent, and reproducible manner.It provides a basis for analysis and comparison, which may help readers, authors, reviewers, and editors to ensure the reliability and credibility of the paper.</p>
<p>In this section, we introduce both "external" impact and "internal" quality indicators designed to assess the academic impact and the content reliability of literature reviews.Our methodologies are cost-effective, transparent, and fully automated, facilitating real-time, reproducible evaluations across various dimensions of review papers.We have appraised a wide range of reviews using the proposed measurements, and further provide comparative and visual analysis for reviews in diverse fields, venues, and periods.</p>
<p>Impact Indicators</p>
<p>Perhaps assessing the impact of a paper sounds very simple, but simple things are always the most difficult.Bibliometrics is a research field that uses quantitative analysis and statistics to appraise the impact of scholarly publications.It is believed to play an important role in an individual's academic career, such as grant proposals or candidates for academic positions [45].However, most existing metric methods suffer from several limitations including unfair comparison, misuse, and manipulation as introduced in the "San Francisco Declaration on Research Assessment" (DORA) [29] and the Leiden Manifesto [38].As presented in Tab. 2, many works have tried to address the above-mentioned limitations.For example, Altmetric [98] collects evidence from the social web (X or Twitter, Facebook, etc.) to track and analyze the online attention and engagement that research outputs receive.While Altmetric provides valuable insights into the wider impact and engagement of research, bias toward the English language and none peer-reviewed further limit its broader application.Paper [80] compares two well-known articlelevel field-independent citation metrics, Field-Weighted Citation Impact (FWCI) and Relative Citation Ratio (RCR), and suggests they perform equally well in normalizing citations across research fields.However, these metrics require a pre-defined field (e.g.Scopus All Science Journal Classification category) and thus fail to accurately evaluate papers in newly emerging subfields.The Field Normalized Citation Success Index (FNCSI), as proposed in Ref [90], [96], is defined as the probability that a paper published in Journal A is cited more than a randomly selected paper published in Journal B. While FNCSI is robust, its reliance on pre-defined topic keywords, as well as its exclusive suitability for journal assessment, should be emphasized.</p>
<p>As the Leiden Manifesto [38] states: metrics should "Account for variation by field in publication and citation practices".We develop the concept of impact indicators, which is a measure used to gauge the impact of a certain paper in its field.The reason for using the term "indicator" rather than "metric" is we believe that the impact of a certain paper cannot be fully and accurately measured.While metrics like citation counts, h-index, or journal impact factors could indicate a paper's influence within the academic community, they fail to capture all aspects of its impact.For instance, a research paper might lead to significant advancements in theory, methods, or understanding in its field, none of which would necessarily be reflected in academic metrics.Similarly, a paper might contribute novel concepts or techniques that become influential over time but are not initially evident in citation counts.Furthermore, these metrics don't capture the quality of the research itself, such as the soundness of its methodology or the validity of its conclusions.Hence, the term "indicator" is favored, as it suggests a signal without asserting to encompass the entirety of the academic paper's contribution.</p>
<p>Based on the discussion, we propose the T N CSI and IEI.</p>
<p>Topic Normalized Citation Success Index (TNCSI)</p>
<p>The Topic Normalized Citation Success Index (TNCSI) is a fieldnormalized article-level index that aims to assess the impact In the field of PAMI, citation distribution typically follows an exponential decay pattern, with a small subset of publications receiving the majority of citations.For a better presentation, the data obtained with a citation count greater than 5000 is ignored.</p>
<p>of research publications on a specific topic by normalizing the citation count to a scale ranging from 0 to 1. Based on the selected k papers and the corresponding citation counts p c for each paper p, we can calculate the discrete citation frequency distribution of the k papers in a certain topic.We may further consider the distribution as a probability mass function:
P (X = x) = Citationx k (1)
where Citation x represents the number of papers with x citations.The k papers related to the topic and their meta-data could be retrieved using a pre-defined topic keyword.through online scholar search engines or API, such as Semantic Scholar, CrossRef, or Google Scholar, as mentioned in Sec.2.2.In addition, we can restrict the selection of k papers by filtering out those not published within the specified timeframe to obtain a collection of k ′ papers.This would allow the paper to be evaluated only with papers published in a specific time period (e.g., the same year), thus indicating the relative impact of the paper over a certain period.</p>
<p>We count the citations of papers across various topics with the help of Semantic Scholar API and plot them in Fig. 4. For each keyword, we retrieved up to 1000 (limited by Semantic Scholar) relevant literature using the API.Considering that the number of papers with x citations generally follows an exponential decay, we utilize the maximum likelihood estimation method to fit P (X = x) and obtain the probability density function (PDF) of a continuous exponential decay distribution:
f (x) = λe −λx , x ≥ 0(2)
where f (x) represents the probability density at the value x, and λ is the results obtained from the maximum likelihood estimation, representing the scale parameter controlling the scaling.Finally, the definite integral of f (x) over the interval [0, citeN um] gives us the desired T N CSI:
T N CSI = citeNum 0 λe −λx dx(3)
Specifically, T N CSI s indicates the relative impact of a paper compared to others published in the same year.</p>
<p>The T N CSI demonstrates favorable mathematical properties and interpretability.The T N CSI algorithm employs maximum likelihood estimation to convert the probability mass function into a probability density function.This process ensures that, in theory, the T N CSI differentiates between papers with distinct citation counts, avoiding the assignment of identical values to them.T N CSI has a physical meaning; it is the probability that the citation of the specific paper is greater than the citation of any other paper on the same topic.For example, a paper with a T N CSI of 0.5 means it has more citations than half of the papers within the same topic.</p>
<p>Despite the advantages of T N CSI, it still faces a similar challenge to FWCI and RCR: the need to manually pre-define the topic.To address such a limitation, we propose adopting the ChatGPT [75] (gpt-3.5-turboby default) to generate the topic keyword and retrieve related papers according to the keyword.Then, these retrieved k papers will be used to calculate the discrete citation frequency distribution as afore-mentioned.We denote the resulted T N CSI as aT N CSI, where "a" stands for adaptive.</p>
<p>ChatGPT is one of the most advanced and influential large language models in the field of natural language processing [121].Equipped with state-of-the-art language understanding capabilities, ChatGPT has revolutionized the way we interact with AI-powered conversational systems by simply setting "system", "user", and "assistant" roles.The "system" role sets the conversation's behavior and initial context.It provides instructions to guide the assistant's responses.The "user" role represents the individual interacting with ChatGPT, who inputs messages to the assistant.The "assistant" role is the ChatGPT model itself which would respond based on the provided instructions and user input.</p>
<p>As illustrated in Fig. 5, we ask ChatGPT to identify the most representative topic keyword with the paper's title and abstract.In most scenarios, the generated topic word is sufficient to meet expectations, which can be further used as the keyword to retrieve papers from online scholar search engines.Optionally, one can set "System", "User", and "Assistant" roles before the final query to improve response quality and create more tailored interactions with the ChatGPT.In other word, a few-shot user-assistant pair prompts the ChatGPT with context on topic granularity.For example, a paper about the classification of irises by an improved CNN may have different perspectives.Some researchers focus more on the algorithm of the improved CNN, while others may be interested in classifying irises.Such ambiguity would likewise make it difficult for ChatGPT to identify the most representative topic keyword as expected.However, this could be addressed by providing the context which consists of (1) the identical prompt template with the replaced title and abstract as user input, and (2) the expected topic keyword as assistant output.By default, we adopt a well-known paper [26] as an example to guide models to generate the topic keyword for all papers.</p>
<p>Similar to other LLMs, ChatGPT performs various NLP tasks with user-provided natural language prompts.However, natural language prompts could be ambiguous, and even minor modifications can result in significantly different outputs.Thus, we follow the practices of LLM prompt engineering and carefully design the prompt to optimize the desired output.To determine the optimal prompt, we construct a dataset by manually annotating the topic keywords of 201 papers from various domains published later than the ChatGPT being trained and then compare the You are a profound researcher who is good at identifying the topic keyword for the research paper with given title and abstract.</p>
<p>Identifying the topic of the paper based on the given title and abstract.I'm going to write a review of the same topic and I will directly use it as keyword to retrieve enough related reference papers in the same topic from scholar search engine.Avoid using broad or overly general term like 'deep learning', 'taxonomy', or 'surveys'.Instead, focus on keyword that are unique and directly pertinent to the paper's subject.performance of multiple prompts on this dataset.The normalized edit distance [115] is adopted to measure the similarity between the GPT-generated keyword and our annotated keyword, where a lower value indicates a higher quality of the prompt.As can be seen from Tab. 3, some of the designed prompts achieve decent NED scores for papers in various domains.</p>
<p>Impact Evolution Index (IEI)</p>
<p>Imagine a scenario where two papers, A and B, receive the same number of citations.The number of new citations per month for A remains steady, whereas the number of new citations for B grows exponentially.In this context, while acknowledging the importance of Paper A, it is generally assumed that Paper B holds a greater reference value.Analyzing the popularity or citation trends of the literature may help researchers stay informed about the latest developments and identify potential areas for future research.</p>
<p>Most existing approaches treat the estimation of future citations as a sequence modeling task.Abrishami and Aliakbary propose employing the artificial neural network to predict longterm citations of a paper based on the number of its citations in the first few years after publication [1].Zhao and Feng utilize graph structure representation and recurrent neural network modules to predict paper citation counts [120].A more direct approach is to adopt polynomial fitting for scatter data, and calculate the sum of derivatives at each point.However, in practice, polynomial fitting methods tend to be sensitive to outliers when applied to data lacking discernible distribution patterns, potentially compromising their robustness significantly.Consequently, the numerical values obtained may not accurately reflect the underlying citation trend.In contrast to these series analysis-based methods, we propose a morphological theory-grounded Impact Evolution Index (IEI), which converts the citation trend into a clear and interpretable numerical value.</p>
<p>To calculate the IEI, it is necessary to obtain the citations of the paper first(See more in Sec.2.2).Once the citation data is retrieved, we may create a sequence Seq citation about the number of citations.The i ∈ {0, 1, 2..., l} item in the sequence Seq citation [i] represents the number of received citations in the i th month after the publication, where l represents the number of months allocated for trend observation.Then, a sequence Seq time of the same length as Seq citation is generated by enumerating from 0 to l − 1, Typically, the minimum recommended value for l is 6 or higher.This ensures that the data used for the analysis is adequately representative and the results are reliable.We match the items at the same positions in Seq time and Seq citation to determine a set of discrete coordinates {(Seq time [i], Seq citation [i])}, which serve as the control points for shaping the Bézier curve.A Bézier curve is a mathematical representation of smooth curves commonly used in computer graphics, image editing, and design software.The curve starts at the first control point and ends at the last control point, while the intermediate control points influence the curvature and direction of the curve.The number of control points determines the degree n = l − 1 of the curve.
C(t) = n i=0 B i,n (t)P i (4) B i,n (t) = n i (1 − t) n−i t i , t ∈ <a href="5">0, 1</a>
where B i,n (t) represents the coefficient of the Bézier curve at a given parameter value t, which determines the position along the curve (t = 0 means the start and t = 1 means the end).n i is the binomial coefficient, also known as "n choose i".It represents the number of ways to choose i elements from a set of n elements.P i stands for the i th control point of the curve.</p>
<p>Given the continuity of the Bézier curve, we can compute its derivative as follows:
C ′ i,n (t) = n n−1 i=0 n − 1 i (1 − t) n−1−i t i (P i+1 − P i )(6)
Finally, the IEI L l can be obtained by averaging the deriva- tives of l = n + 1 points, as shown in Eq. (7).Note that the value of l can be configured flexibly to meet actual demands.In general, the longer the period analyzed, the more stable the citation trend becomes.We usually prefer to analyze the most recent 6 months of citations when constructing a Bézier curve of degree 5. Furthermore, different months can contribute differently to the IEI.For instance, if we desire closer months to have a greater impact, we can achieve this by adjusting the weighting coefficients, w i , of the derivatives at different points to calculate their weighted averages (See in Eq. ( 8)).In addition, the instantaneous trend could be regarded as the derivative of the last month in the sequence.It can be obtained by setting w l = 1 and the other weighting coefficients to 0. We denote the IEI focused on the last month among the latest l months (excluding the current month) as IEI I l (See in Eq. ( 9)).
IEI L l = (C ′ 0,l−1 (0) + l−1−2 i=0 C ′ i,l−1 (1))/(l − 1)(7)IEI W l = w 0 C ′ 0,l−1 (0) + w 1 C ′ 0,l−1 (1) + . . . + w l C ′ l−1−2,l−1 (1)(8)IEI I l = C ′ l−1−2,l−1 (1)(9)
In general, the longer the period being analyzed, the more stable the citation trend becomes.We usually prefer to analyze the most recent 6 months of citations when constructing a Bézier curve of degree 5</p>
<p>Quality Indicators</p>
<p>Assessing the quality and credibility of a literature review plays a pivotal role in the objective evaluation.Despite its significance, there have been minimal scholarly attempts to quantify the quality of literature reviews or any other types of publications.The paucity of such attempts may be attributed to the complexity and subjectivity involved in the process.However, the emergence of LLMs makes the quantitative quality evaluation for literature reviews no longer out of reach.By adopting an objective evaluation approach, we may move beyond the limitations of traditional metrics (such as simply regarding the citation numbers or the journal impact factor as the paper quality), thus providing a more objective and reproducible evaluation method.</p>
<p>We propose a set of indicators to evaluate literature reviews from the perspectives of reference quality and update urgency.</p>
<p>Reference Quality Measurement (RQM)</p>
<p>A literature review, in its essence, can not fabricate insights from a void.It fundamentally relies on the substance of existing references.Without a solid foundation of credible and high-quality sources, a literature review may lack the necessary building blocks to construct a meaningful analysis or argument.These sources provide the empirical evidence and theoretical context that ground the review, making the role of references indispensable in the creation of a substantial literature review.</p>
<p>The reference quality of a literature review is a multifaceted concept.It usually involves several direct factors such as credibility, relevance, breadth, and depth, etc. Quantitatively evaluating these direct elements poses significant challenges.On the one hand, the relevance of scientific literature is difficult to be precisely defined.Relying on co-citation analysis [92] or the similarity of paper embeddings both suffer from conceptual limitations.On the other hand, practical limitations are evident.For instance, the concept of breadth, also known as coverage, within a literature review, can theoretically be quantified as the ratio of the number of references to the total number of relevant references.However, accurately determining this ratio is challenging.It is difficult to ascertain the complete number of relevant references either by keyword search or by citation network.</p>
<p>Due to the challenges of quantifying the reference quality using the above-mentioned direct factors, we consider an indirect quality indication of each reference with the assistance of the T N CSI presented in Sec 4.1.1.Here, the T N CSI can be regarded as a reference quality indicator based on user voting, which is a statistical result of numerous researchers' comprehensive analyses of those direct factors of a certain paper.Timeliness also matters.A current and up-to-date literature review ensures that the most recent advancements, developments, and perspectives in a particular field are taken into consideration.This temporal relevance enhances the accuracy and effectiveness of research outcomes, as it reflects the current state of knowledge and understanding.By focusing on the currency of the references, we can gauge the extent to which the literature review incorporates the latest research and developments in the field.</p>
<p>To simultaneously consider the quality and timeliness of cited references, we propose modifying the Gompertz function to model the reference quality (see in Eq. ( 10)).The Gompertz function is characterized by its sigmoidal, which indicates a slow growth rate at the start and end of a time period, with a more rapid growth in the middle phase.This pattern is often observed in natural phenomena, such as species dynamics [16], tumor growth [99], etc.
RQM = 1 − e −β•e −(1−ARQ)•Smp (10)
where β is the shift parameter, ARQ stands for average reference quality, and S mp represents the median semester count of the reference age [94], defined as the period spanning from the publication dates of the cited references to the issuance of the review.</p>
<p>The calculation procedures of ARQ are as follows: The first step is the extraction of the cited reference list.For most publications, their reference lists could be provided by Semantic Scholar API.For a small number of reviews, the reference list provided by Semantic Scholar may contain errors.In this case, al-though there are powerful computer vision-based algorithms [14], [22] available for extracting the reference list within PDFs, our requirements are relatively simple and can be effectively met by relying on the heuristic algorithms or ChatGPT.More specifically, for literature review with a relatively fixed citation format, we can use the PDFMiner [79] to read text from PDF files and use heuristic rules to match citations.Alternatively, the text can be analyzed using ChatGPT to extract in-text citations.The second step is similar to the approach presented in Sec.4.1.1,where the ChatGPT and a well-designed prompt (as presented in Fig. 5) are utilized to obtain the topic keyword of the review.Next, we calculate the T N CSI for each reference in the list.To conserve computational resources, we avoid using the ChatGPT to generate keywords for each reference.Instead, the T N CSI of all cited literature is calculated using a sharing topic keyword.Finally, the coverage can be further calculated in Eq. ( 11):
ARQ = N R i=1 T N CSI(Ref i ) N R(11)
where T N CSI(•) refers to the T N CSI value of the i th cited reference, and N R stands for the number of the reference.In certain instances, it has been noted that calculating T N CSI s for each cited literature is also reasonable.However, this paper primarily emphasizes the current impact of the cited references, hence the utilization of T N CSI in this context.The shift parameter β can be set empirically or obtained statistically.For statistical calculation, we first examine the distribution of S mp and ARQ across all papers within the RiPAMI database, and obtain their respective mean values S mp and ARQ.Then, the problem of asserting for β is reconceptualized as an optimization problem.As shown in Eq. ( 12), the objective here is to identify the value of β that maximizes the derivative of RQM (S mp ; β, ARQ), subject to the constraints of S mp = 8 and ARQ = 0.6.Such an approach endows RQM with a more discriminative nature.It should be noted that different fields may result in distinct values of β.For this study, the β has been established as 20.
β opt = arg max β RQM ′ (S mp ; β, ARQ)(12)
The range of RQM extends from 0 to 1, where values closer to 1 signify a higher quality of the referenced literature.As illustrated in Fig. 6 (a), when the ARQ of a paper remains constant, an increase in the variable S m p will lead to a decrease in the RQM value.Conversely, when S m p remains constant, a higher ARQ will elevate the RQM value.</p>
<p>Review Update Index (RUI)</p>
<p>The RU I refers to the measure of the extent to which a literature review is required to be updated due to the iteration of technology, theory, etc.The index is related to both the literature itself and the research interests of the topic.Generally, a high update index suggests that a literature review is in need of an immediate update.Conversely, a lower update index implies that few advances have been made to the investigated field and the review is still up-todate.</p>
<p>To evaluate the RU I, we may start with the coverage of references before and after publication.This coverage ratio can, to some extent, indicate the extent to which a review requires updating within its field.However, as mentioned earlier, accessing the coverage of a review is difficult.Fortunately, this problem is subtly avoided in calculating the ratio of relevant papers before and after publication.Assuming that the ratio of references containing the topic keyword in the title to all references is R k , the total number of relevant articles can be estimated by dividing the number of articles containing those keywords retrieved from a search engine by R k .Note that R k generally remains consistent before and after the publication, the Coverage Difference Ratio (CDR) can then be calculated in Eq. ( 13).The theoretical value range of CDR is greater than 0 to positive infinity.When the CDR of a review equals 1, it indicates that the current field has yielded new publications sufficient to constitute half of the literature referenced in the review.
CDR = N pc • R k R k • N mp = N pc N mp(13)
where N mp and N pc denote the number of relevant literature from the median publication date of the cited references to the publication date of the review, and from the publication date of the review to the current time, respectively.</p>
<p>In addition, similar to the inevitable process of biological aging, literature reviews also undergo a gradual aging process throughout time.Such passage of time bestows upon literature reviews increasing aging progress, where the degree of aging can be conceptualized as a normalized value of the academic impact already achieved.To further explore the aging of reviews in the field of PAMI, we conducted a statistical analysis of the yearly number of newly received citations for reviews published between 2015-2017 in RiPAMI.In contrast to earlier findings, however, the distribution of received citations of reviews over time follows a t-distribution rather than a log-normal distribution of the regular paper, as previously reported in Ref. [27], [66], [71].Due to the insufficient duration of published sample data, the observation of citation-time trend curves is incomplete.Therefore, we conducted a three-degree polynomial fitting on the limited 6-year citation trend data and transformed the positive segment of the fitted curve into a PDF.To obtain the corresponding cumulative distribution function (CDF), we employed the cumulative trapezoidal numerical integration method for an approximate estimation.Thus, the Review Aging Degree (RAD) is given by:
RAD(M pc ) = Mpc/12 0 (px 3 + qx 2 + rx + s) dx(14)
where M pc denotes the duration in months from the publication of the review to the present, p = −0.003,q = 0.001, r = 0.1267, s = 0.0129 are the coefficients obtained by polynomial fitting.Please note that the integral symbol used here is for illustrative purposes only.The strict mathematical definition involves the accumulation of discrete trapezoidal areas.Finally, the RU I could be obtained by weighted summation of CDR and RAD:
RU I = p • CDR + q • RAD(x)(15)
where p and q are set to 10 and 5 in this paper, respectively.A sculptural visualization of the RU I's contours is crystallized in Fig. 6 (b).</p>
<p>Quantitative Evaluations of Literature Reviews</p>
<p>We have carefully selected numerous reviews within the field of PAMI to ensure a representative sample for our evaluation.More numerical results can be found in Tab. 4. As can be seen from the table, literature reviews [7], [18] of different topics that receive approximately the same number of citations exhibit notable discrepancies in their associated aT N CSI.This diver-gence primarily stems from the varying levels of research interest across their respective topics.Surveys [101] in some emerging fields reveal a significant increase in their IEI, as expected.The majority of leading journals demonstrate high RQM values, which reflect the effectiveness of high-standard peer review.In addition, even if the topics covered by the reviews [117], [119] are similar and their publication dates are relatively close, the proposed RU I still provide some indication of the extent to which they need to be updated.Note that, given the time span of less than six months, the calculation of RU I for certain papers is unavailable (denoted with "-").</p>
<p>HUMAN-AUTHORED VS. AI-GENERATED</p>
<p>Overview of AI-generated Literature Reviews</p>
<p>Traditionally, literature reviews have been manually conducted by researchers who analyze and synthesize scholarly sources to provide an overview of the current state of knowledge in a specific field.Recently, with the advancement of AI technologies, there's been a growing interest in leveraging artificial intelligence techniques, especially large language models, to automate or assist in the generation of the literature review.Typically, users are simply required to indicate their area of research interest, and the system will then automatically generate a literature review.</p>
<p>The automated creation of a literature review is a multidisciplinary endeavor that integrates knowledge from various fields.It relies not only on artificial intelligence technologies but also requires the merging of knowledge from other fields such as data science, bibliometrics, database engineering, etc.These components are instrumental in extracting, storing, and synthesizing relevant information from vast amounts of literature.</p>
<p>Early attempts in AI-generated literature reviews involve training language models, e.g.LLama [97], on a large corpus of academic papers, research articles, and other scholarly content.These models can then be used to generate coherent and contextually relevant text based on prompts or queries related to a specific  research topic.However, given that a fully trained or fine-tuned language model has no access to the latest scholarly advances, these models would not generate a literature review that includes the latest scholarly materials.Thus, most existing advanced AIgenerated literature review systems contain three main steps: knowledge retrieval, synthesis, and report.The generation of literature review begins by gathering knowledge from various sources.There is no doubt that a high-quality knowledge retrieval procedure provides a richer context for the LLMs, resulting in the production of more accurate and decent responses.The system usually employs several predefined criteria such as keywords, publication date ranges, and citation numbers to locate, filter, and retrieve relevant resources to be synthesized.These resources are not limited to academic publications, but may also include blogs, official tutorials, and GitHub repositories.Once the relevant knowledge is retrieved, the AI system aims to synthesize the gathered information to create a coherent and comprehensive literature review.This step involves analyzing the retrieved content, identifying relationships between different sources, and organizing the information into meaningful sections.The system may employ LLMs and elaborate prompts to generate the text for each section or the entire survey at once.Finally, the system seamlessly transitions into the report generation stage, where a formatted review is crafted in a cohesive and wellstructured style.This stage involves converting the synthesized content into a final report which is ready for presentation.One of the simplest and most common forms of presentation is plain text.For certain systems, the generated text is arranged in various sections based on the predefined setting.Such a multi-step procedure not only streamlines the literature review generation workflow but also ensures the production of acceptable content.</p>
<p>Though researchers are expecting to save significant time and effort in conducting comprehensive surveys by embracing AIgenerated literature reviews, numerous concerns about ethical issues and fabrications in AI-generated academic content have been raised.A critical appraisal by Zybaczynska [124] highlights that current AI systems typically fall short in providing substantial, accurate information and critical discernment.Elali et al. [28] critically examines the profound challenges brought by the fabrication and falsification of AI-generated research, underscoring its significant impact on the scientific community.Furthermore, numerous academic publishers are cautious about content created by AI.For instance, journals such as Nature" and Science" prohibit listing ChatGPT or other automated tools as authors on papers published in their issues.Nevertheless, we argue that the automated generation of literature reviews not intended for publication remains meaningful.It may assist researchers in staying rapidly abreast of the latest advancements in thriving fields.</p>
<p>Current State of AI-generated Literature Review</p>
<p>In this subsection, our investigation delves into various efforts that have been made in the field of employing AI techniques to generate literature reviews.</p>
<p>Most of the existing systems are only capable of generating literature reviews in plain text form and with a relatively fixed layout.A popular Github repository ChatPaper [43] retrieves papers based on a user-defined keyword with the use of the Google Scholar crawler.After a cosine similarity-based filtering of the retrieved papers, the selected papers are processed sequentially using ChatGPT.The output of Chatpaper is brief and concise, which usually contains a narrative description of the selected papers.Paper Digest [77] is an AI-based online platform that provides the service of automated review generation.Users can generate a literature review by setting specific keywords and further refining the cited source through constraints on publication dates.Similar to ChatPaper, the output of Paper Digest is also a narrative plain text description.Not all systems for generating literature reviews are fully automated.For example, Jenni AI [4] requires user interaction through "AI Command" during the review creation process, enabling the generation of highly customized literature reviews.Additionally, there are numerous online platforms [15], [86] and plugins available in the GPT Store that offer automated literature review generation services.Given the constraints on manuscript length, detailed introductions to these systems will not be provided.</p>
<p>Despite previous discussions indicating that certain publishers refuse to accept non-human authors, scholars have still successfully published AI-generated literature reviews in some of academic journals.Aydın et al. investigate how well can LLMs perform in the generation of the literature review.They employed well-known LLMs including ChatGPT and Google Bard to generate reviews on digital twin in healthcare [9] and metauniverses [8].In their attempts, the Google Bard (or the ChatGPT) was first adopted to paraphrase the abstracts of papers within the last 3 years.Then, they designed several question prompts (such as "What is digital twin") to query the LLMs and rearranged the response in a formatted style.</p>
<p>While promising progress has been achieved in AI-generated literature reviews, there remains considerable room for improvement.The pursuit of automated literature reviews that rival the quality of those written by humans is an ongoing area of research, underscoring the need for continued exploration and development in this field.For example, most existing literature review generation systems are not capable of extracting information from tables or figures.As a result, tables and figures will not be included in the generated reviews.More details about the differences between AI and human-authored reviews will be presented in the next subsection.</p>
<p>Case Study: Comparing Human and AI in Conducting Literature Reviews</p>
<p>We crafted a case study aimed at elucidating the distinctive approaches employed by human researchers and their AI counterparts.To prevent potential information leakage and ensure a fair comparison due to the possibility of ChatGPT having been exposed to relevant reviews during training, we selected a newly emerging field, i.e. prompt learning for large visual models, which developed after September 2021 (the cutoff date for ChatGPT's training), as the focus of our investigation.Considering that literature reviews generated by AI lack academic influence, we only calculate RQM (as introduced in Sec.4.2) of them.</p>
<p>The data presented in Tab 5 reveals that, at present, most AI-generated reviews fail to match the quality of those crafted by human authors.There is still a gap in knowledge retrieval, synthesis, and reporting steps where AI systems lag behind human performance.Despite the high RQM achieved by the PaperDigest, the overall quality of the generated review still falls short of the human-authored counterpart, especially given its reliance on only 5 references.Additionally, to our knowledge, few automated review systems can seamlessly integrate visual elements such as figures and tables.This landscape of predominantly plain textbased AI-generated reviews invites intriguing possibilities for future exploration and enhancement in the field.</p>
<p>It is noteworthy that despite the higher quality of the humanauthored review [100], the timeline from draft to journal acceptance, as indicated by its preprint publication and acceptance dates, extends over six months.In contrast, AI systems can generate reviews within seconds.This delay implies that in rapidly advancing research areas, some of the newest findings might be overlooked by human authors.</p>
<p>CHALLENGES AND THE FUTURE OF THE LITER-ATURE REVIEW</p>
<p>Challenges and future opportunities for both human-authored and AI-generated reviews are discussed in this section.</p>
<p>Challenges</p>
<p>High Rate of obsolescence Knowledge in scientific research continually evolves and updates.Literature reviews require significant time and effort to compile.Due to the slower pace of review crafting, they may not reflect the latest research advancements in a timely manner, especially when these reviews are conducted by human authors and in need of a peer-review process.</p>
<p>Information Overload While our proposed indicators mitigate the problem of information overload to a certain degree, several challenges still await resolution.With the continuous growth of scientific research, the volume of literature is rapidly expanding.Collecting and screening a large number of publications within those blooming fields would cause inevitable incomplete searches and other similar issues.Both human and AI systems have to explore how to retrieve relevant literature more comprehensively and effectively.</p>
<p>Bias Literature reviews conducted in a specific language or region may unintentionally omit relevant studies published in other languages or regions.This language and geographical bias can limit the global perspective and generalizability of the review's findings.Such a bias appears not only in literature reviews written by humans but also in those generated by AI systems (even featuring multilingual capabilities).</p>
<p>Future</p>
<p>AI Empowerment Dynamic Literature Review In the rapidly advancing landscape of scientific research, there is a growing trend toward dynamic reviews generated in real time.While debates persist regarding the adherence of AI-generated content to academic ethics and its suitability for formal publication, the generated reviews for non-publishing purposes remain widely embraced.Given the labor-and time-intensive nature of conducting literature reviews, the exploration of AI technology for the automated generation of dynamic literature reviews stands as an area warranting further investigation.</p>
<p>Automated Literature Appraisal Literature appraisal is important for both human authors and AI systems.Although four quantitative indicators are proposed in this paper, this still makes it difficult to fully capture the academic impact and the quality of a certain review paper.In the future, with more powerful large multimodal models (LMMs) [3], [50], it will be possible to extract valuable information from non-textual modal content (e.g., images and tables) for automated literature appraisal.</p>
<p>Open Science and Advanced Search Engines The open science movement endorses the sharing and accessibility of literature data, thereby providing a greater number of resources for analysis and synthesis.It is recommended that the percentage of open-access papers should be further increased in the era of e-publishing.Furthermore, search engines that rely on semantic similarity, recommend systems, or co-citation networks rather than keyword matching are also encouraged.Such engines will primarily benefit both researchers and AI systems by enabling them to retrieve more relevant papers, thus enhancing the quality of any publications in all of the research fields.</p>
<p>CONCLUSION</p>
<p>This Analysis has provided a fresh perspective on the wealth of literature reviews in PAMI, introducing a systematic approach to categorize and evaluate them.In total, it presents four large language models-empowered quantitative evaluation indicators, a subjective evaluation that includes a typology for literature reviews, a comparison between human-authored and AI-generated  reviews, and a meta-data database named RiPAMI, accompanied by a dataset of review topic key phrases.</p>
<p>The proposed evaluation indicators offer an innovative alternative to traditional bibliometric analysis, enabling a cost-efficient, field-normalized, and real-time assessment of literature reviews' quality and impact.These bibliometric indicators not only furnish clear numerical hints for human researchers but could also offer substantial assistance in the appraisal processes of AI-generated review systems.</p>
<p>Subject evaluations for reviews in PAMI are also provided.Numerous representative reviews of popular research topics are investigated to offer readers a brief overview of these exemplary reviews.In addition, a typology for reviews is introduced.It category the reviews into three major types which are method clustering-based, challenge-oriented, and hybrid literature review.Such typology enhances the understanding of how literature reviews are organized and synthesized.</p>
<p>The study highlights the differences between human-authored and AI-generated literature reviews, pinpointing significant gaps in retrieval, synthesis, and reporting capabilities of AI-generated reviews compared to those crafted by humans.The insights gained from this comparison not only help to understand the current state of AI-generated reviews in PAMI but also suggest how they might evolve with advancing technology.</p>
<p>By constructing the RiPAMI database, we analyze approximately 3,000 review samples in the field of PAMI, which provides statistical support for the proposed quantitative indicators.Furthermore, a dataset of topic keywords is manually annotated to validate the effectiveness of various prompts, enabling the selection of the optimal prompt for more accurate data retrieval.</p>
<p>To further support and expand upon this work, we release the code framework that encompasses functionalities including metadata retrieval, indicator calculation, and data analysis, etc.While it is designed for analysis of literature reviews in PAMI, the framework is universally adaptable, catering to the expansive needs of researchers from disparate fields.</p>
<p>In Practice, 2023.</p>
<p>APPENDIX A VISUALIZATION OF THE PROPOSED IMPACT INDICA-TORS.</p>
<p>To provide readers with a clear understanding of the proposed quality metrics, We render the graphical representations of T N CSI and IEI in Fig. 8 and Fig. 9, respectively.</p>
<p>As can be seen in Fig. 8, TNCSI equals the area under the probability density function curve, which is fitted with the use of maximum likelihood estimation.In Fig. 9, the horizontal axis labeled 0 to 5 inversely denotes the months prior to the current month, with 0 representing 6 months ago and 5 denoting the previous month.The corresponding IEI is approximately −0.76, which indicates a slightly decreasing citation trend of the investigated review over the last six months.</p>
<p>Fig. 1 .
1
Fig. 1.Annual publication trends of literature review in the field of PAMI.A notable rising trend can be observed since 2015, reflecting an increasing scholarly focus and growing recognition of the importance of review articles in synthesizing the state of research in PAMI.The data is collected from the Google Scholar search engine.</p>
<p>Fig. 3 .
3
Fig. 3. Statistics of the RiPAMI database.The samples in the RiPAMI database are characterized by a diversity of publication dates, scholar impact, reference numbers, etc., offering a comprehensive reflection of the state of reviews in the PAMI field.</p>
<p>Fig. 4 .
4
Fig. 4. Distribution of scholarly citations: a comparative histogram of paper counts versus received citations across popular topics in PAMI.In the field of PAMI, citation distribution typically follows an exponential decay pattern, with a small subset of publications receiving the majority of citations.For a better presentation, the data obtained with a citation count greater than 5000 is ignored.</p>
<p>Fig. 5 .
5
Fig. 5. Conceptual illustration of topic keyword generation process.Fewshot prompting may enhance the response quality of the large language model.</p>
<p>Fig. 6 .
6
Fig. 6. 3D visualization of the proposed quality indicators.Panels (a) and (b) respectively depict the value landscapes of RQM and RU I, shaped by two independent variables.</p>
<p>Fig. 7 .
7
Fig. 7. Visualization of references quality of randomly selected reviews published in different journals.Panel (a) displays the citation quality distribution for a journal with an IF Score above 20, while Panel (b) shows the same for a journal with an IF Score below 5.This suggests that reviews published in journals with higher impact factors tend to reference sources of superior quality and greater timeliness.</p>
<p>Fig. 8 .Fig. 9 .
89
Fig. 8. Visualization of the proposed TNCSI.The TNCSI equals the area under the fitted probability density function curve.</p>
<p>TABLE 1 Comparisons between various data source. Selected Keywords SQL TABLE Description Data Type Column Serve as primary key, a UUID. VARCHAR(36) ID The title of the review. Not Null. LONGTEXT Title
1Publication DateDATETIMEThe first publication date of the review.CitationsINTThe number of received citation counts.………</p>
<p>TABLE 2 Metrics for Evaluating Scholar Impact of Papers:
2
"A" and "J" stand for article-level and journal-level.Filed normalized signifies that the metric can be utilized across fields.
MetricAssessingNormalizedPre-definedLevelKeywords FreeCitation Counts A/J×✓Impact FactorJ××FNCSI [96]JField×CiteScore [95]J××SNIP [72]JField×FWCI [30]AFiled×RCR [41]AFiled×TNCSI(Ours)A/JFiled and Value ×aTNCSI(Ours)A/JFiled and Value ✓</p>
<p>and utilize the curve to calculate IEI L6 , IEI W6 , and IEI I6 .Please analyze the title and abstract provided below and identify the main topic or central theme of the review paper.Focus on key term and the overall subject matter to determine the primary area of research or discussion.The output should be formatted as following: xxx Given title and abstract, please provide the searching key phrase for me so that I can use it as keyword to search highly related papers from Google Scholar or Semantic Scholar.Please avoid responding with overly general keyword such as deep learning, taxonomy, or surveys, etc. Answer with the words only in the following format: xxx Identifying the topic of the paper based on the given title and abstract.Avoid using broad or overly general term like 'deep learning', 'taxonomy', or 'surveys'.Instead, focus on keyword that is unique and directly pertinent to the paper's subject.Answer with the word only in the following format: xxx Identifying the topic of the paper based on the given title and abstract.So that I can use it as keyword to search highly related papers from Semantic Scholar.Avoid using broad or overly general term like 'deep learning', 'taxonomy', or 'surveys'.Instead, focus on keyword that is unique and directly pertinent to the paper's subject.Answer with the word only in the following format: xxx Identifying the topic of the paper based on the given title and abstract.I'm going to write a review of the same topic and I will directly use it as keyword to retrieve enough related reference papers in the same topic from scholar search engine.Avoid using broad or overly general term like 'deep learning', 'taxonomy', or 'surveys'.Instead, focus on keyword that are unique and directly pertinent to the paper's subject.Answer with the word only in the following format: xxx Identifying the topic of the paper based on the given title and abstract.I'm going to write a review of the same topic and I will directly use it as keyword to retrieve enough related reference papers in the same topic from scholar search engine.Avoid using broad or overly general term like 'deep learning', 'taxonomy', or 'surveys'.Instead, focus on keyword that are unique and directly pertinent to the paper's subject.Answer with the word only in the following format: xxx
NO. User Prompt ContentFew-shotNED↓1×0.752×0.403×0.364×0.325×0.296✓0.28</p>
<p>TABLE 3
3
Effectiveness of prompt engineering: comparison of various user prompts.</p>
<p>TABLE 4
4
Comparison of various reviews with the proposed indicators and metrics.
Meta-Data</p>
<p>A typically narrative, method clusteringbased review aims to offer valuable insights in visual prompt learning.The selection criteria for references are not specified, but it's clear that each reference has been thoroughly appraised.Searching papers with the user-specified keyword.It seems to appraise the quality of references with private criteria.The generated content is more like a summary.No official explanations are found for how to retrieve references and appraise the quality of the literature.The generated review includes a brief analysis and description of the related concept, current state of development, and gaps.
ReviewReference RQM↑ Automation Level Visual ElementsSALSA Analysis [31][100] by human authors1600.99Manullay✓Automated searching and appraising ref-Review by Jenni [4]--Semi-automated×erences is not supported. The generation process relies on user interaction, and onlynarrative description is provided.Retrieving relevant literature from arXivReview by ChatPaper [43]150Automated×based on multiple LLM-generated keywords with no appraisal step. Each section containsplain descriptions of the related reference.Review by PaperDigest [77]50.99Automated×Review by askyourpdf [15]90.31Automated×</p>
<p>TABLE 5
5
Comparisons between human-authored and AI-generated literature reviews.</p>
<p>Predicting citation counts based on deep neural network learning techniques. A Abrishami, S Aliakbary, Journal of Informetrics. 1322019</p>
<p>Named entity extraction for knowledge graphs: A literature overview. T Al-Moslmi, M G Ocaña, A L Opdahl, C Veres, IEEE Access. 82020</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>Biomedical image segmentation: a survey. Y Alzahrani, B Boufama, SN Computer Science. 22021</p>
<p>Few-shot object detection: A survey. S Antonelli, D Avola, L Cinque, D Crisostomi, G L Foresti, F Galasso, M R Marini, A Mecca, D Pannone, ACM Computing Surveys (CSUR). 5411s2022</p>
<p>Geometric deep learning on molecular representations. K Atz, F Grisoni, G Schneider, Nature Machine Intelligence. 3122021</p>
<p>Google bard generated literature review: metaverse. Ö Aydin, Journal of AI. 712023</p>
<p>Openai chatgpt generated literature review: Digital twin in healthcare. Ö Aydın, E Karaarslan, SSRN 4308687. 2022</p>
<p>Analysis of the hands in egocentric vision: A survey. A Bandini, J Zariffa, IEEE transactions on pattern analysis and machine intelligence. 2020</p>
<p>Google scholar's ranking algorithm: an introductory overview. J Beel, B Gipp, Proceedings of the 12th international conference on scientometrics and informetrics (ISSI'09). the 12th international conference on scientometrics and informetrics (ISSI'09)Rio de Janeiro20091</p>
<p>Academic search engine spam and google scholar's resilience against it. J Beel, B Gipp, Journal of electronic publishing. 1332010</p>
<p>On the robustness of google scholar against spam. J Beel, B Gipp, Proceedings of the 21st ACM Conference on Hypertext and Hypermedia. the 21st ACM Conference on Hypertext and Hypermedia2010</p>
<p>Nougat: Neural optical understanding for academic documents. L Blecher, G Cucurull, T Scialom, R Stojnic, 2023</p>
<p>Ai-powered literature review generator. O Blocktechnology, 2023. 25 December 2023</p>
<p>Application of the gompertz function in studies of growth in dusky salamanders (plethodontidae: Desmognathus). R C Bruce, Copeia. 10412016</p>
<p>Power laws in citation distributions: evidence from scopus. M Brzezinski, Scientometrics. 1032015</p>
<p>Ensemble deep learning in bioinformatics. Y Cao, T A Geddes, J Y H Yang, P Yang, Nature Machine Intelligence. 292020</p>
<p>Survey on svm and their application in image classification. M A Chandra, S Bedi, International Journal of Information Technology. 132021</p>
<p>Review of image classification algorithms based on convolutional neural networks. L Chen, S Li, Q Bai, J Yang, S Jiang, Y Miao, Remote Sensing. 132247122021</p>
<p>Towards large-scale small object detection: Survey and benchmarks. G Cheng, X Yuan, X Yao, K Yan, Q Zeng, X Xie, J Han, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023</p>
<p>M6doc: A large-scale multi-format, multi-type, multi-layout, multi-language, multi-annotation category dataset for modern document layout analysis. H Cheng, P Zhang, S Wu, J Zhang, Q Zhu, Z Xie, J Li, K Ding, L Jin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>A taxonomy of literature reviews. H M Cooper, 1985</p>
<p>Organizing knowledge syntheses: A taxonomy of literature reviews. H M Cooper, Knowledge in society. 111041988</p>
<p>A systematic review of aspectbased sentiment analysis (absa): Domains, methods, and trends. P Denny, K Taskova, J Wicker, arXiv:2311.107772023arXiv preprint</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, International Conference on Learning Representations. 2020</p>
<p>Citation age data and the obsolescence function: Fits and explanations. L Egghe, Information Processing &amp; Management. 2821992</p>
<p>Ai-generated research paper fabrication and plagiarism in the scientific community. F R Elali, L N Rachid, Patterns. 432023</p>
<p>. A S For, Cell Biology, San francisco declaration on research assessment. dora2012</p>
<p>FWCI. Field-weighted citation impact. 25 December 2023</p>
<p>A typology of reviews: an analysis of 14 review types and associated methodologies. M J Grant, A Booth, Health information &amp; libraries journal. 2622009</p>
<p>A survey on selfsupervised learning: Algorithms, applications, and future trends. J Gui, T Chen, Q Cao, Z Sun, H Luo, D Tao, arXiv:2301.057122023arXiv preprint</p>
<p>Deep learning for 3d point clouds: A survey. Y Guo, H Wang, Q Hu, H Liu, L Liu, M Bennamoun, 202043</p>
<p>A survey on vision transformer. K Han, Y Wang, H Chen, X Chen, J Guo, Z Liu, Y Tang, A Xiao, C Xu, Y Xu, Z Yang, Y Zhang, D Tao, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4512023</p>
<p>A brief survey on semantic segmentation with deep learning. S Hao, Y Zhou, Y Guo, Neurocomputing. 4062020</p>
<p>Recent progress in leveraging deep learning methods for question answering. T Hao, X Li, Y He, F L Wang, Y Qu, Neural Computing and Applications. 2022</p>
<p>Evolution of the scientific paper. J E Harmon, 1992Argonne National Lab., IL (United StatesTechnical report</p>
<p>Bibliometrics: the leiden manifesto for research metrics. D Hicks, P Wouters, L Waltman, S De Rijcke, I Rafols, Nature. 52075482015</p>
<p>A review of the gumbel-max trick and its extensions for discrete stochasticity in machine learning. I A Huijben, W Kool, M B Paulus, R J Van Sloun, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4522022</p>
<p>A comprehensive survey of multi-view video summarization. T Hussain, K Muhammad, W Ding, J Lloret, S W Baik, V H C De Albuquerque, Pattern Recognition. 1091075672021</p>
<p>Relative citation ratio (rcr): a new metric that uses citation rates to measure influence at the article level. B I Hutchins, X Yuan, J M Anderson, G M Santangelo, PLoS biology. 149e10025412016</p>
<p>From image to language: A critical analysis of visual question answering (vqa) approaches, challenges, and opportunities. M F Ishmam, M S H Shovon, M Mridha, N Dey, arXiv:2311.003082023arXiv preprint</p>
<p>. Kaixindelele, Chatpaper, 2023</p>
<p>Transformers in vision: A survey. S Khan, M Naseer, M Hayat, S W Zamir, F S Khan, M Shah, ACM computing surveys (CSUR). 5410s2022</p>
<p>The role of metrics in peer assessments. L Langfeldt, I Reymert, D W Aksnes, Research Evaluation. 3012021</p>
<p>A survey on deep learning for named entity recognition. J Li, A Sun, J Han, C Li, IEEE Transactions on Knowledge and Data Engineering. 3412020</p>
<p>Object detection in optical remote sensing images: A survey and a new benchmark. K Li, G Wan, G Cheng, L Meng, J Han, ISPRS journal of photogrammetry and remote sensing. 1592020</p>
<p>A survey of convolutional neural networks: analysis, applications, and prospects. IEEE transactions on neural networks and learning systems. Z Li, F Liu, W Yang, S Peng, J Zhou, 2021</p>
<p>A survey on deep learning in medical image analysis. G Litjens, T Kooi, B E Bejnordi, A A A Setio, F Ciompi, M Ghafoorian, J A Van Der Laak, B Van Ginneken, C I Sánchez, Medical image analysis. 422017</p>
<p>H Liu, C Li, Q Wu, Y J Lee, Visual instruction tuning. 2023</p>
<p>Deep learning for generic object detection: A survey. L Liu, W Ouyang, X Wang, P Fieguth, J Chen, X Liu, M Pietikäinen, International journal of computer vision. 1282020</p>
<p>Chinese named entity recognition: The state of the art. P Liu, Y Guo, F Wang, G Li, Neurocomputing. 4732022</p>
<p>Recent few-shot object detection algorithms: A survey with performance comparison. T Liu, L Zhang, Y Wang, J Guan, Y Fu, J Zhao, S Zhou, ACM Transactions on Intelligent Systems and Technology. 1442023</p>
<p>A review of deep-learning-based medical image segmentation methods. X Liu, L Song, S Liu, Y Zhang, Sustainability. 13312242021</p>
<p>Self-supervised learning: Generative or contrastive. X Liu, F Zhang, Z Hou, L Mian, Z Wang, J Zhang, J Tang, IEEE transactions on knowledge and data engineering. 3512021</p>
<p>Graph self-supervised learning: A survey. Y Liu, M Jin, S Pan, C Zhou, Y Zheng, F Xia, P S Yu, IEEE Transactions on Knowledge and Data Engineering. 3562023</p>
<p>A survey of visual transformers. Y Liu, Y Zhang, Y Wang, F Hou, J Yuan, J Tian, Y Zhang, Z Shi, J Fan, Z He, IEEE Transactions on Neural Networks and Learning Systems. 2023</p>
<p>A survey of visual transformers. Y Liu, Y Zhang, Y Wang, F Hou, J Yuan, J Tian, Y Zhang, Z Shi, J Fan, Z He, IEEE Transactions on Neural Networks and Learning Systems. 2023</p>
<p>A survey on machine learning from few samples. J Lu, P Gong, J Ye, J Zhang, C Zhang, Pattern Recognition. 1391094802023</p>
<p>Loss odyssey in medical image segmentation. J Ma, J Chen, M Ng, R Huang, Y Li, C Li, X Yang, A L Martel, Medical Image Analysis. 711020352021</p>
<p>Adversarial machine learning in image classification: A survey toward the defender's perspective. G R Machado, E Silva, R R Goldschmidt, ACM Computing Surveys (CSUR). 5512021</p>
<p>Online continual learning in image classification: An empirical survey. Z Mai, R Li, J Jeong, D Quispe, H Kim, S Sanner, Neurocomputing. 4692022</p>
<p>Automatic speech recognition: a survey. M Malik, M K Malik, K Mehmood, I Makhdoom, Multimedia Tools and Applications. 802021</p>
<p>Class-incremental learning: survey and performance evaluation on image classification. M Masana, X Liu, B Twardowski, M Menta, A D Bagdanov, J Van De Weijer, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4552022</p>
<p>N Maslej, L Fattorini, E Brynjolfsson, J Etchemendy, K Ligett, T Lyons, J Manyika, H Ngo, J C Niebles, V Parli, Y Shoham, R Wald, J Clark, R Perrault, The ai index 2023 annual report. Stanford, CAApril 2023AI Index Steering Committee, Institute for Human-Centered AI, Stanford UniversityTechnical report</p>
<p>The probability distribution of the age of references in engineering papers. E Matricciani, IEEE Transactions on Professional Communication. 3411991</p>
<p>Segment anything model for medical image analysis: an experimental study. M A Mazurowski, H Dong, H Gu, J Yang, N Konz, Y Zhang, Medical Image Analysis. 891029182023</p>
<p>The matthew effect in science: The reward and communication systems of science are considered. R K Merton, Science. 15938101968</p>
<p>Recent advances in natural language processing via large pre-trained language models: A survey. B Min, H Ross, E Sulem, A P B Veyseh, T H Nguyen, O Sainz, E Agirre, I Heintz, D Roth, 2021ACM Computing Surveys</p>
<p>Image segmentation using deep learning: A survey. S Minaee, Y Boykov, F Porikli, A Plaza, N Kehtarnavaz, D Terzopoulos, 202144</p>
<p>Statistical relationships between downloads and citations at the level of individual documents within a single journal. H F Moed, Journal of the American Society for Information Science and Technology. 56102005</p>
<p>Measuring contextual citation impact of scientific journals. H F Moed, Journal of informetrics. 432010</p>
<p>A survey of named entity recognition and classification. D Nadeau, S Sekine, 2007Lingvisticae Investigationes30</p>
<p>Named entity recognition and relation extraction: State-of-the-art. Z Nasar, S W Jaffry, M K Malik, ACM Computing Surveys (CSUR). 5412021</p>
<p>Chatgpt: optimizing language models for dialogue. 25 December 2023OpenAI</p>
<p>L Papa, P Russo, I Amerini, L Zhou, arXiv:2309.02031A survey on efficient vision transformers: algorithms, techniques, and performance benchmarking. 2023arXiv preprint</p>
<p>Paper Digest, Paper digest ai-powered research platform. 25 December 2023</p>
<p>Synthesizing information systems knowledge: A typology of literature reviews. G Paré, M.-C Trudel, M Jaana, S Kitsiou, Information &amp; Management. 5222015</p>
<p>. Pdfminer, Pdfminer, Nov-202314</p>
<p>Comparison of two article-level, field-independent citation metrics: Fieldweighted citation impact (fwci) and relative citation ratio (rcr). A Purkayastha, E Palmaro, H J Falk-Krzesinski, J Baas, Journal of Informetrics. 1322019</p>
<p>3d object detection for autonomous driving: A survey. R Qian, X Lai, X Li, Pattern Recognition. 1301087962022</p>
<p>A survey on arabic named entity recognition: Past, recent advances, and future trends. X Qu, Y Gu, Q Xia, Z Li, Z Wang, B Huai, arXiv:2302.035122023arXiv preprint</p>
<p>Medical image segmentation using deep semantic-based methods: A review of techniques, applications and emerging trends. I Qureshi, J Yan, Q Abbas, K Shaheed, A B Riaz, A Wahid, M W J Khan, P Szczuko, Information Fusion. 902023</p>
<p>Deep convolutional neural networks for image classification: A comprehensive review. W Rawat, Z Wang, Neural computation. 2992017</p>
<p>A survey on semi-, self-and unsupervised learning for image classification. L Schmarje, M Santarossa, S.-M Schröder, R Koch, IEEE Access. 92021</p>
<p>Seamless -ai literature review tool for scientific research. Seamlees, 2023. 25 December 2023</p>
<p>Automatic text summarization methods: A comprehensive review. G Sharma, D Sharma, SN Computer Science. 41332022</p>
<p>A survey of methods, datasets and evaluation metrics for visual question answering. H Sharma, Vision ComputingA S , Vision ComputingImage. 1161043272021</p>
<p>A survey on label-efficient deep image segmentation: Bridging the gap between weak supervision and dense prediction. W Shen, Z Peng, X Wang, H Wang, J Cen, D Jiang, L Xie, X Yang, Q Tian, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023</p>
<p>The utilization of paper-level classification system on the evaluation of journal impact. Z Shen, S Tong, F Chen, L Yang, 20202006arXiv e-prints</p>
<p>U-net and its variants for medical image segmentation: A review of theory and applications. N Siddique, S Paheding, C P Elkin, V Devabhaktuni, Ieee Access. 92021</p>
<p>Co-citation in the scientific literature: A new measure of the relationship between two documents. H Small, Journal of the American Society for information Science. 2441973</p>
<p>Typologies, taxonomies, and the benefits of policy classification. K B Smith, Policy studies journal. 3032002</p>
<p>Ages of cited references and growth of scientific knowledge: an explication of the gamma distribution in business and management disciplines. A G Stacey, Scientometrics. 12612021</p>
<p>Citescore: Advances, evolution, applications, and limitations. J A Teixeira Da Silva, Publishing Research Quarterly. 3632020</p>
<p>Novel utilization of a paperlevel classification system for the evaluation of journal impact: An update of the cas journal ranking. S Tong, F Chen, L Yang, Z Shen, Quantitative Science Studies. 2023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023arXiv preprint</p>
<p>The altmetric score: a new measure for article-level dissemination and impact. N S Trueger, B Thoma, C H Hsu, D Sullivan, L Peters, M Lin, Annals of emergency medicine. 6652015</p>
<p>Population modeling of tumor growth curves and the reduced gompertz model improve prediction of the age of experimental tumors. C Vaghi, A Rodallec, R Fanciullino, J Ciccolini, J P Mochel, M Mastri, C Poignard, J M Ebos, S Benzekry, PLoS computational biology. 162e10071782020</p>
<p>Review of large vision models and visual prompt engineering. J Wang, Z Liu, L Zhao, Z Wu, C Ma, S Yu, H Dai, Q Yang, Y Liu, S Zhang, Meta-Radiology. 1000472023</p>
<p>L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023arXiv preprint</p>
<p>A survey on curriculum learning. X Wang, Y Chen, W Zhu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4492021</p>
<p>Selfsupervised learning in remote sensing: A review. Y Wang, C Albrecht, N A A Braham, L Mou, X Zhu, IEEE Geoscience and Remote Sensing Magazine. 2022</p>
<p>A survey on sentiment analysis methods, applications, and challenges. M Wankhade, A C S Rao, C Kulkarni, Artificial Intelligence Review. 5572022</p>
<p>Recent advances in swedish and spanish medical entity recognition in clinical texts using deep neural approaches. R Weegar, A Pérez, A Casillas, M Oronoz, BMC medical informatics and decision making. 1972019</p>
<p>Review of automatic text summarization techniques &amp; methods. A P Widyassari, S Rustad, G F Shidik, E Noersasongko, A Syukur, A Affandy, Journal of King Saud University-Computer and Information Sciences. 3442022</p>
<p>Self-supervised learning on graphs: Contrastive, generative, or predictive. L Wu, H Lin, C Tan, Z Gao, S Z Li, IEEE Transactions on Knowledge and Data Engineering. 2021</p>
<p>Generative adversarial networks in medical image segmentation: A review. S Xun, D Li, H Zhu, M Chen, J Wang, J Li, M Chen, B Wu, H Zhang, X Chai, Computers in biology and medicine. 1401050632022</p>
<p>Sentiment analysis using deep learning architectures: a review. A Yadav, D K Vishwakarma, Artificial Intelligence Review. 5362020</p>
<p>A survey on long-tailed visual recognition. L Yang, H Jiang, Q Song, J Guo, International Journal of Computer Vision. 13072022</p>
<p>Diffusion models: A comprehensive survey of methods and applications. L Yang, Z Zhang, Y Song, S Hong, R Xu, Y Zhao, W Zhang, B Cui, M.-H Yang, ACM Computing Surveys. 5642023</p>
<p>Self-supervised learning for recommender systems: A survey. J Yu, H Yin, X Xia, T Chen, J Li, Z Huang, IEEE Transactions on Knowledge and Data Engineering. 2023</p>
<p>A review of recurrent neural networks: Lstm cells and network architectures. Y Yu, X Si, C Hu, J Zhang, Neural computation. 3172019</p>
<p>A survey of sentiment analysis in social media. L Yue, W Chen, X Li, W Zuo, M Yin, Knowledge and Information Systems. 602019</p>
<p>A normalized levenshtein distance metric. L Yujian, L Bo, IEEE transactions on pattern analysis and machine intelligence. 200729</p>
<p>A survey of modern deep learning based object detection models. S S A Zaidi, M S Ansari, A Aslam, N Kanwal, M Asghar, B Lee, Digital Signal Processing. 1261035142022</p>
<p>A survey on audio diffusion models: Text to speech synthesis and enhancement in generative ai. C Zhang, C Zhang, S Zheng, M Zhang, M Qamar, S.-H Bae, I S Kweon, arXiv:2303.133362023arXiv preprint</p>
<p>Recent progresses on object detection: a brief review. H Zhang, X Hong, Multimedia Tools and Applications. 782019</p>
<p>A survey on graph diffusion models: Generative ai in science for molecule, protein and material. M Zhang, M Qamar, T Kang, Y Jung, C Zhang, S.-H Bae, C Zhang, arXiv:2304.015652023arXiv preprint</p>
<p>Utilizing citation network structure to predict paper citation counts: A deep learning approach. Q Zhao, X Feng, Journal of Informetrics. 1611012352022</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Object detection with deep learning: A review. Z.-Q Zhao, P Zheng, S.-T Xu, X Wu, IEEE transactions on neural networks and learning systems. 201930</p>
<p>A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. C Zhou, Q Li, C Li, J Yu, Y Liu, G Wang, K Zhang, C Ji, Q Yan, L He, arXiv:2302.094192023arXiv preprint</p>
<p>Artificial intelligence-generated scientific literaturea critical appraisal. J Zybaczynska, M Norris, S Modi, J Brennan, P Jhaveri, T J Craig, T Al-Shaikhly, The Journal of Allergy and Clinical Immunology. </p>            </div>
        </div>

    </div>
</body>
</html>
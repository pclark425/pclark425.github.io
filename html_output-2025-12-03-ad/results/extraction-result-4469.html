<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4469 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4469</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4469</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-282203239</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.15614v1.pdf" target="_blank">HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination</a></p>
                <p><strong>Paper Abstract:</strong> As language models are increasingly used in scientific workflows, evaluating their ability to propose sets of explanations-not just a single correct answer-becomes critical. Many scientific problems are underdetermined: multiple, mechanistically distinct hypotheses are consistent with the same observations. We introduce HypoSpace, a diagnostic suite that treats LLMs as samplers of finite hypothesis sets and measures three complementary indicators: Validity (precision of proposals consistent with observations), Uniqueness (non-redundancy among proposals), and Recovery (coverage of the enumerated admissible set). We instantiate HypoSpace in three structured domains with deterministic validators and exactly enumerated hypothesis spaces: (i) causal graphs from perturbations, (ii) gravity-constrained 3D voxel reconstruction from top-down projections, and (iii) Boolean genetic interactions. Across instruction-tuned and reasoning-focused models, Validity often remains high while Uniqueness and Recovery degrade as the admissible space grows, revealing mode collapse that is invisible to correctness-only metrics. HypoSpace offers a controlled probe-rather than a leaderboard-for methods that explicitly explore and cover admissible explanation spaces. Code is available at: https://github.com/CTT-Pavilion/_HypoSpace.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4469.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4469.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HypoSpace</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HypoSpace evaluation suite</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diagnostic framework that treats LLMs as samplers of finite hypothesis sets under under-determination and measures selection fidelity, non-redundancy, and coverage via deterministic validators and enumerated admissible sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>HypoSpace (set-valued evaluation under under-determination)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>HypoSpace operationalizes set-valued hypothesis generation: for each instance the ground-truth admissible hypothesis set H_O is exactly enumerated, LLMs are sampled repeatedly to produce a finite proposal set P, and proposals are checked with deterministic validators and task-specific canonicalizers. Behavior is summarized along three complementary indicators (Validity/VR, Novelty/Uniqueness/NR, Recovery/RR). The suite includes three task domains (causal DAGs from perturbations; gravity-constrained 3D voxel reconstruction from top-down projections; Boolean genetic interactions) with controllable difficulty knobs that scale |H_O|. Sampling is order-respecting so novelty is computed relative to earlier samples A_h.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Validity (empirical consistency with observations), Uniqueness/Novelty (non-redundancy among proposals), Recovery/Coverage (fraction of enumerated admissible set discovered); additionally analyzed via information-gain/entropy and failure-mode decomposition (duplicates, parse/constraint failures).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, GPT-5, Gemini-2.5-Pro, Claude-Opus-4, DeepSeek-R1, LLaMA-3.3-70B-Instruct, Grok-4 (evaluated set)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multiple (causal inference, 3D spatial reconstruction / physics, genetic interactions / computational biology)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>set-valued mechanistic hypotheses: causal graphs, 3D mechanistic reconstructions (voxel stacks), Boolean mechanistic programs/expressions</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across the three diagnostic domains HypoSpace reports VR/NR/RR per run. Key empirical patterns: reasoning models often retain high VR while NR and RR degrade predictably as |H_O| grows (evidence of mode collapse). Example numbers from Table 1: in causal inference (hard, nodes=6) GPT-5: VR ~100.00% ±0.00%, NR ~99.20% ±1.40%, RR ~99.20% ±1.40%; non-reasoning LLaMA-3.3-70B-Instruct: VR ~10.40% ±26.00%, NR ~38.00% ±22.40%, RR ~2.30% ±2.70%. In 3D voxel (tp=3, |H_O|=27) GPT-5: VR ~100.00% ±0.00%, NR ~98.80% ±2.20%, RR ~98.80% ±2.20%; LLaMA: VR ~10.50% ±13.10%, NR ~66.70% ±14.30%, RR ~6.00% ±5.60%. In Boolean genetic interactions (full/hard) GPT-5: VR ~65.10% ±12.50%, NR ~49.90% ±14.00%, RR ~48.00% ±14.40%; other models show larger NR/RR drops. Overall, VR often remains high for top reasoning models, while NR and RR show systematic decline with increasing admissible set size.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: uses deterministic validators (forward simulation, projection+gravity, functional agreement) and task-specific canonicalizers to deduplicate; no human raters used for primary scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validation relies on exact enumeration of H_O and deterministic validators for each domain; the authors require soundness, completeness, and controllability of the enumerated ground truth. Domain validators: forward simulation for causal DAGs; projection plus gravity (columnwise prefix) constraints for 3D; exact functional agreement on observed input-output pairs for Boolean programs. Distinctness is enforced via labeled-edge equality, voxelwise tensor equality, or a mechanistic canonicalizer.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Designed as a controlled probe rather than a real-world discovery benchmark: restricted, enumerated toy domains; canonicalizer only collapses local algebraic symmetries (may under/over-collapse semantics); sampling budget choices affect coverage; observed mode collapse means models explore only subsets of H_O even when VR is high; does not measure real-world scientific plausibility or human interpretive quality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>HypoSpace suite: three diagnostic datasets/tasks with enumerated admissible hypothesis sets per instance (Causal inference from single-node perturbations; 3D voxel reconstruction under gravity from top-down projections; Boolean genetic interaction expression space H(F,d) with specified operator sets and depth bounds). Each instance includes enumerated H_O and deterministic validators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4469.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4469.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Validity Rate (VR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Precision-like metric measuring the fraction of proposed hypotheses that are consistent with the observed data using deterministic validators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Validity (VR)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>VR(P) = (1/N) * |{ h in P : val_O(h) = 1 }|. For each proposed hypothesis h the task-specific validator val_O(h) returns 1 if h reproduces all observed effects; VR is the share of proposals that pass the validator, i.e., are appropriate/empirically adequate relative to observations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Empirical adequacy / appropriateness: exact consistency with the observation set O as determined by deterministic domain validators.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, GPT-5, Gemini-2.5-Pro, Claude-Opus-4, DeepSeek-R1, LLaMA-3.3-70B-Instruct, Grok-4 (reported across these models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>causal inference, 3D reconstruction, genetic interactions</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>causal graphs, voxel reconstructions, Boolean expressions (mechanistic models)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>VR often remains high for reasoning models across tasks (many near 100% on easier settings). Examples: causal inference (nodes=4..6) GPT-5 VR ~100% to 99.8%; 3D voxel (tp=1..3) GPT-5 VR ~100% to 100%; Boolean genetic interactions (full) GPT-5 VR ~65.1% ±12.5% indicating domain difficulty increases VR falloff. Non-reasoning models show substantially lower VR in harder settings (e.g., LLaMA VR as low as ~10-30% on some tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated deterministic validation (no human scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Domain-specific forward simulators/checkers: DAG forward model F_G for perturbations; columnwise prefix+projection check for 3D voxels; functional agreement on observed input-output pairs for Boolean programs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>VR conflates being consistent with observations with scientific plausibility; high VR alone does not indicate coverage of alternative hypotheses (mode collapse), and deterministic validators are only possible in structured toy domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4469.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4469.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Novelty / Uniqueness Rate (NR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Order-respecting metric that measures the fraction of proposals that are non-redundant (distinct) within a sampling run.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Novelty / Uniqueness (NR)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>NR(P) = (1/N) * |{ h in P : nov(h; A_h) = 1 }|, where nov(h; A_h) indicates whether h is distinct relative to the set of earlier samples A_h (order-respecting). Distinctness criteria are task-specific (labeled-edge equality for DAGs, voxelwise tensor equality for 3D, canonicalizer-based equivalence for Boolean expressions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Originality / non-redundancy among generated hypotheses; measures internal diversity of proposals within a run.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, GPT-5, Gemini-2.5-Pro, Claude-Opus-4, DeepSeek-R1, LLaMA-3.3-70B-Instruct, Grok-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>causal inference, 3D reconstruction, genetic interactions</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>causal graphs, voxel reconstructions, Boolean expressions</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>NR decreases as |H_O| increases for most models. Reasoning models maintain higher NR than non-reasoning baselines at medium-hard difficulties. Examples: causal (nodes=6) GPT-5 NR ~99.2% ±1.4% vs LLaMA NR ~38.0% ±22.4%; 3D (tp=3) GPT-5 NR ~98.8% ±2.2% vs LLaMA NR ~66.7% ±14.3%; Boolean (full) GPT-5 NR ~49.9% ±14.0% indicating large drop in novelty for large program spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated via exact equality or canonicalizer-based equivalence (no human judgments).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Task-specific distinctness checks: labeled-edge equality, voxelwise tensor equality, mechanistic canonicalizer (collapsing commutativity, idempotence, associativity) for Boolean expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Distinctness depends on canonicalization rules; limited canonicalizers may under-collapse semantically equivalent but syntactically different hypotheses or over-collapse legitimate distinct mechanisms. Order-respecting novelty metric depends on sampling order; duplicates may dominate failure modes (mode collapse).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4469.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4469.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recovery Rate (RR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Coverage metric measuring the fraction of the enumerated admissible hypothesis set H_O that the LLM discovered (valid and novel proposals).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Recovery / Recovery Rate (RR)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>RR(P) = (1/|H_O|) * |{ h in P : val_O(h)=1 AND nov(h; A_h)=1 }|. It integrates validity and non-redundancy to quantify coverage of the enumerated admissible set H_O by the sampled proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Fluency / coverage: the degree to which the model maps/recovers the full set of admissible hypotheses consistent with observations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, GPT-5, Gemini-2.5-Pro, Claude-Opus-4, DeepSeek-R1, LLaMA-3.3-70B-Instruct, Grok-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>causal inference, 3D reconstruction, genetic interactions</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>causal graphs, voxel reconstructions, Boolean expressions</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>RR systematically degrades as |H_O| increases, revealing mode collapse even when VR remains high. Examples: causal (nodes=6) GPT-5 RR ~99.2% ±1.4%; LLaMA RR ~2.3% ±2.7%. 3D (tp=3) GPT-5 RR ~98.8% ±2.2%; LLaMA RR ~6.0% ±5.6%. Boolean (full) GPT-5 RR ~48.0% ±14.4% while many models drop below ~25% RR in hardest settings. Recovery curves (RR@k) and coverage fractions show diminishing returns as sample count grows.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (enumeration + deterministic validation + canonicalization).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Exact enumeration of H_O ensures RR is measured against complete ground truth; validators and canonicalizers ensure soundness and completeness (as claimed) per instance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires enumerability of H_O (not possible for most real-world problems); sensitive to canonicalizer design and sampling budget; may understate meaningful conceptual diversity if canonicalizer collapses distinct mechanistic explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4469.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4469.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deterministic Validators & Enumerated H_O</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deterministic domain validators with exact enumeration of admissible hypothesis sets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methodological approach that uses exactly enumerated ground-truth hypothesis sets and deterministic forward-checking validators to remove subjectivity from evaluation and enable precise coverage measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Deterministic validation against enumerated admissible sets</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each instance the admissible hypothesis set H_O is explicitly enumerated (soundness, completeness, controllability required). Each LLM proposal is checked with a deterministic validator: forward model simulation for causal DAGs, projection+gravity constraints for 3D voxels, and functional agreement on observed pairs for Boolean programs. Distinctness is determined by task-specific equality or mechanistic canonicalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Empirical adequacy (exact agreement with observations), exact deduplication against enumerated set, and coverage measurement against complete H_O.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to all evaluated LLMs in experiments (GPT-4o, GPT-5, Gemini-2.5-Pro, Claude-Opus-4, DeepSeek-R1, LLaMA-3.3-70B-Instruct, Grok-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>causal inference, 3D understanding, genetic interaction modeling</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>causal graphs, voxel reconstructions, Boolean programs</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Enables exact VR/NR/RR computation and revealed that many models maintain high VR while NR and RR decline with |H_O|; deterministic validation allowed quantitative decomposition of failure modes (duplicates dominate).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Fully automated validation (no humans), eliminating LLM-as-judge circularity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Enumeration procedure is required to satisfy soundness and completeness; validators implement domain forward models or constraints. The paper asserts exact enumeration was used for their structured domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Only feasible in small, structured problem classes where H_O can be enumerated; does not generalize as-is to realistic, continuous, or combinatorially vast scientific hypothesis spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4469.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4469.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Canonicalizer (Distinctness)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mechanistic canonicalizer for distinctness/canonical equivalence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task-specific canonicalization procedure that collapses semantically equivalent hypotheses (e.g., commutativity, idempotence, associativity) so that duplicates are measured at the mechanistic level rather than syntactic surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Mechanistic canonicalizer for deduplication</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For Boolean expression hypotheses, a canonicalizer collapses local algebraic symmetries (commutativity, idempotence, associativity flattening) so two expressions are identical if Canon(h) = Canon(h'). For other domains, equivalence checks are labeled-edge equality (causal) and voxelwise tensor equality (3D). The canonicalizer is used when computing NR and RR to avoid counting syntactic variants as distinct.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Mechanistic equivalence (not purely syntactic): identifies duplicates that represent the same underlying mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>symbolic/programmatic domain (Boolean genetic interactions) primarily, with analogous equality rules in other tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Boolean expressions / mechanistic programs</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Canonicalization reduced apparent diversity by collapsing superficial syntactic variants, revealing that some models’ high syntactic diversity corresponded to low mechanistic diversity; in Boolean tasks canonicalizer reveals limited exploration of distinct mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (algorithmic canonicalization applied post-hoc).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Canonicalizer applies algebraic rewrite/flattening rules; equivalence determined by exact canonical forms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limited to implemented algebraic symmetries; may fail to collapse broader semantic equivalences or may over-collapse distinct mechanistic explanations. Choice of canonicalizer affects measured NR/RR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4469.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4469.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Divergent-Creativity (Bayesian measures)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Divergent creativity Bayesian measures (C_count, C_entropy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conceptual evaluation lens using posterior distribution over hypotheses: counts above a plausibility threshold and Shannon entropy measure how spread or rich the posterior is across admissible hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Divergent-Creativity Bayesian measures (C_count, C_entropy)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>C_count(O) = |{ h in H : P(h|O) > ε }| counts how many hypotheses remain plausibly supported; C_entropy(O) = -Σ_i P(h_i|O) log P(h_i|O) measures entropy of the posterior across H. These conceptual measures align with RR (coverage/fluency) and NR (originality) but require posterior estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Posterior spread: number of plausibly supported hypotheses and entropy of posterior distribution (distributional richness).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>conceptual across domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>general hypotheses/mechanistic explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Presented as an optional analysis lens; the paper notes diagnostics do not directly estimate these entropies but that the notions conceptually align with VR/NR/RR.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Conceptual/analytical; estimating P(h|O) would require modeling posterior distributions (not performed directly in primary metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not applied as a primary automated metric in core experiments; presented as a conceptual link to the diagnostic indicators.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires reliable posterior estimates over H, which are not directly available from standard LLM sampling; threshold choice ε and posterior estimation method affect counts/entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4469.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4469.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Information-Gain Sequential Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stepwise information gain (∆I_t) via empirical entropy of discovered patterns</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential, information-theoretic measure tracking entropy change as the model samples hypotheses, indicating whether sampling expands pattern diversity or converges to repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Information-gain sequential analysis (∆I_t)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Define H_t as entropy of the empirical pattern distribution M_t induced by the multiset of hypotheses up to step t; ∆I_t = H_t - H_{t-1}. Positive ∆I_t indicates exploration and expansion of pattern diversity; negative ∆I_t indicates convergence to repeated patterns. Cumulative entropy and per-query information gain curves are plotted to compare models' exploration efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Entropy growth rate and marginal information contribution per query: measures exploration efficiency and susceptibility to mode collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reported for multiple evaluated LLMs (reasoning vs non-reasoning groups)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>applied across the three HypoSpace tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>general mechanistic hypothesis patterns</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Frontier reasoning models exhibit steeper cumulative entropy growth and higher marginal information gain per query, indicating more efficient discovery of distinct valid hypotheses. Non-reasoning models plateau earlier (flatter ∆I_t curves), reflecting diminishing returns and mode collapse. These patterns correlate with measured NR/RR degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated, empirical entropy computed from sampled hypothesis patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical analysis correlating entropy growth curves with NR/RR and coverage; used to decompose exploration dynamics across models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Entropy is computed over observed patterns only (depends on sampling budget); does not yield absolute posterior entropy without a true generative distribution; sensitive to pattern definition and canonicalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4469.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4469.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sampling Protocol (LLM-as-sampler)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-sampler sampling protocol (order-respecting repeated sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Protocol treating the LLM as a stochastic sampler that is repeatedly queried to produce N hypotheses per instance, with order recorded to compute novelty and sequential metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Sampling protocol: LLM-as-sampler (N samples, order-respecting novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>From a fixed prompt and decoding setup, draw N independent samples P = {h1,...,hN} (the paper commonly uses N = |H_O|). Record proposals and their generation order. Novelty checks are order-respecting: nov(h; A_h) compares h to earlier proposals A_h. The protocol enables RR@k curves, information-gain per query, and exploration dynamics analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Exploration behavior under repeated independent draws: duplication rate, order of discovery, marginal information gain per sample, RR@k coverage growth.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to all evaluated LLMs (GPT-4o, GPT-5, Gemini-2.5-Pro, Claude-Opus-4, DeepSeek-R1, LLaMA-3.3-70B-Instruct, Grok-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>applies across tasks/domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>general hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Using N = |H_O| sampling budget allowed direct coverage analysis; empirical findings show sub-linear coverage growth for many models and early saturation (duplicates) evidencing mode collapse. Increasing sample counts yields diminishing returns in unique valid discoveries for many models.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated sampling and deterministic post-hoc checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Protocol validated by measurement reproducibility across repeated runs and by comparing RR/NR/V R curves across models and difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Choice of N and decoding settings impacts observed exploration; sampling independence assumption may be violated by model internal state or prompt/temperature dependencies; protocol scales poorly if |H_O| is large or unenumerable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>HypoBench: Towards systematic and principled benchmarking for hypothesis generation <em>(Rating: 2)</em></li>
                <li>Ideabench: Benchmarking large language models for research idea generation <em>(Rating: 2)</em></li>
                <li>Creativeval: Evaluating creativity of llm-based hardware code generation <em>(Rating: 2)</em></li>
                <li>LiveIdeaBench: Evaluating llms' divergent thinking for scientific idea generation with minimal context <em>(Rating: 2)</em></li>
                <li>Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction <em>(Rating: 2)</em></li>
                <li>Auto-Bench: An automated benchmark for scientific discovery in LLMs <em>(Rating: 2)</em></li>
                <li>Some empirical criteria for attributing creativity to a computer program <em>(Rating: 1)</em></li>
                <li>Creativity In Context: Update To The Social Psychology Of Creativity <em>(Rating: 1)</em></li>
                <li>The creative mind: Myths and mechanisms <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4469",
    "paper_id": "paper-282203239",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "HypoSpace",
            "name_full": "HypoSpace evaluation suite",
            "brief_description": "A diagnostic framework that treats LLMs as samplers of finite hypothesis sets under under-determination and measures selection fidelity, non-redundancy, and coverage via deterministic validators and enumerated admissible sets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "HypoSpace (set-valued evaluation under under-determination)",
            "evaluation_method_description": "HypoSpace operationalizes set-valued hypothesis generation: for each instance the ground-truth admissible hypothesis set H_O is exactly enumerated, LLMs are sampled repeatedly to produce a finite proposal set P, and proposals are checked with deterministic validators and task-specific canonicalizers. Behavior is summarized along three complementary indicators (Validity/VR, Novelty/Uniqueness/NR, Recovery/RR). The suite includes three task domains (causal DAGs from perturbations; gravity-constrained 3D voxel reconstruction from top-down projections; Boolean genetic interactions) with controllable difficulty knobs that scale |H_O|. Sampling is order-respecting so novelty is computed relative to earlier samples A_h.",
            "evaluation_criteria": "Validity (empirical consistency with observations), Uniqueness/Novelty (non-redundancy among proposals), Recovery/Coverage (fraction of enumerated admissible set discovered); additionally analyzed via information-gain/entropy and failure-mode decomposition (duplicates, parse/constraint failures).",
            "model_name": "GPT-4o, GPT-5, Gemini-2.5-Pro, Claude-Opus-4, DeepSeek-R1, LLaMA-3.3-70B-Instruct, Grok-4 (evaluated set)",
            "model_size": null,
            "scientific_domain": "multiple (causal inference, 3D spatial reconstruction / physics, genetic interactions / computational biology)",
            "theory_type": "set-valued mechanistic hypotheses: causal graphs, 3D mechanistic reconstructions (voxel stacks), Boolean mechanistic programs/expressions",
            "human_comparison": false,
            "evaluation_results": "Across the three diagnostic domains HypoSpace reports VR/NR/RR per run. Key empirical patterns: reasoning models often retain high VR while NR and RR degrade predictably as |H_O| grows (evidence of mode collapse). Example numbers from Table 1: in causal inference (hard, nodes=6) GPT-5: VR ~100.00% ±0.00%, NR ~99.20% ±1.40%, RR ~99.20% ±1.40%; non-reasoning LLaMA-3.3-70B-Instruct: VR ~10.40% ±26.00%, NR ~38.00% ±22.40%, RR ~2.30% ±2.70%. In 3D voxel (tp=3, |H_O|=27) GPT-5: VR ~100.00% ±0.00%, NR ~98.80% ±2.20%, RR ~98.80% ±2.20%; LLaMA: VR ~10.50% ±13.10%, NR ~66.70% ±14.30%, RR ~6.00% ±5.60%. In Boolean genetic interactions (full/hard) GPT-5: VR ~65.10% ±12.50%, NR ~49.90% ±14.00%, RR ~48.00% ±14.40%; other models show larger NR/RR drops. Overall, VR often remains high for top reasoning models, while NR and RR show systematic decline with increasing admissible set size.",
            "automated_vs_human_evaluation": "Automated: uses deterministic validators (forward simulation, projection+gravity, functional agreement) and task-specific canonicalizers to deduplicate; no human raters used for primary scoring.",
            "validation_method": "Validation relies on exact enumeration of H_O and deterministic validators for each domain; the authors require soundness, completeness, and controllability of the enumerated ground truth. Domain validators: forward simulation for causal DAGs; projection plus gravity (columnwise prefix) constraints for 3D; exact functional agreement on observed input-output pairs for Boolean programs. Distinctness is enforced via labeled-edge equality, voxelwise tensor equality, or a mechanistic canonicalizer.",
            "limitations_challenges": "Designed as a controlled probe rather than a real-world discovery benchmark: restricted, enumerated toy domains; canonicalizer only collapses local algebraic symmetries (may under/over-collapse semantics); sampling budget choices affect coverage; observed mode collapse means models explore only subsets of H_O even when VR is high; does not measure real-world scientific plausibility or human interpretive quality.",
            "benchmark_dataset": "HypoSpace suite: three diagnostic datasets/tasks with enumerated admissible hypothesis sets per instance (Causal inference from single-node perturbations; 3D voxel reconstruction under gravity from top-down projections; Boolean genetic interaction expression space H(F,d) with specified operator sets and depth bounds). Each instance includes enumerated H_O and deterministic validators.",
            "uuid": "e4469.0",
            "source_info": {
                "paper_title": "HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "VR",
            "name_full": "Validity Rate (VR)",
            "brief_description": "Precision-like metric measuring the fraction of proposed hypotheses that are consistent with the observed data using deterministic validators.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Validity (VR)",
            "evaluation_method_description": "VR(P) = (1/N) * |{ h in P : val_O(h) = 1 }|. For each proposed hypothesis h the task-specific validator val_O(h) returns 1 if h reproduces all observed effects; VR is the share of proposals that pass the validator, i.e., are appropriate/empirically adequate relative to observations.",
            "evaluation_criteria": "Empirical adequacy / appropriateness: exact consistency with the observation set O as determined by deterministic domain validators.",
            "model_name": "GPT-4o, GPT-5, Gemini-2.5-Pro, Claude-Opus-4, DeepSeek-R1, LLaMA-3.3-70B-Instruct, Grok-4 (reported across these models)",
            "model_size": null,
            "scientific_domain": "causal inference, 3D reconstruction, genetic interactions",
            "theory_type": "causal graphs, voxel reconstructions, Boolean expressions (mechanistic models)",
            "human_comparison": false,
            "evaluation_results": "VR often remains high for reasoning models across tasks (many near 100% on easier settings). Examples: causal inference (nodes=4..6) GPT-5 VR ~100% to 99.8%; 3D voxel (tp=1..3) GPT-5 VR ~100% to 100%; Boolean genetic interactions (full) GPT-5 VR ~65.1% ±12.5% indicating domain difficulty increases VR falloff. Non-reasoning models show substantially lower VR in harder settings (e.g., LLaMA VR as low as ~10-30% on some tasks).",
            "automated_vs_human_evaluation": "Automated deterministic validation (no human scoring).",
            "validation_method": "Domain-specific forward simulators/checkers: DAG forward model F_G for perturbations; columnwise prefix+projection check for 3D voxels; functional agreement on observed input-output pairs for Boolean programs.",
            "limitations_challenges": "VR conflates being consistent with observations with scientific plausibility; high VR alone does not indicate coverage of alternative hypotheses (mode collapse), and deterministic validators are only possible in structured toy domains.",
            "uuid": "e4469.1",
            "source_info": {
                "paper_title": "HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "NR",
            "name_full": "Novelty / Uniqueness Rate (NR)",
            "brief_description": "Order-respecting metric that measures the fraction of proposals that are non-redundant (distinct) within a sampling run.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Novelty / Uniqueness (NR)",
            "evaluation_method_description": "NR(P) = (1/N) * |{ h in P : nov(h; A_h) = 1 }|, where nov(h; A_h) indicates whether h is distinct relative to the set of earlier samples A_h (order-respecting). Distinctness criteria are task-specific (labeled-edge equality for DAGs, voxelwise tensor equality for 3D, canonicalizer-based equivalence for Boolean expressions).",
            "evaluation_criteria": "Originality / non-redundancy among generated hypotheses; measures internal diversity of proposals within a run.",
            "model_name": "GPT-4o, GPT-5, Gemini-2.5-Pro, Claude-Opus-4, DeepSeek-R1, LLaMA-3.3-70B-Instruct, Grok-4",
            "model_size": null,
            "scientific_domain": "causal inference, 3D reconstruction, genetic interactions",
            "theory_type": "causal graphs, voxel reconstructions, Boolean expressions",
            "human_comparison": false,
            "evaluation_results": "NR decreases as |H_O| increases for most models. Reasoning models maintain higher NR than non-reasoning baselines at medium-hard difficulties. Examples: causal (nodes=6) GPT-5 NR ~99.2% ±1.4% vs LLaMA NR ~38.0% ±22.4%; 3D (tp=3) GPT-5 NR ~98.8% ±2.2% vs LLaMA NR ~66.7% ±14.3%; Boolean (full) GPT-5 NR ~49.9% ±14.0% indicating large drop in novelty for large program spaces.",
            "automated_vs_human_evaluation": "Automated via exact equality or canonicalizer-based equivalence (no human judgments).",
            "validation_method": "Task-specific distinctness checks: labeled-edge equality, voxelwise tensor equality, mechanistic canonicalizer (collapsing commutativity, idempotence, associativity) for Boolean expressions.",
            "limitations_challenges": "Distinctness depends on canonicalization rules; limited canonicalizers may under-collapse semantically equivalent but syntactically different hypotheses or over-collapse legitimate distinct mechanisms. Order-respecting novelty metric depends on sampling order; duplicates may dominate failure modes (mode collapse).",
            "uuid": "e4469.2",
            "source_info": {
                "paper_title": "HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "RR",
            "name_full": "Recovery Rate (RR)",
            "brief_description": "Coverage metric measuring the fraction of the enumerated admissible hypothesis set H_O that the LLM discovered (valid and novel proposals).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Recovery / Recovery Rate (RR)",
            "evaluation_method_description": "RR(P) = (1/|H_O|) * |{ h in P : val_O(h)=1 AND nov(h; A_h)=1 }|. It integrates validity and non-redundancy to quantify coverage of the enumerated admissible set H_O by the sampled proposals.",
            "evaluation_criteria": "Fluency / coverage: the degree to which the model maps/recovers the full set of admissible hypotheses consistent with observations.",
            "model_name": "GPT-4o, GPT-5, Gemini-2.5-Pro, Claude-Opus-4, DeepSeek-R1, LLaMA-3.3-70B-Instruct, Grok-4",
            "model_size": null,
            "scientific_domain": "causal inference, 3D reconstruction, genetic interactions",
            "theory_type": "causal graphs, voxel reconstructions, Boolean expressions",
            "human_comparison": false,
            "evaluation_results": "RR systematically degrades as |H_O| increases, revealing mode collapse even when VR remains high. Examples: causal (nodes=6) GPT-5 RR ~99.2% ±1.4%; LLaMA RR ~2.3% ±2.7%. 3D (tp=3) GPT-5 RR ~98.8% ±2.2%; LLaMA RR ~6.0% ±5.6%. Boolean (full) GPT-5 RR ~48.0% ±14.4% while many models drop below ~25% RR in hardest settings. Recovery curves (RR@k) and coverage fractions show diminishing returns as sample count grows.",
            "automated_vs_human_evaluation": "Automated (enumeration + deterministic validation + canonicalization).",
            "validation_method": "Exact enumeration of H_O ensures RR is measured against complete ground truth; validators and canonicalizers ensure soundness and completeness (as claimed) per instance.",
            "limitations_challenges": "Requires enumerability of H_O (not possible for most real-world problems); sensitive to canonicalizer design and sampling budget; may understate meaningful conceptual diversity if canonicalizer collapses distinct mechanistic explanations.",
            "uuid": "e4469.3",
            "source_info": {
                "paper_title": "HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Deterministic Validators & Enumerated H_O",
            "name_full": "Deterministic domain validators with exact enumeration of admissible hypothesis sets",
            "brief_description": "Methodological approach that uses exactly enumerated ground-truth hypothesis sets and deterministic forward-checking validators to remove subjectivity from evaluation and enable precise coverage measurement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Deterministic validation against enumerated admissible sets",
            "evaluation_method_description": "For each instance the admissible hypothesis set H_O is explicitly enumerated (soundness, completeness, controllability required). Each LLM proposal is checked with a deterministic validator: forward model simulation for causal DAGs, projection+gravity constraints for 3D voxels, and functional agreement on observed pairs for Boolean programs. Distinctness is determined by task-specific equality or mechanistic canonicalization.",
            "evaluation_criteria": "Empirical adequacy (exact agreement with observations), exact deduplication against enumerated set, and coverage measurement against complete H_O.",
            "model_name": "Applied to all evaluated LLMs in experiments (GPT-4o, GPT-5, Gemini-2.5-Pro, Claude-Opus-4, DeepSeek-R1, LLaMA-3.3-70B-Instruct, Grok-4)",
            "model_size": null,
            "scientific_domain": "causal inference, 3D understanding, genetic interaction modeling",
            "theory_type": "causal graphs, voxel reconstructions, Boolean programs",
            "human_comparison": false,
            "evaluation_results": "Enables exact VR/NR/RR computation and revealed that many models maintain high VR while NR and RR decline with |H_O|; deterministic validation allowed quantitative decomposition of failure modes (duplicates dominate).",
            "automated_vs_human_evaluation": "Fully automated validation (no humans), eliminating LLM-as-judge circularity.",
            "validation_method": "Enumeration procedure is required to satisfy soundness and completeness; validators implement domain forward models or constraints. The paper asserts exact enumeration was used for their structured domains.",
            "limitations_challenges": "Only feasible in small, structured problem classes where H_O can be enumerated; does not generalize as-is to realistic, continuous, or combinatorially vast scientific hypothesis spaces.",
            "uuid": "e4469.4",
            "source_info": {
                "paper_title": "HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Canonicalizer (Distinctness)",
            "name_full": "Mechanistic canonicalizer for distinctness/canonical equivalence",
            "brief_description": "A task-specific canonicalization procedure that collapses semantically equivalent hypotheses (e.g., commutativity, idempotence, associativity) so that duplicates are measured at the mechanistic level rather than syntactic surface forms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Mechanistic canonicalizer for deduplication",
            "evaluation_method_description": "For Boolean expression hypotheses, a canonicalizer collapses local algebraic symmetries (commutativity, idempotence, associativity flattening) so two expressions are identical if Canon(h) = Canon(h'). For other domains, equivalence checks are labeled-edge equality (causal) and voxelwise tensor equality (3D). The canonicalizer is used when computing NR and RR to avoid counting syntactic variants as distinct.",
            "evaluation_criteria": "Mechanistic equivalence (not purely syntactic): identifies duplicates that represent the same underlying mechanism.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "symbolic/programmatic domain (Boolean genetic interactions) primarily, with analogous equality rules in other tasks",
            "theory_type": "Boolean expressions / mechanistic programs",
            "human_comparison": false,
            "evaluation_results": "Canonicalization reduced apparent diversity by collapsing superficial syntactic variants, revealing that some models’ high syntactic diversity corresponded to low mechanistic diversity; in Boolean tasks canonicalizer reveals limited exploration of distinct mechanisms.",
            "automated_vs_human_evaluation": "Automated (algorithmic canonicalization applied post-hoc).",
            "validation_method": "Canonicalizer applies algebraic rewrite/flattening rules; equivalence determined by exact canonical forms.",
            "limitations_challenges": "Limited to implemented algebraic symmetries; may fail to collapse broader semantic equivalences or may over-collapse distinct mechanistic explanations. Choice of canonicalizer affects measured NR/RR.",
            "uuid": "e4469.5",
            "source_info": {
                "paper_title": "HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Divergent-Creativity (Bayesian measures)",
            "name_full": "Divergent creativity Bayesian measures (C_count, C_entropy)",
            "brief_description": "Conceptual evaluation lens using posterior distribution over hypotheses: counts above a plausibility threshold and Shannon entropy measure how spread or rich the posterior is across admissible hypotheses.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_method_name": "Divergent-Creativity Bayesian measures (C_count, C_entropy)",
            "evaluation_method_description": "C_count(O) = |{ h in H : P(h|O) &gt; ε }| counts how many hypotheses remain plausibly supported; C_entropy(O) = -Σ_i P(h_i|O) log P(h_i|O) measures entropy of the posterior across H. These conceptual measures align with RR (coverage/fluency) and NR (originality) but require posterior estimates.",
            "evaluation_criteria": "Posterior spread: number of plausibly supported hypotheses and entropy of posterior distribution (distributional richness).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "conceptual across domains",
            "theory_type": "general hypotheses/mechanistic explanations",
            "human_comparison": null,
            "evaluation_results": "Presented as an optional analysis lens; the paper notes diagnostics do not directly estimate these entropies but that the notions conceptually align with VR/NR/RR.",
            "automated_vs_human_evaluation": "Conceptual/analytical; estimating P(h|O) would require modeling posterior distributions (not performed directly in primary metrics).",
            "validation_method": "Not applied as a primary automated metric in core experiments; presented as a conceptual link to the diagnostic indicators.",
            "limitations_challenges": "Requires reliable posterior estimates over H, which are not directly available from standard LLM sampling; threshold choice ε and posterior estimation method affect counts/entropy.",
            "uuid": "e4469.6",
            "source_info": {
                "paper_title": "HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Information-Gain Sequential Analysis",
            "name_full": "Stepwise information gain (∆I_t) via empirical entropy of discovered patterns",
            "brief_description": "A sequential, information-theoretic measure tracking entropy change as the model samples hypotheses, indicating whether sampling expands pattern diversity or converges to repeats.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Information-gain sequential analysis (∆I_t)",
            "evaluation_method_description": "Define H_t as entropy of the empirical pattern distribution M_t induced by the multiset of hypotheses up to step t; ∆I_t = H_t - H_{t-1}. Positive ∆I_t indicates exploration and expansion of pattern diversity; negative ∆I_t indicates convergence to repeated patterns. Cumulative entropy and per-query information gain curves are plotted to compare models' exploration efficiency.",
            "evaluation_criteria": "Entropy growth rate and marginal information contribution per query: measures exploration efficiency and susceptibility to mode collapse.",
            "model_name": "Reported for multiple evaluated LLMs (reasoning vs non-reasoning groups)",
            "model_size": null,
            "scientific_domain": "applied across the three HypoSpace tasks",
            "theory_type": "general mechanistic hypothesis patterns",
            "human_comparison": false,
            "evaluation_results": "Frontier reasoning models exhibit steeper cumulative entropy growth and higher marginal information gain per query, indicating more efficient discovery of distinct valid hypotheses. Non-reasoning models plateau earlier (flatter ∆I_t curves), reflecting diminishing returns and mode collapse. These patterns correlate with measured NR/RR degradation.",
            "automated_vs_human_evaluation": "Automated, empirical entropy computed from sampled hypothesis patterns.",
            "validation_method": "Empirical analysis correlating entropy growth curves with NR/RR and coverage; used to decompose exploration dynamics across models.",
            "limitations_challenges": "Entropy is computed over observed patterns only (depends on sampling budget); does not yield absolute posterior entropy without a true generative distribution; sensitive to pattern definition and canonicalization.",
            "uuid": "e4469.7",
            "source_info": {
                "paper_title": "HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Sampling Protocol (LLM-as-sampler)",
            "name_full": "LLM-as-sampler sampling protocol (order-respecting repeated sampling)",
            "brief_description": "Protocol treating the LLM as a stochastic sampler that is repeatedly queried to produce N hypotheses per instance, with order recorded to compute novelty and sequential metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Sampling protocol: LLM-as-sampler (N samples, order-respecting novelty)",
            "evaluation_method_description": "From a fixed prompt and decoding setup, draw N independent samples P = {h1,...,hN} (the paper commonly uses N = |H_O|). Record proposals and their generation order. Novelty checks are order-respecting: nov(h; A_h) compares h to earlier proposals A_h. The protocol enables RR@k curves, information-gain per query, and exploration dynamics analysis.",
            "evaluation_criteria": "Exploration behavior under repeated independent draws: duplication rate, order of discovery, marginal information gain per sample, RR@k coverage growth.",
            "model_name": "Applied to all evaluated LLMs (GPT-4o, GPT-5, Gemini-2.5-Pro, Claude-Opus-4, DeepSeek-R1, LLaMA-3.3-70B-Instruct, Grok-4)",
            "model_size": null,
            "scientific_domain": "applies across tasks/domains",
            "theory_type": "general hypothesis generation",
            "human_comparison": false,
            "evaluation_results": "Using N = |H_O| sampling budget allowed direct coverage analysis; empirical findings show sub-linear coverage growth for many models and early saturation (duplicates) evidencing mode collapse. Increasing sample counts yields diminishing returns in unique valid discoveries for many models.",
            "automated_vs_human_evaluation": "Automated sampling and deterministic post-hoc checks.",
            "validation_method": "Protocol validated by measurement reproducibility across repeated runs and by comparing RR/NR/V R curves across models and difficulty levels.",
            "limitations_challenges": "Choice of N and decoding settings impacts observed exploration; sampling independence assumption may be violated by model internal state or prompt/temperature dependencies; protocol scales poorly if |H_O| is large or unenumerable.",
            "uuid": "e4469.8",
            "source_info": {
                "paper_title": "HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination",
                "publication_date_yy_mm": "2025-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "HypoBench: Towards systematic and principled benchmarking for hypothesis generation",
            "rating": 2,
            "sanitized_title": "hypobench_towards_systematic_and_principled_benchmarking_for_hypothesis_generation"
        },
        {
            "paper_title": "Ideabench: Benchmarking large language models for research idea generation",
            "rating": 2,
            "sanitized_title": "ideabench_benchmarking_large_language_models_for_research_idea_generation"
        },
        {
            "paper_title": "Creativeval: Evaluating creativity of llm-based hardware code generation",
            "rating": 2,
            "sanitized_title": "creativeval_evaluating_creativity_of_llmbased_hardware_code_generation"
        },
        {
            "paper_title": "LiveIdeaBench: Evaluating llms' divergent thinking for scientific idea generation with minimal context",
            "rating": 2,
            "sanitized_title": "liveideabench_evaluating_llms_divergent_thinking_for_scientific_idea_generation_with_minimal_context"
        },
        {
            "paper_title": "Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction",
            "rating": 2,
            "sanitized_title": "roll_the_dice_look_before_you_leap_going_beyond_the_creative_limits_of_nexttoken_prediction"
        },
        {
            "paper_title": "Auto-Bench: An automated benchmark for scientific discovery in LLMs",
            "rating": 2,
            "sanitized_title": "autobench_an_automated_benchmark_for_scientific_discovery_in_llms"
        },
        {
            "paper_title": "Some empirical criteria for attributing creativity to a computer program",
            "rating": 1,
            "sanitized_title": "some_empirical_criteria_for_attributing_creativity_to_a_computer_program"
        },
        {
            "paper_title": "Creativity In Context: Update To The Social Psychology Of Creativity",
            "rating": 1,
            "sanitized_title": "creativity_in_context_update_to_the_social_psychology_of_creativity"
        },
        {
            "paper_title": "The creative mind: Myths and mechanisms",
            "rating": 1,
            "sanitized_title": "the_creative_mind_myths_and_mechanisms"
        }
    ],
    "cost": 0.0175115,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HYPOSPACE: EVALUATING LLM CREATIVITY AS SET-VALUED HYPOTHESIS GENERATORS UNDER UNDER-DETERMINATION
17 Oct 2025</p>
<p>Tingting Chen tingting.c@u.nus.edu 
National University of Singapore</p>
<p>Beibei Lin 
National University of Singapore</p>
<p>Zifeng Yuan 
National University of Singapore</p>
<p>Qiran Zou 
National University of Singapore</p>
<p>Hongyu He 
Yew-Soon Ong 
National University of Singapore</p>
<p>Nanyang Technological University</p>
<p>Anirudh Goyal 
Meta Superintelligence Labs</p>
<p>Dianbo Liu 
National University of Singapore</p>
<p>HYPOSPACE: EVALUATING LLM CREATIVITY AS SET-VALUED HYPOTHESIS GENERATORS UNDER UNDER-DETERMINATION
17 Oct 20259D987AA1A6C6A0B31005B2F90C3BCAD4arXiv:2510.15614v1[cs.CL]
As language models are increasingly used in scientific workflows, evaluating their ability to propose sets of explanations-not just a single correct answer-becomes critical.Many scientific problems are underdetermined: multiple, mechanistically distinct hypotheses are consistent with the same observations.We introduce HypoSpace, a diagnostic suite that treats LLMs as samplers of finite hypothesis sets and measures three complementary indicators: Validity (precision of proposals consistent with observations), Uniqueness (non-redundancy among proposals), and Recovery (coverage of the enumerated admissible set).We instantiate HypoSpace in three structured domains with deterministic validators and exactly enumerated hypothesis spaces: (i) causal graphs from perturbations, (ii) gravityconstrained 3D voxel reconstruction from top-down projections, and (iii) Boolean genetic interactions.Across instruction-tuned and reasoning-focused models, Validity often remains high while Uniqueness and Recovery degrade as the admissible space grows, revealing mode collapse that is invisible to correctness-only metrics.HypoSpace offers a controlled probe-rather than a leaderboard-for methods that explicitly explore and cover admissible explanation spaces.Code is available at: https://github.com/CTT-Pavilion/_HypoSpace.</p>
<p>INTRODUCTION</p>
<p>Many scientific inference problems are underdetermined (Van Fraassen, 1980;Stanford, 2010): the same observations admit multiple, mechanistically distinct explanations.A capable scientific reasoning system should therefore not stop at finding one correct answer, but should map the complete admissible set H O and surface multiple non-redundant hypotheses consistent with the data.As LLMs increasingly support scientific research, this capability becomes critical.Yet contemporary LLM evaluations largely reward one-shot correctness (Shojaee et al., 2025;Koblischke et al., 2025;Shojaee et al., 2024;Wang et al., 2024b;Coignion et al., 2024;Hendrycks et al., 2020), leaving open whether models can systematically explore sets of valid explanations under controlled underdetermination.</p>
<p>To address this gap, we introduce HypoSpace, a diagnostic suite that operationalizes classical creativity theory for LLM evaluation.Following established frameworks in divergent thinking research (Guilford, 1950;Torrance, 1966), which require creative outputs to be both novel and appropriate (Amabile, 1996), we define three complementary metrics for hypothesis generation: Validity enforces appropriateness by measuring consistency with observations; Uniqueness quantifies originality through non-redundancy among all proposals; and Recovery operationalizes fluency by measuring coverage of the enumerated admissible set H O .Unlike standard divergent-thinking tests, our framework provides deterministic validators and enumerated ground truth, eliminating rater subjectivity and enabling precise measurement.Our approach treats LLMs as samplers that generate finite sets of candidate hypotheses (Figure 1a).</p>
<p>For each problem instance, we enumerate the complete valid set H O , apply exact validity checks, and assess non-redundancy using task-specific canonicalizers that collapse semantically equivalent forms.By repeatedly sampling and measuring behavior along Validity (VR), Uniqueness (NR), and Recovery (RR), we decouple being correct from exploring comprehensively-a distinction obscured by traditional correctness-only metrics.</p>
<p>We instantiate this diagnostic suite across three structured domains that mirror scientific inference while enabling exact enumeration of valid hypothesis spaces: Causal graphs from perturbations, where models infer all DAGs consistent with single-node intervention observations; 3D voxel reconstruction under gravity, where models reconstruct spatial configurations from top-down projections while satisfying physical constraints; and Boolean genetic interactions, where models propose expressions relating phenotype observations to underlying Boolean programs.Each domain provides natural difficulty scaling through controllable parameters (node counts, grid dimensions, operator complexity) that systematically vary the size of the admissible set |H O |.This controlled enumeration enables three key capabilities: direct measurement of hypothesis space coverage, precise calibration of task difficulty, and systematic analysis of sampling behavior including mode collapse patterns.Our goal is diagnostic measurement rather than leaderboard optimization-we seek to understand and improve how models explore solution spaces under underdetermination.</p>
<p>Evaluation across recent instruction-tuned and reasoning-focused LLMs reveals a consistent and concerning pattern: while models often maintain high Validity rates when generating admissible hypotheses, they exhibit pronounced mode collapse as hypothesis spaces grow.Both Uniqueness and Recovery degrade predictably with increasing |H O | (Figure 1b), indicating that current models tend to circle a small subset of admissible explanations rather than systematically explore the complete space that observations allow.Crucially, because our valid sets are exactly enumerated, these coverage failures are measurable rather than anecdotal, and persist even when traditional accuracy metrics suggest strong performance.The primary contributions of this work are:</p>
<ol>
<li>Theoretical formulation: We frame the evaluation of LLMs' ability to infer multiple distinct hypotheses fitting the same observations as set-valued inference under underdetermination, introducing three diagnostic indicators that systematically separate correctness from exploration capabilities.To the best of our knowledge, this is the very first exploration in this direction.2. Controlled diagnostic suite: Three structured tasks with exact enumeration of valid hypothesis spaces, enabling non-LLM validity checking and objective measurement of coverage.</li>
</ol>
<p>Empirical findings:</p>
<p>A systematic study demonstrating that even frontier reasoning models exhibit pronounced mode collapse-high Validity coupled with degrading Uniqueness and Recovery as |H O | increases.Our information-gain analysis versus query count further shows diminishing returns when the number of inference increases.</p>
<p>Methodological contribution:</p>
<p>A reusable framework for analyzing hypothesis-generation capabilities, designed as a controlled probe for developing improved sampling strategies rather than a competitive benchmark.</p>
<p>We emphasize that HypoSpace does not claim to evaluate real-world scientific discovery.Rather, it abstracts core ingredients of set-valued inference-consistency checking and combinatorial hypothesis spaces-into controlled settings where ground truth is fully specified.This design choice prioritizes measurement precision and cross-model comparability.</p>
<p>RELATED WORK</p>
<p>Scientific-discovery benchmarks probe LLM reasoning in domains such as equation discovery and physics-inspired tasks, and in end-to-end AI-scientist loops (Shojaee et al., 2024;2025;Koblischke et al., 2025;Coignion et al., 2024;Chen et al., 2025), but typically optimize for single-answer correctness and assume a single ground truth per input.Creativity-aware evaluations, which is rooted in novelty/appropriateness and the fluency/originality/flexibility triad (Guilford, 1950;Torrance, 1966;Amabile, 1996;Boden, 2004), have begun to incorporate diversity (e.g., HypoBench, IdeaBench, CreativEval, LiveIdeaBench) (Liu et al., 2025;Guo et al., 2025b;DeLorenzo et al., 2024;Ruan et al., 2024), and propose open-ended/axiomatic frameworks for originality and distributional creativity (Nagarajan et al., 2025;Wang et al., 2024a).Unlike these, we evaluate set-valued inference with deterministic validators and exactly enumerated admissible sets, enabling Sampling protocol.We evaluate models as samplers over hypothesis sets.From a fixed prompt and decoding setup, we draw N independent samples P = { h1 , . . ., hN }, logging both the proposals and their order for novelty checks.In most experiments we set N = |H O | to facilitate coverage analyses and RR@k curves, but other sampling budgets are possible.</p>
<p>INDICATORS</p>
<p>We summarize behavior along three indicators that disentangle selection fidelity, non-redundancy, and coverage.Throughout, val O ( h) ∈ {0, 1} denotes the task-specific validity check and nov( h; A h) ∈ {0, 1} indicates whether h is distinct relative to the set of earlier samples A h ⊆ P (order-respecting).</p>
<p>Validity (VR; appropriateness).
VR(P ) = 1 N { h ∈ P | val O ( h) = 1 } .(1)
VR measures selection fidelity: the share of proposals that satisfy all observations.</p>
<p>Novelty/Uniqueness (NR; originality).
NR(P ) = 1 N { h ∈ P | nov( h; A h) = 1 } .(2)
NR quantifies non-redundancy within a sampling run.</p>
<p>Recovery (RR; fluency/coverage).
RR(P ) = 1 |H O | { h ∈ P | val O ( h) = 1 ∧ nov( h; A h) = 1 } .(3)
RR measures coverage of the enumerated valid set and integrates validity and non-redundancy.</p>
<p>Distinctness criteria.Novelty/uniqueness is task-specific: labeled-edge equality (causal DAGs), voxelwise tensor equality (3D reconstructions), and a mechanistic canonicalizer for local Boolean equivalences (genetic interaction).</p>
<p>DIVERGENT-CREATIVITY VIEW(OPTIONAL ANALYSIS LENS)</p>
<p>Beyond our operational metrics (VR, NR, RR), we can view the hypothesis generation process from the perspective of divergent creativity in a Bayesian lens.Given an observation set O, each candidate hypothesis h i ∈ H has a posterior plausibility P (h i | O).Creativity in this sense reflects how spread out or rich the posterior distribution is across multiple admissible hypotheses.A simple measure is the number of distinct hypotheses supported above a plausibility threshold ϵ:
C count (O) = {h i ∈ H : P (h i | O) &gt; ϵ} ,
which counts how many hypotheses remain comparably plausible given O.A complementary measure is the Shannon entropy of the posterior:
C entropy (O) = − |H| i=1 P (h i | O) log P (h i | O),
which quantifies how evenly probability mass is distributed across hypotheses.High entropy indicates situations where many competing explanations are viable (greater divergent creativity), whereas low entropy reflects convergence toward a single dominant explanation.These measures conceptually align with our diagnostic indicators: RR as coverage/fluency, NR as originality, and VR as appropriateness.</p>
<p>INFORMATION-GAIN VIEW (SEQUENTIAL ANALYSIS LENS)</p>
<p>Although our diagnostics do not directly estimate entropies, they admit an information-theoretic view.Treat generation as a sequential process that grows a repertoire of mechanistic patterns.Let H t be the multiset of hypotheses produced up to step t, M t = M (H t ) the induced set of patterns, and p t (m) the empirical frequency of pattern m ∈ M t .We define the stepwise information gain (entropy change) as
∆I t = H t − H t−1 , H t := − m∈Mt p t (m) log 2 p t (m).
Positive ∆I t indicates expanding pattern diversity (exploration), while negative values signal convergence toward repeated patterns.Empirically, sustained positive ∆I t aligns with higher novelty and improved Uniqueness/Recovery.</p>
<p>INTENDED USE AND SCOPE</p>
<p>Our suite is a diagnostic, not a leaderboard: it isolates underdetermination, supports controlled ablations (sampling budget, model class), and reports interpretable indicators and coverage curves rather than a single scalar score.It does not claim real-world scientific discovery; instead, it provides a calibrated probe for set-valued inference.</p>
<p>HYPOSPACE CONSTRUCTION</p>
<p>We instantiate three structured diagnostics (Figure 2) that mirror common patterns of scientific inference while allowing exact enumeration of H O : causal graphs from perturbations, gravityconstrained 3D voxel worlds from top-down projections, and Boolean genetic interactions from phenotype observations.</p>
<p>CAUSAL INFERENCE FROM PERTURBATIONS</p>
<p>Instance.As in Figure 2a, let V = {v 1 , . . ., v n } be labeled nodes and G ⋆ = (V, E ⋆ ) a latent DAG.We observe single-node interventions and effects,
O = {(s (k) , x (k) )} m k=1 , s (k) ∈ V, x (k) ∈ {0, 1} n , with the forward model F G : V → {0, 1} n given by [F G (v i )] j =    0, j = i 1, v j ∈ desc G (v i ) 0, otherwise. Generation. At each step, the model proposes a candidate DAG G ← F LLM (O, A G, T prompt )
, where A G is the history.</p>
<p>Validity and distinctness.</p>
<p>A proposal is valid iff it reproduces all observed effects:
val O ( G) = 1 ⇔ F G(s (k) ) = x (k) ∀k</p>
<p>3D UNDERSTANDING UNDER GRAVITY</p>
<p>Instance.As in Figure 2b, an observation is a binary top-down projection V ∈ {0, 1} M ×M where V i,j = 1 indicates at least one occupied voxel in column (i, j).Hypotheses are voxel stacks H = H(M, K) ⊆ {0, 1} K×M ×M with K discrete layers (layer 1 bottom).</p>
<p>Generation.At each step, the model proposes h ∈ H ← F LLM (V, A h, T prompt ; M, K).</p>
<p>Validity and distinctness.A reconstruction is valid iff it satisfies projection and gravity (columnwise prefix) constraints: ∀i, j :
K k=1 h(k) i,j = V i,j ∧ ∀i, j, ∀k &gt; 1 : h(k) i,j ≤ h(k−1) i,j
.</p>
<p>Distinctness is voxelwise equality under aligned coordinates.</p>
<p>Scoring and difficulty.We report VR/NR/RR as in Sec. 3. Difficulty is controlled by grid size M , height budget K, and projection density; the number of valid completions grows rapidly with these parameters.</p>
<p>DNA INTERACTION VIA BOOLEAN PROGRAMS</p>
<p>Instance.As in Figure 2c, inputs are maternal/paternal phenotypes x, y ∈ {0, 1} and output π ∈ {0, 1}.An instance specifies an operator set F (e.g., {¬, ∧, ∨}) and a depth bound d defining the hypothesis space H = H(F, d) of expression trees over {x, y} (optionally constants
). Each h ∈ H induces f h : {0, 1} 2 → {0, 1}. Observations are pairs O = {((x (i) , y (i) ), π (i) )} m i=1 . Generation.The model proposes h ∈ H ← F LLM (O, A h, T prompt ; F, d).
Validity and distinctness.Validity requires functional agreement on observed pairs:
val O ( h) = 1 ⇔ f h(x (i) , y (i) ) = π (i) ∀i.
Distinctness is defined by a mechanistic canonicalizer that collapses only local algebraic symmetries implemented in our code: commutativity, idempotence, and associativity flattening for repeated identical operators; two expressions are identical iff Canon(h) = Canon(h ′ ).</p>
<p>Scoring and difficulty.We report VR/NR/RR as in Section 3. Difficulty is controlled by the operator set F, depth d, and the number/coverage of observation pairs; these knobs govern |H O | and the combinatorics of valid programs.</p>
<p>EXPERIMENTS</p>
<p>We evaluate a mix of instruction-tuned and "thinking" LLMs on three diagnostic tasks with enumerated valid sets: Causal Inference, 3D Understanding, and Boolean Genetic Interaction.The model suite includes GPT-4o (Hurst et al., 2024), GPT-5 (OpenAI, 2025), Gemini-2.5-Pro(Comanici et al., 2025), Claude-Opus-4 (Anthropic, 2025), DeepSeek-R1 (Guo et al., 2025a), LLaMA-3.3-70B-Instruct (Touvron et al., 2023), andGrok-4 (xAI, 2025). 1 We group models into reasoning (a.k.a."thinking") and non-reasoning (instruction-tuned) categories.Reasoning models, by default, produce explicit intermediate rationales and are marketed as reasoning-optimized; non-reasoning models typically return short direct answers unless prompted otherwise.We score only the final structured hypothesis, not the rationale text.We report Validity (VR), Novelty/Uniqueness (NR), and Recovery (RR) as defined in Section 3.</p>
<p>EXPERIMENTAL DETAILS AND DIFFICULTY</p>
<p>Each task exposes natural knobs that scale the size of the admissible set Validation and distinctness.Outputs follow strict schemas and are checked by deterministic validators (forward simulation for causal DAGs; projection and gravity for 3D; functional agreement on observed pairs for DNA).Distinctness is assessed via labeled-edge equality (causal), tensor equality (3D), and a mechanistic canonicalizer that collapses local algebraic symmetries (DNA).</p>
<p>RESULTS BY TASK</p>
<p>Results.Table 1 summarizes Validity (VR), Uniqueness (NR), and Recovery (RR) across difficulty for the three diagnostics.In causal inference, simple/medium regimes are near-ceiling for several reasoning models; in the hard case (6 nodes) the top reasoning models sustain near 100% VR and high NR/RR, while other reasoning models show small but consistent NR/RR gaps and nonreasoning models lag in VR (hence RR).In 3D voxel reconstruction, most models attain high VR with 1-2 views, but at 3 views (|H O |=27) gaps widen: frontier reasoning models preserve NR/RR whereas others repeat proposals early, depressing RR.Boolean genetic interactions is most discriminative: as operator set/depth and observation coverage grow, NR and RR drop markedly for all The curve shape reflects each model's marginal contribution of new information with additional samples, revealing differences in exploration strategies and susceptibility to mode collapse.Models with flatter curves show diminishing returns in hypothesis diversity, consistent with the Recovery Rate degradation observed in Table 1.models even when VR remains moderate; the canonicalizer collapses superficial variants, revealing limited exploration of distinct mechanisms.</p>
<p>CROSS-TASK OBSERVATIONS</p>
<p>Across all three domains we see the consistent trend: as |H O | grows, Uniqueness and Recovery drop even while frontier reasoning models (GPT-5, Gemini-2.5-Pro,Claude-Opus-4, DeepSeek-R1, Grok-4) remain high on Validity.By task, Causal Inference is most tractable at our scales (several models near ceiling), 3D Voxel is intermediate with collapse emerging in the hardest multi-view settings, and Boolean Genetic Interactions is most discriminative-its large program space, even after canonicalization, yields the sharpest coverage deficits.Reasoning-capable models consistently beat non-reasoning baselines on NR/RR at medium-hard difficulties, indicating that explicit reasoning mitigates-but does not eliminate-mode collapse.</p>
<p>WHERE COVERAGE FAILS: EVIDENCE FOR MODE COLLAPSE</p>
<p>The systematic decline in RR across increasing task difficulty provides direct evidence for mode collapse in hypothesis generation, where models converge on limited subsets of the admissible space despite maintaining solution validity.This collapse manifests through three complementary mechanisms: models exhibit strong attraction to a small number of preferred hypotheses, leading to early saturation where additional sampling yields diminishing returns in unique valid discoveries; they demonstrate systematic blindness to structural symmetries, particularly evident in Boolean Genetic Interaction where syntactically diverse expressions collapse under canonicalization; and they show sensitivity to sampling budget constraints, with coverage growing sub-linearly even when the number of samples approaches the size of the valid set.</p>
<p>INFORMATION-THEORETIC ANALYSIS OF EXPLORATION DYNAMICS</p>
<p>Figure 3 provides an information-theoretic lens on hypothesis space exploration, revealing fundamental differences in how models navigate the enumerated admissible sets.The cumulative entropy curves (Figure 3a) demonstrate that frontier reasoning models achieve steeper entropy growth, indicating more efficient discovery of distinct valid hypotheses as sampling progresses.In contrast, non-reasoning models plateau earlier, reflecting convergence to smaller hypothesis subsets.The information gain analysis (Figure 3b) further illuminates these exploration dynamics: reasoning  corresponds to reduced Recovery Rates and reveals the extent to which models fail to map the full space of valid explanations, even when maintaining high Validity.</p>
<p>models maintain higher marginal information contributions per query, while non-reasoning models exhibit flatter curves characteristic of diminishing returns.This pattern directly correlates with the Recovery Rate degradation observed in Table 1, providing mechanistic evidence that mode collapse manifests as premature entropy saturation rather than uniform exploration inefficiency.</p>
<p>FAILURE MODE DECOMPOSITION AND COVERAGE ANALYSIS</p>
<p>Figure 4 decomposes the sources of limited hypothesis coverage through complementary analyses of failure types and exploration completeness.The error categorization (Figure 4a) reveals that mode collapse stems primarily from duplicate generation rather than validity failures: while parse errors and violations remain relatively low across models, exact and canonical duplicates constitute the dominant failure mode, particularly for non-reasoning models.This pattern indicates that models can generate structurally valid hypotheses but struggle to diversify beyond preferred templates.The exploration analysis (Figure 4b) quantifies this limitation directly, showing that even high-performing models like GPT-5 and Gemini-2.5-Proexplore only 60-70% of enumerated admissible sets, with weaker models covering substantially smaller fractions.The systematic underexploration persists despite models maintaining high Validity rates, confirming that current LLMs exhibit fundamental constraints in mapping complete hypothesis spaces under underdetermination.</p>
<p>DISCUSSION</p>
<p>As Table 1 shows, indicators worsen as |H O | grows: Causal saturates quickly for top models, while 3D-and especially Boolean-remain discriminative at higher difficulty.Reasoning models consistently beat non-reasoning baselines on NR/RR even when VR is similar on easy cases.The three tasks probe complementary skills (structural, spatial, symbolic) and expose distinct collapse modes (edge-pattern fixation; minimal-height towers; algebraic template reuse).</p>
<p>CONCLUSION</p>
<p>We introduced a diagnostic suite for set-valued evaluation of LLMs under underdetermination, with exact validators and enumerated valid sets.Across three structured tasks, frontier models show high Validity but limited Uniqueness/Recovery as the admissible set grows.Because VR/NR/RR are measured against enumerated ground truth (Table 1), the observed collapse is quantitative, not anecdotal.We view these diagnostics as a controlled probe set-not a leaderboard-for developing methods that map admissible hypothesis spaces (e.g., entropy-seeking decoding, explicit memory across samples, or learned proposal distributions).From the table, llama-3 is the most cost-efficient by a wide margin ($0.05/M tokens), followed by claude-opus-4 and grok-4 at $1.00/M and deepseek-r1 at $1.83/M; gpt-4o ($3.69/M), gpt-5 ($8.10/M), and gemini-2 ($8.69/M) are the most expensive per token, reflecting higher unit prices and longer average reasoning outputs.Despite low unit prices, deepseek-r1 and grok-4 accrued higher total spend due to large per-run budgets (∼2.5M tokens/run), whereas llama-3 and claudeopus-4 remained inexpensive both per token and in total.</p>
<p>APPENDIX A ANALYSIS OF COST AND TOKENS ACROSS LLMS</p>
<p>B EXTENDED RELATED WORK</p>
<p>Benchmarks for Scientific Discovery with LLMs.Recent work explores LLMs for scientific reasoning and discovery via domain benchmarks and end-to-end "AI scientist" setups.Shojaee et al. introduce equation-discovery tasks across physics and chemistry (Shojaee et al., 2024;2025), and Koblischke et al. propose Gravity for physics-inspired reasoning (Koblischke et al., 2025).Coignion et al. assess LLM code generation (Coignion et al., 2024), while Chen et al. develop Auto-Bench to evaluate full AI-scientist loops including query generation and experiment planning (Chen et al., 2025).These efforts provide valuable snapshots of scientific reasoning skills but predominantly emphasize single-answer correctness or task completion and typically assume one ground-truth solution per input.</p>
<p>Creativity-Aware Benchmarks.Creativity has long been framed in psychology as producing outputs that are both novel and appropriate, with divergent thinking operationalized via fluency, originality, and flexibility (Guilford, 1950;Torrance, 1966;Amabile, 1996), and further conceptualized by Boden's taxonomy of combinational, exploratory, and transformational creativity (Boden, 2004).In computational creativity, formal criteria for evaluating novelty/value were articulated by Ritchie (Ritchie, 2007), and exploration-centric search (e.g., novelty search) was proposed to overcome objective-myopia (Lehman &amp; Stanley, 2011).Building on this lineage, recent LLM benchmarks have begun to incorporate diversity/novelty.HypoBench (Liu et al., 2025) and Ide-aBench (Guo et al., 2025b) (Wang et al., 2024a).Beyond core NLP settings, Bhat et al. propose a domain-specific evaluation framework for marketing creativity with LLMs, operationalizing novelty and appropriateness using task-grounded criteria (Bhat et al., 2025).</p>
<p>Figure 1 :
1
Figure 1: HypoSpace evaluation framework and model performance comparison.(a) Our diagnostic approach treats LLMs as samplers over hypothesis spaces.Given observations O, models generate N hypotheses that are validated for consistency with data and deduplicated for uniqueness.We measure three complementary indicators: Validity (VR: precision of valid hypotheses), Uniqueness (NR: non-redundancy among proposals), and Recovery (RR: coverage of the enumerated admissible set H O ).These operationalize creativity theory's appropriateness, originality, and fluency dimensions, respectively.(b) Recovery Rate comparison across models on task of Boolean genetic interactions, showing systematic degradation as hypothesis spaces grow from simple to hard settings, with reasoning models generally outperforming non-reasoning models.</p>
<p>Figure 2 :
2
Figure 2: HypoSpace task instantiations.(a) Causal inference from perturbations: Given intervention observations (e.g., perturbing node C affects nodes B), models propose causal DAGs consistent with the data.Multiple valid graph structures can explain the same perturbation patterns.(b) 3D voxel reconstruction under gravity: From top-down 2D projections, models reconstruct 3D voxel structures satisfying projection constraints and gravity (stacking rules).The same projection admits multiple valid 3D configurations.(c) Boolean genetic interactions: From phenotype observations of parental combinations, models propose Boolean expressions relating inputs to outputs.Expressions that collapse to the same form under our restricted canonicalizer count as one hypothesis.Each domain exemplifies scientific underdetermination where multiple mechanistically distinct hypotheses explain identical observations, enabling measurement of Validity, Uniqueness, and Recovery across enumerated hypothesis spaces.</p>
<p>.</p>
<p>Distinctness is on labeled nodes: two DAGs are identical if their labeled edge sets match; equivalently Canon(G) = Canon(G ′ ).Scoring and difficulty.Given N samples P = ( Gi ) N i=1 , we report VR/NR/RR as in Sec. 3. Difficulty is controlled by n and m (more nodes/interventions typically enlarge |H O |).</p>
<p>Figure 3 :
3
Figure 3: Information-theoretic analysis of hypothesis space exploration in HypoSpace.(a)Cumulative entropy as a function of sequential queries, measuring how uncertainty reduction varies across models as they sample from the hypothesis space.Steeper curves indicate more efficient exploration of distinct valid hypotheses.(b) Information gain per query (with 0.5 vertical offset for visual clarity).The curve shape reflects each model's marginal contribution of new information with additional samples, revealing differences in exploration strategies and susceptibility to mode collapse.Models with flatter curves show diminishing returns in hypothesis diversity, consistent with the Recovery Rate degradation observed in Table1.</p>
<p>DIAGNOSTICS FOR CREATIVE HYPOTHESES GENERATION : DETAILS OF FORMULATION AND INDICATORSProblem setup.Let O denote the full observation space and let O ⊆ O be the subset revealed for a given instance.Let H be the overall hypothesis space and H O ⊆ H the subset of hypotheses that are consistent with O. Our diagnostic suite assumes that, for each instance, H O is explicitly enumerated, enabling exact validity checks and direct measurement of coverage without LLM-as-judge circularity.We require three properties: (i) soundness (every h ∈ H O is consistent with O), (ii) completeness (no valid hypothesis is omitted), and (iii) Controllability (the size of the ground-truth admissible set |H O | is controlled by task parameters-e.g., node count, grid dimensions, operator set/depth).</p>
<p>precise, modelagnostic measurement of Validity, Uniqueness, and Recovery without LLM-as-judge.(Extended related work in Appendix B.) 3</p>
<p>Table 1 :
1
|H O |. Causal Inference varies the number of nodes and observed interventions; 3D Voxel Reconstruction varies the number HypoSpace evaluation.Performance across reasoning and non-reasoning models using three complementary metrics: Validity Rate (VR; precision of hypotheses consistent with observations), Uniqueness/Novelty Rate (NR; non-redundancy among proposals), and Recovery Rate (RR; coverage of the enumerated admissible set H O ).Settings scale from Difficulty level 1 (simple) to 3 (hard) with corresponding increases in hypothesis space size |H O |. Results show mean ± standard deviation across instances.Reasoning models maintain high performance across all metrics, while non-reasoning models show degraded VR and consequently lower RR, particularly in harder settings with larger hypothesis spaces.We highlight the best and second-best results in the table.Boolean Genetic Interaction varies the Boolean operator set/depth and observation coverage.We use three regimes (simple/medium/hard).For transparency, the instance settings are printed in the Table1.Unless noted otherwise, for each instance we draw N independent samples with N = |H O |, compute VR/NR/RR on the N proposals, and aggregate by averaging across instances within task/difficulty (reporting mean±std across repeated runs).
Reasoning ModelsNon-Reasoning ModelsDifficultyMetricgpt-5gemini-2.5-proclaude-opus-4deepseek-r1grok4gpt-4ollama-3.3-70b-instructTask 1: Causal InferenceVR100.00% ± 0.00% 100.00% ± 0.00% 89.30% ± 21.70% 100.00% ± 0.00% 100.00% ± 0.00% 73.80% ± 32.70%30.00% ± 40.10%1 (nodes=4)NR100.00% ± 0.00% 100.00% ± 0.00% 86.20% ± 20.80% 98.20% ± 4.50% 100.00% ± 0.00% 83.20% ± 23.00%83.30% ± 27.00%RR100.00% ± 0.00% 100.00% ± 0.00% 77.50% ± 27.10% 98.20% ± 4.50% 100.00% ± 0.00% 60.90% ± 34.50%24.00% ± 34.40%VR100.00% ± 0.00% 100.00% ± 0.00% 80.10% ± 23.10% 99.00% ± 3.30% 100.00% ± 0.00% 49.00% ± 36.20%17.60% ± 28.10%2 (nodes=5)NR100.00% ± 0.00% 100.00% ± 0.00% 79.90% ± 23.60% 93.50% ± 12.00% 100.00% ± 0.00% 83.50% ± 24.60%86.00% ± 21.00%RR100.00% ± 0.00% 100.00% ± 0.00% 65.40% ± 28.40% 92.50% ± 12.80% 100.00% ± 0.00% 38.30% ± 32.80%17.60% ± 28.10%VR100.00% ± 0.00% 99.80% ± 0.80% 59.30% ± 21.20% 98.20% ± 3.00% 100.00% ± 0.00% 72.80% ± 34.10%10.40% ± 26.00%3 (nodes=6)NR99.20% ± 1.40%99.40 % ± 1.30%90.00% ± 7.90%81.20% ± 9.80%99.80% ± 1.20% 22.90% ± 27.20%38.00% ± 22.40%RR99.20% ± 1.40%99.20% ± 1.40% 51.10% ± 22.10% 79.50% ± 9.30%99.80% ± 1.20%6.70% ± 8.30%2.30% ± 2.70%Task 2: 3D UnderstandingVR100.00% ± 0.00% 100.00% ± 0.00% 100.00% ± 0.00% 96.30% ± 10.50% 100.00% ± 0.00% 63.00% ± 24.60%18.50% ± 16.60%1 (tp=1)NR100.00% ± 0.00% 100.00% ± 0.00% 100.00% ± 0.00% 100.00% ± 0.00% 100.00% ± 0.00% 96.30% ± 10.50%88.90% ± 15.70%RR100.00% ± 0.00% 100.00% ± 0.00% 100.00% ± 0.00% 96.30% ± 10.50% 100.00% ± 0.00% 59.30% ± 26.20%18.50% ± 16.60%VR100.00% ± 0.00% 100.00% ± 0.00% 90.40% ± 12.40% 99.30% ± 2.80% 100.00% ± 0.00% 38.50% ± 19.20%10.40% ± 16.70%2 (tp=2)NR100.00% ± 0.00% 100.00% ± 0.00% 87.80% ± 12.30% 99.60% ± 2.00% 100.00% ± 0.00% 75.20% ± 14.80%84.40% ± 13.30%RR100.00% ± 0.00% 100.00% ± 0.00% 78.10% ± 11.30% 98.90% ± 3.30% 100.00% ± 0.00% 30.70% ± 12.70%8.90% ± 13.60%VR100.00% ± 0.00% 99.90% ± 0.70% 62.30% ± 19.70% 99.00% ± 1.60%99.90% ± 0.70%19.40% ± 9.30%10.50% ± 13.10%3 (tp=3)NR98.80% ± 2.20%95.20% ± 4.60% 81.10% ± 10.60% 86.20% ± 5.20%99.90% ± 0.70% 57.90% ± 14.00%66.70% ± 14.30%RR98.80% ± 2.20%95.10% ± 4.80% 48.70% ± 11.50% 85.20% ± 4.90%99.80% ± 0.90%14.30% ± 6.50%6.00% ± 5.60%Task 3: DNA InteractionVR100.00% ± 0.00% 100.00% ± 0.00% 88.30% ± 16.50% 83.90% ± 15.50% 89.30% ± 21.70% 88.40% ± 26.20%95.60% ± 18.70%1 (basic)NR100.00% ± 0.00% 100.00% ± 0.00% 69.90% ± 19.40% 82.90% ± 16.30% 89.30% ± 21.70% 38.10% ± 21.40%30.10% ± 14.70%RR100.00% ± 0.00% 100.00% ± 0.00% 69.90% ± 19.40% 82.90% ± 16.30% 89.30% ± 21.70% 38.10% ± 21.40%29.00% ± 15.70%VR74.90% ± 21.40% 72.30% ± 21.10% 52.70% ± 23.20% 57.20% ± 21.70% 52.20% ± 20.40% 83.00% ± 27.50%55.80% ± 45.50%2 (extended)NR74.90% ± 21.40% 72.30% ± 21.10% 54.90% ± 24.50% 57.20% ± 21.70% 52.20% ± 20.40% 31.60% ± 21.50%18.40% ± 16.70%RR72.30% ± 21.30% 72.30% ± 21.10% 48.40% ± 23.20% 53.90% ± 24.50% 50.80% ± 20.70% 27.30% ± 22.90%14.90% ± 17.80%VR65.10% ± 12.50% 52.20% ± 15.50% 36.90% ± 15.90% 41.00% ± 13.60% 40.60% ± 15.00% 68.10% ± 37.40%66.60% ± 43.90%3 (full)NR49.90% ± 14.00% 48.90% ± 14.90% 24.20% ± 10.20% 36.60% ± 12.30% 37.90% ± 15.20% 21.10% ± 14.30%14.30% ± 9.90%RR48.00% ± 14.40% 47.40% ± 13.40% 23.80% ± 10.20% 36.00% ± 12.80% 36.50% ± 14.90% 14.10% ± 10.50%11.00% ± 9.40%of top-down views at fixed height budget;</p>
<p>Failure modes and exploration patterns in HypoSpace hypothesis generation.(a) Error categorization across models: Distribution of failure types including parse failures (malformed outputs), constraint violations (structurally invalid hypotheses), invalid generations (inconsistent with observations), and duplicates (exact and canonical equivalences).Higher proportions of duplicates indicate mode collapse tendencies.(b) Hypothesis space coverage: Fraction of the enumerated admissible set H O explored versus unexplored by each model.Limited exploration (lower blue bars)
38% 36% 27% Error Type Parse Failures 38% 59% Constraint Violations34% 63% Invalid Exact Duplicates 34% 60% 6%37% 43% 18% Canonical Duplicates 11% 73%25% 64%Percentage of Solution Space (%)20 40 60 80 100 12066% 34% Solution Space Explored65% 35%49% 51% Unexplored48% 52%38% 62%82%89%15%10%18%11%0G p t 5 G e m in i 2 .5 P roG ro k 4 D e e p s e e k R 1 C la u d e O p u s 4L la m a 3 .3 G p t 4 O 7 0 B In s tr u c t(a) Collapse Type Analysis(b) Exploration AnalysisFigure 4:</p>
<p>Table 2 :
2
Model Cost Efficiency AnalysisTable 2 compares spend and token usage across seven models under a fixed nine-run budget.A run denotes one execution of a model on a fixed (task, difficulty) condition, during which the model generates N samples; in Table 1, each row corresponds to one such run.
ModelTotal Cost ($) Total Tokens Cost/1M Tokens ($) Avg Tokens/Runclaude-opus-43.513,511,3721.00390,152deepseek-r141.4522,701,2101.832,522,356gemini-2.5-pro145.8816,789,2288.691,865,469gpt-4o10.662,886,1613.69320,684gpt-593.2511,510,6958.101,278,966grok-422.2122,214,0561.002,468,228llama-3.3-70b-instruct0.163,060,7410.05340,082</p>
<p>quantify hypothesis novelty via instruction prompts and post-hoc analysis; CreativEval (DeLorenzo et al., 2024) measures fluency, flexibility, originality, and elaboration in LLM-generated code; LiveIdeaBench (Ruan et al., 2024) evaluates divergent thinking across Guilford's creativity dimensions; Nagarajan et al. design open-ended algorithmic tasks explicitly targeting originality and diversity, arguing next-token prediction is myopic for creative leaps and showing seed conditioning, multi-token objectives, and diffusion can elicit more diverse outputs (Nagarajan et al., 2025); and Wang et al. formalize Relative and Statistical Creativity as distributional indistinguishability from human creators, yielding practical measures for prompt-conditioned autoregressive models</p>
<p>All models were accessed via OpenRouter OpenRouter (2025), except GPT-5, which was evaluated via the OpenAI API. To preserve double-blind review, provider/version metadata and full prompts/decoding settings are deferred to the appendix C. Our total API budget was ∼ $1,000.
However, most such evaluations rely on LLM-as-judge or human raters and lack a formally enumerated hypothesis space, making coverage and non-redundancy difficult to measure precisely.Our work complements these efforts by providing deterministic validators and exactly enumerated admissible sets, enabling model-agnostic, set-level measurement of Validity, Uniqueness, and Recovery.Table3lists the models included in our evaluation and clarifies the access path."Provider" refers to the model's origin organization (e.g., OpenAI, Google, Anthropic), not the API gateway; all models were accessed via OpenRouter except GPT-5, which was called through the OpenAI API.Full prompts and decoding settings are provided in our repository.C MODEL METADATA AND PROMPTS
Creativity In Context: Update To The Social Psychology Of Creativity. Teresa M Amabile, 1996Hachette UK</p>
<p>Large language model. Anthropic, 2025Claude opus 4</p>
<p>Creativity benchmark: A benchmark for marketing creativity for llm models. Ninad Bhat, Kieran Browne, Pip Bingemann, arXiv:2509.097022025arXiv preprint</p>
<p>The creative mind: Myths and mechanisms. Margaret A Boden, 2004Routledge</p>
<p>Auto-bench: An automated benchmark for scientific discovery in llms. Tingting Chen, Srinivas Anumasa, Beibei Lin, Vedant Shah, Anirudh Goyal, Dianbo Liu, arXiv:2502.152242025arXiv preprint</p>
<p>A performance study of llm-generated code on leetcode. Tristan Coignion, Clément Quinton, Romain Rouvoy, Proceedings of the 28th international conference on evaluation and assessment in software engineering. the 28th international conference on evaluation and assessment in software engineering2024</p>
<p>Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, arXiv:2507.062612025arXiv preprint</p>
<p>Creativeval: Evaluating creativity of llm-based hardware code generation. Matthew Delorenzo, Vasudev Gohil, Jeyavijayan Rajendran, 2024 IEEE LLM Aided Design Workshop (LAD). IEEE2024</p>
<p>. Joy Paul, Guilford , Creativity. American psychologist. 591950</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025aarXiv preprint</p>
<p>Ideabench: Benchmarking large language models for research idea generation. Sikun Guo, Hassan Amir, Guangzhi Shariatmadari, Albert Xiong, Myles Huang, Corey M Kim, Stefan Williams, Aidong Bekiranov, Zhang, Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V2025b2</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Nolan Koblischke, Hyunseok Jang, Kristen Menou, Mohamad Ali-Dib, arXiv:2501.18411Gravity-bench-v1: A benchmark on gravitational physics discovery for agents. 2025arXiv preprint</p>
<p>Abandoning objectives: Evolution through the search for novelty alone. Joel Lehman, Kenneth O Stanley, Evolutionary computation. 1922011</p>
<p>Hypobench: Towards systematic and principled benchmarking for hypothesis generation. Haokun Liu, Sicong Huang, Jingyu Hu, Yangqiaoyu Zhou, Chenhao Tan, arXiv:2504.115242025arXiv preprint</p>
<p>Roll the dice &amp; look before you leap: Going beyond the creative limits of next-token prediction. Chen Henry Vaishnavh Nagarajan, Charles Wu, Aditi Ding, Raghunathan, arXiv:2504.152662025arXiv preprint</p>
<p>Flagship multimodal large language model released. 2025. August 7, 2025OpenAIGpt-5</p>
<p>Openrouter, Openrouter, A unified interface for llms. </p>
<p>Some empirical criteria for attributing creativity to a computer program. Minds and Machines. Graeme Ritchie, 200717</p>
<p>Liveideabench: Evaluating llms' divergent thinking for scientific idea generation with minimal context. Kai Ruan, Xuan Wang, Jixiang Hong, Peng Wang, Yang Liu, Hao Sun, arXiv:2412.175962024arXiv preprint</p>
<p>Llm-sr: Scientific equation discovery via programming with large language models. Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K Reddy, arXiv:2404.184002024arXiv preprint</p>
<p>Llm-srbench: A new benchmark for scientific equation discovery with large language models. Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Chandan K Khoa D Doan, Reddy, arXiv:2504.104152025arXiv preprint</p>
<p>Exceeding our grasp: Science, history, and the problem of unconceived alternatives. Stanford Kyle, 2010Oxford University Press</p>
<p>Torrance tests of creative thinking. Torrance Paul, Educational and psychological measurement. 1966</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>The scientific image. C Bas, Van Fraassen, 1980Oxford University Press</p>
<p>Haonan Wang, James Zou, Michael Mozer, Anirudh Goyal, Alex Lamb, Linjun Zhang, Weijie J Su, Zhun Deng, Michael Qizhe Xie, Hannah Brown, arXiv:2401.01623Can ai be as creative as humans?. 2024aarXiv preprint</p>
<p>Mmlu-pro: A more robust and challenging multitask language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Advances in Neural Information Processing Systems. 2024b. August 202537Grok 4 model card</p>            </div>
        </div>

    </div>
</body>
</html>
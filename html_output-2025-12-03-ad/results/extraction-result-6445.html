<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6445 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6445</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6445</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-5e4597eb21a393b23e473cf66cb5ae8b27cab03e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5e4597eb21a393b23e473cf66cb5ae8b27cab03e" target="_blank">ExpeL: LLM Agents Are Experiential Learners</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> The Experiential Learning (ExpeL) agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks, indicating a consistent enhancement in its performance as it accumulates experiences.</p>
                <p><strong>Paper Abstract:</strong> The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6445.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6445.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experiential Learning (ExpeL) agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based planning agent that stores its own trial trajectories in an external vector store and extracts high-level natural-language insights from successes/failures; at test time it augments prompts with retrieved similar successful trajectories and a list of extracted insights to improve single‑try decision making without parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses an experience pool (Faiss vectorstore) of self-generated trajectories plus a dynamically maintained list of natural‑language 'insights' (rules) created/edited by an LLM. During evaluation the policy LLM is given: (1) the concatenated insights as part of the instruction and (2) top‑k retrieved successful trajectories as in‑context examples; insight extraction is performed by an LLM that can ADD/EDIT/UPVOTE/DOWNVOTE insights based on sampled success/failure pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector store (Faiss) for episodic trajectories + declarative natural-language insight list</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text trajectories (full trajectories stored), dense embeddings of task/trajectory text (all-mpnet-base-v2) for retrieval, and a list of extracted natural-language insights with importance counts</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>write: trajectories appended to Faiss experience pool during training; insights updated iteratively by an LLM_insights using ADD/EDIT/UPVOTE/DOWNVOTE and importance counts; read: Faiss kNN retrieval (inner-product similarity on embeddings) to obtain top-k similar successful trajectories; insights are concatenated into the instruction prompt and included in context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA, ALFWorld, WebShop (evaluation); FEVER (transfer experiment using adapted insights)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop question answering (HotpotQA), embodied planning/interactive environment (ALFWorld), web interaction/shopping (WebShop), fact verification transfer (FEVER)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>HotpotQA: 39.0% success rate ±1.7; ALFWorld: 59.0% success rate ±0.3; FEVER (transfer setting using adapted insights): 70% ±0.7; ALFWorld (with Reflexion reattempts + ExpeL): R0 59.0%, R1 60.4%, R2 63.4%, R3 64.2% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>HotpotQA baseline ReAct: 28.0% ±1.4; ALFWorld baseline ReAct: 40.0% ±0.3; FEVER baselines: Act 58% ±0.0, ReAct 63% ±0.4; ALFWorld ReAct+Reflexion R0: 40.3% (see Table 1-3 and Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (%) (task-specific reward-based success)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Context-window scaling: extracted insights currently fit within token limits but may require an additional retrieval step for lifelong scaling; reliance on closed-source API models (gpt-3.5/gpt-4) rather than local weights; possible latency and cost of repeated API calls for insight extraction and retrieval during training; retrieval and insight length increase prompt size which may increase inference latency; using reflections in insight construction can introduce hallucinations which degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Works only with textual observations (not visual); insights produced from reflections sometimes hallucinate and harm performance — including reflections in the insight extraction step decreased HotpotQA performance; WebShop performance did not uniformly exceed self-reflection baselines in all settings (authors note room for improvement); extracted insights must be kept within token limits or require retrieval management; method lacks theoretical guarantees compared to RL approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Zhao, A.; Huang, D.; Xu, Q.; Lin, M.; Liu, Y.-J.; and Gao, H. 2024. ExpeL: LLM Agents Are Experiential Learners. AAAI (2024).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpeL: LLM Agents Are Experiential Learners', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6445.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6445.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExpeL (retrieve-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ExpeL variant: retrieval-only (no extracted insights used at inference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of ExpeL that uses only the Faiss experience pool retrieval of successful trajectories as in-context examples but does not supply the extracted high‑level insights in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ExpeL (retrieve-only)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Stores successful trajectories in a Faiss vectorstore and retrieves top-k similar trajectories to include as few‑shot examples during evaluation, but omits the separate list of extracted natural-language insights from prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector store (Faiss) for episodic trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text trajectories and dense embeddings (all-mpnet-base-v2) for retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Faiss kNN retrieval (inner-product) over embeddings to obtain top-k similar successful trajectories; trajectories appended to pool during training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, general evaluations reported alongside ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>embodied planning / interactive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ALFWorld (Table 2, R0): 54.5% success rate (R0); across reattempt rounds R0–R3: 54.5%, 57.5%, 59.7%, 60.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct baseline (no retrieval/insights): ALFWorld 40.0% ±0.3 (Table 3); ReAct+Reflexion R0: 40.3% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Shows that retrieval alone provides substantial gains but combining with extracted insights is synergistic; retrieval depends on embedding quality and choice of similarity metric—random sampling of trajectories yields significant performance drops; retrieval can increase prompt size and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Retrieval-only underperforms the full ExpeL that also supplies insights, indicating retrieval without high-level abstractions is insufficient for some domains; selection strategy matters (random sampling performs much worse; reasoning-step similarity slightly worse than task similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Zhao, A.; Huang, D.; Xu, Q.; Lin, M.; Liu, Y.-J.; and Gao, H. 2024. ExpeL: LLM Agents Are Experiential Learners. AAAI (2024).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpeL: LLM Agents Are Experiential Learners', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6445.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6445.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExpeL (insights-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ExpeL variant: insights-only (no retrieval of similar trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant that uses only the extracted natural-language insights (rules) in the prompt and does not retrieve stored successful trajectories as in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ExpeL (insights-only)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses the LLM_insights-generated list of high-level natural-language insights as mandatory instruction text for the policy LLM at inference time but omits retrieval of example trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>declarative natural-language insight list (condensed memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>a ranked list of natural-language insights/rules with associated importance counts</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>insights are created/edited/removed by LLM_insights using operators (ADD/EDIT/UPVOTE/DOWNVOTE) on sampled success/failure sets and are concatenated into the instruction prompt at inference time</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA (notably large effect), ALFWorld, WebShop analyses</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop QA, embodied planning, web interaction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>HotpotQA: ExpeL (full) 39.0% SR; ablations show that providing learned insights produces a substantial boost over ReAct; exact numeric for insights-only variant not tabulated as a single line in main tables but authors state both components (insights and retrieval) are essential and synergistic.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct baseline HotpotQA: 28.0% ±1.4; hand-crafted insights (manual) gave 32.0% ±1.1 while learned insights gave larger gains (ExpeL 39.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Quality of insights depends on the LLM used for insight extraction (gpt-4 produced better insights than gpt-3.5); including reflections in the insight extraction pipeline sometimes harms performance due to hallucinations from reflection outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Insights derived solely from a small set of fewshots (without experience gathering) gave no advantage over ReAct; reflections-added insights degraded performance in ablation, indicating sensitivity to source data quality for insight extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Zhao, A.; Huang, D.; Xu, Q.; Lin, M.; Liu, Y.-J.; and Gao, H. 2024. ExpeL: LLM Agents Are Experiential Learners. AAAI (2024).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpeL: LLM Agents Are Experiential Learners', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6445.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6445.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning + Acting) agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A planning framework that interleaves natural-language reasoning traces ('Thought') and environment actions to enable LLMs to plan and act in environments; used here as the base policy without persistent cross‑task memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Interleaves reasoning (textual chain-of-thought) and actions in the prompt to guide an LLM policy; in this paper ReAct is used as the base planner for trajectory generation during experience collection and as a baseline during evaluation. It does not maintain persistent cross-task memory (stateless between tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA, ALFWorld, WebShop (baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop QA, embodied planning, web interaction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>HotpotQA: 28.0% ±1.4 (Table 3); ALFWorld: 40.0% ±0.3 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>No persistent memory; can be improved by adding other components like retrieval or insights (as shown by ExpeL comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lacks recollection of previous tasks across context windows; limited by context window size and cannot learn across tasks without external mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. ICLR (used/reference in Zhao et al. 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpeL: LLM Agents Are Experiential Learners', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6445.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6445.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (self-reflecting language agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-improvement framework where an LLM agent analyzes failed trajectories to produce reflections that guide retries on the same task; used in ExpeL to collect diverse success/failure pairs but is itself largely stateless across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generates natural-language reflections from failed trajectories which are fed back as additional context for subsequent retries of the same task (intra-task improvement); ExpeL leverages Reflexion during experience gathering to increase the number and diversity of successful and failed trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>ephemeral self-reflection buffer (per-task), not persistent cross-task memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>concatenated textual reflections for the current task (nu_{n,z} in paper), appended between retries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>writes: LLM_reflect produces reflection text appended to per-task reflection context; reads: policy LLM conditions on the current reflection string for the next retry</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used during experience gathering for HotpotQA, ALFWorld, WebShop</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>intra-task self-improvement / planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ReAct+Reflexion reported (ALFWorld R0–R3): 40.3%, 47.8%, 52.2%, 54.4% (Table 2) showing improvements across retries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct baseline without Reflexion: ALFWorld 40.0% ±0.3 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (%) across retry rounds</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Reflexion requires task reattempts and environment feedback at test time, making it unsuitable where only one attempt is allowed; reflections can hallucinate and when used in insight extraction may degrade downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Stateless across tasks — does not accumulate cross-task insights; dependent on task repeatability and availability of environment feedback during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Shinn, N.; Cassano, F.; Gopinath, A.; Narasimhan, K. R.; and Yao, S. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning. NeurIPS (used/reference in Zhao et al. 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpeL: LLM Agents Are Experiential Learners', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6445.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6445.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) / retrieval-augmented methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of approaches that augment LLMs with an external retrieval component (database/vectorstore) to condition generation on retrieved passages or examples; cited as related work and conceptual precedent for ExpeL's retrieval of past trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Provides LLMs access to external databases via retrieval (e.g., kNN over embeddings) which are then included in the model context to ground or improve generation; conceptually similar to ExpeL's retrieval of self-generated trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external database / vector store</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>text passages or demonstration examples stored as text plus embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>similarity search / retriever retrieves relevant documents or examples and concatenates them into prompt context</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>general NLP grounding / retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Helps mitigate hallucinations and provide grounding but increases prompt size and retrieval latency; RAG depends on the quality and relevance of the retrieval corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated experimentally in this paper beyond conceptual comparison; retrieval quality and prompt length constraints remain practical challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Li, H.; Su, Y.; Cai, D.; Wang, Y.; and Liu, L. 2022. A Survey on Retrieval-Augmented Text Generation. (cited in Zhao et al. 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpeL: LLM Agents Are Experiential Learners', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6445.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6445.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenerativeAgents (memory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents with persistent memory (Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multi-agent systems that maintain persistent memory stores with retrieval based on recency, relevance and importance to support open‑ended behaviour; cited as prior work demonstrating persistent memory in generative agent settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (with memory mechanisms)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Use a persistent memory mechanism for each agent that stores observations and events and provides retrieval based on recency/relevance/importance for downstream behavior and interaction; cited as inspiration/related work to ExpeL's use of memory, although those works are typically open‑ended rather than task‑solving.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>persistent memory store (recency/relevance/importance-based retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>textual memory entries (events, observations) with metadata (timestamps, importance)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>heuristic retrieval based on recency, relevance and importance (as described in Park et al.); not specifically experimented in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-agent simulation / generative agent behaviour</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Works are open-ended; memory provides richer interaction but raises questions of scaling, privacy, and consistency; not directly benchmarked here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Different problem focus (open‑ended simulations) from ExpeL's task-solving focus; cited as related work but no experimental comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Park, J. S.; O’Brien, J.; Cai, C. J.; Morris, M. R.; Liang, P.; and Bernstein, M. S. 2023. Generative Agents: Interactive Simulacra of Human Behavior. (cited in Zhao et al. 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpeL: LLM Agents Are Experiential Learners', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language Agents with Verbal Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>A Survey on Retrieval-Augmented Text Generation <em>(Rating: 2)</em></li>
                <li>Generative Agents: Interactive Simulacra of Human Behavior <em>(Rating: 2)</em></li>
                <li>Large-scale Retrieval for Reinforcement Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6445",
    "paper_id": "paper-5e4597eb21a393b23e473cf66cb5ae8b27cab03e",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "ExpeL",
            "name_full": "Experiential Learning (ExpeL) agent",
            "brief_description": "An LLM-based planning agent that stores its own trial trajectories in an external vector store and extracts high-level natural-language insights from successes/failures; at test time it augments prompts with retrieved similar successful trajectories and a list of extracted insights to improve single‑try decision making without parameter updates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ExpeL",
            "agent_description": "Uses an experience pool (Faiss vectorstore) of self-generated trajectories plus a dynamically maintained list of natural‑language 'insights' (rules) created/edited by an LLM. During evaluation the policy LLM is given: (1) the concatenated insights as part of the instruction and (2) top‑k retrieved successful trajectories as in‑context examples; insight extraction is performed by an LLM that can ADD/EDIT/UPVOTE/DOWNVOTE insights based on sampled success/failure pairs.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "external vector store (Faiss) for episodic trajectories + declarative natural-language insight list",
            "memory_representation": "raw text trajectories (full trajectories stored), dense embeddings of task/trajectory text (all-mpnet-base-v2) for retrieval, and a list of extracted natural-language insights with importance counts",
            "memory_access_mechanism": "write: trajectories appended to Faiss experience pool during training; insights updated iteratively by an LLM_insights using ADD/EDIT/UPVOTE/DOWNVOTE and importance counts; read: Faiss kNN retrieval (inner-product similarity on embeddings) to obtain top-k similar successful trajectories; insights are concatenated into the instruction prompt and included in context.",
            "task_name": "HotpotQA, ALFWorld, WebShop (evaluation); FEVER (transfer experiment using adapted insights)",
            "task_category": "multi-hop question answering (HotpotQA), embodied planning/interactive environment (ALFWorld), web interaction/shopping (WebShop), fact verification transfer (FEVER)",
            "performance_with_memory": "HotpotQA: 39.0% success rate ±1.7; ALFWorld: 59.0% success rate ±0.3; FEVER (transfer setting using adapted insights): 70% ±0.7; ALFWorld (with Reflexion reattempts + ExpeL): R0 59.0%, R1 60.4%, R2 63.4%, R3 64.2% (Table 2).",
            "performance_without_memory": "HotpotQA baseline ReAct: 28.0% ±1.4; ALFWorld baseline ReAct: 40.0% ±0.3; FEVER baselines: Act 58% ±0.0, ReAct 63% ±0.4; ALFWorld ReAct+Reflexion R0: 40.3% (see Table 1-3 and Table 2).",
            "has_comparative_results": true,
            "performance_metric": "Success Rate (%) (task-specific reward-based success)",
            "tradeoffs_reported": "Context-window scaling: extracted insights currently fit within token limits but may require an additional retrieval step for lifelong scaling; reliance on closed-source API models (gpt-3.5/gpt-4) rather than local weights; possible latency and cost of repeated API calls for insight extraction and retrieval during training; retrieval and insight length increase prompt size which may increase inference latency; using reflections in insight construction can introduce hallucinations which degrade performance.",
            "limitations_or_failure_cases": "Works only with textual observations (not visual); insights produced from reflections sometimes hallucinate and harm performance — including reflections in the insight extraction step decreased HotpotQA performance; WebShop performance did not uniformly exceed self-reflection baselines in all settings (authors note room for improvement); extracted insights must be kept within token limits or require retrieval management; method lacks theoretical guarantees compared to RL approaches.",
            "citation": "Zhao, A.; Huang, D.; Xu, Q.; Lin, M.; Liu, Y.-J.; and Gao, H. 2024. ExpeL: LLM Agents Are Experiential Learners. AAAI (2024).",
            "uuid": "e6445.0",
            "source_info": {
                "paper_title": "ExpeL: LLM Agents Are Experiential Learners",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ExpeL (retrieve-only)",
            "name_full": "ExpeL variant: retrieval-only (no extracted insights used at inference)",
            "brief_description": "A variant of ExpeL that uses only the Faiss experience pool retrieval of successful trajectories as in-context examples but does not supply the extracted high‑level insights in the prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ExpeL (retrieve-only)",
            "agent_description": "Stores successful trajectories in a Faiss vectorstore and retrieves top-k similar trajectories to include as few‑shot examples during evaluation, but omits the separate list of extracted natural-language insights from prompts.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "external vector store (Faiss) for episodic trajectories",
            "memory_representation": "raw text trajectories and dense embeddings (all-mpnet-base-v2) for retrieval",
            "memory_access_mechanism": "Faiss kNN retrieval (inner-product) over embeddings to obtain top-k similar successful trajectories; trajectories appended to pool during training.",
            "task_name": "ALFWorld, general evaluations reported alongside ExpeL",
            "task_category": "embodied planning / interactive tasks",
            "performance_with_memory": "ALFWorld (Table 2, R0): 54.5% success rate (R0); across reattempt rounds R0–R3: 54.5%, 57.5%, 59.7%, 60.4%.",
            "performance_without_memory": "ReAct baseline (no retrieval/insights): ALFWorld 40.0% ±0.3 (Table 3); ReAct+Reflexion R0: 40.3% (Table 2).",
            "has_comparative_results": true,
            "performance_metric": "Success Rate (%)",
            "tradeoffs_reported": "Shows that retrieval alone provides substantial gains but combining with extracted insights is synergistic; retrieval depends on embedding quality and choice of similarity metric—random sampling of trajectories yields significant performance drops; retrieval can increase prompt size and latency.",
            "limitations_or_failure_cases": "Retrieval-only underperforms the full ExpeL that also supplies insights, indicating retrieval without high-level abstractions is insufficient for some domains; selection strategy matters (random sampling performs much worse; reasoning-step similarity slightly worse than task similarity).",
            "citation": "Zhao, A.; Huang, D.; Xu, Q.; Lin, M.; Liu, Y.-J.; and Gao, H. 2024. ExpeL: LLM Agents Are Experiential Learners. AAAI (2024).",
            "uuid": "e6445.1",
            "source_info": {
                "paper_title": "ExpeL: LLM Agents Are Experiential Learners",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ExpeL (insights-only)",
            "name_full": "ExpeL variant: insights-only (no retrieval of similar trajectories)",
            "brief_description": "A variant that uses only the extracted natural-language insights (rules) in the prompt and does not retrieve stored successful trajectories as in-context examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ExpeL (insights-only)",
            "agent_description": "Uses the LLM_insights-generated list of high-level natural-language insights as mandatory instruction text for the policy LLM at inference time but omits retrieval of example trajectories.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "declarative natural-language insight list (condensed memory)",
            "memory_representation": "a ranked list of natural-language insights/rules with associated importance counts",
            "memory_access_mechanism": "insights are created/edited/removed by LLM_insights using operators (ADD/EDIT/UPVOTE/DOWNVOTE) on sampled success/failure sets and are concatenated into the instruction prompt at inference time",
            "task_name": "HotpotQA (notably large effect), ALFWorld, WebShop analyses",
            "task_category": "multi-hop QA, embodied planning, web interaction",
            "performance_with_memory": "HotpotQA: ExpeL (full) 39.0% SR; ablations show that providing learned insights produces a substantial boost over ReAct; exact numeric for insights-only variant not tabulated as a single line in main tables but authors state both components (insights and retrieval) are essential and synergistic.",
            "performance_without_memory": "ReAct baseline HotpotQA: 28.0% ±1.4; hand-crafted insights (manual) gave 32.0% ±1.1 while learned insights gave larger gains (ExpeL 39.0%).",
            "has_comparative_results": true,
            "performance_metric": "Success Rate (%)",
            "tradeoffs_reported": "Quality of insights depends on the LLM used for insight extraction (gpt-4 produced better insights than gpt-3.5); including reflections in the insight extraction pipeline sometimes harms performance due to hallucinations from reflection outputs.",
            "limitations_or_failure_cases": "Insights derived solely from a small set of fewshots (without experience gathering) gave no advantage over ReAct; reflections-added insights degraded performance in ablation, indicating sensitivity to source data quality for insight extraction.",
            "citation": "Zhao, A.; Huang, D.; Xu, Q.; Lin, M.; Liu, Y.-J.; and Gao, H. 2024. ExpeL: LLM Agents Are Experiential Learners. AAAI (2024).",
            "uuid": "e6445.2",
            "source_info": {
                "paper_title": "ExpeL: LLM Agents Are Experiential Learners",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning + Acting) agent",
            "brief_description": "A planning framework that interleaves natural-language reasoning traces ('Thought') and environment actions to enable LLMs to plan and act in environments; used here as the base policy without persistent cross‑task memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ReAct",
            "agent_description": "Interleaves reasoning (textual chain-of-thought) and actions in the prompt to guide an LLM policy; in this paper ReAct is used as the base planner for trajectory generation during experience collection and as a baseline during evaluation. It does not maintain persistent cross-task memory (stateless between tasks).",
            "model_size": null,
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": "HotpotQA, ALFWorld, WebShop (baselines)",
            "task_category": "multi-hop QA, embodied planning, web interaction",
            "performance_with_memory": "",
            "performance_without_memory": "HotpotQA: 28.0% ±1.4 (Table 3); ALFWorld: 40.0% ±0.3 (Table 3).",
            "has_comparative_results": true,
            "performance_metric": "Success Rate (%)",
            "tradeoffs_reported": "No persistent memory; can be improved by adding other components like retrieval or insights (as shown by ExpeL comparisons).",
            "limitations_or_failure_cases": "Lacks recollection of previous tasks across context windows; limited by context window size and cannot learn across tasks without external mechanisms.",
            "citation": "Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. ICLR (used/reference in Zhao et al. 2024).",
            "uuid": "e6445.3",
            "source_info": {
                "paper_title": "ExpeL: LLM Agents Are Experiential Learners",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (self-reflecting language agent)",
            "brief_description": "A self-improvement framework where an LLM agent analyzes failed trajectories to produce reflections that guide retries on the same task; used in ExpeL to collect diverse success/failure pairs but is itself largely stateless across tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Reflexion",
            "agent_description": "Generates natural-language reflections from failed trajectories which are fed back as additional context for subsequent retries of the same task (intra-task improvement); ExpeL leverages Reflexion during experience gathering to increase the number and diversity of successful and failed trajectories.",
            "model_size": null,
            "memory_used": false,
            "memory_type": "ephemeral self-reflection buffer (per-task), not persistent cross-task memory",
            "memory_representation": "concatenated textual reflections for the current task (nu_{n,z} in paper), appended between retries",
            "memory_access_mechanism": "writes: LLM_reflect produces reflection text appended to per-task reflection context; reads: policy LLM conditions on the current reflection string for the next retry",
            "task_name": "Used during experience gathering for HotpotQA, ALFWorld, WebShop",
            "task_category": "intra-task self-improvement / planning",
            "performance_with_memory": "ReAct+Reflexion reported (ALFWorld R0–R3): 40.3%, 47.8%, 52.2%, 54.4% (Table 2) showing improvements across retries.",
            "performance_without_memory": "ReAct baseline without Reflexion: ALFWorld 40.0% ±0.3 (Table 3).",
            "has_comparative_results": true,
            "performance_metric": "Success Rate (%) across retry rounds",
            "tradeoffs_reported": "Reflexion requires task reattempts and environment feedback at test time, making it unsuitable where only one attempt is allowed; reflections can hallucinate and when used in insight extraction may degrade downstream performance.",
            "limitations_or_failure_cases": "Stateless across tasks — does not accumulate cross-task insights; dependent on task repeatability and availability of environment feedback during evaluation.",
            "citation": "Shinn, N.; Cassano, F.; Gopinath, A.; Narasimhan, K. R.; and Yao, S. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning. NeurIPS (used/reference in Zhao et al. 2024).",
            "uuid": "e6445.4",
            "source_info": {
                "paper_title": "ExpeL: LLM Agents Are Experiential Learners",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG) / retrieval-augmented methods",
            "brief_description": "A family of approaches that augment LLMs with an external retrieval component (database/vectorstore) to condition generation on retrieved passages or examples; cited as related work and conceptual precedent for ExpeL's retrieval of past trajectories.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Retrieval-Augmented Generation (RAG)",
            "agent_description": "Provides LLMs access to external databases via retrieval (e.g., kNN over embeddings) which are then included in the model context to ground or improve generation; conceptually similar to ExpeL's retrieval of self-generated trajectories.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "external database / vector store",
            "memory_representation": "text passages or demonstration examples stored as text plus embeddings",
            "memory_access_mechanism": "similarity search / retriever retrieves relevant documents or examples and concatenates them into prompt context",
            "task_name": null,
            "task_category": "general NLP grounding / retrieval-augmented generation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Helps mitigate hallucinations and provide grounding but increases prompt size and retrieval latency; RAG depends on the quality and relevance of the retrieval corpus.",
            "limitations_or_failure_cases": "Not evaluated experimentally in this paper beyond conceptual comparison; retrieval quality and prompt length constraints remain practical challenges.",
            "citation": "Li, H.; Su, Y.; Cai, D.; Wang, Y.; and Liu, L. 2022. A Survey on Retrieval-Augmented Text Generation. (cited in Zhao et al. 2024).",
            "uuid": "e6445.5",
            "source_info": {
                "paper_title": "ExpeL: LLM Agents Are Experiential Learners",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GenerativeAgents (memory)",
            "name_full": "Generative Agents with persistent memory (Park et al.)",
            "brief_description": "Multi-agent systems that maintain persistent memory stores with retrieval based on recency, relevance and importance to support open‑ended behaviour; cited as prior work demonstrating persistent memory in generative agent settings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (with memory mechanisms)",
            "agent_description": "Use a persistent memory mechanism for each agent that stores observations and events and provides retrieval based on recency/relevance/importance for downstream behavior and interaction; cited as inspiration/related work to ExpeL's use of memory, although those works are typically open‑ended rather than task‑solving.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "persistent memory store (recency/relevance/importance-based retrieval)",
            "memory_representation": "textual memory entries (events, observations) with metadata (timestamps, importance)",
            "memory_access_mechanism": "heuristic retrieval based on recency, relevance and importance (as described in Park et al.); not specifically experimented in this paper",
            "task_name": null,
            "task_category": "multi-agent simulation / generative agent behaviour",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Works are open-ended; memory provides richer interaction but raises questions of scaling, privacy, and consistency; not directly benchmarked here.",
            "limitations_or_failure_cases": "Different problem focus (open‑ended simulations) from ExpeL's task-solving focus; cited as related work but no experimental comparisons in this paper.",
            "citation": "Park, J. S.; O’Brien, J.; Cai, C. J.; Morris, M. R.; Liang, P.; and Bernstein, M. S. 2023. Generative Agents: Interactive Simulacra of Human Behavior. (cited in Zhao et al. 2024).",
            "uuid": "e6445.6",
            "source_info": {
                "paper_title": "ExpeL: LLM Agents Are Experiential Learners",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "A Survey on Retrieval-Augmented Text Generation",
            "rating": 2,
            "sanitized_title": "a_survey_on_retrievalaugmented_text_generation"
        },
        {
            "paper_title": "Generative Agents: Interactive Simulacra of Human Behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Large-scale Retrieval for Reinforcement Learning",
            "rating": 1,
            "sanitized_title": "largescale_retrieval_for_reinforcement_learning"
        }
    ],
    "cost": 0.018818,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ExpeL: LLM Agents Are Experiential Learners</h1>
<p>Andrew Zhao, ${ }^{\text {D }}$ Daniel Huang, ${ }^{\text {A }}$ Quentin Xu, ${ }^{\text {A }}$ Matthieu Lin, ${ }^{\text {A }}$ Yong-Jin Liu, ${ }^{\text {A }}$ Gao Huang ${ }^{\text {A }}$<br>${ }^{\text {A }}$ Department of Automation, BNRist, Tsinghua University<br>${ }^{\text {A }}$ Department of Computer Science, BNRist, Tsinghua University<br>{zqc21, huang-jy22, xgd22, lyh21}@mails.tsinghua.edu.cn,<br>{liuyongjin, gaohuang}@tsinghua.edu.cn</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h4>Abstract</h4>
<p>The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments. ${ }^{1}$</p>
<p>A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$. Tom Mitchell</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1 Introduction</p>
<p>Machine learning research has long been captivated by the potential of autonomous agents and their capabilities. In recent times, incorporating large language models into these agents (Wang et al. 2023b; Xi et al. 2023) has unveiled a broad spectrum of applications, even extending beyond academia (Yang et al. 2023a; Nakajima 2023; SignificantGravitas 2023). One of the significant advantages of LLMs lies in their world knowledge, allowing them to be inherently versatile across various scenarios (Zhao et al. 2023b).</p>
<p>On the one hand, previous works investigated finetuning LLMs with a large number of environment interactions (Yao et al. 2023c) or with a large amount of human-labeled datasets (Nakano et al. 2021; Shaw et al. 2023). This class of methods incurs high computational costs and needs access to the LLM’s parametric weights. Furthermore, finetuning an LLM restricts its functionalities and can hurt its generalization abilities (Du et al. 2022). On the other hand, prompting methods can augment an LLM with better sequential decision-making planning abilities with only a few in-context examples (Hao et al. 2023; Lin et al. 2023b; Sun et al. 2023). However, since current LLMs are bounded by context window size (Tworkowski et al. 2023), these agents have no recollections of what they have seen, and therefore no learning can be done outside of a few demonstrations. So, how can we strike a balance between these paradigms?</p>
<p>We present the Experiential Learning (ExpeL) agent as a solution. Our agent autonomously gathers experiences from a collection of training tasks through trial and error. From these experiences, it derives natural language insights and employs its own successful experiences as in-context examples during test time. Our agent’s learning process is analogous to a student studying for an exam and then taking it on a single attempt, reflecting many real-world situations. Unlike self-improvement methods like Reflexion (Shinn et al. 2023), our approach emphasizes the importance of retaining experiences across multiple tasks to enhance agent performance. Moreover, ExpeL learns without parameter updates, making it compatible with powerful closed-source models like GPT-4 or Claude. Lastly, the experience-gathering step does not require a large amount of data or human labels.</p>
<p>We evaluated ExpeL on three vastly different domains and consistently outperformed strong baselines. Additionally, we showcased a transfer learning scenario where our agent that accumulated knowledge from source tasks showed positive forward transfer to target tasks. Finally, we highlighted some unexpected emerged abilities the ExpeL agent gained.</p>
<p>In summary, our key contributions are as follows: (1) we introduced ExpeL, a novel LLM agent that autonomously learns from experience without gradient updates; (2) We evaluated ExpeL on a diverse set of tasks to showcase its learning abilities and improvement on top of existing planning methods; (3) we showed a novel setting of transfer learning for our LLM agent and demonstrated forward transferability from source tasks to target tasks. Lastly, we believe that as planning algorithms and foundational models continue to improve, ExpeL’s paradigm stands to gain significant benefits from their enhanced performances.</p>
<h2>2 Related Work</h2>
<p>We discuss the most relevant related works in this section. See Appendix A for detailed discussions on related works.</p>
<p>Prompt-based Learning: Prompt-based learning refines label prediction tasks by modifying the input context, facilitating swift adaptation to new tasks with minimal data (Liu et al. 2023a). This approach capitalizes on LLMs for answers without parameter tuning as they can be augmented using in-context learning (Brown et al. 2020). LAMA (Petroni et al. 2019) and GPT-3 (Brown et al. 2020) are early works that promoted this formulation. Efforts to reduce the intricacies of prompt design include automatic reasoning chains for NLP (Kojima et al. 2022; Zhang et al. 2023). Similarly, the ExpeL agent also autonomously learns from experiences using extracted insights and self-generated in-context trajectories by altering the execution prompt.
Retrieval Augmented Generation (RAG): Retrieval allows LLMs to access databases, mitigating hallucinations (Li et al. 2022; Wang, Yang, and Wei 2023; Rubin, Herzig, and Berant 2022; Liu et al. 2022). Retrieval has also been used to enhance the capabilities of decision-making agents (Humphreys et al. 2022; Zhao et al. 2023a). In contrast to these works, we focus on retrieving the ExpeL agent’s self-generated experiences, thus reducing the dependency on gold examples and leveraging domain-specific corpus.
Planning for LLM Agents: Application of LLM agents in fields like robotics, natural sciences, game-playing, and workflows has surged, with emphasis on their world knowledge in fewshot settings (Ha, Florence, and Song 2023; Mu et al. 2023; Bran et al. 2023; Boiko, MacKnight, and Gomes 2023; Yang et al. 2023b; Lin et al. 2023a; Nakano et al. 2021; Wang et al. 2023c; Liu et al. 2023b). Moreover, LLMs have demonstrated promising zero/few-shot planning and reasoning capabilities in various configurations (Sumers et al. 2023), including embodied environments and reasoning tasks (Huang et al. 2022; Yao et al. 2023a; Wei et al. 2022b; Yao et al. 2023b; Gong et al. 2023).
Self-improvement and Memory for LLM Agents: Agents like Reflexion showcase feedback-based improvement, yet often lack cross-task memory (Shinn et al. 2023). Other agents exhibit potential in persistent memory within multiagent contexts (Park et al. 2023; Maas et al. 2023). Our ExpeL agent combines these approaches, focusing on tasksolving while benefiting from self-generated in-context examples and abstracted insights from memory.</p>
<h2>3 Preliminaries</h2>
<p>Complex Interactive Tasks We work with complex interactive tasks where at each time step $i \in{0, \ldots, H}$, the agent receives an observation $o \in \mathcal{O}$, and from its observation history $H_{t}$ decides to perform action $a \in \mathcal{A}$. The objective of the agent is to achieve some goal $g \in \mathcal{G}$. We only deal with deterministic environments in this work.</p>
<p>Large Language Models A large language model is a statistical model of the natural language, typically a neural network. In our setting, we use an autoregressive language model (OpenAI 2023; Brown et al. 2020; Touvron</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 1: ExpeL Agent Overview. Left: ExpeL operates in three stages: (1) Collection of success and failure experiences into a pool. (2) Extraction/abstraction of cross-task knowledge from these experiences. (3) Application of the gained insights and recall of past successes in evaluation tasks. Right: (A) Illustrates the experience gathering process via Reflexion <em>Shinn et al. (2023)</em>, enabling task reattempt after self-reflection on failures. (B) Illustrates the insight extraction step. When presented with success/failure pairs or a list of $L$ successes, the agent dynamically modifies an existing list of insights $i$ using operations ADD, UPVOTE, DOWNVOTE, and EDIT. This process has an emphasis on extracting prevalent failure patterns or best practices.</p>
<p>et al. (2023b, a); Chowdhery et al. (2023)<em>, which given an ordered list of existing tokens $\mathbf{x}=\left{x_{1},x_{2},...,x_{l-1}\right}$, outputs the probability of the next token $p(x_{l} \mid x_{&lt;l})$. An instruction-following LLM </em>Thoppilan et al. (2022); Chung et al. (2022); Wei et al. (2022a)<em> is typically finetuned on various NLP tasks that are formatted into instruction, input, response tuples </em>Taori et al. (2023)<em>. Instruction-tuned models are better at following natural language instructions which alleviates the need for heavy prompt engineering </em>Wei et al. (2022a)*.</p>
<p>ReAct and Reflexion ReAct <em>Yao et al. (2023b)</em> and Reflexion <em>Shinn et al. (2023)</em> are promising frameworks enabling the aforementioned proficiency of LLMs in reasoning and self-improvement. ReAct explicitly intertwines observations, actions, and thoughts, providing a foundation for robust planning and reasoning capabilities. Building upon it, Reflexion introduces an additional reflective step before reattempting the subsequent trial of the same task, enhancing the model’s adaptive learning process.</p>
<h2>4 ExpeL: An Experiential Learning Agent</h2>
<p>Recent advancements in generative LLMs suggest an intriguing approach. Rather than altering the LLM parameters, adjusting the prompts may be more beneficial: this strategy ensures that the LLM’s inherent common sense knowledge remains intact, allowing for superior generalization <em>Liu et al. (2023a)</em>. Furthermore, some of the most potent language models are proprietary <em>OpenAI (2023); Anthropic (2023)</em>. Thus, focusing on prompt-based methods seems promising as a way to harness the strengths of these advanced LLMs. Additionally, previous works on learning in LLM agents have primarily been trained on extensive human-labeled datasets <em>Lin et al. (2023a); Shaw et al. (2023)</em> or improved via iterative retries <em>Shinn et al. (2023)</em> on a single task. A relatively less explored area is facilitating agents to learn autonomously from their own experiences, similar to a student gaining insights from practicing for an exam. The student tackles practice problems multiple times to derive insights. At the exam, the student rely solely on these insights and draw memories of similar problems to answer the questions with one attempt. With this in mind, we wish to design an LLM agent that autonomously gathers experiences and extracts insights, then uses these cross-task insights and memories of similar tasks to aid its decision-making.</p>
<p>We aim to enhance a planning LLM agent, such as ReAct, with learning abilities that allow it to improve through inter-task experiences without any parameter updates. Inspired by the cognitive abilities inherent in human learning, as well as the benefits observed in self-learning autonomous agents and the progress made in prompt-based methods, we developed the Experiential Learning (ExpeL) agent. During the training stage, the agent interacts with the environment, gathering experiences via trial and error. These experiences are stored in an experience pool <em>Lin (1992)</em>. From this pool, the agent later extracts insights, similar to off-policy learn-</p>
<p>ing <em>Watkins and Dayan (1992)</em>, in which the agent can learn from experiences of a behavior policy. During the evaluation stage, the agent attempts unseen tasks with a single try, augmented with extracted insights and successful trajectories in its experience pool gathered from the training stage. Refer to Fig. 1 for detailed information on our agent framework.</p>
<h3>4.1 Gathering Experiences</h3>
<p>To gather diverse experiences that can be useful to extract information from, we leverage Reflexion <em>Shinn et al. (2023)</em> to continuously retry the training task at most $Z$ times. In particular, the agent will be given a training task $t_{n}$ at the $z$-th trial, fewshot examples $F_{\text{manual}}$ and past reflections $\nu_{n,z}$ (initially, $\nu_{n,0}$ is the empty string). At first, the agent will attempt the task with fewshot examples concatenated with its current trajectory $\tau_{n,0}$ as the context, and use ReAct <em>Yao et al. (2023b)</em> as the base planning algorithm, $\operatorname{LLM}<em n_0="n,0">{\operatorname{ReAct}}\left(\cdot \mid \tau</em>}, F_{\text{manual}}, \nu_{n,0}\right)$. On the $z$-th trial, when the agent finishes the task or the maximum number of steps $H$ is reached, the ExpeL agent’s experience pool $\mathcal{B}$ ingests the trajectory $\tau_{n, z}$. Then, if the agent succeeds, it moves on to the next task. However, if the agent fails, it will look at its failed trajectory and self-reflect to produce $\nu_{n, z+1}=\operatorname{concat}\left(\nu_{n, z}, \operatorname{LLM<em n_="n," z="z">{\text {reflect }}\left(\tau</em>}\right)\right)$ to see where it can do better on the next retry, concatenated with the previous reflections. In the next retry, the agent will augment its context with reflection $\nu_{n, z+1}$, the input to the LLM policy, $\operatorname{LLM<em n_="n," z_1="z+1">{\operatorname{ReAct}}\left(\cdot \mid \tau</em>\right)$.}, F_{\text {manual }}, \nu_{n, z+1</p>
<p>To highlight, this trial and error way of gathering experiences not only improves the chances of getting more positive examples for experience recall during evaluation but also allows for collecting valuable success/failure pairs used for comparisons during insight extraction (Sec. 4.2). The pseudo-code can be found in Alg. 1.</p>
<h3>4.2 Learning from Experiences</h3>
<p>Human learning occurs mainly either by storing successful trajectories in memory, which can be later recalled as specific examples, or by extracting high-level insights from experiences, enabling generalization to novel situations. ExpeL considers both of these learning modes to boost task performance. Concretely, an instruction $I$ given to an LLM agent can be broken down into task specifications and fewshot examples. We can augment task specifications with an agent’s extracted insights from past experiences, where an instruction-following LLM can be leveraged <em>OpenAI (2023)</em> to follow them closely. For fewshot examples, we can allow the agent to retrieve from its experience pool with top- $k$ relevant examples to aid its decisions. Next, we detail our experience recall and insight extraction mechanisms.</p>
<p>Similar Experiences as Demonstrations Works have shown that using in-context examples that are semantically similar to the task at hand results in better performance <em>Liu et al. (2022)</em>. Moreover, when involved in a novel situation, humans also recall from their memory similar tasks they’ve solved as references when attempting the task <em>Kahneman (2011)</em>. Motivated by these observations, we propose experi-
ence recall to retrieve successful trajectories from the experience pool gathered during training based on task similarity.</p>
<p>Concretely, we used the Faiss vectorstore <em>Johnson et al. (2019)</em> as the experience pool, kNN retriever and all-mpnet-base-v2 <em>Song et al. (2020)</em> embedder to obtain top- $k$ successful trajectories that have the maximum inner-product task similarity with the evaluation task. The advantage of using task similarity as the retrieval rank is that if the agent repeats a task or does a task similar to an existing successful trajectory from the experience pool, the agent only needs to closely imitate the successful trajectory and have less burden on ability extrapolation.</p>
<p>Learning from Successes and Failures To leverage the diverse outcomes gathered during the experience collection phase, we believe the agent should analyze experiences in two distinct ways. First, we let the agent compare a failed trajectory with a successful trajectory for the same task. This comparison offers a concrete understanding of the agent’s shortcomings, highlighting the correct and incorrect actions. Second, we let the agent identify patterns within a set of successful trajectories from different tasks. This approach sheds light on common "good practices" that the agent can adopt to ensure success in evaluation tasks.</p>
<p>For the implementation, we give the agent’s instructionfollowing $\mathrm{LLM}<em _insights="{insights" _text="\text">{\text {insights }}$ several operators to apply on an existing set of insights $i$. We initialize the set of insights to an empty set $i=\emptyset$ and iteratively provide the LLM with fail/success pairs or lists of $L$ successes (created by sampling without replacement) from the experience pool. The operations the LLM can perform are: ADD a new insight, EDIT the content of an existing insight, DOWNVOTE to disagree with an existing insight, or UPVOTE to agree with an existing insight. A newly added insight will have an initial importance count of two associated with it, and the count will increment if subsequent operators UPVOTE or EDIT are applied to it and will decrement when DOWNVOTE is applied to it. If an insight’s importance count reaches zero, it will be removed. This particular design choice robustifies the process since even successful trajectories can be suboptimal and mislead the generated insights. The prompt template we used can be found in Fig. 2. We kept the maximum size for a list of successes to $L$ and used gpt-4-0613 as the default $\mathrm{LLM}</em>$. We empirically found that gpt-4-0613 is better than gpt-3.5-turbo-0613 at following instructions on how to use the insight extraction operators and hallucinated less. Pseudo-code for this process can be found in Alg. 2. Finally, ExpeL utilizes these generated insights $i$ in the task inference phase, described next.}</p>
<h3>4.3 Task Inference</h3>
<p>After the agent gathers experiences, extracts insights from them, and sets up a vectorstore of successful trajectories, it can proceed to the evaluation. For each task, the task specifications will be augmented with the concatenation of the full list of extracted insights $i=\operatorname{concat}\left(i_{1}, i_{2}, i_{3}, \ldots\right)$, and the top- $k$ trajectories with the highest task similarity will be retrieved and used as fewshot in-context examples, $F_{\text {similar tasks }}$. Fig. 3 shows an example prompt template structure, and a</p>
<p>| You are an advanced reasoning agent that can add, edit or remove rules from your existing rule set, based on forming new critiques of past task trajectories. You will be given... |
| :--: | :--: |
| two previous task trials in which you ... | successful tasks trials in which you ... |
| [Task description] ... | [Task description]. |
| one successful and one unsuccessful trial. You failed the trial because... |  |
| [Task failure reasons]. |  |
| Here are the two previous trials to compare and critique: | Here are the trials: |
| [Failed/Succeeded Trajectories] | [Succeeded Trajectories] |</p>
<p>Here are the EXISTING RULES:
[Currently existing insights]
By examining...
and contrasting to the the successful trials, successful trial, ...
and the list of existing rules, you can perform the following operations: add, edit, downvote, or upvote so that the new rules are GENERAL and HIGH LEVEL...
critiques of the failed trial... insights of the successful trials...
or proposed way of Thought so they can be used...
to avoid similar failures when as helpful tips to different
encountered with different tasks in the future.
questions in the future.
Have an emphasis on...
critiquing how to... tips that help the agent...
perform better Thought and Action.
Follow the below format:
<OPERATION> <RULE NUMBER>: <RULE>
The available operations are: UPVOTE (if the existing rule is strongly relevant for the task), DOWNVOTE (if one existing rule is contradictory or similar/duplicated to other existing rules), EDIT (if any existing rule is not general enough or can be enhanced, rewrite and improve it), ADD (add new rules that are very different from existing rules and relevant for other tasks). Each needs to CLOSELY follow their corresponding formatting below:</p>
<p>UPVOTE <EXISTING RULE NUMBER>: <EXISTING RULE>
DOWNVOTE <EXISTING RULE NUMBER>: <EXISTING RULE>
EDIT <EXISTING RULE NUMBER>: <NEW MODIFIED RULE>
ADD <NEW RULE NUMBER>: <NEW RULE>
Do not mention the trials in the rules because all the rules should be GENERALLY APPLICABLE. Each rule should be concise and easy to follow. Any operation can be used MULTIPLE times. Do at most 4 operations and each existing rule can only get a maximum of 1 operation. Below are the operations you do to the above list of EXISTING RULES:</p>
<p>Figure 2: Insight Extraction Prompt Template. The prompt template ExpeL agents used for insight extraction. The same template is used both for success/fail pairs (A, in yellow) and $L$-sized successes (B, in green).
pseudo-code for this step can be found in Alg. 3. We believe as the list of extracted insights grows, retrieval could be a feasible solution to manage the context window size.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Task Inference Prompt Template. We illustrate ExpeL's prompt template during evaluation. The areas with a white background are identical to the base ReAct agent's inputs. We differ by (purple areas) having additional extracted insights from past experience, and dynamically retrieved successful in-context examples from past experiences based on task similarity.</p>
<h3>4.4 Transfer Learning</h3>
<p>After demonstrating how learning by using experiences from a training set can benefit an LLM agent in solving an unseen task in the same task distribution, we investigate another interesting setting where knowledge accumulated from a source task distribution could be useful for a target task distribution with minimal target task examples for the ExpeL agent. Like most transfer learning settings, we assume that the source and target tasks exhibit common knowledge. Therefore, experiences accumulated from source tasks can benefit the agent in solving a new set of target tasks.</p>
<p>Similar to pretraining on source task and finetuning on target task in transfer learning literature (Zhuang et al. 2020), we propose to use the extracted insights $i$ from the source task and fewshot examples from the target task to "finetune" the insights so that they are more applicable in the target task. We hypothesize that using target task fewshot examples can better ground the insights into the target task and mitigate hallucinations. An example prompt template to "finetune" extracted insights from a source domain to tailor them</p>
<p>Algorithm 1: ExpeL - Experience Gathering</p>
<h2>Initialize:</h2>
<p>Policy $\mathrm{LLM}<em _reflect="{reflect" _text="\text">{\text {ReAct }}$
Self-reflection model $\mathrm{LLM}</em>$
Collection of tasks $\mathcal{T}}<em _manual="{manual" _text="\text">{\text {train }}$
Fewshot examples $F</em>$
Experience pool $\mathcal{B} \leftarrow F_{\text {manual }}$
Number of training tasks $N$
Maximum retry number $Z$
Maximum step number $H$
Current task index $n \leftarrow 1$
while task $n \leq N$ do
$t_{n} \leftarrow \mathcal{T}}<em 0="0" n_="n,">{\text {train }}[n]$
Reflection $\nu</em> \leftarrow$ ""'
for trial $z=0$ to $Z$ do
$o_{0} \leftarrow \operatorname{env} \cdot \operatorname{reset}\left(t_{n}\right)$
Initialize trajectory $\tau_{n, z} \leftarrow o_{0}$
for timestep $i=0$ to $H$ do
$a_{i} \leftarrow \operatorname{LLM}<em i="i">{\text {ReAct }}\left(a</em>\right)$
$o_{i+1}, r_{i+1}$, done $\leftarrow \operatorname{env} \cdot \operatorname{step}\left(a_{i}\right)$
$\tau_{n, z} \leftarrow \tau_{n, z} \cup\left{\left(o_{i}, a_{i}, o_{i+1}, r_{i+1}\right)\right}$
if done then
break
end if
end for
$\mathcal{B} \leftarrow \mathcal{B} \cup \tau_{n, z}$
if done or $z=Z$ then
$n \leftarrow n+1$
break
else
$\nu_{n, z+1} \leftarrow \operatorname{concat}\left(\nu_{n, z}+\operatorname{LLM}} \mid \tau_{n, z}, F_{\text {manual }}, \nu_{n, z<em n_="n," z="z">{\text {reflect }}\left(\tau</em>\right)\right)$
end if
end for
end while
return $\mathcal{B}$</p>
<p>Algorithm 2: ExpeL - Insight Extraction
Initialize:
Experience pool $\mathcal{B}$ (from Alg. 1)
Insight extraction model $\mathrm{LLM}<em _success="{success" _text="\text">{\text {insights }}$
Set of insights $i \leftarrow \emptyset$
Divide the successes in $\mathcal{B}$ into $L$-sized chunks:
$C</em>\right}\right.$,
$\left.\left{\tau_{L+1}^{\text {success }}, \tau_{L+2}^{\text {success }}, \ldots \tau_{2 L}^{\text {success }}\right}, \ldots\right}$
Construct fail/success tuples of the same tasks in $\mathcal{B}$ :
$C_{\text {compare }}=\left{\left(\tau_{1}^{\text {success }}, \tau_{1,0}^{\text {fail }}\right),\left(\tau_{1}^{\text {success }}, \tau_{1,1}^{\text {fail }}\right), \ldots\right.$,
$\left.\left(\tau_{2}^{\text {success }}, \tau_{2,0}^{\text {fail }}\right), \ldots\right}$
for each $c_{\text {compare }}$ in $C_{\text {compare }}$ do
$i \leftarrow \mathrm{LLM}}}=\left{\left{\tau_{1}^{\text {success }}, \tau_{2}^{\text {success }}, \ldots \tau_{L}^{\text {success }<em _compare="{compare" _text="\text">{\text {insights }}\left(c</em>, i\right)$
end for
for each $c_{\text {success }}$ in $C_{\text {success }}$ do
$i \leftarrow \mathrm{LLM}}<em _success="{success" _text="\text">{\text {insights }}\left(c</em>, i\right)$
end for
return $i$}</p>
<p>Algorithm 3: ExpeL - Evaluation</p>
<h2>Initialize:</h2>
<p>ExpeL agent $\mathrm{LLM}<em _evaluation="{evaluation" _text="\text">{\text {ExpeL }}$
Text Embedder $\mathcal{E}$
Experience pool $\mathcal{B}$ (from Alg. 1)
Set of insights $i$ (from Alg. 2)
Collection of evaluation tasks $\mathcal{T}</em>$
Number of evaluation tasks $M$
Number of fewshots $k$
Number of successes $S \leftarrow 0$
for task $m=1$ to $M$ do
$t_{m} \leftarrow \mathcal{T}}<em 0="0">{\text {evaluation }}[m]$
$o</em>\right)$
Initialize trajectory $\tau_{m} \leftarrow o_{0}$
$F_{\text {similar tasks }} \leftarrow$ Faiss $\left(t_{m}, \mathcal{B}, \mathcal{E}, k\right)$
for timestep $i=1$ to $H$ do
$a_{i} \leftarrow \mathrm{LLM}} \leftarrow \operatorname{env} \cdot \operatorname{reset}\left(t_{m<em i="i">{\text {ExpeL }}\left(a</em>, i\right)$
$o_{i+1}, r_{i+1}$, done $\leftarrow \operatorname{env} \cdot \operatorname{step}\left(a_{i}\right)$
$\tau_{m} \leftarrow \tau_{m} \cup\left{\left(o_{i}, a_{i}, o_{i+1}, r_{i+1}\right)\right}$
if done then
break
end if
end for
if $r_{i+1}=1$ then
$S \leftarrow S+1$
end if
end for
return $\frac{S}{M}$
to a target domain is illustrated in Fig. 4.} \mid \tau_{m}, F_{\text {similar tasks }</p>
<h3>4.5 ExpeL's Strengths</h3>
<p>In this section, we outline the key strengths of our framework. First and foremost, ExpeL offers inherent interpretability, as both the extracted experiences and successful trajectories are presented in natural language. This design allows users to easily inspect, modify, or remove potentially harmful trajectories/insights - a challenge in finetuned models. Moreover, users can seamlessly add expert insights or trajectories to an ExpeL agent. Additionally, our learning approach is highly accessible; it demands less data, reduces computational resources, and is straightforward to implement. Furthermore, self-improvement methods like Reflexion (Shinn et al. 2023) facilitate intra-task improvements, but ExpeL enables inter-task learning. ExpeL does not rely on retries during deployment, which certain domains require. On the flexibility front, the ExpeL agent boasts a significant level of versatility. It is not restricted to specific language models and complements existing strategies aimed at enhancing LLM agent planning capabilities. Moreover, when applied in conjunction with them, ExpeL might even improve the capabilities of finetuned agents. Another strength lies in continuous improvement. Our method stands to benefit from the ongoing enhancements in foundational models. As an illustration, our experiments show that using gpt-4 to extract insights outperforms gpt-3.5-turbo (refer to Sec. 5.6). Lastly, we introduced a method for trans-</p>
<table>
<thead>
<tr>
<th>Knowledge Finetuning</th>
</tr>
</thead>
<tbody>
<tr>
<td>You are a teacher agent that passes on experience to student agents. You came up with the following rules to help you achieve the task of (Source Task) effectively. The number at the end are the importance you gave to each of the rules.</td>
</tr>
<tr>
<td>RULES:</td>
</tr>
<tr>
<td>[Extracted insights from Source Task]</td>
</tr>
<tr>
<td>Now a student agent is trying to solve a similar (Target Task).</td>
</tr>
<tr>
<td>Some examples of this new task are:</td>
</tr>
<tr>
<td>[Fixed fewshot examples of Target Task]</td>
</tr>
<tr>
<td>Give a concise and easy to follow instructional paragraph based on the RULES for the student agent to solve (Target Task). Do not state where each sentence is using whichever rule, and make sure the paragraph is VERY CONCISE and EASY TO FOLLOW!</td>
</tr>
<tr>
<td>Knowledge transfer</td>
</tr>
<tr>
<td>The following paragraph is insights a teacher agent provided to you. It is MANDATORY for you to follow these insights as CLOSELY as possible as they will help you perform the (Target Task) tasks efficiently.</td>
</tr>
<tr>
<td>[Finetuned insights]</td>
</tr>
<tr>
<td>[Target Task desciption + fewshot]</td>
</tr>
<tr>
<td>(Target Task)</td>
</tr>
</tbody>
</table>
<p>Table 5.1: Example of a 5-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Main Results. Average task success rates (std. error in gray arrows) across three different domains: HotpotQA, ALFWorld, and WebShop. ReAct and Act are used as baselines. ExpeL consistently outperforms the baselines on all domains, highlighting the importance of learning from experience. Additionally, we compare ExpeL with ExpeL (retrieve-only) and ExpeL (insights-only) to highlight that both insight extraction and task similarity retrieval are <em>essential</em> and <em>synergistic</em>.</p>
<p>ALFWorld (54% at R3 vs. 59%) <em>without repeated attempts</em>. While Reflexion improves results by iteratively refining insights through repeated task execution (R1, R2, R3...), our ExpeL agent leverages cross-task learning by accumulating task experience. However, it is noteworthy that there remains room for improvement in the context of WebShop tasks, approaching the lower side of Reflexion's success rates.</p>
<h3>5.3 Agent Behavioral Analysis</h3>
<p>In this section, we highlight some observations made by manually inspecting the trajectories of ReAct agents and ExpeL agents, and by pinpointing possible causes of how some unexpected behaviors might have emerged. Please visit the paper's webpage, https://andrewzh112.github.io/expel, for full trajectory demos illustrating the following findings.</p>
<h4>Hypothesis Formulation &amp; Constraints Adaptation</h4>
<p>After extracting the insights from experiences gathered in the training set, we noticed the agent subsequently gained the ability to <em>reassess</em> its whole trajectory in the <em>last steps</em> and conclusively end the task rather than expressing its ineptitude in providing a solution. This ability was particularly observed in HotpotQA (Fig. 16, 17 in Appendix) where a likely influential insight was stating that the agent should "consider the answer might be in the observations already made". Therefore the agent would finish by proposing the most probable answer given its past observations rather than concluding with "Unknown" or "Information not available".</p>
<h4>World Model Belief Update</h4>
<p>We noticed our ExpeL agent updated its beliefs through the insights and over its gained experience. This belief thereby update enables the agent to avoid unnecessary actions and increase efficiency in solving a given task. For example, in ALFWorld, the agent completely changed the priors it had in ReAct on the likely locations of a pan (from drawers/countertops/cabinets to stove-burners). This behavior emerged from the extracted insight claiming that "when searching for an item" it needs to "consider its nature and its typical usage" (Fig. 18 in Appendix), leading the agent to promptly and accurately find the correct item at the first step while the ReAct agent could not find it in time.</p>
<h4>Self-correction</h4>
<p>Although ReAct was sometimes not able to reassess its situation when attempting to solve a task, ExpeL demonstrated its proficiency in identifying and rectifying missteps. Notably, when incorrectly taking an object in ALFWorld, the agent has shown its ability to put it back and resume the task by searching for the proper object (Fig. 19 in Appendix). This highlights ExpeL's capacity to recover from errors and stay on course without hallucinating when completing tasks. This behavior is possibly encouraged by the generated insight "reassess the situation and consider alternative actions" if "an attempt does not progress the task".</p>
<h3>5.4 Transfer Learning</h3>
<p>In this experiment, we use the HotpotQA dataset (Yang et al. 2018) as source tasks and the FEVER dataset (Thorne et al. 2018) as target tasks. Like the HotpotQA dataset, we equip the agent with the ability to navigate on Wikipedia using a Docstore API; therefore, we hypothesize that some of the knowledge obtained from HotpotQA tasks should also be beneficial when transferred to the FEVER tasks. We use gpt-4-0613 for adapting the HotpotQA insights into FEVER insights. We use the same fewshot examples to fine-tune the insights as the ones that will be used during task execution. We compare our ExpeL Transfer agent's transfer learning ability with (1) ReAct; (2) Act; and (3) an agent that "finetunes" insights without task demonstrations. Notice that since source and target tasks are inherently different, we do not have an experience pool to retrieve from; thus, the ExpeL Transfer agents use the existing fixed fewshot examples as in-context examples.</p>
<p>Tab. 1 showcases the transfer learning results. Both agents</p>
<table>
<thead>
<tr>
<th></th>
<th>FEVER (SR %)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Act</td>
<td>$58 \pm 0.0$</td>
</tr>
<tr>
<td>ReAct</td>
<td>$63 \pm 0.4$</td>
</tr>
<tr>
<td>ExpeL Transfer w/o Task Demos</td>
<td>$65 \pm 1.7$</td>
</tr>
<tr>
<td>ExpeL Transfer</td>
<td>$\mathbf{7 0} \pm 0.7$</td>
</tr>
</tbody>
</table>
<p>Table 1: Transfer Results. We transfer insights extracted from HotpotQA to FEVER. Act and ReAct are baseline agents, ExpeL w/o Task Demos does not utilize fewshot examples when altering the insights for the target task.</p>
<table>
<thead>
<tr>
<th></th>
<th>R0</th>
<th>R1</th>
<th>R2</th>
<th>R3</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReAct+Reflexion</td>
<td>$40.3 \%$</td>
<td>$47.8 \%$</td>
<td>$52.2 \%$</td>
<td>$54.4 \%$</td>
</tr>
<tr>
<td>ExpeL retrieve only</td>
<td>$54.5 \%$</td>
<td>$57.5 \%$</td>
<td>$59.7 \%$</td>
<td>$60.4 \%$</td>
</tr>
<tr>
<td>ExpeL+Reflexion</td>
<td>$\mathbf{5 9 . 0 \%}$</td>
<td>$\mathbf{6 0 . 4 \%}$</td>
<td>$\mathbf{6 3 . 4 \%}$</td>
<td>$\mathbf{6 4 . 2 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Success Rate on ALFWorld with Reflexion Rounds. ExpeL and Reflexion appear to be synergistic in the ALFWorld environment (Highlight = ExpeL with one attempt). R1-R3 were obtained from failed R0 checkpoints.
that transferred knowledge from the source domain saw performance gains. Notably, the agent with a few in-context examples had a more significant improvement than the one without, indicating the effectiveness of the proposed "finetuning" method in transfer learning scenarios.</p>
<h3>5.5 ExpeL with Task Reattempts</h3>
<p>While not being the central focus of our study, we present preliminary findings on the effectiveness of incorporating task reattempts into the evaluation phase using ExpeL by resuming the failed checkpoints from R0. The performance of ExpeL combined with Reflexion, alongside two baselines: ReAct/Reflexion and ExpeL without insights (ExpeL retrieve only), is detailed in Table 2. The results demonstrate a notable improvement in the success rate when ExpeL is paired with Reflexion, with the success rate increasing as the number of task reattempts grows.</p>
<h3>5.6 Ablation Studies</h3>
<p>One main component of ExpeL is the agent's ability to autonomously gather valuable experiences benefiting its own learning. Therefore, we wish to investigate if the number of useful experiences impacts the downstream performance of ExpeL. We designed two different agents to compare our agent with. The first one only has access to initial fewshot examples and extracts insights from them. The second gathers experience using ReAct where the agent has no retries. Thus, the agent will not only get less successful trajectories but will also not have any success/failure comparison pairs during insights extraction. We conducted experiments in the HotpotQA environment and presented the results in Fig. 6. As we can see, the agent that extracts insights from the existing fewshots has no advantage compared to the ReAct agent, illustrating that experience is essential for ExpeL to learn from. This was reflected in a significantly better performance for the two other agents having access to more experience. Furthermore, the ExpeL agent with access to a di-
verse set of experiences (failure and success pairs obtained using Reflexion) performs better than the agent using only ReAct during experience gathering.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Effects of experience pool size on performance</p>
<p>Figure 6: Effects of Experience on Performance. We highlight the correlation between the number of diverse experience samples and the final performance. Concretely, we compare ExpeL with (1) ReAct, (2) ExpeL that only has access to fewshot examples, and (3) ExpeL that only uses ReAct during the experience gathering step. It is evident that extra autonomously collected experiences are essential to ExpeL's success and that diversity of success/failure data gathered using Reflexion was superior to using ReAct only.</p>
<p>Next, we will scrutinize the efficacy of the insight extraction step of ExpeL. Since insights had the most significant impact on the HotpotQA environment (Fig. 5), we performed the ablations on insights in this environment. We use three dimensions to ablate the design choices for insight extraction by creating the following variants of ExpeL agents: (1) human-crafted insights (Fig. 12 in Appendix), which were manually engineered by carefully studying the agent's mistakes during the experience gathering step; (2) adding reflections $\nu$ into the insights construction step in addition to using fail/success pairs and lists of successes; (3) using gpt-3.5-turbo-0613 as the LLM ${ }_{\text {insights. }}$. Results in Tab. 3 show several significant findings: (1) learned insights by the agent are more advantageous than handcrafted ones; (2) using reflections in addition to success/ failure pairs and lists of successes is disadvantageous, possibly due to reflections sometimes outputting hallucinations, therefore misleading the insight extraction stage; and (3) a better LLM is more advantageous at improving ExpeL's performance, suggesting our agent will enjoy free performance boosts with the ever-improving nature of base foundation models.</p>
<p>Lastly, we investigated the design choice of using task similarity as the ranking score for retrieving successful incontext examples in ALFWorld. In particular, we use (1) reason similarity by retrieving top- $k$ trajectories with the most similar reasoning step as the latest reasoning step in the current trajectory, and (2) randomly sampling successful trajectories from the experience pool. We clearly observe in Tab. 3 that retrieving with task similarity (ExpeL) performs the best. Reason similarity is still advantageous but slightly drops in performance, possibly due to dynamically changing fewshots during a single trajectory, causing instabilities. Lastly, random sampling has a significant drop in performance, suggesting that our design choice of selecting the most pertinent in-context example is advantageous.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">HotpotQA (SR \%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ReAct</td>
<td style="text-align: center;">$28.0 \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: left;">Hand-crafted insights</td>
<td style="text-align: center;">$32.0 \pm 1.1$</td>
</tr>
<tr>
<td style="text-align: left;">Insights with reflections</td>
<td style="text-align: center;">$29.0 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo insights</td>
<td style="text-align: center;">$32.0 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: left;">ExpeL (ours)</td>
<td style="text-align: center;">$\mathbf{3 9 . 0} \pm 1.7$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">ALFWorld (SR \%)</td>
</tr>
<tr>
<td style="text-align: left;">ReAct</td>
<td style="text-align: center;">$40.0 \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: left;">Reasoning similarity</td>
<td style="text-align: center;">$48.5 \pm 2.1$</td>
</tr>
<tr>
<td style="text-align: left;">Random sampled</td>
<td style="text-align: center;">$42.5 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: left;">ExpeL (ours)</td>
<td style="text-align: center;">$\mathbf{5 9 . 0} \pm 0.3$</td>
</tr>
</tbody>
</table>
<p>Table 3: Ablations Results. Upper: Ablations on insight extraction. Hand-crafted insights enjoyed a performance boost over ReAct but were less effective than LLM-generated ones. Furthermore, adding reflections to the insight-generating process hurt performance. Lastly, better LLM base models give better insights. Lower: Ablations on in-context examples selection strategy. Randomly selected baseline has a significant drop in performance while ranking using reason similarity also has a noticeable dip.</p>
<h2>6 Conclusion and Limitations</h2>
<p>Limitations In this work, we investigated tasks with textual observation, which is limiting in real-world scenarios. Thus, incorporating image observations will make our method more generally applicable. Using Vision-Language Models or captioning models to supplement the LLM to enable image observations could be an interesting new avenue of research. Additionally, we investigated the efficacy of our method by using closed-source API LLMs, which can be off-limits in some applications. Exploring LLM agents using open-source LLMs should be another promising future work (Zeng et al. 2023). Furthermore, since our extracted insights do not exceed the current LLM's token limit, we can fit them into the agent's context window. However, extra retrieval steps for insights might be needed for truly lifelong learning agents to ensure a manageable context window size. Lastly, unlike reinforcement learning methods, prompting techniques lack theoretical underpinnings that could potentially impact the efficiency of the resulting policies. Future research should explore the integration of these approaches to yield more effective and optimal solutions.</p>
<p>In summary, we introduced ExpeL, a novel learning LLM agent that autonomously gathers experience from a set of training tasks to improve its abilities in solving evaluation tasks without access to model parameters. We demonstrated its learning abilities by showing its performance gain compared to vanilla ReAct and Act agents. Furthermore, we investigated a transfer learning scenario where extracting insights from a set of source tasks can benefit the ExpeL agent in solving a target task. Lastly, we presented several unexpected emerged abilities our agent developed at the end of its training. We believe that autonomously learning from experience is essential for developing human-like intelligent agents, and our ExpeL agent is a step toward that goal.</p>
<h2>Acknowledgement</h2>
<p>This work is supported in part by the National Key R\&amp;D Program of China (2022ZD0114900), the National Natural Science Foundation of China under Grants 62022048, U2336214, and 62332019, and the Guoqiang Institute of Tsinghua University.</p>
<h2>References</h2>
<p>Anthropic. 2023. Introducing Claude.
Boiko, D. A.; MacKnight, R.; and Gomes, G. 2023. Emergent Autonomous Scientific Research Capabilities of Large Language Models. arXiv preprint.
Bran, A. M.; Cox, S.; White, A. D.; and Schwaller, P. 2023. ChemCrow: Augmenting Large-Language Models with Chemistry Tools. arXiv preprint.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language Models are Few-Shot Learners. NeurIPS.
Chase, H. 2023. Langchain.
Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; Schuh, P.; Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer, N.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B.; Pope, R.; Bradbury, J.; Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.; Michalewski, H.; Garcia, X.; Misra, V.; Robinson, K.; Fedus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.; Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K.; Eck, D.; Dean, J.; Petrov, S.; and Fiedel, N. 2023. PaLM: Scaling Language Modeling with Pathways. JMLR.
Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fedus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al. 2022. Scaling Instruction-Finetuned Language Models. arXiv preprint.
Du, M.; He, F.; Zou, N.; Tao, D.; and Hu, X. 2022. Shortcut Learning of Large Language Models in Natural Language Understanding: A Survey. arXiv preprint.
Gong, R.; Huang, Q.; Ma, X.; Vo, H.; Durante, Z.; Noda, Y.; Zheng, Z.; Zhu, S.-C.; Terzopoulos, D.; Fei-Fei, L.; et al. 2023. MindAgent: Emergent Gaming Interaction. arXiv preprint.
Gur, I.; Furuta, H.; Huang, A.; Safdari, M.; Matsuo, Y.; Eck, D.; and Faust, A. 2023. A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. arXiv preprint.
Ha, H.; Florence, P.; and Song, S. 2023. Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition. In CoRL. PMLR.
Hao, S.; Gu, Y.; Ma, H.; Hong, J. J.; Wang, Z.; Wang, D. Z.; and Hu, Z. 2023. Reasoning with Language Model is Planning with World Model. arXiv preprint.</p>
<p>Huang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022. Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. In ICML. PMLR.
Humphreys, P.; Guez, A.; Tieleman, O.; Sifre, L.; Weber, T.; and Lillicrap, T. 2022. Large-scale Retrieval for Reinforcement Learning. NeurIPS.
Johnson, J.; Douze, M.; and Jégou, H. 2019. Billion-scale Similarity Search with GPUs. IEEE Transactions on Big Data.
Kahneman, D. 2011. Thinking, Fast and Slow. Farrar, Straus and Giroux.
Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa, Y. 2022. Large Language Models are Zero-Shot Reasoners. NeurIPS.
Li, H.; Su, Y.; Cai, D.; Wang, Y.; and Liu, L. 2022. A Survey on Retrieval-Augmented Text Generation. arXiv preprint.
Lin, B. Y.; Fu, Y.; Yang, K.; Ammanabrolu, P.; Brahman, F.; Huang, S.; Bhagavatula, C.; Choi, Y.; and Ren, X. 2023a. SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks. NeurIPS.
Lin, K.; Agia, C.; Migimatsu, T.; Pavone, M.; and Bohg, J. 2023b. Text2Motion: From Natural Language Instructions to Feasible Plans. Autonomous Robots.
Lin, L.-J. 1992. Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching. Machine learning.
Liu, J.; Shen, D.; Zhang, Y.; Dolan, B.; Carin, L.; and Chen, W. 2022. What Makes Good In-Context Examples for GPT3? In DeeLIO. Association for Computational Linguistics.
Liu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig, G. 2023a. Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. ACM Computing Surveys.
Liu, X.; Yu, H.; Zhang, H.; Xu, Y.; Lei, X.; Lai, H.; Gu, Y.; Ding, H.; Men, K.; Yang, K.; et al. 2023b. AgentBench: Evaluating LLMs as Agents. arXiv preprint.
Liu, Z.; Bahety, A.; and Song, S. 2023. REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. In CoRL. PMLR.
Maas; Carey; Wheeler; Saatchi; Billington; and Shamash. 2023. To Infinity and Beyond: SHOW-1 and Showrunner Agents in Multi-Agent Simulations. arXiv preprint.
Mirchandani, S.; Xia, F.; Florence, P.; Ichter, B.; Driess, D.; Arenas, M. G.; Rao, K.; Sadigh, D.; and Zeng, A. 2023. Large Language Models as General Pattern Machines. In CoRL. PMLR.
Mu, Y.; Zhang, Q.; Hu, M.; Wang, W.; Ding, M.; Jin, J.; Wang, B.; Dai, J.; Qiao, Y.; and Luo, P. 2023. EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. NeurIPS.
Nakajima, Y. 2023. BabyAGI. https://github.com/ yoheinakajima/babyagi.
Nakano, R.; Hilton, J.; Balaji, S. A.; Wu, J.; Ouyang, L.; Kim, C.; Hesse, C.; Jain, S.; Kosaraju, V.; Saunders, W.; Jiang, X.; Cobbe, K.; Eloundou, T.; Krueger, G.; Button, K.;</p>
<p>Knight, M.; Chess, B.; and Schulman, J. 2021. WebGPT: Browser-Assisted Question-Answering with Human Feedback. arXiv preprint.
OpenAI. 2023. GPT-4 Technical Report.
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and Lowe, R. 2022. Training Language Models to Follow Instructions with Human Feedback. In NeurIPS.
Park, J. S.; O’Brien, J.; Cai, C. J.; Morris, M. R.; Liang, P.; and Bernstein, M. S. 2023. Generative Agents: Interactive Simulacra of Human Behavior. In ACM Symposium on User Interface Software and Technology.
Petroni, F.; Rocktäschel, T.; Riedel, S.; Lewis, P.; Bakhtin, A.; Wu, Y.; and Miller, A. 2019. Language Models as Knowledge Bases? In EMNLP-IJCNLP. Association for Computational Linguistics.
Qian, C.; Cong, X.; Yang, C.; Chen, W.; Su, Y.; Xu, J.; Liu, Z.; and Sun, M. 2023. Communicative Agents for Software Development. arXiv:2307.07924.
Rubin, O.; Herzig, J.; and Berant, J. 2022. Learning To Retrieve Prompts for In-Context Learning. In NAACL. Association for Computational Linguistics.
Schaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2015. Prioritized Experience Replay. In ICLR.
Shaw, P.; Joshi, M.; Cohan, J.; Berant, J.; Pasupat, P.; Hu, H.; Khandelwal, U.; Lee, K.; and Toutanova, K. 2023. From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces. NeurIPS.
Shinn, N.; Cassano, F.; Gopinath, A.; Narasimhan, K. R.; and Yao, S. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning. In NeurIPS.
Shridhar, M.; Yuan, X.; Côté, M.-A.; Bisk, Y.; Trischler, A.; and Hausknecht, M. 2021. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In ICLR.
Significant-Gravitas. 2023. AutoGPT. https://github.com/ Significant-Gravitas/Auto-GPT.
Song, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T.-Y. 2020. MPNet: Masked and Permuted Pre-training for Language Understanding. NeurIPS.
Sumers, T. R.; Yao, S.; Narasimhan, K.; and Griffiths, T. L. 2023. Cognitive Architectures for Language Agents. arXiv preprint.
Sun, H.; Zhuang, Y.; Kong, L.; Dai, B.; and Zhang, C. 2023. AdaPlanner: Adaptive Planning from Feedback with Language Models. NeurIPS.
Sutton, R. S.; and Barto, A. G. 2018. Reinforcement Learning: An Introduction. MIT press.
Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford Alpaca: An Instruction-Following LLaMA Model. https: //github.com/tatsu-lab/stanford_alpaca.
Thoppilan, R.; De Freitas, D.; Hall, J.; Shazeer, N.; Kulshreshtha, A.; Cheng, H.-T.; Jin, A.; Bos, T.; Baker, L.; Du,</p>
<p>Y.; et al. 2022. LaMDA: Language Models for Dialog Applications. arXiv preprint.
Thorne, J.; Vlachos, A.; Christodoulopoulos, C.; and Mittal, A. 2018. FEVER: a Large-scale Dataset for Fact Extraction and VERification. In NAACL.
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023a. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023b. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint.
Tworkowski, S.; Staniszewski, K.; Pacek, M.; Wu, Y.; Michalewski, H.; and Miłoś, P. 2023. Focused Transformer: Contrastive Training for Context Scaling. In NeurIPS.
Wang, G.; Xie, Y.; Jiang, Y.; Mandlekar, A.; Xiao, C.; Zhu, Y.; Fan, L.; and Anandkumar, A. 2023a. Voyager: An Openended Embodied Agent with Large Language Models. arXiv preprint.
Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang, J.; Chen, Z.; Tang, J.; Chen, X.; Lin, Y.; et al. 2023b. A Survey on Large Language Model Based Autonomous Agents. arXiv preprint.
Wang, L.; Yang, N.; and Wei, F. 2023. Learning to Retrieve In-Context Examples for Large Language Models. arXiv preprint.
Wang, S.; Liu, C.; Zheng, Z.; Qi, S.; Chen, S.; Yang, Q.; Zhao, A.; Wang, C.; Song, S.; and Huang, G. 2023c. Avalon’s Game of Thoughts: Battle Against Deception through Recursive Contemplation. arXiv preprint.
Watkins, C. J.; and Dayan, P. 1992. Q-learning. Machine learning.
Wei, J.; Bosma, M.; Zhao, V.; Guu, K.; Yu, A. W.; Lester, B.; Du, N.; Dai, A. M.; and Le, Q. V. 2022a. Finetuned Language Models are Zero-Shot Learners. In ICLR.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022b. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS.
Wu, J.; Antonova, R.; Kan, A.; Lepert, M.; Zeng, A.; Song, S.; Bohg, J.; Rusinkiewicz, S.; and Funkhouser, T. 2023. TidyBot: Personalized Robot Assistance with Large Language Models. Autonomous Robots.
Xi, Z.; Chen, W.; Guo, X.; He, W.; Ding, Y.; Hong, B.; Zhang, M.; Wang, J.; Jin, S.; Zhou, E.; et al. 2023. The Rise and Potential of Large Language Model Based Agents: A Survey. arXiv preprint.
Yang, S.; Nachum, O.; Du, Y.; Wei, J.; Abbeel, P.; and Schuurmans, D. 2023a. Foundation Models for Decision Making: Problems, Methods, and Opportunities. arXiv preprint.
Yang, Z.; Li, L.; Wang, J.; Lin, K.; Azarnasab, E.; Ahmed, F.; Liu, Z.; Liu, C.; Zeng, M.; and Wang, L. 2023b. MMREACT: Prompting ChatGPT for Multimodal Reasoning and Action. arXiv preprint.</p>
<p>Yang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W.; Salakhutdinov, R.; and Manning, C. D. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In EMNLP. Association for Computational Linguistics.
Yao, S.; Chen, H.; Yang, J.; and Narasimhan, K. 2022. WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. In NeurIPS.
Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao, Y.; and Narasimhan, K. 2023a. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. NeurIPS.
Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y. 2023b. ReAct: Synergizing Reasoning and Acting in Language Models. In ICLR.
Yao, W.; Heinecke, S.; Niebles, J. C.; Liu, Z.; Feng, Y.; Xue, L.; Murthy, R.; Chen, Z.; Zhang, J.; Arpit, D.; Xu, R.; Mui, P.; Wang, H.; Xiong, C.; and Savarese, S. 2023c. Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization.
Yue, Y.; Kang, B.; Ma, X.; Huang, G.; Song, S.; and Yan, S. 2023. Offline Prioritized Experience Replay. arXiv preprint. Zeng, A.; Liu, M.; Lu, R.; Wang, B.; Liu, X.; Dong, Y.; and Tang, J. 2023. AgentTuning: Enabling Generalized Agent Abilities for LLMs. arXiv preprint.
Zhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2023. Automatic Chain of Thought Prompting in Large Language Models. In ICLR.
Zhao, A.; Zhu, E.; Lu, R.; Lin, M.; Liu, Y.-J.; and Huang, G. 2023a. Augmenting Unsupervised Reinforcement Learning with Self-Reference. arXiv preprint.
Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023b. A Survey of Large Language Models. arXiv preprint.
Zhuang, F.; Qi, Z.; Duan, K.; Xi, D.; Zhu, Y.; Zhu, H.; Xiong, H.; and He, Q. 2020. A Comprehensive Survey on Transfer Learning. Proceedings of the IEEE.
Zitkovich, B.; Yu, T.; Xu, S.; Xu, P.; Xiao, T.; Xia, F.; Wu, J.; Wohlhart, P.; Welker, S.; Wahid, A.; et al. 2023. RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. In CoRL. PMLR.</p>
<p>Appendix</p>
<h1>A Detailed Related Works</h1>
<h2>A. 1 Prompt-based Learning</h2>
<p>Prompt-based learning is a paradigm where the language model that originally outputs the label $\mathbf{y}$ from context $\mathbf{c}$ improves on the label prediction task with a modified context $\hat{\mathbf{c}}$ (Liu et al. 2023a). This framework is compelling as it enables the usage of pre-trained LLMs trained on vast text volumes. Furthermore, a new prompting function supports fewshot or zero-shot learning, thereby adapting swiftly to tasks with minimal or no labeled data. Specifically, tuning-free prompting directly produces answers using a pre-trained language model's prompt without altering its parameters. This method can be enhanced with answered prompts, a strategy termed in-context learning (Brown et al. 2020). Examples include LAMA (Petroni et al. 2019), GPT-3 (Brown et al. 2020) and CoT (Wei et al. 2022b). Its benefits include efficiency, no parameter updates, avoidance of catastrophic forgetting, and zero/fewshot setting applicability. However, it demands intricate prompt engineering and domain knowledge expertise to increase accuracy. Works like AutoPrompt and Zero-shot-CoT (Kojima et al. 2022; Zhang et al. 2023) alleviate the burden on the engineer by automatically generating reasoning chains for NLP reasoning tasks. Likewise, ExpeL agent automatically gathers experiences in sequential decision-making tasks, generates its own insights, and uses these insights alongside successful in-context examples to inform its decisions, taking the burdens away from heavy manual prompt engineering and the requirement of expert domain knowledge.</p>
<h2>A. 2 Retrieval Augmented Generation</h2>
<p>Retrieval augmented generation has gained popularity, which is helpful to reduce hallucination and give LLMs access to internal databases (Li et al. 2022). Several works in the field of NLP demonstrated the efficacy of retrieving in-context examples (Wang, Yang, and Wei 2023; Rubin, Herzig, and Berant 2022) from a database of gold demonstrations. On the contrary, our work explores LLM agents retrieving from their own generated experiences, which lessens the burden of the user's engineering efforts and domain expertise.</p>
<h2>A. 3 LLM Agents</h2>
<p>Research involving using LLMs as the "brain" of an agent has surged in recent years. LLM agents have been instantiated in many areas such as robotics (Ha, Florence, and Song 2023; Zitkovich et al. 2023; Mu et al. 2023; Mirchandani et al. 2023; Wu et al. 2023), natural sciences (Bran et al. 2023; Boiko, MacKnight, and Gomes 2023) and automated workflows (Yang et al. 2023b; Gur et al. 2023). Most of these works leverage LLMs' strong common sense knowledge to achieve downstream tasks in a zero or fewshot manner to keep the LLM's strong world knowledge priors. Our ExpeL agent also leverages the powerful world knowledge of LLMs. Concretely, we use LLMs during gathering experience, extracting insights, and downstream execution steps.</p>
<p>Planning LLMs have demonstrated the ability to plan in embodied environments in a zero-shot manner (Huang et al. 2022). However, many works show that LLMs' planning ability can be further enhanced by improving their reasoning capabilities (Yao et al. 2023a; Wei et al. 2022b). The ReAct agent (Yao et al. 2023b) demonstrates a combination of reasoning and acting. This approach has not only been proven to be superior to agents that only output actions in various scenarios, but also provides insight into what the agent is thinking while acting. Because of its simplicity and effectiveness, we used ReAct as our base planning algorithm.</p>
<p>Self-improvement A class of methods that leverages LLMs' ability to self-reflect based on feedback from the environment has shown their superiority compared to algorithms that do not have an awareness of doing the task a second time (Shinn et al. 2023; Liu, Bahety, and Song 2023). In particular, the Reflexion agent (Shinn et al. 2023) provides a verbal hypothesis on why a task failed based on the failed trajectory/environment feedback and improved if given a second chance. However, self-reflecting methods assume the tasks are repeatable, and environment feedback is available at test time. Furthermore, selfreflection methods are stateless and cannot learn cross-task insights. Instead, our approach leverages the strengths of Reflexion and uses it to gather more failed/successful trajectories to extract insights from them and perform better at test time. Works like Voyager (Wang et al. 2023a) explored skill learning in specific environments like Minecraft.</p>
<p>Memory Mechanisms Agents with persistent long-term memory have demonstrated exciting results in multi-agent settings (Park et al. 2023; Maas et al. 2023; Qian et al. 2023). These works usually have multiple instantiations of generative agents that interact with each other and simulate human societies or fictional settings. In generative agents (Park et al. 2023), agents have a memory mechanism where they can retrieve information based on recency, relevance, and importance, much like how humans sometimes refer to and associate with different memories during their day. These lines of work usually are open-ended, while ExpeL agents are task-solving. Like generative agents, our work also uses memory: successful in-context examples and extracted insights as condensed memory which were both gathered from the agent's own experience.</p>
<h1>A. 4 Reinforcement Learning</h1>
<p>Our agent gathers experience autonomously, reminiscent of online reinforcement learning methods Sutton and Barto 2018. Especially, our method uses off-policy learning Watkins and Dayan 1992, where the policy uses Reflexion during experience gathering and performs policy improvement via insight extraction and retrieval of similar tasks as in-context examples. Specifically, the retrieval step is similar to experience replay (Lin 1992), where research has been done to select which examples to give the agent for training Schaul et al. 2015; Yue et al. 2023. However, unlike these existing methods, ExpeL doesn't require access to model parameters, the design of complicated reward or loss functions, or a large number of environment interactions.</p>
<h2>B Broader Impacts</h2>
<p>Our research focuses on LLM agents. If these autonomous programs are given internet access, there's a risk they might cause unexpected harm. However, techniques such as RLHF could potentially mitigate these adverse effects Nakano et al. 2021; Ouyang et al. 2022.</p>
<h2>C Computational Resources</h2>
<p>All experiments were performed on a desktop: Intel(R) Core(TM) i9-9900K CPU @ 3.60 GHz with 16 cores, 64GB RAM, and a single NVIDIA GeForce RTX 2080 Ti.</p>
<h2>D Environment Details</h2>
<h2>D. 1 Evaluation Task Set</h2>
<p>We employ four-fold validation for all experiments. We train on one half of the dataset and evaluate on the other half, and vice versa. All results include the mean and standard error of the results across the folds. For HotpotQA, we assess performance using 100 validation tasks from the distractor dev split of the HotPotQA dataset Yang et al. 2018, which were also used by ReAct and Reflexion. In the case of ALFWorld Shridhar et al. 2021, we utilized the 134 solvable tasks that ReAct and Reflexion used. Similarly, for WebShop tasks, we evaluated using the same 100 tasks used by ReAct and Reflexion.</p>
<h2>D. 2 Prompts/Fewshot Examples</h2>
<p>We used the same fewshot examples/prompts from ReAct and Reflexion Yao et al. 2023b; Shinn et al. 2023 during appropriate stages. For WebShop, we added one additional fewshot to make the environment have two fewshot examples. We show our prompt templates in Appendix F and will make the code publicly available.</p>
<h2>D. 3 WebShop Environment Specific Detail</h2>
<p>We slightly modified WebShop environment found at https://github.com/princeton-nlp/WebShop. Our goal was to ensure each experiment instantiation was deterministic. In the original version, item prices and price constraints in instructions were generated by sampling from a uniform range. Instead, we used the average value. While this should produce a result similar to the original implementation on average, it ensures consistency across different instantiations for easier reproducibility. Lastly, we extend the items per page from 3 to 10 since recent LLMs saw an increase in context window size that can accommodate more observations.</p>
<h2>D. 4 WebShop Reward Function</h2>
<p>Another metric introduced in the WebShop Yao et al. 2022 is their reward function, which converts the similarity between expected product attributes and the attributes of the purchased product into a value ranging from 0 to 1 :</p>
<p>$$
r=\frac{\left|U_{\text {att }} \cap Y_{\text {att }}\right|+\left|U_{\text {opt }} \cap Y_{\text {opt }}\right|+\mathbb{I}\left[y_{\text {price }} \leq u_{\text {price }}\right]}{\left|U_{\text {att }}\right|+\left|U_{\text {opt }}\right|+1} \cdot r_{\text {type }}
$$</p>
<p>where,</p>
<p>$$
r_{\text {type }}=\left{\begin{array}{ll}
0, &amp; \text { if } \text { TextMatch }=0 \
0.1, &amp; \text { if } \text { TextMatch }&lt;0.1 \
0.5, &amp; \text { if } \text { TextMatch } \leq 0.2 \text { and query not match and category not match } \
1, &amp; \text { otherwise. }
\end{array}\right.
$$</p>
<p>Since a single query could yield multiple appropriate items, WebShop utilizes a matching reward for assessment. The term "TextMatch" denotes the textual overlap of pronouns, nouns, and proper nouns between the selected product's title and the target product's title Liu et al. 2023b).</p>
<table>
<thead>
<tr>
<th>Retrieval Parameters</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Vectorstore</td>
<td>Faiss</td>
</tr>
<tr>
<td>Retriever type</td>
<td>kNN</td>
</tr>
<tr>
<td>Embedder</td>
<td>all-mpnet-base-v2</td>
</tr>
<tr>
<td>Agent Hyperparameters</td>
<td></td>
</tr>
<tr>
<td>Max Reflection Retries</td>
<td>3</td>
</tr>
<tr>
<td>Reflection LLM</td>
<td>gpt-3.5-turbo-0613</td>
</tr>
<tr>
<td>Policy LLM</td>
<td>gpt-3.5-turbo-0613</td>
</tr>
<tr>
<td>Insight Extraction LLM</td>
<td>gpt-4-0613</td>
</tr>
<tr>
<td>Decoding Temperature</td>
<td>0</td>
</tr>
<tr>
<td>Decoding Strategy</td>
<td>greedy</td>
</tr>
<tr>
<td>HotpotQA-specific Parameters</td>
<td></td>
</tr>
<tr>
<td>Number of Success Examples in Insight Extraction $L$</td>
<td>8</td>
</tr>
<tr>
<td>Max Number of Environment Steps $H$</td>
<td>7</td>
</tr>
<tr>
<td>Max Number of Fewshot Examples $k$</td>
<td>6</td>
</tr>
<tr>
<td>Max Number of Reflection Fewshot Examples $k_{\text {reflections }}$</td>
<td>2</td>
</tr>
<tr>
<td>WebShop-specific Parameters</td>
<td></td>
</tr>
<tr>
<td>Number of Success Examples in Insight Extraction $L$</td>
<td>4</td>
</tr>
<tr>
<td>Max Number of Environment Steps $H$</td>
<td>15</td>
</tr>
<tr>
<td>Max Number of Fewshot Examples $k$</td>
<td>2</td>
</tr>
<tr>
<td>Max Number of Reflection Fewshot Examples $k_{\text {reflections }}$</td>
<td>2</td>
</tr>
<tr>
<td>Searched items per page</td>
<td>10</td>
</tr>
<tr>
<td>ALFWorld-specific Parameters</td>
<td></td>
</tr>
<tr>
<td>Number of Success Examples in Insight Extraction $L$</td>
<td>8</td>
</tr>
<tr>
<td>Max Number of Environment Steps $H$</td>
<td>20</td>
</tr>
<tr>
<td>Max Number of Fewshot Examples $k$</td>
<td>2</td>
</tr>
<tr>
<td>Max Number of Reflection Fewshot Examples $k_{\text {reflections }}$</td>
<td>2</td>
</tr>
<tr>
<td>Searched items per page</td>
<td>10</td>
</tr>
<tr>
<td>ALFWorld-specific Parameters</td>
<td></td>
</tr>
<tr>
<td>Number of Success Examples in Insight Extraction $L$</td>
<td>8</td>
</tr>
<tr>
<td>Max Number of Environment Steps $H$</td>
<td>20</td>
</tr>
<tr>
<td>Max Number of Fewshot Examples $k$</td>
<td>2</td>
</tr>
<tr>
<td>Max Number of Reflection Fewshot Examples $k_{\text {reflections }}$</td>
<td>2</td>
</tr>
<tr>
<td>FEVER-specific Parameters</td>
<td></td>
</tr>
<tr>
<td>Max Number of Environment Steps $H$</td>
<td>7</td>
</tr>
<tr>
<td>Max Number of Fewshot Examples $k$</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Table 4: Environment, Retrieval and Agent Parameters.</p>
<h2>Appendix F Prompt Templates</h2>
<h3>F.1 Policy/Actor Prompt Templates</h3>
<p>Policy/actor prompt templates were taken from ReAct ( [yao2023react]) (https://github.com/ysymyth/ReAct) with minimal alterations to fit extracted insights for our ExpeL agents.</p>
<p>You are QA system. Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types:
(1) Search|entity|, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.
(2) Lookup|keyword|, which returns the next sentence containing keyword in the last passage successfully found by Search.
(3) Finish|answer|, which returns the answer and finishes the task.</p>
<p>The following are some experience you gather on a similar task of question answering using Wikipedia API. Use these as references to help you perform this task:
[Extracted insights]
You may take maximum of 7 steps.
Here are some examples:
[Retrieved successful trajectories]
(END OF EXAMPLES)
Now it's your turn!
Question: ...
Action H: Finish[...]
Observation H: Answer is ...</p>
<p>Figure 7: ExpeL HotpotQA Acting Template.</p>
<h1>ALFWorld</h1>
<p>You are Alfred. Follow the syntax of the examples closely when taking actions.
The following are some experience you gather on a similar task of completing a household task by interacting in a household environment. Use these as references to help you perform this task:
[Extracted insights]
You may take maximum of 20 steps.
Here are some examples:
[Retrieved successful trajectories]
(END OF EXAMPLES)
Now it's your turn!
You are in the middle of a room. Looking quickly around you, you see a ...
Your task is to: ...
$&gt;$
Task is ...</p>
<p>Figure 8: ExpeL ALFWorld Acting Template.</p>
<p>You are Webshop. Follow the syntax of the examples closely when taking actions.
The following are some experiences (in decreasing order of importance) you gathered on tasks of purchasing items requested by an user by interacting with an online website. Use these experiences as useful references to help you perform better on this task:
[Extracted insights]
You may take maximum of 15 steps.
Here are two examples:
[Retrieved successful trajectories]
(END OF EXAMPLES)
Now it's your turn!
Instruction: ...
... :
Action: Click[Buy Now]
Observation: Your score (min 8.8, max 1.8): ...</p>
<p>Figure 9: ExpeL WebShop Acting Template.</p>
<h1>Fever</h1>
<p>You are fact verifier. Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types:
(1) Search|entity|, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.
(2) Lookup|keyword|, which returns the next sentence containing keyword in the last passage successfully found by Search.
(3) Finish|answer|, which returns the answer and finishes the task, answer should be one of REFUTES, SUPPORTS or NOT ENOUGH INFO.</p>
<p>The following paragraph are insights a teacher agent provided to you. It is MANDATORY for you to follow these insights as CLOSELY as possible as they will help you perform the fact verification tasks efficiently:</p>
<h2>[Transfered insights]</h2>
<p>You may take maximum of 7 steps.
Here are some examples:
[Fixed fewshot examples]
(END OF EXAMPLES)
Now it's your turn!
Claim: ...
Action H: Finish[...]
Observation H: Episode finished, reward $=$...</p>
<p>Figure 10: ExpeL FEVER Acting Template.</p>
<h1>F. 2 Transfer Learning Prompt Template</h1>
<h2>G Example Insights</h2>
<p>Below are some example insights extracted by gpt-4-0613 or humans by examining the failed and successful trajectories. Some interesting insights are highlighted in purple (including the emergent ones demonstrated in Sec. 5.3).</p>
<h2>G. 1 HotpotQA insights</h2>
<h2>HotpotQA 7</h2>
<ol>
<li>Always ensure to exhaust all possible search queries related to the task at hand before concluding.</li>
<li>When searching for information, consider using different combinations of keywords, and if necessary, break down complex queries into simpler, more specific ones to increase the chances of finding the required information.</li>
<li>If a search query does not yield the desired results, consider rephrasing, using synonyms, or altering the specificity of the query for a more effective search.</li>
<li>When faced with a complex task, consider breaking it down into smaller, more manageable parts and tackle each part individually. This can help in better understanding the task and in formulating more effective search queries.</li>
<li>Always verify the information obtained from the search results to ensure it directly answers the task question before concluding.</li>
<li>When dealing with tasks involving historical events or figures, consider searching for timelines, historical records, or biographical information to provide a comprehensive understanding of the context.</li>
<li>When a search query does not yield the expected results, consider revising the search strategy by looking for related information that can lead to the answer.</li>
<li>If the information is not found after several attempts, consider that the answer might be in the observations already made. Review them carefully before making further searches.</li>
<li>If the task involves finding a specific event in history, consider searching for timelines or historical records related to the topic.</li>
<li>When comparing two or more entities based on a certain attribute (like birth dates), ensure to keep track of the attribute for each entity separately to avoid confusion.</li>
<li>Always double-check your final answer against the information obtained to ensure accuracy before concluding the task.</li>
<li>When searching for specific episodes or events, include the name of the series or event in the search query to increase the chances of finding the required information.</li>
<li>When comparing the attributes of multiple entities, ensure to keep a clear record of each entity's attributes to avoid confusion and ensure accurate comparison.</li>
</ol>
<p>Figure 11: An example of Extracted Insights for HotpotQA. One component resulting to the improved performance of ExpeL on HotpotQA can be traced to several pivotal insights extracted from its past experiences. A special emphasis is placed on insights 2 and 4 , which suggest breaking down complex questions into simpler queries, reminiscent of the mechanism of Auto-GPT (Significant-Gravitas 2023). Besides, as mentioned, the emergent abilities arising from insight 8 were discussed in Sec. 5.3 and illustrated in Fig. 16, 17.</p>
<ol>
<li>Do single searches on the same step and not multiple actions on the same line.</li>
<li>There are only Search[<topic>], Lookup[<topic>], Finish[<answer>] actions, do not invent your own actions.</li>
<li>Do the given task until the very end.</li>
<li>When encountering "Could not find" or "Similar[...]", look at the results closely where some might be helpful searches.</li>
<li>Use Search[topic (category)] to narrow down search when the initial search is too broad.</li>
<li>Do not use the same query as previous steps when doing Search.</li>
<li>Do not call for/interact with user or experts.</li>
<li>Do not think you can access other resources, you only have access to Wikipedia API.</li>
<li>Do not finish the task without providing an answer.</li>
<li>When you successfully find a page with search, you can follow up with Lookup[<topic>] to find specific information.</li>
</ol>
<p>Figure 12: Hand-crafted insights for the HotpotQA environment. This figure summarizes insights derived through a manual examination of both successful and unsuccessful Reflexion (Shinn et al. 2023) trajectories. These insights have been carefully crafted to address the most prevalent mistakes. On a related note, we observe that GPT-4 is able to extract a variety of insights (Fig. 11) in common with these hand-crafted ones, as depicted in this figure. For instance, insights 3 and 6 underscore the importance of exhausting all steps before conceding and diversifying search keywords to achieve better results if initial attempts are inconclusive. This illustrates that our proposed method accompanied by powerful gpt-4-0613 LLM, shows traces of human-like abstraction capabilities.</p>
<h1>G. 2 ALFWorld insights</h1>
<h1>ALFWorld</h1>
<ol>
<li>Always confirm the presence of an item before attempting to interact with it.</li>
<li>Before using an appliance or storage, ensure it is ready for use. If not, take necessary actions to prepare it. Once an item is found and prepared (if necessary), it should be placed in the designated location immediately to avoid confusion.</li>
<li>If an item is not found in one location, systematically check the next likely location based on the nature of the item and common household organization, without assuming its presence.</li>
<li>Prioritize checking locations that are most likely to contain the item you are looking for, based on the nature of the item, common household organization, and the task at hand.</li>
<li>If an attempt to interact with an item fails or does not progress the task, reassess the situation and consider alternative actions or locations before repeating the same action.</li>
<li>Limit the number of consecutive thoughts without actions to ensure progress in task completion and avoid reaching the maximum allowed number of steps.</li>
<li>When an item is found, ensure to specify the item's number and location when interacting with it.</li>
<li>Always ensure to check all possible locations before concluding that an item is not present.</li>
<li>Keep track of the locations already checked to avoid unnecessary repetition and to save time.</li>
<li>When a task involves multiple items of the same type, do not assume that all items will be found in different locations. Some may be found in the same location.</li>
<li>When multiple items are involved in a task, ensure to interact with each item in the correct order as per the task requirement.</li>
<li>When a task requires examining an item, ensure to physically interact with the item rather than just thinking about examining it.</li>
<li>If a task requires cooling an item, consider using appliances that are specifically designed for cooling, such as a fridge or a freezer.</li>
<li>When a task requires the use of a specific item, ensure to correctly identify and use the exact item as per the task requirement. Do not confuse it with similar items.</li>
<li>When searching for an item, consider the nature of the item and its typical usage. For example, a pan is more likely to be found on a stoveburner or countertop rather than in a cabinet or drawer.</li>
<li>When a task requires placing an item in a specific location, ensure the location is suitable and has enough space for the item. If the location is not suitable, find an alternative location that fulfills the task requirement.</li>
<li>When a task requires cleaning an item, consider using a sinkbasin or other suitable cleaning tools or appliances.</li>
<li>When a task requires moving an item, ensure to check the destination location's capacity before placing the item to avoid cluttering.</li>
<li>When a task involves multiple items of the same type, ensure to keep track of the number of items already found and interacted with to avoid confusion and ensure task completion.</li>
<li>When a task requires heating or cooling an item, ensure to use the appropriate appliance such as a microwave for heating or a fridge for cooling.</li>
</ol>
<p>Figure 13: An example of Extracted Insights for ALFWorld. We showcase insights extracted by our agent in the ALFWorld Environment. Some particularly interesting insights are highlighted in purple.</p>
<h1>G. 3 WebShop insights</h1>
<h2>WebShop</h2>
<ol>
<li>Before purchasing, verify that the product meets all the specific requirements mentioned in the task instruction.</li>
<li>Always ensure to understand the product attributes (like color, size, etc.) mentioned in the product description, and select them if they are selectable options to ensure the product matches the task instruction.</li>
<li>Use precise search terms to get relevant results, but avoid being overly specific which might limit the search results.</li>
<li>Always think and validate the product details before making a purchase.</li>
<li>Always include the price range in the search query to narrow down the search results to the most relevant products within the specified budget.</li>
<li>Always consider the product's price before purchasing to ensure it falls within the budget specified in the task instruction.</li>
<li>Always double-check the product type before purchasing. Do not rely solely on the product attributes, but also consider the product's main category to ensure it matches the task instruction.</li>
<li>Always include all the task requirements in the search query, including the price range, to get the most relevant results.</li>
<li>Always ensure to check the product's style if it is mentioned in the task instruction. If the style is a selectable option, make sure to select the correct style before purchasing.</li>
<li>Always ensure the product selected is exactly what is requested in the task instruction. If the product has multiple variations, make sure to select the correct one.</li>
<li>Always read the product description and reviews if available, to gain more information about the product and ensure it meets the task requirements.</li>
<li>If the product has multiple pack options (like pack of 2, pack of 3, etc.), ensure to select the correct pack that matches the task instruction before purchasing.</li>
<li>If the product has multiple variations, ensure to select the correct one that matches the task instruction. If the correct variation is not available, consider other products from the search results.</li>
<li>If the first product selection attempt is unsuccessful, do not repeat the same search query. Instead, consider other products from the initial search results or modify the search query to get more relevant results.</li>
<li>If the product has selectable options, ensure to select the correct options that match the task instruction before purchasing.</li>
</ol>
<p>Figure 14: An example of Extracted Insights for WebShop. We showcase insights extracted by our agent in the WebShop Environment. Some particularly interesting insights are highlighted in purple.</p>
<h2>G. 4 FEVER insights</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author.
Copyright (C) 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
${ }^{1}$ Visit https://andrewzh112.github.io/expel for project page, and https://github.com/LeapLabTHU/ExpeL for code.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
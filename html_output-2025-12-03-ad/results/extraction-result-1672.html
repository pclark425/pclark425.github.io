<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1672 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1672</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1672</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-269791461</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.10315v2.pdf" target="_blank">T RANSIC : Sim-to-Real Policy Transfer by Learning from Online Correction</a></p>
                <p><strong>Paper Abstract:</strong> : Learning in simulation and transferring the learned policy to the real world has the potential to enable generalist robots. The key challenge of this approach is to address simulation-to-reality (sim-to-real) gaps. Previous methods often require domain-specific knowledge a priori . We argue that a straightforward way to obtain such knowledge is by asking humans to observe and assist robot policy execution in the real world. The robots can then learn from humans to close various sim-to-real gaps. We propose T RANSIC , a data-driven approach to enable successful sim-to-real transfer based on a human-in-the-loop framework. T RANSIC allows humans to augment simulation policies to overcome various unmodeled sim-to-real gaps holistically through intervention and online correction. Residual policies can be learned from human corrections and integrated with simulation policies for autonomous execution. We show that our approach can achieve successful sim-to-real transfer in complex and contact-rich manipulation tasks such as furniture assembly. Through synergistic integration of policies learned in simulation and from humans, T RANSIC is effective as a holistic approach to addressing various, often coexisting sim-to-real gaps. It displays attractive properties such as scaling with human effort. Videos and code are available at transic-robot.github.io .</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1672.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1672.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TRANSIC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-in-the-loop sim-to-real transfer framework that (1) trains strong base policies in simulation (teacher RL distilled to student joint-position policies), (2) collects human teleoperation interventions on the real robot, and (3) learns a gated residual policy from those online corrections to integrate with the simulation policy at deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Franka Emika manipulator (tabletop single-arm setup)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A single Franka Emika robot arm mounted on a tabletop, instrumented with multi-view RealSense cameras and a joint position controller, used for contact-rich whole-arm manipulation and furniture-assembly skills.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (contact-rich furniture assembly)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Isaac Gym (NVIDIA PhysX backend)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>High-performance GPU-based physics simulator (Isaac Gym) using the NVIDIA PhysX engine to simulate rigid-body dynamics, contacts/collisions, object geometry, and robot kinematics; synthetic point-cloud observations are rendered from object poses for high-throughput RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>High-performance physics-based simulation (PhysX) for rigid contacts and kinematics, but not perfectly matching all real-world parameters (controller, friction, fine joint friction/inertia, sensor noise and photorealism remain imperfect).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics and contacts/collisions, object geometry (via ground-truth point clouds), robot kinematics, end-effector poses, multi-object scenes; supports operational-space controller (OSC) teacher execution and joint-position student execution in sim.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Exact joint friction/complex friction models, nuanced actuator dynamics and controller imperfections, some contact realism nuances, sensor photorealistic rendering (RGB), and all real-world noise sources — these were approximated or not fully modeled; point clouds were synthesized (not rendered photorealistic).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real Franka Emika robot with 4 fixed + 1 wrist RealSense cameras for multi-view point-cloud fusion, 3D-printed support wall on the tabletop, furniture parts from FurnitureBench; human operator teleoperation via SpaceMouse for online corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Short-horizon contact-rich manipulation skills required for furniture assembly: Stabilize (push tabletop into corner), Reach and Grasp (grasp a leg), Insert (insert a leg into tabletop hole), and Screw (screw a leg into tabletop); also demonstrated chaining for longer-horizon tasks (lamp and table assembly).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Teacher policies trained with reinforcement learning (PPO) using an operational-space controller in simulation; action-space distillation to student policies (behavior cloning to joint-position actions); residual policy trained (supervised / likelihood maximization) on human intervention correction data; gating classifier learned to apply residuals.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Real-world task success rate (%) measured over 20 real-robot trials per method/task; also counts of real trajectories and intervention events reported (number of human corrections).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Reported real-world success rates for TRANSIC: Stabilize 100%, Reach & Grasp 95%, Insert 45%, Screw 85% (average reported ~81% across these four tasks). Additional results: in robustness tests across five deliberately created sim-to-real gap types TRANSIC averaged 77% success.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Simulation/design-time randomizations and augmentations used during training and distillation: randomized object initial poses and orientations, randomized robot pose/joint noise during teacher training, randomized curricula over object geometry and randomization strength, point-cloud augmentation (random translation, jitter), and proprioceptive noise; note: domain randomization compared as a baseline as well.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Perception gap (synthetic vs measured point clouds), embodiment mismatch (e.g., different gripper finger length), controller inaccuracies (OSC vs real joint-position controller mismatches; underactuated controller emulation), dynamics realism (surface friction differences), and object asset mismatch (different object models/assets).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Key enablers: (1) point-cloud input and encoder regularization with paired sim/real point clouds to reduce perception gap; (2) action-space distillation from OSC teacher to joint-position student to reduce controller mismatch; (3) human-in-the-loop online correction to collect targeted real-world failure modes; (4) learning a gated residual policy (separate residual + gating classifier) so the base sim policy is retained and residuals applied only when needed; (5) data augmentation and curricula during sim training; (6) modest amounts of real correction data (dozens of corrections) sufficed.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Qualitative requirements discussed: accurate modeling of contact dynamics, controller fidelity and embodiment parameters materially affect transfer; specifically, operational-space controllers are sensitive to accurate robot inertial/friction parameters and are hard to transfer, whereas joint-position controllers are easier to deploy — making action-space choice a critical fidelity consideration. No strict quantitative fidelity thresholds were provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Not fine-tuning the base policy directly; instead collected human-in-the-loop correction data and trained a residual policy and gating classifier. Real data sizes reported per task: Stabilize 20 trajectories (62 corrections), Reach & Grasp 100 trajectories (434 corrections), Insert 90 trajectories (489 corrections), Screw 17 trajectories (58 corrections). Residual policies trained supervisedly on pre/post intervention state differences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Controlled experiments where five different simulation-reality gap types were intentionally created showed TRANSIC averaged 77% success across these gap types, while a strong interactive IL baseline (IWR) averaged ~18%; TRANSIC improved substantially when trained with data specifically collected addressing a particular gap.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TRANSIC demonstrates that combining high-throughput simulation training (teacher RL + distillation to joint-position student) with small amounts of targeted human correction data and a learned gated residual policy yields significantly better sim-to-real transfer for contact-rich manipulation than alternatives (direct transfer, domain randomization alone, fine-tuning baselines, or interactive IL that overwrites the base policy). The method is data-efficient (dozens of corrections), addresses multiple coexisting sim-to-real gaps holistically, scales with human effort, is robust to noisy/suboptimal corrections and reduced camera inputs, and avoids catastrophic forgetting by keeping the simulation base policy intact and learning residuals gated to be applied only when necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'T RANSIC : Sim-to-Real Policy Transfer by Learning from Online Correction', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1672.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1672.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action Space Distillation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action Space Distillation (OSC teacher → Joint-Position student)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A teacher-student distillation approach where a teacher policy is trained in simulation using an operational-space controller (OSC) for sample efficiency, and its successful trajectories are relabeled to joint-position actions to train a student policy that directly outputs joint positions that are feasible to execute on the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Franka Emika manipulator (student policy deployment)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Student policy executes joint-position commands under a real joint position controller; designed to match deployable low-level control on the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Isaac Gym (simulation execution of OSC teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics-based simulation where teacher policies operate in task space (OSC) and trajectories are recorded; synthetic point clouds provide observation inputs for student distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>High-performance physics for teacher RL, but operational-space control relies on accurate robot parameter modeling (which is imperfect in sim); distillation reduces sensitivity to mismodeled torque/OSC dynamics by training student to produce joint positions directly.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Teacher simulates task-space dynamics, nullspace torques, end-effector delta poses; student models joint kinematics and is trained on recorded joint trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Simulator does not and cannot perfectly model accurate torque-level effects, precise joint friction, and subtle actuator dynamics — areas where OSC is sensitive; RGB photorealism not modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real robot uses a joint position controller to execute the distilled student joint positions; this reduces controller mismatch during deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Same contact-rich manipulation tasks (Stabilize, Reach & Grasp, Insert, Screw); distillation used to obtain deployable student policy.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Teacher trained with RL (PPO) using OSC in sim; dataset of trajectories D_teacher recorded and relabeled (next-step joint positions), then student trained via behavior cloning with point-cloud inputs and data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Empirical contribution to final transfer success (ablation/justification): distillation found crucial to overcome controller sim-to-real gap; specific metric indirect — improved real-world success when using joint-position student vs deploying OSC teacher.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>During distillation: point-cloud augmentations (random translation, jitter with given probabilities), proprioceptive noise, and paired real/sim point-cloud regularization were used to regularize encoder and reduce perception overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Controller mismatch (OSC requires accurately modeled robot inertial, friction, gravity compensation) is a major contributor; using OSC directly in real world without accurate parameters leads to large joint deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Distilling from OSC teacher to joint-position student makes the action space match the real robot's available controller, reducing controller sim-to-real mismatch and making deployment feasible; regularizing point-cloud encoder with paired sim/real examples further helps perception transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper argues that task-space (OSC) controllers require accurate modeling of inertial/gravity/friction to transfer, implying high-fidelity modeling is necessary if OSC is used directly; distillation to joint-position removes that requirement by matching the deployed controller.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Action-space choice materially affects sim-to-real transfer: OSC-based teacher policies are sample-efficient in sim but fragile to mismatch in real-world dynamics and controller parameters, whereas distilling trajectories into joint-position student policies makes deployment more robust and is a crucial step to reduce controller-related sim-to-real gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'T RANSIC : Sim-to-Real Policy Transfer by Learning from Online Correction', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1672.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1672.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gated Residual Policy (Human-in-the-Loop)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned Gated Residual Policy from Online Human Correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to learn residual corrective actions from human teleoperation interventions: humans intervene during base-policy execution, the pre/post-intervention state differences form training targets for a residual policy, and a learned gating classifier determines when to apply residuals at deployment, preserving the base policy while using residuals only when necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Franka Emika manipulator (residual policy augmenting base student policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Residual policy takes the same observation as the base student policy (point cloud, proprioception) and is conditioned on base-policy outputs; outputs deltas in joint positions and a gripper-action toggle; a gating head outputs whether to apply the residual.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>N/A (residual learned from real-world human teleoperation data collected during deployment of the sim-trained base policy)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Not applicable — residual policy is trained on real-world teleoperation correction traces collected during on-robot execution synchronized with the simulated-base deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>N/A (this component addresses the residual real-vs-sim gap rather than being simulated itself).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Residuals implicitly compensate for unmodeled real-world aspects such as controller underactuation, unmodeled friction, embodiment differences, and perception errors by learning corrective deltas from human teleoperation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Residual learning does not attempt to explicitly model the specific physical parameters (e.g., exact friction coefficients); it learns corrective actions as a data-driven residual.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real robot with human operator monitoring and teleoperating via 3Dconnexion SpaceMouse; pre/post intervention robot states recorded to build the correction dataset (D_H).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Same furniture-assembly manipulation skills; residuals used to patch failures of the sim-trained base policy during real execution (error recovery, unsticking, safety-aware corrections, failure prevention).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning / maximum-likelihood training on residual actions (difference between post- and pre-intervention robot states) derived from human teleoperation corrections; gating head trained as a classifier (two-stage training: first encoder + action head, then freeze and train intervention head).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Improvement in real-world success rates compared to baselines and ablations; quantitative comparisons vs baseline interactive IL methods (e.g., TRANSIC improved average success ~1.24× compared to best baselines and outperformed IWR by ~0.75× on average). Also measured dependence on dataset size (scalability).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Using learned gated residual policies, TRANSIC achieved the real-world rates reported above (e.g., average 81% across tasks). Ablations show gating improves performance (e.g., gating vs no gating in Screw: 85% vs 55%). Robustness tests: with noisy correction data average success dropped only ~6% relative.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Residuals compensate for perception error (noisy/partial point clouds), embodiment mismatch (different gripper length), controller inaccuracy/underactuation, dynamics differences (increased surface friction), and object asset mismatch; these are the very gaps where residual corrections were effective.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Effective human-in-the-loop collection (teleoperation traces), modeling residuals as differences (post - pre intervention) to stabilize training, learning a gating classifier to avoid catastrophic forgetting and to manage data imbalance (corrections are relatively rare), and encoder regularization via paired sim/real point clouds to keep visual features consistent.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Not quantitative; demonstrated that if residuals are learned in a gated fashion and applied only when necessary, many fidelity shortcomings (controller mismatches, friction/dynamics differences, perception jitter) can be compensated with modest real data, obviating the need for perfect simulator fidelity in those aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Residual policy trained on collected human corrections: dataset sizes per task reported (e.g., 62 corrections for Stabilize from 20 trajectories, 434 corrections for Reach & Grasp from 100 trajectories, 489 corrections for Insert from 90 trajectories, 58 corrections for Screw from 17 trajectories). Gated residual model trained with Adam, LR 1e-4, two-stage training (encoder+action head, then intervention head).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Residual policy experiments across deliberately induced sim-to-real gap types (perception error, underactuated controller, embodiment mismatch, dynamics difference, object asset mismatch) show TRANSIC substantially outperforms an interactive-IL baseline (IWR) across all gap types (TRANSIC avg 77% vs IWR ~18%). Gating ablations show learned gating nearly matches human gating performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning a residual corrective policy from a small number of targeted human teleoperation interventions and gating its application enables robust sim-to-real transfer by (1) preventing catastrophic forgetting of the sim-trained base policy, (2) compensating a wide variety of sim-to-real gaps in a domain-agnostic way, and (3) scaling favorably with limited human correction data while remaining robust to noisy/suboptimal corrections and partial sensory inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'T RANSIC : Sim-to-Real Policy Transfer by Learning from Online Correction', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Industreal: Transferring contact-rich assembly tasks from simulation to reality <em>(Rating: 2)</em></li>
                <li>Efficient sim-to-real transfer of contact-rich manipulation skills with online admittance residual learning <em>(Rating: 2)</em></li>
                <li>Learning to throw arbitrary objects with residual physics (Tossingbot / residual learning work by Zeng et al.) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1672",
    "paper_id": "paper-269791461",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "TRANSIC",
            "name_full": "TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction",
            "brief_description": "A human-in-the-loop sim-to-real transfer framework that (1) trains strong base policies in simulation (teacher RL distilled to student joint-position policies), (2) collects human teleoperation interventions on the real robot, and (3) learns a gated residual policy from those online corrections to integrate with the simulation policy at deployment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Franka Emika manipulator (tabletop single-arm setup)",
            "agent_system_description": "A single Franka Emika robot arm mounted on a tabletop, instrumented with multi-view RealSense cameras and a joint position controller, used for contact-rich whole-arm manipulation and furniture-assembly skills.",
            "domain": "general robotics manipulation (contact-rich furniture assembly)",
            "virtual_environment_name": "Isaac Gym (NVIDIA PhysX backend)",
            "virtual_environment_description": "High-performance GPU-based physics simulator (Isaac Gym) using the NVIDIA PhysX engine to simulate rigid-body dynamics, contacts/collisions, object geometry, and robot kinematics; synthetic point-cloud observations are rendered from object poses for high-throughput RL training.",
            "simulation_fidelity_level": "High-performance physics-based simulation (PhysX) for rigid contacts and kinematics, but not perfectly matching all real-world parameters (controller, friction, fine joint friction/inertia, sensor noise and photorealism remain imperfect).",
            "fidelity_aspects_modeled": "Rigid-body dynamics and contacts/collisions, object geometry (via ground-truth point clouds), robot kinematics, end-effector poses, multi-object scenes; supports operational-space controller (OSC) teacher execution and joint-position student execution in sim.",
            "fidelity_aspects_simplified": "Exact joint friction/complex friction models, nuanced actuator dynamics and controller imperfections, some contact realism nuances, sensor photorealistic rendering (RGB), and all real-world noise sources — these were approximated or not fully modeled; point clouds were synthesized (not rendered photorealistic).",
            "real_environment_description": "Real Franka Emika robot with 4 fixed + 1 wrist RealSense cameras for multi-view point-cloud fusion, 3D-printed support wall on the tabletop, furniture parts from FurnitureBench; human operator teleoperation via SpaceMouse for online corrections.",
            "task_or_skill_transferred": "Short-horizon contact-rich manipulation skills required for furniture assembly: Stabilize (push tabletop into corner), Reach and Grasp (grasp a leg), Insert (insert a leg into tabletop hole), and Screw (screw a leg into tabletop); also demonstrated chaining for longer-horizon tasks (lamp and table assembly).",
            "training_method": "Teacher policies trained with reinforcement learning (PPO) using an operational-space controller in simulation; action-space distillation to student policies (behavior cloning to joint-position actions); residual policy trained (supervised / likelihood maximization) on human intervention correction data; gating classifier learned to apply residuals.",
            "transfer_success_metric": "Real-world task success rate (%) measured over 20 real-robot trials per method/task; also counts of real trajectories and intervention events reported (number of human corrections).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Reported real-world success rates for TRANSIC: Stabilize 100%, Reach & Grasp 95%, Insert 45%, Screw 85% (average reported ~81% across these four tasks). Additional results: in robustness tests across five deliberately created sim-to-real gap types TRANSIC averaged 77% success.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Simulation/design-time randomizations and augmentations used during training and distillation: randomized object initial poses and orientations, randomized robot pose/joint noise during teacher training, randomized curricula over object geometry and randomization strength, point-cloud augmentation (random translation, jitter), and proprioceptive noise; note: domain randomization compared as a baseline as well.",
            "sim_to_real_gap_factors": "Perception gap (synthetic vs measured point clouds), embodiment mismatch (e.g., different gripper finger length), controller inaccuracies (OSC vs real joint-position controller mismatches; underactuated controller emulation), dynamics realism (surface friction differences), and object asset mismatch (different object models/assets).",
            "transfer_enabling_conditions": "Key enablers: (1) point-cloud input and encoder regularization with paired sim/real point clouds to reduce perception gap; (2) action-space distillation from OSC teacher to joint-position student to reduce controller mismatch; (3) human-in-the-loop online correction to collect targeted real-world failure modes; (4) learning a gated residual policy (separate residual + gating classifier) so the base sim policy is retained and residuals applied only when needed; (5) data augmentation and curricula during sim training; (6) modest amounts of real correction data (dozens of corrections) sufficed.",
            "fidelity_requirements_identified": "Qualitative requirements discussed: accurate modeling of contact dynamics, controller fidelity and embodiment parameters materially affect transfer; specifically, operational-space controllers are sensitive to accurate robot inertial/friction parameters and are hard to transfer, whereas joint-position controllers are easier to deploy — making action-space choice a critical fidelity consideration. No strict quantitative fidelity thresholds were provided.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Not fine-tuning the base policy directly; instead collected human-in-the-loop correction data and trained a residual policy and gating classifier. Real data sizes reported per task: Stabilize 20 trajectories (62 corrections), Reach & Grasp 100 trajectories (434 corrections), Insert 90 trajectories (489 corrections), Screw 17 trajectories (58 corrections). Residual policies trained supervisedly on pre/post intervention state differences.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Controlled experiments where five different simulation-reality gap types were intentionally created showed TRANSIC averaged 77% success across these gap types, while a strong interactive IL baseline (IWR) averaged ~18%; TRANSIC improved substantially when trained with data specifically collected addressing a particular gap.",
            "key_findings": "TRANSIC demonstrates that combining high-throughput simulation training (teacher RL + distillation to joint-position student) with small amounts of targeted human correction data and a learned gated residual policy yields significantly better sim-to-real transfer for contact-rich manipulation than alternatives (direct transfer, domain randomization alone, fine-tuning baselines, or interactive IL that overwrites the base policy). The method is data-efficient (dozens of corrections), addresses multiple coexisting sim-to-real gaps holistically, scales with human effort, is robust to noisy/suboptimal corrections and reduced camera inputs, and avoids catastrophic forgetting by keeping the simulation base policy intact and learning residuals gated to be applied only when necessary.",
            "uuid": "e1672.0",
            "source_info": {
                "paper_title": "T RANSIC : Sim-to-Real Policy Transfer by Learning from Online Correction",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Action Space Distillation",
            "name_full": "Action Space Distillation (OSC teacher → Joint-Position student)",
            "brief_description": "A teacher-student distillation approach where a teacher policy is trained in simulation using an operational-space controller (OSC) for sample efficiency, and its successful trajectories are relabeled to joint-position actions to train a student policy that directly outputs joint positions that are feasible to execute on the real robot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Franka Emika manipulator (student policy deployment)",
            "agent_system_description": "Student policy executes joint-position commands under a real joint position controller; designed to match deployable low-level control on the real robot.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "Isaac Gym (simulation execution of OSC teacher)",
            "virtual_environment_description": "Physics-based simulation where teacher policies operate in task space (OSC) and trajectories are recorded; synthetic point clouds provide observation inputs for student distillation.",
            "simulation_fidelity_level": "High-performance physics for teacher RL, but operational-space control relies on accurate robot parameter modeling (which is imperfect in sim); distillation reduces sensitivity to mismodeled torque/OSC dynamics by training student to produce joint positions directly.",
            "fidelity_aspects_modeled": "Teacher simulates task-space dynamics, nullspace torques, end-effector delta poses; student models joint kinematics and is trained on recorded joint trajectories.",
            "fidelity_aspects_simplified": "Simulator does not and cannot perfectly model accurate torque-level effects, precise joint friction, and subtle actuator dynamics — areas where OSC is sensitive; RGB photorealism not modeled.",
            "real_environment_description": "Real robot uses a joint position controller to execute the distilled student joint positions; this reduces controller mismatch during deployment.",
            "task_or_skill_transferred": "Same contact-rich manipulation tasks (Stabilize, Reach & Grasp, Insert, Screw); distillation used to obtain deployable student policy.",
            "training_method": "Teacher trained with RL (PPO) using OSC in sim; dataset of trajectories D_teacher recorded and relabeled (next-step joint positions), then student trained via behavior cloning with point-cloud inputs and data augmentation.",
            "transfer_success_metric": "Empirical contribution to final transfer success (ablation/justification): distillation found crucial to overcome controller sim-to-real gap; specific metric indirect — improved real-world success when using joint-position student vs deploying OSC teacher.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "During distillation: point-cloud augmentations (random translation, jitter with given probabilities), proprioceptive noise, and paired real/sim point-cloud regularization were used to regularize encoder and reduce perception overfitting.",
            "sim_to_real_gap_factors": "Controller mismatch (OSC requires accurately modeled robot inertial, friction, gravity compensation) is a major contributor; using OSC directly in real world without accurate parameters leads to large joint deviations.",
            "transfer_enabling_conditions": "Distilling from OSC teacher to joint-position student makes the action space match the real robot's available controller, reducing controller sim-to-real mismatch and making deployment feasible; regularizing point-cloud encoder with paired sim/real examples further helps perception transfer.",
            "fidelity_requirements_identified": "Paper argues that task-space (OSC) controllers require accurate modeling of inertial/gravity/friction to transfer, implying high-fidelity modeling is necessary if OSC is used directly; distillation to joint-position removes that requirement by matching the deployed controller.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Action-space choice materially affects sim-to-real transfer: OSC-based teacher policies are sample-efficient in sim but fragile to mismatch in real-world dynamics and controller parameters, whereas distilling trajectories into joint-position student policies makes deployment more robust and is a crucial step to reduce controller-related sim-to-real gaps.",
            "uuid": "e1672.1",
            "source_info": {
                "paper_title": "T RANSIC : Sim-to-Real Policy Transfer by Learning from Online Correction",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Gated Residual Policy (Human-in-the-Loop)",
            "name_full": "Learned Gated Residual Policy from Online Human Correction",
            "brief_description": "A method to learn residual corrective actions from human teleoperation interventions: humans intervene during base-policy execution, the pre/post-intervention state differences form training targets for a residual policy, and a learned gating classifier determines when to apply residuals at deployment, preserving the base policy while using residuals only when necessary.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Franka Emika manipulator (residual policy augmenting base student policy)",
            "agent_system_description": "Residual policy takes the same observation as the base student policy (point cloud, proprioception) and is conditioned on base-policy outputs; outputs deltas in joint positions and a gripper-action toggle; a gating head outputs whether to apply the residual.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "N/A (residual learned from real-world human teleoperation data collected during deployment of the sim-trained base policy)",
            "virtual_environment_description": "Not applicable — residual policy is trained on real-world teleoperation correction traces collected during on-robot execution synchronized with the simulated-base deployment.",
            "simulation_fidelity_level": "N/A (this component addresses the residual real-vs-sim gap rather than being simulated itself).",
            "fidelity_aspects_modeled": "Residuals implicitly compensate for unmodeled real-world aspects such as controller underactuation, unmodeled friction, embodiment differences, and perception errors by learning corrective deltas from human teleoperation.",
            "fidelity_aspects_simplified": "Residual learning does not attempt to explicitly model the specific physical parameters (e.g., exact friction coefficients); it learns corrective actions as a data-driven residual.",
            "real_environment_description": "Real robot with human operator monitoring and teleoperating via 3Dconnexion SpaceMouse; pre/post intervention robot states recorded to build the correction dataset (D_H).",
            "task_or_skill_transferred": "Same furniture-assembly manipulation skills; residuals used to patch failures of the sim-trained base policy during real execution (error recovery, unsticking, safety-aware corrections, failure prevention).",
            "training_method": "Supervised learning / maximum-likelihood training on residual actions (difference between post- and pre-intervention robot states) derived from human teleoperation corrections; gating head trained as a classifier (two-stage training: first encoder + action head, then freeze and train intervention head).",
            "transfer_success_metric": "Improvement in real-world success rates compared to baselines and ablations; quantitative comparisons vs baseline interactive IL methods (e.g., TRANSIC improved average success ~1.24× compared to best baselines and outperformed IWR by ~0.75× on average). Also measured dependence on dataset size (scalability).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Using learned gated residual policies, TRANSIC achieved the real-world rates reported above (e.g., average 81% across tasks). Ablations show gating improves performance (e.g., gating vs no gating in Screw: 85% vs 55%). Robustness tests: with noisy correction data average success dropped only ~6% relative.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Residuals compensate for perception error (noisy/partial point clouds), embodiment mismatch (different gripper length), controller inaccuracy/underactuation, dynamics differences (increased surface friction), and object asset mismatch; these are the very gaps where residual corrections were effective.",
            "transfer_enabling_conditions": "Effective human-in-the-loop collection (teleoperation traces), modeling residuals as differences (post - pre intervention) to stabilize training, learning a gating classifier to avoid catastrophic forgetting and to manage data imbalance (corrections are relatively rare), and encoder regularization via paired sim/real point clouds to keep visual features consistent.",
            "fidelity_requirements_identified": "Not quantitative; demonstrated that if residuals are learned in a gated fashion and applied only when necessary, many fidelity shortcomings (controller mismatches, friction/dynamics differences, perception jitter) can be compensated with modest real data, obviating the need for perfect simulator fidelity in those aspects.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Residual policy trained on collected human corrections: dataset sizes per task reported (e.g., 62 corrections for Stabilize from 20 trajectories, 434 corrections for Reach & Grasp from 100 trajectories, 489 corrections for Insert from 90 trajectories, 58 corrections for Screw from 17 trajectories). Gated residual model trained with Adam, LR 1e-4, two-stage training (encoder+action head, then intervention head).",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Residual policy experiments across deliberately induced sim-to-real gap types (perception error, underactuated controller, embodiment mismatch, dynamics difference, object asset mismatch) show TRANSIC substantially outperforms an interactive-IL baseline (IWR) across all gap types (TRANSIC avg 77% vs IWR ~18%). Gating ablations show learned gating nearly matches human gating performance.",
            "key_findings": "Learning a residual corrective policy from a small number of targeted human teleoperation interventions and gating its application enables robust sim-to-real transfer by (1) preventing catastrophic forgetting of the sim-trained base policy, (2) compensating a wide variety of sim-to-real gaps in a domain-agnostic way, and (3) scaling favorably with limited human correction data while remaining robust to noisy/suboptimal corrections and partial sensory inputs.",
            "uuid": "e1672.2",
            "source_info": {
                "paper_title": "T RANSIC : Sim-to-Real Policy Transfer by Learning from Online Correction",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Industreal: Transferring contact-rich assembly tasks from simulation to reality",
            "rating": 2,
            "sanitized_title": "industreal_transferring_contactrich_assembly_tasks_from_simulation_to_reality"
        },
        {
            "paper_title": "Efficient sim-to-real transfer of contact-rich manipulation skills with online admittance residual learning",
            "rating": 2,
            "sanitized_title": "efficient_simtoreal_transfer_of_contactrich_manipulation_skills_with_online_admittance_residual_learning"
        },
        {
            "paper_title": "Learning to throw arbitrary objects with residual physics (Tossingbot / residual learning work by Zeng et al.)",
            "rating": 1,
            "sanitized_title": "learning_to_throw_arbitrary_objects_with_residual_physics_tossingbot_residual_learning_work_by_zeng_et_al"
        }
    ],
    "cost": 0.0177055,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction</p>
<p>Yunfan Jiang 
Stanford University</p>
<p>Chen Wang 
Stanford University</p>
<p>Ruohan Zhang 
Stanford University</p>
<p>Jiajun Wu 
Stanford University</p>
<p>Li Fei-Fei 
Stanford University</p>
<p>TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction
54DDE48CF91A216B391032B8FCFC2B8ESim-to-Real TransferHuman-in-the-LoopHuman Correction Joint Position Controller Teacher Policy (Trained with RL) Operational Space Controller Student Policy Proprioception Privileged Object Poses Proprioception Synthetic Point Cloud Action Space Distillation (a) Simulation Policy Training through Action Space Distillation
Learning in simulation and transferring the learned policy to the real world has the potential to enable generalist robots.The key challenge of this approach is to address simulation-to-reality (sim-to-real) gaps.Previous methods often require domain-specific knowledge a priori.We argue that a straightforward way to obtain such knowledge is by asking humans to observe and assist robot policy execution in the real world.The robots can then learn from humans to close various sim-to-real gaps.We propose TRANSIC, a data-driven approach to enable successful sim-to-real transfer based on a human-in-the-loop framework.TRANSIC allows humans to augment simulation policies to overcome various unmodeled sim-to-real gaps holistically through intervention and online correction.Residual policies can be learned from human corrections and integrated with simulation policies for autonomous execution.We show that our approach can achieve successful sim-to-real transfer in complex and contact-rich manipulation tasks such as furniture assembly.Through synergistic integration of policies learned in simulation and from humans, TRANSIC is effective as a holistic approach to addressing various, often coexisting sim-to-real gaps.It displays attractive properties such as scaling with human effort.Videos and code are available at transic-robot.github.io.</p>
<p>Here, the robot attempts to first align the light bulb with the base and then insert and screw the light bulb into the base.c) A human operator monitors robot behaviors, intervenes, and provides online correction through teleoperation when necessary.Human data are collected to train a residual policy to tackle various sim-to-real gaps in a holistic manner.d) The simulation and the residual policies are integrated together during test time to achieve a successful sim-to-real transfer for contact-rich tasks, such as screwing a light bulb into the base.</p>
<p>Introduction</p>
<p>Learning in simulation is a potential approach to the realization of generalist robots capable of solving sophisticated decision-making tasks [1,2].Learning to solve these tasks requires a large amount of training data [3][4][5].Providing unlimited training supervision through state-of-the-art simulation [6][7][8][9][10] could alleviate the burden of collecting data in the real world with physical robots [11,12].Therefore, it is crucial to seamlessly transfer and deploy robot control policies acquired in simulation, usually through reinforcement learning (RL), to real-world hardware.Successful demonstrations of this simulation-to-reality (sim-to-real) approach have been shown in dexterous in-hand manipulation [13][14][15][16][17], locomotion [18][19][20][21][22][23][24][25][26][27], and quadrotor flight [28,29].</p>
<p>Nevertheless, replicating similar success in manipulation tasks with robotic arms remains surprisingly challenging, with only a few cases in simple non-prehensile manipulation [30][31][32][33], industry assembly under restricted settings [34][35][36][37][38], and peg swinging [39].The difficulty mainly stems from the unavoidable sim-to-real gaps [40], including but not limited to perception gap [41][42][43], embodiment mismatch [18,44,45], controller inaccuracy [46][47][48], and dynamics realism [49].Traditionally, researchers tackle them through system identification [18,30,50,51], domain randomization [13,[52][53][54][55], real-world adaptation [56,57], and simulator augmentation [58][59][60].Many of these approaches require explicit, domain-specific knowledge and expertise in tasks or simulators.</p>
<p>Although for a particular simulation-reality pair, there may exist specific inductive biases that can be hand-crafted post hoc to close the sim-to-real gap [18], this knowledge is often not available a priori.Identifying its effects on task completion is also intractable.</p>
<p>We argue that a straightforward and feasible way for humans to obtain such knowledge is to observe and assist policy execution in the real world.If humans can assist the robot to successfully accomplish the tasks in the real world, sim-to-real gaps are effectively addressed.This naturally leads to a generally applicable paradigm that can cover different priors across simulations and realitieshuman-in-the-loop learning [61][62][63] and shared-autonomy [64,65].</p>
<p>Our key insight is that the human-in-the-loop framework is promising for addressing the sim-toreal gaps as a whole, in which humans directly assist the physical robots during policy execution by providing online correction signals.The knowledge required to close sim-to-real gaps can be learned from human signals.We present TRANSIC (transferring policies sim-to-real by learning from online correction, Fig. 1), a data-driven approach to enable the successful transfer of robot manipulation policies trained with RL in simulation to the real world.In TRANSIC, once the base robot policies are acquired from simulation training, they are deployed to real robots where human operators monitor the execution.When the robot makes mistakes or gets stuck, humans interrupt and assist robot policies through teleoperation.Such human intervention data are collected to train a residual policy, after which the base policy and the residual policy are combined to solve contactrich manipulation tasks.Unlike previous approaches that heavily rely on domain knowledge, since humans can successfully assist the robot trained in silico to complete real-world tasks, sim-to-real gaps are implicitly handled and addressed by humans in a domain-agnostic manner.</p>
<p>To summarize, our key contribution is a novel, holistic human-in-the-loop method called TRANSIC to tackle sim-to-real policy transfer for manipulation tasks.Through extensive evaluation, we show that our method leads to more effective sim-to-real transfer compared to traditional methods [50,52] and requires less real-robot data compared to the prevalent imitation learning and offline RL algorithms [66][67][68][69].We demonstrate that successful sim-to-real transfer of short-horizon skills can solve long-horizon, contact-rich manipulation tasks, such as furniture assembly.</p>
<p>Sim-to-Real Policy Transfer by Learning from Online Correction</p>
<p>An overview of TRANSIC is shown in Fig. 2.This section starts with a brief preliminary review, followed by a description of simulation training.We then introduce residual policies learned from human intervention and online correction and present an integrated framework for deployment during testing.Lastly, we provide implementation details.</p>
<p>Preliminaries</p>
<p>We formulate a robot manipulation task as an infinite-horizon discrete-time Markov Decision Process (MDP) M := (S, A, T , R, γ, ρ 0 ), where S is the state space, and A is the action space.At time step t, a robot observes s t ∈ S, executes an action a t , and receives a scalar reward r t from the reward function R(s t , a t ).The environment progresses to the next state following the transition function T (s t+1 |s t , a t ).The robot learns a parameterized policy π θ (•|s) to maximize the expected discounted return
J := E τ ∼pπ θ [ ∞ t=0 γ t r t ]
over induced trajectory distribution τ := (s 0 , a 0 , r 0 , ...) ∼ p π θ , where s 0 ∼ ρ 0 is sampled from the initial state distribution and γ ∈ [0, 1) is a discount factor.We consider simulation and real environments as two different MDPs.We adopt an intervention-based learning framework [66,67] where a human operator can intervene and take control during the execution of the robot policy.</p>
<p>Learning Base Policies in Simulation with RL</p>
<p>Policy Learning with 3D Representation Object geometry matters for contact-rich manipulation.For example, a robot should ideally insert a light bulb into the lamp base with the thread facing down.To retain such 3D information and facilitate sim-to-real transfer, we propose to use point cloud as the main visual modality.Typical RGB observation used in visuomotor policy training [70] suffers from several drawbacks that hinder successful transfer, such as vulnerability to different camera poses [71] and discrepancies between synthetic and real images [42].Well-calibrated point cloud observation can bypass these issues and has been successfully demonstrated [14,72].During the simulation RL training phase, we synthesize point cloud observations for higher throughput.Concretely, given the synthetic point cloud of the m-th object P (m) ∈ R K×3 , we transform it into the global frame through
P (m) g = P (m) R (m) ⊺ + p (m) ⊺ .
Here, R (m) ∈ R 3×3 and p (m) ∈ R 3×1 denote the object's orientation and translation in the global frame.Further, the point cloud representation of a scene S with M objects is aggregated as P S = M m=1 P (m) g and subsequently used as policy input.</p>
<p>Action Space Distillation A suitable action abstraction is critical for efficient learning [46,47] as well as sim-to-real transfer [48].A high-level controller such as the operational space controller (OSC) [73] facilitates RL exploration [46] but may hinder sim-to-real transfer because it requires accurate modeling of robot parameters, such as joint friction, mass, and inertia [74]; on the other hand, a low-level action space such as the joint position ensures consistent deployment in simulation and real hardware, but renders trial-and-error RL impractical.We draw inspiration from the teacherstudent framework [15,[75][76][77] and propose to first train the teacher policy π teacher through RL with OSC and then distill successful trajectories into the student policy π student with joint position control.Specifically, we roll out π teacher and record the robot's joint position at every simulated time interval δt to construct a dataset D teacher = {τ (n) } N n=1 .We then relabel actions from the endeffector's poses to joint positions.Such a relabeled dataset is ready to train student policies through behavior cloning.We name this approach as action space distillation and find it crucial to overcome the sim-to-real controller gap.Furthermore, teacher policies directly receive privileged observations for ease of learning, while student policies learn on synthetic point-cloud inputs to match real-world measurements.The student policy parameterized by θ, π student and D pcd = {(P real , P sim ) (i) } N i=1 is a separate dataset that contains N pairs of matched point clouds in simulation and reality for regularization purpose.Further justifications of this distillation phase can be found in Appendix D.1.</p>
<p>Learning Residual Policies from Online Correction</p>
<p>Human-in-the-Loop Data Collection Once the student policy is obtained from simulation, it is used directly as the base policy π B to bootstrap the data collection.Naïvely deploying the base policy on real robots usually results in inferior performance and unsafe motion due to various sim-to-real gaps.In TRANSIC, the base policy is instead deployed in a way that is fully synchronized with the human operator.Concretely, at time step t, once a B t ∼ π B is deployed, a human operator needs to decide whether intervention is necessary, indicated as 1 H t .Intervention is not necessary for most task execution when the robot is approaching objects of interest.However, when the robot tends to behave abnormally, the human operator intervenes and takes full control through teleoperation to correct robot errors.In these cases, the robot's pre-and post-intervention states, as well as intervention indicator 1 H t , are collected to construct the online correction dataset
D H ← D H ∪ 1 H t , q pre t , q post t
. This procedure is illustrated in Appendix Algorithm 1.</p>
<p>Human Correction as Residual Policies</p>
<p>Properly modeling human correction can be challenging.This is because humans usually solve tasks not purely based on current observation, hence the non-Markovian decision process [68].Therefore, directly fine-tuning the base policy π B on human correction dataset D H leads to large motions and even model collapse (Sec.3).Inspired by prior work on learning residuals to compensate for unknown dynamics and noisy observations [78][79][80], we propose to incorporate human correction behaviors with residual policies.Concretely, at the time of intervention, a residual policy π R ψ parameterized by ψ learns to predict human intervention as the difference between post-and pre-intervention robot states: a R = q post ⊖ q pre , where ⊖ denotes generalized subtraction.For continuous variables such as joint position, it computes the numerical difference; for binary variables such as opening and closing the gripper, it computes exclusive nor.The residual policy is then trained to maximize the likelihood of human correction:
L residual = −E D H log π R ψ (a R |•) .</p>
<p>An Integrated Deployment Framework</p>
<p>In practice, we find that learning a gating function to control whether to apply residual actions or not leads to better empirical performance (Appendix D.3).We call this learned gated residual policy.Denote the gating function as g ψ (•).It shares the same feature encoder with the residual policy π R and is jointly learned through classification on the same correction dataset D H .At the inference time, we effectively use g ψ as an indicator function 1 g to determine whether to apply the predicted residual actions.The policy effectively being deployed to autonomously complete tasks is an integration of base policy π B and residual policy π R , gated by g:
π deployed = π B ⊕ 1 g π R . A
joint position controller is used during deployment.</p>
<p>Implementation Details</p>
<p>We use the Isaac Gym [9] simulator.Proximal policy optimization (PPO) [81] is used to train teacher policies from scratch with task-specific reward functions and curricula.Student policies  3 Experiments</p>
<p>We answer the following research questions through experiments.</p>
<p>Q1: Does TRANSIC lead to better transfer performance while requiring less real-world data?Q2: How effectively can TRANSIC address different types of sim-to-real gaps?Q3: How does TRANSIC scale with human effort?Q4: Does TRANSIC exhibit intriguing properties, such as generalization to unseen objects, policy robustness, ability to solve long-horizon tasks, and other emergent behaviors?</p>
<p>Tasks, Baselines, and Evaluation Protocol</p>
<p>As shown in Fig. 3, we consider complex contact-rich manipulation tasks (Stabilize, Reach and Grasp, Insert, and Screw) that require high precision in FurnitureBench [85].These tasks are challenging and ideal for testing sim-to-real transfer, since perception, embodiment, controller, and dynamics gaps all need to be addressed.We collect 20, 100, 90, and 17 real-robot trajectories with human correction, respectively.These amount to 62, 434, 489, and 58 corrections for each task.See Appendix B.1 for the detailed system setup.We compare with three groups of baselines.1) Traditional sim-to-real methods: It includes domain randomization and data augmentation [52] ("DR.&amp; Data Aug."), real-world fine-tuning through BC ("BC Fine-Tune") and implicit Q-learning [69] ("IQL Fine-Tune").To estimate the performance lower bound, we also include "Direct Transfer" without any data augmentation or real-world fine-tuning.2) Interactive imitation learning (IL): It includes HG-Dagger [66] and IWR [67].3) Learning from real-robot data only: It includes BC [86], BC-RNN [68], and IQL [69] that are trained on real-robot demonstrations only.All evaluations are conducted on the real robot and consist of 20 trials starting with different objects and robot poses.See Appendix C for details.</p>
<p>Results</p>
<p>TRANSIC is effective for sim-to-real transfer and requires significantly less real-world data (Q1).As shown in Fig. 4 and Table A.XI, TRANSIC achieves the best performance on average and in all four tasks with significant margins.What are the reasons for successful transfer?We observe that adding real-world human correction data does not guarantee improvement.For example, among traditional sim-to-real methods, the best baseline BC Fine-Tune outperforms DR. &amp; Data Aug. by 7%, but IQL Fine-Tune leads to worse performance.In contrast, TRANSIC effectively uses human correction data, which boosts average performance by 1.24×.Not only does it achieve</p>
<p>Transic</p>
<p>Traditional Sim-to-Real Interactive IL Real Data Only the best transfer performance, but it also improves simulation policies the most among various sim-to-real approaches.</p>
<p>Furthermore, TRANSIC outperforms interactive IL methods, including HG-Dagger and IWR, by 0.75× on average.Although both of them weigh the intervention data higher during training, we find that they tend to erase the original policy and lead to catastrophic forgetting.In contrast, by incorporating human correction with a separate residual policy and integrating both base and residual policies through gating, TRANSIC combines the best properties of both policies during deployment.It relies on the simulation policy for robust execution most of the time; when the base policy is likely to fail, it automatically applies the residual policy to prevent failures and correct mistakes.</p>
<p>Finally, TRANSIC only requires dozens of real-robot corrections to achieve superior performance.However, methods such as BC-RNN and IQL trained on such a limited number of trajectories suffer from overfitting and model collapse.TRANSIC achieves 3.6× better performance than them.This result highlights the importance of first training in simulation and then leveraging sim-to-real transfer for robot learning practitioners.In summary, we show that in sim-to-real transfer, a good base policy learned from the simulation can be combined with limited real-world data to achieve success.However, effectively utilizing human correction data to address the sim-to-real gap is challenging, especially when we want to prevent catastrophic forgetting of the base policy.</p>
<p>Perception Embodiment
Dynamics Controller Asset
TRANSIC is effective in addressing different sim-toreal gaps (Q2).We shed light on its ability to close each individual sim-to-real gap by creating five different simulation-reality pairs.For each of them, we intentionally create large gaps between the simulation and the real world.These gaps are applied to real-world settings, including perception error, underactuated controller, embodiment mismatch, dynamics difference, and object asset mismatch.Note that these are artificial settings for a controlled study.See Appendix C.3 for detailed setups.</p>
<p>As shown in Fig. 5, TRANSIC achieves an average success rate of 77% across five different simulation-reality pairs with deliberately exacerbated sim-to-real gaps.This indicates its remarkable ability to close these individual gaps.In contrast, the best baseline method, IWR, only achieves an average success rate of 18%.We attribute this effectiveness in addressing different sim-to-real gaps to the residual policy design.Zeng et al. [80] echos our finding that residual learning is an effective tool to compensate for domain factors that cannot be explicitly modeled.Furthermore, training with data specifically collected from a particular setting generally increases TRANSIC's performance.However, this is not the case for IWR, where fine-tuning on new data can even lead to worse performance.These results show that TRANSIC is better not only in addressing multiple sim-to-real gaps as a whole but also in handling individual gaps of a very different nature.TRANSIC scales with human effort (Q3).We demonstrate that TRANSIC scales better with human data than the best baseline, IWR, as shown in Fig. 6 and Table A.XII.If we increase the dataset size from 25% to 75% of the full size, TRANSIC improves on average by 42%.In contrast, IWR only achieves a 23% relative improvement.Additionally, for tasks other than Insert, IWR performance plateaus at an early stage and even starts to decrease as more human data becomes available.We hypothesize that IWR suffers from catastrophic forgetting and struggles to properly model the behavioral modes of humans and trained robots.On the other hand, TRANSIC bypasses these issues by learning gated residual policies only from human correction.Intriguing properties and emergent behaviors of TRANSIC (Q4).As shown in Fig. 7, TRANSIC can achieve an average success rate of 75% when zero-shot evaluated on assembling a lamp.However, IWR can only succeed once every three attempts.This evidence suggests that TRANSIC is not overfitting to a particular object; instead, it has learned reusable skills for category-level object generalization.Further, with the ablation results shown in Table 1, TRANSIC exhibits intriguing properties including effective gating, policy robustness against reduced cameras and suboptimal correction data, and consistency in learned visual features.See Appendix C.5 for detailed setups and discussions.Qualitatively, TRANSIC shows several representative behaviors that resemble humans.For instance, they include error recovery, unsticking, safety-aware actions, and failure prevention, as shown in Fig. A.12. Finally, we demonstrate that successful sim-to-real transfer of individual skills can be effectively chained together to enable long-horizon contact-rich manipulation (Fig. 8).See videos at transic-robot.github.io.Robot Learning via Sim-to-Real Transfer Physics-based simulations have become a driving force for developing robotic skills [6][7][8][9][10].However, the domain gap between the simulators and reality is not negligible [40].Successful sim-to-real transfer includes locomotion [18][19][20][21][22][23][24][25][26][27], dexterous in-hand manipulation [13][14][15][16][17], and simple non-prehensile manipulation [30][31][32][33][34][35][36][37][38][39].In this work, we tackle more challenging sim-to-real transfer for complex whole-arm manipulation tasks and successfully demonstrate that our approach can solve sophisticated contact-rich tasks.More importantly, it requires significantly fewer real-robot data compared to the behavior cloning approach [68].This makes solutions based on simulators and sim-to-real transfer more appealing to roboticists.The robot assembles a square table from FurnitureBench [85].Videos are available at transic-robot.github.io.</p>
<p>Related Work</p>
<p>Sim-to-Real Gaps in Manipulation Tasks</p>
<p>The sim-to-real gaps can be coarsely categorized as follows: a) perception gap [41][42][43], where synthetic sensory observations differ from those measured in the real world; b) embodiment mismatch [18,44,45], where the robot models used in simulation do not match the real-world hardware precisely; c) controller inaccuracy [46][47][48], meaning that the results of deploying the same commands differ in simulation and real hardware; and d) poor physical realism [49], where physical interactions such as contact and collision are poorly simulated [87].Traditional methods to address them include system identification [18,30,50,51], domain randomization [13,[52][53][54][55], real-world adaptation [56], and simulator augmentation [58][59][60].However, system identification is mostly engineered on a case-by-case basis.Domain randomization suffers from the inability to identify and randomize all physical parameters.Methods with real-world adaptation, usually through meta-learning [88], incur potential safety concerns during the adaptation phase.In contrast, our method leverages human intervention data to implicitly overcome the gap in a domain-agnostic way, leading to a safer deployment.</p>
<p>Human-in-The-Loop Robot Learning Human-in-the-loop machine learning is a prevalent framework to inject human knowledge into autonomous systems [62,89,90].The recent trend focuses on continually improving robots' capability with human feedback [91] and autonomously generating corrective intervention data [92].Our work further extends this trend by showing that sim-toreal gaps can be effectively eliminated by using human intervention and correction signals.In shared autonomy, robots and humans share the control authority to achieve a common goal [64,65,[93][94][95].This control paradigm has been largely studied in assistive robotics and human-robot collaboration [96][97][98].In this work, we provide a novel perspective by employing it in the sim-to-real transfer of robot control policies and demonstrating its importance in attaining effective transfer.</p>
<p>Limitations and Conclusion</p>
<p>Limitations 1) Current tasks are in a single-arm tabletop scenario.Though TRANSIC can potentially be applied to more complicated robots with recent teleoperation interfaces [99][100][101][102][103]. 2) Human operators still manually decide when to intervene.This could be automated using failure detection techniques [104,105].3) TRANSIC requires simulation policies with reasonable performance.Nevertheless, it is compatible with recent advances in synthesizing manipulation data [106,107].</p>
<p>In this work, we present TRANSIC, a human-in-the-loop method for sim-to-real transfer in contactrich manipulation tasks.We show that combining a strong base policy from simulation with limited real-world data can be effective.However, utilizing human correction data without causing catastrophic forgetting of the base policy is challenging.TRANSIC overcomes this by learning a gated residual policy from a small amount of human correction data.We show that TRANSIC effectively addresses various sim-to-real gaps, both collectively and individually, and scales with human effort.height.The object's irregular shape limits certain grasping poses.For example, the end-effector needs to be near orthogonal to the table leg in the xy plane and far away from the screw thread.Therefore, we design a curriculum over the object geometry to warm up the RL learning.It gradually adjusts the object geometry from a cube, to a cuboid, and finally the table leg.In all curriculum stages, the reward function is
r t = w distance d + w lif ted 1 lif ted + w success 1 success . (A.2)
Here, w distance is the weight for distance reward, w lif ted is the reward for the leg being lifted, and w success is the success weight.d is the distance to the table leg and is calculated as
d = 1 − tanh 10 4 (d eef + d lef t f inger + d right f inger + d orthogonal ) , (A.3)
where d eef is the distance between the end-effector and the table leg, d lef t f inger is the distance between the left gripper tip to the table leg, d right f inger is the distance between the right gripper tip to the table leg, and d orthogonal is the difference between the current and the orthogonal grasping orientations.We set w distance = 0.1, w lif ted = 1.0, and w success = 200.0.The episode length is 50.One episode terminates upon success or timeout.</p>
<p>A.2.3 Insert</p>
<p>In this task, the robot needs to insert a pre-grasped table leg into the far right assembly hole of the tabletop, while the tabletop is already stabilized.The tabletop is initialized at the coordinate (0.53, 0.05) relative to the robot base.We then randomly translate it with displacements sampled from U(−0.02, 0.02) along x and y directions.We also apply random Z rotation with values drawn from U(−45°, 45°).We further randomize the robot's pose by adding noises sampled from U(−0.25, 0.25) to joint positions.The task is successful when the table leg remains vertical and is close to the correct assembly position within a small threshold.We design curricula over the randomization strength to facilitate the learning.The following reward function is used:
r t = w distance d + w success 1 success , (A.4)</p>
<p>A.4.2 Observation Space</p>
<p>Student policies receive observations that can be obtained in the real world.They are pointcloud and proprioceptive observations.We synthesize point clouds from objects' 6D poses to improve the training throughput.Concretely, given the groundtruth point cloud of the m-th object P (m) ∈ R K×3 , we transform it into the global frame through P (m) g = P (m) R (m) ⊺ + p (m) ⊺ .Here R (m) ∈ R 3×3 and p (m) ∈ R 3×1 denote the object's orientation and translation in the global frame.Further, the point-cloud representation of a scene S with M objects is aggregated as P S = M m=1 P (m) g .For the robot, we only include point clouds for its two fingers and ignore other parts.To facilitate policies to differentiate gripper fingers from the scene, we extend the coordinate dimension to include a semantic label ∈ {0, 1} that indicates gripper fingers or not.This information can be obtained on real robots through forward kinematics.A full point cloud is then downsampled to 768 points.Table A.VI lists the observation space.</p>
<p>A.4.3 Action Space Distillation</p>
<p>To reduce the controller sim-to-real gap before transfer, we train student policies to output in the configuration space.To achieve that, we relabel actions â in trajectories generated by teacher policies from end-effector's delta poses to absolute joint positions.This is equivalent to set ât = q t+1 for all time steps.Therefore, the action space for student policies is A student = (q, 1 gripper ), where q ∈ R 7 is the joint position within the valid range.In simulation, student policies' actions are deployed with a joint position controller.</p>
<p>A.4.4 Model Architecture</p>
<p>We use feed-forward policies for tasks Reach and Grasp and Insert and recurrent policies for tasks Stabilize and Screw as we find they achieve the best distillation results.PointNets [82] are used to encode point clouds.Recall that each point in the point cloud also contains a semantic label indicating the gripper or not.We concatenate point coordinates with these semantic labels' vector embeddings before passing into the PointNet encoder.We use Gaussian Mixture Models (GMM) [68] as the action head.Detailed model hyperparameters are listed in Table A.VII.</p>
<p>A.4.5 Data Augmentation</p>
<p>We apply strong data augmentation during distillation.For point-cloud observations, random translation and random jitter are independently applied with a probability P pcd aug = 0.4.We also add Gaussian noises to proprioceptive observations.Augmentation parameters are listed in , where we set β = 10 −3 .We use the Adam optimizer [112] with a learning rate of 10 −4 during training.We periodically roll out student policies in simulation for 1, 000 episodes.We then select the checkpoint that corresponds to the highest success rate to use as the base policy in the real-world learning stage.</p>
<p>B Real-World Learning Details</p>
<p>In this section, we provide details about real-world learning, including the hardware setup, humanin-the-loop data collection, and residual policy training.The residual policy takes the same observations as the base policy (Table A.VI).Furthermore, to effectively predict residual actions, it is also conditioned on base policy's outputs.Its action head outputs eight-dim vectors, while the first seven dimensions correspond to residual joint positions and the last dimension determines whether to negate base policy's gripper action or not.Besides, a separate intervention head predicts whether the residual action should be applied or not (learned gated residual policy, Sec.2.4).
a B ← a B ∼ π B (o) o next ← E.deploy(a B )
▷ human decides intervention or not
1 H ← π H .intervene(o, o next ) if 1 H then q pre ← E.robot state ▷ deploy human correction a H ← a H ∼ π H (o, o next ) o next ← E.deploy(a H ) q post ← E.robot state ▷ update dataset D H ← D H ∪ q pre , q post , 1 H , o end ▷ update the next observation o ← o next end
For tasks Stabilize and Insert, we use a PointNet [82] as the point-cloud encoder.For tasks Reach and Grasp and Screw, we use a Perceiver [83,84] as the point-cloud encoder.Residual policies are instantiated as feed-forward policies in all tasks.We use GMM as the action head and a simple two-way classifier as the intervention head.Model hyperparameters are summarized in Table A</p>
<p>B.4.2 Training Details</p>
<p>To train the learned gated residual policy, we first only learn the feature encoder and the action head.</p>
<p>We then freeze the entire model and only learn the intervention head.We opt for this two-stage training since we find that training both action and intervention heads at the same time will result in sub-optimal residual action prediction.We follow the best practice for policy training, including using learning rate warm-up and cosine annealing [117].Training hyperparameters are listed in Table A.X.</p>
<p>C Experiment Settings and Evaluation Details</p>
<p>In this section, we provide details about our experiment settings and evaluation protocols.</p>
<p>C.1 Task Definition</p>
<p>As shown in Fig. 3, we quantitatively benchmark four tasks.They are fundamental skills required to assemble a square table from FurnitureBench [85].We randomize objects' initial poses during evaluation.</p>
<p>• Stabilize: The robot pushes the square tabletop to the right corner of the wall such that it remains stable in following assembly steps.</p>
<p>• Reach and Grasp: The robot reaches and grasps the table leg.It needs to properly adjust the end effector's orientation to avoid infeasible grasping poses.</p>
<p>• Insert: The robot inserts the pre-grasped table leg to the far right assembly hole of the tabletop.</p>
<p>• Screw: The robot's end-effector is initialized close to an inserted table leg and it screws the table leg clockwise into the tabletop.</p>
<p>C.2 Main Experiments</p>
<p>We evaluate all methods on four tasks for 20 trials.Each trail starts with different objects and robot poses.We make our best efforts to ensure the same initial settings when evaluating different methods.Specifically, we take pictures for these 20 different initial configurations and refer to them when resetting a new trial.See Figs.A.14, A.15, A.16, A.17 for initial configurations of tasks Stabilize, Reach and Grasp, Insert, and Screw, respectively.We follow Liu et al. [91] to label reward for IQL.Full numerical results are provided in Table A.XI.We explain how different sim-to-real gaps are created.</p>
<p>Perception Error This is done by applying random jitter to 25% points from point clouds with probability P = 0.6, as visualized in Fig. A.7.We test this sim-to-real gap on the task Reach and Grasp.The jittering noise is sampled independently from the distribution N (0, 0.03).We clip the noise to be within the ± 0.03 range.Underactuated Controller This is done by making the joint position controller less accurate.We test this gap on the task Insert.We emulate an underactuated controller through early stopping.</p>
<p>Concretely, at every time a new joint position goal q goal is set, we record the distance to the goal in configuration space d q = ∥q − q goal ∥ and sample a factor Γ ∼ U(0.80, 0.95).The controller will stop reaching the desired goal once it achieves Γ progress, i.e., stop early when ∥q − q goal ∥ ≤ (1 − Γ)d q .Embodiment Mismatch This is done by changing the robot gripper to be shorter length as demonstrated in Fig. A.9.We test this gap on the task Screw.We notice that the 9 cm length difference incurs a significant gap.</p>
<p>Dynamics Difference This is done by changing object surfaces and increasing friction.We test this gap on the task Stabilize.Concretely, we attach friction tapes to the square tabletop's surface to increase friction, hence change the dynamics (Fig</p>
<p>C.4 Data Scalability Experiments</p>
<p>In Table A.XII, we show quantitative results for scalability with human correction dataset size on four tasks.</p>
<p>C.5 Ablation Studies</p>
<p>C.5.1 Effects of Different Gating Mechanisms</p>
<p>We introduce the learned gated residual policy in Sec.2.4 where the gating mechanism controls when to apply residual actions.To assess the quality of learned gating, we compare its performance with an actual human operator performing gating.Results are shown in Table 1 (row "w/ Human Gating").It is evident that the learned gating mechanism only incurs negligible performance drops compared to human gating.This suggests that TRANSIC can reliably operate in a fully autonomous setting once the gating mechanism is learned.</p>
<p>C.5.2 Policy Robustness</p>
<p>We investigate the policy robustness against 1) point cloud observations with inferior quality by removing two cameras, and 2) suboptimal correction data with noise injection.We remove two cameras and only keep three.Note that this is the same number of cameras as in FurnitureBench [85].</p>
<p>For tasks other than Insert, we keep the wrist camera, the right front camera, and the left rear camera.</p>
<p>For the task Insert, we keep two front cameras and the left rear camera.We simulate suboptimal correction data by injecting noise into residual actions a R .This noise is of large magnitude, which follows the normal distribution with zero mean and standard deviation corresponding to 5% of the largest residual action in the dataset.Results are shown in Table 1 (rows "Reduced Cameras" and "Noisy Correction").We highlight that TRANSIC is robust to partial point cloud inputs caused by the reduced number of cameras.We attribute this to the heavy point cloud downsampling employed during training.Fishman et al. [118] echos our finding that policies trained with downsampled synthetic point cloud inputs can generalize to partial point cloud observations obtained in the real world without the need for shape completion.Meanwhile, when the correction data used to learn residual policies are suboptimal, TRANSIC only shows a relative decrease of 6% in the average success rate.We attribute this to the advantage of our integrated deployment-when the residual policy behaves suboptimally, the base policy could still compensate for the error in subsequent steps.Stabilize TRANSIC 35% 80% 80% 100% 100% IWR [67] 70% 75% 80% 65%</p>
<p>Reach and Grasp TRANSIC 60% 65% 80% 90% 95% IWR [67] 60% 65% 40% 40%</p>
<p>Insert TRANSIC 5% 20% 35% 40% 45% IWR [67] 5% 15% 30% 40% Screw TRANSIC 35% 50% 65% 75% 85% IWR [67] 20% 40% 40% 40%</p>
<p>C.5.3 Consistency in Learned Visual Features</p>
<p>To learn consistent visual features between the simulation and reality, we propose to regularize the point cloud encoder during the distillation stage.As shown in Table 1 (row "w/o Regularization"), the performance significantly decreases without such regularization, especially for tasks that require fine-grained visual features.Without it, simulation policies would overfit to synthetic point-cloud observations and hence are not ideal for sim-to-real transfer.</p>
<p>C.6 Qualitative Analysis and Emergent Behaviors</p>
<p>We examine the distribution of the collected human correction dataset.During the human-in-theloop data collection, the probability of intervening and correcting is reasonably low (P correction ≈ 0.20).This is consistent with our intuition that, with a good base policy, interventions are not necessary for most of the time.However, they become critical when the robot tends to behave abnormally due to unaddressed sim-to-real gaps.Moreover, as highlighted in</p>
<p>D Additional Experiment Results and Discussions</p>
<p>D.1 Empirical Justifications for Action Space Distillation</p>
<p>Reasons for the proposed action space distillation are twofold.The first is mainly because an OSC is hard to sim-to-real transfer, while a joint position controller can be seamlessly transferred.As suggested in Nakanishi et al. [74], an OSC requires accurate modeling of robot parameters, such as the task-space inertia matrix and gravity compensation.System identification helps but is insufficient.Furthermore, it is often the case that given the same joint torque, the end-effector moves differently in simulation and the real world.Because an OSC uses a task-space error to compute joint torques, this will lead to large joint position deviation.shows no sign of improvement.In contrast, TRANSIC takes around 3 days to train on NVIDIA GeForce RTX 3090 GPUs.Therefore, the distillation is important to make the training feasible.other hand, by training the residual policy only on human corrections, we offload learning whether to apply the correction to a gating function and thus reduce the negative effects of unbalanced data.</p>
<p>D.4 Long-Horizon Tasks Statistics</p>
<p>We show statistics about task length from FurnitureBench [85]  We adopt an intervention-based learning framework [66,67,91] where a human operator can intervene and take control during the execution of the robot base policy π B .Denote the human policy as π H , the following combined policy is deployed during data collection:
π deployed = 1 H π H + 1 − 1 H π B , (A.7)
where 1 H is a binary function indicating human interventions.Introducing a trajectory distribution q(τ ) that consists of two observation-action distributions generated by the robot ρ B and human operator ρ H , the original RL objective leads to the maximization of a variational lower bound on logarithmic return [67,121]:
J (θ, q) = E q(τ ) [log R(τ ) + log p π θ − log q(τ )] ,(A.8)
where p π θ is the induced trajectory distribution.While the human operator optimizes Eq.A.8 through intervention and correction, the robot learner maximizes it through θ = arg max Various intervention-based policy learning methods have been derived by weighting observationaction pairs in Eq.A.9 differently.For example, HG-Dagger [66] completely ignores robot data D B and only trains on human data D H that contain intervention samples.This is equivalent to q(τ ) ∝ ρ H . Intervention Weighted Regression (IWR) [67] balances the data distribution by emphasizing human intervention: q(τ ) ∝ αρ H + ρ B with α = |D B |/|D H |. Non-intervention-based methods such as traditional behavior cloning (BC) [86] only learn on D H with full human demonstrations instead of intervention.This effectively sets q(τ ) ∝ ρ H .</p>
<p>F Extended Related Work</p>
<p>Robot Learning via Sim-to-Real Transfer Physics-based simulations [6-10, 49, 122-124] have become a driving force [1,2] for developing robotic skills in tabletop manipulation [125][126][127][128], mobile manipulation [129][130][131][132], fluid and deformable object manipulation [133][134][135][136], dexterous in-hand manipulation [13][14][15][16][17], locomotion with various robot morphology [18][19][20][21][22][23][24][25][26]137], object tossing [80], acrobatic flight [28,29], etc.However, the domain gap between the simulators and the reality is not negligible [10].Successful sim-to-real transfer includes locomotion [18][19][20][21][22][23][24][25][26][27], in-hand re-orientation for dexterous hands where objects are initially placed near the robot [13][14][15][16][17], and non-prehensile manipulation limited to simple tasks [30][31][32][33][34][35][36][37][38][39].In this work, we tackle more challenging sim-to-real transfer for complex manipulation tasks and successfully demonstrate that our approach can solve sophisticated contact-rich manipulation tasks.More importantly, it requires significantly fewer real-robot data compared to the prevalent imitation learning and offline RL approaches [68,69,86].This makes solutions that are based on simulators and sim-to-real transfer more appealing to roboticists.</p>
<p>Sim-to-Real Gaps in Manipulation Tasks Despite the complex manipulation skills recently learned with RL in simulation [138], directly deploying learned control policies to physical robots often fails.The sim-to-real gaps [10,40,44,139] that contribute to this performance discrepancy can be coarsely categorized as follows: a) perception gap [18,[41][42][43], where synthetic sensory observations differ from those measured in the real world; b) embodiment mismatch [18,44,45], where the robot models used in simulation do not match the real-world hardware precisely; c) controller inaccuracy [46][47][48], meaning that the results of deploying the same high-level commands (such as in configuration space [140] and task space [141]) differ in simulation and real hardware; and d) poor physical realism [49], where physical interactions such as contact and collision are poorly simulated [87].</p>
<p>Although these gaps may not be fully bridged, traditional methods to address them include system identification [18,30,50,51], domain randomization [13,[52][53][54][55], real-world adaptation [56], and simulator augmentation [58][59][60].However, system identification is mostly engineered on a case-bycase basis.Domain randomization suffers from the inability to identify and randomize all physical parameters.Methods with real-world adaptation, usually through meta-learning [88], incur potential safety concerns during the adaptation phase.Most of these approaches also rely on explicit and domain-specific knowledge about tasks and the simulator a priori.For instance, to perform system identification for closing the embodiment gap for a quadruped, Tan et al. [18] disassembles the physical robot and carefully calibrates parameters including size, mass, and inertia.Kim et al. [32] reports that collaborative robots, such as the commonly used Franka Emika robot, have intricate joint friction that is hard to identify and randomized in typical physics simulators.To make a simulator more akin to the real world, Chebotar et al. [39] deploys trained virtual robots multiple times to refine the distributions of simulation parameters.This procedure not only introduces a significant real-world sampling effort, but also incurs potential safety concerns due to deploying suboptimal policies.In contrast, our method leverages human intervention data to implicitly overcome the transferring problem in a domain-agnostic way and also leads to a safer deployment.</p>
<p>Human-in-The-Loop Robot Learning Human-in-the-loop machine learning is a prevalent framework to inject human knowledge into autonomous systems [62,89,90].Various forms of human feedback exist [63], ranging from passive judgement, such as preference [142][143][144][145][146][147][148][149][150][151] and evaluation [152][153][154][155][156][157], to active involvement, including intervention [158][159][160] and correction [161,162].They are widely adopted in solutions for sequential decision-making tasks.For instance, interactive imitation learning [66,67,91,163] leverages human intervention and correction to help naïve imitators address data mismatch and compounding error.In the context of RL, reward functions can be derived to better align agent behaviors with human preferences [145,148,149,152].Noticeably, recent trend focuses on continually improving robots' capability by iteratively updating and deploying policies with human feedback [91], combining active human involvement with RL [162], and autonomously generating corrective intervention data [92].Our work further extends this trend by showing that sim-to-real gaps can be effectively eliminated by using human intervention and correction signals.</p>
<p>In shared autonomy, robots and humans share the control authority to achieve a common goal [64,65,[93][94][95].This control paradigm has been largely studied in assistive robotics and human-robot collaboration [96][97][98].In this work, we provide a novel perspective by employing it in sim-to-real transfer of robot control policies and demonstrating its importance in attaining effective transfer.</p>
<p>Human-in-the-Loop Correction (d) Successful Transfer with Learned Residual Policy</p>
<p>Figure 1 :
1
Figure 1: TRANSIC for sim-to-real transfer in contact-rich robotic manipulation tasks.a) and b) Naïvely deploying policies trained in simulation usually fails due to various sim-to-real gaps.Here, the robot attempts to first align the light bulb with the base and then insert and screw the light bulb into the base.c) A human operator monitors robot behaviors, intervenes, and provides online correction through teleoperation when necessary.Human data are collected to train a residual policy to tackle various sim-to-real gaps in a holistic manner.d) The simulation and the residual policies are integrated together during test time to achieve a successful sim-to-real transfer for contact-rich tasks, such as screwing a light bulb into the base.</p>
<p>Figure 2 :
2
Figure 2: TRANSIC method overview.a) Base policies are first trained in simulation through action space distillation with demonstrations generated by RL teacher policies.Base policies take point cloud as input to reduce perception gap.b) The acquired base policies are first deployed with a human operator monitoring the execution.The human intervenes and corrects through teleoperation when necessary.Such correction data are collected to learn residual policies.Finally, both residual policies and base policies are integrated during test time to achieve a successful transfer.</p>
<p>Figure 3 :
3
Figure 3: Four tasks benchmarked in this work.They are fundamental skills required to assemble a square table from FurnitureBench [85].The task definition can be found in Appendix C.1.are parameterized as Gaussian Mixture Models (GMMs) [68].See Appendix A for more details.During human-in-the-loop data collection, we use a 3Dconnexion SpaceMouse as the teleoperation interface.Residual policies use state-of-the-art point cloud encoders [82-84] and GMM as the action head.More training hyperparameters are provided in Appendix B.4.</p>
<p>Figure 4 :
4
Figure 4: Average success rates over four benchmarked tasks.Numerical results in Table A.XI.</p>
<p>Figure 5 :
5
Figure 5: Robustness to different simto-real gaps.Numbers are averaged success rates (%).Polar bars represent performances after training with data collected specifically to address a particular gap.Dashed lines are zero-shot performances.Shaded circles show average performances.</p>
<p>Figure 6 :
6
Figure 6: Scalability with human correction data.Per-task results are shown in Table A.XII.</p>
<p>Figure 7 :
7
Figure 7: Generalization to unseen objects from a new category.Success rates are averaged over tasks Reach and Grasp and Screw.</p>
<p>(a) Assemble a lamp (160 seconds in 1× speed).(b) Assemble a square table (550 seconds in 1× speed).</p>
<p>Figure 8 :
8
Figure 8: a) The robot assembles a lamp.b)The robot assembles a square table from FurnitureBench[85].Videos are available at transic-robot.github.io.</p>
<p>Figure A. 1 :
1
Figure A.1: Visualization of simulated tasks.</p>
<p>Figure A. 2 :
2
Figure A.2: Visualization of paired point clouds in simulation (red) and reality (blue).</p>
<p>Figure A. 4 :SpaceMouseFigure A. 5 :
45
Figure A.4: Visualization of real-world point-cloud observations.We obtain them by 1) cropping point clouds fused from multi-view cameras based on coordinates, 2) removing statistical and radius outliers, 3) removing points corresponding to gripper fingers and replacing with synthetic point clouds through forward kinematics, 4) uniformly sampling without replacement, and 5) appending semantic labels to indicate gripper fingers (red) and the scene (blue).</p>
<p>Figure A. 6 :
6
Figure A.6: Cumulative distribution function (CDF) of human correction.Shaded regions represent standard deviation.Human correction happens at different times across tasks.This fact necessitates TRANSIC's learned gating mechanism.</p>
<p>Figure A. 7 :
7
Figure A.7: Visualization of introduced perception error.a) The original point-cloud observation.b) The erroneous point-cloud observation with random jitter.</p>
<p>Fig. A.8 visualizes the effect.</p>
<p>Figure A. 8 :
8
Figure A.8: Visualization of the trajectory realized by an underactuated controller.The plot displays the end-effector's position in the XY plane.It shows a reference circular movement, a trajectory tracked by the normal controller, and a trajectory tracked by the underactuated controller.</p>
<p>Figure A. 9 :
9
Figure A.9: Two different gripper fingers used to create embodiment mismatch.Policies are trained with the longer finger and tested on the shorter finger.</p>
<p>Figure A. 10 :Figure A. 11 :
1011
Figure A.10: Two square tabletops used to create dynamics difference.a) The original surface is smooth.b) We attach friction tapes to change the dynamics.</p>
<p>Fig. A.6, interventions happen at different times across tasks.This fact renders heuristics-based methods [119] for deciding when to intervene difficult, and further necessitates our learned residual policy.Several representative behaviors learned by TRANSIC are demonstrated in Fig. A.12.</p>
<p>Figure A.12: Emergent behaviors learned by TRANSIC.a) Error recovery.Left: The robot tries to insert the table leg but the direction is wrong; Right: TRANSIC raises the end effector and moves to the correct insertion position.b) Unsticking.Left: The robot hovers for a while and never reaches the light bulb; Right: TRANSIC helps the robot get unstuck and move to the bulb.c) Safety-aware actions.Left: When pushing the tabletop, the gripper is too low and bends.This might damage the robot; Right: TRANSIC compensates for the command that causes the end effector to move too low.d) Failure prevention.Left: The light bulb will fall and break after gripper opening; Right: TRANSIC adjusts the bulb to a stable pose to prevent failure.</p>
<p>Figure A. 13 :
13
Figure A.13: Learning curves for RL with point-cloud observations and joint position actions.</p>
<p>θ∈ΘE</p>
<p>(s,a)∼q(τ ) [log π θ (a|s)] .(A.9)</p>
<p>Figure A. 14 :
14
Figure A.14: Initial settings for evaluating the task Stabilize.</p>
<p>Figure A. 15 :
15
Figure A.15: Initial settings for evaluating the task Reach and Grasp.</p>
<p>Figure A. 16 :
16
Figure A.16: Initial settings for evaluating the task Insert.</p>
<p>Figure A. 17 :
17
Figure A.17: Initial settings for evaluating the task Screw.</p>
<p>Figure A. 18 :
18
Figure A.18: Initial settings for the experiment Object Asset Mismatch.</p>
<p>Table 1 :
1
Ablation study results.Numbers are success rates.
Average StabilizeReach and GraspInsert ScrewOriginal81%100%95%45%85%w/ Human Gating82%100%95%50%85%Reduced Cameras80%95%90%40%95%Noisy Correction76%85%80%45%95%w/o Regularization55%85%55%20%60%</p>
<p>Table A .
A
VI: The observation space for student policies.
NameDimensionPoint Cloud Proprioceptive768 × 4Joint Position7Cosine Joint Position7Sine Joint Position7End-Effector Position3End-Effector Rotation4Gripper Width1</p>
<p>Table A .
A
Table A.VIII.VII: Model hyperparameters for student policies.
HyperparameterValueHyperparameterValuePoint CloudRNNPointNet Hidden Dim256RNN TypeLSTM [113]PointNet Hidden Depth2RNN Num Layers2PointNet Output Dim256RNN Hidden Dim512PointNet ActivationGELU [114]RNN Horizon5Gripper Semantic Embd Dim128GMM Action HeadFeature FusionHidden Dim128MLP Hidden Dim512Hidden Depth3MLP Hidden Depth1Num Modes5MLP ActivationReLUActivationReLUTable A.VIII: Data augmentation used in distillation.HyperparameterValueHyperparameterValuePoint CloudProprioceptionAugmentation Probability Random Translation Distribution U(−0.04, 0.04) 0.4 Random Jittering Ratio 0.1Prop. Noise Distribution N (0, 0.1) Prop. Noise Low -0.3 Prop. Noise High 0.3Random Jittering Distribution Random Jittering LowN (0, 0.01) -0.015Random Jittering High0.015
A.4.6 Training DetailsTo regularize point-cloud features, we separately collect a dataset containing 59 pairs of matched point clouds in simulation and reality.One pair from them is visualized in Fig A.2. Student policies are trained by minimizing the loss in Sec.2.2</p>
<p>Algorithm 1: Human Intervention and Online Correction Data Collection input : Base policy π B , human policy π H , real-world environment E output : Human correction dataset D H initialize: D H ← ∅
o ← E.reset() while not E.terminated do ▷ deploy the base policy</p>
<p>.IX.
Table A.IX: Model hyperparameters for residual policies.HyperparameterValueHyperparameterValuePointNetFeature FusionPointNet Hidden Dim256MLP Hidden Dim512PointNet Hidden Depth2MLP Hidden Depth1PointNet Output Dim256MLP ActivationReLUPointNet ActivationGELUGMM Action HeadGripper Semantic Embd Dim128Hidden Dim128PerceiverHidden Depth3Perceiver Hidden Dim256Num Modes5Perceiver Number of Heads8ActivationReLUPerceiver Number of Queries8Intervention HeadGripper Semantic Embd Dim128Hidden Dim128Base Policy Action ConditioningHidden Depth3Base Policy Gripper Action Embd Dim64ActivationReLU</p>
<p>Table A .
A
X: Hyperparameters used in residual policy training.
HyperparameterValueLearning Rate10 −4Weight Decay0Learning Rate Warm Up Steps1, 000Learning Rate Cosine Decay Steps 100, 000Minimal Learning Rate10 −6OptimizerAdam</p>
<p>Table A .
A
XI: Success rates per tasks.TRANSIC outperforms all baseline methods in all four tasks.
TasksTRANSICDirect TransferDR. &amp; Data Aug. [52]BC Fine-TuneIQL Fine-TuneHG-Dagger [66] IWR [67] BC [86] BC-RNN [68] IQL [69]Stabilize100%10%35%55%0%65%65%40%40%5%Reach and Grasp95%35%60%35%0%30%40%25%0%5%Insert45%0%15%15%25%35%40%10%5%0%Screw85%0%35%50%65%40%40%15%25%0%
C.3 Experiments with Different Sim-to-Real Gaps C.3.1 Experiment Setup</p>
<p>Table A .
A
XII: Quantitative results for scalability with human correction dataset size on four tasks.
Method0Correction Dataset Size (%) 25 50 75 100</p>
<p>in Table A.XVI.
Table A.XVI: Statistics about long-horizon tasks from FurnitureBench [85].Number of Steps Average Human Demo LengthLamp5942 MinutesSquare Table16896 MinutesE Extended PreliminariesE.1 Intervention-Based Policy Learning
8th Conference on Robot Learning (CoRL 2024), Munich, Germany.
https://developer.nvidia.com/physx-sdk
https://github.com/frankaemika/franka_ros
AcknowledgmentsWe are grateful to Josiah Wong, Chengshu (Eric) Li, Weiyu Liu, Wenlong Huang, Stephen Tian, Sanjana Srivastava, and the SVL PAIR group for their helpful feedback and insightful discussions.This work is in part supported by the Stanford Institute for Human-Centered AI (HAI), ONR MURI N00014-22-1-2740, ONR MURI N00014-21-1-2801, and Schmidt Sciences.Ruohan Zhang is partially supported by the Wu Tsai Human Performance Alliance Fellowship.A Simulation Training DetailsIn this section, we provide details about simulation training, including the used simulator backend, task designs, reinforcement learning (RL) training of teacher policy, and student policy distillation.A.1 The SimulatorWe use Isaac Gym Preview 4[9]as the simulator backend.NVIDIA PhysX 1 is used as the physics engine to provide realistic and precise simulation.Simulation settings are listed in TableA.I.The robot model is from Franka ROS package 2 .We borrow furniture models from FurnitureBench[85]to create various tasks that require complex and contact-rich manipulation.A.2 Task ImplementationsWe implement four tasks based on the furniture model square table: Stabilize, Reach and Grasp, Insert, and Screw.An overview of simulated tasks is shown in Fig A.1.We elaborate on their initial conditions, success criteria, reward functions, and other necessary information.A.2.1 StabilizeIn this task, the robot needs to push the square tabletop to the right corner of the wall such that it is supported and remains stable in following assembly steps.The robot is initialized such that its gripper locates at a neutral position.The tabletop is initialized at the coordinate (0.54, 0.00) relative to the robot base.We then randomly translate it with displacements drawn from U(−0.015, 0.015) along x and y directions (the distance unit is meter hereafter).We also apply random Z rotation with values drawn from U(−15°, 15°).Four table legs are initialized in the scene.The task is successful only when the following three conditions are met:1) The square tabletop contacts the front and right walls;2) The square tabletop is within a pre-defined region;3) No table leg is in the pre-defined region.We use the following reward function:where w success is the success reward, 1 success indicates the success according to aforementioned conditions, w q penalizes large joint velocities, qt is the joint velocity, w action penalizes large action commands, and a t represents the action command at time step t.We set w success = 10, w q = 10 −5 , and w action = 10 −5 .The episode length is 100.One episode terminates upon success or timeout.A.2.2 Reach and GraspIn this task, the robot needs to reach and grasp a We set w distance = 1.0 and w success = 100.0.The episode length is 100.One episode terminates upon success or timeout.A.2.4 ScrewIn this task, the robot is initialized such that its end-effector is close to an inserted table leg.It needs to screw the table leg clockwise into the tabletop.We design curricula over the action space: at the early stage, the robot only controls the end-effector's orientation; at the latter stage, it gradually takes full control.We slightly randomize object and robot poses during initialization.The reward function isHere, 1 f ailure indicates the task failure, w screw is the screwing reward weight, d screw measures the screwed angle, w success is the success weight, and 1 success indicates the task success.The task is considered as successful when the leg has been screwed 180 °into the tabletop.It is considered as failed when the table leg tilts more than 10 °from the vertical pose.We set w screw = 0.1, w success = 100.0,and w deviation = 10 −2 .The episode length is 200.One episode terminates upon success, failure, or timeout.Controller and Action Space An operational space controller (OSC)[73]is used in teacher policy training to improve sample efficiency.We follow Mistry and Righetti[108]to add nullspace control torques to prevent large changes in joint configuration.The action space is thus the change of end-effector's pose.We further add a binary action to control gripper's opening and closing.Formally, it can be expressed as A teacher = (δx, δy, δz, δr, δp, δy, 1 gripper ), where (δx, δy, δz) ∈ R 3 is the translation change, (δr, δp, δy) ∈ R 3 is the rotation change, and 1 gripper ∈ {0, 1} is the gripper action.A.3 TeacherA.3.3 RL Training DetailsWe use the model-free RL algorithm Proximal Policy Optimization (PPO)[81]to learn teacher policies.Hyperparameters are listed in TableA.V. We customize the framework from Makoviichuk and Makoviychuk[110]to use as our training framework.We use four fixed cameras and one wrist camera for point cloud reconstruction.They are three RealSense D435 and two RealSense D415.There is also a 3d-printed three-sided wall glued on top of the table to provide external support.We use a joint position controller from the Deoxys library[115]to control our robot at 1000 Hz.Left CamerasRight Cameras Wrist CameraFixed WallA.3: System setup.Our system consists of a Franka Emika 3 robot mounted on the tabletop, four fixed cameras and one wrist camera (positioned at the rear side of the end-effector) for point cloud reconstruction, and a 3d-printed three-sided wall glued onto tabletop to provide external support.B.2 Obtaining Point Clouds from Multi-View CamerasWe use multi-view cameras for point cloud reconstruction to avoid occlusions.Specifically, we first calibrate all cameras to obtain their poses in the robot base frame.We then transform captured point clouds in camera frames to the robot base frame and concatenate them together.We further perform cropping based on coordinates and remove statistical and radius outliers.To identify points belonging to the gripper so that we can add gripper semantic labels (Sec.A.4.2), we compute poses for two gripper fingers through forward kinematics.We then remove measured points corresponding to gripper fingers through K-nearest neighbor, given fingers' poses and synthetic point clouds.Subsequently, we add semantic labels to points belonging to the scene and synthetic gripper's point clouds.Finally, we uniformly down-sample without replacement.We opt to not use farthest point sampling[116]due to its slow speed.One example is shown inD.2 Distilling Simulation Base Policy with Diffusion PolicyWe experiment with learning simulation base policies (Sec.2.2) with the Diffusion Policy[120].Concretely, when performing action space distillation to learn student policies, we replace the Gaussian Mixture Model (GMM) action head with the Diffusion Policy.Proper data augmentation (TableAThe comparison between GMMs on the real robot is shown in Table.A.XIV.We highlight two findings.First, the significant domain difference between simulation and reality generally exists regardless of different policy parameterizations.Second, since the Diffusion Policy plans and executes a future trajectory, it is more vulnerable to simulation-to-reality gaps due to planning inaccuracy and the consequent compounding error.Only executing the first action from the planned trajectory and re-planning at every step may help, but the inference latency renders the real-time execution infeasible.D.3 Gating Function Justification and Conceptual ComparisonRecall several design choices in the proposed gating mechanism: 1) takes inputs of unstructured sensory observations (point cloud); 2) conditioned on base policy's outputs for effective prediction; 3) the intervention classifier shares the same feature encoder with the residual policy; and 4) the entire pipeline is learned end-to-end.We contrast against several mechanisms from the literature.Although this gating function and residual policy can be merged into one, we opt not to do so for better empirical performance.In the task Screw, using the gating function with the residual policy achieves a success rate of 85%, while only using the residual policy achieves a success rate of 55%.We hypothesize that this is mainly due to the data imbalance-because intervention and correction happen with a low frequency (P correction ≈ 0.20), most residual actions are zero.If we naively learn a residual policy from them, it will be biased toward predicting near-zero actions.On the
On the use of simulation in robotics: Opportunities, challenges, and suggestions for moving forward. H Choi, C Crump, C Duriez, A Elmquist, G Hager, D Han, F Hearl, J Hodgins, A Jain, F Leve, C Li, F Meier, D Negrut, L Righetti, A Rodriguez, J Tan, J Trinkle, 10.1073/pnas.1907856118Proceedings of the National Academy of Sciences. 11812021</p>
<p>The role of physics-based simulators in robotics. Annual Review of Control. C K Liu, D Negrut, 10.1146/annurev-control-072220-093055Robotics, and Autonomous Systems. 412021</p>
<p>Apprenticeship learning via inverse reinforcement learning. P Abbeel, A Y Ng, 10.1145/1015330.1015430Proceedings of the Twenty-First International Conference on Machine Learning, ICML '04. the Twenty-First International Conference on Machine Learning, ICML '04New York, NY, USAAssociation for Computing Machinery20041</p>
<p>Search-based structured prediction. H D Iii, J Langford, D Marcu, arXiv: Arxiv-0907.07862009arXiv preprint</p>
<p>Foundation models for decision making: Problems, methods, and opportunities. S Yang, O Nachum, Y Du, J Wei, P Abbeel, D Schuurmans, arXiv: Arxiv- 2303.041292023arXiv preprint</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 10.1109/IROS.2012.6386109IEEE/RSJ International Conference on Intelligent Robots and Systems. 2012. 2012</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, </p>
<p>Y Zhu, J Wong, A Mandlekar, R Martín-Martín, A Joshi, S Nasiriany, Y Zhu, arXiv: Arxiv-2009.12293robosuite: A modular simulation framework and benchmark for robot learning. 2020arXiv preprint</p>
<p>V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, G State, arXiv: Arxiv-2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. 2021arXiv preprint</p>
<p>BEHAVIOR-1k: A benchmark for embodied AI with 1,000 everyday activities and realistic simulation. C Li, C Gokmen, G Levine, R Martín-Martín, S Srivastava, C Wang, J Wong, R Zhang, M Lingelbach, J Sun, M Anvari, M Hwang, M Sharma, A Aydin, D Bansal, S Hunter, K.-Y Kim, A Lou, C R Matthews, I Villa-Renteria, J H Tang, C Tang, F Xia, S Savarese, H Gweon, K Liu, J Wu, L Fei-Fei, 6th Annual Conference on Robot Learning. 2022</p>
<p>. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, J Ibarz, B Ichter, A Irpan, T Jackson, S Jesmonth, N J Joshi, R Julian, D Kalashnikov, Y Kuang, I Leal, K.-H Lee, S Levine, Y Lu, U Malla, D Manjunath, I Mordatch, O Nachum, C Parada, J Peralta, E Perez, K Pertsch, J Quiambao, K Rao, M Ryoo, G Salazar, P Sanketi, K Sayed, J Singh, S Sontakke, A Stone, C Tan, H Tran, V Vanhoucke, S Vega, Q Vuong, F Xia, T Xiao, P Xu, S Xu, T Yu, B Zitkovich, Arxiv-2212.068172022Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv</p>
<p>Robocat: A self-improving generalist agent for robotic manipulation. K Bousmalis, G Vezzani, D Rao, C Devin, A X Lee, M Bauza, T Davchev, Y Zhou, A Gupta, A Raju, A Laurens, C Fantacci, V Dalibard, M Zambelli, M Martins, R Pevceviciute, M Blokzijl, M Denil, N Batchelor, T Lampe, E Parisotto, K Żołna, S Reed, S G Colmenarejo, J Scholz, A Abdolmaleki, O Groth, J.-B Regli, O Sushkov, T Rothörl, J E Chen, Y Aytar, D Barker, J Ortiz, M Riedmiller, J T Springenberg, R Hadsell, F Nori, N Heess, arXiv: Arxiv-2306.117062023arXiv preprint</p>
<p>Solving rubik's cube with a robot hand. I Openai, M Akkaya, M Andrychowicz, M Chociej, B Litwin, A Mcgrew, A Petron, M Paino, G Plappert, R Powell, J Ribas, N Schneider, J Tezak, P Tworek, L Welinder, Q Weng, W Yuan, L Zaremba, Zhang, arXiv: Arxiv-1910.071132019arXiv preprint</p>
<p>Visual dexterity: In-hand reorientation of novel and complex object shapes. T Chen, M Tippur, S Wu, V Kumar, E Adelson, P , 10.1126/scirobotics.adc9244Science Robotics. 8842023</p>
<p>Y Chen, C Wang, L Fei-Fei, C K Liu, arXiv: Arxiv-2309.00987Sequential dexterity: Chaining dexterous policies for long-horizon manipulation. 2023arXiv preprint</p>
<p>H Qi, A Kumar, R Calandra, Y Ma, J Malik, arXiv: Arxiv-2210.04887-hand object rotation via rapid motor adaptation. 2022arXiv preprint</p>
<p>H Qi, B Yi, S Suresh, M Lambeta, Y Ma, R Calandra, J Malik, arXiv: Arxiv-2309.09979General in-hand object rotation with vision and touch. 2023arXiv preprint</p>
<p>J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, arXiv: Arxiv- 1804.10332Sim-to-real: Learning agile locomotion for quadruped robots. 2018arXiv preprint</p>
<p>RMA: rapid motor adaptation for legged robots. A Kumar, Z Fu, D Pathak, J Malik, 10.15607/RSS.2021.XVII.011Robotics: Science and Systems XVII, Virtual Event. D A Shell, M Toussaint, M A Hsieh, July 12-16, 20212021</p>
<p>Z Zhuang, Z Fu, J Wang, C Atkeson, S Schwertfeger, C Finn, H Zhao, arXiv: Arxiv-2309.05665Robot parkour learning. 2023arXiv preprint</p>
<p>Neural volumetric memory for visual locomotion control. R Yang, G Yang, X Wang, arXiv: Arxiv-2304.012012023arXiv preprint</p>
<p>Biped dynamic walking using reinforcement learning. H Benbrahim, J A Franklin, 10.1016/S0921-8890(97)00043-2S0921-8890(97)00043-2Robotics and Autonomous Systems. 0921-88902231997Robot Learning: The New Wave</p>
<p>Reinforcement learning-based cascade motion policy design for robust 3d bipedal locomotion. G A Castillo, B Weng, W Zhang, A Hereid, 10.1109/ACCESS.2022.3151771IEEE Access. 1020135-20148, 2022</p>
<p>L Krishna, G A Castillo, U A Mishra, A Hereid, S Kolathaya, arXiv: Arxiv-2109.12665Linear policies are sufficient to realize robust bipedal walking on challenging terrains. 2021arXiv preprint</p>
<p>Blind bipedal stair traversal via sim-to-real reinforcement learning. J Siekmann, K Green, J Warila, A Fern, J Hurst, arXiv: Arxiv-2105.083282021arXiv preprint</p>
<p>Real-world humanoid locomotion with reinforcement learning. I Radosavovic, T Xiao, B Zhang, T Darrell, J Malik, K Sreenath, arXiv: Arxiv-2303.033812023arXiv preprint</p>
<p>Reinforcement learning for versatile, dynamic, and robust bipedal locomotion control. Z Li, X B Peng, P Abbeel, S Levine, G Berseth, K Sreenath, arXiv: Arxiv- 2401.168892024arXiv preprint</p>
<p>Champion-level drone racing using deep reinforcement learning. E Kaufmann, L Bauersfeld, A Loquercio, M Müller, V Koltun, D Scaramuzza, 10.1038/s41586-023-06419-4Nature. 2023</p>
<p>Reaching the limit in autonomous racing: Optimal control versus reinforcement learning. Y Song, A Romero, M Müller, V Koltun, D Scaramuzza, 10.1126/scirobotics.adg1462Science Robotics. 8822023</p>
<p>Planar robot casting with real2sim2real self-supervised learning. V Lim, H Huang, L Y Chen, J Wang, J Ichnowski, D Seita, M Laskey, K Goldberg, arXiv: Arxiv- 2111.048142021arXiv preprint</p>
<p>Learning to grasp the ungraspable with emergent extrinsic dexterity. W Zhou, D Held, of Proceedings of Machine Learning Research. K Liu, D Kulic, J Ichnowski, Auckland, New ZealandPMLRCoRL 2022, 14-18 December 2022. 2022205Conference on Robot Learning</p>
<p>Pre-and post-contact policy decomposition for nonprehensile manipulation with zero-shot sim-to-real transfer. M Kim, J Han, J Kim, B Kim, arXiv: Arxiv- 2309.027542023arXiv preprint</p>
<p>Learning generalizable pivoting skills. X Zhang, S Jain, B Huang, M Tomizuka, D Romeres, 10.1109/ICRA48891.2023.10161271IEEE International Conference on Robotics and Automation, ICRA 2023. London, UKIEEEMay 29 -June 2, 20232023</p>
<p>Reinforcement learning of impedance policies for peg-in-hole tasks: Role of asymmetric matrices. S Kozlovsky, E Newman, M Zacksenhouse, 10.1109/LRA.2022.3191070IEEE Robotics and Automation Letters. 742022</p>
<p>Sim-to-real transfer of bolting tasks with tight tolerance. D Son, H Yang, D Lee, 10.1109/IROS45743.2020.93416442020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2020</p>
<p>Industreal: Transferring contact-rich assembly tasks from simulation to reality. B Tang, M A Lin, I Akinola, A Handa, G S Sukhatme, F Ramos, D Fox, Y S Narang, 10.15607/RSS.2023.XIX.039Robotics: Science and Systems XIX. K E Bekris, K Hauser, S L Herbert, J Yu, Daegu, Republic of KoreaJuly 10-14, 20232023</p>
<p>Efficient sim-to-real transfer of contact-rich manipulation skills with online admittance residual learning. X Zhang, C Wang, L Sun, Z Wu, X Zhu, M Tomizuka, 7th Annual Conference on Robot Learning. 2023</p>
<p>Bridging the sim-to-real gap with dynamic compliance tuning for industrial insertion. X Zhang, M Tomizuka, H Li, arXiv: Arxiv-2311.074992023arXiv preprint</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, arXiv: Arxiv-1810.056872018arXiv preprint</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. N Jakobi, P Husbands, I Harvey, Advances in Artificial Life. F Morán, A Moreno, J J Merelo, P Chacón, Berlin, Heidelberg; Berlin HeidelbergSpringer1995</p>
<p>Towards adapting deep visuomotor representations from simulated to real environments. E Tzeng, C Devin, J Hoffman, C Finn, X Peng, S Levine, K Saenko, T Darrell, ArXiv, abs/1511.071112015</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, S Levine, V Vanhoucke, arXiv: Arxiv-1709.078572017arXiv preprint</p>
<p>Retinagan: An object-aware approach to sim-to-real transfer. D Ho, K Rao, Z Xu, E Jang, M Khansari, Y Bai, arXiv: Arxiv-2011.031482020arXiv preprint</p>
<p>Crossing the reality gap in evolutionary robotics by promoting transferable controllers. S Koos, J.-B Mouret, S Doncieux, 10.1145/1830483.1830505Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation, GECCO '10. the 12th Annual Conference on Genetic and Evolutionary Computation, GECCO '10New York, NY, USAAssociation for Computing Machinery2010</p>
<p>Z Xie, G Berseth, P Clary, J Hurst, M Van De Panne, arXiv: Arxiv-1803.05580Feedback control for cassie with deep reinforcement learning. 2018arXiv preprint</p>
<p>Variable impedance control in end-effector space: An action space for reinforcement learning in contact-rich tasks. R Martín-Martín, M A Lee, R Gardner, S Savarese, J Bohg, A Garg, arXiv: Arxiv-1906.088802019arXiv preprint</p>
<p>J Wong, V Makoviychuk, A Anandkumar, Y Zhu, Oscar, arXiv: Arxiv-2110.00704Data-driven operational space control for adaptive and robust robot manipulation. 2021arXiv preprint</p>
<p>On the role of the action space in robot manipulation learning and sim-to-real transfer. E Aljalbout, F Frank, M Karl, P Van Der Smagt, arXiv: Arxiv-2312.036732023arXiv preprint</p>
<p>Factory: Fast contact for robotic assembly. Y S Narang, K Storey, I Akinola, M Macklin, P Reist, L Wawrzyniak, Y Guo, Á Moravánszky, G State, M Lu, A Handa, D Fox, 10.15607/RSS.2022.XVIII.035Robotics: Science and Systems XVIII. K Hauser, D A Shell, S Huang, New York City, NY, USAJune 27 -July 1, 20222022</p>
<p>L Ljung, 10.1007/978-1-4612-1768-8_11System Identification. Boston, MA1998</p>
<p>P Chang, T Padir, arXiv: Arxiv-2002.02538Sim2real2sim: Bridging the gap between simulation and real-world in flexible object manipulation. 2020arXiv preprint</p>
<p>X B Peng, M Andrychowicz, W Zaremba, P Abbeel, arXiv: Arxiv-1710.06537Sim-to-real transfer of robotic control with dynamics randomization. 2017arXiv preprint</p>
<p>Dextreme: Transfer of agile in-hand manipulation from simulation to reality. A Handa, A Allshire, V Makoviychuk, A Petrenko, R Singh, J Liu, D Makoviichuk, K V Wyk, A Zhurkevich, B Sundaralingam, Y S Narang, 10.1109/ICRA48891.2023.10160216IEEE International Conference on Robotics and Automation, ICRA 2023. London, UKIEEEMay 29 -June 2, 20232023</p>
<p>J Wang, Y Qin, K Kuang, Y Korkmaz, A Gurumoorthy, H Su, X Wang, arXiv: Arxiv-2402.14795Cyberdemo: Augmenting simulated human demonstration for real-world dexterous manipulation. 2024arXiv preprint</p>
<p>ACDC: Automated creation of digital cousins for robust policy learning. T Dai, J Wong, Y Jiang, C Wang, C Gokmen, R Zhang, J Wu, L Fei-Fei, 8th Annual Conference on Robot Learning. 2024</p>
<p>Meta-reinforcement learning for robotic industrial insertion tasks. G Schoettler, A Nair, J A Ojea, S Levine, E Solowjow, arXiv: Arxiv-2004.144042020arXiv preprint</p>
<p>Cherry-Picking with Reinforcement Learning. Y Zhang, L Ke, A Deshpande, A Gupta, S Srinivasa, 10.15607/RSS.2023.XIX.021Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsDaegu, Republic of KoreaJuly 2023</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N D Ratliff, D Fox, 10.1109/ICRA.2019.8793789International Conference on Robotics and Automation, ICRA 2019. Montreal, QC, CanadaIEEEMay 20-24, 2019. 2019</p>
<p>Grounded action transformation for robot learning in simulation. J P Hanna, P Stone, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17. the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17AAAI Press2017</p>
<p>E Heiden, D Millard, E Coumans, G S Sukhatme, arXiv: Arxiv-2007.06045Augmenting differentiable simulators with neural networks to close the sim2real gap. 2020arXiv preprint</p>
<p>C A Cruz, T Igarashi, arXiv: Arxiv-2105.12949A survey on interactive reinforcement learning: Design principles and open challenges. 2021arXiv preprint</p>
<p>Understanding the relationship between interactions and outcomes in human-in-the-loop machine learning. Y Cui, P Koppol, H Admoni, S Niekum, R Simmons, A Steinfeld, T Fitzgerald, 10.24963/ijcai.2021/599Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. Z.-H Zhou, the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-2182021</p>
<p>Leveraging human guidance for deep reinforcement learning tasks. R Zhang, F Torabi, L Guan, D H Ballard, P Stone, arXiv: Arxiv-1909.099062019arXiv preprint</p>
<p>S Javdani, S S Srinivasa, J A Bagnell, arXiv: Arxiv-1503.07619Shared autonomy via hindsight optimization. 2015arXiv preprint</p>
<p>S Reddy, A D Dragan, S Levine, arXiv: Arxiv-1802.01744Shared autonomy via deep reinforcement learning. 2018arXiv preprint</p>
<p>Hg-dagger: Interactive imitation learning with human experts. M Kelly, C Sidrane, K Driggs-Campbell, M J Kochenderfer, arXiv: Arxiv-1810.028902018arXiv preprint</p>
<p>Human-in-theloop imitation learning using remote teleoperation. A Mandlekar, D Xu, R Martín-Martín, Y Zhu, L Fei-Fei, S Savarese, arXiv: Arxiv-2012.067332020arXiv preprint</p>
<p>A Mandlekar, D Xu, J Wong, S Nasiriany, C Wang, R Kulkarni, L Fei-Fei, S Savarese, Y Zhu, R Martín-Martín, arXiv: Arxiv-2108.03298What matters in learning from offline human demonstrations for robot manipulation. 2021arXiv preprint</p>
<p>Offline reinforcement learning with implicit q-learning. I Kostrikov, A Nair, S Levine, arXiv: Arxiv-2110.061692021arXiv preprint</p>
<p>Reinforcement and imitation learning for diverse visuomotor skills. Y Zhu, Z Wang, J Merel, A Rusu, T Erez, S Cabi, S Tunyasuvunakool, J Kramár, R Hadsell, N Freitas, N Heess, arXiv: Arxiv-1802.095642018arXiv preprint</p>
<p>Decomposing the generalization gap in imitation learning for visual robotic manipulation. A Xie, L Lee, T Xiao, C Finn, arXiv: Arxiv-2307.036592023arXiv preprint</p>
<p>Dexpoint: Generalizable point cloud reinforcement learning for sim-to-real dexterous manipulation. Y Qin, B Huang, Z.-H Yin, H Su, X Wang, arXiv: Arxiv- 2211.094232022arXiv preprint</p>
<p>A unified approach for motion and force control of robot manipulators: The operational space formulation. O Khatib, 10.1109/JRA.1987.1087068IEEE Journal on Robotics and Automation. 311987</p>
<p>Operational space control: A theoretical and empirical comparison. J Nakanishi, R Cory, M Mistry, J Peters, S Schaal, 10.1177/0278364908091463The International Journal of Robotics Research. 2762008</p>
<p>. A A Rusu, S G Colmenarejo, C Gulcehre, G Desjardins, J Kirkpatrick, R Pascanu, V Mnih, K Kavukcuoglu, R Hadsell, arXiv: Arxiv- 1511.062952015Policy distillation. arXiv preprint</p>
<p>T Chen, M Tippur, S Wu, V Kumar, E Adelson, P , arXiv: Arxiv-2211.11744Visual dexterity: In-hand dexterous manipulation from depth. 2022arXiv preprint</p>
<p>A system for general in-hand object re-orientation. T Chen, J Xu, P , of Proceedings of Machine Learning Research. A Faust, D Hsu, G Neumann, London, UKPMLR8-11 November 2021. 2021164Conference on Robot Learning</p>
<p>Residual reinforcement learning for robot control. T Johannink, S Bahl, A Nair, J Luo, A Kumar, M Loskyll, J A Ojea, E Solowjow, S Levine, arXiv: Arxiv- 1812.032012018arXiv preprint</p>
<p>T Silver, K Allen, J Tenenbaum, L Kaelbling, arXiv: Arxiv-1812.06298Residual policy learning. 2018arXiv preprint</p>
<p>A Zeng, S Song, J Lee, A Rodriguez, T Funkhouser, Tossingbot, arXiv: Arxiv-1903.11239Learning to throw arbitrary objects with residual physics. 2019arXiv preprint</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv: Arxiv-1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Pointnet: Deep learning on point sets for 3d classification and segmentation. C R Qi, H Su, K Mo, L J Guibas, arXiv: Arxiv-1612.005932016arXiv preprint</p>
<p>A Jaegle, F Gimeno, A Brock, A Zisserman, O Vinyals, J Carreira, Perceiver, arXiv: Arxiv-2103.03206General perception with iterative attention. 2021arXiv preprint</p>
<p>Set transformer: A framework for attention-based permutation-invariant neural networks. J Lee, Y Lee, J Kim, A R Kosiorek, S Choi, Y W Teh, arXiv: Arxiv- 1810.008252018arXiv preprint</p>
<p>Furniturebench: Reproducible real-world benchmark for long-horizon complex manipulation. M Heo, Y Lee, D Lee, J J Lim, 10.15607/RSS.2023.XIX.041Robotics: Science and Systems XIX. K E Bekris, K Hauser, S L Herbert, J Yu, Daegu, Republic of KoreaJuly 10-14, 20232023</p>
<p>Alvinn: An autonomous land vehicle in a neural network. D A Pomerleau, Advances in Neural Information Processing Systems. D Touretzky, Morgan-Kaufmann19881</p>
<p>On the similarities and differences among contact models in robot simulation. P C Horak, J C Trinkle, 10.1109/LRA.2019.2891085IEEE Robotics and Automation Letters. 422019</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. C Finn, P Abbeel, S Levine, arXiv: Arxiv-1703.034002017arXiv preprint</p>
<p>Interactive machine learning. J A Fails, D R Olsen, 10.1145/604045.604056Proceedings of the 8th International Conference on Intelligent User Interfaces, IUI '03. the 8th International Conference on Intelligent User Interfaces, IUI '03New York, NY, USAAssociation for Computing Machinery2003</p>
<p>Power to the people: The role of humans in interactive machine learning. S Amershi, M Cakmak, W B Knox, T Kulesza, 10.1609/aimag.v35i4.2513AI Magazine. 354Dec. 2014</p>
<p>Robot learning on the job: Human-inthe-loop autonomy and learning during deployment. H Liu, S Nasiriany, L Zhang, Z Bao, Y Zhu, arXiv: Arxiv-2211.084162022arXiv preprint</p>
<p>Interventional data generation for robust and data-efficient robot imitation learning. R Hoque, A Mandlekar, C R Garrett, K Goldberg, D Fox, First Workshop on Out-of-Distribution Generalization in Robotics at CoRL 2023. 2023</p>
<p>Characterizing efficiency of human robot interaction: a case study of shared-control teleoperation. J Crandall, M Goodrich, 10.1109/IRDS.2002.1043932IEEE/RSJ International Conference on Intelligent Robots and Systems. 20022</p>
<p>A policy-blending formalism for shared control. A D Dragan, S S Srinivasa, 10.1177/0278364913490324The International Journal of Robotics Research. 3272013</p>
<p>Human-in-the-loop optimization of shared autonomy in assistive robotics. D Gopinath, S Jain, B D Argall, 10.1109/LRA.2016.2593928IEEE Robotics and Automation Letters. 212017</p>
<p>Formalizing assistive teleoperation. A D Dragan, S S Srinivasa, July, 2012MIT Press376</p>
<p>Shared autonomy with learned latent actions. H J Jeon, D P Losey, D Sadigh, arXiv: Arxiv-2005.032102020arXiv preprint</p>
<p>no, to the right" -online language corrections for robotic manipulation via shared autonomy. Y Cui, S Karamcheti, R Palleti, N Shivakumar, P Liang, D Sadigh, arXiv: Arxiv-2301.025552023arXiv preprint</p>
<p>P Wu, Y Shentu, Z Yi, X Lin, P Abbeel, arXiv: Arxiv-2309.13037Gello: A general, low-cost, and intuitive teleoperation framework for robot manipulators. 2023arXiv preprint</p>
<p>C Wang, H Shi, W Wang, R Zhang, L Fei-Fei, C K Liu, arXiv: Arxiv-2403.07788Dexcap: Scalable and portable mocap data collection system for dexterous manipulation. 2024arXiv preprint</p>
<p>Z Fu, T Z Zhao, C Finn, arXiv: Arxiv-2401.02117Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. 2024arXiv preprint</p>
<p>S Dass, W Ai, Y Jiang, S Singh, J Hu, R Zhang, P Stone, B Abbatematteo, R Martín-Martín, arXiv: Arxiv-2403.07869Telemoma: A modular and versatile teleoperation system for mobile manipulation. 2024arXiv preprint</p>
<p>T He, Z Luo, W Xiao, C Zhang, K Kitani, C Liu, G Shi, arXiv: Arxiv-2403.04436Learning human-tohumanoid real-time whole-body teleoperation. 2024arXiv preprint</p>
<p>Model-based runtime monitoring with interactive imitation learning. H Liu, S Dass, R Martín-Martín, Y Zhu, arXiv: Arxiv-2310.175522023arXiv preprint</p>
<p>Reflect: Summarizing robot experiences for failure explanation and correction. Z Liu, A Bahety, S Song, arXiv: Arxiv-2306.157242023arXiv preprint</p>
<p>A Mandlekar, S Nasiriany, B Wen, I Akinola, Y Narang, L Fan, Y Zhu, D Fox, arXiv: Arxiv-2310.17596Mimicgen: A data generation system for scalable robot learning using human demonstrations. 2023arXiv preprint</p>
<p>Juicer: Data-efficient imitation learning for robotic assembly. L Ankile, A Simeonov, I Shenfeld, P , arXiv: Arxiv-2404.037292024arXiv preprint</p>
<p>Operational space control of constrained and underactuated systems. M N Mistry, L Righetti, Robotics: Science and Systems. 2011</p>
<p>Fast and accurate deep network learning by exponential linear units (elus). D.-A Clevert, T Unterthiner, S Hochreiter, arXiv: Arxiv-1511.072892015arXiv preprint</p>
<p>rl-games: A high-performance framework for reinforcement learning. D Makoviichuk, V Makoviychuk, May 2021</p>
<p>High-dimensional continuous control using generalized advantage estimation. J Schulman, P Moritz, S Levine, M Jordan, P Abbeel, arXiv: Arxiv-1506.024382015arXiv preprint</p>
<p>D P Kingma, J Ba, arXiv: Arxiv-1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Comput. 0899-766798nov 1997</p>
<p>D Hendrycks, K Gimpel, arXiv: Arxiv-1606.08415Gaussian error linear units (gelus). 2016arXiv preprint</p>
<p>Viola: Imitation learning for vision-based manipulation with object proposal priors. Y Zhu, A Joshi, P Stone, Y Zhu, 6th Annual Conference on Robot Learning. 2022</p>
<p>Pointnet++: Deep hierarchical feature learning on point sets in a metric space. C R Qi, L Yi, H Su, L J Guibas, arXiv: Arxiv-1706.024132017arXiv preprint</p>
<p>I Loshchilov, F Hutter, arXiv: Arxiv-1608.03983Sgdr: Stochastic gradient descent with warm restarts. 2016arXiv preprint</p>
<p>Motion policy networks. A Fishman, A Murali, C Eppner, B Peele, B Boots, D Fox, of Proceedings of Machine Learning Research. K Liu, D Kulic, J Ichnowski, Auckland, New ZealandPMLRCoRL 2022, 14-18 December 2022. 2022205Conference on Robot Learning</p>
<p>R Hoque, A Balakrishna, E Novoseller, A Wilcox, D S Brown, K Goldberg, arXiv: Arxiv-2109.08273Thriftydagger: Budget-aware novelty and risk gating for interactive imitation learning. 2021arXiv preprint</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, S Feng, Y Du, Z Xu, E Cousineau, B Burchfiel, S Song, 10.15607/RSS.2023.XIX.026Robotics: Science and Systems XIX. K E Bekris, K Hauser, S L Herbert, J Yu, Daegu, Republic of KoreaJuly 10-14, 20232023</p>
<p>A Abdolmaleki, J T Springenberg, Y Tassa, R Munos, N Heess, M Riedmiller, arXiv: Arxiv-1806.06920Maximum a posteriori policy optimisation. 2018arXiv preprint</p>
<p>Drake: Model-based design and verification for robotics. R Tedrake, Drake Development Team, 2019</p>
<p>F Xiang, Y Qin, K Mo, Y Xia, H Zhu, F Liu, M Liu, H Jiang, Y Yuan, H Wang, L Yi, A X Chang, L J Guibas, H Su, arXiv: Arxiv-2003.08515Sapien: A simulated part-based interactive environment. 2020arXiv preprint</p>
<p>Orbit: A unified simulation framework for interactive robot learning environments. M Mittal, C Yu, Q Yu, J Liu, N Rudin, D Hoeller, J L Yuan, P P Tehrani, R Singh, Y Guo, H Mazhar, A Mandlekar, B Babich, G State, M Hutter, A Garg, arXiv: Arxiv-2301.041952023arXiv preprint</p>
<p>A Zeng, P Florence, J Tompson, S Welker, J Chien, M Attarian, T Armstrong, I Krasin, D Duong, A Wahid, V Sindhwani, J Lee, arXiv: Arxiv-2010.14406Transporter networks: Rearranging the visual world for robotic manipulation. 2020arXiv preprint</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, arXiv: Arxiv-2109.120982021arXiv preprint</p>
<p>Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, Vima, arXiv: Arxiv-2210.03094General robot manipulation with multimodal prompts. 2022arXiv preprint</p>
<p>M Shridhar, L Manuelli, D Fox, arXiv: Arxiv-2209.05451Perceiver-actor: A multi-task transformer for robotic manipulation. 2022arXiv preprint</p>
<p>D Batra, A X Chang, S Chernova, A J Davison, J Deng, V Koltun, S Levine, J Malik, I Mordatch, R Mottaghi, M Savva, H Su, arXiv: Arxiv-2011.01975Rearrangement: A challenge for embodied ai. 2020arXiv preprint</p>
<p>Multi-skill mobile manipulation for object rearrangement. J Gu, D S Chaplot, H Su, J Malik, arXiv: Arxiv-2209.027782022arXiv preprint</p>
<p>S Yenamandra, A Ramachandran, K Yadav, A Wang, M Khanna, T Gervet, T.-Y Yang, V Jain, A W Clegg, J Turner, Z Kira, M Savva, A Chang, D S Chaplot, D Batra, R Mottaghi, Y Bisk, C Paxton, Homerobot, arXiv: Arxiv-2306.11565Open-vocabulary mobile manipulation. 2023arXiv preprint</p>
<p>Imitating shortest paths in simulation enables effective navigation and manipulation in the real world. K Ehsani, T Gupta, R Hendrix, J Salvador, L Weihs, K.-H Zeng, K P Singh, Y Kim, W Han, A Herrasti, R Krishna, D Schwenk, E Vanderbilt, A Kembhavi, arXiv: Arxiv-2312.029762023arXiv preprint</p>
<p>Y Wu, W Yan, T Kurutach, L Pinto, P Abbeel, arXiv: Arxiv-1910.13439Learning to manipulate deformable objects without demonstrations. 2019arXiv preprint</p>
<p>Flingbot: The unreasonable effectiveness of dynamic manipulation for cloth unfolding. H Ha, S Song, arXiv: Arxiv-2105.036552021arXiv preprint</p>
<p>D Seita, Y Wang, S J Shetty, E Y Li, Z Erickson, D Held, Toolflownet, arXiv: Arxiv-2211.09006Robotic manipulation with tools via predicting tool flow from point clouds. 2022arXiv preprint</p>
<p>X Lin, Z Huang, Y Li, J B Tenenbaum, D Held, C Gan, Diffskill, arXiv: Arxiv-2203.17275Skill abstraction from differentiable physics for deformable object manipulations with tools. 2022arXiv preprint</p>
<p>Hierarchical reinforcement learning for precise soccer shooting skills using a quadrupedal robot. Y Ji, Z Li, Y Sun, X B Peng, S Levine, G Berseth, K Sreenath, arXiv: Arxiv-2208.011602022arXiv preprint</p>
<p>Y J Ma, W Liang, G Wang, D.-A Huang, O Bastani, D Jayaraman, Y Zhu, L Fan, A Anandkumar, arXiv: Arxiv-2310.12931Eureka: Human-level reward design via coding large language models. 2023arXiv preprint</p>
<p>Leveraging multiple simulators for crossing the reality gap. A Boeing, T Bräunl, 10.1109/ICARCV.2012.64853132012 12th International Conference on Control Automation Robotics &amp; Vision (ICARCV). 2012</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, 10.1126/scirobotics.aau5872Science Robotics. 42658722019</p>
<p>Reinforcement learning on variable impedance controller for high-precision robotic assembly. J Luo, E Solowjow, C Wen, J A Ojea, A M Agogino, A Tamar, P Abbeel, 10.1109/ICRA.2019.8793506International Conference on Robotics and Automation, ICRA 2019. Montreal, QC, CanadaIEEEMay 20-24, 2019. 2019</p>
<p>The k-armed dueling bandits problem. Y Yue, J Broder, R Kleinberg, T Joachims, 10.1016/j.jcss.2011.12.028Journal of Computer and System Sciences. 0022-00007852012. 2011Cloud Computing</p>
<p>Learning trajectory preferences for manipulators via iterative improvement. A Jain, B Wojcik, T Joachims, A Saxena, arXiv: Arxiv-130620136294arXiv preprint</p>
<p>P Christiano, J Leike, T B Brown, M Martic, S Legg, D Amodei, arXiv: Arxiv-1706.03741Deep reinforcement learning from human preferences. 2017arXiv preprint</p>
<p>Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences. E Bıyık, D P Losey, M Palan, N C Landolfi, G Shevchuk, D Sadigh, arXiv: Arxiv-2006.140912020arXiv preprint</p>
<p>Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. K Lee, L Smith, P Abbeel, arXiv: Arxiv- 2106.050912021arXiv preprint</p>
<p>Skill preferences: Learning to extract and execute robotic skills from human feedback. X Wang, K Lee, K Hakhamaneshi, P Abbeel, M Laskin, arXiv: Arxiv- 2108.053822021arXiv preprint</p>
<p>L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, arXiv: Arxiv-2203.02155Training language models to follow instructions with human feedback. 2022arXiv preprint</p>
<p>Active reward learning from online preferences. V Myers, E Bıyık, D Sadigh, arXiv: Arxiv-2302.135072023arXiv preprint</p>
<p>R Rafailov, A Sharma, E Mitchell, S Ermon, C D Manning, C Finn, arXiv: Arxiv- 2305.18290Direct preference optimization: Your language model is secretly a reward model. 2023arXiv preprint</p>
<p>J Hejna, R Rafailov, H Sikchi, C Finn, S Niekum, W B Knox, D Sadigh, arXiv: Arxiv- 2310.13639Contrastive preference learning: Learning from human feedback without rl. 2023arXiv preprint</p>
<p>Reinforcement learning from human reward: Discounting in episodic tasks. W B Knox, P Stone, 10.1109/ROMAN.2012.63438622012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication. 2012</p>
<p>Tactile guidance for policy refinement and reuse. B D Argall, E L Sauser, A G Billard, 10.1109/DEVLRN.2010.55788722010 IEEE 9th International Conference on Development and Learning. 2010</p>
<p>International Foundation for Autonomous Agents and Multiagent Systems. T Fitzgerald, E Short, A Goel, A Thomaz, Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS '19. the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS '19Richland, SC2019Human-guided trajectory adaptation for tool transfer. ISBN 9781450363099</p>
<p>Learning robot objectives from physical human interaction. A V Bajcsy, D P Losey, M K O'malley, A D Dragan, Conference on Robot Learning. 2017</p>
<p>Interactively shaping robot behaviour with unlabeled human instructions. A Najar, O Sigaud, M Chetouani, arXiv: Arxiv-1902.016702019arXiv preprint</p>
<p>Learning reward functions from scale feedback. N Wilde, E Bıyık, D Sadigh, S L Smith, arXiv: Arxiv-2110.002842021arXiv preprint</p>
<p>Query-efficient imitation learning for end-to-end autonomous driving. J Zhang, K Cho, arXiv: Arxiv-1605.064502016arXiv preprint</p>
<p>Trial without error: Towards safe reinforcement learning via human intervention. W Saunders, G Sastry, A Stuhlmueller, O Evans, arXiv: Arxiv-1707.051732017arXiv preprint</p>
<p>Z Wang, X Xiao, B Liu, G Warnell, P Stone, Appli, arXiv: Arxiv-2011.00400Adaptive planner parameter learning from interventions. 2020arXiv preprint</p>
<p>An interactive framework for learning continuous actions policies based on corrective feedback. C Celemin, J R Del Solar, 10.1007/s10846-018-0839-z/fulltext.htmlJournal of Intelligent &amp; Robotic Systems. 952018</p>
<p>Learning from active human involvement through proxy value propagation. Z Peng, W Mo, C Duan, Q Li, B Zhou, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>C Celemin, R Pérez-Dattari, E Chisari, G Franzese, L De Souza Rosa, R Prakash, Z Ajanović, M Ferraz, A Valada, J Kober, arXiv: Arxiv-2211.00600Interactive imitation learning in robotics: A survey. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
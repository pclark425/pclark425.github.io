<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4795 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4795</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4795</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-6c22a8153daf6a236a4a4740c3b57bcf0bea5486</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6c22a8153daf6a236a4a4740c3b57bcf0bea5486" target="_blank">DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following</a></p>
                <p><strong>Paper Venue:</strong> IEEE Robotics and Automation Letters</p>
                <p><strong>Paper TL;DR:</strong> This work presents DialFRED, a dialogue-enabled embodied instruction following benchmark based on the ALFRED benchmark, and proposes a questioner-performer framework wherein the questioner is pre-trained with the human-annotated data and fine-tuned with reinforcement learning.</p>
                <p><strong>Paper Abstract:</strong> Language-guided Embodied AI benchmarks requiring an agent to navigate an environment and manipulate objects typically allow one-way communication: the human user gives a natural language command to the agent, and the agent can only follow the command passively. We present DialFRED, a dialogue-enabled embodied instruction following benchmark based on the ALFRED benchmark. DialFRED allows an agent to actively ask questions to the human user; the additional information in the user's response is used by the agent to better complete its task. We release a human-annotated dataset with 53 K task-relevant questions and answers and an oracle to answer questions. To tackle DialFRED, we propose a questioner-performer framework wherein the questioner is pre-trained with the human-annotated data and fine-tuned with reinforcement learning. Experimental results show that asking the right questions leads to significantly improved task performance. We make DialFRED publicly available and encourage researchers to propose and evaluate their solutions to building dialog-enabled embodied agents: https://github.com/xfgao/DialFRED.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4795.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4795.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Performer (Episodic Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performer — Episodic Transformer-based action predictor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The performer is an attention-based multi-layer Episodic Transformer that encodes the full history (instructions, QA dialog, visual observations, action history) and predicts low-level actions to complete embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Episodic transformer for vision-and-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Performer (Episodic Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An Episodic Transformer that takes as input the instruction, dialog (questions+answers), a sequence of visual observations and past actions, and predicts the next action(s) for embodied instruction following; used as the action-execution component in all baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic / history buffer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Encodes and attends over the full task history: the step-by-step instruction, any question-answer pairs (dialog history), sequential visual features and past actions — i.e., an explicit episodic history used to condition future action predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dialogue-enabled embodied instruction following (sub-goal level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an initial environment state and a (possibly ambiguous) instruction, the agent must navigate and manipulate objects (multi-step, long-horizon) to transform the environment to the target state; the agent may receive or ask QA dialog to disambiguate or obtain missing information.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>DialFRED (augmented ALFRED)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>With QA dialog provided to the performer at task start (All QAs): Seen Success Rate 43.4%, Unseen Success Rate 32.0%, Seen PWSR 31.2%, Unseen PWSR 19.9% (Table I, baseline 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Instruction only (no questions/QAs provided): Seen Success Rate 25.4%, Unseen Success Rate 18.3%, Seen PWSR 18.4%, Unseen PWSR 11.4% (Table I, baseline 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Providing dialog (QA) history to the Episodic Transformer substantially improved task performance: +18.0 percentage points SR on seen (25.4 -> 43.4) and +13.7 points on unseen (18.3 -> 32.0); PWSR increased similarly, indicating that access to QA/history materially helps multi-step embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance depends on quality and availability of QA answers (oracle/human); perturbing oracle answers (making answers missing 50% of time) reduces effectiveness, particularly when location answers are missing. The episodic model and QA inputs can overfit to seen scenes (better gains on seen than unseen). Also adding dialog increases interaction cost (questions asked) and requires an answering mechanism (oracle or human).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Encoding and attending over an episodic history (instructions + dialog + observations + action history) significantly improves multi-step embodied task completion. Dialog-provided facts (especially location information) are among the most useful pieces of memory to ground actions; access to accurate QA history yields large SR gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4795.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4795.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Questioner (Seq2Seq LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Questioner — Sequence-to-sequence LSTM question generation policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence LSTM that generates task-relevant clarification questions (type + object) conditioned on the instruction and visual observation, pre-trained on human QA data and fine-tuned with reinforcement learning to decide when and what to ask.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Questioner (Seq2Seq LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LSTM encoder-decoder with attention that produces a compact question token pair (question type and target object) or 'none' for no-question; pre-trained on 53K human QA annotations and fine-tuned with RL to trade off number of questions vs task success.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dialog / short-term contextual memory (history of prior QAs and instruction state)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Maintains and conditions on prior dialog (asked questions and received answers) as part of the state when deciding subsequent questions; when allowed to ask mid-task, it uses the latest performer rollout and accumulated QAs to form the next question.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>When and what to ask during embodied instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate clarification questions that, after receiving answers, improve the performer's ability to complete long-horizon navigation and manipulation sub-goals in DialFRED.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>DialFRED (augmented ALFRED)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>RL anytime (questioner fine-tuned with RL, can ask mid-task): Seen SR 47.8%, Unseen SR 33.6%, Seen PWSR 34.2%, Unseen PWSR 20.4%, average NQ 0.71 (Table I, baseline 6). A more permissive timing (Fixed 1) increases Seen SR to 51.9% but with many more questions (NQ 21.39) (Table III).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Instruction only baseline (no questions allowed): Seen SR 25.4%, Unseen SR 18.3% (Table I, baseline 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>A learned questioner that uses dialog history and can ask during execution (RL anytime) outperforms the no-question baseline by large margins (e.g., Seen SR +22.4 points) and outperforms simply providing all QAs at start (All QAs: seen SR 43.4%) — showing that active question generation + use of answers as memory is more effective and more sample-efficient than static upfront information in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Trade-off between number of questions and gains: aggressive questioning (many questions) can further improve SR but is costly; question usefulness depends on oracle/human answer availability and accuracy. The questioner must learn when to avoid invalid/unanswerable questions (penalized). Generalization to unseen environments is weaker than seen. RL fine-tuning requires a simulator/oracle to provide answers during training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Pretraining on human QA data followed by RL fine-tuning to optimize task-level reward yields an agent that asks fewer but more useful questions; timing matters (mid-task questions help more than only-beginning questions), and location-type answers are particularly valuable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4795.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4795.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle (scene-grounded answerer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle — scene metadata grounded answer generator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A templated answer generator that uses ground-truth scene metadata (object positions, materials, pixel colors, agent goal positions) to answer predefined question types (location, appearance, direction) for training/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Oracle (scene-grounded answerer)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A non-learned component with access to environment ground-truth: computes relative object directions and containers for location answers, extracts color/material metadata for appearance, and computes end-location relative directions for direction questions; outputs template language answers.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>ground-truth environment state (simulated persistent memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Acts as an authoritative memory source by reading the simulator's scene graph/metadata and generating natural-language templated answers that serve as factual memory for the agent (i.e., provides reliable external facts on demand).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Answering task-relevant questions for embodied agents during DialFRED tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide accurate answers to location, appearance, and direction questions posed by the questioner, based on simulator ground-truth, to enable the performer to ground instructions and plan actions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>DialFRED (augmented ALFRED)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When oracle answers are fully available, question-enabled systems show large SR gains vs no-questions (e.g., All QAs baseline: Seen SR 43.4% vs Instruction only 25.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>When oracle answers are partially removed/perturbed (oracle answers withheld 50% for one question type), task success drops — perturbing location answers caused the largest SR drop (Table II: RL anytime original Unseen SR 33.6% vs RL Loc perturb Unseen SR 32.2% — drop present; larger relative effects reported across ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The presence of an accurate scene-grounded oracle (persistent environment-memory) substantially increases agent performance. Ablations that withhold answers for certain question types (particularly location) cause noticeable performance degradation and the learned questioner adapts by asking other question types but cannot fully recover the lost utility of missing location answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Oracle is a simulator-specific privileged component (requires ground-truth access) and is not available in real-world deployment without equivalent perception/SLAM capabilities; performance depends on oracle coverage (some question types can be invalid/unanswerable) and the system needs to handle missing/invalid answers. Reliance on an oracle may mask perception challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Episodic transformer for vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>Vision-and-dialog navigation <em>(Rating: 2)</em></li>
                <li>Just ask: An interactive learning framework for vision and language navigation <em>(Rating: 2)</em></li>
                <li>TEACh: Task-driven Embodied Agents that Chat <em>(Rating: 2)</em></li>
                <li>Embodied Question Answering <em>(Rating: 2)</em></li>
                <li>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks <em>(Rating: 2)</em></li>
                <li>RMM: A recursive mental model for dialog navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4795",
    "paper_id": "paper-6c22a8153daf6a236a4a4740c3b57bcf0bea5486",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "Performer (Episodic Transformer)",
            "name_full": "Performer — Episodic Transformer-based action predictor",
            "brief_description": "The performer is an attention-based multi-layer Episodic Transformer that encodes the full history (instructions, QA dialog, visual observations, action history) and predicts low-level actions to complete embodied tasks.",
            "citation_title": "Episodic transformer for vision-and-language navigation",
            "mention_or_use": "use",
            "agent_name": "Performer (Episodic Transformer)",
            "agent_description": "An Episodic Transformer that takes as input the instruction, dialog (questions+answers), a sequence of visual observations and past actions, and predicts the next action(s) for embodied instruction following; used as the action-execution component in all baselines.",
            "memory_type": "episodic / history buffer",
            "memory_description": "Encodes and attends over the full task history: the step-by-step instruction, any question-answer pairs (dialog history), sequential visual features and past actions — i.e., an explicit episodic history used to condition future action predictions.",
            "task_name": "Dialogue-enabled embodied instruction following (sub-goal level)",
            "task_description": "Given an initial environment state and a (possibly ambiguous) instruction, the agent must navigate and manipulate objects (multi-step, long-horizon) to transform the environment to the target state; the agent may receive or ask QA dialog to disambiguate or obtain missing information.",
            "benchmark_name": "DialFRED (augmented ALFRED)",
            "performance_with_memory": "With QA dialog provided to the performer at task start (All QAs): Seen Success Rate 43.4%, Unseen Success Rate 32.0%, Seen PWSR 31.2%, Unseen PWSR 19.9% (Table I, baseline 2).",
            "performance_without_memory": "Instruction only (no questions/QAs provided): Seen Success Rate 25.4%, Unseen Success Rate 18.3%, Seen PWSR 18.4%, Unseen PWSR 11.4% (Table I, baseline 1).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Providing dialog (QA) history to the Episodic Transformer substantially improved task performance: +18.0 percentage points SR on seen (25.4 -&gt; 43.4) and +13.7 points on unseen (18.3 -&gt; 32.0); PWSR increased similarly, indicating that access to QA/history materially helps multi-step embodied tasks.",
            "limitations_or_challenges": "Performance depends on quality and availability of QA answers (oracle/human); perturbing oracle answers (making answers missing 50% of time) reduces effectiveness, particularly when location answers are missing. The episodic model and QA inputs can overfit to seen scenes (better gains on seen than unseen). Also adding dialog increases interaction cost (questions asked) and requires an answering mechanism (oracle or human).",
            "key_insights": "Encoding and attending over an episodic history (instructions + dialog + observations + action history) significantly improves multi-step embodied task completion. Dialog-provided facts (especially location information) are among the most useful pieces of memory to ground actions; access to accurate QA history yields large SR gains.",
            "uuid": "e4795.0",
            "source_info": {
                "paper_title": "DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Questioner (Seq2Seq LSTM)",
            "name_full": "Questioner — Sequence-to-sequence LSTM question generation policy",
            "brief_description": "A sequence-to-sequence LSTM that generates task-relevant clarification questions (type + object) conditioned on the instruction and visual observation, pre-trained on human QA data and fine-tuned with reinforcement learning to decide when and what to ask.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Questioner (Seq2Seq LSTM)",
            "agent_description": "An LSTM encoder-decoder with attention that produces a compact question token pair (question type and target object) or 'none' for no-question; pre-trained on 53K human QA annotations and fine-tuned with RL to trade off number of questions vs task success.",
            "memory_type": "dialog / short-term contextual memory (history of prior QAs and instruction state)",
            "memory_description": "Maintains and conditions on prior dialog (asked questions and received answers) as part of the state when deciding subsequent questions; when allowed to ask mid-task, it uses the latest performer rollout and accumulated QAs to form the next question.",
            "task_name": "When and what to ask during embodied instruction following",
            "task_description": "Generate clarification questions that, after receiving answers, improve the performer's ability to complete long-horizon navigation and manipulation sub-goals in DialFRED.",
            "benchmark_name": "DialFRED (augmented ALFRED)",
            "performance_with_memory": "RL anytime (questioner fine-tuned with RL, can ask mid-task): Seen SR 47.8%, Unseen SR 33.6%, Seen PWSR 34.2%, Unseen PWSR 20.4%, average NQ 0.71 (Table I, baseline 6). A more permissive timing (Fixed 1) increases Seen SR to 51.9% but with many more questions (NQ 21.39) (Table III).",
            "performance_without_memory": "Instruction only baseline (no questions allowed): Seen SR 25.4%, Unseen SR 18.3% (Table I, baseline 1).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "A learned questioner that uses dialog history and can ask during execution (RL anytime) outperforms the no-question baseline by large margins (e.g., Seen SR +22.4 points) and outperforms simply providing all QAs at start (All QAs: seen SR 43.4%) — showing that active question generation + use of answers as memory is more effective and more sample-efficient than static upfront information in many settings.",
            "limitations_or_challenges": "Trade-off between number of questions and gains: aggressive questioning (many questions) can further improve SR but is costly; question usefulness depends on oracle/human answer availability and accuracy. The questioner must learn when to avoid invalid/unanswerable questions (penalized). Generalization to unseen environments is weaker than seen. RL fine-tuning requires a simulator/oracle to provide answers during training.",
            "key_insights": "Pretraining on human QA data followed by RL fine-tuning to optimize task-level reward yields an agent that asks fewer but more useful questions; timing matters (mid-task questions help more than only-beginning questions), and location-type answers are particularly valuable.",
            "uuid": "e4795.1",
            "source_info": {
                "paper_title": "DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Oracle (scene-grounded answerer)",
            "name_full": "Oracle — scene metadata grounded answer generator",
            "brief_description": "A templated answer generator that uses ground-truth scene metadata (object positions, materials, pixel colors, agent goal positions) to answer predefined question types (location, appearance, direction) for training/evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Oracle (scene-grounded answerer)",
            "agent_description": "A non-learned component with access to environment ground-truth: computes relative object directions and containers for location answers, extracts color/material metadata for appearance, and computes end-location relative directions for direction questions; outputs template language answers.",
            "memory_type": "ground-truth environment state (simulated persistent memory)",
            "memory_description": "Acts as an authoritative memory source by reading the simulator's scene graph/metadata and generating natural-language templated answers that serve as factual memory for the agent (i.e., provides reliable external facts on demand).",
            "task_name": "Answering task-relevant questions for embodied agents during DialFRED tasks",
            "task_description": "Provide accurate answers to location, appearance, and direction questions posed by the questioner, based on simulator ground-truth, to enable the performer to ground instructions and plan actions.",
            "benchmark_name": "DialFRED (augmented ALFRED)",
            "performance_with_memory": "When oracle answers are fully available, question-enabled systems show large SR gains vs no-questions (e.g., All QAs baseline: Seen SR 43.4% vs Instruction only 25.4%).",
            "performance_without_memory": "When oracle answers are partially removed/perturbed (oracle answers withheld 50% for one question type), task success drops — perturbing location answers caused the largest SR drop (Table II: RL anytime original Unseen SR 33.6% vs RL Loc perturb Unseen SR 32.2% — drop present; larger relative effects reported across ablations).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "The presence of an accurate scene-grounded oracle (persistent environment-memory) substantially increases agent performance. Ablations that withhold answers for certain question types (particularly location) cause noticeable performance degradation and the learned questioner adapts by asking other question types but cannot fully recover the lost utility of missing location answers.",
            "limitations_or_challenges": "Oracle is a simulator-specific privileged component (requires ground-truth access) and is not available in real-world deployment without equivalent perception/SLAM capabilities; performance depends on oracle coverage (some question types can be invalid/unanswerable) and the system needs to handle missing/invalid answers. Reliance on an oracle may mask perception challenges.",
            "uuid": "e4795.2",
            "source_info": {
                "paper_title": "DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Episodic transformer for vision-and-language navigation",
            "rating": 2
        },
        {
            "paper_title": "Vision-and-dialog navigation",
            "rating": 2
        },
        {
            "paper_title": "Just ask: An interactive learning framework for vision and language navigation",
            "rating": 2
        },
        {
            "paper_title": "TEACh: Task-driven Embodied Agents that Chat",
            "rating": 2
        },
        {
            "paper_title": "Embodied Question Answering",
            "rating": 2
        },
        {
            "paper_title": "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks",
            "rating": 2
        },
        {
            "paper_title": "RMM: A recursive mental model for dialog navigation",
            "rating": 1
        }
    ],
    "cost": 0.0122415,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following</h1>
<p>Xiaofeng Gao<sup>1</sup>, Qiaozi Gao<sup>2</sup>, Ran Gong<sup>1</sup>, Kaixiang Lin<sup>2</sup>, Govind Thattai<sup>2</sup>, Gaurav S. Sukhatme<sup>3</sup></p>
<p><strong>Abstract</strong>—Language-guided Embodied AI benchmarks requiring an agent to navigate an environment and manipulate objects typically allow one-way communication: the human user gives a natural language command to the agent, and the agent can only follow the command passively. We present DialFRED, a dialogue-enabled embodied instruction following benchmark based on the ALFRED benchmark. DialFRED allows an agent to actively ask questions to the human user; the additional information in the user's response is used by the agent to better complete its task. We release a human-annotated dataset with 53K task-relevant questions and answers and an oracle to answer questions. To tackle DialFRED, we propose a questioner-performer framework wherein the questioner is pre-trained with the human-annotated data and fine-tuned with reinforcement learning. Experimental results show that asking the right questions leads to significantly improved task performance. We make DialFRED publicly available and encourage researchers to propose and evaluate their solutions to building dialog-enabled embodied agents: https://github.com/xfgao/DialFRED</p>
<p><strong>Index Terms</strong>—Natural Dialog for HRI, Multi-Modal Perception for HRI, Human-Robot Collaboration</p>
<h2>I. Introduction</h2>
<p>Robot assistants need to understand natural language and interact with the environment. To help build language-driven embodied agents, various benchmarks based on predefined tasks, datasets and evaluation metrics have been proposed [1], [2], allowing fair comparisons between different approaches. In these benchmarks, the agent is often given an instruction, following which it is supposed to execute the corresponding sequence of actions. Even with natural language instructions, such tasks are often overwhelming for the agent <em>on its own</em> due to two major challenges: 1) resolving ambiguities in natural language and grounding instructions to actions in a rich environment, and 2) planning for long-horizon action sequences and recovering from possible failures.</p>
<p>Humans, faced with inadequate information for a task, seek assistance from others. Similarly, embodied agents should be able to actively ask questions to humans, and utilize the verbal</p>
<p>Manuscript received: February 24, 2022; Revised June 6, 2022; Accepted July 8, 2022.</p>
<p>This paper was recommended for publication by Editor Gentiane Venture upon evaluation of the Associate Editor and Reviewers' comments. This work was supported by Amazon Alexa AI.</p>
<p><sup>1</sup>Xiaofeng Gao and Ran Gong are with the Center for Vision, Cognition, Learning, and Autonomy, UCLA. {xfgao, nikepupu}@ucla.edu</p>
<p><sup>2</sup>Qiaozi Gao, Kaixiang Lin and Govind Thattai are with Amazon Alexa AI. {qzgao, kaixianl, thattg}@amazon.com</p>
<p><sup>3</sup>Gaurav S. Sukhatme is with Amazon Alexa AI and the Department of Computer Science, USC Viterbi School of Engineering. sukhatme@amazon.com, gaurav@usc.edu</p>
<p>Digital Object Identifier (DOI): see top of this page.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p><strong>Fig. 1:</strong> Example dialogue between a robot and a human user during task completion. The robot raises questions to obtain additional information (e.g., when the target location is not clear) and to resolve ambiguities (e.g., when facing two knives on the table).</p>
<p>response to overcome challenges in understanding intent and task execution. For example, to deal with ambiguity in human instruction, clarifications are often necessary. As shown in Figure 1, the instruction "pick up the knife," is ambiguous when there are two knives in front of the robot – knowing the color of the intended knife helps the agent ground the instruction to its environment.</p>
<p>We present DialFRED, an embodied instruction following benchmark allowing an agent to 1) actively ask questions to the human user, and 2) use the information in the response to better complete the task. DialFRED is built by augmenting ALFRED [2], an existing benchmark that pairs demonstrations of common household tasks with instructions. ALFRED language instructions are given as high level goals, e.g., <em>Move a knife to the sink</em>, and a sequence of step-by-step instructions (sub-goals), e.g., <em>Move forward to the center table</em>, <em>Pick up the knife</em>, <em>Walk to the sink</em>, <em>Put the knife in the sink</em>. ALFRED only contains 7 types of high level goals and 8 types of sub-goals. Existing work [3], [4] has exploited patterns in ALFRED task structures, and shown that models can achieve state-of-the-art performance by classifying the task type from high-level task instructions alone, even without using step-by-step instructions. To mitigate this issue and ensure the necessity of instruction following, we build DialFRED by augmenting ALFRED for an increased number of task types. In addition, DialFRED facilitates agent-human dialogue by providing human-annotated task-relevant questions and answers.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: The annotation interface for hybrid data collection. The worker first clicks the "begin" button to watch a video clip showing the initial states of the environment. Given the instruction, the worker selects a question to help perform the task. Next, the worker clicks the "show demonstration" button to watch the expert demonstration on how to complete the task. The worker then answers their own question based on what they have learned from the videos. Finally, workers choose whether they think the questions and answers are necessary to help the agent carry out the command.</p>
<p>Contributions. To enable the development and evaluation of dialogue-enabled agents in complex manipulation and navigation tasks, DialFRED consists of a) 25 types of sub-goal level tasks, compared to 8 sub-goals originally available in ALFRED; b) 53K human-annotated task-relevant questions and answers; and c) models for a questioner-performer framework showing that adding dialogue helps to significantly improve the instruction following performance. We make DialFRED publicly available and encourage researchers from related robotics disciplines to propose and evaluate their solutions to dialog-enabled embodied agents.</p>
<h2>II. Related Work</h2>
<p>Embodied Question Answering and Instruction Following. Various virtual environments have been developed to accommodate robot agents completing common household tasks [5], [6], [7]. Building on these environments, tasks and benchmarks that require an interactive agent to extract information from the environment to answer specific questions have been proposed [8], [9]. The other line of work that uses these environments focuses on creating agents to interpret natural language instructions and perform tasks in the environment [1]. ALFRED [2], a recently proposed benchmark along this direction, requires the agent to complete complex household tasks by following natural language instructions. Dialogue-enabled agents in navigation or manipulation tasks have recently been proposed [10], [11] – these focus on action prediction from dialogue history, and do not emphasize the agent's ability to ask clarification questions for better instruction grounding [12]. In this paper, we take a further step in this direction by presenting a benchmark for the agent to actively ask questions and learn from the answers to better finish tasks that require navigation and object interactions. We refer readers to this survey [13] for dataset comparison.</p>
<p>Task-Oriented Dialogue. In task-oriented dialogue, agents rely on skills beyond language modeling (e.g., processing multi-modal sensory data, querying knowledge bases, reasoning based on observations and knowledge [14], [15], [16], [17]). Towards building robust dialogue systems, both data-driven [18], [19] and reinforcement learning approaches [20], [21], [22] have been studied. Studies have shown that the ability to ask for help from humans is crucial for agent failure recovery [23]. For visual language navigation, multiple works study when and how to ask for help [24], [25], [26], [27]. Household tasks however, pose greater challenges to agents compared to navigation-only tasks, due to longer action sequences, compositional task structures and irreversible object state changes. Our benchmark focuses on these complex household tasks requiring both navigation and manipulation.</p>
<h2>III. TASK AND DATASET</h2>
<p>DialFRED requires an agent to follow natural language instructions and perform navigation and object manipulation to finish a household task in a virtual environment. We further enable the agent to ask questions, and use the extra information in the responses to better complete the task.</p>
<p>Each task instance in DialFRED is a tuple of the initial environment state, the target environment state and an instruction. The agent's goal is to perform a sequence of actions to change the environment states to the target. Given a natural language instruction, the agent can choose to ask questions to the human, or to execute physical actions in its environment based on the information in the original instructions together with the questions and answers. Physical actions include all 5 navigation actions (e.g., Turn Left) and all 7 manipulation actions (e.g., Pickup). These actions can change environment states, and some of the changes are irreversible (e.g. Slice). Instead of always emitting a physical action, a dialog-enabled agent may emit a question. To standardize this benchmark, we provide an oracle that can answer a set of predefined types of questions (a crude 'simulated' human user). The oracle has access to the ground-truth states of the virtual environment, allowing it to provide accurate information regarding objects and tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tasks</th>
<th style="text-align: left;">Move \&amp; Pick</th>
<th style="text-align: left;">Pick \&amp; Move</th>
<th style="text-align: left;">Pick, Move \&amp; Slice</th>
<th style="text-align: left;">Move \&amp; Open</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Instructions</td>
<td style="text-align: left;">Grab the fork.</td>
<td style="text-align: left;">Take the tissuebox to <br> the floorlamp.</td>
<td style="text-align: left;">Cut the tomato with a <br> knife.</td>
<td style="text-align: left;">Open the microwave.</td>
</tr>
<tr>
<td style="text-align: left;">Expert <br> demonstrations</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Questions</td>
<td style="text-align: left;">Where is the fork?</td>
<td style="text-align: left;">Where is the floorlamp?</td>
<td style="text-align: left;">What does the knife <br> look like?</td>
<td style="text-align: left;">Which direction should I <br> turn to?</td>
</tr>
<tr>
<td style="text-align: left;">Oracle answers</td>
<td style="text-align: left;">The fork is behind you <br> on the countertop.</td>
<td style="text-align: left;">The floorlamp is to your <br> front right.</td>
<td style="text-align: left;">The knife is silver and <br> made of metal.</td>
<td style="text-align: left;">You should turn right.</td>
</tr>
<tr>
<td style="text-align: left;">Human answers</td>
<td style="text-align: left;">It is on the kitchen <br> island behind you.</td>
<td style="text-align: left;">It is in the corner of the <br> room by the window.</td>
<td style="text-align: left;">It is long and silver with <br> a blade.</td>
<td style="text-align: left;">Turn right and you will <br> see the microwave.</td>
</tr>
</tbody>
</table>
<p>Fig. 3: Examples in our QA dataset. We show instructions and questions asked by humans, and answers provided by both the oracle and humans. Compared to step-by-step instructions, our augmented instructions are concise and general, thus requiring the agent to understand its current state to generate the correct action sequences.</p>
<h2>A. Hybrid data collection</h2>
<p>We collect human annotations on Amazon Mechanical Turk for questions and answers. Each instance in the dataset is a tuple of question type $q \in Q$ asking about a specific property of an object $o \in O$ ( $o$ could be empty for questions not related to objects) and a human answer $a \in A$ for the question at the beginning of the task. Figure 2 shows the data collection interface. The hybrid data collection (HDC) process is as follows:</p>
<p>1) Each annotator watches a 10 second video clip, which displays the state of the environment right before the task. Annotators also see the original language instructions for the task.
2) The annotator then selects one pertinent question (from several predefined questions) that they think may help the agent complete the task. The annotator may also type in a question of their own choosing if none of the provided questions are a good fit.
3) The annotator watches a second video clip - this time of an expert agent performing the task.
4) The annotator answers their own question and provides feedback (yes/no) on whether asking the question was necessary in the given scenario.
To generate the predefined question choices for the annotator to choose from, we consider three types of questions $Q$, related to the location and appearance of the query object $o$ that needs to be interacted with to finish the task, and the relative direction between the agent's current position and the target position to guide the navigation:
5) Location: where is $o$ ?
6) Appearance: what does $o$ look like?
7) Direction: which direction should I turn to?</p>
<p>Given a natural language instruction (e.g. Put the egg in the microwave), we parse it and extract all the nouns (e.g. egg and microwave). We insert each noun into one of the question templates to generate questions (e.g. Where is the egg? and</p>
<p>What does the microwave look like?).
We collected human questions and answers for 29,376 sub-goals (e.g., Take the knife to the counter) on Amazon Mechanical Turk via crowd-sourcing. For each sub-goal, we ask two different annotators to provide questions and answers. And we see a modest level of agreement in question selection between different annotators (Fleiss' $\boldsymbol{x}=0.13$ ). To ensure the quality of the dataset, we first remove invalid annotations when the annotation time is less than 15 seconds. A worker is compensated $\$ 0.25$ for each Human Intelligence Task (HIT); the dataset collection cost is $\sim \$ 10 \mathrm{~K}$. The dataset is gathered over 112 rooms and 80 types of objects. Each human answer contains 6.73 words on average. A lexical complexity analysis on the human answers [28] shows that the number of different words (NDW) in the answers are 7915. The lexical sophistication (the proportion of words not in the 2000 most frequent words in the American National Corpus) is $49 \%$. Example human questions and answers are shown in Figure 3.</p>
<h2>B. Generating answers</h2>
<p>In addition to the human answers, we build an oracle that provides templated answers which can be easier for the agent to understand. In Figure 3, we show answers generated by the oracle for some task examples. To create the oracle, we take advantage of the ground-truth states of objects and the agent in the simulated environment: (i) to answer object location questions, we compute the direction of the object relative to the agent, and the receptacle that contains the object; (ii) to answer object appearance questions, we focus on its color and material. Object material is extracted from scene metadata in the underlying simulation environment (AI2-Thor). For object color, we extract object pixel RGB values from images, and map them to color names; (iii) for direction questions, we check the agent's location at the end of the task in the groundtruth action sequences, and compare it to the agent's initial location. Based on these metadata, we use language templates to generate answers. Some example templates include:</p>
<ul>
<li>Location: The $o$ is to your [direction] in/on the [container].</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: The questioner-performer architecture. The questioner generates questions based on the first person image of the agent and the task instruction. The oracle answers the question based on the scene metadata. The performer takes the image, the instruction, and question and answers as input to predict actions.</p>
<ul>
<li>Appearance: The o is [color] and made of [material].</li>
<li>Direction: You should turn [direction] / You don't need to move.</li>
</ul>
<h2>C. Data augmentation on ALFRED</h2>
<p>Each task in ALFRED has a goal, split into multiple subgoals. Each sub-goal requires the agent to manipulate some objects or move to a target location. Two types of language instructions are given. The high-level task instruction (e.g., Move a knife to the sink) describes the overall goal. The step-by-step instructions (e.g., Pick up the knife) guide the agent to complete each sub-goal. ALFRED exhibits clear patterns in both high-level task structures and sub-goal action sequences: tasks of the same type require very similar subgoal sequences to complete them, and sub-goals of the same type (especially manipulation sub-goals) have almost fixed action sequences. The limited variety in tasks and sub-goals precludes instructions understanding - allowing models that directly classify the task type from high-level task instructions alone to perform well, without even using the step-by-step instructions [3], [4].</p>
<p>To get rid of the strong patterns in both high-level task structures and sub-goal actions in ALFRED, DialFRED uses data augmentation to increase the number of task types, and focuses on instruction following at the sub-goal level to encourage learning from instructions for each task. We also introduce augmentations on the original ALFRED subgoal instructions to add ambiguities in language so that the sequence of actions cannot be fully determined by only focusing on the instruction - reasoning based on knowledge of the environment state is needed. We show examples for DialFRED tasks and instructions in Figure 3.
Sub-goal augmentations. In ALFRED there are 8 only subgoal types (go to, pick up, put, cool, heat, clean, slice, toggle). The action sequences required to finish these are almost fixed. For example, cool object always corresponds to: open fridge, put object into fridge, close fridge, open fridge, take out object. To increase task variations, we augment the sub-goals in two ways. First, we split an original sub-goal into multiple low
level actions, each action corresponding to a new sub-goal. For example, the sub-goal clean object is split into three subgoals: put the object in the sink, turn on the faucet and turn off the faucet. Second, we merge multiple sub-goals into a new sub-goal. For example, sub-goals go to the fridge and open the fridge are merged into a new sub-goal which requires the agent to first go to the fridge and open it. Using these operations, we arrive at 25 sub-goal types in our augmented dataset. In our experiments, we evaluate agent performance on these sub-goals; henceforth referred to as tasks. To standardize the benchmark, we divide the sub-goal task instances into training and validation folds. We further divide the validation fold into seen and unseen splits depending on whether the environment presents in the training fold. This results in 34,253 tasks in the training fold, 1,296 tasks in the validation seen fold and 1,363 tasks in the validation unseen fold. The corresponding low level actions for all DialFRED tasks is displayed in Table IV. Instruction augmentation. To generate instructions for DialFRED tasks, we create instruction templates for each low level action. For tasks created from split sub-goals, we directly use the template as instruction. For tasks created from combined sub-goals, we concatenate the instructions of low level actions within the sub-goal. In addition to the step-by-step instructions describing low level actions, we generate new instructions that only describe one major action in the task. For example, for the task go to the microwave and open the microwave (Figure 3), a human may only describe the main action "open the microwave" in the instruction. The agent cannot determine the action sequence solely based on the instruction. It needs to have an understanding of its position in the room to decide which action to take next. We belive these instructions match human commands in real-world scenrios and are more challenging than the original step-by-step instructions. The example instructions for all DialFRED tasks is displayed in Table IV.</p>
<h2>IV. Method</h2>
<p>Our baseline for the DialFRED benchmark has two key components, a questioner and a performer (Figure 4). The questioner asks questions based on the task instruction and</p>
<p>agent observations. The performer predicts a sequence of actions to execute in the environment based on the original instruction and the questions and answers. A good questioner knows both when to ask a question and what question(s) to ask, so that the task can be better completed by the performer. We first train the questioner using human-annotated data, i.e., give the model a good starting point by mimicking human judgement. To improve the coordination between the questioner and the performer, we fine-tune the questioner with reinforcement learning. The questions and answers (QAs) together form a dialogue between two entities: the agent (represented by the performer and questioner model) and the human (represented by the instruction and the answers).</p>
<h2>A. Architecture</h2>
<p>Questioner. Our questioner model (Figure 5) is based on a sequence-to-sequence architecture with attention [29]. It has two modules: an encoder and a decoder, both implemented using LSTM [30]. The instructions are first tokenized and go through an embedding layer to produce token embeddings $e_{1: N}$. At each time step, the LSTM encoder takes the embedding of the current token $e_{n}$ and the previous encoder hidden state $h_{n-1}$ to produce a new hidden state $h_{n}$. As a result, the encoder generates a sequence of hidden states $h_{1: N}$. The final encoder hidden state $h_{N}$ is used to initialize the decoder's hidden state $d_{0}$. The decoder uses the image ResNet feature $I$, the previous question token $w_{i-1}$ and the previous decoder hidden state $d_{i-1}$ to produce the new decoder hidden state $d_{i}$. The decoder hidden state $d_{i}$ is used to attend over encoder hidden states $h_{1: N}$ and predict the next question token $w_{i}$ via a dot-product attention mechanism. Each question is represented by two tokens, including one for the question type (i.e. loc for location, app for appearance and dir for direction), and the other for the target object. The questioner can also choose not to ask a question by generating a none token. We pre-train the questioner using questions selected by Turkers (Section III-A) on the training split.
Performer. Our performer is based on the Episodic Transformer [31], an attention-based multi-layer transformer model that encodes the full history of the instruction and QAs, visual observations and action history to predict future actions. To enable the model to handle all possible QAs from the questioner and oracle, we pre-train it on the training split by providing it with instructions and all possible questions, including a combination of question types and answers. Model parameters are optimized by minimizing the cross-entropy between predicted and expert actions.</p>
<h2>B. Questioner fine-tuning using RL</h2>
<p>The goal of the questioner is to ask necessary questions to help the performer finish the task. Thus we fine-tune the questioner using reinforcement learning to learn when to ask a question and what question(s) to ask on the validation seen split. The learning system for question asking is modeled as a Markov Decision Process, specified by a tuple $<S, A, T, R>$, where $S$ is the state space (including the language instructions and visual observations), $A=Q \times O$ is the action space of all
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5: The Questioner model. Given the instruction and image feature $I$, our model generates question tokens $w_{1: i}$.
possible questions (in our cases a combination of question types $Q$ and target object $O$ ), $T\left(s^{\prime} \mid s, a\right)$ is the transition function encoding how the performer can advance the task given the question and its corresponding answers, and $R(s, a)$ is the reward function encoding the reward for each $(s, a)$ pair. Our questioner model can be viewed as a policy network $\pi_{\theta}(s, a)=p(a \mid s ; \theta)$ mapping each state vector $s$ to a stochastic questioning policy, which can be learned based on the reward function to strike a balance between the number of questions and performance gain.</p>
<p>We adopt the following reward function structure to address this trade-off: reward for task completion $r_{\text {suc }}=1.0$, penalty for each step $r_{\text {step }}=-0.01$, penalty per question asked $r_{q}=-0.05$, penalty per invalid question $r_{\text {invalid }}=-0.1$. Some questions generated by the questioner cannot be answered by the oracle, e.g. the appearance of task-irrelevant objects. Thus we add a penalty for invalid questions. The reward forms the actor critic loss function [32], the gradient of which is used to update the model so that it can learn to ask necessary questions at correct time. The performer model is not updated during questioner fine-tuning.</p>
<h2>C. Heuristic-based questioner</h2>
<p>Inspired by [27], we implemented a questioner based on model confusion. The idea is that if the performer is not confident, the output action distribution would have high entropy; model confusion could be a good heuristic to know when to ask a question. We sort action probabilities in decreasing order; an agent is confused if the minimum difference between the top two actions is less than a threshold $\varepsilon$ throughout the action sequences:</p>
<p>$$
\min <em _sorted="{sorted" _text="\text">{t}\left(p</em>[1]\right)&lt;\varepsilon
$$}}^{t}[0]-p_{\text {sorted }}^{t</p>
<p>where the threshold $\varepsilon$ is used to control the degree of confusion for asking questions. In practice, we set the confusion threshold $\varepsilon=0.5$ in the experiment.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$#$</th>
<th style="text-align: center;">Expt setting</th>
<th style="text-align: center;">Seen SR</th>
<th style="text-align: center;">Unseen SR</th>
<th style="text-align: center;">Seen PWSR</th>
<th style="text-align: center;">Unseen PWSR</th>
<th style="text-align: center;">NQ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Instruction only</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">All QAs</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">3.24</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Random QA</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">0.81</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Random MC</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">$\mathbf{3 5 . 5}$</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">0.52</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">RL begin</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">RL anytime</td>
<td style="text-align: center;">$\mathbf{4 7 . 8}$</td>
<td style="text-align: center;">$\mathbf{3 3 . 6}$</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">$\mathbf{2 0 . 4}$</td>
<td style="text-align: center;">0.71</td>
</tr>
</tbody>
</table>
<p>TABLE I: Performance of the baselines. Seen SR and unseen SR represent the success rate on valid seen and valid unseen splits. PWSR is the path weighted success rate. NQ is the number of questions asked by the questioner. The best results are highlighted in boldface.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Expt setting</th>
<th style="text-align: center;">Unseen SR</th>
<th style="text-align: center;">Unseen PWSR</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">Loc Perc.</th>
<th style="text-align: center;">App Perc.</th>
<th style="text-align: center;">Dir Perc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.06</td>
</tr>
<tr>
<td style="text-align: left;">RL anytime</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.21</td>
</tr>
<tr>
<td style="text-align: left;">RL Loc perturb</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.38</td>
</tr>
<tr>
<td style="text-align: left;">RL App perturb</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: left;">RL Dir perturb</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">Random Loc perturb</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">Random App perturb</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">Random Dir perturb</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.33</td>
</tr>
</tbody>
</table>
<p>TABLE II: Ablation study by perturbing the oracle. We start from two settings: a questioner that has been fine-tuned for asking questions at any time and a questioner that asks a random question at the beginning. We perturb the oracle by not providing answers for one question type $50 \%$ of the time. Loc Perc, App Perc and Dir Perc represent the percentage questions about object locations, object appearance and directions respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Experiment setting</th>
<th style="text-align: center;">Seen SR</th>
<th style="text-align: center;">Unseen SR</th>
<th style="text-align: center;">Seen PWSR</th>
<th style="text-align: center;">Unseen PWSR</th>
<th style="text-align: center;">NQ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RL anytime (Fixed 1)</td>
<td style="text-align: center;">$\mathbf{5 1 . 9}$</td>
<td style="text-align: center;">$\mathbf{3 4 . 7}$</td>
<td style="text-align: center;">$\mathbf{3 6 . 3}$</td>
<td style="text-align: center;">$\mathbf{2 1 . 2}$</td>
<td style="text-align: center;">21.39</td>
</tr>
<tr>
<td style="text-align: left;">RL anytime (Fixed 5)</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">RL anytime (Fixed 10)</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: left;">RL anytime (MC)</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">0.31</td>
</tr>
</tbody>
</table>
<p>TABLE III: Effect of question timing. We manipulate the number of steps the performer rolls out before the questioner can ask the next question. For (Fixed 1), we modify the rewards ( $r_{\text {invalid }}=-0.01, r_{q}=-0.002$ ) to promote question asking. For $(M C)$, the questioner asks questions based on the performer model confusion.</p>
<h2>V. EXPERIMENTS</h2>
<p>We evaluate the baseline models on our dataset. We terminate the episode when it exceeds 1000 steps or has more than 10 failed actions.</p>
<h2>A. Evaluation metrics</h2>
<p>We evaluate model performance using success rate and path weighted success rate. In DialFRED, task success $s$ is a binary indicator of whether the task goal conditions have been achieved. For example, the task "clean the fork" requires the states of the fork to be changed from "dirty" to "clean". Task success $s$ is defined as 1 if the goal conditions have been fully achieved, and 0 otherwise. Success rate is calculated via averaging $s$ across all tasks. Path weighted success rate further takes into consideration the number of actions the robot takes. The path weighted success score for a task is calculated by:</p>
<p>$$
p_{s}=s * L^{<em>} / \max \left(L^{</em>}, \hat{L}\right)
$$</p>
<p>where $s$ is the original task success indicator ( 0 or 1 ), $\hat{L}$ is the number of steps taken by the agent to complete the task, and $L^{*}$ is the number of steps in the expert demonstration. Intuitively, the more steps the robot took, the lower the score. Again, the path weighted success rate is calculated via averaging $p_{s}$ across all tasks. To understand the trade-off between the number of questions asked and task performance, we measure the number of questions throughout the task.</p>
<h2>B. Baselines</h2>
<p>We implemented 6 baselines and evaluated their performance on our augmented dataset (Table I). In all baselines we use the Episodic Transformer model as the performer. In baselines $2-6$, the performer is trained using imitation learning on expert action sequences with language instructions and QAs. In baselines $5-6$, the questioner is pre-trained on human dialogues in the training split, and fine-tuned using reinforcement learning on the valid seen split. Baseline details are itemized below:</p>
<p>1) In this baseline, no questions are allowed, the performer is trained to predict the action sequences based on the language instruction and visual observations.
2) In addition to the instruction, the performer gets all valid QAs at the beginning of the task, including a combination of all three question types (i.e. location, appearance and direction) and objects mentioned in the instruction.
3) Questions are sampled randomly based on type: $25 \%$ for each of the 3 types and $25 \%$ for no question. Given a selected question type, questions and answers for all relevant objects are given to the performer in addition to the instructions.
4) When the action sequences generated by the performer satisfy the model confusion criterion (Equation (1)), the performer is randomly provided with a valid QA as input.
5) The questioner is fine-tuned using reinforcement learning</p>
<p>to learn whether to ask a question and what question to ask at the beginning of a task. The performer uses the QA to generate the action sequence to finish the task.
6) Similar to baseline 5, the questioner is fine-tuned using reinforcement learning, but now it can ask questions in the middle of the task. Given the instruction and previous QAs, we roll out the performer for 5 steps, following which the questioner is allowed to generate new questions and thus get new answers.</p>
<h2>C. Results</h2>
<p>We display (Table I) the task performance (success rate) on validation seen and unseen splits for all baseline models. Comparing the results of baselines $1-3$, we see that adding QAs to the instructions improves task performance on both splits. Comparing the results of baselines $2-5$, we see that the fine-tuned questioner achieves the best performance, with a smaller number of questions. Comparing the results of two reinforcement learning baselines $(5,6)$, we see that enabling the agent to ask questions in the middle of the task improves performance at the cost of more questions and answers. In addition, the random MC baseline achieves reasonable performance on the valid seen split, but not on the valid unseen split. Since the performer is pre-trained on the training split, which has the same scene as the valid seen split, and is given random combinations of different types of QAs as input, it is not surprising that the random $M C$ baseline does not generalize well to unseen environments.</p>
<h2>D. Ablation Study</h2>
<p>Perturbed oracle. We perform an ablation study (Table II) by perturbing the oracle. For each question type, we limit the oracle to provide answers for $50 \%$ of the asked questions. The original fine-tuned questioner ( $R L$ anytime) has a similar number of questions (and distribution) as the human data it is pre-trained on - most questions are about object locations. With the perturbation, the model asks slightly fewer questions. We observe that the fine-tuned questioner model adapts to the deficient oracle. For example, after perturbation on object location answers, the questioner asks $28 \%$ fewer location questions and $81 \%$ more direction questions instead. Looking at model performance after perturbation, we find that perturbing the location answers reduces the performance the most. This result matches the large proportion of location questions asked by both humans and the learning-tuned model, indicating that location answers are probably most useful for task completion. We perform the same perturbations on the oracle for the random questioner. Comparing the performance of models before and after perturbation, we see that the drop in SR caused by the perturbations for the learning-tuned questioners are smaller than the drops for the random questioners. The difference can be explained by the learning-tuned questioner's ability to adjust the proportion of questions to adapt to the deficient oracle.
Question timing. To understand how question timing affects performance, we change the number of action steps the performer executes before allowing the questioner to ask a
question. We add a setting, RL anytime (MC), which allows the questioner to ask questions based on model confusion (Equation (1)) during fine-tuning. The results (Table III) show that reducing the number of steps between questions leads to slightly better performance, but it also requires the oracle to answer significantly more questions. Comparing the results of model confusion with fixed timing, we find that it achieves reasonable performance at relatively low cost.</p>
<h2>VI. CONCLUSION</h2>
<p>We presented DialFRED, a dialogue-enabled embodied instruction following benchmark that allows an agent to actively ask questions while interacting with the environment to finish a household task. DialFRED is generated by augmenting ALFRED to increase task and language variations. It includes an oracle to answer questions and a human-annotated dataset with 53 K task-relevant questions and answers - a potential resource to model how humans ask and answer task-oriented questions. To tackle DialFRED, we propose a questionerperformer baseline (and variants) wherein the questioner is pre-trained with the human-annotated data and fine-tuned with reinforcement learning. Experimental results show that asking the right questions leads to significantly improved task performance. Extending existing embodied instruction following benchmarks with dialogue is a promising avenue of research towards truly interactive embodied agents. Along these lines, we posit that the general framework of oracleguided reinforcement training and the hybrid data annotation method we employ may be useful to "dialogue-enable" other embodied instruction following tasks.</p>
<h2>REFERENCES</h2>
<p>[1] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. Van Den Hengel, "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 3674-3683.
[2] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox, "Alfred: A benchmark for interpreting grounded instructions for everyday tasks," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. $10740-10749$.
[3] V. Blukis, C. Paxton, D. Fox, A. Garg, and Y. Artzi, "A persistent spatial semantic representation for high-level natural language instruction execution," in Conference on Robot Learning, 2021, pp. 706-717.
[4] S. Y. Min, D. S. Chaplot, P. K. Ravikumar, Y. Bisk, and R. Salakhutdinov, "FILM: Following instructions in language with modular methods," in International Conference on Learning Representations, 2022.
[5] E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi, "Ai2-thor: An interactive 3d environment for visual ai," arXiv preprint arXiv:1712.05474, 2017.
[6] X. Gao, R. Gong, T. Shu, X. Xie, S. Wang, and S. Zhu, "Vrktchen: an interactive 3d virtual environment for task-oriented learning," arXiv, vol. abs/1903.05757, 2019.
[7] F. Xia, W. B. Shen, C. Li, P. Kasimbeg, M. E. Tchapmi, A. Toshev, R. Martín-Martín, and S. Savarese, "Interactive gibson benchmark: A benchmark for interactive navigation in cluttered environments," IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 713-720, 2020.
[8] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, "Embodied Question Answering," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.
[9] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi, "Iqa: Visual question answering in interactive environments," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 4089-4098.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">#</th>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Example Low Level Actions</th>
<th style="text-align: center;">Example Instructions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Clean</td>
<td style="text-align: center;">put sink, turnon faucet, turnoff faucet, pickup pot</td>
<td style="text-align: center;">Wash the pot.</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Close</td>
<td style="text-align: center;">close drawer</td>
<td style="text-align: center;">Close the drawer.</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Cool</td>
<td style="text-align: center;">open fridge, put fridge, close fridge, open fridge, take tomato, close fridge</td>
<td style="text-align: center;">Chill the tomato.</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Heat</td>
<td style="text-align: center;">open microwave, put microwave, close microwave, turnon microwave, turnoff microwave, open microwave, pickup apple, close microwave</td>
<td style="text-align: center;">Cook the apple.</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Move</td>
<td style="text-align: center;">goto cart</td>
<td style="text-align: center;">Go to the cart.</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Open</td>
<td style="text-align: center;">open safe</td>
<td style="text-align: center;">Open the safe.</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Pick</td>
<td style="text-align: center;">pickup egg</td>
<td style="text-align: center;">Take the egg.</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Put</td>
<td style="text-align: center;">put plate</td>
<td style="text-align: center;">Put on the plate.</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Slice</td>
<td style="text-align: center;">slice lettuce</td>
<td style="text-align: center;">Cut the lettuce.</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Turn on</td>
<td style="text-align: center;">turnon desklamp</td>
<td style="text-align: center;">Power on the desklamp.</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">Turn off</td>
<td style="text-align: center;">turnoff microwave</td>
<td style="text-align: center;">Switch off the microwave.</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Move \&amp; Clean</td>
<td style="text-align: center;">goto sinkbasin, put sink, turnon faucet, turnoff faucet, pickup ladle</td>
<td style="text-align: center;">Go to the sinkbasin, wash the ladle.</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">Move \&amp; Close</td>
<td style="text-align: center;">goto laptop, close laptop</td>
<td style="text-align: center;">Close the laptop.</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">Move \&amp; Cool</td>
<td style="text-align: center;">goto fridge, open fridge, put fridge, close fridge, open fridge, take potato, close fridge</td>
<td style="text-align: center;">Move to the fridge, cool the potato.</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">Move \&amp; Heat</td>
<td style="text-align: center;">goto microwave, open microwave, put microwave, close microwave, turnon microwave, turnoff microwave, open microwave, pickup potato, close microwave</td>
<td style="text-align: center;">Heat up the potato.</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">Move \&amp; Open</td>
<td style="text-align: center;">goto microwave, open microwave</td>
<td style="text-align: center;">Open the microwave.</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">Move \&amp; Pick</td>
<td style="text-align: center;">goto countertop, pickup fork</td>
<td style="text-align: center;">Grab the fork.</td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">Move \&amp; Put</td>
<td style="text-align: center;">goto countertop, put countertop</td>
<td style="text-align: center;">Place on the countertop.</td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">Move \&amp; Slice</td>
<td style="text-align: center;">goto bread, slice bread</td>
<td style="text-align: center;">Cut the bread.</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">Move \&amp; Turn on</td>
<td style="text-align: center;">goto desklamp, turnon desklamp</td>
<td style="text-align: center;">Switch on the desklamp.</td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">Open \&amp; Pick \&amp; Close</td>
<td style="text-align: center;">open microwave, pickup cup, close microwave</td>
<td style="text-align: center;">Pick up the cup.</td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">Open \&amp; Put \&amp; Close</td>
<td style="text-align: center;">open fridge, put fridge, close fridge</td>
<td style="text-align: center;">Put into the fridge.</td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">Pick \&amp; Move</td>
<td style="text-align: center;">pickup mug, goto sidetable</td>
<td style="text-align: center;">Take the mug to the sidetable.</td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">Pick \&amp; Move \&amp; Put</td>
<td style="text-align: center;">pickup keychain, goto armchair, put keychain</td>
<td style="text-align: center;">Put the keychain on the armchair.</td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">Pick \&amp; Move \&amp; Slice</td>
<td style="text-align: center;">pickup knife, goto bread, slice bread</td>
<td style="text-align: center;">Cut the bread with a knife</td>
</tr>
</tbody>
</table>
<p>TABLE IV: List of all 25 tasks in DialFRED and their example low level actions and instructions. Note that here we do not further split "goto" into low level actions (e.g. move forward, turn left, etc.).
[10] J. Thomason, M. Murray, M. Cakmak, and L. Zettlemoyer, "Vision-and-dialog navigation," in Conference on Robot Learning, 2020, pp. 394-406.
[11] A. Padmakumar, J. Thomason, A. Shrivastava, P. Lange, A. NarayanChen, S. Gella, R. Piramithu, G. Tur, and D. Hakkani-Tur, "TEACh: Task-driven Embodied Agents that Chat," in Conference on Artificial Intelligence (AAAI), 2022.
[12] K. Jokinen and M. McTear, "Spoken dialogue systems," Synthesis Lectures on Human Language Technologies, vol. 2, no. 1, pp. 1-151, 2009 .
[13] J. Gu, E. Stefani, Q. Wu, J. Thomason, and X. E. Wang, "Vision-andlanguage navigation: A survey of tasks, methods, and future directions," arXiv preprint arXiv:2203.12667, 2022.
[14] V. Rus, B. Wyse, P. Piwek, M. Lintean, S. Stoyanchev, and C. Moldovan, "The first question generation shared task evaluation challenge," in Proceedings of the 6th International Natural Language Generation Conference, 2010.
[15] J. Gao, M. Galley, and L. Li, "Neural approaches to conversational ai," in The 41st International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval, 2018, pp. 1371-1374.
[16] J. Y. Chai, Q. Gao, L. She, S. Yang, S. Saba-Sadiya, and G. Xu, "Language to action: Towards interactive task learning with physical agents." in IJCAI, 2018, pp. 2-9.
[17] J. Thomason, A. Padmakumar, J. Sinapov, N. Walker, Y. Jiang, H. Yedidsion, J. Hart, P. Stone, and R. Mooney, "Jointly improving parsing and perception for natural language commands through human-robot dialog," Journal of Artificial Intelligence Research, vol. 67, pp. 327-374, 2020.
[18] N. Mrkšić, D. Ó Séaghdha, T.-H. Wen, B. Thomson, and S. Young, "Neural belief tracker: Data-driven dialogue state tracking," in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Vancouver, Canada, July 2017, pp. $1777-1788$.
[19] E. Hosseini-Asl, B. McCann, C.-S. Wu, S. Yavuz, and R. Socher, "A simple language model for task-oriented dialogue," arXiv preprint arXiv:2005.00796, 2020.
[20] P.-H. Su, M. Gašić, N. Mrkšić, L. M. Rojas-Barahona, S. Ultes, D. Vandyke, T.-H. Wen, and S. Young, "On-line active reward learning for policy optimisation in spoken dialogue systems," in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Berlin, Germany, Aug. 2016, pp. $2431-2441$.
[21] B. Peng, X. Li, L. Li, J. Gao, A. Celikylmaz, S. Lee, and K.-F. Wong, "Composite task-completion dialogue policy learning via hierarchical deep reinforcement learning," in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2017.
[22] X. Hu, Z. Wen, Y. Wang, X. Li, and G. de Melo, "Interactive question clarification in dialogue via reinforcement learning," in Proceedings of the 28th International Conference on Computational Linguistics: Industry Track, 2020, pp. 78-89.
[23] S. Tellex, R. Knepper, A. Li, D. Rus, and N. Roy, "Asking for help using inverse semantics," in Proceedings of Robotics: Science and Systems, Berkeley, USA, July 2014.
[24] K. Nguyen, D. Dey, C. Brockett, and B. Dolan, "Vision-based navigation with language-based assistance via imitation learning with indirect intervention," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 12527-12537.
[25] K. Nguyen and H. Daumé III, "Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning," in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), November 2019.
[26] H. R. Roman, Y. Bisk, J. Thomason, A. Celikylmaz, and J. Gao, "Rmm: A recursive mental model for dialog navigation," in Findings of the 2020 Conference on Empirical Methods in Natural Language Processing, 2020.
[27] T.-C. Chi, M. Shen, M. Eric, S. Kim, and D. Hakkani-tur, "Just ask: An interactive learning framework for vision and language navigation," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 03, 2020, pp. 2459-2466.
[28] X. Lu, "The relationship of lexical richness to the quality of esl learners' oral narratives," The Modern Language Journal, vol. 96, no. 2, pp. 190208, 2012.
[29] M.-T. Luong, H. Pham, and C. D. Manning, "Effective approaches to attention-based neural machine translation," in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1412-1421.
[30] S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural computation, vol. 9, no. 8, pp. 1735-1780, 1997.
[31] A. Pashevich, C. Schmid, and C. Sun, "Episodic transformer for vision-and-language navigation," 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 15922-15932, 2021.
[32] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2186 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2186</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2186</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-59.html">extraction-schema-59</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-277313413</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.18865v3.pdf" target="_blank">Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of large language models offers new possibilities for structured exploration of scientific knowledge. Rather than viewing scientific discovery as isolated ideas or content, we propose a structured approach that emphasizes the role of method combinations in shaping disruptive insights. Specifically, we investigate how knowledge unit--especially those tied to methodological design--can be modeled and recombined to yield research breakthroughs. Our proposed framework addresses two key challenges. First, we introduce a contrastive learning-based mechanism to identify distinguishing features of historically disruptive method combinations within problem-driven contexts. Second, we propose a reasoning-guided Monte Carlo search algorithm that leverages the chain-of-thought capability of LLMs to identify promising knowledge recombinations for new problem statements.Empirical studies across multiple domains show that the framework is capable of modeling the structural dynamics of innovation and successfully highlights combinations with high disruptive potential. This research provides a new path for computationally guided scientific ideation grounded in structured reasoning and historical data modeling.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2186.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2186.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disruption Index</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bibliometric measure that quantifies whether a paper displaces prior work (disruptive) versus consolidating it (developmental), computed from patterns of citations to the focal paper and its references.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>citation counts / traditional bibliometrics (as contrasted proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>disruptiveness / paradigm-shift signal (Disruption Index computed from citation relationships)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>Disruption Index (DI) defined as D = (n_i - n_j) / (n_i + n_j + n_k), where n_i = papers citing focal paper only, n_j = papers citing both focal paper and its references, n_k = papers citing only the references</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>DI is computed explicitly by the provided formula; the paper gives an example DI value (Watson & Crick = 0.62) as an instance of high disruptiveness. The paper states qualitatively that traditional citation counts measure adoption rather than disruptiveness but does not provide a numerical mapping between citation counts and DI in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Paper cites prior literature (Park et al. 2023) reporting that papers and patents have become less disruptive over time; no DI-versus-citation-time numeric trajectories are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Applied across datasets in this paper: computer science (DBLP), biomedical (PubMed - depression), and patents (PatSnap - medical robotics); DI as a concept referenced in prior cross-field bibliometrics (e.g., Nobel papers).</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>The paper does not report field-by-field numeric comparisons between DI and citation-based proxies; it applies DI-conceptually across multiple domains but gives no explicit comparative magnitudes by field.</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Not applicable for DI itself; DI is the target predicted variable for the authors' models.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>The paper uses DI as an alternative/ground-truth target to correct reliance on citation counts when evaluating transformative potential; no numerical evaluation of how much existing citation-based assessments are corrected is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>The authors note scarcity of highly disruptive papers in real-world datasets can bias predictive models toward low DI values and address this in model training (entropy-weighting, secondary learning).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td>Watson & Crick's 1953 DNA double-helix paper cited as an example with DI = 0.62 and validated by expert assessment; Wu et al.'s application of DI to Nobel-winning papers is cited as validation of DI's ability to identify breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>This paper adopts DI as the ground-truth target for supervised learning: retrospective bibliometric data (DBLP, PubMed, PatSnap) are used to fine-tune models to predict DI for problem-method combinations; DI is sourced from literature-derived citation relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Disruption Index (DI) provides a quantitative target that better distinguishes paradigm-shifting work from incremental adoption, and the authors integrate DI into their framework as the ground-truth measure for disruptive potential.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2186.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2186.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Traditional bibliometrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation counts / h-index / journal impact factor (traditional proxy metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard bibliometric proxies widely used to assess scientific influence, which the paper argues primarily reflect dissemination/adoption rather than transformative change.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>citation counts, h-index, journal impact factor (JIF)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>disruptiveness / paradigm-shift indicators (as represented by DI in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>Operationalized in the paper by contrasting traditional citation-based measures with the Disruption Index (DI) as the measure of novelty/transformational effect.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>The paper states qualitatively that citation counts measure adoption rather than transformativness and are biased toward incremental research; no direct quantitative mapping (e.g., correlation coefficients) between citation counts and DI is reported in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>The paper reports no numeric magnitudes quantifying the divergence between citation-based proxies and DI; it cites literature documenting biases (e.g., bias toward incremental work) but does not provide effect sizes here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>The paper references literature (Park et al. 2023) indicating papers and patents are becoming less disruptive over time, implying temporal shifts in what citation counts capture, but provides no numeric time-course comparing proxies to DI.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>General commentary across science and technology literature; experiments in the paper operate on CS (DBLP), biomedical (PubMed), and patents (PatSnap) datasets but do not present field-specific proxy-vs-DI comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not reported quantitatively; the paper notes cross-domain application but does not quantify differences in how citation proxies fail across fields.</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>The paper proposes using DI-based evaluation (instead of raw citation counts) and integrates DI as the supervised target for predictive models to reduce overreliance on citation proxies; no numeric reduction in proxy-truth gap is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Authors discuss that traditional bibliometrics and training data are skewed (favor incremental work), contributing to bias against novelty; they cite literature on bias against novelty but present no numeric estimate of the prevalence of incremental work in historical data within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Conceptual critique supported by citations to bibliometrics literature and by positioning DI as a corrective; empirical experiments in the paper focus on predicting DI rather than directly measuring citation-vs-DI correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Traditional bibliometric measures (citations, JIF, h-index) primarily capture dissemination/adoption and are insufficient to identify transformative, disruptive work; DI is recommended as a more appropriate quantitative target for that purpose.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2186.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2186.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy+GPP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy with Probabilistic Perturbation (optimization algorithm used by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative search/optimization mechanism that combines greedy local improvement with occasional probabilistic acceptance of non-optimal moves to escape local optima when searching for high-disruptiveness problem-method combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>automated predictive identification of disruptive work (algorithmic selection based on predicted DI)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Disruption Index (DI) used as the target/ground-truth for hit determination (DI > 0.5 considered high-disruptiveness)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>High-disruptiveness defined as DI > 0.5 (used as binary threshold in hit-rate evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Empirical hit rates reported: Greedy+GPP identifies method combinations with DI > 0.5 at rates DBLP 26.3%, PubMed 28.1%, PatSnap 24.6%; best competing standard greedy hit rates are DBLP 19.3%, PubMed 21.5%, PatSnap 18.2%; improvement over best LLM reported as +9.1%, +9.1%, +9.0% respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Applied experiments on three datasets representing different domains: DBLP (computer science/AI conferences), PubMed (depression-related biomedical literature), PatSnap (medical robotics patents).</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Reported per-dataset hit rates (DBLP 26.3%, PubMed 28.1%, PatSnap 24.6%) show modest variation across fields in method identification performance, but the paper does not directly interpret these as differences in proxy-versus-ground-truth gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>As above: Greedy+GPP hit rates: DBLP 26.3%, PubMed 28.1%, PatSnap 24.6%; improvement over best baseline LLM: +9.1% (DBLP & PubMed), +9.0% (PatSnap).</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>GPP is presented as an optimization mechanism to better search for high-DI combinations; it is evaluated by hit-rate improvement compared to standard greedy and LLM baselines, demonstrating empirical effectiveness in discovering DI>0.5 candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Not directly about training distribution, but the optimization is applied on model-predicted DI which was trained with entropy-weighting and secondary learning to offset rarity of high-DI examples.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Algorithmic optimization experiment: the dynamic method optimization module (with GPP) iteratively modifies candidate methods and uses the DI-prediction model to score configurations; hit rates for DI>0.5 are reported across three datasets and compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Greedy with Probabilistic Perturbation substantially improves the discovery hit-rate for high-disruptiveness (DI>0.5) problem-method combinations vs standard greedy and LLM baselines (≈+9% absolute improvement).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2186.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2186.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Training corrections</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entropy-weighted loss + secondary learning + KL balancing (training mechanisms to correct bias)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of training techniques the authors implement to mitigate bias from rarity of highly disruptive examples: entropy-weighted sample loss to up-weight rare disruptive cases, secondary fine-tuning on high-error samples, and KL-based balancing to avoid prediction drift.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>model-based DI prediction (addressing bias from training on predominantly non-disruptive historical data)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Disruption Index (DI) is the supervised target that training aims to predict more accurately for rare high-DI samples.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>DI; rarity-weighting explicitly designed to favor correct prediction of high-disruptiveness (novel/transformational) samples.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>The paper defines the weighted loss L_entropy = sum_i w_i * ℓ(y_i, ŷ_i) with w_i = -log(p(ŷ_i)) to increase weight on rare predictions; no numerical reduction in the proxy-truth gap is reported, but ablation shows worsened errors when these mechanisms are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Applied during DI-prediction model training on DBLP, PubMed, and PatSnap datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Ablation tables (Table 4) show consistent performance degradation across all three datasets when secondary learning or deviation-aware alignment is removed, indicating these mechanisms helped across fields, but the paper does not quantify field-specific bias magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Ablation: removing secondary learning or deviation-aware alignment increases MSE/MAE across datasets (Table 4), demonstrating these corrections materially improve DI prediction accuracy; exact numeric degradations are in Table 4 (e.g., Full framework DBLP MSE 0.0093 vs w/o secondary learning 0.0187).</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>Entropy-weighting to upweight rare high-DI samples, secondary learning on top 20% highest-error samples, and KL-divergence regularization between primary and secondary models to stabilize training; authors report these reduce prediction error relative to ablated variants.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Explicitly acknowledged: scarcity of highly disruptive papers biases predictions toward low-DI; the described corrections are implemented to compensate for this imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Model training and ablation study: predictive models for DI are trained with and without the described correction mechanisms and evaluated across MSE/MAE/WMSE/WMAE on three datasets to assess the impact of these mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Weighting rare high-DI examples and targeted secondary learning with KL balancing materially improves predictive accuracy for disruptive (novel/transformational) cases versus models trained without these corrections (demonstrated by lower MSE/MAE in the full framework).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A dynamic network measure of technological change <em>(Rating: 2)</em></li>
                <li>Papers and patents are becoming less disruptive over time <em>(Rating: 2)</em></li>
                <li>Bias against novelty in science: A cautionary tale for users of bibliometric indicators <em>(Rating: 2)</em></li>
                <li>The incidence and role of negative citations in science <em>(Rating: 2)</em></li>
                <li>Atypical combinations and scientific impact <em>(Rating: 2)</em></li>
                <li>Large teams develop and small teams disrupt science and technology <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2186",
    "paper_id": "paper-277313413",
    "extraction_schema_id": "extraction-schema-59",
    "extracted_data": [
        {
            "name_short": "DI",
            "name_full": "Disruption Index",
            "brief_description": "A bibliometric measure that quantifies whether a paper displaces prior work (disruptive) versus consolidating it (developmental), computed from patterns of citations to the focal paper and its references.",
            "citation_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
            "mention_or_use": "use",
            "proxy_metric_type": "citation counts / traditional bibliometrics (as contrasted proxy)",
            "ground_truth_measure": "disruptiveness / paradigm-shift signal (Disruption Index computed from citation relationships)",
            "novelty_transformation_measure": "Disruption Index (DI) defined as D = (n_i - n_j) / (n_i + n_j + n_k), where n_i = papers citing focal paper only, n_j = papers citing both focal paper and its references, n_k = papers citing only the references",
            "quantitative_relationship": "DI is computed explicitly by the provided formula; the paper gives an example DI value (Watson & Crick = 0.62) as an instance of high disruptiveness. The paper states qualitatively that traditional citation counts measure adoption rather than disruptiveness but does not provide a numerical mapping between citation counts and DI in this study.",
            "gap_magnitude": null,
            "temporal_pattern": "Paper cites prior literature (Park et al. 2023) reporting that papers and patents have become less disruptive over time; no DI-versus-citation-time numeric trajectories are reported in this paper.",
            "field_studied": "Applied across datasets in this paper: computer science (DBLP), biomedical (PubMed - depression), and patents (PatSnap - medical robotics); DI as a concept referenced in prior cross-field bibliometrics (e.g., Nobel papers).",
            "field_differences": "The paper does not report field-by-field numeric comparisons between DI and citation-based proxies; it applies DI-conceptually across multiple domains but gives no explicit comparative magnitudes by field.",
            "multiplicative_vs_additive": null,
            "automated_system_performance": "Not applicable for DI itself; DI is the target predicted variable for the authors' models.",
            "correction_mechanism": "The paper uses DI as an alternative/ground-truth target to correct reliance on citation counts when evaluating transformative potential; no numerical evaluation of how much existing citation-based assessments are corrected is provided.",
            "training_distribution_bias": "The authors note scarcity of highly disruptive papers in real-world datasets can bias predictive models toward low DI values and address this in model training (entropy-weighting, secondary learning).",
            "counterexamples": "Watson & Crick's 1953 DNA double-helix paper cited as an example with DI = 0.62 and validated by expert assessment; Wu et al.'s application of DI to Nobel-winning papers is cited as validation of DI's ability to identify breakthroughs.",
            "study_design": "This paper adopts DI as the ground-truth target for supervised learning: retrospective bibliometric data (DBLP, PubMed, PatSnap) are used to fine-tune models to predict DI for problem-method combinations; DI is sourced from literature-derived citation relationships.",
            "key_finding": "Disruption Index (DI) provides a quantitative target that better distinguishes paradigm-shifting work from incremental adoption, and the authors integrate DI into their framework as the ground-truth measure for disruptive potential.",
            "uuid": "e2186.0"
        },
        {
            "name_short": "Traditional bibliometrics",
            "name_full": "Citation counts / h-index / journal impact factor (traditional proxy metrics)",
            "brief_description": "Standard bibliometric proxies widely used to assess scientific influence, which the paper argues primarily reflect dissemination/adoption rather than transformative change.",
            "citation_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
            "mention_or_use": "mention",
            "proxy_metric_type": "citation counts, h-index, journal impact factor (JIF)",
            "ground_truth_measure": "disruptiveness / paradigm-shift indicators (as represented by DI in this paper)",
            "novelty_transformation_measure": "Operationalized in the paper by contrasting traditional citation-based measures with the Disruption Index (DI) as the measure of novelty/transformational effect.",
            "quantitative_relationship": "The paper states qualitatively that citation counts measure adoption rather than transformativness and are biased toward incremental research; no direct quantitative mapping (e.g., correlation coefficients) between citation counts and DI is reported in this work.",
            "gap_magnitude": "The paper reports no numeric magnitudes quantifying the divergence between citation-based proxies and DI; it cites literature documenting biases (e.g., bias toward incremental work) but does not provide effect sizes here.",
            "temporal_pattern": "The paper references literature (Park et al. 2023) indicating papers and patents are becoming less disruptive over time, implying temporal shifts in what citation counts capture, but provides no numeric time-course comparing proxies to DI.",
            "field_studied": "General commentary across science and technology literature; experiments in the paper operate on CS (DBLP), biomedical (PubMed), and patents (PatSnap) datasets but do not present field-specific proxy-vs-DI comparisons.",
            "field_differences": "Not reported quantitatively; the paper notes cross-domain application but does not quantify differences in how citation proxies fail across fields.",
            "multiplicative_vs_additive": null,
            "automated_system_performance": null,
            "correction_mechanism": "The paper proposes using DI-based evaluation (instead of raw citation counts) and integrates DI as the supervised target for predictive models to reduce overreliance on citation proxies; no numeric reduction in proxy-truth gap is reported.",
            "training_distribution_bias": "Authors discuss that traditional bibliometrics and training data are skewed (favor incremental work), contributing to bias against novelty; they cite literature on bias against novelty but present no numeric estimate of the prevalence of incremental work in historical data within this paper.",
            "counterexamples": null,
            "study_design": "Conceptual critique supported by citations to bibliometrics literature and by positioning DI as a corrective; empirical experiments in the paper focus on predicting DI rather than directly measuring citation-vs-DI correlations.",
            "key_finding": "Traditional bibliometric measures (citations, JIF, h-index) primarily capture dissemination/adoption and are insufficient to identify transformative, disruptive work; DI is recommended as a more appropriate quantitative target for that purpose.",
            "uuid": "e2186.1"
        },
        {
            "name_short": "Greedy+GPP",
            "name_full": "Greedy with Probabilistic Perturbation (optimization algorithm used by authors)",
            "brief_description": "An iterative search/optimization mechanism that combines greedy local improvement with occasional probabilistic acceptance of non-optimal moves to escape local optima when searching for high-disruptiveness problem-method combinations.",
            "citation_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
            "mention_or_use": "use",
            "proxy_metric_type": "automated predictive identification of disruptive work (algorithmic selection based on predicted DI)",
            "ground_truth_measure": "Disruption Index (DI) used as the target/ground-truth for hit determination (DI &gt; 0.5 considered high-disruptiveness)",
            "novelty_transformation_measure": "High-disruptiveness defined as DI &gt; 0.5 (used as binary threshold in hit-rate evaluation).",
            "quantitative_relationship": "Empirical hit rates reported: Greedy+GPP identifies method combinations with DI &gt; 0.5 at rates DBLP 26.3%, PubMed 28.1%, PatSnap 24.6%; best competing standard greedy hit rates are DBLP 19.3%, PubMed 21.5%, PatSnap 18.2%; improvement over best LLM reported as +9.1%, +9.1%, +9.0% respectively.",
            "gap_magnitude": null,
            "temporal_pattern": null,
            "field_studied": "Applied experiments on three datasets representing different domains: DBLP (computer science/AI conferences), PubMed (depression-related biomedical literature), PatSnap (medical robotics patents).",
            "field_differences": "Reported per-dataset hit rates (DBLP 26.3%, PubMed 28.1%, PatSnap 24.6%) show modest variation across fields in method identification performance, but the paper does not directly interpret these as differences in proxy-versus-ground-truth gaps.",
            "multiplicative_vs_additive": null,
            "automated_system_performance": "As above: Greedy+GPP hit rates: DBLP 26.3%, PubMed 28.1%, PatSnap 24.6%; improvement over best baseline LLM: +9.1% (DBLP & PubMed), +9.0% (PatSnap).",
            "correction_mechanism": "GPP is presented as an optimization mechanism to better search for high-DI combinations; it is evaluated by hit-rate improvement compared to standard greedy and LLM baselines, demonstrating empirical effectiveness in discovering DI&gt;0.5 candidates.",
            "training_distribution_bias": "Not directly about training distribution, but the optimization is applied on model-predicted DI which was trained with entropy-weighting and secondary learning to offset rarity of high-DI examples.",
            "counterexamples": null,
            "study_design": "Algorithmic optimization experiment: the dynamic method optimization module (with GPP) iteratively modifies candidate methods and uses the DI-prediction model to score configurations; hit rates for DI&gt;0.5 are reported across three datasets and compared to baselines.",
            "key_finding": "Greedy with Probabilistic Perturbation substantially improves the discovery hit-rate for high-disruptiveness (DI&gt;0.5) problem-method combinations vs standard greedy and LLM baselines (≈+9% absolute improvement).",
            "uuid": "e2186.2"
        },
        {
            "name_short": "Training corrections",
            "name_full": "Entropy-weighted loss + secondary learning + KL balancing (training mechanisms to correct bias)",
            "brief_description": "A set of training techniques the authors implement to mitigate bias from rarity of highly disruptive examples: entropy-weighted sample loss to up-weight rare disruptive cases, secondary fine-tuning on high-error samples, and KL-based balancing to avoid prediction drift.",
            "citation_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
            "mention_or_use": "use",
            "proxy_metric_type": "model-based DI prediction (addressing bias from training on predominantly non-disruptive historical data)",
            "ground_truth_measure": "Disruption Index (DI) is the supervised target that training aims to predict more accurately for rare high-DI samples.",
            "novelty_transformation_measure": "DI; rarity-weighting explicitly designed to favor correct prediction of high-disruptiveness (novel/transformational) samples.",
            "quantitative_relationship": "The paper defines the weighted loss L_entropy = sum_i w_i * ℓ(y_i, ŷ_i) with w_i = -log(p(ŷ_i)) to increase weight on rare predictions; no numerical reduction in the proxy-truth gap is reported, but ablation shows worsened errors when these mechanisms are removed.",
            "gap_magnitude": null,
            "temporal_pattern": null,
            "field_studied": "Applied during DI-prediction model training on DBLP, PubMed, and PatSnap datasets.",
            "field_differences": "Ablation tables (Table 4) show consistent performance degradation across all three datasets when secondary learning or deviation-aware alignment is removed, indicating these mechanisms helped across fields, but the paper does not quantify field-specific bias magnitudes.",
            "multiplicative_vs_additive": null,
            "automated_system_performance": "Ablation: removing secondary learning or deviation-aware alignment increases MSE/MAE across datasets (Table 4), demonstrating these corrections materially improve DI prediction accuracy; exact numeric degradations are in Table 4 (e.g., Full framework DBLP MSE 0.0093 vs w/o secondary learning 0.0187).",
            "correction_mechanism": "Entropy-weighting to upweight rare high-DI samples, secondary learning on top 20% highest-error samples, and KL-divergence regularization between primary and secondary models to stabilize training; authors report these reduce prediction error relative to ablated variants.",
            "training_distribution_bias": "Explicitly acknowledged: scarcity of highly disruptive papers biases predictions toward low-DI; the described corrections are implemented to compensate for this imbalance.",
            "counterexamples": null,
            "study_design": "Model training and ablation study: predictive models for DI are trained with and without the described correction mechanisms and evaluated across MSE/MAE/WMSE/WMAE on three datasets to assess the impact of these mechanisms.",
            "key_finding": "Weighting rare high-DI examples and targeted secondary learning with KL balancing materially improves predictive accuracy for disruptive (novel/transformational) cases versus models trained without these corrections (demonstrated by lower MSE/MAE in the full framework).",
            "uuid": "e2186.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A dynamic network measure of technological change",
            "rating": 2
        },
        {
            "paper_title": "Papers and patents are becoming less disruptive over time",
            "rating": 2
        },
        {
            "paper_title": "Bias against novelty in science: A cautionary tale for users of bibliometric indicators",
            "rating": 2
        },
        {
            "paper_title": "The incidence and role of negative citations in science",
            "rating": 2
        },
        {
            "paper_title": "Atypical combinations and scientific impact",
            "rating": 2
        },
        {
            "paper_title": "Large teams develop and small teams disrupt science and technology",
            "rating": 2
        }
    ],
    "cost": 0.00997425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations
14 Apr 2025</p>
<p>Junlan Chen 
Sun Yat-sen University
GuangzhouChina</p>
<p>Kexin Zhang 
Sun Yat-sen University
GuangzhouChina</p>
<p>Daifeng Li lidaifeng@mail.sysu.edu.cn 
Sun Yat-sen University
GuangzhouChina</p>
<p>Yangyang Feng 
Sun Yat-sen University
GuangzhouChina</p>
<p>Yuxuan Zhang 
Sun Yat-sen University
GuangzhouChina</p>
<p>Bowen Deng 
Sun Yat-sen University
GuangzhouChina</p>
<p>Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations
14 Apr 2025567B202A2AAC0C2C15CCA7E343CA733EarXiv:2503.18865v3[cs.AI]Scientific InnovationKnowledge RecombinationLLM ReasoningProblem-Method Structure
The emergence of large language models (LLMs) offers new possibilities for structured exploration of scientific knowledge.Rather than viewing scientific discovery as isolated ideas or content, we propose a structured approach that emphasizes the role of method combinations in shaping disruptive insights.Specifically, we investigate how knowledge units-especially those tied to methodological design-can be modeled and recombined to yield research breakthroughs.Our proposed framework addresses two key challenges.First, we introduce a contrastive learning-based mechanism to identify distinguishing features of historically disruptive method combinations within problemdriven contexts.Second, we propose a reasoning-guided Monte Carlo search algorithm that leverages the chain-of-thought capability of LLMs to identify promising knowledge recombinations for new problem statements.Empirical studies across multiple domains show that the framework is capable of modeling the structural dynamics of innovation and successfully highlights combinations with high disruptive potential.This research provides a new path for computationally guided scientific ideation grounded in structured reasoning and historical data modeling.</p>
<p>Introduction</p>
<p>Recent advances in Large Language Models (LLMs) have significantly enhanced text understanding and generation capabilities [27,33,2], demonstrating expertlevel performance across various domains [3].In scientific discovery, LLMs have been applied to generate research ideas and synthesize existing knowledge [3,39,8,17], confirming their potential in assisting scientific exploration.However, despite these advancements, existing approaches still exhibit several limitations: (1) the inability to systematically identify and integrate fine-grained knowledge components, resulting in scientific discovery that remains at a macro-level of idea generation rather than precise matching of research problems and methods; (2) the hallucination phenomenon in LLMs, where models generate problem-solving approaches that lack actual literature support, potentially leading research efforts astray; and (3) the absence of objective metrics to assess the transformative impact of newly proposed discoveries, as current methods predominantly rely on subjective expert alignment rather than quantitative evaluations of scientific breakthroughs.</p>
<p>Research has shown that the most influential scientific discoveries primarily stem from the combination-particularly atypical combinations-of traditional ideas from prior work [32,34,36].Innovation emerges when prior innovations or their components are assembled into an original design [16,24,35].Schumpeter [31], a pioneer in innovation theory, posited that innovation is fundamentally "the recombination of elements of production", meaning a novel combination of production elements or conditions.Similarly, Nelson and Winter [25] argued that "the creation of novelty in art, science, and technology largely depends on the recombination of pre-existing conceptual and physical materials."In scientific research, research questions and methods serve as fundamental building blocks, and their combination determines the scientific novelty of a publication [22].Despite this, existing studies largely focus on LLM-driven idea generation rather than systematically identifying, filtering, and combining problem-method pairs to enhance the effectiveness of scientific discovery.</p>
<p>To address this gap, we introduce the Disruptive Index (DI) to quantify whether a scientific discovery drives a paradigm shift.Disruptive innovation represents fundamental transformations in scientific and technological progress, distinct from incremental improvements that merely refine existing paradigms.Traditional impact metrics, such as citation counts, primarily measure the extent of a technology's adoption rather than its transformative potential.The Disruptive Index (DI), proposed by Funk and Owen-Smith [11], captures whether a scientific discovery supersedes previous approaches rather than merely reinforcing the status quo.A prominent example is Watson and Crick's (1953) discovery of the DNA double-helix structure, which superseded previous approaches, such as Pauling's triple-helix model, and fundamentally altered the field of molecular biology.Their study, with a DI score of 0.62 [28], exemplifies a highly disruptive scientific breakthrough, validated extensively through expert assessments [11,37].Therefore, beyond relying on LLM-generated research ideas, it is essential to construct a framework that integrates DI-based evaluations to systematically assess the transformative impact of problem-method combinations.</p>
<p>Building upon these insights, we propose a problem-method combination framework for scientific discovery, inspired by how scientific breakthroughs emerge from the recombination of existing knowledge.Given a research question, our framework first retrieves and synthesizes relevant papers, then employs an LLM assistant to determine whether specific papers can serve as sources for new scientific discoveries related to the question and extracts a candidate set of methods.Subsequently, we introduce an innovative disruptive index evaluation framework to quantify the disruptiveness of problem-method combinations.Specifically, through model fine-tuning, our assistant generates combination strategies based on research questions and candidate methods.To evaluate the disruptiveness of these strategies, we identify potential source literature in our database, analyze differences between source strategies and current strategies, and propose an adaptive bias-aware alignment model to predict disruptive indices based on these differences.Finally, we iteratively explore candidate method sets to identify the most disruptive problem-method combinations.</p>
<p>We conduct extensive experiments on publication databases across three scientific domains.Our results demonstrate that the proposed framework outperforms state-of-the-art methods in predicting the disruptiveness of problemmethod combinations.Furthermore, validation on real-world high-disruptiveness publications confirms the framework's ability to identify highly disruptive scientific discoveries.</p>
<p>The primary contributions of this research are as follows:</p>
<p>1.A novel framework for scientific discovery that systematically identifies and integrates problem-method combinations rather than relying solely on LLM-generated research ideas.2. A disruptive index evaluation framework that quantitatively assesses the potential disruptiveness of new scientific discoveries, improving upon traditional impact metrics.3. Extensive experimental validation demonstrating the effectiveness of our approach in identifying high-disruptiveness discoveries across multiple scientific domains.</p>
<p>2 Related Work</p>
<p>LLMs in Scientific Discovery and Research Ideation</p>
<p>Large Language Models (LLMs) have demonstrated significant potential in scientific discovery, particularly in generating novel research ideas.Various benchmarks and frameworks have been developed to evaluate the quality of LLMgenerated research hypotheses.One approach involves the establishment of Ide-aBench, a benchmark designed to standardize the assessment of research ideas produced by LLMs [18].Another method introduces the Creativity Index, which incorporates supervised fine-tuning and Direct Preference Optimization (DPO) to evaluate originality, feasibility, impact, and reliability in idea generation [9].Additionally, the MOOSE-Chem multi-agent framework has been implemented, utilizing problem decomposition strategies to improve the quality of LLM-generated research hypotheses in chemistry [39].These efforts reflect a broader trend toward leveraging LLMs for structured scientific ideation.However, despite advancements in benchmark development and evaluation methodologies, existing approaches remain largely reliant on textual synthesis rather than structured reasoning or methodological integration.</p>
<p>The need for systematic frameworks that enhance the logical progression of research ideation and ensure scientific rigor continues to be a critical challenge.</p>
<p>Beyond direct idea generation, LLMs have been incorporated into structured frameworks to enhance the logical progression of research ideation.One such approach is the Chain-of-Ideas (CoI) Agent, an LLM-based system that organizes relevant literature into a structured chain, simulating the progressive development of a research domain and strengthening ideation capabilities [17].To systematically evaluate generated ideas, the Idea Arena protocol has been designed, ensuring that evaluation criteria align with human research preferences.Experimental results indicate that the CoI Agent outperforms conventional methods and produces research ideas of comparable quality to those generated by human researchers.</p>
<p>Additionally, ResearchAgent has been introduced as an iterative framework that refines research ideas through the integration of an academic graph and knowledge retrieval mechanisms.Multiple LLM-powered reviewing agents are employed to provide structured feedback, aligning evaluations with human-defined criteria.This structured review process enhances the clarity, novelty, and validity of generated ideas, demonstrating effectiveness across multiple disciplines [3].</p>
<p>Despite the substantial advancements of Large Language Models (LLMs) in scientific discovery, existing methods still exhibit several critical limitations.First, current research primarily focuses on generating research hypotheses or ideas but fails to systematically explore the finer-grained composition of methodological elements tailored to specific research questions.This limitation results in a lack of effective knowledge recombination mechanisms, rendering LLMs incapable of constructing genuinely innovative research pathways that align with established scientific methodologies.</p>
<p>Second, current LLM-driven research ideation methods heavily rely on providing extensive background information [17], where LLMs are exposed to a large volume of related literature to enhance their inferential capabilities.While this strategy enriches the contextual breadth of generated content, it compromises the traceability of research ideas, making it difficult to directly associate LLM-generated hypotheses with specific prior studies.Consequently, researchers often face challenges in verifying the theoretical foundations and scientific validity of these generated ideas.</p>
<p>Finally, when evaluating LLM-generated research hypotheses, existing approaches predominantly rely on human preference alignment, where assessments are based on subjective ratings or semantic similarity measures.However, there is a notable lack of objective metrics to rigorously quantify the scientific impact of these ideas.In particular, current research lacks systematic methods to evaluate the transformative potential or disruptive impact of newly generated methodologies, thereby making it challenging to distinguish LLM-generated research ideas from truly groundbreaking scientific discoveries.</p>
<p>These limitations underscore the necessity of developing a more systematic and intelligent scientific discovery framework based on problem-method matching, ensuring traceability, scientific rigor, and quantitative evaluation of research ideas.Such a framework would enable a more structured integration of methodological elements while incorporating quantitative analysis to assess the potential impact of newly generated methodologies.</p>
<p>Knowledge Combination and Recombinant Innovation in Scientific Discovery</p>
<p>Innovation fundamentally arises from the combination and recombination of knowledge [38,30].The concept of knowledge recombination has gained increasing attention in the literature, with over 1,000 articles in top management journals leveraging this framework to analyze scientific innovation [38].Recombinant innovation is regarded as a major driver of new idea generation, and its frequent occurrence in scientific research underscores the necessity of understanding how scientific knowledge is integrated and combined in academic publications [7].</p>
<p>Within the domain of scientific discovery, research questions and research methods serve as the fundamental building blocks that determine the novelty and impact of scientific contributions [22].Existing studies have examined scientific novelty through various combination-based perspectives, but have yet to fully address the temporal evolution and semantic complexity of research questions and methods.To bridge this gap, recent work has proposed a life-index novelty measurement, incorporating the frequency and age of research questions and methods, alongside semantic novelty assessment using deep learning and representation learning techniques [22].These advancements highlight the importance of systematically integrating research questions and methods to characterize scientific novelty.Despite these insights, current methodologies predominantly focus on evaluating novelty at the level of individual concepts, rather than systematically modeling how methodological elements are combined to address specific research questions.While research questions and methods both constitute integral knowledge elements in scientific articles, existing studies rarely explore how their structured integration contributes to groundbreaking discoveries.This limitation suggests the need for a systematic framework that formalizes the combination of research questions and methods to assess their transformative potential.</p>
<p>Moreover, the lack of structured mechanisms for problem-method matching has hindered the ability to predict which methodological innovations lead to significant scientific breakthroughs.Although LLM-based approaches have been employed to generate research ideas, they primarily rely on retrieving or synthesizing prior knowledge, rather than systematically aligning methods with research questions to facilitate novel knowledge recombination.This gap underscores the necessity of developing an intelligent framework that systematically integrates research methods as core knowledge elements and models their structured composition for scientific discovery.</p>
<p>The Necessity of the Disruption Index (DI) in Evaluating Scientific Breakthroughs</p>
<p>Traditional metrics for assessing the impact of scientific research, such as citation counts, h-index, and journal impact factor, have long been used as standard measures of scientific influence.However, these indicators primarily capture the magnitude of a study's dissemination rather than its ability to challenge existing paradigms [13? , 40].While citation counts are intuitive and widely adopted, they suffer from inherent limitations, including bias toward incremental research, ignoring negative citations, and reinforcing conservative citation behavior [6,26,10].Consequently, traditional bibliometric indicators often fail to distinguish between studies that reinforce the status quo and those that disrupt established knowledge structures.</p>
<p>To address these limitations, Funk and Owen-Smith (2017)introduced the Disruption Index (DI) as a metric to quantify the extent to which new technological advancements displace or reinforce existing knowledge [11].Inspired by prior literature on technological shifts, they argued that the dichotomy between competence-enhancing and competence-destroying innovations was insufficient for characterizing real-world technological evolution.Instead, they proposed that disruptiveness exists on a continuum, where some innovations incrementally improve existing knowledge, while others render previous technologies obsolete [1].</p>
<p>Originally developed to measure technological innovation using vast patent databases such as the U.S. Patent Citations Data File, the Disruption Index was later extended to scientific research by Wu et al. (2019), who applied the metric to bibliometrics [37].They demonstrated that DI could effectively differentiate groundbreaking discoveries from incremental advancements by analyzing its values in Nobel Prize-winning papers and comparing disruption levels between review papers and their original research articles.</p>
<p>The DI quantifies disruptiveness using the following formulation:
D = n i − n j n i + n j + n k ,(1)
where n i is the number of papers that cite the focal paper exclusively, n j represents papers citing both the focal paper and its references, and n k denotes papers that cite only the references of the focal paper [37].This formulation allows DI to capture the extent to which a research contribution redefines its field, rather than merely accumulating citations.</p>
<p>The necessity of DI stems from its ability to quantitatively evaluate scientific breakthroughs, offering a more precise alternative to traditional citationbased measures.As disruptive innovation is characterized by a paradigm shift that redirects collective attentio*, DI provides a robust framework for distinguishing transformative research from incremental progress [20].Given that existing LLM-based research ideation models primarily focus on generating novel ideas without evaluating their potential to challenge existing scientific conventions, integrating DI into scientific discovery frameworks can significantly enhance the assessment of research novelty and impact.These considerations highlight the limitations of existing citation-based indicators in capturing scientific breakthroughs and underscore the importance of incorporating DI into intelligent scientific discovery frameworks.A disruptionaware approach could enable more systematic evaluations of research impact, ensuring that novel problem-method combinations are assessed not just for their feasibility but also for their potential to drive substantial scientific advancements.</p>
<p>Our research proposes a novel scientific discovery paradigm that not only provides methodological support for evaluating scientific innovation but also establishes a new technological paradigm for intelligent research tools.Through question-methodology combinatorial logic, we have constructed a framework that achieves objective quantitative assessment of the disruptive potential of research ideas and generates corresponding reasoning chains.This study introduces an improved framework with three core modules to enhance disruptive knowledge prediction and method combination exploration.</p>
<p>Methodology</p>
<p>The Problem-Driven Method Exploration Module identifies potential method combinations based on specific research questions, providing innovative and targeted strategies.</p>
<p>The Disruptive Knowledge Prediction Module predicts the disruptive potential of given problems and methods using a deviation-awareness mechanism and a secondary learning approach to ensure accuracy and reliability.</p>
<p>The Dynamic Method Optimization Module iteratively refines method combinations based on disruptive index feedback, enhancing their disruptive potential.</p>
<p>This framework offers a systematic approach for researchers to uncover disruptive knowledge and drive scientific innovation.</p>
<p>Problem-Driven Method Exploration Module</p>
<p>To enhance the efficiency of method exploration and reduce resource consumption, this study designs a Problem-Driven Method Exploration Module that constructs a paper database indexed by problems and methods.Traditional approaches often require repeated evaluation of the relationship between new problems and paper abstracts, which not only increases computational costs but also reduces retrieval efficiency.To address this, we propose an efficient retrieval mechanism that rapidly identifies potential method candidates relevant to specific research problems.</p>
<p>The paper database is built upon a large-scale collection of academic literature.First, we preprocess textual information such as paper titles, abstracts, and keywords to remove redundant and noisy data, ensuring data accuracy and consistency.Subsequently, natural language processing (NLP) techniques are employed to extract the research problem and research method from each paper.These two elements are then indexed as key entries to facilitate efficient retrieval through a problem-driven approach.</p>
<p>During the method exploration process, when a new research problem P new is proposed, the system embeds it into a semantic vector space:
v Pnew = Embed(P new )
Then, a similarity function is applied to retrieve the top-k similar problems:
P sim = {P i | sim(v Pnew , v Pi ) ≥ δ}
The associated methods M sim = {M i | P i ∈ P sim } are collected, and a heuristic filtering mechanism H is applied:
M final = H(M sim , sim(•), rule(•))
Finally, the system constructs candidate problem-method pairs:
C = {(P new , M ) | M ∈ M final }
These selected candidates support downstream disruptive knowledge prediction and method optimization.</p>
<p>Disruptive Index Prediction Model Methodology</p>
<p>This study introduces a disruptive index prediction model consisting of interconnected sub-modules designed to precisely evaluate the innovative potential of specific problem-method combinations.Specifically, the overall module comprises three sub-modules: problem-method summary generation, identification, extraction, and refinement of key reference information, and final prediction of the disruptive index.</p>
<p>The first sub-module aims to automatically generate highly concise summaries for given problem-method pairs.To enhance the accuracy and logical coherence of generated summaries, we employ a summary generation model fine-tuned using Low-Rank Adaptation (LoRA) [15].This model effectively learns from existing real-world literature summaries within a targeted downstream task context.Additionally, a set of meticulously crafted prompts guide the model stepby-step in describing the logical relationships and details involved in combining problems and methods (detailed prompt design is provided in the Appendix).</p>
<p>Formally, given a problem-method pair (P, M ) and a task-specific prompt Prompt, the summary generation model G θ produces a concise summary S:
S = G θ (P, M, Prompt)
Through this approach, our trained model significantly outperforms current state-of-the-art methods, demonstrating superior logical consistency and completeness of information.</p>
<p>The second sub-module integrates both the identification of key reference documents and the extraction of essential information.Given the computationally intensive nature of precise matching across large literature databases, we initially filter candidate references based on the semantic similarity between the given problem-method combination and the problem-method indices from a structured literature database.</p>
<p>Let D be the structured database and (P, M ) the input pair.We first compute the similarity and select the top-k references:
R top = {R i ∈ D | sim((P, M ), (P i , M i )) ≥ δ}
This approach ensures that only references with highly relevant problemmethod associations are considered, constraining the selection to 100 references.Subsequently, we employ a frozen pre-trained model to perform a fine-grained semantic comparison between the generated summaries and candidate reference summaries, thereby accurately identifying the key source references relevant to the problem-method combination.</p>
<p>Let F denote the frozen model and S the generated summary.We obtain the extracted information I:
I = F (S, R top )
Once the key references are identified, we further refine the extracted information to reduce computational burden and minimize noise introduced by large-scale raw textual inputs.Inspired by the RAHA approach [19], we leverage a frozen pre-trained model to compare the generated summary with the identified reference summaries and extract only the most critical, relevant information required for the model input.This integrated method effectively captures complex citation relationships while ensuring the quality of the extracted input information, ultimately improving the efficiency and accuracy of subsequent predictions.</p>
<p>The third sub-module utilizes the generated problem-method summaries and extracted critical information to predict the disruptive index for problem-method combinations.This sub-module employs a prediction model fine-tuned via LoRA.During fine-tuning, supervision is provided using real summaries and extracted reference information.Formally, the prediction model D ϕ outputs a disruption score y based on S and I: y = D ϕ (S, I)</p>
<p>Due to the scarcity of highly disruptive papers in real-world datasets, potentially biasing predictions towards low-disruptive outcomes, we introduce an entropy-based weighted evaluation metric [4].</p>
<p>The weighted loss for entropy-aware learning is given by:
L entropy = N i=1 w i • ℓ(y i , ŷi ), w i = − log(p(ŷ i ))
This metric assigns higher weights to rare but highly disruptive samples, enhancing the model's capability to identify high-disruptiveness cases.</p>
<p>Furthermore, we design a secondary learning mechanism for challenging-toclassify samples, selecting the top 20% of instances with the highest prediction errors for reinforcement training.To prevent prediction drift caused by secondary training, we introduce a KL-divergence-based balancing mechanism to stabilize training outcomes.The KL divergence loss between primary and secondary distributions is defined as:
L KL = D KL (D primary ϕ ∥ D secondary ϕ )
To further stimulate the cognitive and evaluative capabilities of the model, we propose an iterative deviation-awareness mechanism.Specifically, after each prediction iteration, the results are fed back to the model, prompting self-awareness and evaluation of prediction deviations.Let θ t be the model parameters at iteration t, then parameter update is guided by the deviation gradient:
θ t+1 = θ t − η • ∇Deviation(ŷ t , y t )
Experimental results demonstrate that, compared to existing state-of-the-art methods, our proposed framework significantly and comprehensively improves performance in predicting the disruptive index of problem-method combinations.</p>
<p>Dynamic Method Optimization Module</p>
<p>The dynamic method optimization module is designed to iteratively refine method combinations based on feedback from the disruptive index, enhancing their disruptive potential over multiple optimization cycles.This process ensures that method selection and adjustments remain continuously optimized within the problem-method space to maximize impact.</p>
<p>At the core of this module lies a feedback-driven iterative optimization mechanism.Specifically, the system evaluates the disruptive index of a given problemmethod combination at each iteration and utilizes this information to guide subsequent modifications.</p>
<p>Formally, let (P, M t ) represent the problem and current method configuration at iteration t, and let y t be the corresponding disruptive index:
y t = D ϕ (S t , I t )
The optimization process follows a greedy algorithm, selecting adjustments in each iteration that locally maximize the disruptive index:
M t+1 = arg max M ′ ∈N (Mt) D ϕ (S ′ , I ′ )
where N (M t ) denotes the neighborhood of candidate method variants generated by replacing, augmenting, or modifying components of M t .By progressively favoring configurations that yield higher indices, the framework systematically converges towards method combinations with enhanced disruptive impact.</p>
<p>However, traditional greedy algorithms are prone to getting trapped in local optima, leading to stagnation in suboptimal solutions.To address this limitation, we incorporate a Greedy with Probabilistic Perturbation (GPP) approach [14], enhancing the global search capability.</p>
<p>In GPP, with a small probability ϵ, a non-optimal method M rand ∈ N (M t ) is accepted:
M t+1 = arg max M ′ D ϕ (S ′ , I ′ ), with probability 1 − ϵ M rand , with probability ϵ
This probabilistic mechanism allows the optimization process to escape local optima and explore method configurations with long-term disruptive potential.By leveraging this stochastic perturbation strategy, the optimization process effectively balances local search with global exploration.</p>
<p>Furthermore, to prevent stagnation in local optima and ensure comprehensive exploration of the method space, the optimization process integrates adaptive constraints.</p>
<p>Let C(M t ) denote constraint-based penalty terms for overfitting, we define the total objective with regularization as:
L opt = −D ϕ (S t , I t ) + λ • C(M t )
This ensures that method updates remain meaningful and generalizable across problem contexts.</p>
<p>The optimization module also includes an adaptive learning component that dynamically adjusts the weight of disruptive index feedback based on observed trends over multiple iterations.Let w t denote the weight at time t, updated based on temporal smoothing:
w t+1 = α • w t + (1 − α) • y t
This mechanism enables the system to prioritize consistently improving adjustments while attenuating noise introduced by short-term evaluation anomalies.</p>
<p>Empirical evaluations indicate that the incorporation of the GPP mechanism effectively enhances the global search capability, enabling the algorithm to escape local optima while maintaining efficient optimization performance.Overall, this dynamic optimization strategy significantly improves the identification and refinement of disruptive method combinations.By integrating iterative feedback, local optimization, adaptive constraints, and stochastic perturbation, this module provides a more efficient and systematic solution for method selection, ultimately fostering the discovery of high-impact scientific innovations.</p>
<p>Experiments</p>
<p>Data Sources</p>
<p>To evaluate the effectiveness of our proposed framework, we conduct experiments on three citation-based datasets: DBLP, PubMed, and PatSnap.Given the broad scope of these datasets, we focus on specific domains to ensure targeted analysis.</p>
<p>For DBLP, we extract records from 2011 to 2021 covering 14,533 publications from CCF-A conferences in the field of artificial intelligence.From PubMed, we select 96,612 research articles related to depression, published between 2015 and 2025.Lastly, for PatSnap, we use 6,677 patent records on medical robotics, with legal status marked as active, covering the period from 2020 to 2025.Further details on dataset characteristics and preprocessing are provided in Appendix A.</p>
<p>Baselines</p>
<p>To comprehensively assess the performance of our framework, we compare it against a set of established baselines, including both general-purpose large language models and specialized pre-trained models for scientific and technical domains.</p>
<p>We consider the following baselines:</p>
<p>(1) General-purpose LLMs: GPT and Claude, widely used for natural language understanding and text generation tasks.</p>
<p>(2) SciBERT [29], a pre-trained language model designed specifically for scientific text processing, which has demonstrated strong performance in scientific literature comprehension and reasoning tasks.</p>
<p>(3) RoBERTa [21], an optimized variant of BERT that enhances training robustness and performance across multiple NLP tasks.</p>
<p>(4) LLaMA 3 [12], the latest iteration in the LLaMA series of large-scale language models, which offers improved efficiency and reasoning capabilities.</p>
<p>(5) Qwen-7B [5], an autoregressive generative language model based on masked language modeling, optimized for diverse text generation and completion tasks.</p>
<p>All of these models are publicly accessible, allowing for reproducible benchmarking and comparative evaluation of our proposed framework.</p>
<p>Experimental Setup</p>
<p>Our experiments are conducted using PyTorch on four NVIDIA A800 GPUs.The ablation study is performed based on Qwen-7B.The model optimization is implemented using the Adam optimizer, with a learning rate set to 1e-5 and a gradient clipping threshold fixed at 0.2.</p>
<p>For model configurations, the problem-method summary generation model is set to handle a maximum input length of 1000 tokens, while the disruptive index prediction model is configured with a maximum input length of 7000 tokens.The batch size is consistently set to 4 across all experiments.The adapter used in the second LLM is configured with a low-rank dimension of 64.</p>
<p>We employ the PEFT (Parameter-Efficient Fine-Tuning) library to insert adapters into the last attention or feedforward layers of the LLM [23].This analysis is performed based on a principled examination of the forward components.</p>
<p>Both training and testing iterations are set to K = 5.For other baseline models, the number of training epochs is fixed at 5, and the optimal model checkpoint is selected based on validation set performance metrics.</p>
<p>Main Results</p>
<p>We present the main results on the DBLP, PubMed, and PatSnap datasets in Table 1.Our framework, which integrates two models along with the overall system, consistently outperforms existing state-of-the-art LLMs and PLMs across multiple evaluation metrics.Table 1 reports the cosine similarity and ROUGE scores between problem-method summaries generated by our framework and their corresponding ground-truth summaries.We use Qwen-32B as an example for evaluating the summarization performance.The fine-tuned model not only surpasses existing general-purpose LLMs such as GPT and Claude but also demonstrates superior performance over non-fine-tuned pre-trained models.</p>
<p>This confirms the effectiveness of our approach in refining the alignment between problem-method pairs and their corresponding textual representations.As shown in Table 2, we evaluate the effectiveness of our disruptive index prediction model based on four key metrics: MSE, MAE, weighted MSE (WMSE), and weighted MAE (WMAE).Our model, incorporating adaptive bias awareness and secondary sample learning, achieves lower error rates across all metrics, demonstrating its superiority over general-purpose LLMs, pre-trained language models (PLMs), and large language models (LLMs) fine-tuned on scientific tasks.The improvements in these evaluation metrics indicate that our framework effectively captures the disruptive potential of problem-method combinations with higher accuracy and robustness.As shown in Table 3, our full framework, designed for disruptive index prediction, outperforms general LLMs across all evaluation metrics.The consistent improvements in MSE, MAE, WMSE, and WMAE confirm the effectiveness of integrating problem-method pairs and disruptive index prediction to enhance scientific discovery.</p>
<p>Ablation Study</p>
<p>To analyze the contributions of individual components within our framework, we conduct an ablation study, as shown in Table 4. (1) Framework w/o fine-tuning the problem-method summarization model: Removing fine-tuning from the problem-method summarization model results in a noticeable performance degradation across all datasets.This demonstrates the importance of task-specific adaptation in improving the alignment between problem-method pairs and their textual representations.Without fine-tuning, the generated summaries exhibit lower quality, impacting the overall framework's ability to capture meaningful research insights.</p>
<p>(2) Framework w/o relevance assessment and information extraction: Excluding the step of relevance judgment and structured information extraction significantly reduces the effectiveness of disruptive index prediction.The removal of this module leads to an increase in MSE and MAE, as the model is unable to accurately capture contextual knowledge essential for evaluating problem-method disruptiveness.This highlights the necessity of refining input information before disruptive potential estimation.</p>
<p>(3) Framework w/o secondary learning: The absence of secondary learning-where high-error samples undergo further training-results in higher prediction errors across all four key evaluation metrics: MSE, MAE, WMSE, and WMAE.This confirms that the secondary learning mechanism enhances model robustness by mitigating the impact of hard-to-classify instances, ensuring better generalization.</p>
<p>(4) Framework w/o deviation-aware alignment: Removing deviationaware alignment leads to a decline in performance by increasing error rates and reducing model consistency.Without this module, the framework struggles to adjust predictions based on previously observed discrepancies, limiting its ability to refine predictions dynamically.</p>
<p>Overall, the ablation study confirms that each component plays a crucial role in improving the framework's predictive performance.The degradation in results when removing any of these modules underscores their importance in systematically enhancing problem-method integration and disruptive potential assessment.</p>
<p>Greedy Algorithm Optimization Results</p>
<p>To evaluate the effectiveness of our dynamic method optimization module, we conduct experiments measuring its ability to identify high-disruptiveness method combinations.Table 5 reports the hit rate (Disruptive Index &gt; 0.5) on three datasets.</p>
<p>Our framework leverages a Greedy with Probabilistic Perturbation (GPP) approach to iteratively optimize problem-method combinations based on the disruptive index.Compared to a standard greedy algorithm, GPP significantly improves the ability to escape local optima, resulting in superior method selection.Specifically, the model incorporating GPP achieves a higher disruptive index score across all datasets, demonstrating improved long-term optimization capabilities.The observed performance gains are attributed to two key advantages of GPP: (1) Balancing exploration and exploitation, where the probabilistic acceptance of non-optimal choices prevents premature convergence, and (2) Adaptive weighting of disruptive index feedback, which enables a more refined and responsive optimization trajectory.These mechanisms collectively enhance the ability of our framework to discover novel, high-impact problem-method combinations.</p>
<p>Conclusion</p>
<p>In this study, we propose a novel framework for scientific discovery that systematically integrates problem-method combinations with disruptive index prediction.Our approach leverages fine-tuned LLMs for problem-method summarization, an adaptive bias-aware alignment model for disruptive index estimation, and a dynamic optimization strategy incorporating Greedy with Probabilistic Perturbation (GPP) to iteratively refine method selection.</p>
<p>Empirical results on DBLP, PubMed, and PatSnap confirm the effectiveness of our framework.Compared to existing general-purpose LLMs, pre-trained language models, and baseline methods, our approach consistently achieves higher accuracy in problem-method summarization, disruptive index prediction, and high-impact method identification.The introduction of GPP significantly enhances search efficiency by balancing exploitation and exploration, ensuring the discovery of truly novel and disruptive scientific insights.</p>
<p>Our findings contribute to the advancement of AI-driven scientific discovery by demonstrating the value of structured problem-method integration and adaptive learning strategies.Future research may explore expanding the framework to broader domains and improving interpretability to further assist researchers in generating groundbreaking discoveries.</p>
<p>Limitations</p>
<p>While our proposed framework demonstrates strong performance in integrating problem-method combinations with disruptive index prediction, it has two primary limitations.</p>
<p>First, for entirely emerging scientific fields with minimal prior work, our framework may encounter challenges due to a lack of sufficient historical data.The effectiveness of the problem-method integration and disruptive index prediction relies on existing structured research literature.In domains with scarce prior knowledge, the search space for potential method combinations becomes significantly larger, reducing search efficiency and increasing the likelihood of suboptimal results.</p>
<p>Second, our framework involves a multi-step process that includes problemmethod summarization, source validation, information extraction, secondary learning, and deviation-aware alignment.While each step enhances accuracy, it also increases computational complexity and execution time.The sequential nature of these processes results in higher processing overhead, which may limit the scalability of our approach when applied to large-scale real-time applications.</p>
<p>Future research should explore ways to mitigate these limitations, including optimizing search strategies for data-scarce fields and improving computational efficiency through parallelization and adaptive learning techniques.</p>
<p>Fig. 1 .
1
Fig. 1.Enter Caption</p>
<p>Table 1 .
1
Comparison of Question-Method Pair Summarization Performance Across Different Datasets Using Cosine Similarity and ROUGE
ModelDBLPPubMedPatentSimilarity ROUGE Similarity ROUGE Similarity ROUGEGPT-4o0.5030.2060.4320.2250.4470.133GPT-4 Turbo0.5200.2050.3450.1870.4360.133Claude 3.50.4690.2040.3810.1540.3430.095Claude 3.70.4600.2140.4910.1390.2990.085Qwen-32B (Pre-trained) 0.5520.3150.4230.2420.4560.158Qwen-32B (Fine-tuned)0.5580.3250.5000.3240.6120.404</p>
<p>Table 2 .
2
Disruptive Index Prediction Using Summarization and Information Across Different Datasets Using MSE, MAE, WMSE, and WMAE
ModelDBLPPubMedPatentMSE MAE WMSE WMAE MSE MAE WMSE WMAE MSE MAE WMSE WMAEGPT-4o0.3607 0.5728 0.5821 0.7175 0.3255 0.5369 0.2321 0.4026 0.3018 0.5162 0.0884 0.1770GPT-4 Turbo0.3607 0.5728 0.5821 0.7175 0.3549 0.5621 0.1742 0.2919 0.3455 0.5594 0.6520 0.7707Claude 3.50.4346 0.6115 0.3162 0.4318 0.4269 0.6077 0.3029 0.4219 0.4242 0.5887 0.9875 0.9295Claude 3.70.4699 0.6580 0.2653 0.4131 0.4751 0.6593 0.2728 0.3773 0.4242 0.5887 0.9875 0.9295SciBERT0.3125 0.4153 0.3641 0.3817 0.4218 0.1642 0.3486 0.4357 0.4871 0.5092 0.4217 0.4561RoBERTa0.4351 0.5715 0.3105 0.3751 0.4017 0.4521 0.3465 0.4213 0.4154 0.5184 0.4156 0.4364LLaMA 30.5612 0.6143 0.3155 0.4182 0.4832 0.5961 0.5942 0.4118 0.5624 0.7334 0.3284 0.3912Qwen-7B0.6845 0.8223 0.4558 0.4526 0.4839 0.6587 0.1587 0.2908 0.6843 0.8151 0.5172 0.5241SciBERT(Fine-tuned) 0.0142 0.0247 0.5472 0.6781 0.0093 0.0145 0.3148 0.4207 0.0135 0.0241 0.4151 0.5124RoBERTa(Fine-tuned) 0.0091 0.0154 0.6245 0.6578 0.0075 0.0921 0.2947 0.4814 0.0183 0.0214 0.4521 0.4873LLaMA 3(Fine-tuned) 0.0124 0.0325 0.5175 0.5412 0.0091 0.0124 0.3541 0.4168 0.0265 0.3457 0.5142 0.5321Qwen-7B (Fine-tuned) 0.0052 0.0121 0.6172 0.6739 0.0020 0.0072 0.2533 0.4604 0.0144 0.0181 0.4325 0.4512</p>
<p>Table 3 .
3
Utilizing Question-Method Pairs to Predict Disruptive Index Results with MSE, MAE, WMSE, and WMAE Metrics
ModelDBLPPubMedPatentMSE MAE WMSE WMAE MSE MAE WMSE WMAE MSE MAE WMSE WMAEGPT-4o0.1191 0.3062 0.5887 0.7399 0.2053 0.4182 0.6545 0.7036 0.1216 0.3044 1.0043 0.9041GPT-4 Turbo 0.1597 0.3628 0.4771 0.6453 0.3289 0.5196 0.3690 0.5558 0.1523 0.3556 1.0803 0.9312Claude 3.50.2291 0.4268 0.9919 0.8767 0.1731 0.3742 1.0523 0.9006 0.2176 0.4201 1.2142 0.9428Claude 3.70.4319 0.6143 0.1355 0.3382 0.1146 0.3021 0.6827 0.7224 0.4594 0.6559 0.1437 0.3431Our Framework 0.0093 0.0111 0.0941 0.1364 0.0154 0.0342 0.1147 0.2142 0.0218 0.0412 0.1241 0.2962</p>
<p>Table 4 .
4
Ablation study of our framework using Qwen-7B across DBLP, PubMed, and Patent datasets with MSE, MAE, WMSE, and WMAE metrics.
Model VariantDBLPPubMedPatentMSE MAE WMSE WMAE MSE MAE WMSE WMAE MSE MAE WMSE WMAEFull Framework (Ours)0.0093 0.0111 0.0941 0.1364 0.0154 0.0342 0.1147 0.2142 0.0218 0.0412 0.1241 0.2962w/o Summarization Fine-tuning 0.0274 0.0461 0.1403 0.2093 0.0412 0.0623 0.1889 0.2563 0.0586 0.0721 0.2034 0.3121w/o Relevance + Extraction0.0351 0.0592 0.1881 0.2672 0.0526 0.0783 0.2102 0.2907 0.0735 0.0897 0.2345 0.3374w/o Secondary Learning0.0187 0.0329 0.1187 0.1766 0.0291 0.0503 0.1506 0.2384 0.0375 0.0594 0.1741 0.3022w/o Deviation-Aware Alignment 0.0216 0.0382 0.1279 0.1917 0.0318 0.0562 0.1663 0.2496 0.0414 0.0638 0.1862 0.3195</p>
<p>Table 5 .
5
Hit Rate (%) of High-Disruptiveness Method Combinations (Disruptive Index &gt; 0.5) Identified by Various Methods
MethodDBLP PubMed PatSnapGPT-4o (ChatGPT)16.4% 18.2% 14.9%Claude 3.515.7% 17.1% 14.3%Claude 3.717.2% 19.0% 15.6%Standard Greedy19.3% 21.5% 18.2%Greedy + GPP (Ours)26.3% 28.1% 24.6%Improvement over best LLM +9.1% +9.1% +9.0%
J. Chen et al.</p>
<p>Extra credit for disruption: Trend of disruption in radiology academic journals. A Abu-Omar, P Kennedy, M Yakub, J Robbins, A Yassin, N Verma, M Scaglione, F Khosa, Clinical Radiology. 77122022</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. J Baek, S K Jauhar, S Cucerzan, S J Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>A characterization of entropy in terms of information loss. J C Baez, T Fritz, T Leinster, Entropy. 13112011</p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, B Hui, L Ji, M Li, J Lin, R Lin, D Liu, G Liu, C Lu, K Lu, J Ma, R Men, X Ren, X Ren, C Tan, S Tan, J Tu, P Wang, S Wang, W Wang, S Wu, B Xu, J Xu, A Yang, H Yang, J Yang, S Yang, Y Yao, B Yu, H Yuan, Z Yuan, J Zhang, X Zhang, Y Zhang, Z Zhang, C Zhou, J Zhou, X Zhou, T Zhu, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>The incidence and role of negative citations in science. C Catalini, N Lacetera, A Oettl, Proceedings of the National Academy of Sciences. 112452015</p>
<p>Scientific knowledge combination in networks: New perspectives on analyzing knowledge absorption and integration. H Chen, J Liu, Z Liu, EEKE/AII@ JCDL. 2023</p>
<p>Empowering ai as autonomous researchers: Evaluating llms in generating novel research ideas through automated metrics. D Dasgupta, A Mondal, P P Chakrabarti, 2nd AI4Research Workshop: Towards a Knowledgegrounded Scientific Research Lifecycle. </p>
<p>Empowering AI as autonomous researchers: Evaluating LLMs in generating novel research ideas through automated metrics. D Dasgupta, A Mondal, P P Chakrabarti, 2nd AI4Research Workshop: Towards a Knowledge-grounded Scientific Research Lifecycle. 2024</p>
<p>Tradition and innovation in scientists' research strategies. J G Foster, A Rzhetsky, J A Evans, American sociological review. 8052015</p>
<p>A dynamic network measure of technological change. R J Funk, J Owen-Smith, Management science. 6332017</p>
<p>A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>The impact of collaboration and knowledge networks on citations. J Guan, Y Yan, J J Zhang, Journal of Informetrics. 1122017</p>
<p>A probabilistic greedy search algorithm for combinatorial optimisation with application to the set covering problem. M Haouari, J Chaouachi, Journal of the Operational Research Society. 5372002</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, ICLR. 1232022</p>
<p>The burden of knowledge and the "death of the renaissance man": Is innovation getting harder?. B F Jones, The Review of Economic Studies. 7612009</p>
<p>L Li, W Xu, J Guo, R Zhao, X Li, Y Yuan, B Zhang, Y Jiang, Y Xin, R Dang, arXiv:2410.13185Chain of ideas: Revolutionizing research via novel idea development with llm agents. 2024arXiv preprint</p>
<p>C Liang, L Huang, J Fang, H Dou, W Wang, Z F Wu, Y Shi, J Zhang, X Zhao, Y Liu, arXiv:2412.11767Idea-bench: How far are generative models from professional designing?. 2024arXiv preprint</p>
<p>C Lin, J Ren, G He, Z Jiang, H Yu, X Zhu, arXiv:2402.08874Recurrent alignment with hard attention for hierarchical text rating. 2024arXiv preprint</p>
<p>New directions in science emerge from disconnection and discord. Y Lin, J A Evans, L Wu, Journal of Informetrics. 1611012342022</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Combination of research questions and methods: A new measurement of scientific novelty. Z Luo, W Lu, J He, Y Wang, Journal of Informetrics. 1621012822022</p>
<p>Peft: State-of-the-art parameter-efficient fine-tuning methods. S Mangrulkar, S Gugger, L Debut, Y Belkada, S Paul, B Bossan, Peft: State-of-the-art parameter-efficient fine-tuning methods. 2022</p>
<p>A new method for identifying recombinations of existing knowledge associated with high-impact innovation. S Mukherjee, B Uzzi, B Jones, M Stringer, Journal of Product Innovation Management. 3322016</p>
<p>An evolutionary theory of economic change. R R Nelson, S G Winter, 1985harvard university press</p>
<p>Global citation inequality is on the rise. M W Nielsen, J P Andersen, Proceedings of the National Academy of Sciences. 1187e20122081182021</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 352022</p>
<p>Papers and patents are becoming less disruptive over time. M Park, E Leahey, R J Funk, Nature. 61379422023</p>
<p>N Reimers, I Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>Search and recombination process to innovate: a review of the empirical evidence and a research agenda. T Savino, A Messeni Petruzzelli, V Albino, International Journal of Management Reviews. 1912017</p>
<p>Business cycles: A theoretical, historical and statistical analysis of the capitalist process. J A Schumpeter, Acessado em. 41939. 1964</p>
<p>Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references. I Tahamtan, L Bornmann, Journal of informetrics. 1232018</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Atypical combinations and scientific impact. B Uzzi, S Mukherjee, M Stringer, B Jones, Science. 34261572013</p>
<p>Collaboration and creativity: The small world problem. B Uzzi, J Spiro, American journal of sociology. 11122005</p>
<p>Bias against novelty in science: A cautionary tale for users of bibliometric indicators. J Wang, R Veugelers, P Stephan, Research Policy. 4682017</p>
<p>Large teams develop and small teams disrupt science and technology. L Wu, D Wang, J A Evans, Nature. 56677442019</p>
<p>A knowledge recombination perspective of innovation: review and new research directions. T Xiao, M Makhija, S Karim, Journal of Management. 4862022</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Z Yang, W Liu, B Gao, T Xie, Y Li, W Ouyang, S Poria, E Cambria, D Zhou, arXiv:2410.070762024arXiv preprint</p>
<p>Team size, research variety, and research performance: do coauthors' coauthors matter. N Zhu, C Liu, Z Yang, Journal of Informetrics. 1541012052021</p>            </div>
        </div>

    </div>
</body>
</html>
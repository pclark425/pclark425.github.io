<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8700 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8700</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8700</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-156.html">extraction-schema-156</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <p><strong>Paper ID:</strong> paper-258822829</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.11597v1.pdf" target="_blank">Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments</a></p>
                <p><strong>Paper Abstract:</strong> Trustworthiness of artificially intelligent agents is vital for the acceptance of human-machine teaming in industrial manufacturing environments. Predictable behaviours and explainable (and understandable) rationale allow humans collaborating with (and building) these agents to understand their motivations and therefore validate decisions that are made. To that aim, we make use of G\"ardenfors's cognitively inspired Conceptual Space framework to represent the agent's knowledge using concepts as convex regions in a space spanned by inherently comprehensible quality dimensions. A simple typicality quantification model is built on top of it to determine fuzzy category membership and classify instances interpretably. We apply it on a use case from the manufacturing domain, using objects' physical properties obtained from cobots' onboard sensors and utilisation properties from crowdsourced commonsense knowledge available at public knowledge bases. Such flexible knowledge representation based on property decomposition allows for data-efficient representation learning of typically highly specialist or specific manufacturing artefacts. In such a setting, traditional data-driven (e.g., computer vision-based) classification approaches would struggle due to training data scarcity. This allows for comprehensibility of an AI agent's acquired knowledge by the human collaborator thus contributing to trustworthiness. We situate our approach within an existing explainability framework specifying explanation desiderata. We provide arguments for our system's applicability and appropriateness for different roles of human agents collaborating with the AI system throughout its design, validation, and operation.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8700.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8700.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConceptualSpaces</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual Spaces (Gärdenfors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cognitively motivated geometric representational framework that encodes concepts as convex regions in a multi-dimensional space of interpretable quality domains, enabling prototype-based similarity, concept combination, and mappings between continuous sensory data and symbolic labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conceptual spaces: The geometry of thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Conceptual spaces (geometric prototype regions)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts are represented as convex regions in a space spanned by interpretable quality domains (e.g., colour, size, utilisation). Instances are vectors of quality-dimension values; similarity and typicality arise from geometry (distance, region membership); algebraic operations on regions model composition and metaphor/metonymy.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid (continuous geometric intermediary between symbolic and subsymbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Categorization; typicality/prototype effects; similarity vs relatedness; concept combination; few-shot / data-efficient learning; mapping from sensory inputs to symbols (grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper argues and demonstrates (in simulation) that conceptual spaces provide an interpretable middle layer: prototypes and convex regions enable fuzzy typicality scoring (used to classify objects in a cobot simulation). Cites empirical neural evidence (grid/hexadirectional codes) supporting geometric organisation of concepts in brain. Shows practical gains for data-scarce industrial artefacts by combining physical and utilisation dimensions and using geometric prototypes for classification and explainable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Presented as a 'middle ground' between symbolic and neural distributed representations: it preserves continuous sensor-grounding like neural models while making concepts explicit/symbolic via region/prototype grounding; favoured over purely data-driven deep-vision approaches in data-scarce domains due to better data-efficiency and inherent interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Current implementation simplifies spaces with orthogonal basis vectors and does not yet model property correlations; selection of appropriate quality domains is critical and non-trivial; empirical validation with humans (behavioral/fMRI) and full real-world deployment remain future work.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Conceptual spaces can bridge neuro-symbolic gap by grounding symbols in interpretable continuous structure, naturally produce prototype/typicality effects and similarity metrics, support algebraic concept operations, and facilitate trust/certifiability via inherently interpretable representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8700.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8700.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mu-w model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>µw-model (typicality / fuzzy classification model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prototype-inspired, cognitively motivated typicality model that assigns a membership (µ) per quality-dimension and a weight (w) per dimension to compute an instance's representativeness vector and overall typicality for concepts, yielding interpretable fuzzy classifications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Prototype + weighted-dimension fuzzy membership model (µw-model)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Each concept has prototypical values on quality dimensions; for an instance, a membership function µ quantifies typicality per dimension and weights w quantify dimension importance; the instance vector is a weighted combination of µ values and prototype basis vectors, and concept typicality is the vector norm.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>feature-based, prototype, fuzzy hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Typicality effects in categorization; prototype theory; classification under limited data; explainable decision justification (local and global explanations).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Applied in the cobot simulation: µ and w are estimated from labelled instances and crowdsourced utilisation data; the model successfully produces interpretable classifications where utilisation properties (with higher weights) can override surface similarity, and flags disputable classifications for review. Visualisations of membership and weights provided to users in qualitative validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Compared implicitly to black-box vision/deep-learning classifiers: µw is more interpretable and more data-efficient for specialist objects; compared to pure symbolic systems it retains continuous graded membership; compared to exemplar-based approaches it abstracts via prototypes and weighted dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Weights currently arbitrated or derived from heuristic cognitive hypotheses (inverse variability); membership sometimes binary for utilisation heuristics; no large-scale quantitative evaluation presented; may not capture interdependencies between dimensions (assumes independence in current instantiation).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Provides an explicit, cognitively-motivated mechanism to quantify typicality and feature importance that supports interpretable classifications, debugging, counterfactual reasoning, and human-understandable explanations in operational systems.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8700.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8700.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PrototypeTheory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototype theory / prototypical representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cognitive-semantic account where categories are centered on prototypical exemplars and graded membership, explaining typicality and family-resemblance effects in categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cognition and Categorization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Prototype representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Categories are organized around prototypes (central tendency exemplars); individual items have graded membership based on similarity to prototype(s); explains typicality gradients and non-necessary-and-sufficient feature sets.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>feature-based, prototype</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Typicality gradients; family resemblance effects; graded categorization and category-based induction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used as conceptual/theoretical foundation for the µw-model and Conceptual Spaces instantiation; motivates using prototypes and graded µ-membership functions to capture human-like categorization effects.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Contrasted with classical rule-based symbolic accounts (which require necessary and sufficient features) and with exemplar models (which store instances); prototype theory chosen for its cognitive plausibility and fit with geometric conceptual spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Prototype accounts can struggle with certain induction tasks and contexts where explicit rules or causal/theory knowledge dominate; the paper acknowledges need to incorporate causal/essentialist structures for some concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Prototype structure aligns naturally with geometric conceptual spaces and supports interpretable graded classification; when combined with dimension weights, it can model differential importance of features in different domains (e.g., utilisation vs surface properties).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8700.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8700.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SymbolicRep</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic representations (classical symbolic knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional symbolic knowledge representations using discrete symbols and logical relations (e.g., ontologies, synsets) that are human-meaningful but face challenges in modelling concept acquisition from continuous sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Symbolic representations (ontologies, symbols, logical structures)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts are encoded as discrete symbols linked by relations and rules; readily supports logical reasoning, explicit labels and ontological structure but lacks native handling of graded similarity or continuous sensory grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Logical inference; label grounding; symbolic reasoning relevant to certification and human-readable conceptualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper notes symbolic representations' interpretability and amenability to logical calculus but highlights their difficulty modelling concept acquisition from sensorimotor data; motivates need for an intermediary geometric representation to ground symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Presented as complementary to conceptual spaces: symbols provide labels/meaning but benefit from grounding in continuous geometric spaces; purely symbolic systems lack the graded similarity and data-driven acquisition strengths of geometric/subsymbolic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Problematic for learning from continuous/ambiguous sensory data and for modelling prototypicality and graded membership; mapping between neural representations and symbols is a known challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Symbolic systems are useful for high-level reasoning/certification but should be interfaced with grounded continuous representations (e.g., conceptual spaces) for embodied AI in complex environments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8700.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8700.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DistributedNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributed / Deep neural network representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Subsymbolic representations learned by deep neural networks that encode regularities of sensory input in distributed activation patterns; powerful for perception but often opaque and explanatorily limited.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Distributed neural representations (deep learning feature embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Knowledge is encoded in high-dimensional, continuous activation patterns and weights learned from data; representations support robust perceptual mapping but are not directly interpretable or symbol-grounded.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed / subsymbolic</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Perceptual classification (e.g., object recognition); learning from large datasets; high-performance vision tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper frames deep neural approaches as high-performing but 'black-box' and problematic for trust/certification in safety-critical industrial settings; contrasts them with property-decomposition/geometric approaches better suited to data-scarce specialist domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Deep networks are powerful for large-data perceptual tasks but are less interpretable; conceptual spaces + µw-model advocated when training data scarce and interpretability required; conceptual spaces can act as an interface between neural encodings and symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Opacity, lack of inherent explanations, and data hunger are limitations for industrial/rare-object contexts; post-hoc XAI is common but less preferable to inherent interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Suggests integrating neural perceptual detectors with interpretable geometric representations to combine perceptual power with explainability and symbol grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8700.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8700.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KnowledgeGraphs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic networks / knowledge graphs (ConceptNet, WordNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Symbolic/crowdsourced structured resources (e.g., ConceptNet, WordNet) encoding commonsense relations (UsedFor, PartOf, MadeOf) used to supply utilisation properties and background knowledge for concept modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conceptnet 5.5: An open multilingual graph of general knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Semantic network / knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Entities and relations are nodes and edges in a graph encoding commonsense knowledge (e.g., `drill` UsedFor `drilling`); values and relation weights can be extracted programmatically to populate utilisation quality-dimensions in conceptual spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / graph-structured</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Commonsense reasoning; property/utilisation extraction for classification; bootstrapping of few-shot concept understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper uses ConceptNet and WordNet programmatically to extract utilisation properties which, when integrated into conceptual space dimensions and weighted appropriately, improve classification and interpretability in simulation; demonstrates a pipeline for grounding utilisation features from crowdsourced knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Knowledge graphs supply higher-level utilisation and causal information not directly available from sensory detectors; combined with conceptual spaces they provide complementary symbolic constraints and improve interpretability versus raw neural features alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Crowdsourced responses are noisy and require heuristics (grouping semantically similar responses, pruning low-weight edges); current heuristics yield binary utilisation values rather than continuous strengths — a noted limitation for fine-grained typicality.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Integration of semantic networks with geometric conceptual representations enables richer, human-aligned concept descriptions (including affordances/uses) and supports explainable classification in applied systems.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8700.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8700.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PropertyDecomp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Property decomposition / feature-based (utilisation + physical properties)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representing concepts as decompositions into interpretable quality dimensions (physical properties like colour/size and utilisation/affordance properties) that are explicitly measured and weighted for classification and explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Property-decomposition (interpretable feature-based conceptual representation)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts are defined by values across a selected set of interpretable quality dimensions (dimensions may be physical sensor-derived or semantic utilisation-derived); each dimension has a membership function and weight contributing to overall concept typicality.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>feature-based, hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Ad-hoc classification; few-shot learning; operator-understandable explanation; affordance-based inference; typicality judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper demonstrates in simulation that decomposing concepts into physical and utilisation properties yields data-efficient learning and more comprehensible classification rationale (e.g., utilisation can override surface similarity), supporting trust and debugging by human users.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Favoured over end-to-end vision classifiers in data-scarce domains because explicit features and weights enable interpretability; complements symbolic knowledge graphs and conceptual spaces by making dimensions explicit and inspectable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Requires careful selection and modelling of quality dimensions; current system often assumes independence of dimensions and uses heuristics for some utilisation values; capturing correlations and causal dependencies remains future work.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Property decomposition makes concept representations inherently comprehensible, supports frugal AI (data-efficiency), and facilitates human-machine teaming by providing inspectable and modifiable component explanations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8700.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8700.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FuzzyMembership</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fuzzy membership / fuzzy set representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of fuzzy-set membership functions to quantify graded typicality of an instance with respect to a concept along continuous quality dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fuzzy sets.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Fuzzy membership across quality dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Each quality-dimension has a membership function µ (fuzzy set) that returns a graded score for how representative an instance's value is of the prototype; these per-dimension scores are combined (with weights) into an overall typicality measure.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>fuzzy / graded, feature-based</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Typicality gradients in categorization; graded membership and disputable category judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper employs fuzzy membership (µ) to quantify per-dimension typicality and uses the normative vector combination to produce human-intelligible typicality scores; this supports flagging of disputable classifications and generation of natural-language explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Fuzzy graded membership bridges strict symbolic membership (binary) and opaque distributed embeddings by providing graded, interpretable scores; when combined with prototype geometry it aligns with human categorization phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>The empirical shape/parameterisation of membership functions depends on observed distributions and heuristic choices; some utilisation extraction currently yields binary values limiting fuzzy granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Fuzzy membership formalises human-like graded category membership within the conceptual-space/prototype framework and provides a mechanism for interpretable uncertainty and counterfactual reasoning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8700.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8700.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EventForceResult</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Event representation via force and result vectors (Conceptual Space extension)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed extension of the conceptual space framework to represent events as force and result vectors, enabling geometric encoding of event structure and potentially task typicality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Event representation via force-result vectors (conceptual-space extension)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Events are modelled as vectors capturing forces (agents' actions) and resulting changes; these vectors become dimensions or patterns in an event conceptual space enabling typicality judgments and event classification.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid geometric extension (event-level)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Event recognition; gauging typicality of task execution; affordance inference; action recognition from video.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper cites recent work proposing force/result vector event encodings and suggests this as promising for shop-floor task typicality and utilisation inference, but notes interpretability of force-pattern dimensions is a recognised challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Presented as an extension of object-level conceptual spaces to events; offers a structured, geometric alternative to purely statistical action-recognition methods, with potential gains in interpretability if force/result dimensions can be made comprehensible.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Interpretability of force vectors is challenging; methods and user-validated mappings from force patterns to human-understandable dimensions remain to be developed and validated.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>If interpretable event-dimensions can be defined, conceptual-space event representations could support explainable reasoning about tasks and action typicality in embodied human-AI teams.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8700.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8700.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeuroSymbolicBridge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neurosymbolic intermediary / hybrid mapping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The conceptual-space level is proposed as an intermediary representation enabling systematic mapping between continuous neural encodings and discrete symbolic representations to improve interpretability and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Neurosymbolic intermediary (conceptual-space as lingua franca)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>A middle-level geometric representation abstracts sensory inputs into interpretable points/prototypes and grounds symbols onto these prototypes, enabling bidirectional mapping between distributed neural features and symbolic labels.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid (neural-symbolic interface)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Symbol grounding; interpretability; explainable classification; mapping between perception and language/symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper argues conceptual spaces function well as a mapping layer between neural detectors and symbolic knowledge bases, supporting explainability, debugging, and certifiability; used operationally in their pipeline combining sensor-extracted properties with ConceptNet-derived utilisation features.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Framed as preferable to direct mapping from neural embeddings to symbols (opaque) or purely symbolic systems (ungrounded), offering a structured and interpretable interface between subsymbolic perception and symbolic cognition.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Realising systematic neurosymbolic mappings in practice is non-trivial; empirical neuroscience validation (e.g., fMRI studies) and formalised mapping algorithms are future work.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>A middle-level geometric representation can operationally bridge perception and symbolic reasoning, enabling more transparent and trustworthy AI systems in embodied contexts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conceptual spaces: The geometry of thought <em>(Rating: 2)</em></li>
                <li>Conceptnet 5.5: An open multilingual graph of general knowledge <em>(Rating: 2)</em></li>
                <li>Organizing conceptual knowledge in humans with a gridlike code <em>(Rating: 2)</em></li>
                <li>Cognition and Categorization <em>(Rating: 2)</em></li>
                <li>Fuzzy sets. <em>(Rating: 1)</em></li>
                <li>Reasoning about categories in conceptual spaces <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8700",
    "paper_id": "paper-258822829",
    "extraction_schema_id": "extraction-schema-156",
    "extracted_data": [
        {
            "name_short": "ConceptualSpaces",
            "name_full": "Conceptual Spaces (Gärdenfors)",
            "brief_description": "A cognitively motivated geometric representational framework that encodes concepts as convex regions in a multi-dimensional space of interpretable quality domains, enabling prototype-based similarity, concept combination, and mappings between continuous sensory data and symbolic labels.",
            "citation_title": "Conceptual spaces: The geometry of thought",
            "mention_or_use": "use",
            "representational_format_name": "Conceptual spaces (geometric prototype regions)",
            "representational_format_description": "Concepts are represented as convex regions in a space spanned by interpretable quality domains (e.g., colour, size, utilisation). Instances are vectors of quality-dimension values; similarity and typicality arise from geometry (distance, region membership); algebraic operations on regions model composition and metaphor/metonymy.",
            "format_type": "hybrid (continuous geometric intermediary between symbolic and subsymbolic)",
            "cognitive_task_or_phenomenon": "Categorization; typicality/prototype effects; similarity vs relatedness; concept combination; few-shot / data-efficient learning; mapping from sensory inputs to symbols (grounding).",
            "key_findings": "Paper argues and demonstrates (in simulation) that conceptual spaces provide an interpretable middle layer: prototypes and convex regions enable fuzzy typicality scoring (used to classify objects in a cobot simulation). Cites empirical neural evidence (grid/hexadirectional codes) supporting geometric organisation of concepts in brain. Shows practical gains for data-scarce industrial artefacts by combining physical and utilisation dimensions and using geometric prototypes for classification and explainable outputs.",
            "comparison_with_other_formats": "Presented as a 'middle ground' between symbolic and neural distributed representations: it preserves continuous sensor-grounding like neural models while making concepts explicit/symbolic via region/prototype grounding; favoured over purely data-driven deep-vision approaches in data-scarce domains due to better data-efficiency and inherent interpretability.",
            "limitations_or_counter_evidence": "Current implementation simplifies spaces with orthogonal basis vectors and does not yet model property correlations; selection of appropriate quality domains is critical and non-trivial; empirical validation with humans (behavioral/fMRI) and full real-world deployment remain future work.",
            "theoretical_claims_or_implications": "Conceptual spaces can bridge neuro-symbolic gap by grounding symbols in interpretable continuous structure, naturally produce prototype/typicality effects and similarity metrics, support algebraic concept operations, and facilitate trust/certifiability via inherently interpretable representations.",
            "uuid": "e8700.0"
        },
        {
            "name_short": "mu-w model",
            "name_full": "µw-model (typicality / fuzzy classification model)",
            "brief_description": "A prototype-inspired, cognitively motivated typicality model that assigns a membership (µ) per quality-dimension and a weight (w) per dimension to compute an instance's representativeness vector and overall typicality for concepts, yielding interpretable fuzzy classifications.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "Prototype + weighted-dimension fuzzy membership model (µw-model)",
            "representational_format_description": "Each concept has prototypical values on quality dimensions; for an instance, a membership function µ quantifies typicality per dimension and weights w quantify dimension importance; the instance vector is a weighted combination of µ values and prototype basis vectors, and concept typicality is the vector norm.",
            "format_type": "feature-based, prototype, fuzzy hybrid",
            "cognitive_task_or_phenomenon": "Typicality effects in categorization; prototype theory; classification under limited data; explainable decision justification (local and global explanations).",
            "key_findings": "Applied in the cobot simulation: µ and w are estimated from labelled instances and crowdsourced utilisation data; the model successfully produces interpretable classifications where utilisation properties (with higher weights) can override surface similarity, and flags disputable classifications for review. Visualisations of membership and weights provided to users in qualitative validation.",
            "comparison_with_other_formats": "Compared implicitly to black-box vision/deep-learning classifiers: µw is more interpretable and more data-efficient for specialist objects; compared to pure symbolic systems it retains continuous graded membership; compared to exemplar-based approaches it abstracts via prototypes and weighted dimensions.",
            "limitations_or_counter_evidence": "Weights currently arbitrated or derived from heuristic cognitive hypotheses (inverse variability); membership sometimes binary for utilisation heuristics; no large-scale quantitative evaluation presented; may not capture interdependencies between dimensions (assumes independence in current instantiation).",
            "theoretical_claims_or_implications": "Provides an explicit, cognitively-motivated mechanism to quantify typicality and feature importance that supports interpretable classifications, debugging, counterfactual reasoning, and human-understandable explanations in operational systems.",
            "uuid": "e8700.1"
        },
        {
            "name_short": "PrototypeTheory",
            "name_full": "Prototype theory / prototypical representation",
            "brief_description": "A cognitive-semantic account where categories are centered on prototypical exemplars and graded membership, explaining typicality and family-resemblance effects in categorization.",
            "citation_title": "Cognition and Categorization",
            "mention_or_use": "mention",
            "representational_format_name": "Prototype representation",
            "representational_format_description": "Categories are organized around prototypes (central tendency exemplars); individual items have graded membership based on similarity to prototype(s); explains typicality gradients and non-necessary-and-sufficient feature sets.",
            "format_type": "feature-based, prototype",
            "cognitive_task_or_phenomenon": "Typicality gradients; family resemblance effects; graded categorization and category-based induction.",
            "key_findings": "Used as conceptual/theoretical foundation for the µw-model and Conceptual Spaces instantiation; motivates using prototypes and graded µ-membership functions to capture human-like categorization effects.",
            "comparison_with_other_formats": "Contrasted with classical rule-based symbolic accounts (which require necessary and sufficient features) and with exemplar models (which store instances); prototype theory chosen for its cognitive plausibility and fit with geometric conceptual spaces.",
            "limitations_or_counter_evidence": "Prototype accounts can struggle with certain induction tasks and contexts where explicit rules or causal/theory knowledge dominate; the paper acknowledges need to incorporate causal/essentialist structures for some concepts.",
            "theoretical_claims_or_implications": "Prototype structure aligns naturally with geometric conceptual spaces and supports interpretable graded classification; when combined with dimension weights, it can model differential importance of features in different domains (e.g., utilisation vs surface properties).",
            "uuid": "e8700.2"
        },
        {
            "name_short": "SymbolicRep",
            "name_full": "Symbolic representations (classical symbolic knowledge)",
            "brief_description": "Traditional symbolic knowledge representations using discrete symbols and logical relations (e.g., ontologies, synsets) that are human-meaningful but face challenges in modelling concept acquisition from continuous sensory input.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representational_format_name": "Symbolic representations (ontologies, symbols, logical structures)",
            "representational_format_description": "Concepts are encoded as discrete symbols linked by relations and rules; readily supports logical reasoning, explicit labels and ontological structure but lacks native handling of graded similarity or continuous sensory grounding.",
            "format_type": "symbolic",
            "cognitive_task_or_phenomenon": "Logical inference; label grounding; symbolic reasoning relevant to certification and human-readable conceptualizations.",
            "key_findings": "Paper notes symbolic representations' interpretability and amenability to logical calculus but highlights their difficulty modelling concept acquisition from sensorimotor data; motivates need for an intermediary geometric representation to ground symbols.",
            "comparison_with_other_formats": "Presented as complementary to conceptual spaces: symbols provide labels/meaning but benefit from grounding in continuous geometric spaces; purely symbolic systems lack the graded similarity and data-driven acquisition strengths of geometric/subsymbolic methods.",
            "limitations_or_counter_evidence": "Problematic for learning from continuous/ambiguous sensory data and for modelling prototypicality and graded membership; mapping between neural representations and symbols is a known challenge.",
            "theoretical_claims_or_implications": "Symbolic systems are useful for high-level reasoning/certification but should be interfaced with grounded continuous representations (e.g., conceptual spaces) for embodied AI in complex environments.",
            "uuid": "e8700.3"
        },
        {
            "name_short": "DistributedNN",
            "name_full": "Distributed / Deep neural network representations",
            "brief_description": "Subsymbolic representations learned by deep neural networks that encode regularities of sensory input in distributed activation patterns; powerful for perception but often opaque and explanatorily limited.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representational_format_name": "Distributed neural representations (deep learning feature embeddings)",
            "representational_format_description": "Knowledge is encoded in high-dimensional, continuous activation patterns and weights learned from data; representations support robust perceptual mapping but are not directly interpretable or symbol-grounded.",
            "format_type": "distributed / subsymbolic",
            "cognitive_task_or_phenomenon": "Perceptual classification (e.g., object recognition); learning from large datasets; high-performance vision tasks.",
            "key_findings": "Paper frames deep neural approaches as high-performing but 'black-box' and problematic for trust/certification in safety-critical industrial settings; contrasts them with property-decomposition/geometric approaches better suited to data-scarce specialist domains.",
            "comparison_with_other_formats": "Deep networks are powerful for large-data perceptual tasks but are less interpretable; conceptual spaces + µw-model advocated when training data scarce and interpretability required; conceptual spaces can act as an interface between neural encodings and symbols.",
            "limitations_or_counter_evidence": "Opacity, lack of inherent explanations, and data hunger are limitations for industrial/rare-object contexts; post-hoc XAI is common but less preferable to inherent interpretability.",
            "theoretical_claims_or_implications": "Suggests integrating neural perceptual detectors with interpretable geometric representations to combine perceptual power with explainability and symbol grounding.",
            "uuid": "e8700.4"
        },
        {
            "name_short": "KnowledgeGraphs",
            "name_full": "Semantic networks / knowledge graphs (ConceptNet, WordNet)",
            "brief_description": "Symbolic/crowdsourced structured resources (e.g., ConceptNet, WordNet) encoding commonsense relations (UsedFor, PartOf, MadeOf) used to supply utilisation properties and background knowledge for concept modelling.",
            "citation_title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "mention_or_use": "use",
            "representational_format_name": "Semantic network / knowledge graph",
            "representational_format_description": "Entities and relations are nodes and edges in a graph encoding commonsense knowledge (e.g., `drill` UsedFor `drilling`); values and relation weights can be extracted programmatically to populate utilisation quality-dimensions in conceptual spaces.",
            "format_type": "symbolic / graph-structured",
            "cognitive_task_or_phenomenon": "Commonsense reasoning; property/utilisation extraction for classification; bootstrapping of few-shot concept understanding.",
            "key_findings": "Paper uses ConceptNet and WordNet programmatically to extract utilisation properties which, when integrated into conceptual space dimensions and weighted appropriately, improve classification and interpretability in simulation; demonstrates a pipeline for grounding utilisation features from crowdsourced knowledge.",
            "comparison_with_other_formats": "Knowledge graphs supply higher-level utilisation and causal information not directly available from sensory detectors; combined with conceptual spaces they provide complementary symbolic constraints and improve interpretability versus raw neural features alone.",
            "limitations_or_counter_evidence": "Crowdsourced responses are noisy and require heuristics (grouping semantically similar responses, pruning low-weight edges); current heuristics yield binary utilisation values rather than continuous strengths — a noted limitation for fine-grained typicality.",
            "theoretical_claims_or_implications": "Integration of semantic networks with geometric conceptual representations enables richer, human-aligned concept descriptions (including affordances/uses) and supports explainable classification in applied systems.",
            "uuid": "e8700.5"
        },
        {
            "name_short": "PropertyDecomp",
            "name_full": "Property decomposition / feature-based (utilisation + physical properties)",
            "brief_description": "Representing concepts as decompositions into interpretable quality dimensions (physical properties like colour/size and utilisation/affordance properties) that are explicitly measured and weighted for classification and explanation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "Property-decomposition (interpretable feature-based conceptual representation)",
            "representational_format_description": "Concepts are defined by values across a selected set of interpretable quality dimensions (dimensions may be physical sensor-derived or semantic utilisation-derived); each dimension has a membership function and weight contributing to overall concept typicality.",
            "format_type": "feature-based, hybrid",
            "cognitive_task_or_phenomenon": "Ad-hoc classification; few-shot learning; operator-understandable explanation; affordance-based inference; typicality judgments.",
            "key_findings": "Paper demonstrates in simulation that decomposing concepts into physical and utilisation properties yields data-efficient learning and more comprehensible classification rationale (e.g., utilisation can override surface similarity), supporting trust and debugging by human users.",
            "comparison_with_other_formats": "Favoured over end-to-end vision classifiers in data-scarce domains because explicit features and weights enable interpretability; complements symbolic knowledge graphs and conceptual spaces by making dimensions explicit and inspectable.",
            "limitations_or_counter_evidence": "Requires careful selection and modelling of quality dimensions; current system often assumes independence of dimensions and uses heuristics for some utilisation values; capturing correlations and causal dependencies remains future work.",
            "theoretical_claims_or_implications": "Property decomposition makes concept representations inherently comprehensible, supports frugal AI (data-efficiency), and facilitates human-machine teaming by providing inspectable and modifiable component explanations.",
            "uuid": "e8700.6"
        },
        {
            "name_short": "FuzzyMembership",
            "name_full": "Fuzzy membership / fuzzy set representation",
            "brief_description": "Use of fuzzy-set membership functions to quantify graded typicality of an instance with respect to a concept along continuous quality dimensions.",
            "citation_title": "Fuzzy sets.",
            "mention_or_use": "use",
            "representational_format_name": "Fuzzy membership across quality dimensions",
            "representational_format_description": "Each quality-dimension has a membership function µ (fuzzy set) that returns a graded score for how representative an instance's value is of the prototype; these per-dimension scores are combined (with weights) into an overall typicality measure.",
            "format_type": "fuzzy / graded, feature-based",
            "cognitive_task_or_phenomenon": "Typicality gradients in categorization; graded membership and disputable category judgments.",
            "key_findings": "Paper employs fuzzy membership (µ) to quantify per-dimension typicality and uses the normative vector combination to produce human-intelligible typicality scores; this supports flagging of disputable classifications and generation of natural-language explanations.",
            "comparison_with_other_formats": "Fuzzy graded membership bridges strict symbolic membership (binary) and opaque distributed embeddings by providing graded, interpretable scores; when combined with prototype geometry it aligns with human categorization phenomena.",
            "limitations_or_counter_evidence": "The empirical shape/parameterisation of membership functions depends on observed distributions and heuristic choices; some utilisation extraction currently yields binary values limiting fuzzy granularity.",
            "theoretical_claims_or_implications": "Fuzzy membership formalises human-like graded category membership within the conceptual-space/prototype framework and provides a mechanism for interpretable uncertainty and counterfactual reasoning.",
            "uuid": "e8700.7"
        },
        {
            "name_short": "EventForceResult",
            "name_full": "Event representation via force and result vectors (Conceptual Space extension)",
            "brief_description": "A proposed extension of the conceptual space framework to represent events as force and result vectors, enabling geometric encoding of event structure and potentially task typicality.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representational_format_name": "Event representation via force-result vectors (conceptual-space extension)",
            "representational_format_description": "Events are modelled as vectors capturing forces (agents' actions) and resulting changes; these vectors become dimensions or patterns in an event conceptual space enabling typicality judgments and event classification.",
            "format_type": "hybrid geometric extension (event-level)",
            "cognitive_task_or_phenomenon": "Event recognition; gauging typicality of task execution; affordance inference; action recognition from video.",
            "key_findings": "Paper cites recent work proposing force/result vector event encodings and suggests this as promising for shop-floor task typicality and utilisation inference, but notes interpretability of force-pattern dimensions is a recognised challenge.",
            "comparison_with_other_formats": "Presented as an extension of object-level conceptual spaces to events; offers a structured, geometric alternative to purely statistical action-recognition methods, with potential gains in interpretability if force/result dimensions can be made comprehensible.",
            "limitations_or_counter_evidence": "Interpretability of force vectors is challenging; methods and user-validated mappings from force patterns to human-understandable dimensions remain to be developed and validated.",
            "theoretical_claims_or_implications": "If interpretable event-dimensions can be defined, conceptual-space event representations could support explainable reasoning about tasks and action typicality in embodied human-AI teams.",
            "uuid": "e8700.8"
        },
        {
            "name_short": "NeuroSymbolicBridge",
            "name_full": "Neurosymbolic intermediary / hybrid mapping",
            "brief_description": "The conceptual-space level is proposed as an intermediary representation enabling systematic mapping between continuous neural encodings and discrete symbolic representations to improve interpretability and grounding.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representational_format_name": "Neurosymbolic intermediary (conceptual-space as lingua franca)",
            "representational_format_description": "A middle-level geometric representation abstracts sensory inputs into interpretable points/prototypes and grounds symbols onto these prototypes, enabling bidirectional mapping between distributed neural features and symbolic labels.",
            "format_type": "hybrid (neural-symbolic interface)",
            "cognitive_task_or_phenomenon": "Symbol grounding; interpretability; explainable classification; mapping between perception and language/symbols.",
            "key_findings": "Paper argues conceptual spaces function well as a mapping layer between neural detectors and symbolic knowledge bases, supporting explainability, debugging, and certifiability; used operationally in their pipeline combining sensor-extracted properties with ConceptNet-derived utilisation features.",
            "comparison_with_other_formats": "Framed as preferable to direct mapping from neural embeddings to symbols (opaque) or purely symbolic systems (ungrounded), offering a structured and interpretable interface between subsymbolic perception and symbolic cognition.",
            "limitations_or_counter_evidence": "Realising systematic neurosymbolic mappings in practice is non-trivial; empirical neuroscience validation (e.g., fMRI studies) and formalised mapping algorithms are future work.",
            "theoretical_claims_or_implications": "A middle-level geometric representation can operationally bridge perception and symbolic reasoning, enabling more transparent and trustworthy AI systems in embodied contexts.",
            "uuid": "e8700.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conceptual spaces: The geometry of thought",
            "rating": 2,
            "sanitized_title": "conceptual_spaces_the_geometry_of_thought"
        },
        {
            "paper_title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "rating": 2,
            "sanitized_title": "conceptnet_55_an_open_multilingual_graph_of_general_knowledge"
        },
        {
            "paper_title": "Organizing conceptual knowledge in humans with a gridlike code",
            "rating": 2,
            "sanitized_title": "organizing_conceptual_knowledge_in_humans_with_a_gridlike_code"
        },
        {
            "paper_title": "Cognition and Categorization",
            "rating": 2,
            "sanitized_title": "cognition_and_categorization"
        },
        {
            "paper_title": "Fuzzy sets.",
            "rating": 1,
            "sanitized_title": "fuzzy_sets"
        },
        {
            "paper_title": "Reasoning about categories in conceptual spaces",
            "rating": 1,
            "sanitized_title": "reasoning_about_categories_in_conceptual_spaces"
        }
    ],
    "cost": 0.01656375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments</p>
<p>Vedran Galetić vedran.galetic@airbus.com 
Airbus Central R&amp;T AI Research Team Filton
Airbus Central R&amp;T AI Research Team Filton
United Kingdom, United Kingdom</p>
<p>Alistair Nottle alistair.nottle@airbus.com 
Airbus Central R&amp;T AI Research Team Filton
Airbus Central R&amp;T AI Research Team Filton
United Kingdom, United Kingdom</p>
<p>Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments
Knowledge Representation and ReasoningConceptual SpacesEx- plainable AITrustworthy AIComprehensibilityInterpretabilityHuman-Machine TeamingEmbodied Intelligent AgentsFrugal AI
Trustworthiness of artificially intelligent agents is vital for the acceptance of human-machine teaming in industrial manufacturing environments. Predictable behaviours and explainable (and understandable) rationale allow humans collaborating with (and building) these agents to understand their motivations and therefore validate decisions that are made. To that aim, we make use of Gärdenfors's cognitively inspired Conceptual Space framework to represent the agent's knowledge using concepts as convex regions in a space spanned by inherently comprehensible quality dimensions. A simple typicality quantification model is built on top of it to determine fuzzy category membership and classify instances interpretably. We apply it on a use case from the manufacturing domain, using objects' physical properties obtained from cobots' onboard sensors and utilisation properties from crowdsourced commonsense knowledge available at public knowledge bases. Such flexible knowledge representation based on property decomposition allows for data-efficient representation learning of typically highly specialist or specific manufacturing artefacts. In such a setting, traditional data-driven (e.g., computer vision-based) classification approaches would struggle due to training data scarcity. This allows for comprehensibility of an AI agent's acquired knowledge by the human collaborator thus contributing to trustworthiness. We situate our approach within an existing explainability framework specifying explanation desiderata. We provide arguments for our system's applicability and appropriateness for different roles of human agents collaborating with the AI system throughout its design, validation, and operation.</p>
<p>INTRODUCTION</p>
<p>Use of Artificial Intelligence (AI) is increasing across industry, often providing performance approaching, or even surpassing, that of humans on cognitive tasks for specific applications (e.g., [32,51]). This in turn leads to an increase in the use of embodied AI agents cooperating with humans and becoming embedded on critical systems. There is an increasing need to build trust within these teams. For instance, within the frame of aerospace manufacturing and operations, AI can assist workers in practical tasks such as: optimising scheduling of resources; automated visual inspections; and natural language interactions for receiving commands and reporting events; as well reducing user's cognitive load and supporting automated decision making in fleet management.</p>
<p>Whilst performance gains can be clear and measurable, ensuring acceptance of these technologies, and thereby realising those gains, is difficult. Explainability is one of key elements in building appropriate trust (i.e., not complacency), which is necessary for their acceptance.</p>
<p>Current high-performing AI systems, often based on deep neural network models (i.e., Deep Learning (DL)), tend to be 'black boxes', shrouding their internal knowledge and decision making processes, which in turn may not be readily accessible or interpretable to human users and subjects interacting with, or developing, them. Explainable AI (XAI) is widely seen as one of the cornerstones of AI trustworthiness. Indeed, the European Commission's High Level Expert Group on Artificial Intelligence (HLEGAI) specifies 'explainability' (or explicability) as one of their four ethical principles of fundamental trustworthiness of AI, with the others being: respect for human autonomy, prevention of harm, and fairness [28] (and XAI can play a part in ensuring those other three principles). Furthermore, the Group specifies seven key requirements that are to be addressed throughout an AI product's life-cycle, from which 'transparency', 'accountability', and 'human agency and oversight' are ones clearly related to explainability. These guidelines are upheld by the European Union Aviation Safety Agency (EASA), focusing on challenges around certification that black-box AI models impose, echoing explainability as one of the three main components of trustworthy AI [14], and fully recognising human-centricity in its AI Roadmap [13].</p>
<p>While the interest in XAI research has been tremendous both in academia and industry (e.g., [4,12,24,41]), the outputs have primarily been focussed on post-hoc explainability methods and techniques, i.e., providing explanations of an already developed high-performing yet 'explanatorily opaque' machine learning systems. Instead, we attempt inherent (or intrinsic) explainability by incorporating interpretability requirements early in the system development cycle and focusing on building inherently interpretable 1 and understandable AI systems. In this paper we demonstrate this through a more inherently explainable knowledge representation of the AI agent.</p>
<p>We model the agent's knowledge using a complementary combination of abstracted information from the agent's sensors and openly available commonsense general knowledge sources, subject to simple heuristic engineering. By representing both classes of knowledge on the same representation framework, typical for hybrid AI approaches, allows for knowledge modelling flexibility across application domains. Also, it addresses challenges of (high-performant) computer vision-based approaches of object classification pertaining to data scarcity for rare and specialist objects (not uncommon in aerospace manufacturing) and account of its rationale and outputs in a human-understandable actionable way.</p>
<p>INTERPRETABLE KNOWLEDGE REPRESENTATION</p>
<p>One means of increasing trustworthiness, and consequent certifiability 2 , of an AI agent teaming with humans in operational environments is through inherently interpretable knowledge representation modelling. This allows for the agent's comprehensibility, i.e., the ability to represent its acquired knowledge in humanunderstandable terms [4]. Symbolic representations are a classical way to represent knowledge due to symbols' intrinsic meaning. Although symbols are amenable for computational approaches involving logical calculus, one challenge of this level of representation is modelling the intelligent agent's concept acquisition 3 . Artificially intelligent agents, especially those that are embedded in complex environments and perform higher cognitive tasks, are predominantly based on deep neural networks, excelling at learning from statistical regularities of sensory inputs. Human mind employs prominent qualities of both of these representations.</p>
<p>Therefore, another challenge is mapping between the continuous space of knowledge representation engendered by learnt network models and the symbolic representations. Systematic neurosymbolic mapping, or an intermediary knowledge representation interfacing with both levels, would alleviate one of the major challenges of neural representations, namely, their opacity and consequent incomprehensibility of learnt knowledge, standing in the way of the AI's trustworthiness and certifiability.</p>
<p>Conceptual Knowledge Representation</p>
<p>The semantic theory of Conceptual Spaces [20,21] is an apt formalism in bridging the neuro-symbolic gap in knowledge modelling, proved useful in various application domains [57]. Concepts have inherent meaning, often denoted by corresponding symbols (those 1 As a note, we do not equate inherent interpretability with algorithmic or model transparency characteristic [41]. We adopt a more general definition of interpretability as 'the ability to explain or to present in understandable terms to a human' [12]. 2 By Certifiability we mean the ability for a system to be certified for use in a regulated environment. 3 It is, however, fair to acknowledge some promising steps forward in modelling interpretable concept acquisition and representation on the neural implementation level, e.g., see [9]. concepts that are languageable), whilst retaining their continuous nature arising from neural sensing of environmental input. The framework represents concepts as convex regions in a geometric space spanned by quality domains. A concept (e.g., apple) is described by pertaining ranges across quality domains (e.g., colour, size, taste) and encompasses instances (e.g., this green and sour apple sitting on my desk) represented as vectors characterised by specific properties (e.g., green, sour).</p>
<p>The Conceptual Spaces framework adopts basic tenets of cognitive semantics (inherited in turn from cognitive psychology), thus assuming prototypical effects in categorisation [46][47][48] and schematicity [37]. Conceptual algebraic operations and space mappings can be used to describe metaphoric and metonymic communication operations, while modelling concept combinations and quantifying similarity 4 arise naturally from the very structure of the geometric space.</p>
<p>Moreover, it has been empirically demonstrated that neural populations exhibit geometric (hexadirectional) organisation of conceptual knowledge in the mind, demonstrated on spatial tasks in rodents [25] as well as humans on spatial and, crucially, abstract non-spatial tasks [10]. It is suggested that cognitive maps of concepts may utilise place cells for concept indexing within a geometric space spanned by the hippocampal-entorhinal grid cell system. That makes Conceptual Spaces a valuable candidate for a neuroscientifically (along with cognitively) motivated knowledge representation framework [7].</p>
<p>Crowdsourced Knowledge Base</p>
<p>Humans are able to rapidly construct hyper-hypotheses based on remarkably little data [53,54] and utilise them as background knowledge and constraints in various tasks such as ad-hoc classification. Capturing the structure and content of commonsense knowledge is a challenging task in modelling an artificially intelligent agent. As a remedy, manually-crafted and crowdsourced knowledge bases can be used as a proxy. They are typically knowledge graphs such as ontologies (e.g., [38]) or hierarchically organised synsets [44].</p>
<p>ConceptNet [50] is an openly-available knowledge graph constructed through orchestrated input from both structured manuallycrafted sources (e.g., Wiktionary, OpenCyc [38]) and crowdsourcing campaigns. Despite heterogeneous sources and a vast number of concepts (8 million nodes) and relations between them (21 million edges), the knowledge base is remarkably tidy and manageable with only 36 consolidated relation types. Some of these relations such as UsedFor, MadeOf, and PartOf are of particular relevance for our use case in the aerospace manufacturing domain.</p>
<p>As we will see in the upcoming sections, utilisation properties that are acquired from this knowledge base shall be instrumental in modelling concepts as utilisation properties tend to be more relevant for classifying manufacturing objects than their surface physical properties. </p>
<p>Typicality Model</p>
<p>Whilst some very developed and faithful Conceptual Space implementations exist [5], we draw inspiration from the Conceptual Space framework and property decomposition-based representation of concepts that it employs, and propose utilising a simple model for interpretable fuzzy [56] classification. The model, called µw-model [17], relies on the prototype theory [46] and empirical theories on causal status [1,2] and concept centrality [49]. The model's name originates from two parameters that are used to describe an instance's typicality status in the frame of a concept, namely: the membership function µ quantifying typicality for one quality dimension (see Fig. 1); and the weight w per quality dimension. It is thereby possible to ascertain and explain that, for example, although an object observed in the manufacturing environment has surface similarities to the prototype of a previously learnt class, the facts that it is used for a different activity (see § 2.2) and that the utilisation property is much weightier than surface properties in the manufacturing environment jointly steer its classification to another class (e.g., Fig. 2).</p>
<p>These parameters are learnt in different ways, yet both are based on the agent's property detection capabilities (e.g., visual sensor) accompanied by tutor-provided labels of encountered instances. Based on instances' labels, and quality dimension value distributions, the agent is able to quantify µ frequentistically (e.g., assuming the prototype has the most occurring value), while w is quantified by using an empirically confirmed [19] hypothesis, based on concept centrality, on inverse relationship between the dimension's weight on one side and its variability of instances' property values across the category on the other (e.g., colour tends to be more important for natural kinds [48] such as apple than for artefacts such as car as the latter can usually be in an arbitrary colour).</p>
<p>Each instance is represented as a vector as shown in Eq. 1:
− → ( ) = ∑︁ √︄ ( ) ( ) ( ) · − → (1)
where is a quality dimension (e.g., width), − → are basis vectors spanning the space, is a quality domain (e.g., size), is an instance (e.g., this apple), ( ) is the weight of the quality dimension for the concept (e.g., apple), and ( ) is the typicality measure of the instance for the concept with respect to the quality dimension (e.g., the representativeness of 'this zebra-striped ball' for the concept zebra with respect to the quality dimension texture; which, as a note, would be high for this dimension, but extremely low for virtually any other dimension).</p>
<p>Typicality of the instance c with respect to concept C is the second norm of vector in Eq. 1, thus is calculated as in Eq. 2:
( ) = ∥ − → ( )∥ = √︄ ∑︁ ( ) ( ) 2 ( )(2)
while it is sensible to treat the instance as one pertaining to that concept for which its typicality is the highest, as in Eq. 3.
( ) = max ( )(3)
As the system is able to flag any disputable classifications (e.g., those for which the difference between the maximal and the second largest typicality from Eq. 2 is not above a threshold) in an interpretable way, it is possible to investigate the case using intrinsically interpretable explanation of the rationale as, e.g., a developer debugging the system or an operator on the shop floor.</p>
<p>To examine the feasibility of our model in a 'real-world' scenario, we applied the described knowledge modelling approach to an aerospace manufacturing process. There is the potential to make improvements to existing capability as well as develop new ones though the use of collaborative robots, or 'cobots'. These 'cobots' will improve quality and free up skilled workers to focus on their specialities. Therefore, explainability of decisions from robotic assistants is vital to building acceptance and trust and enabling effective collaboration, allowing these benefits to be realised.</p>
<p>In order to build within cobots an awareness of their environment they need to be able to rapidly and efficiently recognise objects which they may not have seen previously as well as with clear and understandable explanations for this recognition. By modelling the artificial agent's knowledge interpretably, using the Conceptual Space framework [20], we reduce the need for post-hoc explanation.</p>
<p>As an example, a cobot may classify an object as a drill because it because it is of similar size, shape, colour, and material composition, as examples used during the training process. Capturing utilisation properties, which can indicate for instance that the item is used for drilling, allows us to produce classification which is by its very nature interpretable. For example, the operator may obtain a natural language output such as 'I believe this is a drill as it looks similar to other drills I've seen in the past, and it is used for drilling', accompanied by a visualisation with associated typicality measures. Moreover, if an object bears surface similarities to previously seen instances of a drill, but is perceived to be used for riveting would instead be classified as a riveter. This reflects that utilisation properties of industrial artefacts are more heavily weighted in a classification than the surface properties (Fig. 2).</p>
<p>Setup</p>
<p>Industrial environments can be extremely sensitive to disruption, particularly with just-in-time manufacturing processes. Therefore, current industrial setups do not readily allow for the deployment of experimental robotics on the factory floor. To combat this we have instead used a simulated environment of a cobot in a factory setting to provide training data, as well as to get feedback and validation of the proposed methodologies. We have used the Webots Open Source Robotics Simulator [11] to create, in the first instance, a simple environment with a controllable e-puck robot [15] equipped with a simple sensor package, such as a standard vision sensor (i.e., a camera) as well as a time-of-flight sensor.</p>
<p>The simulation environment is populated with a variety of objects, using simplistic idealised examples such as 'Green Ball' or 'Red Cube', to allow for easier recognition, but still 'relatable' and inherently interpretable objects (see Fig. 4). As we enhanced the AI system, we introduced additional objects to represent real-world, ecologically relevant, objects such as a hammer or screwdriver.</p>
<p>To further augment the properties available, we attach custom properties to the objects within the simulator (e.g., stripy texture). This assists in the determining of ground truth values for colour, texture, utilisation properties, etc. These values can be extracted from the simulated environment programmatically along with the labels of the objects to provide a training set from which to learn a conceptual representation using inherently explainable and interpretable terms.</p>
<p>A conceptual space is learned by Voronoi tessellation [23] around prototypes. In the simpler simulation environment, the objects are considered to already be idealised prototypes in order to make it easier to carry out proof-of-concept space construction. With the migration to a more complex simulation environment, prototypes are calculated as centroids of the labelled instances used during training.</p>
<p>The robot can then be manipulated by a user to move through the simulation, and the field-of-view of the camera is determined. Any objects coming within this field of vision will then trigger the Webots software to forward the image to our API, alongside the ground truth properties relating to that object.</p>
<p>In a real-world implementation, capturing of images and detection of the objects therein would be carried out by state-of-the-art object / region of interest detection algorithms. As these are not the focus of our research, we have instead extracted ground truth data and imagery from the simulation environment itself. This modular approach allows for algorithms (such as property detectors) to be inserted as needed, and users can select the most appropriate algorithm for the use case being exploited.</p>
<p>This geometric representation level can thus be treated as an intermediary "middle ground" between the symbolic and neural levels by, on the one hand, abstracting sensory inputs onto appropriate points in a conceptual space, whilst, on the other hand, grounding symbols onto concept prototypes (as suggested in, e.g., [52]).</p>
<p>Property Extraction for Interpretable Classification</p>
<p>Separate property detectors are applied to extract properties of the observed object. These take two broad categories of detectors: physical property detectors and utilisation property detectors. Basic physical properties can be inferred from the sensors on the robotic platform. Examples of properties we have experimented with include: texture (using Concept Activation Vectors [30] to determine distinctive textural properties of an object, e.g., stripey, smooth, etc.); colour (represented within the HSB colour space, using simple computer vision approaches to determine the dominant colour of the object); size (determined by a depth-aware camera); current explorations include shape parametrisation (e.g., following [6]). A simple data flow diagram (Fig. 3) shows the high-level implementation we have used. Utilisation properties require additional transformations to derive. Sources for these could include task recognition through computer vision techniques (e.g., [16] to determine which tasks are being carried out in the observed video stream (e.g., 'hammering' or 'drilling'); or newer work around the Conceptual Space framework proposing event representation using force and result vectors [22,43]. Currently, utilisation properties are extracted in the training phase using crowdsourced knowledge bases.</p>
<p>General Knowledge Extraction.</p>
<p>We use ConceptNet's (see § 2.2) API 5 to acquire crowdsourced values for the UsedFor property of various manufacturing artefacts (e.g., drill). Each response (e.g.,</p>
<p>Simulation Environment</p>
<p>Virtual Sensors Property Extractors Concept Algebra Interpretable Classification Figure 3: Data flow in the manufacturing use case. Currently, simulated environment is used in the proof-of-concept scenario. Virtual, visual-based sensors are used to capture objects in the environment. Captured images are fed into the property extractor modules (e.g., for colour, texture, etc.), each dedicated to extracting the value corresponding to the pertaining quality dimension (e.g., hue, stripy, etc.). It is worth mentioning that some quality domains, whose extraction is more challenging due to limitations of the simulated environment (e.g., size), are quantified using the simulated environment specification directly, which is a temporary fix in line with the fact the focus of our work is not as much on the property detectors and computer vision as on concept building and class inference proof of concept in the "back end". An instance described by its (standardised) property values is represented as a vector and its representativeness measures for candidate classes quantified using the µw-model.</p>
<p>'drilling holes in things') is accompanied by the weight, reflecting the source's reliability. Due to the nature of free-form submissions, messiness of the corresponding response cannot be avoided (e.g., 'drilling holes in things' and 'drill a hole in something' are separate entries). To fix that, in the first iteration we group all responses with similar semantics, such as the examples above, and run the softmax function across all weights of semantically different items to acquire the quantity that we use to ground the membership function 6 for this utilisation property. Concretely, we get
( _ _ _ _ ) = 0.9999997
which is a reasonable quantity.</p>
<p>Since the described procedure would be quite cumbersome and time-consuming to run for each utilisation property, a more thorough approach to automating utilisation knowledge extraction from the public knowledge bases was developed. It consults ConceptNet and WordNet programmatically, via their respective APIs, alongside the NLTK 7 package, does not require any manual intervention, and proceeds in the following steps:</p>
<p>(1) Find the appropriate WordNet synset (i.e., a meaning that can be represented by multiple synonymous lexemes) for the given object label (e.g., 'drill'). The condition is that it denotes an artefact noun. In case there are multiple such synsets, as in the case of 'hammer' (handheld tool and a gun part), heuristically choose one whose encompassed lexemes have the highest occurrence frequency in the corpus; (2) Get all synonym lexeme lemmas for that synset using Word-Net; (3) For each lexeme, extract edges from ConceptNet originating from WordNet (higher credibility than crowdsourced data) that have the lexeme as the start node and 'UsedFor' as the relation. End nodes denote utilisation properties; (a) If no such edge is found (as, e.g., for forklift), then consult ConceptNet's crowdsourced knowledge. Of all edges that have the start node and relation as specified, eliminate those whose weight is 1.0 or less (these can be considered unreliable or noise). In each end node of the remaining edges, find a verb using part-of-speech tagger, lemmatise it (e.g., change 'carrying' to 'carry'), and add it to the list of utilisations; (b) If that does not yield a meaningful utilisation property either (as it does not, e.g., for riveter), use a heuristic inspired by low inflection of English language: Stem the artifact name ('riveter' to 'rivet') and check whether it can be used as a verb ('hammer', 'drill' are both nouns and verbs). If it can, add it to the utilisation list; (4) For the extracted verb synsets, extract only those whose meaning falls under WordNet categories 'contact', 'change' or 'motion' (more can be added where necessary). Synsets characterised as, e.g., 'verb.cognitive' would be excluded; (5) Use the retrieved synsets to infer the value for each selected utilisation quality dimension (currently, 'drill', 'hammer', 'lift', 'rivet'). Do this by comparing these synsets to physical utilisation synsets for each quality dimension, again retrieved from knowledge bases. Where there is a non-empty intersection of synsets, set the value of the corresponding quality dimension to one; where there is not, set it to zero 8 . * * * The majority of the described pipeline allows for the implementation of state-of-the-art property detectors. Similarly, the simulation environment could be substituted with real-world cobots with realworld sensors maintaining the same interfaces throughout.</p>
<p>Quality dimensions' membership values, along with weights, make up the representativeness vector (Eq. 1), from which we measure the instance's typicality across concepts (Eq. 2) and determine for which one its representativeness is the highest (Eq. 3, visualised in Fig. 4). The representativeness score and associated property weights and typicality measures are directly human-comprehensible and constitute our interpretable classification approach, as an (at this point simplified) alternative to traditional computer visionbased object recognition approaches.</p>
<p>Situation in the XAI Ecosystem</p>
<p>Our knowledge representation-based inherent explainability approach may be situated in a wider XAI framework against roles of system users and previously proposed explanation characteristics. This way it may be compared against other XAI techniques and Figure 4: Simulated industrial use case environment. We start with the simple case with simulated idealised objects (upper left), such as 'Red Sphere', 'Wooden Cube', etc. The robot encounters these objects and extracts ground-truth property values available from the simulator (colour, shape, composition), used as the basis for learning the conceptual space. Having validated the data pipeline and knowledge representation modelling, we moved towards the more ecologically valid manufacturing simulated environment (upper middle), where quality dimensions include physical properties like size, texture, composition, and utilisation properties. Generally, every instance's quality dimension values are used as input for membership quantification per various concepts of interest. The spider charts (lower) show example membership values per quality dimensions across four artefact concepts for two example observations. Together with the quality domain weights (currently arbitrated, in the future quantified via empirical hypotheses, see § 2.3), these membership values make up the vector representation of the instance, as per Eq. 1. The bar chart (upper right) represents the typicality of the two example instances across the four observed concepts, calculated via Eq. 2.</p>
<p>represent a candidate for the right application context and provided requirements.</p>
<p>Considering development of explainable AI systems teaming with humans in manufacturing environments, the following roles [55] of agents interacting with the AI can be envisioned for such applications:</p>
<p>• Creator -Developer of the AI system using property decomposition explainability for sanity check and debugging, as well as guidance in hyperparameter selection (e.g., threshold for disputable classification, see § 2.3); • Operator and Executor (possibly the same agent) -Manufacturing worker provided with an understandable explanation of the classifier's rationale (see Fig. 4), ready to act upon the output (e.g., take and use the intended fetched tool) or flag the system's incorrect behaviour; • Decision-Subject -In a more advanced use case, a human agent teaming with an embodied AI agent on a physical task (e.g., processing a large component), able to understand the agent's rationale of a tool choice. Obviously, trustworthiness of the AI system is of essence for the Decision-Subject role; • Examiner -Prior to industrialisation and deployment, any AI to be used in applications involving human safety requirements needs to be certified, for which thorough sanity checks of the AI's learnt knowledge and operation rationale are crucial.</p>
<p>It should be noted that the expectations from users will likely evolve as interactions increase. For instance, when first working together, it is likely an operator will require more frequent and detailed explanations as they begin to build trust. As this trust develops, it is likely that either less detailed or frequent explanations will be required, and could even cause user frustration (if a system consistently explains a 'simple' action undertaken many times per shift). This longitudinal approach is a matter of future work as discussed in § 4.</p>
<p>Our explainability approach can be described across various axes of the explanation characterisation framework, such as one by [26, Table 1]. Although a rigorous empirical validation of the technique in operation is pending, some characteristics may already be ascertained, e.g.:</p>
<p>• Interpretability -High-level interpretability of classification output endowed with visualisation artefacts involving interpretable quality domains and understandable diagrams. As opposed to post-hoc explainability, employed knowledge modelling via decomposition into interpretable quality dimensions is an integral part of the system's design; • Local/global explainability -In the interpretable classification use case ( § 3.2) it is possible to explore the membership function and weights per quality dimensions and thus validate the categorisation rationale, which adds to global explainability; each instance classification is accompanied by local explainability visual (or textual) artefacts (e.g., Fig. 4); • Feature importance quantifiability and visualisation -Learnt knowledge includes empirically validated dimension weights, which provides a cognitively-motivated importance quantification; • Explanation by example -The representational space is partitioned around the prototypes (see § 3.2), which may in turn be used to elucidate a seemingly unlikely categorisation outcome; • Interactivity -A simple visual tool has been developed that allows for interactive probing of the classifier by manipulating quality dimension weights and membership functions of the model, as well as property values of an instance; • Counterfactual reasoning -Using the interactive tool (see above) it is possible to understand what the categorisation outcome would have been had the parameters or properties been different.</p>
<p>The authors [26] treat the 'generalisability' characteristic as versatility across types of applicable black-box models and as such pertains solely to post-hoc explainability techniques. However, when discussing versatility of utilisation, it is worth noting that the underlying Conceptual Space framework and the proposed typicality model and its parameter learning allow for flexibility of application domain, with an (admittedly strong) assumption of selecting an appropriate set of quality domains and dimensions and their apt modelling. Once these are selected for a given domain, the pertaining parameters are learnt from the combination of exploring in the world (i.e., from observed distributions of quality dimension values) and expert guidance (i.e., providing the labels as input to the concept space construction process).</p>
<p>Qualitative Validation</p>
<p>Empirical validation of interpretability and explainability are still emerging topics in the field of XAI. To try and obtain validation of the work undertaken, we have consulted with subject matter experts and potential users to assess and understand the quality of the of the proposed system.</p>
<p>As discussed in § 3.3, we identified a number of different user roles, e.g., Creator-Developer, Operator, etc., and presented them with relevant explanations and demonstrations of the system. User feedback was then taken into account to implement improvements. For instance, a number of different visualisations were evaluated for displaying 'typicality' of objects to users (e.g., spider / radar charts, bar charts, doughnut charts, polar charts, etc.), to determine which ones conveyed the most intelligible and useful information.</p>
<p>On top of that, semantics of the interpretable classification output was validated with domain experts, for example, the two-level information presentation. This presentation includes visualisation of the overall calculated representativeness of an instance across categories, which can in turn be 'zoomed-into' to explore typicality across utilised quality dimensions, as in the bar chart and spider charts from Fig. 4, respectively.</p>
<p>Future work will include a rigorous methodological framework for more objective evaluation (on top of subjective, reporting-based approaches) of explainability and interpretability metrics, building on previous domain-relevant and domain-independent work such as [26] and [58].</p>
<p>CONCLUSIONS AND FUTURE WORK</p>
<p>We pursue a knowledge modelling approach towards inherent (vs. post-hoc) explainability of (embodied) AI agents teaming with humans in industrial environments. Heterogeneous knowledge involves physical properties acquirable by equipped sensors and utilisation properties obtained from publicly available crowdsourced and expert-manufactured resources of general knowledge. The properties are consulted by a simple classification model drawing inspiration from Gärdenfors's Conceptual Space framework. The model is defined by the membership function in the context of fuzzy set theory (quantified frequentistically from experience with environment); and the weight of the property (based on empirically confirmed cognitive semantic hypotheses).</p>
<p>Highly specific industrial environments, such as the aerospace manufacturing one, involve rare and specialist objects, for which there typically do not exist image datasets of sufficient size to train data-hungry high-performant (convolutional) neural networks for object classification. Therefore, our move from computer vision object recognition to property decomposition-based interpretable classification is relevant for such applications characterised by data scarcity, thus necessitating frugal AI approaches. Furthermore, decomposing concepts into interpretable property components facilitates model validation and debugging by a developer, scrutinous examination by a certifier, behaviour understandability for a teaming operator, and rationale interpretability for a decision-subject.</p>
<p>Whilst the described use case is a rather limited and illustratory one, it does open a few avenues for future research in knowledge representation and inference modelling of (embodied) AI agents teaming with humans in manufacturing and other industrial and operational environments.</p>
<p>Humans are remarkably successful in learning concepts from scarce data [27], the mechanics and phenomenology of which is of high interest to cognitive science, cognitive neuroscience, computational neuroscience, and artificial intelligence [34]. One of the challenges is modelling acquisition of core concepts and intuitive theories (e.g., in physics and psychology) as well as generic causal structures [36], all supporting commonsense reasoning. Some promising generative models are based on Bayesian reasoning in the context of hierarchies and structures of hypotheses and associated inductive constraints [54]. An artificial agent able to acquire and manipulate core concepts arguably makes its behaviour more predictable, its rationales more interpretable, and it itself more trustworthy.</p>
<p>In the simplified examples used we represent quality dimensions via orthogonal basis vectors spanning the conceptual space. However, in reality, property correlations are an important component of concept description [45], stemming from generic, causally originating, core concept structures, and empirically demonstrated by humans' effortless acquisition of systematic correlations [8,29,31,42], especially for natural kinds (e.g., many times the colour of fruit predicts its taste and ripeness). It is a matter of further exploration to address property interdependencies, possibly taking into account psychological essentialism, according to which perceptible properties are surface manifestation of entities' true nature [18,33].</p>
<p>Moreover, for artefacts, which can theoretically take arbitrary values of surface properties, it makes sense to focus on the objects' purported utilisation capability (affordances) stemming from their physical properties, like shape or composition (e.g., a large handheld-sized object with a hard flat metal head is likely to be used to hit nails). Attention-driven visual classification and visualisation techniques may be promising approaches for utilisation inference.</p>
<p>Whilst the current use case deals only with physical objects, newer work around the Conceptual Space framework proposes event representation using force and result vectors [22,43]. This work justifies further exploration as it may prove particularly useful on the shop floor environments. At the same time, one shall need to pay attention to interpretability of pertaining quality domains in this respect.</p>
<p>More recent work in the Conceptual Space theory is focussed on modelling events via force and result vectors [22,43] using the same underlying representational structure as in the described case for (physical) objects. This research is particularly relevant for industrial domains involving human-AI teaming, e.g., for gauging typicality of task execution. A recognised challenge is interpretability of force patterns constituting quality dimensions, which the event-based extension of the Conceptual Space framework is based on.</p>
<p>Cognitive modelling of operators using cognitive architectures [3,35] is a useful building block for modelling AI cognitive assistants aware of task representations, capable of anomalous behaviour detection, and cognitive load estimation as an input to autonomy engagement. A subsymbolic yet interpretable knowledge representation framework would arguably make a promising step towards modelling the declarative module ((e.g., [39,40]), which is yet to be demonstrated in industrial environments involving human-AI teaming.</p>
<p>Reported parallels between Conceptual Spaces' structural principles and empirical findings on conceptual knowledge neural implementation (see §2.1) invite researchers to explore the appropriateness of such an immanently multidimensional representation for mapping onto what appears to be two-dimensional hexadirectional grid-based encoding system on the neural substrate level [7]. Whilst running human studies (e.g., functional magnetic resonance imaging (fMRI) experiments) for hypothesis testing around candidate mapping mechanisms would not be feasible in industrial environments where the authors are affiliated, they shall make every effort to liaise with academic partners who may be better positioned to execute such studies, on top of continually following advances in the area.</p>
<p>Explainability capabilities of an AI system should be able to gauge the human user's expectations stemming from their experience with the task at hand and the teaming AI agent. Hence a longitudinal approach in context-aware trust and interpretability measurement, possibly involving user's overt or implicit feedback, and consequent XAI system's adaptability should be considered in the future.</p>
<p>Finally, it is important to recognise that the path towards a trustworthy AI is multi-faceted and explainability represents one pillar, albeit arguably the one most human user-focussed. The other pillars that are essential for responsible and ethical AI design are robustness and learning assurance, and fairness and nondiscrimination [13,14,28]). Obviously, AI trustworthiness should be addressed in an interdisciplinary manner, one difficult without a strong collaboration of academia and industry.</p>
<p>Figure 1 :
1Illustration of the membership functions for a continuous quality dimension across three example concepts (prototypical instances are assumed to bear prototypical value of the property) and of a nominal quality dimension for the concept.</p>
<p>Figure 2 :
2The artificial agent wants to classify a new object (upper right image) and extracts its physical and utilisation properties. The table (below) illustrates the and values for the 'Drill' concept. Although the new object is physically similar to the learnt 'Drill' prototype (upper left image shows a typical instance of 'Drill'), it will not be classified as one due to incompatibility of the utilisation property bearing the highest weight.
Conceptual Spaces also allow for discerning between similarity (e.g., car and van) and relatedness (e.g., car and driver), which is one of the challenges of distributional semantics approach.
http://api.conceptnet.io (accessed on 3 October 2022)
Somewhat counter-intuitively, what is called 'weight' in the ConceptNet system is semantically closer to the membership function of the w-model than the weight . See § 2.3 for details. 7 https://www.nltk.org/ (accessed on 4 October 2022)
Obviously, the proposed heuristics only allow for binary values of the UsedFor property. While the current approach served our purpose well, it is a matter of future work to infer continuous values.</p>
<p>Causal status effect in children's categorization. Susan A Woo-Kyoung Ahn, Jennifer A Gelman, Jill Amsterlaw, Charles W Hohenstein, Kalish, Cognition. 76Woo-kyoung Ahn, Susan A Gelman, Jennifer A Amsterlaw, Jill Hohenstein, and Charles W Kalish. 2000. Causal status effect in children's categorization. Cognition 76, 2 (2000), B35-B43.</p>
<p>Causal status as a determinant of feature centrality. Nancy S Woo-Kyoung Ahn, Mary E Kim, Martin J Lassaline, Dennis, Cognitive Psychology. 41Woo-kyoung Ahn, Nancy S Kim, Mary E Lassaline, and Martin J Dennis. 2000. Causal status as a determinant of feature centrality. Cognitive Psychology 41, 4 (2000), 361-416.</p>
<p>An integrated theory of the mind. Daniel John R Anderson, Bothell, D Michael, Scott Byrne, Christian Douglass, Yulin Lebiere, Qin, Psychological review. 1111036John R Anderson, Daniel Bothell, Michael D Byrne, Scott Douglass, Christian Lebiere, and Yulin Qin. 2004. An integrated theory of the mind. Psychological review 111, 4 (2004), 1036.</p>
<p>Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information fusion. 58Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Ben- netot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, et al. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. In- formation fusion 58 (2020), 82-115.</p>
<p>A thorough formalization of conceptual spaces. Lucas Bechberger, Kai-Uwe Kühnberger, Joint German/Austrian Conference on Artificial Intelligence (Künstliche Intelligenz. SpringerLucas Bechberger and Kai-Uwe Kühnberger. 2017. A thorough formalization of conceptual spaces. In Joint German/Austrian Conference on Artificial Intelligence (Künstliche Intelligenz). Springer, Springer, 58-71.</p>
<p>Representing Complex Shapes with Conceptual Spaces. Lucas Bechberger, Margit Scheibel, Second International Workshop'Concepts in Action: Representation, Learning, and Application. CARLALucas Bechberger and Margit Scheibel. 2020. Representing Complex Shapes with Conceptual Spaces. In Second International Workshop'Concepts in Action: Representation, Learning, and Application'(CARLA 2020).</p>
<p>Navigating cognition: Spatial codes for human thinking. L S Jacob, Peter Bellmund, Gärdenfors, I Edvard, Christian F Moser, Doeller, Science. 3626766Jacob LS Bellmund, Peter Gärdenfors, Edvard I Moser, and Christian F Doeller. 2018. Navigating cognition: Spatial codes for human thinking. Science 362, 6415 (2018), eaat6766.</p>
<p>Unsupervised concept learning and value systematicitiy: A complex whole aids learning the parts. Dorrit Billman, James Knutson, Journal of Experimental Psychology: Learning, Memory, and Cognition. 22458Dorrit Billman and James Knutson. 1996. Unsupervised concept learning and value systematicitiy: A complex whole aids learning the parts. Journal of Experi- mental Psychology: Learning, Memory, and Cognition 22, 2 (1996), 458.</p>
<p>Explainable neural networks that simulate reasoning. J Paul, Milo M Blazek, Lin, Nature Computational Science. 1Paul J Blazek and Milo M Lin. 2021. Explainable neural networks that simulate reasoning. Nature Computational Science 1, 9 (2021), 607-618.</p>
<p>Organizing conceptual knowledge in humans with a gridlike code. Alexandra O Constantinescu, Jill X O&apos;reilly, Timothy Ej Behrens, Science. 352Alexandra O Constantinescu, Jill X O'Reilly, and Timothy EJ Behrens. 2016. Organizing conceptual knowledge in humans with a gridlike code. Science 352, 6292 (2016), 1464-1468.</p>
<p>Webots: robot simulator. Cyberbotics, Cyberbotics. 2022. Webots: robot simulator. https://cyberbotics.com/. Accessed: 2022-02-18.</p>
<p>Towards a rigorous science of interpretable machine learning. Finale Doshi, - Velez, Been Kim, arXiv:1702.08608arXiv preprintFinale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of inter- pretable machine learning. arXiv preprint arXiv:1702.08608 (2017).</p>
<p>EASA Artificial Intelligence Roadmap 1.0 A human-centric approach to AI in aviation. Easa, EASA. 2020. EASA Artificial Intelligence Roadmap 1.0 A human-centric approach to AI in aviation. EASA. https://www.easa.europa.eu/document-library/general- publications/easa-artificial-intelligence-roadmap-10</p>
<p>EASA Concept Paper: First usable guidance for Level 1 machine learning applications. Easa, EASA. 2021. EASA Concept Paper: First usable guidance for Level 1 machine learning applications. EASA. https://www.easa.europa.eu/sites/default/files/dfu/ easa_concept_paper_first_usable_guidance_for_level_1_machine_learning_ applications_-_proposed_issue_01_1.pdf</p>
<p>. Epfl, EPFL. 2018. e-puck education robot. http://www.e-puck.org/. Accessed: 2022- 02-18.</p>
<p>Slowfast networks for video recognition. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slow- fast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision. 6202-6211.</p>
<p>An Aggressive Robin in the Backyard: Formal Quantification of Prototypicality Level within the Frame of the Prototype Semantic Theory of Cognitive Linguistics. Suvremena lingvistika (Contemporary Linguistics). Vedran Galetić, 3771Vedran Galetić. 2011. An Aggressive Robin in the Backyard: Formal Quantification of Prototypicality Level within the Frame of the Prototype Semantic Theory of Cognitive Linguistics. Suvremena lingvistika (Contemporary Linguistics) 37, 71 (2011).</p>
<p>Towards the cognitive plausibility of conceptual space models. Vedran Galetić, 41Suvremena lingvistika (Contemporary LinguisticsVedran Galetić. 2015. Towards the cognitive plausibility of conceptual space models. Suvremena lingvistika (Contemporary Linguistics) 41, 80 (2015), 71-85.</p>
<p>Formalisation and quantification of a cognitively motivated concecptual space model based on the prototype theory. Vedran Galetić, Ph.D. Dissertation. University of ZagrebVedran Galetić. 2016. Formalisation and quantification of a cognitively moti- vated concecptual space model based on the prototype theory. Ph.D. Dissertation. University of Zagreb.</p>
<p>Conceptual spaces: The geometry of thought. Peter Gardenfors, MIT pressPeter Gardenfors. 2004. Conceptual spaces: The geometry of thought. MIT press.</p>
<p>The geometry of meaning: Semantics based on conceptual spaces. Peter Gardenfors, MIT pressPeter Gardenfors. 2014. The geometry of meaning: Semantics based on conceptual spaces. MIT press.</p>
<p>An Epigenetic Approach to Semantic Categories. Peter Gärdenfors, IEEE Transactions on Cognitive and Developmental Systems. 12Peter Gärdenfors. 2018. An Epigenetic Approach to Semantic Categories. IEEE Transactions on Cognitive and Developmental Systems 12, 2 (2018), 139-147.</p>
<p>Reasoning about categories in conceptual spaces. Peter Gärdenfors, Mary-Anne Williams, IJCAI. Citeseer. Peter Gärdenfors and Mary-Anne Williams. 2001. Reasoning about categories in conceptual spaces. In IJCAI. Citeseer, 385-392.</p>
<p>DARPA's explainable artificial intelligence (XAI) program. David Gunning, David Aha, AI magazine. 40David Gunning and David Aha. 2019. DARPA's explainable artificial intelligence (XAI) program. AI magazine 40, 2 (2019), 44-58.</p>
<p>Microstructure of a spatial map in the entorhinal cortex. Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, Edvard I Moser, Nature. 436Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, and Edvard I Moser. 2005. Microstructure of a spatial map in the entorhinal cortex. Nature 436, 7052 (2005), 801-806.</p>
<p>A systematic method to understand requirements for explainable AI (XAI) systems. Mark Hall, Daniel Harborne, Richard Tomsett, Vedran Galetic, Santiago Quintana-Amate, Alistair Nottle, Alun Preece, Proceedings of the IJCAI Workshop on eXplainable Artificial Intelligence (XAI 2019). the IJCAI Workshop on eXplainable Artificial Intelligence (XAI 2019)Macau, China11Mark Hall, Daniel Harborne, Richard Tomsett, Vedran Galetic, Santiago Quintana- Amate, Alistair Nottle, and Alun Preece. 2019. A systematic method to understand requirements for explainable AI (XAI) systems. In Proceedings of the IJCAI Work- shop on eXplainable Artificial Intelligence (XAI 2019), Macau, China, Vol. 11.</p>
<p>. Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, Matthew Botvinick, Neuroscience-inspired artificial intelligence. Neuron. 95Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, and Matthew Botvinick. 2017. Neuroscience-inspired artificial intelligence. Neuron 95, 2 (2017), 245-258.</p>
<p>Ethics guidelines for Trustworthy AI. Hlgeai, HLGEAI. 2019. Ethics guidelines for Trustworthy AI. European Commission. https: //digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</p>
<p>How children know the relevant properties for generalizing object names. S Susan, Linda B Jones, Smith, Developmental Science. 5Susan S Jones and Linda B Smith. 2002. How children know the relevant properties for generalizing object names. Developmental Science 5, 2 (2002), 219-232.</p>
<p>Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, PMLRInternational conference on machine learning. Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning. PMLR, 2668-2677.</p>
<p>What's behind different kinds of kinds: effects of statistical density on learning and representation of categories. Heidi Kloos, M Vladimir, Sloutsky, Journal of Experimental Psychology: General. 13752Heidi Kloos and Vladimir M Sloutsky. 2008. What's behind different kinds of kinds: effects of statistical density on learning and representation of categories. Journal of Experimental Psychology: General 137, 1 (2008), 52.</p>
<p>The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Matthieu Komorowski, A Leo, Omar Celi, Badawi, C Anthony, Gordon, Nature medicine. 24Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. 2018. The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Nature medicine 24, 11 (2018), 1716-1720.</p>
<p>Inductive inference and its natural ground: An essay in naturalistic epistemology. Hilary Kornblith, Mit PressHilary Kornblith. 1995. Inductive inference and its natural ground: An essay in naturalistic epistemology. Mit Press.</p>
<p>Cognitive computational neuroscience. Nikolaus Kriegeskorte, Pamela K Douglas, Nature neuroscience. 21Nikolaus Kriegeskorte and Pamela K Douglas. 2018. Cognitive computational neuroscience. Nature neuroscience 21, 9 (2018), 1148-1160.</p>
<p>A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. E John, Christian Laird, Paul S Lebiere, Rosenbloom, 38Ai MagazineJohn E Laird, Christian Lebiere, and Paul S Rosenbloom. 2017. A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. Ai Magazine 38, 4 (2017), 13-26.</p>
<p>Building machines that learn and think like people. Brenden M Lake, Joshua B Tomer D Ullman, Samuel J Tenenbaum, Gershman, Behavioral and brain sciences. 40Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. 2017. Building machines that learn and think like people. Behavioral and brain sciences 40 (2017).</p>
<p>Women, fire, and dangerous things: What categories reveal about the mind. George Lakoff, University of Chicago pressGeorge Lakoff. 2008. Women, fire, and dangerous things: What categories reveal about the mind. University of Chicago press.</p>
<p>Building large knowledge-based systems; representation and inference in the Cyc project. B Douglas, Lenat, V Ramanathan, Guha, Addison-Wesley Longman Publishing Co., IncDouglas B Lenat and Ramanathan V Guha. 1989. Building large knowledge-based systems; representation and inference in the Cyc project. Addison-Wesley Longman Publishing Co., Inc.</p>
<p>Conceptual spaces for cognitive architectures: A lingua franca for different levels of representation. Antonio Lieto, Antonio Chella, Marcello Frixione, Biologically inspired cognitive architectures. 19Antonio Lieto, Antonio Chella, and Marcello Frixione. 2017. Conceptual spaces for cognitive architectures: A lingua franca for different levels of representation. Biologically inspired cognitive architectures 19 (2017), 1-9.</p>
<p>Dual PECCS: a cognitive system for conceptual representation and categorization. Antonio Lieto, Daniele P Radicioni, Valentina Rho, Journal of Experimental &amp; Theoretical Artificial Intelligence. 29Antonio Lieto, Daniele P Radicioni, and Valentina Rho. 2017. Dual PECCS: a cognitive system for conceptual representation and categorization. Journal of Experimental &amp; Theoretical Artificial Intelligence 29, 2 (2017), 433-452.</p>
<p>The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery. Zachary C Lipton, Queue. 16Zachary C Lipton. 2018. The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue 16, 3 (2018), 31-57.</p>
<p>The parallel distributed processing approach to semantic cognition. L James, Timothy T Mcclelland, Rogers, Nature reviews neuroscience. 4James L McClelland and Timothy T Rogers. 2003. The parallel distributed pro- cessing approach to semantic cognition. Nature reviews neuroscience 4, 4 (2003), 310-322.</p>
<p>Construals of meaning: The role of attention in robotic language production. Anne-Laure Mealier, Grégoire Pointeau, Peter Gärdenfors, Peter Ford Dominey, Interaction Studies. 17Anne-Laure Mealier, Grégoire Pointeau, Peter Gärdenfors, and Peter Ford Dominey. 2016. Construals of meaning: The role of attention in robotic lan- guage production. Interaction Studies 17, 1 (2016), 41-69.</p>
<p>WordNet: a lexical database for English. A George, Miller, Commun. ACM. 38George A Miller. 1995. WordNet: a lexical database for English. Commun. ACM 38, 11 (1995), 39-41.</p>
<p>Formalizing conceptual spaces. Martin Raubal, Formal ontology in information systems, proceedings of the third international conference (FOIS 2004). 114Martin Raubal. 2004. Formalizing conceptual spaces. In Formal ontology in information systems, proceedings of the third international conference (FOIS 2004), Vol. 114. 153-164.</p>
<p>Cognition and Categorization. L. Erlbaum Associates, Chapter Principles of categorization. Eleanor Rosch and Barbara B. LloydEleanor Rosch and Barbara B. Lloyd (Eds.). 1978. Cognition and Categorization. L. Erlbaum Associates, Chapter Principles of categorization, 27-48.</p>
<p>Family resemblances: Studies in the internal structure of categories. Eleanor Rosch, Carolyn B Mervis, Cognitive psychology. 7Eleanor Rosch and Carolyn B Mervis. 1975. Family resemblances: Studies in the internal structure of categories. Cognitive psychology 7, 4 (1975), 573-605.</p>
<p>Basic objects in natural categories. Eleanor Rosch, Carolyn B Mervis, Wayne D Gray, M David, Penny Johnson, Boyes-Braem, Cognitive psychology. 8Eleanor Rosch, Carolyn B Mervis, Wayne D Gray, David M Johnson, and Penny Boyes-Braem. 1976. Basic objects in natural categories. Cognitive psychology 8, 3 (1976), 382-439.</p>
<p>Feature centrality and conceptual coherence. A Steven, Sloman, C Bradley, Woo-Kyoung Love, Ahn, Cognitive Science. 22Steven A Sloman, Bradley C Love, and Woo-Kyoung Ahn. 1998. Feature centrality and conceptual coherence. Cognitive Science 22, 2 (1998), 189-228.</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Thirty-first AAAI conference on artificial intelligence. Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Thirty-first AAAI conference on artificial intelligence.</p>
<p>Going deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1-9.</p>
<p>Gramatika i predočavanje. Elżbieta Tabakowska, Filozofski Fakultet Sveučilišta u Zagrebu-FF Pressuvod u kognitivnu lingvistikuElżbieta Tabakowska. 2005. Gramatika i predočavanje: uvod u kognitivnu lingvis- tiku. Filozofski Fakultet Sveučilišta u Zagrebu-FF Press.</p>
<p>Theory-based Bayesian models of inductive learning and reasoning. B Joshua, Tenenbaum, L Thomas, Charles Griffiths, Kemp, Trends in cognitive sciences. 107Joshua B Tenenbaum, Thomas L Griffiths, and Charles Kemp. 2006. Theory-based Bayesian models of inductive learning and reasoning. Trends in cognitive sciences 10, 7 (2006), 309-318.</p>
<p>How to grow a mind: Statistics, structure, and abstraction. B Joshua, Charles Tenenbaum, Kemp, L Thomas, Noah D Griffiths, Goodman, science. 331Joshua B Tenenbaum, Charles Kemp, Thomas L Griffiths, and Noah D Goodman. 2011. How to grow a mind: Statistics, structure, and abstraction. science 331, 6022 (2011), 1279-1285.</p>
<p>Interpretable to whom? A role-based model for analyzing interpretable machine learning systems. Richard Tomsett, Dave Braines, Dan Harborne, Alun Preece, Supriyo Chakraborty, arXiv:1806.07552arXiv preprintRichard Tomsett, Dave Braines, Dan Harborne, Alun Preece, and Supriyo Chakraborty. 2018. Interpretable to whom? A role-based model for analyzing interpretable machine learning systems. arXiv preprint arXiv:1806.07552 (2018).</p>
<p>Fuzzy sets. In Fuzzy sets, fuzzy logic, and fuzzy systems: selected papers by Lotfi A Zadeh. A Lotfi, Zadeh, World ScientificLotfi A Zadeh. 1996. Fuzzy sets. In Fuzzy sets, fuzzy logic, and fuzzy systems: selected papers by Lotfi A Zadeh. World Scientific, 394-432.</p>
<p>Applications of conceptual spaces. Frank Zenker, Peter Gärdenfors, 25Frank Zenker and Peter Gärdenfors. 2015. Applications of conceptual spaces. Cited on 25 (2015).</p>
<p>Towards an Integrated Evaluation Framework for XAI: An Experimental Study. Qiyuan Zhang, Mark Hall, Mark Johansen, Vedran Galetic, Jacques Grange, Santiago Quintana-Amate, Alistair Nottle, Dylan M Jones, Phillip L Morgan, 10.1016/j.procs.2022.09.450Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 26th International Conference KES2022. 207Qiyuan Zhang, Mark Hall, Mark Johansen, Vedran Galetic, Jacques Grange, Santiago Quintana-Amate, Alistair Nottle, Dylan M Jones, and Phillip L Morgan. 2022. Towards an Integrated Evaluation Framework for XAI: An Experimental Study. Procedia Computer Science 207 (2022), 3884-3893. https://doi.org/10.1016/ j.procs.2022.09.450 Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 26th International Conference KES2022.</p>            </div>
        </div>

    </div>
</body>
</html>
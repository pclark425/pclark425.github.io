<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6497 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6497</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6497</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-258212839</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.09797v6.pdf" target="_blank">Progressive-Hint Prompting Improves Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6497.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6497.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PHP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progressive-Hint Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative prompting method that feeds previous model answers back into subsequent prompts as 'hints' and repeats until two consecutive answers stabilize, intended to progressively guide LLMs toward correct reasoning outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Progressive-Hint Prompting (PHP) + Complex CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school math word problems (arithmetic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>95.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Complex CoT (without PHP)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>3.5</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report PHP is orthogonal to CoT/self-consistency and combines with them; PHP yields larger benefits with more powerful models and more powerful prompts (Complex CoT). PHP uses iterative refinement (previous answers as hints) and tends to converge to a stable answer; hint quality matters.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Progressive-Hint Prompting Improves Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6497.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6497.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PHP-SVAMP-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progressive-Hint Prompting (PHP) combined with Complex CoT on SVAMP using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of PHP combined with Complex Chain-of-Thought prompting evaluated on the SVAMP arithmetic robustness dataset with GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Progressive-Hint Prompting (PHP) + Complex CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SVAMP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>robustness-focused arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>91.9</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Complex CoT (without PHP)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.8</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>PHP improves SOTA on SVAMP when paired with GPT-4 and Complex CoT; authors emphasize stronger gains with stronger models and prompts, and that non-merge PHP is preferred for powerful prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Progressive-Hint Prompting Improves Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6497.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6497.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PHP-AQuA-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progressive-Hint Prompting (PHP) combined with Complex CoT on AQuA using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PHP applied to multi-choice math problems in AQuA using GPT-4 and Complex CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Progressive-Hint Prompting (PHP) + Complex CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AQuA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>algebraic/multi-step math questions (multiple choice)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>79.9</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Complex CoT (without PHP)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>3.5</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report PHP improves GPT-4 performance on AQuA; they note hint quality and inclusion of both correct and incorrect hints affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Progressive-Hint Prompting Improves Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6497.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6497.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PHP-MATH-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progressive-Hint Prompting (PHP) combined with Complex CoT on MATH using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PHP applied to the MATH dataset with GPT-4, showing improved performance over prior CoT-based results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Progressive-Hint Prompting (PHP) + Complex CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>high-school and contest-style mathematics problems (multi-step mathematical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>53.9</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Complex CoT (without PHP)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>3.6</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>PHP raises GPT-4 performance on MATH by a few percentage points; authors highlight greater absolute gains on harder benchmarks and with stronger prompts/models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Progressive-Hint Prompting Improves Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6497.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6497.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PHP (text-davinci-003) GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progressive-Hint Prompting combined with (base) Complex CoT evaluated with text-davinci-003 on GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation showing that adding PHP to prompts yields an accuracy improvement on GSM8K with text-davinci-003 using greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Progressive-Hint Prompting + Complex CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy improvement (percentage points)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>4.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Complex CoT (without PHP)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>4.2</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper explicitly reports a 4.2 percentage-point improvement on GSM8K with text-davinci-003 when using PHP vs Complex CoT with greedy decoding; authors attribute improved robustness to the iterative use of prior answers as hints.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Progressive-Hint Prompting Improves Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6497.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6497.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (w/ PHP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency sampling combined with Progressive-Hint Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combining self-consistency (sampling multiple reasoning paths and aggregating) with PHP to improve accuracy and reduce required sampled reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfconsistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency + PHP</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble (sampling + aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>general (reported reductions across datasets, e.g., GSM8K/Complex CoT experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step reasoning evaluated via multiple sampled reasoning paths and majority/complex voting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sample-paths reduction (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>46.17</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Self-Consistency (without PHP)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>46.17</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors claim PHP can reduce the number of sampled reasoning paths needed by self-consistency (cost reduction) while maintaining or improving accuracy; they quantify an example reduction (~46.17%) in sampled paths required when PHP is used.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Progressive-Hint Prompting Improves Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6497.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6497.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT + SC (MultiArith, text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with Self-Consistency, and further with PHP, on MultiArith using text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison showing CoT with self-consistency reaches high accuracy on MultiArith and that PHP further improves that accuracy and/or reduces cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>CoT + Self-Consistency (with and without PHP)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble (sampling) + sequential (when PHP applied)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (self-consistency) vs homogeneous (PHP iterations)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step arithmetic reasoning (synthetic arithmetic dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>97.1</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT + Self-Consistency (without PHP)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>0.6</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper reports CoT with self-consistency reaches 96.5% (text-davinci-003) and PHP raises it to 97.1%; authors point out PHP can both improve absolute accuracy and reduce self-consistency sample requirements (cost).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Progressive-Hint Prompting Improves Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6497.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6497.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complex CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complex Chain-of-Thought (Complex CoT / Complexity-based prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that prefers using complex exemplars (higher complexity prompts) to elicit stronger multi-step reasoning from LLMs; used as a more powerful prompt in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Complexity-based prompting for multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Complex Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (exemplar-based CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (reported as baseline for comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>91.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Standard CoT / Standard prompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors use Complex CoT as a stronger base prompt and report that PHP provides larger improvements when paired with stronger prompts (Complex CoT > CoT > Standard). Also report merge vs non-merge PHP ablations for Complex CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Progressive-Hint Prompting Improves Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Selfconsistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Complexity-based prompting for multi-step reasoning <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6497",
    "paper_id": "paper-258212839",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "PHP",
            "name_full": "Progressive-Hint Prompting",
            "brief_description": "An iterative prompting method that feeds previous model answers back into subsequent prompts as 'hints' and repeats until two consecutive answers stabilize, intended to progressively guide LLMs toward correct reasoning outcomes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "reasoning_method_name": "Progressive-Hint Prompting (PHP) + Complex CoT",
            "reasoning_method_type": "sequential (iterative refinement)",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school math word problems (arithmetic reasoning)",
            "performance_metric": "accuracy (percentage)",
            "performance_value": 95.5,
            "comparison_target_method": "Complex CoT (without PHP)",
            "performance_difference": 3.5,
            "statistical_significance": null,
            "analysis_notes": "Authors report PHP is orthogonal to CoT/self-consistency and combines with them; PHP yields larger benefits with more powerful models and more powerful prompts (Complex CoT). PHP uses iterative refinement (previous answers as hints) and tends to converge to a stable answer; hint quality matters.",
            "ablation_study_present": true,
            "uuid": "e6497.0",
            "source_info": {
                "paper_title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "PHP-SVAMP-GPT4",
            "name_full": "Progressive-Hint Prompting (PHP) combined with Complex CoT on SVAMP using GPT-4",
            "brief_description": "Application of PHP combined with Complex Chain-of-Thought prompting evaluated on the SVAMP arithmetic robustness dataset with GPT-4.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "reasoning_method_name": "Progressive-Hint Prompting (PHP) + Complex CoT",
            "reasoning_method_type": "sequential (iterative refinement)",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "SVAMP",
            "task_description": "robustness-focused arithmetic word problems",
            "performance_metric": "accuracy (percentage)",
            "performance_value": 91.9,
            "comparison_target_method": "Complex CoT (without PHP)",
            "performance_difference": 2.8,
            "statistical_significance": null,
            "analysis_notes": "PHP improves SOTA on SVAMP when paired with GPT-4 and Complex CoT; authors emphasize stronger gains with stronger models and prompts, and that non-merge PHP is preferred for powerful prompts.",
            "ablation_study_present": true,
            "uuid": "e6497.1",
            "source_info": {
                "paper_title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "PHP-AQuA-GPT4",
            "name_full": "Progressive-Hint Prompting (PHP) combined with Complex CoT on AQuA using GPT-4",
            "brief_description": "PHP applied to multi-choice math problems in AQuA using GPT-4 and Complex CoT prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "reasoning_method_name": "Progressive-Hint Prompting (PHP) + Complex CoT",
            "reasoning_method_type": "sequential (iterative refinement)",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "AQuA",
            "task_description": "algebraic/multi-step math questions (multiple choice)",
            "performance_metric": "accuracy (percentage)",
            "performance_value": 79.9,
            "comparison_target_method": "Complex CoT (without PHP)",
            "performance_difference": 3.5,
            "statistical_significance": null,
            "analysis_notes": "Authors report PHP improves GPT-4 performance on AQuA; they note hint quality and inclusion of both correct and incorrect hints affect outcomes.",
            "ablation_study_present": true,
            "uuid": "e6497.2",
            "source_info": {
                "paper_title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "PHP-MATH-GPT4",
            "name_full": "Progressive-Hint Prompting (PHP) combined with Complex CoT on MATH using GPT-4",
            "brief_description": "PHP applied to the MATH dataset with GPT-4, showing improved performance over prior CoT-based results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "reasoning_method_name": "Progressive-Hint Prompting (PHP) + Complex CoT",
            "reasoning_method_type": "sequential (iterative refinement)",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "MATH",
            "task_description": "high-school and contest-style mathematics problems (multi-step mathematical reasoning)",
            "performance_metric": "accuracy (percentage)",
            "performance_value": 53.9,
            "comparison_target_method": "Complex CoT (without PHP)",
            "performance_difference": 3.6,
            "statistical_significance": null,
            "analysis_notes": "PHP raises GPT-4 performance on MATH by a few percentage points; authors highlight greater absolute gains on harder benchmarks and with stronger prompts/models.",
            "ablation_study_present": true,
            "uuid": "e6497.3",
            "source_info": {
                "paper_title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "PHP (text-davinci-003) GSM8K",
            "name_full": "Progressive-Hint Prompting combined with (base) Complex CoT evaluated with text-davinci-003 on GSM8K",
            "brief_description": "Evaluation showing that adding PHP to prompts yields an accuracy improvement on GSM8K with text-davinci-003 using greedy decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_size": null,
            "reasoning_method_name": "Progressive-Hint Prompting + Complex CoT",
            "reasoning_method_type": "sequential (iterative refinement)",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school math word problems",
            "performance_metric": "accuracy improvement (percentage points)",
            "performance_value": 4.2,
            "comparison_target_method": "Complex CoT (without PHP)",
            "performance_difference": 4.2,
            "statistical_significance": null,
            "analysis_notes": "Paper explicitly reports a 4.2 percentage-point improvement on GSM8K with text-davinci-003 when using PHP vs Complex CoT with greedy decoding; authors attribute improved robustness to the iterative use of prior answers as hints.",
            "ablation_study_present": true,
            "uuid": "e6497.4",
            "source_info": {
                "paper_title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Self-Consistency (w/ PHP)",
            "name_full": "Self-Consistency sampling combined with Progressive-Hint Prompting",
            "brief_description": "Combining self-consistency (sampling multiple reasoning paths and aggregating) with PHP to improve accuracy and reduce required sampled reasoning paths.",
            "citation_title": "Selfconsistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_size": null,
            "reasoning_method_name": "Self-Consistency + PHP",
            "reasoning_method_type": "ensemble (sampling + aggregation)",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "general (reported reductions across datasets, e.g., GSM8K/Complex CoT experiments)",
            "task_description": "multi-step reasoning evaluated via multiple sampled reasoning paths and majority/complex voting",
            "performance_metric": "sample-paths reduction (percentage)",
            "performance_value": 46.17,
            "comparison_target_method": "Self-Consistency (without PHP)",
            "performance_difference": 46.17,
            "statistical_significance": null,
            "analysis_notes": "Authors claim PHP can reduce the number of sampled reasoning paths needed by self-consistency (cost reduction) while maintaining or improving accuracy; they quantify an example reduction (~46.17%) in sampled paths required when PHP is used.",
            "ablation_study_present": true,
            "uuid": "e6497.5",
            "source_info": {
                "paper_title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "CoT + SC (MultiArith, text-davinci-003)",
            "name_full": "Chain-of-Thought with Self-Consistency, and further with PHP, on MultiArith using text-davinci-003",
            "brief_description": "Comparison showing CoT with self-consistency reaches high accuracy on MultiArith and that PHP further improves that accuracy and/or reduces cost.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_size": null,
            "reasoning_method_name": "CoT + Self-Consistency (with and without PHP)",
            "reasoning_method_type": "ensemble (sampling) + sequential (when PHP applied)",
            "reasoning_style_diversity": "diverse (self-consistency) vs homogeneous (PHP iterations)",
            "benchmark_name": "MultiArith",
            "task_description": "multi-step arithmetic reasoning (synthetic arithmetic dataset)",
            "performance_metric": "accuracy (percentage)",
            "performance_value": 97.1,
            "comparison_target_method": "CoT + Self-Consistency (without PHP)",
            "performance_difference": 0.6,
            "statistical_significance": null,
            "analysis_notes": "Paper reports CoT with self-consistency reaches 96.5% (text-davinci-003) and PHP raises it to 97.1%; authors point out PHP can both improve absolute accuracy and reduce self-consistency sample requirements (cost).",
            "ablation_study_present": true,
            "uuid": "e6497.6",
            "source_info": {
                "paper_title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Complex CoT",
            "name_full": "Complex Chain-of-Thought (Complex CoT / Complexity-based prompting)",
            "brief_description": "A prompting strategy that prefers using complex exemplars (higher complexity prompts) to elicit stronger multi-step reasoning from LLMs; used as a more powerful prompt in the paper.",
            "citation_title": "Complexity-based prompting for multi-step reasoning",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_size": null,
            "reasoning_method_name": "Complex Chain-of-Thought",
            "reasoning_method_type": "sequential (exemplar-based CoT)",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "GSM8K (reported as baseline for comparisons)",
            "task_description": "grade-school math word problems",
            "performance_metric": "accuracy (percentage)",
            "performance_value": 91.5,
            "comparison_target_method": "Standard CoT / Standard prompt",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Authors use Complex CoT as a stronger base prompt and report that PHP provides larger improvements when paired with stronger prompts (Complex CoT &gt; CoT &gt; Standard). Also report merge vs non-merge PHP ablations for Complex CoT.",
            "ablation_study_present": true,
            "uuid": "e6497.7",
            "source_info": {
                "paper_title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Selfconsistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Complexity-based prompting for multi-step reasoning",
            "rating": 2,
            "sanitized_title": "complexitybased_prompting_for_multistep_reasoning"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 1,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.017623,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Progressive-Hint Prompting Improves Reasoning in Large Language Models
7 Oct 2024</p>
<p>Chuanyang Zheng cyzheng21@cse.cuhk.edu.hk 
3 HKU, 4 HKUSTParisXI</p>
<p>Zhengying Liu 
3 HKU, 4 HKUSTParisXI</p>
<p>Enze Xie 
3 HKU, 4 HKUSTParisXI</p>
<p>Zhenguo Li 
3 HKU, 4 HKUSTParisXI</p>
<p>Yu Li 
3 HKU, 4 HKUSTParisXI</p>
<p>Cuhk 
3 HKU, 4 HKUSTParisXI</p>
<p>Progressive-Hint Prompting Improves Reasoning in Large Language Models
7 Oct 202454F2685663AE0F914E37651C3CC7671EarXiv:2304.09797v6[cs.CL]
The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability.However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses.This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers.PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance.We conducted extensive and comprehensive experiments on seven benchmarks.The results show that PHP significantly improves accuracy while remaining highly efficient.For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency.With GPT-4 and PHP, we achieve state-ofthe-art performances on SVAMP (89.1%  91.9%), GSM8K (92%  95.5%), AQuA (76.4%  79.9%) and MATH (50.3%  53.9%).Preprint.Under review.</p>
<p>Introduction</p>
<p>While Large Language Models (LLMs) have demonstrated remarkable performance across various NLP tasks [1][2][3], their ability to reason is often perceived as a limitation that cannot be overcome merely by increasing the scale of the model [4,5].Prompt engineering in large-scale models has shown comparable or superior performance to full training set fine-tuning in enhancing reasoning ability, while also being significantly more sample-efficient [6,7].One area of research that aims to address this limitation is the use of Chain-of-Thought (CoT) approaches to promote intermediate reasoning steps [8][9][10].Other works in this area, such as Least-to-Most [9] and Complex CoT [10], have also explored this direction.Another area of research is self-consistency-related approaches.In comparison to CoT-related work that focuses on designing better prompts, self-consistency proposes to sample multiple answers from the LLMs and arrive at the correct answer through a majority vote [10].This approach is further improved upon by complex-based selection [10].CoT-related and self-consistency-related works can be seamlessly combined without any conflict.</p>
<p>Prior research has not explored the potential of leveraging the outputs of LLM to refine reasoning paths iteratively.It stands to reason that similar to human cognition, LLM could benefit from reevaluating and adjusting its generated reasoning paths in order to correct errors and enhance overall performance.In this paper, we propose a new method named Progressive-Hint Prompting (PHP) that involves sequentially interacting with LLM to approach the correct answer gradually.The method operates as follows: (1) given a question, we ask the LLM to provide a Base Answer; (2)   Our proposed Progressive-Hint Prompting method combines the generated answers and questions for double-checking purposes, which is divided into two stages.In the first stage, we generate a base answer by passing to the LLM a concatenation of the current question and a base prompt, such as CoT or Complex CoT.In the second stage, we generate the subsequent answers via the corresponding progressive-hint prompt, such as Progressive-Hint Prompting CoT (PHP-CoT) or Progressive-Hint Prompting Complex CoT (PHP-Complex CoT), for the subsequent interaction.The interaction stops when two consecutive answers are the same.Purple Box: The input of LLM.Orange Box: The output of LLM.</p>
<p>the question and answer to re-ask the LLM and obtain the Subsequent Answer; (3) we repeat the operation in ( 2) until the answer is stable and does not change over the last two answers.PHP follows a human-like thought process where previous answers are leveraged as hints to arrive at the correct answer after re-evaluating the question.</p>
<p>Figure 1 illustrates the proposed PHP framework.We use the base prompt to obtain the initial base answer, and then employ the PHP prompt for subsequent questions.If the current answer matches the previous answer, it is more likely to be correct, and we terminate the LLM inquiry.With Complex CoT and GPT-4, after adding PHP, the performance achieves SOTA with 91.9% on SVAMP [11], 95.5% on GSM8K [12], and 79.9% on AQuA [13] and 53.9% on MATH [14].</p>
<p>In summary, our contributions are as follows:</p>
<p> We propose a new method, Progressive-Hint Prompting (PHP), alongside CoT and selfconsistency, for improving LLM reasoning abilities. We demonstrate the effectiveness of PHP through extensive experimentation, including baseline comparisons and ablation studies, using four LLMs, text-davinci-002 and textdavinci-003, GPT-3.5-Turbo and GPT-4 [15][16][17]. The experiment results show that our method can also improve performance with selfconsistency. We believe that progressive-hint prompting represents an important step towards automatic sequential interaction with LLMs and hope that it inspires future research in this field.</p>
<p>Related Work</p>
<p>Emergent Abilities and Multi-Step Reasoning.LLMs are particularly skilled at in-context learning, which involves adhering to the structure of prompts (typically few-shot) and completing corresponding tasks [15,[18][19][20].Among the diverse range of language comprehension tasks, we are particularly interested in multi-step reasoning because it exhibits two unique features.Firstly, LLMs significantly outperform smaller models on multi-step reasoning tasks [8], whereas their performance gains on tasks like sentiment classification can be limited [19].Secondly, few-shot prompting outperforms full training set fine-tuning in multi-step reasoning tasks, even when conducted on LLMs [7].</p>
<p>Chain-of-Thought Reasoning.Chain-of-thought (CoT) prompting [8] is a prominent work that demonstrates the multi-step reasoning capacities of LLMs.This approach suggests that the reasoning ability can be elicited through a chain of thoughts, where an answer directly follows a question without intermediate reasoning steps.Least-to-Most prompting [9], which follows the same research direction, divides reasoning into problem breakdown parts and problem answer parts and describes the reasoning steps in more detail.Similarly, the complex CoT [10] highlights the importance of prompt complexity and selects the most complex questions and their answers as prompts.To reduce the human workload, the Auto-CoT is proposed [21].Other works have found that using specific phrases like "Let's think step by step" [6] can improve performance.</p>
<p>Reasoning Path Extraction.Previous research has investigated various task-specific methods for identifying reasoning paths, including constructing semantic graphs [22], developing Recurrent Neural Network (RNN) models to retrieve reasoning paths from a Wikipedia graph [23], using human-annotated reasoning paths on math problems for fine-tuning [12], or training an extractor with heuristic-based pseudo reasoning paths [24].A novel research work, named Self-Consistency [25], couples the generation of reasoning paths and a final answer by sampling from the decoder and using aggregation to retrieve the most consistent answer without extra modules.This approach has shown great promise, and it has the potential to outperform existing methods in terms of accuracy.Furthermore, the vote complex [10], rank and select samples via complexity, is proposed to improve the self-consistency performance.This approach is particularly useful when dealing with complex reasoning problems.</p>
<p>3 Progressive-Hint Prompting Q: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?A: There are 15 trees originally.Then there were 21 trees after the Grove workers planted some more.So there must have been 21 -15 = 6 trees that were planted.The answer is 6.</p>
<p>Progressive-Hint Prompting 1: Hint is the correct answer Q: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?(Hint: The answer is near to 6).A: We know the Answer Hints: 6.With the Answer Hints: 6, we will answer the question.There are 15 trees originally.Then there were 21 trees after the Grove workers planted some more.So there must have been 21 -15 = 6 trees that were planted.The answer is 6.</p>
<p>Progressive-Hint Prompting 2: Hint is the incorrect answer Q: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?(Hint: The answer is near to 10, 8).A: We know the Answer Hints: 10, 8.With the Answer Hints: 10, 8, we will answer the question.There are 15 trees originally.Then there were 21 trees after the Grove workers planted some more.So there must have been 21 -15 = 6 trees that were planted.The answer is 6.</p>
<p>One salient aspect of humanity is our ability to not only think once, but to also double-check our answers.In this paper, we propose that this process can be simulated in language models by sequentially employing previous answers.In other words, a model can generate an answer and then</p>
<p>Experiments</p>
<p>Datasets and Models.We evaluate PHP on seven datasets (AddSub [26], MultiArith [27], Sin-gleEQ [28], SVAMP [11], GSM8K [12], AQuA [13]) and MATH [14].We choose the datasets because we focus on the reasoning ability of the model.The utilized for both the Standard and CoT prompts are sourced from the original CoT paper [8], whereas the prompt utilized for the Complex CoT [10] prompt is derived from the corresponding Complex CoT publication.Also, to validate our proposed method performance, we employ four models: text-davinci-002 and text-davinci-003, GPT-3.5-Turbo and GPT-4 [15][16][17].All models are employed via OpenAI API key.</p>
<p>Prompts.We have shown the proposed process pipeline in the Method part.We show all the prompts in the Appendix and supplementary materials.</p>
<p>Main Results</p>
<p>The main results of our study are presented in Table 2, with all methods using greedy decoding (i.e.temperature = 0).Our findings indicate that the proposed PHP improves performance, particularly when working with powerful prompts and models.PHP works better when the LLM is more powerful.In terms of model power, our analysis indicates that PHP is most effective when applied with powerful models.Specifically, when examining CoT and Complex CoT prompts, we found that while text-davinci-002 generally yielded a performance improvement after adding hints, there were occasions when performance would decline.However, when we replaced text-davinci-002 with text-davinci-003, performance improvement became more consistent and significant.For example, PHP-Complex CoT using text-davinci-002 improved performance by 3.6%, but then increased further to 4.6% with text-davinci-003.Similarly, on AQuA dataset, using PHP-Complex CoT resulted in a performance drop of 0.4% with text-davinci-002 but a 1.2% improvement with text-davinci-003.The text-davinci-002 is finetuned with supervised instruction tuning, while the text-davinci-003 is finetuned with reinforcement learning.The improved performance with text-davinci-003 can be attributed to its enhanced power, making it better at understanding and employing the given hint.</p>
<p>PHP works better when the prompt is more powerful.After analyzing our data, it was determined that the prompt's power has a significant impact on the performance of the system.Our experimental results revealed that while the inclusion of PHP produced modest improvements with standard prompts, CoT and Complex CoT prompts demonstrated substantial gains in performance.Particularly noteworthy is the fact that the most potent prompt, Complex CoT, exhibited the most substantial performance improvement in comparison to the Standard prompt and CoT prompt.This finding provides compelling evidence that a superior prompt leads to greater effectiveness of the system.</p>
<p>The Interaction Number decreases when the model is more powerful and the prompt is less powerful.The number of interactions refers to how many times the agent engages with the LLMs.</p>
<p>The interaction number is one when the agent receives the first answer and increases to two for the second answer.In Figure 2, we illustrate the interaction number of various models and prompts.Our findings indicate that: 1) The interaction number for text-davinci-003 is typically lower than that of text-davinci-002 when given the same prompt.This is primarily due to the higher accuracy of text-davinci-003, resulting in a higher probability of the base answer and subsequent answers being correct, thus requiring fewer interactions to obtain the final correct answer; 2) When using the same models, the interaction number generally increases as the prompt becomes more powerful.This is because the LLMs achieves better reasoning ability when the prompt becomes more potent, allowing them to leverage the hints to jump out of the incorrect answers, and ultimately leading to a higher number of interactions required to reach the final answer.The quality of the hint significantly affects the performance.Shown in and the subsequent answer is based on PHP-Complex CoT (or PHP-CoT), it is comparable to having two individuals collaborating to solve a problem.Therefore, in such circumstances, the system's performance may be further enhanced.</p>
<p>Impact of the Hint Quality</p>
<p>Ablation Study</p>
<p>Furthermore, we conducted an ablation study to verify the criticality of the two sentences in answers: 1) P1: We know the Answer Hints A 1 , ..., A p ; 2) P2: With the Answer Hints A 1 , ..., A p , we will answer the question.Moreover, we introduced a new type of prompt called CoT-Merge and Complex CoT-Merge.Firstly, we combined the original prompt with the PHP prompt into a single file.Subsequently, we utilized the same Merge Prompt for both the base answer and subsequent answers.Also, we prove that both correct and incorrect hints are necessary in prompt design.</p>
<p>The Proposed P1 and P2 are necessary.Incorporating the sentences P1 and P2 resulted in better performance for CoT with PHP across three of the six datasets.However, the significance of these two sentences became particularly apparent when we employed Complex CoT.With this method, better performance was achieved on five of the six datasets after adding P1 and P2.For instance, Complex CoT improved its performance from 78.0% to 80.0% on the SVAMP dataset, and from 68.3% to 71.6% on the GSM8K dataset.This highlights that sentences P1 and P2 can exhibit more potent abilities, particularly when the model's logical capacity is superior.Consequently, we can conclude that P1 and P2 will likely enhance model performance to a greater extent, particularly with more powerful prompts and models.</p>
<p>Non-Merge based PHP is better than merge based PHP when prompts are more powerful.Regarding the CoT with PHP-CoT, the initial answer is derived from the CoT prompt, and subsequently, Hence, when prompts are better, the performance of non-merge based method will be better than merge-based method.</p>
<p>Both correct and incorrect hints are needed in the prompt design.Table 5 demonstrates that the use of PHP was superior to its absence when the designed prompt included both correct and incorrect hints.Specifically, the provision of a correct hint in the prompt promoted the generation of answers that matched the given hint.Conversely, the provision of incorrect answers in the prompt encouraged the generation of alternative answers, with the aid of the given hint.</p>
<p>Performance with Self-Consistency</p>
<p>As we discussed before, our proposed method can combine with CoT and self-consistency to further improve the model performance.The results are shown in Table 6.Following the self-consistency paper, we sample paths with numbers 5, 10, 20 and 40, and the model temperature 0.7.</p>
<p>PHP further improves performance.By utilizing similar prompts and sample path numbers, we discovered that our proposed PHP-CoT and PHP-Complex CoT always achieve superior performance when compared to CoT and Complex CoT, shown in Table 6 and Figure 3.For instance, CoT with self-consistency was able to attain a 96.5% accuracy on the MultiArith dataset with a sample path of 10, 20 and 40.Therefore, we can conclude that the best performance by CoT with self-consistency is 96.5% with text-davinci-003.However, after implementing PHP, the performance skyrocketed to  According to 1) and 2), we can see that PHP could further improve performance.With result 3), we found that the PHP could even reduce the cost of self-consistency.97.1%.Similarly, we observed CoT with self-consistency on SVAMP, achieving the best accuracy of 83.3% with 20 sampled paths, and further improved to 83.7% upon implementing PHP.This illustrates that PHP could break the performance bottleneck and further improve performance.</p>
<p>PHP could reduce the cost of self-consistency.Incorporating the PHP can also lead to cost reduction.It is widely acknowledged that self-consistency involves an increased number of reasoning paths, resulting in a higher cost.The Table 6 illustrates that PHP can be an effective approach for reducing this cost, while still preserving performance gains.As shown in Figure 3, using Complex CoT with self-consistency, a 78.1% accuracy can be reached with 40 sample paths, while incorporating PHP reduces the required sample amount to 102.1531=21.531paths, and results in an even better accuracy of 78.2%.</p>
<p>Performance with Chat Model</p>
<p>In the previous section, we follow the previous work settings and employ text generation models for our experiments.With the release API of GPT-3.5-Turbo and GPT-4, we validate the performance of Complex CoT with PHP on the same six datasets.We use greedy decoding (i.e.temperature = 0) and Complex CoT as prompt for both models.</p>
<p>Analyze GPT-3.5-Turbo.As depicted in Table 7, the proposed PHP enhances performance, improving 2.3% on GSM8K and 3.2% on AQuA.However, GPT-3.5-Turbo exhibits a reduced ability to adhere to prompts compared to text-davinci-003.We provide two examples to illustrate this point: a)</p>
<p>In cases where the given hints are absent, GPT-3.5-Turbofails to answer the question and responds with a statement such as, "We cannot answer this question as the answer hint is missing.Please provide the answer hint to proceed.".In contrast, text-davinci-003 autonomously generates and fills in the missing answer hint before addressing the question (as demonstrated in the Appendix); b) When more than ten hints are provided, GPT-3.5-Turbo may respond with "We cannot determine the correct answer as multiple answer hints are given.Please provide only one answer hint for the question.".</p>
<p>Analyze GPT-4.After deploying the GPT-4 model, we were able to achieve the new SOTA performance on the SVAMP, GSM8K, AQuA and MATH benchmarks.Our proposed PHP method consistently improves the performance of GPT-4.Furthermore, compared to the GPT-3.5-Turbomodel, we observed a reduction in the number of interactions required by GPT-4, which aligns with the finding that "The Interaction Number decreases when the model is more powerful."</p>
<p>Conclusion</p>
<p>This paper introduces a novel approach named Progressive-Hint Prompting (PHP) for interacting with LLMs, which offers multiple advantages: 1) PHP achieves substantial performance improvements on math reasoning tasks, leading to state-of-the-art results on several reasoning benchmarks; 2) with more powerful models and prompts, PHP can better and consistently benefit the LLMs; 3) PHP can be easily combined with CoT and self-consistency to further improve performance.</p>
<p>To better enhance the progressive-hint prompting approach, future research endeavors can focus on improving the design of handcrafted hints in the question phase and prompt sentences in the answer part.Additionally, novel hints that aid the LLMs to reconsider the questions can be identified and extracted beside the answer.</p>
<p>[62] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.Scaling laws for neural language models.arXiv preprint arXiv:2001.08361,2020.</p>
<p>[63] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna</p>
<p>Implementation Details</p>
<p>We have provided the code in our supplementary materials.</p>
<p>Limitation and Further Work</p>
<p>In this paper, we will explore the limitations of our proposed progressive-hint prompting technique and discuss possible avenues for further improvement.</p>
<p>The Progressive-Hint Prompt is handcrafted.Our proposed progressive-hint prompts are handcrafted by humans, similar to other related techniques such as Chain-Of-Thought and Complex Chain of Thought.As such, we aim to design Auto Progressive Hint in the future to improve its efficiency.</p>
<p>For instance, we could continuously build and update the progressive hint during testing.</p>
<p>The hint is defined as the answer.In this paper, we defined the hint as the answer.However, we acknowledge that the concept of hint encompasses other possibilities generated by models.These hints may include model confidence, reasoning path, and even interaction number.</p>
<p>The interaction number is at least two.Our design requires at least two interactions to determine the final answers.Therefore, using greedy decoding (where the sample path is always 1) as opposed to self-consistency can increase costs.Going forward, we plan to employ a powerful model initially to obtain the base answer then rely on cheaper models for subsequent answers.This approach could help us reduce the overall cost.</p>
<p>Broader Impacts</p>
<p>Progressive-Hint Prompting aims to enhance the reasoning ability of Large Language Models (LLMs) by utilizing previous answers.We believe that the integration of PHP with LLM can be applied in a variety of areas, including: 1) Assisting students, particularly those from low-income areas, in learning more effectively and obtaining accurate answers with the help of LLM and PHP; 2) Aiding mathematicians in solving complex mathematical problems; 3) and other reasoning-related applications.By leveraging PHP with LLM, we hope to improve the performance of these models and enable their use in various practical scenarios.</p>
<p>Model Response with Different Hints</p>
<p>Model Response with different hints.When presented with the hint 100, the model accurately solves the question.Conversely, the model can be misled if the provided hint strays too far from the correct answer.For instance, when the hint is 0, the model calculates the amount of not used carrots is 80 pounds, outputs an answer of 0.</p>
<p>Interaction Cases</p>
<p>In this section, we provide more interaction cases.Table 9: A GSM8K example of text-davinci-003 response with greedy decoding, and the prompt is PHP-Complex CoT.Based on our analysis, it appears that a hint that is too far from the correct answer has the potential to mislead LLMs and lead to an incorrect final answer.Conversely, a more effective hint can result in a more precise outcome.Question: 200 pounds of carrots will be distributed to 40 restaurants in a certain city.Each restaurant is to receive 2 pounds of carrots.How many pounds of carrots will not be used?Hint: 0.We know the Answer Hints: 0. With the Answer Hints: 0, we will answer the question.Let's think step by step We know that 200 pounds of carrots are to be distributed to 40 restaurants.Each restaurant is to receive 2 pounds of carrots.Therefore, 40 restaurants x 2 pounds of carrots = 80 pounds of carrots.The total amount of carrots used is 200 pounds -80 pounds = 120 pounds.The amount of carrots not used is 200 pounds -120 pounds = 80 pounds.The answer is 0 Hint: 100.We know the Answer Hints: 100.With the Answer Hints: 100, we will answer the question.Let's think step by step We know that 200 pounds of carrots are to be distributed to 40 restaurants.Each restaurant will receive 2 pounds of carrots, so 40 restaurants x 2 pounds = 80 pounds of carrots.This means that 200 pounds of carrots -80 pounds of carrots = 120 pounds of carrots will not be used.The answer is 120</p>
<p>Success Case</p>
<p>Question: James decides to run 3 sprints 3 times a week.He runs 60 meters each sprint.How many total meters does he run a week?A: Let's think step by step James runs 3 sprints 3 times a week, so he runs 3<em>3 = 9 sprints a week.Each sprint is 60 meters, so he runs 9</em>60 = 540 meters a week.</p>
<p>The answer is 540.</p>
<p>Question: James decides to run 3 sprints 3 times a week.He runs 60 meters each sprint.How many total meters does he run a week?(Hint: The answer is near to 540).A: We know the Answer Hints: 540.With the Answer Hints: 540, we will answer the question.Let's think step by step James runs 3 sprints 3 times a week.Each sprint is 60 meters, so he runs 3 x 60 = 180 meters each time.He runs 3 times a week, so he runs 180 x 3 = 540 meters each week.The answer is 540.</p>
<p>Question: Two track teams are competing against each other in a 4 by 400 meter relay; a race where each competing team has four members that each run 400 meters, or one lap, around a standard track.One of the two teams is very well-rounded and each of their members will run their 400 meter leg in precisely 55 seconds.The other team is less well-rounded; their first runner will run their 400 meter leg in 60 seconds then each subsequent runner will be 3 seconds faster than the previous runner.Using this information, how many seconds will the faster team win by? A: Let's think step by step The first team will take 4 x 55 seconds = 220 seconds to complete the race.</p>
<p>The second team will take 60 + 57 + 54 + 51 = 222 seconds to complete the race.</p>
<p>The first team will win by 220 -222 = -2 seconds.</p>
<p>The answer is -2.</p>
<p>Question: Two track teams are competing against each other in a 4 by 400 meter relay; a race where each competing team has four members that each run 400 meters, or one lap, around a standard track.One of the two teams is very well-rounded and each of their members will run their 400 meter leg in precisely 55 seconds.The other team is less well-rounded; their first runner will run their 400 meter leg in 60 seconds then each subsequent runner will be 3 seconds faster than the previous runner.Using this information, how many seconds will the faster team win by? (Hint: The answer is near to -2).A: We know the Answer Hints: -2.With the Answer Hints: -2, we will answer the question.Let's think step by step The first team will finish in 55 seconds x 4 runners = 220 seconds.</p>
<p>The second team will finish in 60 seconds + (3 seconds x 3 runners) = 75 seconds.</p>
<p>increase the price to original?Answer Choices: (A) 21% (B) 20% (C) 25% (D) 33% (E) 18% A: Let's think step by step The shopkeeper gave a discount of 20% on all the items.Therefore, the new price of the items = 80% of the original price.</p>
<p>To get back to the original price, the shopkeeper must increase the price by (100% -80%) = 20%.</p>
<p>The answer is (B), 20%</p>
<p>Question: A shopkeeper in order to promote his new shop put a discount of 20% on all the items for one day.Now he must sell the items at original price the other day.By what percentage must he increase the price to original?Answer Choices: (A) 21% (B) 20% (C) 25% (D) 33% (E) 18% (Hint:</p>
<p>The answer is near to (B)).A: We know the Answer Hints: (B).With the Answer Hints: (B), we will answer the question.Let's think step by step Let the original price of the item be x.Then, the price after discount = x -(20/100)x = 0.8x To get the original price, the shopkeeper must increase the price by (100/80)% = 25%.</p>
<p>The answer is (C), 25%</p>
<p>Question: A shopkeeper order to promote his new shop put a discount of 20% on all the items for one day.Now he must sell the items at original price the other day.By what percentage must he increase the price to original?Answer Choices: (A) 21% (B) 20% (C) 25% (D) 33% (E) 18% (Hint:</p>
<p>The answer is near to (B), (C)).A: We know the Answer Hints: (B), (C).With the Answer Hints: (B), (C), we will answer the question.</p>
<p>Let's think step by step Let the original price of the item be x.Then, the discounted price of the item = x -(20/100)x = 0.8x To get the original price, the shopkeeper must increase the price by (100/80)% = 25%.</p>
<p>The answer is (C), 25%</p>
<p>Failure Case</p>
<p>Question: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy.She gives the chickens their feed in three separate meals.In the morning, she gives her flock of chickens 15 cups of feed.In the afternoon, she gives her chickens another 25 cups of feed.How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi's flock is 20 chickens?A: Let's think step by step In the morning, Wendi gives her flock of chickens 15 cups of feed.Each chicken gets 3 cups of feed, so 15 cups of feed is for 5 chickens.</p>
<p>In the afternoon, Wendi gives her chickens another 25 cups of feed.Each chicken gets 3 cups of feed, so 25 cups of feed is for 8 chickens.So, in total, Wendi has fed 5+8=13 chickens.Since Wendi's flock is 20 chickens, she needs to feed 20-13=7 more chickens.Each chicken needs 3 cups of feed, so Wendi needs to give her chickens 7*3=21 cups of feed in the final meal of the day.</p>
<p>The answer is 21.</p>
<p>Question: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy.She gives the chickens their feed in three separate meals.In the morning, she gives her flock of chickens 15 cups of feed.In the afternoon, she gives her chickens another 25 cups of feed.How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi's flock is 20 chickens?(Hint: The answer is near to 21).A: We know the Answer Hints: 21.With the Answer Hints: 21, we will answer the question.Let's think step by step Wendi feeds each of her chickens three cups of feed per day.</p>
<p>In the morning, she gives her flock of chickens 15 cups of feed.This means that 15 cups of feed is enough for 5 chickens.</p>
<p>In the afternoon, she gives her chickens another 25 cups of feed.This means that 25 cups of feed is enough for 8 chickens.So, she has already fed 13 chickens in the morning and afternoon.She has 20 chickens in total, so she needs to feed 20-13 = 7 more chickens.Each chicken needs 3 cups of feed, so she needs to give 7 chickens x 3 cups of feed = 21 cups of feed in the final meal of the day.The answer 21.</p>
<p>Question The probability that neither of these two events will occur is the product of the two probabilities, which is 0.44 x 0.26 = 0.1144.The greatest value for the probability that neither of these two events will occur is 0.1144.The answer is (A), 0.22.</p>
<p>Question: If the probability that Stock A will increase in value during the next month is 0.56, and the probability that Stock B will increase in value during the next month is 0.74.What is the greatest value for the probability that neither of these two events will occur?Answer Choices: (A) 0.22 (B) 0.26 (C) 0.37 (D) 0.46 (E) 0.63 (Hint: The answer is near to (A)).A: We know the Answer Hints: (A).With the Answer Hints: (A), we will answer the question.Let's think step by step The probability that neither of these two events will occur is 1 -(0.56 + 0.74) = 1 -1.30= 0.30 The greatest value for the probability that neither of these two events will occur is 0.30.</p>
<p>The answer is (A), 0.22.</p>
<p>10.3 Response when Miss Hint for GPT-3.5 Text-Divinci-003</p>
<p>Question: Jerry has two pools, both with leaks emptying them out at 4 gallons/minute.4 minutes ago, the big pool had twice as much water as the small pool.Now the big pool has four times as much water as the small pool.How much water does the small pool have now?(Hint: The answer is near to ).</p>
<p>A: We know the Answer Hints: 16.With the Answer Hints: 16, we will answer the question.Let's think step by step 4 minutes ago, the big pool had twice as much water as the small pool.This means that the small pool had 4 gallons of water.In 4 minutes, the big pool has lost 4 gallons x 4 minutes = 16 gallons of water.</p>
<p>The small pool has also lost 4 gallons x 4 minutes = 16 gallons of water.Therefore, the small pool now has 4 gallons -16 gallons = -12 gallons of water.Since it is impossible to have negative gallons of water, the small pool must have 0 gallons of water.</p>
<p>The answer is 0.</p>
<p>Q: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?(Hint: The answer is near to 6).A: We know the Answer Hints: 6.With the Answer Hints: 6, we will answer the question.The answer is 6.</p>
<p>Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? (Hint: The answer is near to 10, 8  12 Chain-Of-Thought Prompt 12.1 For AddSub, MultiArith, SingleEQ, SVAMP and GSM8K</p>
<p>Base Chain-Of-Thought Q: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?A: There are 15 trees originally.Then there were 21 trees after the Grove workers planted some more.So there must have been 21 -15 = 6 trees that were planted.The answer is 6.</p>
<p>Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: There are originally 3 cars.Then 2 more cars arrive.Now 3 + 2 = 5 cars are in the parking lot.</p>
<p>The answer is 5.</p>
<p>Q Q: There were nine computers in the server room.Five more computers were installed each day, from monday to thursday.How many computers are now in the server room?A: There were originally 9 computers.For each day from monday to thursday, 5 more computers were installed.Progressive-Hint Prompting Chain-Of-Thought (PHP-CoT)</p>
<p>Q: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?(Hint: The answer is near to 6).A: We know the Answer Hints: 6.With the Answer Hints: 6, we will answer the question.There are 15 trees originally.Then there were 21 trees after the Grove workers planted some more.So there must have been 21 -15 = 6 trees that were planted.The answer is 6.Let's think step by step the simple interest for the first year is 10200 / 2 is rs .5100 and compound interest for first year also is rs .5100. the compound interest for second year on rs .5100 for one year so rate of the interest = ( 100 * 1530 ) / ( 5100 * 1 ) = 30% p so p = ( 100 * 10200 ) / ( 30 * 2 ) = rs .17000</p>
<p>The answer is (b), rs .17000</p>
<p>Figure 1 :
1
Figure1: Our proposed Progressive-Hint Prompting method combines the generated answers and questions for double-checking purposes, which is divided into two stages.In the first stage, we generate a base answer by passing to the LLM a concatenation of the current question and a base prompt, such as CoT or Complex CoT.In the second stage, we generate the subsequent answers via the corresponding progressive-hint prompt, such as Progressive-Hint Prompting CoT (PHP-CoT) or Progressive-Hint Prompting Complex CoT (PHP-Complex CoT), for the subsequent interaction.The interaction stops when two consecutive answers are the same.Purple Box: The input of LLM.Orange Box: The output of LLM.</p>
<p>Figure 3 :
3
Figure3: We show the results: 1) CoT of MultiArith; 2) CoT of SVAMP; 3) Complex CoT of GSM8K.According to 1) and 2), we can see that PHP could further improve performance.With result 3), we found that the PHP could even reduce the cost of self-consistency.</p>
<p>(a) 50 (b) 45 (c) 65 (d) 78 (e) 64 A: The answer is (a).Q: If a / b = 3/4 and 8a + 5b = 22,then find the value of a. Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2 A: The answer is (b).Q: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance?Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km A: The answer is (e).Q: How many keystrokes are needed to type the numbers from 1 to 500?Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788 A: The answer is (b).Progressive-Hint Prompting Standard (PHP-Standard) Q: John found that the average of 15 numbers is 40.If 10 is added to each number then the mean of the numbers is? (Hint: The answer is near to (a)).Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64 A: We know the Answer Hints: (a).With the Answer Hints: (a), we will answer the question.The answer is (a).Q: If a / b = 3/4 and 8a + 5b = 22,then find the value of a. (Hint: The answer is near to (b)).Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2 A: We know the Answer Hints: (b).With the Answer Hints: (b), we will answer the question.The answer is (b).Q: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance?(Hint: The answer is near to (d), (c)).Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km A: We the Answer Hints: (d), (c).With the Answer Hints: (d), (c), we will answer the question.The answer is (e).Q: How many keystrokes are needed to type the numbers from 1 to 500?(Hint: The answer is near to (a)).Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788 A: We know the Answer Hints: (a).With the Answer Hints: (a), we will answer the question.The answer is (b).Q: John found that the average of 15 numbers is 40.If 10 is added to each number then the mean of the numbers is? (Hint: The answer is near to (a), (d)).Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64 A: We know the Answer Hints: (a), (d).With the Answer Hints: (a), (d), we will answer the question.The answer is (a).Q: If a / b = 3/4 and 8a + 5b = 22,then find the value of a. (Hint: The answer is near to (a)).Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2 A: We know the Answer Hints: (a).With the Answer Hints: (a), we will answer the question.The answer is (b).Q: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance?(Hint: The answer is near to (b)).Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km A: We know the Answer Hints: (b).With the Answer Hints: (b), we will answer the question.The answer is (e).Q: How many keystrokes are needed to type the numbers from 1 to 500?(Hint: The answer is near to (b), (a)).Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788 A: We know the Answer Hints: (b), (a).With the Answer Hints: (b), (a), we will answer the question.The answer is (b).</p>
<p>So 4 * 5 = 20 computers were added.Now 9 + 20 = 29 computers are now in the server room.The answer is 29.Q: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?A: Michael started with 58 golf balls.He lost 23 on Tuesday, and lost 2 more on wednesday.So he had 58 -23 = 35 at the end of Tuesday, and 35 -2 = 33 at the end of wednesday.The answer is 33.Q: Olivia has $23.She bought five bagels for $3 each.How much money does she have left?A: Olivia had 23 dollars.She bought 5 bagels for 3 dollars each.So she spent 5 * 3 = 15 dollars.Now she has 23 -15 = 8 dollars left.The answer is 8.</p>
<p>Q:</p>
<p>If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? (Hint: The answer is near to 10, 8).A: We know the Answer Hints: 10, 8.With the Answer Hints: 10, 8, we will answer the question.There are originally 3 cars.Then 2 more cars arrive.Now 3 + 2 = 5 cars are in the parking lot.The answer is 5. Q: Leah had 32 chocolates and her sister had 42.If they ate 35, how many pieces do they have left in total?(Hint: The answer is near to 30, 35).A: We know the Answer Hints: 30, 35.With the Answer Hints: 30, 35, we will answer the question.then , we have : a + b + c = ( 45 x 3 ) = 135 . . .( i ) a + b = ( 40 x 2 ) = 80 . . .( ii ) b + c = ( 45 x 2 ) = 90 . . .( iii ) adding ( ii ) and ( iii ), we get : a + 2 b + c = 170 . . .( iv ) subtracting ( i ) from ( iv ), we get : b = 35.b' s weight = 35 kg .The answer is (d), 35 kg Question: the compound and the simple interests on a certain sum at the same rate of interest for two years are rs .11730 and rs .10200 respectively .find the sum (Hint: The answer is near to (e), (c)).Options: (a) rs .17037 (b) rs .17000 (c) rs .17276 (d) rs .170287 (e) rs .171881 A: We know the Answer Hints: (e), (c).With the Answer Hints: (e), (c), we will answer the question.</p>
<p>we combine
Base PromptingLLMBaseAnswerProgressive-Hint PromptingLLMSecondAnswerProgressive-Hint PromptingLLMThirdAnswerProgressive-Hint PromptingLLMFourthAnswer</p>
<p>Table 1 :
1
Illustration of Progressive-Hint Prompting.Blue Color: The difference between Base CoT and PHP-CoT.Red Color: The handcrafted Hint in the designed prompt.Base Prompting (e.g.CoT)</p>
<p>Table 2 :
2
PHP, when applied to different LLMs and prompting methods, can help to improve the performance.Meanwhile, PHP works better when the model and prompt are more powerful.The results are with greedy decoding.
PromptPHPDatasetAverageAddSub MultiArith SingleEQ SVAMP GSM8K AQuAStandard [8] 79.4 80.534.0 31.880.7 79.964.8 64.215.1 14.725.5 25.549.91 49.43(+1.1)(-2.2)(-0.8)(-0.6)(-0.4)(0.0)(-0.48)GPT-3.5 text-davinci-002CoT [8] 85.8 86.889.1 89.089.7 90.172.9 72.349.5 51.144.4 45.671.89 72.48(+1.0)(-0.1)(+0.4)(-0.6)(+1.6)(+1.2) (+0.59)Complex CoT [10] 82.5 83.789.8 90.187.7 89.970.4 74.657.6 61.237.4 37.070.89 72.75(+1.2)(+0.3)(+2.2)(+4.2)(+3.6)(-0.4)(+1.86)Standard [8] 89.1 89.136.3 36.083.8 83.668.7 68.715.9 16.028.3 28.353.68 53.61(0.0)(-0.3)(-0.2)(0.0)(+0.1)(0.0)(-0.07)GPT-3.5 text-davinci-003CoT [8] 90.6 91.193.6 94.092.7 93.581.0 81.356.1 57.544.0 44.476.33 76.96(+0.5)(+0.4)(+0.8)(+0.3)(+1.4)(+0.4) (+0.63)Complex CoT [10] 86.3 88.194.8 95.091.5 94.077.4 80.067.0 71.648.8 50.077.63 79.78(+1.8)(+0.2)(+2.5)(+2.6)(+4.6)(+1.2) (+2.15)A d d S u b M u lt iA ri th S in g le E Q S V A M P G S M 8 K A Q u A 2.00 2.02 2.04 2.06 2.08 2.10 2.12 Interaction Number Standard Text-Davinci-002 Text-Davinci-003A d d S u b M u lt iA ri th S in g le E Q S V A M P G S M 8 K A Q u A 2.0 2.2 2.4 2.6 2.8 3.0 3.2 CoT Text-Davinci-002 Text-Davinci-003A d d S u b M u lt iA ri th S in g le E Q S V A M P G S M 8 K A Q u A 2.0 2.2 2.4 2.6 2.8 3.0 3.2 Complex CoT Text-Davinci-002 Text-Davinci-003Figure 2: The Interaction Number refers to the frequency at which we need to consult the LLMuntil we receive conclusive responses. With an analysis of various models and prompts, it has beenobserved that: 1) A stronger model leads to a decreased interaction number; 2) An improved promptresults in an increased interaction number.</p>
<p>Table 3 :
3
Performance with different Base Answers.Initially, the base prompt provides base answers to the model and PHP generates the subsequent answers.The results are from text-davinci-003 with greedy decoding.
PHPBase PromptDatasetAverageAddSub MultiArith SingleEQ SVAMP GSM8K AQuAStandard [8]89.136.083.668.716.028.353.61PHP-StandardCoT [8]92.480.592.178.550.242.572.70Complex CoT [10]90.680.692.977.260.345.674.53Standard [8]90.892.590.780.252.340.974.56PHP-CoTCoT [8]91.194.093.581.357.544.476.96Complex CoT [10]90.696.893.781.262.650.079.14Standard [8]88.380.193.380.465.535.473.83PHP-Complex CoTCoT [8]88.895.694.881.470.645.679.46Complex CoT [10]88.195.094.080.071.650.079.78</p>
<p>Table 3
3
, to enhance the PHP-Standard, replacing the base prompt Standard with Complex CoT or CoT leads to a significant improvement in the final performance.For PHP-Standard, we observe that GSM8K performance amplifies from 16.0% with base prompt Standard to 50.2% with base prompt CoT and 60.3% with base prompt Complex CoT.Conversely, replacing the base prompt Complex CoT with Standard will reduce the final performance.For example, after replacing base prompt Complex CoT with Standard, the performance of PHP-Complex CoT drops from 71.6% to 65.5% on GSM8K dataset.Performance may further improve if PHP is not designed from the corresponding base prompt.The results indicate that the CoT with PHP-Complex CoT achieved a high accuracy rate of 96.8% on the MultiArith dataset, surpassing the performance of the CoT with PHP-CoT.Similarly, the Complex CoT with PHP-CoT demonstrated a notable accuracy rate of 95.6% on the same dataset, outperforming the Complex CoT with PHP-Complex CoT.The rationale behind these findings is twofold: 1) the performance of CoT and Complex CoT are similar on all six datasets, and 2) since the Base answer is provided by CoT (or Complex CoT)</p>
<p>Table 4 :
4
Ablation Study.CoT-Merge: for the CoT base prompt and the PHP-CoT prompt, we employ the prompt that contains both base prompt and the PHP.P1: We know the Answer Hints A 1 , ..., A p .P2: With the Answer Hints A 1 , ..., A p , we will answer the question.According to the experiment results, we see that both the proposed P1 and P2 are necessary.Meanwhile, non-merge based method is better than merge based method when prompts are more powerful.The results are from text-davinci-003 with greedy decoding.
MethodP1 P2DatasetAverageAddSub MultiArith SingleEQ SVAMP GSM8K AQuACoT-Merge 91.394.693.179.558.650.077.8591.193.593.380.058.144.876.80CoT [8]  90.8 91.393.1 93.892.9 93.580.7 80.558.8 58.243.7 46.476.66 77.28 91.194.093.581.357.544.476.96Complex CoT-Merge  88.894.394.678.170.246.878.8087.893.393.778.068.350.378.56Complex CoT [10]  87.8 88.395.1 94.394.2 94.678.5 79.170.5 69.348.4 46.879.08 78.73 88.195.094.080.071.650.079.78</p>
<p>Table 5 :
5
Analysis of Hint Design (Shown in Figure1).Correct: The hints of designed prompt are the same as the correct answers.Incorrect: The hints of the designed prompt are the incorrect answers.
MethodHintDatasetAverageCorrect Incorrect AddSub MultiArith SingleEQ SVAMP GSM8K AQuA90.693.692.781.056.144.076.33CoT [8]91.694.393.381.957.043.776.9691.193.593.179.757.945.276.7491.194.093.581.357.544.476.9686.394.891.577.467.048.877.63Complex CoT [10]88.394.093.877.868.646.478.1488.194.694.079.270.248.479.0888.195.094.080.071.650.079.78
Green: The performance is better than without progressive-hint.Red:The performance is worse than without progressive-hint.The results are from text-davinci-003 with greedy decoding.theanswer is obtained from the PHP-CoT.Notably, compared to other CoT-base methods, CoT-Merge achieves the best performance.However, compared to other Complex CoT-based methods, we observe that non-merge PHP-Complex CoT with both P1 and P2 achieves the best performance.</p>
<p>Table 6 :
6
The results after adding Self-Consistency (SC).Number: The interaction number between agent and LLM.The best results of adding PHP are highlighted with red color, and the best results without PHP are highlighted with green color.We find that PHP further improves performance, even adding self-consistency.Meanwhile, PHP may reduce the cost of self-consistency.
PromptSCPHPDatasetAverageAddSub MultiArith SingleEQ SVAMP GSM8K AQuA590.695.394.481.663.349.279.06590.896.694.883.566.349.680.265Number 2.00752.04332.00982.10902.54582.0157 2.12181090.696.593.883.065.549.279.761090.897.193.883.567.550.080.45CoT [8]10 Number 2.0075 20  91.12.0283 96.52.0059 94.22.0510 83.32.2145 68.02.0118 2.0531 55.1 81.362091.696.594.483.768.655.181.6420 Number 2.00502.03662.00982.02502.11442.0078 2.03304091.696.594.882.967.353.181.034091.696.695.083.768.453.181.3940 Number 2.00502.03002.00502.03202.05302.0000 2.0208588.197.093.180.473.551.580.60589.697.395.282.576.951.982.235Number 2.03782.01662.03342.23702.53902.0118 2.14591088.698.393.382.476.454.382.211089.198.595.283.478.254.783.18Complex CoT [10]10 Number 2.0177 20  88.62.0016 98.02.0295 93.82.059 82.52.1531 77.72.0078 2.0447 56.2 82.802089.898.095.883.678.656.283.6620 Number 2.02532.00002.01962.03302.04012.0000 2.01964088.398.594.883.978.158.683.704088.698.595.884.779.058.684.2040 Number 2.01012.00002.01372.02102.03482.0039 2.013795.2 95.5 95.8 96.0 96.2 96.5 Accuracy 96.8 97.05 #Sampled Reasoning Paths 1 0 2 0 4 0 CoT of MultiArith</p>
<p>Table 7 :
7
Performance of Complex CoT with GPT-3.5-Turbo and GPT-4, employing greedy decoding.Number: The average interaction number with LLM.
PHPDatasetAverageAddSub MultiArith SingleEQ SVAMPGSM8KAQuAPrevious SOTA94.9 [27]100 [25]95.5 [29] 89.1 [30] 92.0 [17] 76.4 [31]91.3185.597.592.581.082.857.482.78GPT-3.585.398.092.983.185.160.684.16Turbo(-0.2)(+0.5)(+0.4)(+2.1)(+2.3)(+3.2)(+1.38)Number2.10372.01332.06102.35702.34262.32282.200089.397.893.190.594.977.590.51GPT-489.6 (+0.3)98.1 (+0.3)93.1 (0.0)91.9 (+1.4)95.5 (+0.6)79.9 (+2.4)91.34 (+0.83)Number2.01262.00332.00192.07002.05072.29132.0716</p>
<dl>
<dt>Table 8 :</dt>
<dt>8</dt>
<dt>[14]ormance of Complex CoT with GPT-3.5-Turbo and GPT-4 on MATH dataset, employing greedy decoding.Number:The average interaction number with LLM.Overall: The results overall MATH subtopics[14].</dt>
<dt>PHPMATH DatasetInterAlgebra Precalculus Geometry NumTheory Probability PreAlgebra Algebra OverallPrevious SOTA[7]-------50.30GPT-4 CoT[17]-------42.50GPT-3.5-Turbo Complex CoT (Ours)  Number14.6 17.1 (+2.5) 4.274616.8 16.1 (-0.7) 3.962522.3 25.4 (+3.1) 4.336133.4 35.1 (+1.7) 3.816629.7 33.7 (+4.0) 3.759453.8 57.7 (+3.9) 3.152649.1 51.1 (+2.0) 3.071634.12 36.50 (+2.38) 3.6673GPT-4 Complex CoT (Ours)  Number23.4 26.3 (+2.9) 3.241426.7 29.8 (+3.1) 3.243536.5 41.9 (+5.4) 3.223349.6 55.7 (+6.1) 3.174053.1 56.3 (+3.2) 2.812271.6 73.8 (+2.2) 2.322670.8 74.3 (+3.5) 2.472650.36 53.90 (+3.54) 2.8494</dt>
<dd>
<p>A basket contains 25 oranges among which 1 is bad, 20% are unripe, 2 are sour and the rest are good.How many oranges are good?A: Let's think step by step There are 25 oranges in the basket, of which 1 is bad, 20% are unripe, and 2 are sour.: A basket contains 25 oranges among which 1 is bad, 20% are unripe, 2 are sour and the rest are good.How many oranges are good?(Hint: The answer is near to 2).A: We know the Answer Hints: 2. With the Answer Hints: 2, we will answer the question.Let's think step by step There are 25 oranges in the basket. 1 is bad, so there are 24 oranges left.20% of the oranges are unripe, so 20% of 24 = 4.8 oranges are unripe.2 oranges are sour, so there are 24-4.8-2= 17.2 oranges left.The rest of the oranges are good, so the number of good oranges is 17.2.The answer is 17.2.
This means that 1 + 20% + 2 = 23 oranges are not good.Therefore, 25 -23 = 2 oranges are good.The answer is 2.Question: A basket contains 25 oranges among which 1 is bad, 20% are unripe, 2 are sour and therest are good. How many oranges are good? (Hint: The answer is near to 2, 17.2).A: We know the Answer Hints: 2, 17.2. With the Answer Hints: 2, 17.2, we will answer the question.Let's think step by stepThere are 25 oranges in the basket.1 is bad, so there are 24 oranges left.20% of 24 oranges are unripe, so 20% of 24 = 4.8 oranges are unripe.2 oranges are sour, so there are 24-4.8-2 = 17.2 oranges left.These 17.2 oranges are good.The answer is 17.2.Question: If the probability that Stock A will increase in value during the next month is 0.56, and theprobability that Stock B will increase in value during the next month is 0.74. What is the greatestvalue for the probability that neither of these two events will occur? Answer Choices: (A) 0.22 (B)0.26 (C) 0.37 (D) 0.46 (E) 0.63A: Let's think step by stepThe probability that Stock A will not increase in value during the next month is 1 -0.56 = 0.44The probability that Stock B will not increase in value during the next month is 1 -0.74 = 0.26
Question</p>
</dd>
<dt>30,35)We know the Answer Hints: 10, 8.With the Answer Hints: 10, 8, we will answer the question.The answer is 5. Q: Leah had 32 chocolates and her sister had 42.If they ate 35, how many pieces do they have left in total?(Hint:The answer is near to30,35).A:We know the Answer Hints: 30, 35.With the Answer Hints: 30, 35, we will answer the question.The answer is 39.Q: Jason had 20 lollipops.He gave Denny some lollipops.Now Jason has 12 lollipops.How many lollipops did Jason give to Denny? (Hint: The answer near to 8, 12).A: We know the Answer Hints: 8, 12.With the Answer Hints: 8, 12, we will answer the question.The answer is 8.Q: Shawn has five toys.For Christmas, he got two toys each from his mom and dad.How many toys does he have now?(Hint: The answer is near to 9, 5).A: We know the Answer Hints: 9, 5.With the Answer Hints: 9, 5, we will answer the question.The answer is 9. John found that the average of 15 numbers is 40.If 10 is added to each number then the mean of the numbers is? Answer Choices:</dt>
<dt>11.2 For AQuABase StandardQ:</dt>
<dt>Q: There were nine computers in the server room.Five more computers were installed each day, from monday to thursday.How many computers are now in the server room?(Hint:The answer is near to 20).A: We know the Answer Hints: 20.With the Answer Hints: 20, we will answer the question.The answer is 29.Q: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?(Hint:The answer is near to 45).A: We know the Answer Hints: 45.With the Answer Hints: 45, we will answer the question.The answer is 33.Q: Olivia has $23.She bought five bagels for $3 each.How much money does she have left?(Hint: The answer is near to 8).A: We know the Answer Hints: 8.With the Answer Hints: 8, we will answer the question.The answer is 8.</dt>
<dd>
<p>Leah had 32 chocolates and her sister had 42.If they ate 35, how many pieces do they have left in total?A: Originally, Leah had 32 chocolates and her sister had 42.So in total they had 32 + 42 = 74.After eating 35, they had 74 -35 = 39 pieces left in total.The answer is 39.Q: Jason had 20 lollipops.He gave Denny some lollipops.Now Jason has 12 lollipops.How many lollipops did Jason give to Denny?A: Jason had 20 lollipops originally.Then he had 12 after giving some to Denny.So he gave Denny 20 -12 = 8 lollipops.The answer is 8.Q: Shawn has five toys.For Christmas, he got two toys each from his mom and dad.How many toys does he have now?A: Shawn started with 5 toys.He then got 2 toys each from his mom and dad.So he got 2 * 2 = 4 more toys.Now he has 5 + 4 = 9 toys.The answer is 9.</p>
</dd>
</dl>
<p>The difference between the two teams is 220 -75 = 145 seconds.The faster team will win by 145 seconds.The answer is 145 seconds.Question: Two track teams are competing against each other in a 4 by 400 meter relay; a race where each competing team has four members that each run 400 meters, or one lap, around a standard track.One of the two teams is very well-rounded and each of their members will run their 400 meter leg in precisely 55 seconds.The other team is less well-rounded; their first runner will run their 400 meter leg in 60 seconds then each subsequent runner will be 3 seconds faster than the previous runner.Using this information, how many seconds will the faster team win by? (Hint: The answer is near to -2, 145).A: We know the Answer Hints: -2, 145.With the Answer Hints: -2, 145, we will answer the question.Let's think step by step The first team will take 4 x 55 = 220 seconds to complete the race.The second team will take 60 + 57 + 54 + 51 = 222 seconds to complete the race.The first team will finish the race 2 seconds faster than the second team.The answer is -2.Question: Two track teams are competing against each other in a 4 by 400 meter relay; a race where each competing team has four members that each run 400 meters, or one lap, around a standard track.One of the two teams is very well-rounded and each of their members will run their 400 meter leg in precisely 55 seconds.The other team is less well-rounded; their first runner will run their 400 meter leg in 60 seconds then each subsequent runner will be 3 seconds faster than the previous runner.Using this information, how many seconds will the faster team win by? (Hint: The answer is near to -2, 145, -2).A: We know the Answer Hints: -2, 145, -2.With the Answer Hints: -2, 145, -2, we will answer the question.Let's think step by step The first team will run their 4 legs in 55 seconds each, so their total time will be 4 x 55 = 220 seconds.The second team's first runner will run their 400 meter leg in 60 seconds, then each subsequent runner will be 3 seconds faster than the previous runner.So the second runner will run in 57 seconds, the third runner in 54 seconds, and the fourth runner in 51 seconds.The total time for the second team will be 60 + 57 + 54 + 51 = 222 seconds.The difference in time between the two teams is 222 -220 = 2 seconds.The faster team will win by 2 seconds.The answer is 2.Question: Two track teams are competing against each other in a 4 by 400 meter relay; a race where each competing team has four members that each run 400 meters, or one lap, around a standard track.One of the two teams is very well-rounded and each of their members will run their 400 meter leg in precisely 55 seconds.The other team is less well-rounded; their first runner will run their 400 meter leg in 60 seconds then each subsequent runner will be 3 seconds faster than the previous runner.Using this information, how many seconds will the faster team win by? (Hint: The answer is near to -2, 145, -2, 2).A: We know the Answer Hints: -2, 145, -2, 2. With the Answer Hints: -2, 145, -2, 2, we will answer the question.Let's think step by step The first team will run their 4 legs in 55 seconds each, for a total of 4 x 55 = 220 seconds.The second team will run their 4 legs in 60, 57, 54, and 51 seconds, for a total of 60 + 57 + 54 + 51 = 222 seconds.The difference between the two teams is 222 -220 = 2 seconds.The faster team will win by 2 seconds.The answer is 2.Question: A shopkeeper in order to promote his new shop put a discount of 20% on all the items for one day.Now he must sell the items at original price the other day.By what percentage must he 13 Complex Chain-Of-Thought Prompt 13.1 For AddSub, MultiArith, SingleEQ, SVAMP and GSM8K Base Complex Chain-Of-Thought Question: Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week.They have 2 chapters of their textbook to study and 4 worksheets to memorize.They figure out that they should dedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet.If they plan to study no more than 4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break every hour, include 3 10-minute snack breaks each day, and 30 minutes for lunch each day?A: Let's think step by step.Angelo and Melanie think they should dedicate 3 hours to each of the 2 chapters, 3 hours x 2 chapters = 6 hours total.For the worksheets they plan to dedicate 1.5 hours for each worksheet, 1.5 hours x 4 worksheets = 6 hours total.Angelo and Melanie need to start with planning 12 hours to study, at 4 hours a day, 12 / 4 = 3 days.However, they need to include time for breaks and lunch.Every hour they want to include a 10-minute break, so 12 total hours x 10 minutes = 120 extra minutes for breaks.They also want to include 3 10-minute snack breaks, 3 x 10 minutes = 30 minutes.And they want to include 30 minutes for lunch each day, so 120 minutes for breaks + 30 minutes for snack breaks + 30 minutes for lunch = 180 minutes, or 180 / 60 minutes per hour = 3 extra hours.So Angelo and Melanie want to plan 12 hours to study + 3 hours of breaks = 15 hours total.They want to study no more than 4 hours each day, 15 hours / 4 hours each day = 3.75They will need to plan to study 4 days to allow for all the time they need.The answer is 4 Question: Mark's basketball team scores 25 2 pointers, 8 3 pointers and 10 free throws.Their opponents score double the 2 pointers but half the 3 pointers and free throws.What's the total number of points scored by both teams added together?A: Let's think step by step Mark's team scores 25 2 pointers, meaning they scored 25<em>2= 50 points in 2 pointers.His team also scores 6 3 pointers, meaning they scored 8</em>3= 24 points in 3 pointers They scored 10 free throws, and free throws count as one point so they scored 10<em>1=10 points in free throws.All together his team scored 50+24+10= 84 points Mark's opponents scored double his team's number of 2 pointers, meaning they scored 50</em>2=100 points in 2 pointers.His opponents scored half his team's number of 3 pointers, meaning they scored 24/2= 12 points in 3 pointers.They also scored half Mark's team's points in free throws, meaning they scored 10/2=5 points in free throws.All together Mark's opponents scored 100+12+5=117 points The total score for the game is both team's scores added together, so it is 84+117=201 points The answer is 201 Question: Bella has two times as many marbles as frisbees.She also has 20 more frisbees than deck cards.If she buys 2/5 times more of each item, what would be the total number of the items she will have if she currently has 60 marbles?A: Let's think step by step When Bella buys 2/5 times more marbles, she'll have increased the number of marbles by 2/5<em>60 = 24 The total number of marbles she'll have is 60+24 = 84 If Bella currently has 60 marbles, and she has two times as many marbles as frisbees, she has 60/2 = 30 frisbees.If Bella buys 2/5 times more frisbees, she'll have 2/5</em>30 = 12 more frisbees.The total number of frisbees she'll have will increase to 30+12 = 42 Bella also has 20 more frisbees than deck cards, meaning she has 30-20 = 10 deck cards If she buys 2/5 times more deck cards, she'll have 2/5<em>10 = 4 more deck cards.The total number of deck cards she'll have is 10+4 = 14 Together, Bella will have a total of 14+42+84 = 140 items The answer is 140 Question: A group of 4 fruit baskets contains 9 apples, 15 oranges, and 14 bananas in the first three baskets and 2 less of each fruit in the fourth basket.How many fruits are there?A: Let's think step by step For the first three baskets, the number of apples and oranges in one basket is 9+15=24 In total, together with bananas, the number of fruits in one basket is 24+14=38 for the first three baskets.Since there are three baskets each having 38 fruits, there are 3</em>38=114 fruits in the first three baskets.The number of apples in the fourth basket is 9-2=7 There are also 15-2=13 oranges in the fourth basket The combined number of oranges and apples in the fourth basket is 13+7=20 The fourth basket also contains 14-2=12 bananas.In total, the fourth basket has 20+12=32 fruits.The four baskets together have 32+114=146 fruits.The answer is 146 Question: You can buy 4 apples or 1 watermelon for the same price.You bought 36 fruits evenly split between oranges, apples and watermelons, and the price of 1 orange is $0.50.How much does 1 apple cost if your total bill was $66? A: Let's think step by step If 36 fruits were evenly split between 3 types of fruits, then I bought 36/3 = 12 units of each fruit If 1 orange costs $0.50 then 12 oranges will cost $0.50<em> 12 = $6 If my total bill was $66 and I spent $6 on oranges then I spent $66 -6 =60 on the other 2 fruit types.Assuming the price of watermelon is W, and knowing that you can buy 4 apples for the same price and that the price of one apple is A, then 1W=4A If we know we bought 12 watermelons and 12 apples for $60, then we know that $60 = 12W + 12A Knowing that 1W=4A, then we can convert the above to $60 = 12(4A) + 12A 60 = 48A + 12A 60 = 60A Then we know the price of one apple (A) is $60/60= $1 The answer is 1 Question: Susy goes to a large school with 800 students, while Sarah goes to a smaller school with only 300 students.At the start of the school year, Susy had 100 social media followers.She gained 40 new followers in the first week of the school year, half that in the second week, and half of that in the third week.Sarah only had 50 social media followers at the start of the year, but she gained 90 new followers the first week, a third of that in the second week, and a third of that in the third week.After three weeks, how many social media followers did the girl with the most total followers have?A: Let's think step by step After one week, Susy has 100+40 = 140 followers.In the second week, Susy gains 40/2 = 20 new followers.In the third week, Susy gains 20/2 = 10 new followers.In total, Susy finishes the three weeks with 140+20+10 = 170 total followers.After one week, Sarah has 50+90 = 140 followers.After the second week, Sarah gains 90/3 = 30 followers.After the third week, Sarah gains 30/3 = 10 followers.So, Sarah finishes the three weeks with 140+30+10 = 180 total followers.Thus, Sarah is the girl with the most total followers with a total of 180.The answer is 180 Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box.He rearranged five of these boxes into packages of six highlighters each and sold them for $3 per package.He sold the rest of the highlighters separately at the rate of three pens for $2. How much profit did he make in total, in dollars?A: Let's think step by step Sam bought 12 boxes x 10 =120 worth of highlighters.He bought 12 * 30 = 360 highlighters in total.Sam then took 5 boxes  6 highlighters/box = 30 highlighters.He sold these boxes for 5 * $3 = $15 After selling these 5 boxes there were 360 -30 = 330 highlighters remaining.These form 330 / 3 = 110 groups of three pens.He sold each of these groups for $2 each, so made 110 * 2 = $220 from them.In total, then, he earned $220 + $15 = $235.Since his original cost was $120, he earned $235 -$120 = $115 in profit.The answer is 115 Question: In a certain school, 2/3 of the male students like to play basketball, but only 1/5 of the female students like to play basketball.What percent of the population of the school do not like to play basketball if the ratio of the male to female students is 3:2 and there are 1000 students?A: Let's think step by step The students are divided into 3 + 2 = 5 parts where 3 parts are for males and 2 parts are for females.Each part represents 1000/5 = 200 students.So, there are 3 x 200 = 600 males.And there are 2 x 200 = 400 females.Hence, 600 x 2/3 = 400 males play basketball.And 400 x 1/5 = 80 females play basketball.A total of 400 + 80 = 480 students play basketball.Therefore, 1000 -480 = 520 do not like to play basketball.The percentage of the school that do not like to play basketball is 520/1000 * 100 = 52 The answer is 52Progressive-Hint Prompting Complex Chain-Of-Thought (PHP-Complex CoT) Question: Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week.They have 2 chapters of their textbook to study and 4 worksheets to memorize.They figure out that they should dedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet.If they plan to study no more than 4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break every hour, include 3 10-minute snack breaks each day, and 30 minutes for lunch each day?(Hint: The answer is near to 4).A: We know the Answer Hints: 4. With the Answer Hints: 4, we will answer the question.Let's think step by step.Angelo and Melanie think they should dedicate 3 hours to each of the 2 chapters, 3 hours x 2 chapters = 6 hours total.For the worksheets they plan to dedicate 1.5 hours for each worksheet, 1.5 hours x 4 worksheets = 6 hours total.Angelo and Melanie need to start with planning 12 hours to study, at 4 hours a day, 12 / 4 = 3 days.However, they need to include time for breaks and lunch.Every hour they want to include a 10-minute break, so 12 total hours x 10 minutes = 120 extra minutes for breaks.They also want to include 3 10-minute snack breaks, 3 x 10 minutes = 30 minutes.And they want to include 30 minutes for lunch each day, so 120 minutes for breaks + 30 minutes for snack breaks + 30 minutes for lunch = 180 minutes, or 180 / 60 minutes per hour = 3 extra hours.So Angelo and Melanie want to plan 12 hours to study + 3 hours of breaks = 15 hours total.They want to study no more than 4 hours each day, 15 hours / 4 hours each day = 3.75They will need to plan to study 4 days to allow for all the time they need.The answer is 4 Question: Mark's basketball team scores 25 2 pointers, 8 3 pointers and 10 free throws.Their opponents score double the 2 pointers but half the 3 pointers and free throws.What's the total number of points scored by both teams added together?(Hint:The answer is near to 201).A: We know the Answer Hints: 201.With the Answer Hints: 201, we will answer the question.Let's think step by step Mark's team scores 25 2 pointers, meaning they scored 25</em>2= 50 points in 2 pointers.His team also scores 6 3 pointers, meaning they scored 8<em>3= 24 points in 3 pointers They scored 10 free throws, and free throws count as one point so they scored 10</em>1=10 points in free throws.All together his team scored 50+24+10= 84 points Mark's opponents scored double his team's number of 2 pointers, meaning they scored 50<em>2=100 points in 2 pointers.His opponents scored half his team's number of 3 pointers, meaning they scored 24/2= 12 points in 3 pointers.They also scored half Mark's team's points in free throws, meaning they scored 10/2=5 points in free throws All together Mark's opponents scored 100+12+5=117 points The total score for the game is both team's scores added together, so it is 84+117=201 points The answer is 201 Question: Bella has two times as many marbles as frisbees.She also has 20 more frisbees than deck cards.If she buys 2/5 times more of each item, what would be the total number of the items she will have if she currently has 60 marbles?(Hint: The answer is near to 160,145).A: We know the Answer Hints: 160, 145.With the Answer Hints: 160, 145, we will answer the question.Let's think step by step When Bella buys 2/5 times more marbles, she'll have increased the number of marbles by 2/5</em>60 = 24 The total number of marbles she'll have is 60+24 = 84 If Bella currently has 60 marbles, and she has two times as many marbles as frisbees, she has 60/2 = 30 frisbees.If Bella buys 2/5 times more frisbees, she'll have 2/5<em>30 = 12 more frisbees.The total number of frisbees she'll have will increase to 30+12 = 42 Bella also has 20 more frisbees than deck cards, meaning she has 30-20 = 10 deck cards If she buys 2/5 times more deck cards, she'll have 2/5</em>10 = 4 more deck cards.The total number of deck cards she'll have is 10+4 = 14 Together, Bella will have a total of 14+42+84 = 140 items The answer is 140 Question: A group of 4 fruit baskets contains 9 apples, 15 oranges, and 14 bananas in the first three baskets and 2 less of each fruit in the fourth basket.How many fruits are there?(Hint: The answer is near to 93, 141).A: We know the Answer Hints: 93, 141.With the Answer Hints: 93, 141, we will answer the question.Let's think step by step For the first three baskets, the number of apples and oranges in one basket is 9+15=24 In total, together with bananas, the number of fruits in one basket is 24+14=38 for the first three baskets.Since there are three baskets each having 38 fruits, there are 3<em>38=114 fruits in the first three baskets.The number of apples in the fourth basket is 9-2=7 There are also 15-2=13 oranges in the fourth basket The combined number of oranges and apples in the fourth basket is 13+7=20 The fourth basket also contains 14-2=12 bananas.In total, the fourth basket has 20+12=32 fruits.The four baskets together have 32+114=146 fruits.The answer is 146 Question: You can buy 4 apples or 1 watermelon for the same price.You bought 36 fruits evenly split between oranges, apples and watermelons, and the price of 1 orange is $0.50.How much does 1 apple cost if your total bill was $66? (Hint: The answer is near to 1, 5).A: We know the Answer Hints: 1, 5. With the Answer Hints: 1, 5, we will answer the question.Let's think step by step If 36 fruits were evenly split between 3 types of fruits, then I bought 36/3 = 12 units of each fruit If 1 orange costs $0.50 then 12 oranges will cost $0.50</em> 12 = $6 If my total bill was $66 and I spent $6 on oranges then I spent $66 -$6 = $60 on the other 2 fruit types.Assuming the price of watermelon is W, and knowing that you can buy 4 apples for the same price and that the price of one apple is A, then 1W=4A If we know we bought 12 watermelons and 12 apples for $60, then we know that $60 = 12W + 12A Knowing that 1W=4A, then we can convert the above to $60 = 12(4A) + 12A $60 = 48A + 12A $60 = 60A Then we know the price of one apple (A) is $60/60= $1 The answer is 1 Question: Susy goes to a large school with 800 students, while Sarah goes to a smaller school with only 300 students.At the start of the school year, Susy had 100 social media followers.She gained 40 new followers in the first week of the school year, half that in the second week, and half of that in the third week.Sarah only had 50 social media followers at the start of the year, but she gained 90 new followers the first week, a third of that in the second week, and a third of that in the third week.After three weeks, how many social media followers did the girl with the most total followers have?(Hint: The answer is near to 180, 160).A: We know the Answer Hints: 180, 160.With the Answer Hints: 180, 160, we will answer the question.Let's think step by step After one week, Susy has 100+40 = 140 followers.In the second week, Susy gains 40/2 = 20 new followers.In the third week, Susy gains 20/2 = 10 new followers.In total, Susy finishes the three weeks with 140+20+10 = 170 total followers.After one week, Sarah has 50+90 = 140 followers.After the second week, Sarah gains 90/3 = 30 followers.After the third week, Sarah gains 30/3 = 10 followers.So, Sarah finishes the three weeks with 140+30+10 = 180 total followers.Thus, Sarah is the girl with the most total followers with a total of 180.The answer is 180 Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box.He rearranged five of these boxes into packages of six highlighters each and sold them for $3 per package.He sold the rest of the highlighters separately at the rate of three pens for $2. How much profit did he make in total, in dollars?(Hint: The answer is near to 120).A: We know the Answer Hints: 120.With the Answer Hints: 120, we will answer the question.Let's think step by step Sam bought 12 boxes x $10 = $120 worth of highlighters.He bought 12 * 30 = 360 highlighters in total.Sam then took 5 boxes  6 highlighters/box = 30 highlighters.He sold these boxes for 5 * $3 = $15 After selling these 5 boxes there were 360 -30 = 330 highlighters remaining.These form 330 / 3 = 110 groups of three pens.He sold each of these groups for $2 each, so made 110 * 2 = $220 from them.In total, then, he earned $220 + $15 = $235.Since his original cost was $120, he earned $235 -$120 = $115 in profit.The answer is 115 Question: In a certain school, 2/3 of the male students like to play basketball, but only 1/5 of the female students like to play basketball.What percent of the population of the school do not like to play basketball if the ratio of the male to female students is 3:2 and there are 1000 students?(Hint: The answer is near to 45).A: We know the Answer Hints: 45.With the Answer Hints: 45, we will answer the question.Let's think step by step The students are divided into 3 + 2 = 5 parts where 3 parts are for males and 2 parts are for females.Each part represents 1000/5 = 200 students.So, there are 3 x 200 = 600 males.And there are 2 x 200 = 400 females.Hence, 600 x 2/3 = 400 males play basketball.And 400 x 1/5 = 80 females play basketball.A total of 400 + 80 = 480 students play basketball.Therefore, 1000 -480 = 520 do not like to play basketball.The percentage of the school that do not like to play basketball is 520/1000 * 100 = 52 The answer is 52 A: Let's think step by step We know x &lt; y &lt; z to find the least possible value for z -x, we need to find the values for z and x that can be closest to each other.if x is some even number, then what could be minimum possible odd z. if x is some even number, y -x &gt; 5 ; y &gt; x + 5 minimum value for y = x + 5 + 2 = x + 7 (note : x + 5 is as even + odd = odd and nearest odd greater than x + 5 is x + 5 + 2) minimum value for z = y + 2 = x + 7 + 2 = x + 9 (note : z = y + 2 because both z and y are odd, difference between two odd numbers is 2) s = z -x = x + 9 -x = 9The answer is (d), 9Question: what is the difference between the c .i .on rs .6000 for 1 1 / 2 years at 4 % per annum compounded yearly and half -yearly ?Options: (a) s .Let's think step by step We know x &lt; y &lt; z to find the least possible value for z -x, we need to find the values for z and x that can be closest to each other.if x is some even number, then what could be minimum possible odd z. if x is some even number, y -x &gt; 5 ; y &gt; x + 5 minimum value for y = x + 5 + 2 = x + 7 (note : x + 5 is as even + odd = odd and nearest odd greater than x + 5 is x + 5 + 2) minimum value for z = y + 2 = x + 7 + 2 = x + 9 (note : z = y + 2 because both z and y are odd, difference between two odd numbers is 2) s = z -x = x + 9 -x = 9The answer is (d), 9Question: what is the difference between the c .i .on rs .6000 for 1 1 / 2 years at 4 % per annum compounded yearly and half -yearly ?(Hint: The answer is near to (e), (d)).Options: (a) s .
A survey of the usages of deep learning for natural language processing. Julian R Daniel W Otter, Medina, Kalita, IEEE transactions on neural networks and learning systems. 202032</p>
<p>Pre-trained models for natural language processing: A survey. Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, Xuanjing Huang, Science China Technological Sciences. 63102020</p>
<p>Natural language processing. Fundamentals of artificial intelligence. Chowdhary, Chowdhary, 2020</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.114462021arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adri Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. 202213</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, arXiv:2206.14858202219arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032022. 1, 3, 4, 5, 6, 7, 8arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, Ed Chi, The Eleventh International Conference on Learning Representations. 202313</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, The Eleventh International Conference on Learning Representations. 2023. 1, 3, 4, 5, 6, 7, 8</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies202124</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168202134arXiv preprint</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics201714</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.03874202129arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 2020334</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023. 2, 4, 9arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, arXiv:2010.159802020arXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022arXiv preprint</p>
<p>Exploiting reasoning chains for multi-hop science question answering. Weiwen Xu, Yang Deng, Huihui Zhang, Deng Cai, Wai Lam, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Learning to retrieve reasoning paths over wikipedia graph for question answering. Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, Caiming Xiong, International Conference on Learning Representations. 2020</p>
<p>Multi-hop question answering via reasoning chains. Jifan Chen, Shih-Ting Lin, Greg Durrett, arXiv:1910.026102019arXiv preprint</p>
<p>Selfconsistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, The Eleventh International Conference on Learning Representations. 202339</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, EMNLP. 2014</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsSeptember 201549</p>
<p>Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Transactions of the Association for Computational Linguistics. 342015</p>
<p>Active prompting with chain-ofthought for large language models. Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang, arXiv:2302.122462023arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Boosted prompt ensembles for large language models. Silviu Pitis, Andrew Michael R Zhang, Jimmy Wang, Ba, arXiv:2304.059702023arXiv preprint</p>
<p>Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, Julius Berner, arXiv:2301.13867Mathematical capabilities of chatgpt. 2023arXiv preprint</p>
<p>Autoformalization with large language models. Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Norman Rabe, Charles E Staats, Mateja Jamnik, Christian Szegedy, Advances in Neural Information Processing Systems. 2022</p>
<p>Thor: Wielding hammers to integrate language models and automated theorem provers. Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzygd, Piotr Mio, Yuhuai Wu, Mateja Jamnik, Advances in Neural Information Processing Systems. 2022</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. Sean Albert Q Jiang, Jin Peng Welleck, Wenda Zhou, Jiacheng Li, Mateja Liu, Timothe Jamnik, Yuhuai Lacroix, Guillaume Wu, Lample, arXiv:2210.122832022arXiv preprint</p>
<p>Stanislas Polu, Jesse Michael Han, Kunhao Zheng, arXiv:2202.01344Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. 2022arXiv preprint</p>
<p>Towards a human-like open-domain chatbot. Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, arXiv:2001.099772020arXiv preprint</p>
<p>Hypertree proof search for neural theorem proving. Guillaume Lample, Marie-Anne Lachaux, Thibaut Lavril, Xavier Martinet, Amaury Hayat, Gabriel Ebner, Aurlien Rodriguez, Timothe Lacroix, arXiv:2205.114912022arXiv preprint</p>
<p>Pushing the limits of rule reasoning in transformers through natural language satisfiability. Kyle Richardson, Ashish Sabharwal, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Learning an executable neural semantic parser. Jianpeng Cheng, Siva Reddy, Vijay Saraswat, Mirella Lapata, Computational Linguistics. 4512019</p>
<p>Learning to retrieve prompts for incontext learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, arXiv:2210.030572022arXiv preprint</p>
<p>A learning algorithm for boltzmann machines. Geoffrey E David H Ackley, Terrence J Hinton, Sejnowski, Cognitive science. 911985</p>
<p>Learning to retrieve reasoning paths over wikipedia graph for question answering. Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, Caiming Xiong, arXiv:1911.104702019arXiv preprint</p>
<p>Measuring and improving consistency in pretrained language models. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schtze, Yoav Goldberg, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Language models for dialog applications. 2022arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothe Lachaux, Baptiste Lacroix, Naman Rozire, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Selective annotation makes language models better few-shot learners. Jungo Su Hongjin, Chen Henry Kasai, Weijia Wu, Tianlu Shi, Jiayi Wang, Rui Xin, Mari Zhang, Luke Ostendorf, Noah A Zettlemoyer, Smith, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, International Conference on Machine Learning. PMLR2021</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, arXiv:2111.020802021arXiv preprint</p>
<p>Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning. Colin Wei, Sang Michael Xie, Tengyu Ma, Advances in Neural Information Processing Systems. 342021</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2207.00747Rationaleaugmented ensembles in language models. 2022arXiv preprint</p>
<p>What makes reading comprehension questions easier?. Saku Sugawara, Kentaro Inui, Satoshi Sekine, Akiko Aizawa, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Sentence-bert: Sentence embeddings using siamese bertnetworks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>What makes good in-context examples for gpt-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, Weizhu Chen, The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. DeeLIO 2022. 2022Proceedings of Deep Learning Inside Out</p>
<p>On the advance of making language models better reasoners. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, arXiv:2206.023362022arXiv preprint</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Internetaugmented language models through few-shot prompting for open-domain question answering. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev, arXiv:2203.051152022arXiv preprint</p>
<p>Why machine reading comprehension models learn shortcuts?. Yuxuan Lai, Chen Zhang, Yansong Feng, Quzhe Huang, Dongyan Zhao, arXiv:2106.010242021arXiv preprint</p>
<p>A: We know the Answer Hints: 20. With the Answer Hints: 20, we will answer the question. There were originally 9 computers. For each day from monday to thursday, 5 more computers were installed. So 4 * 5 = 20 computers were added. Now 9 + 20 = 29 computers are now in the server room. The answer is 29. Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?. Originally, A: We know the Answer Hints: 45. With the Answer Hints: 45, we will answer the question. Michael started with 58 golf balls. He lost 23 on Tuesday, and lost 2 more on wednesday. With the Answer Hints: 8, 12, we will answer the question. Jason had 20 lollipops originally. Then he had 12 after giving some to Denny. So he gave Denny 20 -12 = 8 lollipops. The answer is 8. Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? (Hint: The answer is near to 9, 5). Hint: The answer is near to 45. So he had 58 -23 = 35 at the end of Tuesday, and 35 -2 = 33 at the end of wednesday. The answer is 33. Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left? (Hint: The answer is near to 8</p>
<p>A: We know the Answer Hints: 8. With the Answer Hints: 8, we will answer the question. Olivia had 23 dollars. She bought 5 bagels for 3 dollars each. So she spent 5 * 3 = 15 dollars. Now she has 23 -15 = 8 dollars left. The answer is. 8</p>
<p>For AQuA Base Chain-Of-Thought Q: John found that the average of 15 numbers is 40. 64If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e)</p>
<p>A: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be 50. The answer is (a</p>
<p>A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance? Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km A: The distance that the person traveled would have been 20 km/hr * 2.5 hrs = 50 km. 1788The answer is (e). Q: How many keystrokes are needed to type the numbers from 1 to 500? Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e)</p>
<p>There are 90 two-digit numbers from 10 to 99. There are 401 three-digit numbers from 100 to 500. A: There are 9 one-digit numbers from 1 to 9. PHP-CoT9) = 1392. The answer is (b)</p>
<p>John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? (Hint: The answer is near to (a)</p>
<p>Answer Choices: (a) 50 (b) 45 (c) 65 (d). 64</p>
<p>With the Answer Hints: (a), we will answer the question. If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be 50. A: We know the Answer Hints: (a). The answer is (a</p>
<p>If a / b = 3/4 and 8a + 5b = 22,then find the value of a. Hint: The answer is near to (b)</p>
<p>A: We know the Answer Hints: (b). With the Answer Hints: (b). we will answer the question. a / b = 3/4, then b = 4a / 3. So 8a + 5(4a / 3) = 22. This simplifies to 8a + 20a / 3 = 22, which means 44a / 3</p>
<p>A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance?. Hint: The answer is near to (d), (c)</p>
<p>With the Answer Hints: (d), (c), we will answer the question. The distance that the person traveled would have been 20 km/hr * 2.5 hrs = 50 km. 1788Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km A: We know the Answer Hints: (d), (c). The answer is (e). Q: How many keystrokes are needed to type the numbers from 1 to 500? (Hint: The answer is near to (a)). Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e)</p>
<p>There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401 three-digit numbers from 100 to 500. A: We know the Answer Hints: (a). 9) = 1392. The answer is (b). Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? (Hint: The answer is near to (a), (d)</p>
<p>Answer Choices: (a) 50 (b) 45 (c) 65 (d). 64</p>
<p>With the Answer Hints: (a), (d), we will answer the question. If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be 50. A: We know the Answer Hints: (a). (d). The answer is (a</p>
<p>If a / b = 3/4 and 8a + 5b = 22,then find the value of a. Hint: The answer is near to (a)</p>
<p>A: We know the Answer Hints. With the Answer Hints: (a). we will answer the question. a / b = 3/4, then b = 4a / 3. So 8a + 5(4a / 3) = 22. This simplifies to 8a + 20a / 3 = 22, which means 44a / 3</p>
<p>A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance?. Hint: The answer is near to (b)</p>
<p>With the Answer Hints: (b), we will answer the question. The distance that the person traveled would have been 20 km/hr * 2.5 hrs = 50 km. Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km A: We know the Answer Hints: (b). The answer is (e). Q: How many keystrokes are needed to type the numbers from 1 to 500? (Hint: The answer is near to (b), (a)</p>
<p>Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d). 1788</p>
<p>With the Answer Hints: (b), (a), we will answer the question. There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401 three-digit numbers from 100 to 500. A: We know the Answer Hints: (b). 93) = 1392. The answer is (b</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1964 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1964</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1964</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-281505303</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.20021v1.pdf" target="_blank">Embodied AI: From LLMs to World Models</a></p>
                <p><strong>Paper Abstract:</strong> Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving the evolution from cyberspace to physical systems. Recent breakthroughs in Large Language Models (LLMs) and World Models (WMs) have drawn significant attention for embodied AI. On the one hand, LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition. On the other hand, WMs empower embodied AI by building internal representations and future predictions of the external world, facilitating physical law-compliant embodied interactions. As such, this paper comprehensively explores the literature in embodied AI from basics to advances, covering both LLM driven and WM driven works. In particular, we first present the history, key technologies, key components, and hardware systems of embodied AI, as well as discuss its development via looking from unimodal to multimodal angle. We then scrutinize the two burgeoning fields of embodied AI, i.e., embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs, meticulously delineating their indispensable roles in end-to-end embodied cognition and physical laws-driven embodied interactions. Building upon the above advances, we further share our insights on the necessity of the joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. In addition, we examine representative applications of embodied AI, demonstrating its wide applicability in real-world scenarios. Last but not least, we point out future research directions of embodied AI that deserve further investigation.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1964.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1964.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Do as I can, not as I say: Grounding language in robotic affordances</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that constrains LLM planning by a pretrained natural-language action library and evaluates candidate language-action sequences with value functions to ground high-level instructions into feasible robot actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do as i can, not as i say: Grounding language in robotic affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SayCan (affordance-grounded LLM planner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses a pretrained LLM to convert natural-language instructions into natural-language action sequences constrained by a real-world pretrained natural language action library; feasibility of proposed actions is evaluated by value functions (affordance scoring) conditioned on the current environment state.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Constrains LLM proposals with a pretrained natural-language action library and scores actions with value functions (affordance-based grounding rather than pixel-level cross-attention).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>action-level / language-action library (high-level symbolic/affordance representations)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>none/implicit (affordance-based grounding; no explicit spatial 3D grounding described in this paper's summary)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>instruction following / manipulation / service robotics</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Limited by a fixed natural language action library and specific physical environment; difficulty adapting to new robots/environments (semantic grounding constrained to prebuilt affordance set).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Paper notes failure modes such as infeasible plans proposed by LLMs if not constrained by affordance library; no quantitative frequencies reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Handles some environment-specific feasibility via value functions, but overall limited adaptability across different robot embodiments and environments.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>LLM outputs filtered/scored by affordance/value models (language-level constraint and verification loop)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Affordance-constrained LLM planning improves feasibility of high-level instruction-to-action mapping, but reliance on a fixed action library limits adaptability and real-world generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1964.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1964.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E: An embodied multimodal language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language model that integrates continuous real-world sensor encodings with a pretrained large language model to map percepts to words and enable multitask embodied control within a fixed action space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Palm-e: An embodied multimodal language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-E (VLM for embodied control)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Trains visual and language encodings end-to-end in conjunction with a pretrained LLM, incorporates continuous real-world sensor modality encodings into the VLM, and links words to percepts to perform multi-task embodied control via a fixed action-space mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Multimodal encoding pipeline that injects continuous sensor (visual, proprioceptive) representations into an LLM backbone to ground language tokens to perceptual features and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>multimodal token-level representations (language-aligned perceptual encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit via continuous sensor encodings; explicit spatial detail not specified in review</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>embodied manipulation, instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world robot sensors / continuous sensor modalities (as described)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>MLLMs like PaLM-E can struggle to enforce physics-compliant dynamics and real-time adaptation; review flags grounding to physical constraints as a limitation of MLLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Qualitative failure modes noted: MLLMs may propose physically implausible actions (e.g., ignoring friction/material properties) and have poor responsiveness to rapid environmental feedback; no numeric breakdown provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Not specified; general limitation cited that MLLMs rely on pretraining and struggle with continuous physical adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Early multimodal fusion into LLM context (injecting sensor encodings into LLM input representation)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>End-to-end injection of continuous sensor encodings into LLMs enables language-aligned perception-to-action mapping, but LLM-driven grounding lacks built-in physics compliance and fast adaptation to environmental feedback.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1964.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1964.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2: Visionlanguage-action models transfer web knowledge to robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action (VLA) model that tokenizes multimodal robot state (image, language, actions) and uses an LLM to generate tokens which are detokenized into low-level robot actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-2: Visionlanguage-action models transfer web knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2 (Vision-Language-Action model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encodes the robot's current image, language instruction, and robot action at a timestep into token sequences; an LLM performs semantic reasoning and task decomposition on these tokens; generated tokens are detokenized into final continuous or discrete actions.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Tokenization of multimodal inputs (images and action/state) into a language-like token stream processed by an LLM; grounding occurs by mapping LLM-generated tokens back to executable actions (detokenization).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>timestep-level multimodal tokens (sequential language-like representation of perception and action)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit in encoded image/state tokens; explicit spatial maps not described in review</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>embodied manipulation / low-level control</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>robot onboard camera / real-world imagery (as described)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Review highlights MLLMs/VLAs (including RT-2) can have poor real-time adaptation and may not enforce physics constraints without additional mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Qualitative: potential latency/detokenization mismatch and physics-violating proposals; no numerical failure analysis in review.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Not detailed; general limitations of LLM-based VLAs in adapting to new environments noted.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Sequential token-level fusion (images + actions + language → tokens → LLM processing → detokenized actions)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Tokenized multimodal pipelines enable direct mapping from perception and instruction to actions, but grounding depends on correct detokenization and suffers from latency and physics-mismatch issues without world-model constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1964.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1964.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PerAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PerAct (3D voxel-based manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manipulation approach that uses 3D voxel representations to achieve precise action outputs, reported to reach millimeter-level grasp accuracy in review's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PerAct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Employs 3D voxelized scene representations to map visual 3D context directly to manipulation actions, enabling fine-grained spatial grounding for dexterous manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Direct mapping from 3D voxel representation to action outputs (spatially precise vision-to-action grounding in voxel space).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>3D voxel-level (geometric, spatially explicit)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>explicit 3D voxel grid / coordinates (depth-aware spatial grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>dexterous manipulation / grasping</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>robotic manipulation (likely real-world or physics-simulated 3D scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Voxel-based grounding helps precision but review notes broader challenges: MLLMs may still ignore physical properties; WMs required for physics-aware validation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>No quantitative failure breakdown provided; review emphasizes need for physics-aware validation to avoid failure in contact-rich tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Spatial (3D) perception directly used to predict actions; specific multimodal fusion not detailed in review.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>3D voxel representations enable very precise spatial grounding for manipulation, but success still depends on integrating physical dynamics (world models) for robust, generalizable behavior.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1964.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1964.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP-based grounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP-based vision-language alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Contrastive image-text embedding models (CLIP-style) used to align visual scenes and linguistic cues, reducing ambiguity in object recognition and enabling open-vocabulary grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP-based alignment (contrastive V+L)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses joint image-text contrastive pretraining to produce shared embeddings for images and text; these embeddings are used to align visual observations with language queries, enabling region-level or scene-level grounding via nearest-neighbor or cross-modal retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>CLIP visual encoder (backbone unspecified in review)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained on large-scale image-text pairs (web-scale contrastive pretraining as in CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Contrastive alignment of image and text embeddings (embedding similarity used to ground language to visual content), often used as retrieval or grounding signal.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene-level / region-level embeddings (open-vocabulary grounding possible)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>typically implicit (embedding-based); can be combined with region proposals for region-level grounding</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>active perception / 3D scene understanding / grounding for task planning</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>2D camera images, sometimes combined with 3D point clouds (as reported for 3D scene methods leveraging vision-language embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>CLIP-style embeddings reduce semantic ambiguity but may not encode physical dynamics or fine-grained spatial relations needed for manipulation; novel object recognition depends on training coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Qualitative: ambiguity in cluttered scenes and lack of physics/dynamics modeling cause grounding errors; no numerical frequencies provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Often limited; review suggests additional fine-tuning or combination with WMs needed for sim-to-real or domain shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Improvements attributed to web-scale pretraining, but no quantitative scaling law reported in review.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Embedding similarity / retrieval (contrastive fusion); can be combined downstream with attention-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Contrastive CLIP-style alignment is effective for open-vocabulary semantic grounding and reducing label dependence, but insufficient alone for physics-aware control and real-time adaptation in embodied tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1964.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1964.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEO-SLAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SEO-SLAM (MLLM-enhanced SLAM labeling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An MLLM-augmented SLAM approach that uses multimodal LLM capabilities to generate more descriptive object labels and dynamically correct detection biases during SLAM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SEO-SLAM (MLLM-enhanced SLAM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Integrates MLLMs into SLAM pipelines to produce richer semantic labels for observed objects and maintain a multiclass confusion matrix to mitigate detection bias during map construction.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Semantic grounding via MLLM-generated labels aligned to observed visual entities, improving SLAM's semantic mapping by mapping words to percepts.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>object-level semantic labels integrated into SLAM maps</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>explicit spatial mapping via SLAM (pose, map) enriched with semantic labels</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>visual SLAM / active perception</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>robotic RGB-D or camera-driven SLAM environments (static and dynamic scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>MLLM-enhanced semantic labeling helps dynamic scene robustness but SLAM still faces geometric vs. semantic trade-offs; dynamic objects remain a challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Review highlights dynamic scenes and detection biases as failure sources; no quantitative frequencies given.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Semantic injection into SLAM map (label fusion with geometric optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Using MLLMs to enrich SLAM semantics improves mapping in dynamic settings, but geometric-semantic integration and dynamic-object handling remain bottlenecks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1964.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1964.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmbodiedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EmbodiedGPT (vision-language pretraining via embodied chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language pretraining approach that aligns 2D visual inputs with language-guided goals to support embodied reasoning and goal-conditioned perception.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Embodiedgpt: Vision-language pre-training via embodied chain of thought.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EmbodiedGPT (vision-language foundation for embodied tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Leverages vision-language pretraining and chain-of-thought style reasoning to map 2D visual inputs into semantically rich features that align with language goals, enabling goal-driven perception and reasoning in embodied settings.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Vision-language alignment with chain-of-thought-style supervision to produce language-aligned visual representations for planning and subgoal prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>2D image / scene-level semantically enriched representations</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit semantic features; explicit 3D/metric details not emphasized in the review summary</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>active perception, task-driven self-planning, embodied cognition</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>egocentric/robot camera views (as typical in embodied datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>MLLM pretraining improves semantic grounding but lacks inherent physics modeling; real-time feedback adaptation remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>No quantitative failure breakdown; qualitative limitations include lack of physics compliance and poor adaptation to dynamic changes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Vision-language pretraining with chain-of-thought supervision (semantic fusion into language space)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Vision-language pretraining with chain-of-thought helps semantic grounding and subgoal prediction, but requires coupling with world models for physics-aware, real-time interaction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1964.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1964.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kosmos-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kosmos-2: Grounding multimodal large language models to the world</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal LLM approach designed to ground language and perception by aligning multimodal inputs with LLM representations to enable embodied reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Kosmos-2: Grounding multimodal large language models to the world.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Kosmos-2 (MLLM for embodied grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adapts multimodal large language models to embodied contexts by aligning visual-linguistic representations to world-relevant semantics, enabling semantic reasoning over perception for downstream embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Cross-modal alignment of visual and language representations within an MLLM backbone to ground language in percepts; specific fusion mechanism not detailed in review.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>image/scene-level semantic features aligned to language</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit in multimodal representations; no explicit 3D spatial encoding described in the review summary</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>embodied cognition, active perception, instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>2D camera imagery / multimodal sensory inputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>While Kosmos-2 enhances multimodal semantic grounding, review emphasizes MLLMs generally lack physics-aware dynamics and quick adaptation to environment feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>No numeric failure breakdown; qualitative failure modes include semantic-physical misalignment and latency in closed-loop interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Multimodal alignment within an LLM backbone (implicit cross-modal fusion)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>MLLMs like Kosmos-2 strengthen semantic grounding across modalities but must be combined with world models to enforce physics and temporal coherence for safe embodied control.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1964.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1964.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JEPA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Joint-Embedding Predictive Architecture (JEPA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A predictive representation-learning architecture that learns latent mappings between inputs and their predicted future representations without pixelwise reconstruction, prioritizing semantic prediction in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-supervised learning from images with a joint-embedding predictive architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>JEPA (latent predictive architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns to predict future latent representations (joint embedding) of observations rather than reconstructing pixels, enabling semantic feature extraction that can transfer across downstream embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Latent-space predictive alignment: predictions in embedding space are aligned with multimodal cues (semantic grounding) rather than pixel-level reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>latent/semantic representation level (abstracted features)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit topological/spatiotemporal structure encoded in latent embeddings; explicit 3D coords not the focus</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>world modeling / future prediction / planning support</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>video/visual sequences for predictive modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>JEPA prioritizes semantic prediction but may lack the fine-grained, physics-aware details needed for low-level control without additional grounding from MLLMs or explicit physics models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Qualitative: may fail to capture fine contact/physical dynamics needed for manipulation; no quantitative breakdown provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Report suggests JEPA pretrained representations transfer well across downstream tasks, but dependency on pretraining domain remains a factor.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Predictive latent embedding matching (joint embedding of context and prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>JEPA-style latent predictive grounding gives semantically rich world representations that improve generalization and sample efficiency for planning, but needs coupling with physics-aware components for executable actions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1964.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1964.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Joint MLLM-WM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Joint Multimodal LLM - World Model driven embodied AI architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed architecture that integrates MLLMs for semantic task reasoning with World Models for physics-aware internal representations and future prediction, closing the loop for grounded, executable planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Joint MLLM-WM architecture (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid architecture where MLLMs decompose high-level tasks into subgoals and WMs simulate/predict outcomes and validate physical feasibility; memory updating and self-state inputs create a feedback loop for continual learning and closed-loop execution; intended to enforce semantic-physical alignment and long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Semantic proposals from MLLMs are validated and grounded by WM-predicted physics-aware simulations and memory updates; two-way data exchange (plans → WM simulation → memory → MLLM refinement) acts as grounding and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>multi-level: language-level plans (MLLM) + latent physics-aware world-state embeddings (WM); integrates object-level and scene-level representations.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>explicit in WM (spatio-temporal latent/structured latent encoding); MLLM provides semantic labels mapped to WM entities; paper notes need for spatial mapping, memory structures and proprioceptive self-state input.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>long-horizon planning, service robotics, UAV missions, manipulation, navigation (general embodied tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>intended for real-world robotic and simulation domains (photorealistic sim and real robot sensors)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper explicitly identifies perception/grounding bottlenecks: real-time synchronization mismatch (MLLM latency vs WM fast control), semantic-physical misalignment (plans violating unmodeled constraints), and scalable memory management (WM state updates overwhelming MLLM context).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Enumerated qualitative failure modes: delayed responses from semantic processing leading to outdated plans; MLLM-generated plans that violate physics (e.g., ignoring friction/torque limits); memory/context bloat causing degraded decision quality. No quantitative frequencies provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Proposed approach suggests WM can aid domain adaptation by simulating environment dynamics and supporting sim-to-real transfer, but concrete techniques/results not provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Iterative bidirectional fusion: MLLM → plan → WM simulation/prediction → memory update → MLLM refinement (closed-loop semantic-physical fusion)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Combining MLLMs' semantic reasoning with WMs' physics-aware simulation addresses core grounding failures (physics violations, poor real-time adaptation), but introduces engineering challenges: synchronization/latency, semantic-physical alignment, and scalable memory/context filtering; empirical ablations and quantitative comparisons remain an open direction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as i can, not as i say: Grounding language in robotic affordances. <em>(Rating: 2)</em></li>
                <li>Palm-e: An embodied multimodal language model. <em>(Rating: 2)</em></li>
                <li>Rt-2: Visionlanguage-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>PerAct: [PerAct manipulation paper referenced as PerAct in review] <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision. <em>(Rating: 2)</em></li>
                <li>Embodiedgpt: Vision-language pre-training via embodied chain of thought. <em>(Rating: 2)</em></li>
                <li>Kosmos-2: Grounding multimodal large language models to the world. <em>(Rating: 2)</em></li>
                <li>Self-supervised learning from images with a joint-embedding predictive architecture. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1964",
    "paper_id": "paper-281505303",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "SayCan",
            "name_full": "Do as I can, not as I say: Grounding language in robotic affordances",
            "brief_description": "An approach that constrains LLM planning by a pretrained natural-language action library and evaluates candidate language-action sequences with value functions to ground high-level instructions into feasible robot actions.",
            "citation_title": "Do as i can, not as i say: Grounding language in robotic affordances.",
            "mention_or_use": "mention",
            "model_name": "SayCan (affordance-grounded LLM planner)",
            "model_description": "Uses a pretrained LLM to convert natural-language instructions into natural-language action sequences constrained by a real-world pretrained natural language action library; feasibility of proposed actions is evaluated by value functions (affordance scoring) conditioned on the current environment state.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Constrains LLM proposals with a pretrained natural-language action library and scores actions with value functions (affordance-based grounding rather than pixel-level cross-attention).",
            "representation_level": "action-level / language-action library (high-level symbolic/affordance representations)",
            "spatial_representation": "none/implicit (affordance-based grounding; no explicit spatial 3D grounding described in this paper's summary)",
            "embodied_task_type": "instruction following / manipulation / service robotics",
            "embodied_task_name": null,
            "visual_domain": null,
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Limited by a fixed natural language action library and specific physical environment; difficulty adapting to new robots/environments (semantic grounding constrained to prebuilt affordance set).",
            "failure_mode_analysis": "Paper notes failure modes such as infeasible plans proposed by LLMs if not constrained by affordance library; no quantitative frequencies reported.",
            "domain_shift_handling": "Handles some environment-specific feasibility via value functions, but overall limited adaptability across different robot embodiments and environments.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "LLM outputs filtered/scored by affordance/value models (language-level constraint and verification loop)",
            "sample_efficiency": null,
            "key_findings_grounding": "Affordance-constrained LLM planning improves feasibility of high-level instruction-to-action mapping, but reliance on a fixed action library limits adaptability and real-world generalization.",
            "uuid": "e1964.0"
        },
        {
            "name_short": "PaLM-E",
            "name_full": "PaLM-E: An embodied multimodal language model",
            "brief_description": "A vision-language model that integrates continuous real-world sensor encodings with a pretrained large language model to map percepts to words and enable multitask embodied control within a fixed action space.",
            "citation_title": "Palm-e: An embodied multimodal language model.",
            "mention_or_use": "mention",
            "model_name": "PaLM-E (VLM for embodied control)",
            "model_description": "Trains visual and language encodings end-to-end in conjunction with a pretrained LLM, incorporates continuous real-world sensor modality encodings into the VLM, and links words to percepts to perform multi-task embodied control via a fixed action-space mapping.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Multimodal encoding pipeline that injects continuous sensor (visual, proprioceptive) representations into an LLM backbone to ground language tokens to perceptual features and actions.",
            "representation_level": "multimodal token-level representations (language-aligned perceptual encodings)",
            "spatial_representation": "implicit via continuous sensor encodings; explicit spatial detail not specified in review",
            "embodied_task_type": "embodied manipulation, instruction following",
            "embodied_task_name": null,
            "visual_domain": "real-world robot sensors / continuous sensor modalities (as described)",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "MLLMs like PaLM-E can struggle to enforce physics-compliant dynamics and real-time adaptation; review flags grounding to physical constraints as a limitation of MLLMs.",
            "failure_mode_analysis": "Qualitative failure modes noted: MLLMs may propose physically implausible actions (e.g., ignoring friction/material properties) and have poor responsiveness to rapid environmental feedback; no numeric breakdown provided.",
            "domain_shift_handling": "Not specified; general limitation cited that MLLMs rely on pretraining and struggle with continuous physical adaptation.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Early multimodal fusion into LLM context (injecting sensor encodings into LLM input representation)",
            "sample_efficiency": null,
            "key_findings_grounding": "End-to-end injection of continuous sensor encodings into LLMs enables language-aligned perception-to-action mapping, but LLM-driven grounding lacks built-in physics compliance and fast adaptation to environmental feedback.",
            "uuid": "e1964.1"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2: Visionlanguage-action models transfer web knowledge to robotic control",
            "brief_description": "A vision-language-action (VLA) model that tokenizes multimodal robot state (image, language, actions) and uses an LLM to generate tokens which are detokenized into low-level robot actions.",
            "citation_title": "Rt-2: Visionlanguage-action models transfer web knowledge to robotic control.",
            "mention_or_use": "mention",
            "model_name": "RT-2 (Vision-Language-Action model)",
            "model_description": "Encodes the robot's current image, language instruction, and robot action at a timestep into token sequences; an LLM performs semantic reasoning and task decomposition on these tokens; generated tokens are detokenized into final continuous or discrete actions.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Tokenization of multimodal inputs (images and action/state) into a language-like token stream processed by an LLM; grounding occurs by mapping LLM-generated tokens back to executable actions (detokenization).",
            "representation_level": "timestep-level multimodal tokens (sequential language-like representation of perception and action)",
            "spatial_representation": "implicit in encoded image/state tokens; explicit spatial maps not described in review",
            "embodied_task_type": "embodied manipulation / low-level control",
            "embodied_task_name": null,
            "visual_domain": "robot onboard camera / real-world imagery (as described)",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Review highlights MLLMs/VLAs (including RT-2) can have poor real-time adaptation and may not enforce physics constraints without additional mechanisms.",
            "failure_mode_analysis": "Qualitative: potential latency/detokenization mismatch and physics-violating proposals; no numerical failure analysis in review.",
            "domain_shift_handling": "Not detailed; general limitations of LLM-based VLAs in adapting to new environments noted.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Sequential token-level fusion (images + actions + language → tokens → LLM processing → detokenized actions)",
            "sample_efficiency": null,
            "key_findings_grounding": "Tokenized multimodal pipelines enable direct mapping from perception and instruction to actions, but grounding depends on correct detokenization and suffers from latency and physics-mismatch issues without world-model constraints.",
            "uuid": "e1964.2"
        },
        {
            "name_short": "PerAct",
            "name_full": "PerAct (3D voxel-based manipulation)",
            "brief_description": "A manipulation approach that uses 3D voxel representations to achieve precise action outputs, reported to reach millimeter-level grasp accuracy in review's summary.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "PerAct",
            "model_description": "Employs 3D voxelized scene representations to map visual 3D context directly to manipulation actions, enabling fine-grained spatial grounding for dexterous manipulation.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Direct mapping from 3D voxel representation to action outputs (spatially precise vision-to-action grounding in voxel space).",
            "representation_level": "3D voxel-level (geometric, spatially explicit)",
            "spatial_representation": "explicit 3D voxel grid / coordinates (depth-aware spatial grounding)",
            "embodied_task_type": "dexterous manipulation / grasping",
            "embodied_task_name": null,
            "visual_domain": "robotic manipulation (likely real-world or physics-simulated 3D scenes)",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Voxel-based grounding helps precision but review notes broader challenges: MLLMs may still ignore physical properties; WMs required for physics-aware validation.",
            "failure_mode_analysis": "No quantitative failure breakdown provided; review emphasizes need for physics-aware validation to avoid failure in contact-rich tasks.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Spatial (3D) perception directly used to predict actions; specific multimodal fusion not detailed in review.",
            "sample_efficiency": null,
            "key_findings_grounding": "3D voxel representations enable very precise spatial grounding for manipulation, but success still depends on integrating physical dynamics (world models) for robust, generalizable behavior.",
            "uuid": "e1964.3"
        },
        {
            "name_short": "CLIP-based grounding",
            "name_full": "CLIP-based vision-language alignment",
            "brief_description": "Contrastive image-text embedding models (CLIP-style) used to align visual scenes and linguistic cues, reducing ambiguity in object recognition and enabling open-vocabulary grounding.",
            "citation_title": "Learning transferable visual models from natural language supervision.",
            "mention_or_use": "mention",
            "model_name": "CLIP-based alignment (contrastive V+L)",
            "model_description": "Uses joint image-text contrastive pretraining to produce shared embeddings for images and text; these embeddings are used to align visual observations with language queries, enabling region-level or scene-level grounding via nearest-neighbor or cross-modal retrieval.",
            "visual_encoder_type": "CLIP visual encoder (backbone unspecified in review)",
            "visual_encoder_pretraining": "Pretrained on large-scale image-text pairs (web-scale contrastive pretraining as in CLIP)",
            "grounding_mechanism": "Contrastive alignment of image and text embeddings (embedding similarity used to ground language to visual content), often used as retrieval or grounding signal.",
            "representation_level": "scene-level / region-level embeddings (open-vocabulary grounding possible)",
            "spatial_representation": "typically implicit (embedding-based); can be combined with region proposals for region-level grounding",
            "embodied_task_type": "active perception / 3D scene understanding / grounding for task planning",
            "embodied_task_name": null,
            "visual_domain": "2D camera images, sometimes combined with 3D point clouds (as reported for 3D scene methods leveraging vision-language embeddings)",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "CLIP-style embeddings reduce semantic ambiguity but may not encode physical dynamics or fine-grained spatial relations needed for manipulation; novel object recognition depends on training coverage.",
            "failure_mode_analysis": "Qualitative: ambiguity in cluttered scenes and lack of physics/dynamics modeling cause grounding errors; no numerical frequencies provided.",
            "domain_shift_handling": "Often limited; review suggests additional fine-tuning or combination with WMs needed for sim-to-real or domain shifts.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Improvements attributed to web-scale pretraining, but no quantitative scaling law reported in review.",
            "fusion_mechanism": "Embedding similarity / retrieval (contrastive fusion); can be combined downstream with attention-based models.",
            "sample_efficiency": null,
            "key_findings_grounding": "Contrastive CLIP-style alignment is effective for open-vocabulary semantic grounding and reducing label dependence, but insufficient alone for physics-aware control and real-time adaptation in embodied tasks.",
            "uuid": "e1964.4"
        },
        {
            "name_short": "SEO-SLAM",
            "name_full": "SEO-SLAM (MLLM-enhanced SLAM labeling)",
            "brief_description": "An MLLM-augmented SLAM approach that uses multimodal LLM capabilities to generate more descriptive object labels and dynamically correct detection biases during SLAM.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SEO-SLAM (MLLM-enhanced SLAM)",
            "model_description": "Integrates MLLMs into SLAM pipelines to produce richer semantic labels for observed objects and maintain a multiclass confusion matrix to mitigate detection bias during map construction.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Semantic grounding via MLLM-generated labels aligned to observed visual entities, improving SLAM's semantic mapping by mapping words to percepts.",
            "representation_level": "object-level semantic labels integrated into SLAM maps",
            "spatial_representation": "explicit spatial mapping via SLAM (pose, map) enriched with semantic labels",
            "embodied_task_type": "visual SLAM / active perception",
            "embodied_task_name": null,
            "visual_domain": "robotic RGB-D or camera-driven SLAM environments (static and dynamic scenes)",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "MLLM-enhanced semantic labeling helps dynamic scene robustness but SLAM still faces geometric vs. semantic trade-offs; dynamic objects remain a challenge.",
            "failure_mode_analysis": "Review highlights dynamic scenes and detection biases as failure sources; no quantitative frequencies given.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Semantic injection into SLAM map (label fusion with geometric optimization)",
            "sample_efficiency": null,
            "key_findings_grounding": "Using MLLMs to enrich SLAM semantics improves mapping in dynamic settings, but geometric-semantic integration and dynamic-object handling remain bottlenecks.",
            "uuid": "e1964.5"
        },
        {
            "name_short": "EmbodiedGPT",
            "name_full": "EmbodiedGPT (vision-language pretraining via embodied chain-of-thought)",
            "brief_description": "A vision-language pretraining approach that aligns 2D visual inputs with language-guided goals to support embodied reasoning and goal-conditioned perception.",
            "citation_title": "Embodiedgpt: Vision-language pre-training via embodied chain of thought.",
            "mention_or_use": "mention",
            "model_name": "EmbodiedGPT (vision-language foundation for embodied tasks)",
            "model_description": "Leverages vision-language pretraining and chain-of-thought style reasoning to map 2D visual inputs into semantically rich features that align with language goals, enabling goal-driven perception and reasoning in embodied settings.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Vision-language alignment with chain-of-thought-style supervision to produce language-aligned visual representations for planning and subgoal prediction.",
            "representation_level": "2D image / scene-level semantically enriched representations",
            "spatial_representation": "implicit semantic features; explicit 3D/metric details not emphasized in the review summary",
            "embodied_task_type": "active perception, task-driven self-planning, embodied cognition",
            "embodied_task_name": null,
            "visual_domain": "egocentric/robot camera views (as typical in embodied datasets)",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "MLLM pretraining improves semantic grounding but lacks inherent physics modeling; real-time feedback adaptation remains limited.",
            "failure_mode_analysis": "No quantitative failure breakdown; qualitative limitations include lack of physics compliance and poor adaptation to dynamic changes.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Vision-language pretraining with chain-of-thought supervision (semantic fusion into language space)",
            "sample_efficiency": null,
            "key_findings_grounding": "Vision-language pretraining with chain-of-thought helps semantic grounding and subgoal prediction, but requires coupling with world models for physics-aware, real-time interaction.",
            "uuid": "e1964.6"
        },
        {
            "name_short": "Kosmos-2",
            "name_full": "Kosmos-2: Grounding multimodal large language models to the world",
            "brief_description": "A multimodal LLM approach designed to ground language and perception by aligning multimodal inputs with LLM representations to enable embodied reasoning.",
            "citation_title": "Kosmos-2: Grounding multimodal large language models to the world.",
            "mention_or_use": "mention",
            "model_name": "Kosmos-2 (MLLM for embodied grounding)",
            "model_description": "Adapts multimodal large language models to embodied contexts by aligning visual-linguistic representations to world-relevant semantics, enabling semantic reasoning over perception for downstream embodied tasks.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Cross-modal alignment of visual and language representations within an MLLM backbone to ground language in percepts; specific fusion mechanism not detailed in review.",
            "representation_level": "image/scene-level semantic features aligned to language",
            "spatial_representation": "implicit in multimodal representations; no explicit 3D spatial encoding described in the review summary",
            "embodied_task_type": "embodied cognition, active perception, instruction following",
            "embodied_task_name": null,
            "visual_domain": "2D camera imagery / multimodal sensory inputs",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "While Kosmos-2 enhances multimodal semantic grounding, review emphasizes MLLMs generally lack physics-aware dynamics and quick adaptation to environment feedback.",
            "failure_mode_analysis": "No numeric failure breakdown; qualitative failure modes include semantic-physical misalignment and latency in closed-loop interaction.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Multimodal alignment within an LLM backbone (implicit cross-modal fusion)",
            "sample_efficiency": null,
            "key_findings_grounding": "MLLMs like Kosmos-2 strengthen semantic grounding across modalities but must be combined with world models to enforce physics and temporal coherence for safe embodied control.",
            "uuid": "e1964.7"
        },
        {
            "name_short": "JEPA",
            "name_full": "Joint-Embedding Predictive Architecture (JEPA)",
            "brief_description": "A predictive representation-learning architecture that learns latent mappings between inputs and their predicted future representations without pixelwise reconstruction, prioritizing semantic prediction in latent space.",
            "citation_title": "Self-supervised learning from images with a joint-embedding predictive architecture.",
            "mention_or_use": "mention",
            "model_name": "JEPA (latent predictive architecture)",
            "model_description": "Learns to predict future latent representations (joint embedding) of observations rather than reconstructing pixels, enabling semantic feature extraction that can transfer across downstream embodied tasks.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Latent-space predictive alignment: predictions in embedding space are aligned with multimodal cues (semantic grounding) rather than pixel-level reconstruction.",
            "representation_level": "latent/semantic representation level (abstracted features)",
            "spatial_representation": "implicit topological/spatiotemporal structure encoded in latent embeddings; explicit 3D coords not the focus",
            "embodied_task_type": "world modeling / future prediction / planning support",
            "embodied_task_name": null,
            "visual_domain": "video/visual sequences for predictive modeling",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "JEPA prioritizes semantic prediction but may lack the fine-grained, physics-aware details needed for low-level control without additional grounding from MLLMs or explicit physics models.",
            "failure_mode_analysis": "Qualitative: may fail to capture fine contact/physical dynamics needed for manipulation; no quantitative breakdown provided.",
            "domain_shift_handling": "Report suggests JEPA pretrained representations transfer well across downstream tasks, but dependency on pretraining domain remains a factor.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Predictive latent embedding matching (joint embedding of context and prediction)",
            "sample_efficiency": null,
            "key_findings_grounding": "JEPA-style latent predictive grounding gives semantically rich world representations that improve generalization and sample efficiency for planning, but needs coupling with physics-aware components for executable actions.",
            "uuid": "e1964.8"
        },
        {
            "name_short": "Joint MLLM-WM",
            "name_full": "Joint Multimodal LLM - World Model driven embodied AI architecture",
            "brief_description": "A proposed architecture that integrates MLLMs for semantic task reasoning with World Models for physics-aware internal representations and future prediction, closing the loop for grounded, executable planning.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "Joint MLLM-WM architecture (proposed)",
            "model_description": "Hybrid architecture where MLLMs decompose high-level tasks into subgoals and WMs simulate/predict outcomes and validate physical feasibility; memory updating and self-state inputs create a feedback loop for continual learning and closed-loop execution; intended to enforce semantic-physical alignment and long-horizon planning.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Semantic proposals from MLLMs are validated and grounded by WM-predicted physics-aware simulations and memory updates; two-way data exchange (plans → WM simulation → memory → MLLM refinement) acts as grounding and verification.",
            "representation_level": "multi-level: language-level plans (MLLM) + latent physics-aware world-state embeddings (WM); integrates object-level and scene-level representations.",
            "spatial_representation": "explicit in WM (spatio-temporal latent/structured latent encoding); MLLM provides semantic labels mapped to WM entities; paper notes need for spatial mapping, memory structures and proprioceptive self-state input.",
            "embodied_task_type": "long-horizon planning, service robotics, UAV missions, manipulation, navigation (general embodied tasks)",
            "embodied_task_name": null,
            "visual_domain": "intended for real-world robotic and simulation domains (photorealistic sim and real robot sensors)",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper explicitly identifies perception/grounding bottlenecks: real-time synchronization mismatch (MLLM latency vs WM fast control), semantic-physical misalignment (plans violating unmodeled constraints), and scalable memory management (WM state updates overwhelming MLLM context).",
            "failure_mode_analysis": "Enumerated qualitative failure modes: delayed responses from semantic processing leading to outdated plans; MLLM-generated plans that violate physics (e.g., ignoring friction/torque limits); memory/context bloat causing degraded decision quality. No quantitative frequencies provided in review.",
            "domain_shift_handling": "Proposed approach suggests WM can aid domain adaptation by simulating environment dynamics and supporting sim-to-real transfer, but concrete techniques/results not provided in review.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Iterative bidirectional fusion: MLLM → plan → WM simulation/prediction → memory update → MLLM refinement (closed-loop semantic-physical fusion)",
            "sample_efficiency": null,
            "key_findings_grounding": "Combining MLLMs' semantic reasoning with WMs' physics-aware simulation addresses core grounding failures (physics violations, poor real-time adaptation), but introduces engineering challenges: synchronization/latency, semantic-physical alignment, and scalable memory/context filtering; empirical ablations and quantitative comparisons remain an open direction.",
            "uuid": "e1964.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances.",
            "rating": 2
        },
        {
            "paper_title": "Palm-e: An embodied multimodal language model.",
            "rating": 2
        },
        {
            "paper_title": "Rt-2: Visionlanguage-action models transfer web knowledge to robotic control.",
            "rating": 2
        },
        {
            "paper_title": "PerAct: [PerAct manipulation paper referenced as PerAct in review]",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision.",
            "rating": 2
        },
        {
            "paper_title": "Embodiedgpt: Vision-language pre-training via embodied chain of thought.",
            "rating": 2
        },
        {
            "paper_title": "Kosmos-2: Grounding multimodal large language models to the world.",
            "rating": 2
        },
        {
            "paper_title": "Self-supervised learning from images with a joint-embedding predictive architecture.",
            "rating": 2
        }
    ],
    "cost": 0.025519249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Embodied AI: From LLMs to World Models
2023 2020 2015</p>
<p>Tongtong Feng fengtongtong@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Member, IEEEXin Wang xinwang@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Fellow, IEEEYu-Gang Jiang 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Wenwu Zhu wwzhu@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>§iii-C 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Vit Sam 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Embodied AI: From LLMs to World Models
2023 2020 2015453D72FF362712A217AACE4F27FA079CEmbodied AILLMsWorld Models
Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving the evolution from cyberspace to physical systems.Recent breakthroughs in Large Language Models (LLMs) and World Models (WMs) have drawn significant attention for embodied AI.On the one hand, LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition.On the other hand, WMs empower embodied AI by building internal representations and future predictions of the external world, facilitating physical law-compliant embodied interactions.As such, this paper comprehensively explores the literature in embodied AI from basics to advances, covering both LLM driven and WM driven works.In particular, we first present the history, key technologies, key components, and hardware systems of embodied AI, as well as discuss its development via looking from unimodal to multimodal angle.We then scrutinize the two burgeoning fields of embodied AI, i.e., embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs, meticulously delineating their indispensable roles in endto-end embodied cognition and physical laws-driven embodied interactions.Building upon the above advances, we further share our insights on the necessity of the joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds.In addition, we examine representative applications of embodied AI, demonstrating its wide applicability in real-world scenarios.Last but not least, we point out future research directions of embodied AI that deserve further investigation.</p>
<p>I. INTRODUCTION</p>
<p>Embodied Artificial Intelligence (AI) originated from the Embodied Turing Test by Alan Turing in 1950 [1], which is designed to explore whether agents can imitate human intelligence to achieve Artificial General Intelligence (AGI).Among them, agents that only solve abstract problems in digital world (cyberspace) are generally defined as disembodied AI, while those that also can interact with the physical world are regarded as embodied AI.Embodied AI builds on foundational insights from cognitive science and neuroscience [2], [3], which claims that intelligence emerges from the dynamic coupling of perception, cognition, and interaction.As shown in Fig. 1, embodied AI includes three key components in a closed-loop manner, i.e., 1) active perception (sensordriven environmental observation), 2) embodied cognition (historical experience-driven cognition updating), and 3) dynamic interaction (actuator-mediated action control).Besides, hardware embodiment [4]- [6] is also critical due to escalating computational and energy demands, particularly under latency and power constraints of devices in real-world deployment scenarios.</p>
<p>The development of embodied AI has evolved from unimodal to multimodal paradigm.In early stage, embodied AI is primarily studied through focusing on individual components with single modality such as vision, language, or action, where the perception, cognition, or interaction component is driven by one sensory input [7], [8], e.g., perception tends to be dominated by the visual modality [9], cognition tends to be dominated by the language modality [10], [11], and interaction tends to be dominated by the action modality [12], [13].Although these methods perform well within individual components, they are limited by the narrow scope of information provided by each modality and the inherent gaps between modalities across components.The continued development of embodied AI witnesses the limitations of unimodal approaches, promoting a significant shift toward integration of multiple sensory modalities [14]- [16].As such, multimodal embodied AI [17], [18] naturally arises to create more adaptive, flexible, and robust agents capable of performing complex tasks in dynamic environments.</p>
<p>Large Language Models (LLMs) empower embodied AI via semantic reasoning [19] and task decomposition [20], [21], bringing high-level natural language instructions and lowlevel natural language actions into embodied cognition.Representative LLM driven works include SayCan [22], which i) provides a real-world pretrained natural language action library to constrain LLMs from proposing infeasible and contextually inappropriate actions; ii) uses LLMs to convert natural language instructions into natural language action sequences; and iii) utilizes value functions to verify the feasibility of natural arXiv:2509.20021v1[cs.AI] 24 Sep 2025 language action sequences in a particular physical environment.These works suggest that LLMs are extremely useful to robots which aim at acting upon high-level, temporally extended instructions expressed in natural language.However, LLMs are only a part of the entire embodied AI system (e.g., embodied cognition), which is limited by a fixed natural language action library and a specific physical environment, making it difficult for LLM driven embodied AI to achieve adaptive expansion for new robots and environments.</p>
<p>Recent breakthroughs in Multimodal LLMs (MLLMs) [23], [24] and World Models (WMs) [25]- [27] have opened up a new frontier in embodied AI research.MLLMs can act on the entire embodied AI system, bridging high-level multimodal inputting and low-level motor action sequences into endto-end embodied applications.Semantic reasoning [28]- [30] leverages MLLMs' cross-modal comprehension to interpret semantics from visual, auditory, or tactile inputs, e.g., identifying objects, inferring spatial relationships, predicting environmental dynamics.Concurrently, task decomposition [31]- [33] employs MLLMs' sequential logic to break complex objectives into sub-tasks while dynamically adapting plans based on sensor feedback.However, MLLMs often fail to ground predictions in physics-compliant dynamics [34] and exhibit poor real-time adaptation [35] to environmental feedback.</p>
<p>On the other hand, WMs empower embodied AI by building internal representations [36]- [40] and making future predictions [41]- [44] of the external world.Such WM driven embodied AI is able to facilitate physical law-compliant embodied interactions in dynamic environments.Internal representations compress rich sensory inputs into structured latent spaces, capturing object dynamics, physics laws, and spatial structures, as well as allowing agents to reason about "what exists" and "how things behave" in their surroundings.Simultaneously, future predictions simulate potential rewards of sequence actions across multiple time horizons aligned with physical laws, thereby preempting risky or inefficient behaviors.However, WM driven approaches struggle with open-ended semantic reasoning [45] and lack the ability of generalizable task decomposition [26] without explicit priors.</p>
<p>Building upon the above advances, we further share our insights on the necessity of developing a joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds.MLLMs enable contextual task reasoning but overlook physical constraints, while WMs excel at physics-aware simulation but lack high-level semantics.The joint of MLLM and WM can bridge semantic intelligence with grounded physical interaction.For instance, EvoAgent [46] designs an autonomous-evolving agent with a joint MLLM-WM driven embodied AI architecture, which can autonomously complete various long-horizon tasks across environments through selfplanning, self-reflection, and self-control, without human intervention.We believe that designing joint MLLM-WM driven embodied AI architectures will dominate next-generation embodied systems, bridging the gap between specialized AI agents and general physical intelligence.</p>
<p>We summarize the representative applications of embodied AI as service robotics, rescue UAVs, industrial Robots, and others etc., demonstrating its wide applicability in real-world scenarios.We also point out potential future directions of embodied AI, including but not limited to autonomous embodied AI, embodied AI hardware, and swarm embodied AI etc.</p>
<p>As shown in Fig. 2, the rest of this paper is organized as follows.Section II introduces the history, key technologies, key components, and hardware system of embodied AI, discussing the development of embodied AI from unimodal to multimodal angle.Section III presents embodied AI with LLMs/MLLMs, and Section IV presents embodied AI with WMs.Section V introduces our insights on designing a joint MLLM-WM driven embodied AI architecture.Section VI briefly examines applications of embodied AI.Potential future directions are discussed in Section VII.</p>
<p>II. EMBODIED AI</p>
<p>This section provides a comprehensive overview of embodied AI.We first take a historical view to introduce the development of embodied AI in Subsection II-A.Based on technological advancements in five foundational areas related to embodied AI, Subsection II-B and Subsection II-C further review the developmental trajectories of core modules in software algorithms and hardware design, respectively.Finally, Subsection II-E discusses an overall analysis of the developmental trends from unimodal to multimodal.</p>
<p>A. The Historical View</p>
<p>The historical evolution of embodied AI reflects successive transitions from early philosophical foundations to technological breakthroughs in robotics and the rise of learning-driven paradigms, while recent progress in LLMs and WMs is driving an ongoing shift toward the next phase of development.</p>
<p>The theoretical roots of embodied AI trace to 1950, when Turing introduced the foundational idea that intelligence is inherently linked to physical experience [47].In the 1980s, cognitive science further formalized this view.Lakoff and Johnson emphasized that human cognition arises from bodily experience rather than disembodied symbolic computation [48], while Harnad's symbol grounding problem highlighted the necessity of connecting symbolic representations to sensorymotor reality [49].Technological advances in robotics during the late 1980s and 1990s brought these ideas into practice.Brooks proposed the subsumption architecture [50], [51], promoting behavior-based control through layered, reactive modules grounded in sensorimotor loops.The Cog project [52] advanced this line by constructing humanoid robots capable of developmental learning, imitation, and social interaction.Recently, the success of the learning-driven paradigm has driven the shift in embodied AI from motion control of robots to adaptive interaction [53].In particular, the development of deep learning enables robots to learn complex nonlinear mappings from raw sensor data to action policy, significantly improving navigation and manipulation tasks [54], [55].</p>
<p>While embodied AI has made notable advances, achieving self-reflection intelligence in dynamic, uncertain environments remains a key challenge.Recent progress in LLMs/MLLMs [23], [24] and WMs [25]- [27] have progressively shown promise in overcoming these challenges.</p>
<p>B. The Key Technologies and Components</p>
<p>Before discussing the ongoing changes, we systematically review the development of key technologies and components.</p>
<p>1) Key Technologies of Embodied AI: The rapid development of embodied AI is closely tied to advances in foundational technological models such as Computer Vision (CV) models, Natural Language Processing (NLP) models, Reinforcement Learning (RL) models, LLMs/MLLMs, and WMs (as shown in Fig. 3), which can significantly enhance the capabilities of agents in perception, cognition and interaction.</p>
<p>Specifically, Classic models in computer vision, such as AlexNet [56], GAN [57], ResNet [58], ViT [59], DDPM [60], MAE [61], and SAM [62] provide the perceptual foundation for embodied agents to interpret high-dimensional sensory inputs in complex environments.In the field of NLP, the evolution from foundational architectures like Transformer [63], BERT [64], and T5 [65] to large-scale systems such as ChatGPT [66], Vicuna [67], and LLaMA [68], has equipped embodied agents with stronger capabilities in language understanding, task planning, and instruction following.RL offers the core algorithmic framework for agents to learn through interaction with their environments.Representative approaches include DQN [69], AlphaGo [70], PPO [71], SAC [72], RLHF [73], and GRPO [74].</p>
<p>Beyond these classical fields, one of the most promising directions in embodied AI lies in the integration of LLMs/MLLMs with WMs.LLMs and MLLMs (like Flamingo [20], Qwen-VL [75], Gemini-1.5 [76], GPT-4o [77], and Deepseek-R1 [78]) provide agents with the ability to understand instructions, reason over multimodal inputs, and generalize across tasks and environments.In contrast, WMs (like Mental Model [26], RSSM [79], JEPA [27], Dreamer-v3 [80], Sora [81], and Genie [36]) enable agents to model and predict environmental dynamics, supporting imaginationbased planning and anticipatory decision-making in dynamic and uncertain environments.</p>
<p>2) Key Components of Embodied AI: Driven by advances in these key technologies, embodied AI has experienced rapid progress.In the following, we present a structured overview of developments in three key components.</p>
<p>a) Active Perception: Active perception refers to the agent selectively acquiring information from environmental observations [16], [82], [83].Existing active perception meth- ods can be roughly divided into three categories: visual SLAM, 3D scene understanding, and active environment exploration.</p>
<p>To offer an effective perspective on active perception approaches, as summarized in Table I, we analyze representative methods along three practical dimensions: sensor type, feature type, and applicable scenarios.</p>
<p>Visual SLAM.Simultaneous Localization and Mapping (SLAM) is a pivotal technology enabling agents to both localize themselves and construct environmental maps in unknown environments [9], [84].As a foundational technology of active perception, visual SLAM has been extensively studied [85], [86].According to Wang et al. [87], existing methods fall into geometric-based and semantic-based categories.Geometric methods exploit spatial or temporal cues [8], such as dense scene flow [88], [89], triangulation consistency [90], and graph structure [91], [92], performing well in static settings but struggling with dynamic scenes.In contrast, semantic methods improve localization and mapping in dynamic environments by leveraging high-level information.Representative early methods include SLAM++ [93], integrating object-level semantics, and DS-SLAM [94], applying deep learning to dynamic scene understanding.Recent models such as TwistSLAM [95] and GS-SLAM [96] further enhance robustness by combining geometric optimization with semantic or generative modeling.3D Scene Understanding.Scene understanding focuses on enabling agents to perceive, segment, and reason about complex environments in a structured and semantically meaningful way.Recent works have advanced this field by integrating vision-language models and generative priors.Early efforts like Gaudi [97] introduced generative models for 3D-aware scene synthesis.Clip2Scene [98] and OpenScene [99] leveraged vision-language embeddings to facilitate labelefficient and open-vocabulary 3D understanding.Structured scene understanding is further enhanced by Lexicon3D [100] and GraphDreamer [101], which model object-level relations in 3D space through structured representations such as scene graphs or semantic lexicons.Meanwhile, region-level multimodal grounding techniques, exemplified by HUGS [102] and RegionPLC [103], incorporate prompts and spatial grounding to achieve fine-grained, goal-conditioned 3D perception.These methods advance holistic, language-aligned 3D understanding.</p>
<p>Active Environment Exploration.Active exploration focuses on enabling agents to autonomously acquire informative observations through interaction with the environment.Early approaches relied on building explicit or implicit environmental models.Representative model-based methods include MAX [104] and Active Neural SLAM [105], which leverage predictive modeling and mapping to support efficient navigation in unseen spaces.In contrast, APT [106] and DBMF-BPI [108] focus on model-free exploration through direct environmental interaction to reduce reliance on explicit modeling.Recent efforts further enhance exploration capabilities by incorporating multimodal perception [109] and semantic reasoning [107].b) Embodied Cognition: Embodied cognition refers to the emergence of internal representations and reasoning capabilities during the interaction, driven by the agent's selfreflection on its perception and accumulated experience [147]- [149].This component forms the core of embodied AI, en- abling agents to perform task planning [150], causal inference [151], and long-horizon reasoning [152], [153].Recent studies of embodied cognition primarily focus on three aspects: task-driven self-planning, memory-driven self-reflection, and embodied multimodal foundation models.Table II presents representative methods analyzed from four perspectives: input modalities, cognition type, reasoning mode, and output type.These dimensions reflect how embodied agents perceive information, form internal models and conduct reasoning.</p>
<p>Task-driven Self-Planning.In task-driven self-planning, agents autonomously generate structured plans based on task goals, environmental context, and internal knowledge, without explicit human instructions [154]- [156].Structured learning is a classical solution that develops latent planning spaces or direct policy mappings, achieving high efficiency within training distributions but lacking robustness to out-of-distribution scenarios.Representative approaches include L3P [110], Egoplaner [112], and ETPNav [115].Recent advances incorporate LLMs or generative models into self-planning.LLM-Planner [111] and AutoAct [113] integrate LLMs into planning by grounding language-guided reasoning into various tasks, while RPG [114] offers a generative perspective, aiming to unify planning and content creation through multimodal reasoning.</p>
<p>Memory-driven Self-Reflection.Memory-driven selfreflection enables agents to leverage past experiences for longhorizon reasoning, error correction, and self-improvement [46], [157].Early studies focus on memory processing, including fixed-size replay buffers [158]- [160] and differentiable memory architectures [161], [162].Recent advances introduce reflective mechanisms, where agents summarize or verbalize past experiences to guide future decisions.Reflexion [116] and Reflect [117] enable agents to iteratively self-correct by integrating verbalized feedback into action planning, while RILA [118] extends reflective reasoning to multimodal semantic navigation.Beyond individual reflection, Optimus-1 [119] and REMAC [120] integrate multimodal or multi-agent memory to support long-horizon collaboration.EvoAgent [46] further advances this direction by coupling continual world modeling with a memory-driven planner, enabling fully autonomous evolution across sequential tasks.</p>
<p>Embodied Multimodal Foundation Models.In the era of MLLMs, embodied multimodal foundation models [163]- [165] have emerged as one of the most promising solutions for unifying planning, reasoning, and other embodied cognitive capabilities.Recent progress is driven by both data construction and model development.Data efforts focus on constructing high-quality benchmarks to support scalable and cognitively meaningful evaluation, such as MuEP [166], ECBench [167], MFE-ETP [168], and EmbodiedBench [18].On the model side, recent advances include affordancegrounded agents (e.g., SayCan [121] and GATO [122]) that align language understanding with embodied action spaces, vision-language pretraining approaches (like EmbodiedGPT [123] and Kosmos-2 [124]) that promote scalable embodied reasoning, and object-centric designs (such as MultiPLY [125] and ManipLLM [28]) that enhance manipulation and interaction capabilities.These models collectively aim to build transferable and generalizable embodied AI. c) Dynamic Interaction: Dynamic interaction refers to the process in which an agent influences the environment through actions or behaviors grounded in its perception and cognition [169], [170].Existing research highlights the significance of this capability in enabling agents not only to respond but also to change their surroundings [171], [172].Studies on dynamic interaction encompass action control, behavioral interaction, and collaborative decision-making.To better understand existing methods, we analyze representative approaches from four perspectives, including input modalities, interaction type, learning paradigm, and task type, as shown in Table III.These dimensions reflect how agents sense the environment, determine the level and structure of interaction, and generate appropriate behaviors in dynamic multi-agent or human-in-the-loop scenarios.</p>
<p>Action Control.Action control generates motor commands for embodied interaction.Early methods were based on control theory with dynamic system modeling [173], [174] or RL via trial and error [175], [176].The former is effective for structured or repetitive tasks, while the latter is adaptable to high-dimensional, nonlinear problems.Recent advances mainly follow three directions.Vision-language-action (VLA) models, such as PaLM-E [14], RT-2 [24], OpenVLA [127],</p>
<p>and CogAgent [128], integrate language-guided reasoning for flexible control and have been comprehensively reviewed by Ma et al. [177].Open-ended frameworks like MineDojo [126] promote continual skill acquisition from open-world knowledge.In addition, Cross-embodiment learning, including CrossFormer [130], HPT [131], and Octo [129], aim to unify policy learning across diverse robots and modalities.</p>
<p>Behavioral Interaction.The behavior of an agent is composed of a sequence of actions.Compared to action control, it emphasizes high-level control through meaningful action patterns, enabling agents to interact in a flexible and goaldirected manner.Recent advances mainly fall into two directions.Imitation learning, including GAIL [132], MGAIL [133], TrafficSim [134], and TrajGen [135], enables efficient acquisition and simulation of complex behaviors.BEHAVIOR-1K [136] provides a large-scale benchmark for evaluating behavior generalization across 1,000 embodied tasks.Behavioraware enhancement methods, such as AgentLens [137] and ECL [138], improve policy robustness and interpretability.Despite these advances, achieving reliable long-horizon behavioral interaction under sparse feedback remains challenging.</p>
<p>Collaborative Decision.Collaborative decision focuses on coordinating multiple agents to achieve shared goals, which is essential for multi-agent systems and human-robot collaboration [178]- [180].Multi-agent RL is a classical solution, with methods like QTRAN [140], QPLEX [141], and Qatten [139] addressing cooperation via centralized training with decentralized execution.MAT [142] reframes MARL as a sequence modeling problem to mitigate scalability limitations in multi-agent RL.Recent advances integrate LLMs and WMs to enhance multi-agent collaboration.MetaGPT [145], CoELA [143], and AgentVerse [144] leverage LLMs for task reasoning and coordination, while COMBO [146] composes modular WMs to support scalable collaborative embodied decision.</p>
<p>C. Hardware</p>
<p>As embodied AI evolves, model complexity and size have grown, increasing computational and energy demands.Embodied systems, often operating in dynamic, real-world environments, face strict latency and power constraints-especially at the edge.Thus, developing hardware-friendly directions that maintain performance while optimizing efficiency is crucial for enabling responsive, energy-aware embodied agents.Hardware optimization in embodied AI typically includes four components: hardware-aware model compression, compiler-level optimization, domain-specific accelerators, and hardwaresoftware co-design.</p>
<p>1) Hardware-aware Model Compression: Quantization and pruning [4] are key techniques for reducing model size and computational cost.In embodied agents, which frequently run on low-power embedded hardware, such techniques are vital for enabling fast and efficient inference.Quantization [181] maps weights and activations to lower bit-widths, while pruning [182] removes redundant parameters.To support realworld embodied tasks, such as robotic control or visual navigation, hardware efficiency metrics like power, performance, and area (PPA) can guide bit-width allocation or pruning ratios [183], enabling task-specific trade-offs between accuracy and deployability on physical platforms.</p>
<p>2) Compiler-level Optimization: Compilers bridge highlevel embodied AI models and hardware execution.In realtime embodied systems, compiler toolchains are essential for efficient processing of sensor data and decision-making.TVM [5], built on LLVM [184] and CUDA, generates optimized code across platforms.These compilers transform computational graphs through operator fusion and redundant computation elimination [185], enabling responsive behavior.Mapping strategies like loop reordering and tiling enhance data locality, parallelism, and memory access [186], all of which are critical to maintaining low-latency inference in embodied agents.</p>
<p>3) Domain-specific Accelerators: With growing computational demands, domain-specific accelerators (DSAs) are a promising solution for embodied AI.These systems, from robots to AR/VR agents, benefit from fast, energy-efficient hardware tailored for frequent operations.Google's TPU [6], typically integrated with CPUs and GPUs via PCIe, accelerates key operations like matrix multiplication.FPGA-based accelerators [187] allow reconfigurability for adapting to new tasks or changing workloads; CGRA accelerators [188] improve structured, dataflow-heavy computations common in perception or control.Meanwhile, ASIC-based accelerators [189] offer high throughput and energy efficiency, ideal for deploying high-performance embodied models in real-world environments.</p>
<p>4) Hardware-software Co-design: Separating algorithm and hardware design can lower runtime efficiency.Hardwaresoftware co-design addresses this through algorithm-system and algorithm-hardware co-optimization.Algorithm-system co-optimization focuses on how to take full advantage of GPU resources like tensor cores and CUDA cores to better support the algorithm [190].Algorithm-hardware co-optimization aims to improve deployment efficiency by tuning both the model and the hardware architecture.For example, we can perform multi-objective optimization based on the types of operators in the network and the configuration parameters of the hardware [191].We can also design different numerical quantization schemes along with matching hardware accelerators to better support embodied AI tasks [192].</p>
<p>D. Benchmarks and Evaluation Metrics</p>
<p>Standardized benchmarks and evaluation metrics are crucial for objectively assessing the performance of embodied AI systems.Widely adopted testbeds include Habitat [193], which provides photorealistic 3D indoor environments for navigation and interaction tasks, and ManiSkill [194], offering physics-based manipulation scenarios with diverse object sets.Simulation platforms like MuJoCo [195] enable precise control evaluation in continuous state-spaces, while EmbodiedBench [18] supports holistic evaluation of visiondriven agents across perception, cognition, and interaction.For UAV applications, AirSim [196], U2UData [197] and U2USim [198] provides high-fidelity aerial environments with dynamic obstacles.These testbeds vary in complexity: Habitat excels in visual realism, ManiSkill in object diversity, MuJoCo in physical accuracy, and EmbodiedBench in multimodal integration.Domain-specific benchmarks like BEHAVIOR-1K [136] further enable granular evaluation of 1,000 everyday activities under realistic constraints.</p>
<p>Key evaluation metrics span three critical dimensions: Task Success Rate measures completion accuracy of goal-oriented objectives (e.g., object manipulation or navigation) [24]; Realtime Responsiveness quantifies decision latency and adaptation speed to environmental changes [199]; and Energy Efficiency evaluates computational cost (FLOPS) and power consumption (Watts) during deployment [4].Additional metrics include Path Length for navigation efficiency [105], Generalization Score for unseen scenarios [200], and Safety Violations for physical compliance [171].For multi-agent systems, Coordination Efficiency [178] and Communication Overhead [201] provide critical insights.Standardized evaluation protocols like those in MFE-ETP [168] ensure fair cross-modal comparisons, though challenges remain in sim-to-real transfer validation [177].</p>
<p>E. From Unimodal to Multimodal</p>
<p>The development of embodied AI has evolved from unimodal to multimodal systems, as shown in embodied AI was primarily concerned with individual modalities, such as vision, language, or action, where the perception, cognition, and interaction were driven by one sensory input [7], [8].As the field matured, the limitations of unimodal embodied AI became apparent, and there has been a significant shift toward integrating multiple sensory modalities [14]- [16].Multimodal embodied AI is now seen as crucial for creating more adaptive, flexible, and robust agents capable of performing complex tasks in dynamic environments [17], [18].</p>
<p>Unimodal embodied AI has benefited from rapid developments in fundamental areas such as computer vision, natural language processing, and reinforcement learning [12], [13].These unimodal methods excel in dealing with a specific module in embodied AI.For example, computer vision techniques have driven advances in visual SLAM and 3D scene understanding in the active perception module [9].Natural language processing techniques, especially LLM, have become popular solutions to address task planning and long-horizon reasoning in the embodied cognition module [10], [11].Although unimodal embodied AI performs well in independent modules, it always faces two inherent limitations.On the one hand, the information contained in a single modality is limited, hindering the performance of perception, cognition, and interaction.For example, visual-only systems struggle to understand environments in dynamic or ambiguous settings, while auditory-based systems face challenges in realworld noise and signal processing [17], [202].On the other hand, diverse and heterogeneous modalities hinder information transfer and sharing among modules.The agent's perception of the environment fails to facilitate the formation of its cognition, while the evolution of cognition fails to facilitate the interaction with the environment.</p>
<p>In contrast, multimodal embodied AI has emerged as a more promising paradigm [18].By integrating data from multiple sensing modalities, such as visual, auditory, and olfactory feedback, these methods can provide a more holistic and precise understanding of the environment.More importantly, multimodal embodied AI can facilitate deeper integration among perception, cognition, and interaction.Recent advances in MLLMs and WMs enable agents to more effectively handle multiple modalities, promising to improve the capabilities of embodied AI [44], [81], [203].The integration of these models is considered a key step toward enabling multimodal embodied AI in dynamic, uncertain environments.</p>
<p>III. EMBODIED AI WITH LLMS/MLLMS</p>
<p>This section provides a comprehensive overview of embodied AI with LLMs/MLLMs.We first elaborate in detail how LLMs boost embodied AI in Subsection III-A and how MLLMs boost embodied AI in Subsection III-B.Then we discuss the classification of MLLMs for embodied AI in Subsection III-C.</p>
<p>A. LLMs Boost Embodied AI</p>
<p>LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition.</p>
<p>1) Semantic Reasoning: Semantic reasoning [19], [204], [205] leverages LLMs to interpret semantics from text instructions by analyzing linguistic patterns [206], contextual relationships [207], and implicit knowledge [208].Through transformer architectures [63], LLMs map input tokens to latent representations, enabling hierarchical abstraction of meaning across syntactic and pragmatic levels.They employ attention mechanisms to weigh relevant semantic cues while suppressing noise, facilitating logical inference and analogical reasoning.By integrating world knowledge from pretraining corpora with task-specific prompts, LLMs dynamically construct conceptual graphs that align textual inputs with intended outcomes.This process supports multi-hop reasoning through probabilistic token prediction, resolving ambiguities by evaluating contextual coherence and semantic plausibility.</p>
<p>2) Task Decomposition: Task decomposition [20], [21] employs LLMs' sequential logic to break complex objectives into sub-tasks by hierarchically analyzing contextual dependencies and goal alignment.Leveraging chain-of-thought prompting, LLMs iteratively parse instructions into actionable steps, prioritizing interdependencies while resolving ambiguities through semantic coherence checks.</p>
<p>Representative works like SayCan [22] first provides a real-world pretrained natural language actions library, which is used to constrain LLMs to propose both feasible and contextually appropriate actions; then uses LLMs to convert natural language instructions into natural language action sequences; finally uses value functions to verify the feasibility of natural language action sequences in a particular physical environment.These works suggest that LLMs are extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language.However, LLMs are only a part of the entire embodied AI system, which is limited by a fixed natural language actions library and a specific physical environment, and it is difficult to achieve adaptive expansion in new robots and environments.</p>
<p>B. MLLMs Boost Embodied AI</p>
<p>MLLMs can act on the entire embodied AI system and can solve LLMs' problems well by bridging high-level multimodal inputting [209] and low-level motor action sequences [210] into end-to-end embodied applications (as shown in Fig. 5).</p>
<p>Compared with LLMs, semantic reasoning [28]- [30] leverages MLLMs' cross-modal comprehension to interpret semantics from visual, auditory, or tactile inputs, e.g., identifying objects, inferring spatial relationships, or predicting environmental dynamics.Concurrently, task decomposition [31]- [33] employs MLLMs' sequential logic to break complex objectives into sub-tasks while dynamically adapting plans based on sensor feedback.MLLMs mainly include Vision-Language Models (VLMs) and Vision-Language-Action models (VLAs).</p>
<p>1) VLMs for Embodied AI: VLMs for embodied AI integrate visual and language instruction understanding to enable physical or virtual agents to perceive their environments in goal-driven tasks [211]- [213].Representative works like PaLM-E [14] first train visual and language encodings end-toend, in conjunction with a pre-trained large language model; then incorporate the results of real-world continuous sensor modalities encodings into VLMs and establish the link between words and percepts; finally, achieve multi-task completion through fixed action space mapping.For navigation, ShapeNet [214], which fine-tunes contrastive embeddings for 3D spatial reasoning, greatly reduces path planning errors.These works suggest that VLMs can combine perception and reasoning in embodied AI to solve a large number of tasks with fixed action spaces.</p>
<p>2) VLAs for Embodied AI: VLAs integrate multimodal inputs with low-level action control through differentiable pipelines.Representative works like RT-2 [24] first encode the robot's current image, language instructions, and robot actions at a specific timestep and convert them into text tokens; then use LLMs for semantic reasoning and task decomposition; finally, de-tokenizes generated tokens into the final action.Octo [129] pretrains on 100K robot demonstrations with language annotations, achieving cross-embodiment tool use.For dexterous manipulation, PerAct [215] utilizes 3D voxel representations to reach millimeter-level grasp accuracy.These works suggest that VLAs can act on the entire embodied AI system and achieve adaptive expansion in new robots and environments.</p>
<p>C. Classification of MLLMs for Embodied AI</p>
<p>MLLMs can empower active perception, embodied cognition, and dynamic interaction of embodied AI.</p>
<p>1) MLLMs for Active Perception: First, MLLMs can enhance 3D SLAM.By grounding visual observations into semantic representations, MLLMs augment traditional SLAM pipelines with high-level contextual information such as object categories, spatial relations, and scene semantics [216], [217].Representative works like SEO-SLAM [218] utilize MLLMs to generate more specific and descriptive labels for objects, while dynamically updating a multiclass confusion matrix to mitigate biases in object detection.Second, MLLMs can enhance 3D scene understanding.Camera-based perception [30] remains the dominant setup in MLLM-driven embodied AI, as RGB inputs align naturally with the visuallanguage pretraining of many foundation models [219]- [221].Representative works like EmbodiedGPT [123] leverage this synergy to map 2D visual inputs into semantically rich features aligned with language-based goals.Finally, MLLMs can enhance active environment exploration.MLLMs have also revolutionized how robots interact with their environments, particularly in feedback-driven closed-loop interactions.Representative works like LLM 3 [222] focus on structured motionlevel feedback, which incorporates signals such as collision detections into the planning loop, allowing the model to iteratively revise symbolic action sequences.MART [223], on the other hand, leverages interaction feedback to improve retrieval quality.</p>
<p>2) MLLMs for Embodied Cognition: First, MLLMs can enhance task-driven self-planning [224]- [226].Embodied agents with MLLMs can either directly map high-level goals to structured action sequences [31], or adopt an intermediate planning strategy that continually interacts with the environment to refine their plans [32].Representative works like CoT-VLA [33] predict intermediate subgoal images that depict the desired outcomes of subtasks, helping the agent visualize and reason through each step of a complex task.Second, MLLMs can enhance memory-driven self-reflecting.MLLMs allow agents to learn from experience using this inherent memory module [129].Representative works like Reflexion [116] enhance agent performance through self-generated linguistic feedback, which is stored in an episodic memory buffer and leveraged to guide future planning.Finally, MLLMs can enhance embodied multimodal foundation models.MLLMs can be adapted to the physical world through continued pretraining or fine-tuning in embodied settings.Representative works include Qwen-VL [75] and InternVL [227], along with models supporting broader modality alignment, such as Qwen2.5-Omni[228].</p>
<p>World Models for Embodied AI</p>
<p>2019-2023-RSSM Transformer</p>
<p>2024-Diffusion</p>
<p>2024-Hierarchical</p>
<p>2023-JEPA</p>
<p>2018-Mental Model</p>
<p>2024-Sora</p>
<p>Internal</p>
<p>Representations of the External World</p>
<p>Future Predictions of the External World</p>
<p>3) MLLMs for Dynamic Interaction: First, MLLMs can enhance action control.MLLMs have ability to decompose complex tasks into actionable subtasks [32].To further produce continuous control signals for each subtask, MLLMs either generate actions autoregressively in a sequential manner [127], [229] or employ auxiliary policy heads to further process their internal representations [129].Recent advances also explore generating executable code with MLLMs [230], enabling robots to follow interpretable and adaptable control policies.Second, MLLMs can enhance behavioral interaction.Through interaction with the environment, MLLMs are also capable of generating sequences of behavioral actions in a single step.Representative works like π-0 [31] combine a vision-language backbone with a flow-matching decoder to produce smooth, temporally extended behavioral trajectories.Finally, MLLMs can enhance collaborative decision-making.One line of research focuses on multi-agent systems that aim to achieve human-level coordination and adapt rapidly to unforeseen challenges [231].For instance, Combo [146] introduces a novel framework that enhances cooperation among decentralized agents operating solely with egocentric visual observations.Other efforts investigate human-agent collaboration.VLAS [232] exemplifies this by aligning human verbal commands with visual context via a speech encoder and a LLaVA-style MLLM [233], enabling fluid and conversational human-agent interaction.</p>
<p>IV. EMBODIED AI WITH WORLD MODELS</p>
<p>This section provides a comprehensive overview of embodied AI with WMs.We first elaborate in detail how WMs boost embodied AI in Subsection IV-A.Then we discuss the classification of WMs for embodied AI in Subsection IV-B.</p>
<p>A. World Models Boost Embodied AI</p>
<p>WMs empower embodied AI by building internal representations and future predictions of the external world (as shown in Fig. 6), facilitating physical law-compliant embodied interactions in dynamic environments.</p>
<p>1) Internal Representations of the External World: Internal representations compress rich sensory inputs into structured latent spaces, capturing object dynamics, physics laws, and spatial structures, allowing agents to reason about "what exists" and "how things behave" in their surroundings.These latent embeddings preserve hierarchical relationships [234] between entities and environments, mirroring the compositional nature of reality itself.The structured nature of these representations facilitates generalization across environments, as abstracted principles (like gravity or object permanence) transcend specific instances.Moreover, they support counterfactual reasoning [40] by maintaining disentangled variables for objects' intrinsic properties [38] and extrinsic relations [39], enabling flexible mental manipulation of individual components.This disentanglement also enhances sample efficiency in learning, as agents transfer knowledge between tasks, sharing latent factors.World models with rich internal representations, can introspect on their own uncertainty about environmental states and actively seek information to resolve ambiguities.By encoding temporal continuity and spatial topology [36], these models naturally enforce consistency constraints during planning, filtering physically implausible actions before execution.Ultimately, such structured latent spaces act as cognitive scaffolding for building causal understanding [37], mirroring how humans develop intuitive theories about their world through compressed sensory experiences.</p>
<p>2) Future Predictions of the External World: Future predictions simulate potential rewards of sequence actions across multiple time horizons aligned with physical laws, thereby preempting risky or inefficient behaviors [41], [42].This predictive capacity bridges short-term actions with long-term goals [43], filtering out trajectories violating physical plausibility (e.g., walking through walls) or strategic coherence (e.g., depleting resources prematurely).Long-horizon prediction [44] allows adaptive balancing of exploration-exploitation tradeoffs, simulating distant outcomes to avoid local optima while maintaining focus on actionable near-term steps.Crucially, these predictions incorporate uncertainty quantification [41], [235], distinguishing predictable regularities (daily patterns) from stochastic events (sudden changes) to optimize riskaware planning.The simulation prediction improves sample efficiency [39], [236]- [238] by replacing costly trial-anderror with mental rehearsal, particularly valuable in safetycritical domains like autonomous driving or robotic surgery.Furthermore, continuous prediction-error minimization drives iterative model refinement [170], [239]- [241], creating selfcorrecting systems that align their internal physics simulators with observed reality.Such anticipatory capabilities ultimately grant artificial agents human-like foresight, transforming reactive responses into purposeful, future-optimized behaviors.</p>
<p>B. Classification of World Models for Embodied AI</p>
<p>Embodied AI with WMs can mainly be divided into three critical structures: the Recurrent State Space Model-based (RSSM-based) WMs for embodied AI, the Joint-Embedding Predictive Architecture-based (JEPA-based) WMs for embodied AI, and the Transformer-based WMs for embodied AI.Hierarchical-based WMs [242] and diffusion-based WMs [243] are similar to other structures and are shown in Fig. 6.</p>
<p>1) RSSM-based WMs for Embodied AI: RSSM constitutes the fundamental architecture underpinning the Dreamer algorithm family [41]- [44].This framework enhances predictive capabilities in latent representations by acquiring temporal environment dynamics through visual inputs, subsequently enabling action selection via latent trajectory optimization.Through orthogonal decomposition of hidden states into probabilistic and deterministic components, the architecture explicitly accounts for both systematic patterns and environmental uncertainties.Its demonstrated effectiveness in robotic motion control applications has inspired numerous derivative studies building upon its theoretical framework.</p>
<p>2) JEPA-based WMs for Embodied AI: JEPA [27] provides a structure for developing autonomous machine intelligence systems.This architecture establishes mapping relationships between input data and anticipated outcomes through representation learning.Diverging from conventional generative approaches, JEPA operates in abstract latent spaces rather than producing pixel-wise reconstructions, thereby prioritizing semantic feature extraction over low-level signal synthesis.A key methodological foundation of JEPA [235] involves selfsupervised training paradigms where neural networks learn to infer occluded or unobserved data segments.Such pretraining on extensive unlabeled datasets enables transfer learning across downstream applications, demonstrating enhanced generalization capabilities for both visual [244], [245] and non-visual domains [246].</p>
<p>3) Transformer-based WMs for Embodied AI: Originating in natural language processing research, the Transformer structure [63] fundamentally relies on attention mechanisms to process input sequences through parallelized context weighting.This design allows simultaneous computation of interelement dependencies, overcoming the sequential processing constraints inherent in Recurrent Neural Networks (RNNs).Empirical evidence demonstrates superior performance in domains requiring persistent memory retention and explicit memory addressing for cognitive reasoning [247], which has propelled its adoption in reinforcement learning research since 2020.Existing advancements have successfully implemented WMs using Transformer variants [38], [40], [248], outperforming RSSM architectures in memory-intensive interactive scenarios [37].Notably, Google's Genie framework [36] employs the Spatial-Temporal Transformer (ST-Transformer) [249] to create synthetic interactive environments through large-scale self-supervised video pretraining.This breakthrough establishes novel paradigms for actionable world modeling, revealing transformative potential for WMs development trajectories.</p>
<p>V. EMBODIED AI WITH MLLMS AND WMS</p>
<p>This section provides a comprehensive overview of embodied AI with MLLMs and WMs.We first elaborate in detail on the limitations of MLLMs and WMs for embodied AI and explain how MLLMs boost WMs reasoning, and how WMs boost MLLMs interaction in Subsection V-A.Then we design a joint MLLM-WM-driven embodied AI architecture in Subsection V-B.Finally, we discuss the advantages and challenges of new architecture in Subsection V-C.</p>
<p>A. MLLMs and WMs</p>
<p>MLLMs enable contextual task reasoning but overlook physical constraints, while WMs excel at physics-aware simulation but lack high-level semantics.Their joint bridges semantic intelligence with grounded physical interaction.</p>
<p>1) The Limitations of MLLMs for Embodied AI (without WMs): MLLMs exhibit two critical limitations in embodied AI applications.First, they often fail to ground predictions [34] in physics-compliant dynamics, leading to impractical plans.For example, ignoring friction or material properties when manipulating objects may cause slippage or task failure.Second, their poor real-time adaptation to environmental feedback limits responsiveness [35].While MLLMs excel at semantic task decomposition, they struggle to adaptively adjust actions when the environment changes dramatically.These limitations stem from their reliance on static, pre-trained knowledge rather than continuous physical interaction.</p>
<p>2) The Limitations of WMs for Embodied AI (without LLMs/MLLMs): WMs face limitations in abstract reasoning and generalization.They struggle with open-ended semantic tasks [45] due to their focus on physical simulation rather than contextual understanding.Additionally, WMs lack generalizable task decomposition [26] without explicit priors.For example, a WM model trained on rigid-object manipulation may fail to adapt to deformable materials without extensive retraining.Their predictive accuracy heavily depends on domainspecific interaction records, hindering scalability across diverse environments.</p>
<p>3) MLLMs Boosting WMs Reasoning: By leveraging crossmodal alignment and semantic grounding, MLLMs enable WMs to process complex environments dynamically, improving semantic reasoning, task decomposition, and human-robot interaction.1) MLLMs can enrich WMs by fusing visual, auditory, and textual data into unified semantic representations.For instance, CLIP-based architectures [250] enable agents to align visual scenes with linguistic cues, reducing ambiguity in object recognition [251].2) MLLMs can augment WM's task decomposition capacity by decomposing high-level goals into executable sub-tasks.Models like GPT-4V [252] generate stepby-step plans using environmental context stored in WM.For robotic manipulation, Code-as-Policies [253] translates natural language instructions into code snippets, leveraging WM to track intermediate states.3) MLLMs enable WMs to refine internal representations through human feedback.Techniques like Reinforcement Learning with Human Feedback (RLHF) [73] allow agents to update WM priors based on corrective inputs [116].Those works in this Subsubsection are all possible ways for MLLMs to boost WMs reasoning, which is not achieved in existing works.</p>
<p>4) WMs boosting MLLMs Interaction:</p>
<p>WMs can play a pivotal role in refining MLLMs by providing physical laws, spatio-temporal relationships, and closed-loop interaction experiences.WMs can mitigate MLLMs' inherent limitations in temporal coherence and environmental grounding, enabling more robust decision-making in dynamic embodied tasks.1) WMs can provide MLLMs with explicit representations of physical laws (e.g., gravity, friction) and commonsense rules to constrain action proposals.For example, Physion++ [254] integrating WM-stored biomechanical models can be used to filter MLLM-generated robotic motions violating torque limits; RoboGuide [255] injects spatial occupancy maps into MLLM planners, preventing collisions during navigation.2) WMs can stabilize MLLMs reasoning by maintaining spatiotemporal context during multimodal processing.For instance, MemPrompt [256] can use WM buffers to align visual object trajectories with linguistic descriptions, resolving ambiguities in cluttered environments; RoboMem [257] can leverage WMprioritized attention to filter irrelevant sensory noise, improving MLLM-based scene understanding.3) WMs can enable iterative refinement of MLLM outputs through closed-loop interaction.Reflexion [116] can store task-execution histories in WM, allowing MLLMs to correct kinematic errors using failure patterns [253].Those works in this Subsubsection are all possible ways for WMs to boost MLLMs' decisions, which has not been achieved in existing works.</p>
<p>B. Joint MLLM-WM-driven Embodied AI Architecture</p>
<p>We propose a joint MLLM-WM-driven embodied AI architecture (as shown in Fig. 7), shedding light on their profound significance in enabling complex tasks within physical worlds.The specific workflow is as follows, with arrows highlighting the data exchange process.</p>
<p>1) Robots → Self-State Inputing → MLLMs/WMs → Hardware Embodiment → Robots: The process initiates with selfstate inputting tracking proprioceptive metrics, such as degrees of freedom, number of sensors, etc.These metrics feed into both WMs and MLLMs: WMs use them to build internal representations of the agent's physical state, while MLLMs contextualize these states for task alignment.Hardware embodiment is focused on implementing WMs and MLLMs into physical devices to solve sim-to-real problems.This bidirectional flow ensures actions respect both mechanical limits and high-level goals.</p>
<p>2) MLLMs → Task Planning → WMs → Memory Updating → MLLMs: MLLMs decompose abstract instructions into subtasks.A forward arrow delivers this plan to WMs, which predict outcomes based on existing environmental modeling.During execution, WMs log outcomes into memory.A vertical arrow transmits these logs to memory updating modules, which structure memory into experiences, represent the forgetting of past task memories, the renewal of current task memories, and the prediction of future task memories.These are then fed back to MLLMs via an arrow, enriching their knowledge base.This enables lifelong learning, where past failures directly inform future planning.</p>
<p>3) Environments → Active Perception → MLLMs/WMs → Dynamic Interaction → Environments: WMs first drive active perception by predicting key environmental changes.Multimodal inputs are then used to construct an internal representation of the external world through WMs and semantic reasoning through MLLMs.Then, the task decomposition of MLLMs and future prediction of WMs enable action selection and environmental interaction.Adaptive perception and interaction of dynamic environments are achieved through continuous iteration.The challenges of joint MLLM-WM-driven embodied AI architecture include 1) real-time synchronization between MLLMs' high-latency semantic processing and WMs' physics-based representation, often leading to delayed responses in dynamic environments; 2) semantic-physical misalignment, where MLLM-generated plans violate unmodeled physical constraints; and 3) scalable memory management, as continuous updates to WM's internal states risk overwhelming MLLMs with irrelevant context.Additionally, training such systems requires vast multimodal datasets covering rare edge cases, while ensuring robustness against sensor noise and partial observability remains unsolved.These challenges need lightweight MLLMs inference, tighter feedback loops, and dynamic context-filtering mechanisms to minimize latency.</p>
<p>VI. EMBODIED AI APPLICATIONS</p>
<p>This section overviews the application of embodied AI in service robots, rescue robots, and other domains, highlighting trends in joint MLLMs and WMs to advance active perception, embodied cognition and dynamic interaction.</p>
<p>A. Service Robotics</p>
<p>Embodied AI is becoming an important technology in the service field.It helps service robots go beyond fixed rules and perform tasks in a flexible way using different types of information.Recent research [258], [259] highlights its flexible applications across various fields.In domestic settings, systems such as RT-2 [229] and SayCan [121] combine language instructions with robot control, allowing robots to do tasks such as stacking dishes or cooking.Few-shot learning methods like AED [260] acquire new skills from limited demonstrations.In healthcare, robots with multiple types of input can help with reminders, rehabilitation, and companionship.[261], [262].In public environments, platforms like Habitat [193] and RT-X [263] support navigation and item delivery, even in changing environments, without needing special training for each task.This makes the system more general and useful in real life.</p>
<p>However, current approaches remain limited in handling long-horizon tasks.As illustrated in Fig. 7, the joint of WMs and MLLMs is emerging as a key strategy for enhancing the autonomy and long-term reasoning capabilities of service robots.The WM maintains an evolving environment model for planning and simulation, while the MLLM grounds commands like "clean up the living room" into adaptive subtasks.This collaboration supports flexible reasoning, goal adaptation, and robust real-world execution.</p>
<p>B. Rescue UAVs</p>
<p>Embodied AI technology technology is changing the way drones are used in disaster situations.Traditional drones are either manually controlled or rely on pre-built maps when in use, which leads to their inability to adapt to the environment independently.However, embodied drones [264], [265] can sense the environment in real time and respond to sudden changes.This ability makes them very useful in dangerous places like earthquake zones, forest fires, or floods.Recent studies show that embodied drones can perform many complex tasks.For instance, with the help of language models, they can understand and follow human voice instructions, helping drones quickly change their actions and enhancing their responses in emergency situations, such as "search near the collapsed bridge" [115], [266]- [269].Secondly, some work use world models to simulate dangerous environments, which helps them avoid danger and plan a safer [270]- [272].Other studies explore how multiple drones can work together to find survivors and map damaged areas [201], [273], [274].</p>
<p>However, despite these advancements, current approaches remain limited in handling long-horizon reasoning and autonomous decision-making under uncertainty.As illustrated in Fig. 7, jointing WMs and MLLMs has emerged as a key strategy for further enhancing UAV autonomy.The WM maintain a continuously evolving spatiotemporal representation of the environment, supporting planning and risk prediction even in GPS-denied conditions.The MLLM grounds commands into structured subtasks based on the UAV's belief state.This coordination improves generalization, long-horizon reasoning, and high-level autonomy in mission-critical conditions.</p>
<p>C. Industrial Robots</p>
<p>Embodied AI is changing the way robots work in factories.With embodied AI, industrial robots [275] can make smarter decisions based on their surroundings.Traditional industrial robots are usually fixed in one place.They use special sensors and tools and are required to complete tasks with very high accuracy.Because of this, they are better at doing jobs that need the same movements again and again.</p>
<p>However, with embodied AI, these robots can do more than repeat actions.By combining MLLMs and WMs, industrial robots can adjust how hard they hold fragile objects, or find a new path when they meet an obstacle.This has already been used in real life.For example, robots in Tesla's factory can find and fix parts that are not lined up, without help from people.At JD, robots [276], [277] use different sensors to sort packages by size and address.In Tmall's warehouse [278], robots use thermal cameras, LiDAR, and RGB sensors to check for problems in the inventory and send alerts when something is wrong.These examples show that embodied AI is helping robots become more flexible, reliable, and smart in factories.</p>
<p>D. Other Applications</p>
<p>In addition to its use in homes, healthcare, and rescue missions, embodied AI is also being applied in educational, virtual, and space environments [279].In smart manufacturing, it supports robots that can work together with humans, perform accurate assembly tasks, and adapt their actions based on changes in the workspace or human behavior [280]- [282].With the help of visual and touch feedback, these robots can safely handle fragile items [283], [284].In education, embodied AI is used in social robots that adjust their speech, gaze, and gestures according to the student's focus and emotions [285]- [287].This helps create a more personalized learning experience and builds long-term trust between students and robots [288], [289].In virtual environments, embodied agents learn to move, interact with objects, and complete tasks that require several steps.They also develop memory over time to improve their performance [290].In space exploration, where conditions are unknown and communication with Earth is delayed, embodied AI allows robots to make decisions on their own and adapt to new surroundings [291].These examples show that embodied AI is becoming more flexible and useful across many fields, helping machines see, act, and learn in both real and virtual worlds.</p>
<p>VII. FUTURE DIRECTIONS</p>
<p>As embodied AI moves from simulation to real-world deployment, future research must prioritize the development of unified and reliable systems across several core domains.Key directions include autonomous embodied AI, embodied AI hardware, swarm embodied AI, and evaluation benchmark.</p>
<p>A. Autonomous Embodied AI</p>
<p>The purpose of autonomous embodied AI is to enable agents to operate independently for a long time in a dynamic and open environment.Future research is expected to develop along several key directions.First, adaptive perception can give the system the ability to autonomously select input data, which can be achieved by dynamically choosing and integrating information from different sensory modalities.Second, Building on this foundation, building environmental awareness is essential.Environmental awareness helps agents quickly adapt to changes, predict the consequences of their actions, and transfer their behavior to new environments.It requires memory architectures that can capture spatiotemporal patterns and model causal relationships.Third, future systems should combine MLLMs with real-time physical interaction, which allows agents to bridge high-level language instructions with low-level control, and accurately model the real physical world.</p>
<p>B. Embodied AI Hardware</p>
<p>Future research in embodied AI hardware is expected to advance in the following four directions.First, hardwareaware model compression will continue to integrate techniques such as quantization and pruning with hardware performance metrics, enabling precise control over the trade-off between model accuracy and deployment efficiency.Second, graphlevel compilation optimization will play a key role in bridging the gap between high-level embodied models and low-level hardware execution, which will focus on more effective operator fusion, scheduling strategies, and memory access efficiency to reduce execution overhead.Third, domain-specific accelerators will be increasingly tailored to the computational characteristics of embodied tasks.Reconfigurable architectures such as FPGA and CGRA offer flexibility and adaptability, while ASIC-based designs provide high efficiency and performance.Fourth, hardware-software co-design will become essential for eliminating mismatches between algorithm behavior and hardware architecture.Joint optimization of model structures and hardware architecture will be critical to achieving realtime, energy-efficient execution in embodied systems.</p>
<p>C. Swarm Embodied AI</p>
<p>Swarm embodied AI refers to the collaborative perception and decision-making of multiple agents.refers to the collaborative perception and decision-making of multiple agents.Because multiple agents can exhibit stronger capabilities when cooperating than a single agent, this kind of "collective intelligence" has aroused the interest of many researchers and is also regarded as an important step for agents to approach humans.First of all, to enable multiple agents to cooperate smoothly, it is necessary to develop collaborative WMs.This model can establish a shared and dynamic environmental representation based on the observations of each agent, forming the basis of collective understanding.Secondly, multi-agent representation learning is very important.It can help the agent understand its own state and also comprehend the situations of other agents.This is the basis for communication and cooperation among agents.In addition, modeling social behavior among agents is also crucial.Role allocation and group decisionmaking can be better achieved through behavioral modeling.Finally, to seamlessly integrate into real-world applications, it is also important to design natural human-swarm interaction interfaces.It may include multimodal language foundations and get-based control methods, making it easier for humans to direct and guide the entire agent group.</p>
<p>D. Explainability and Trustworthiness Embodied AI</p>
<p>Explainability and trustworthiness represent a critical frontier for Embodied AI, essential for its safe, ethical, and widespread real-world deployment as agents increasingly interact physically with humans and dynamic environments.Future research must address several key challenges: Firstly, designing benchmarks that provide real-time, humanunderstandable justifications for agent actions, particularly during unexpected situations or failures, is crucial for user trust and debugging.Secondly, establishing robust mechanisms to ensure agents adhere to ethical principles and human values during autonomous decision-making, especially in morally ambiguous scenarios common in rescue or healthcare applications, requires significant advancement.Thirdly, creating verifiable safety guarantees and certification standards for agents operating in unstructured physical settings, mitigating risks associated with unpredictable interactions, remains an open problem.Finally, enhancing robustness against adversarial attacks, sensor noise, and distribution shifts, ensuring reliable performance despite uncertainties inherent in the real world, is fundamental for trustworthy operation.Addressing these multifaceted research problems in explainability and trustworthiness is paramount, as progress in this direction will unlock the full potential of Embodied AI by fostering user confidence, enabling responsible innovation, and facilitating regulatory acceptance.</p>
<p>E. Other Directions</p>
<p>Several new directions may influence the future development of embodied AI.One important direction is lifelong learning.Agents need to continuously learn new skills without forgetting what they have already learned.Only in this way can they adapt to the dynamic environment and maintain the accuracy of the previously completed tasks.Another key direction is human-in-the-loop learning.Human feedback is very important supervisory information.A small amount of feedback can significantly improve the performance of an agent and make it more human-like.To achieve this goal, we need better methods to enable agents to understand human goals and preferences.Finally, as agents become more autonomous, moral decision-making becomes increasingly important.Future systems should learn to carefully identify moral hazard and follow human values.This will help ensure that the embedded artificial intelligence is both safe and reliable.</p>
<p>Fig. 1 .
1
Fig. 1.The concept of embodied AI.</p>
<p>Fig. 3 .
3
Fig. 3. Key technological models of embodied AI.Advancements in Computer Vision (CV) models, Natural Language Processing (NLP) models, Reinforcement Learning (RL) models, LLMs/MLLMs, and WMs have driven progress in embodied AI.</p>
<p>FigFig. 4 .
4
Fig. 4. Unimodal embodied AI and multimodal embodied AI.(a) Unimodal methods focus on specific modules of embodied AI.They are limited by the narrow scope of information provided by each modality and the inherent gaps between modalities across modules.(b) Multimodal embodied AI methods break these limitations and enable the mutual enhancement of the modules.</p>
<p>Fig. 5 .
5
Fig. 5.The development roadmap of MLLMs for embodied AI.This roadmap highlights the key milestones in their conceptual and practical development.</p>
<p>Fig. 6 .
6
Fig. 6.The development roadmap of WMs for embodied AI.This roadmap highlights the key milestones in their conceptual and practical development.</p>
<p>Fig. 7 .
7
Fig. 7. Embodied AI with MLLMs and WMs.MLLMs can enhance WMs by injecting semantic knowledge for task decomposition and long-horizon reasoning, while WMs can assist MLLMs by building the physical world's internal representations and future predictions, making joint MLLM-WM a promising architecture for embodied systems.</p>
<p>TABLE I COMPARISON
I
OF THREE CATEGORIES OF ACTIVE PERCEPTION METHODS INCLUDING VISUAL SLAM, 3D SCENE UNDERSTANDING, AND ACTIVE ENVIRONMENT EXPLORATION.
CategoryMethodYearSensor TypeFeature TypeApplicable ScenariosCoSLAM [90]2012RGB-DGeometric + VolumetricDynamic SLAMSLAM++ [93]2013RGB-DSemanticObject-level MappingVisual SLAMORB-SLAM [8]2015RGB-D + StereoGeometricDynamic SLAMDS-SLAM [94]2018RGB-DGeometric + SemanticDynamic SLAMTwistSLAM [95]2022RGB-D + StereoGeometric + SemanticDynamic SLAMGS-SLAM [96]2024RGB-DVolumetricObject-level MappingGaudi [97]2022RGBVolumetricGeneral Scene UnderstandingClip2Scene [98]2023 RGB + Point CloudMultimodalLanguage-guided Scene Understanding3D SceneOpenScene [99]2023 RGB + Point CloudMultimodalGeneral Scene UnderstandingUnderstandingLexicon3D [100]2024RGB-DSemanticLanguage-guided Scene UnderstandingGraphDreamer [101]2024RGBTopological + SemanticStructured Scene ReasoningHUGS [102]2024RGB-DMultimodalGeneral Scene UnderstandingRegionPLC [103]2024 RGB + Point CloudMultimodalLanguage-guided Scene UnderstandingMAX [104]2019RGBSemanticSemantic-guided ExplorationActive Neural SLAM [105] 2020RGB-DVolumetricGeometry-based ExplorationActive EnvironmentAPT [106]2021RGBSemanticSemantic-guided ExplorationExplorationConan [107]2023RGBTopologicalGeometry-based ExplorationDBMF-BPI [108]2023RGB-DVolumetricGeometry-based ExplorationActiveRIR [109]2024RGB + AudioMultimodalCross-modal Active Perception</p>
<p>TABLE II COMPARISON
II
OF THREE CATEGORIES OF EMBODIED COGNITION METHODS: TASK-DRIVEN SELF-PLANNING, MEMORY-DRIVEN SELF-REFLECTION, AND EMBODIED MULTIMODAL FOUNDATION MODELS.I, L AND P INDICATE THE IMAGE, LANGUAGE AND POINT CLOUD MODALITIES, RESPECTIVELY.
CategoryMethodYear Input ModalitiesCognition TypeReasoning ModeOutputL3P [110]2021I + LPlannerNeural + SymbolicActionLLM-Planner [111]2023I + LPlannerNeural + SymbolicActionTask-drivenEgoplaner [112]2023IPlannerSymbolicActionSelf-planningAutoAct [113]2024LPlannerNeuralActionRPG [114]2024I + LPlannerNeuralPolicyETPNav [115]2024I + LPlannerNeural + SymbolicPolicyReflexion [116]2023LMemoryBeam + ReplayPolicyReflect [117]2023I + LMemoryNeural + SymbolicPolicyMemory-drivenRILA [118]2024LMemoryNeuralPolicySelf-reflectionOptimus-1 [119]2024I + LMemoryNeuralPolicyEvoAgent [46]2025I + LMemoryNeuralPolicyREMAC [120]2025LMemoryNeural + SymbolicPolicySayCan [121]2022I + LPlanner + AlignerNeuralAnswer + ActionGATO [122]2022I + L + PAlignerNeuralActionEmbodied MultimodalEmbodiedGPT [123]2023I + LAlignerNeuralAnswer + ActionFoundation ModelsKosmos-2 [124]2023I + LAlignerNeuralAnswerMultiPLY [125]2024I + LAlignerNeuralAnswerManipLLM [28]2024I + LAlignerNeuralAnswer + Action</p>
<p>TABLE III COMPARISON
III
OF THREE CATEGORIES OF DYNAMIC INTERACTION METHODS INCLUDING ACTION CONTROL, BEHAVIORAL INTERACTION, AND COLLABORATIVE DECISION-MAKING, ACROSS INPUT MODALITIES, INTERACTION TYPE, MODELING PARADIGM, AND TASK TYPE.I, L, S, P, AND T DENOTE IMAGE, LANGUAGE, STATE, PROPRIOCEPTION, AND TRAJECTORY, RESPECTIVELY.IL DENOTES IMITATION LEARNING.
CategoryMethodYear Input ModalitiesInteraction TypeLearning ParadigmTask TypeMineDojo [126]2022I + LHigh-level PlanningLLMInstruction FollowingPaLM-E [14]2023I + L + PLow-level ControlMLLMEmbodied ManipulationRT-2 [24]2023I + LLow-level ControlVLAEmbodied ManipulationActionOpenVLA [127]2024I + LLow-level ControlVLAEmbodied ManipulationControlCogagent [128]2024I + LLow-level ControlMLLMInstruction FollowingOcto [129]2024I + L + PLow-level ControlVLAEmbodied ManipulationCrossFormer [130]2024I + LLow-level ControlVLAEmbodied ManipulationHPT [131]2024I + LLow-level ControlVLAEmbodied ManipulationGAIL [132]2016TBehavioralILTrajectory LearningMGAIL [133]2017TBehavioralILTrajectory LearningTrafficSim [134]2021TBehavioralRLTrajectory LearningBehavioral InteractionTrajGen [135]2022I + TBehavioralRLTrajectory LearningBehavior-1K [136]2023ITrajectoryILBehavior UnderstandingAgentLens [137]2024I + STrajectoryILBehavior UnderstandingECL [138]2024I + LHigh-level PlanningILEmbodied ManipulationQMIX [139]2018SBehavioralRLCooperative DecisionQtran [140]2019SBehavioralRLCooperative DecisionQPLEX [141]2019SBehavioralRLCooperative DecisionCollaborativeMAT [142]2022SBehavioralRLCooperative DecisionDecisionCoELA [143]2024I + LLow-level ControlLLMCooperative ManipulationAgentVerse [144]2024LHigh-level PlanningLLMAgent Society SimulationMetaGPT [145]2024LHigh-level PlanningLLMAgent Society SimulationCombo [146]2024LHigh-level PlanningLLMCooperative Planning</p>
<p>TABLE IV QUALITATIVE
IV
COMPARISON OF MLLM-ONLY, WM-ONLY, AND JOINT MLLM-WM ARCHITECTURES IN EMBODIED AI.LOW , MEDIUM , HIGH .
PerformanceLLM/MLLM-onlyWM-onlyJoint MLLM-WMSemantic UnderstandingAdvantages in contextual task reasoning and natural language understandingLimited in open-ended semantic understandingCombines high-level semantic abstraction with grounded contextual alignmentTask DecompositionSequential logic enables sub-task planning via language promptsLacks generalizable task decomposition mechanismsSemantic plans refined through physical feasibility via joint planning-execution loopPhysics ComplianceIgnores physical constraints and dynamics in real-world interactionPhysics-aware simulation with temporal consistencyEnforces semantic-physical alignment for safe and executable plansFuture PredictionLacks imagination-based reasoningLong-horizon multi-step prediction with uncertainty modelingCombines symbolic foresight and physically grounded imaginationReal-time InteractionPoor responsiveness to environmental feedback and significant reasoning latencySupports real-time predictive control via future state simulationEnables online adaptation through iterative plan refinement and memory updatingMemory StructureSparse and unstructured memoryStructured latent space encodes object dynamics and causal relationshipsIntegrates semantic memory and world modeling for lifelong learning and reflectionScalabilityLimited to pre-trained task spacePoor transfer to unseen tasks without retrainingCross-task, cross-domain generalization through symbolic and sensorimotor synergyC. DiscussionsJoint MLLM-WM offer a promising architecture for embod-ied AI. As shown in TABLE IV, MLLMs excel in semanticreasoning, enabling high-level task decomposition, contextualunderstanding, and adaptive planning by leveraging multi-modal inputs. Meanwhile, WMs provide grounded, physics-based simulations of environments, ensuring actions alignwith real-world constraints. This synergy allows agents tobalance abstract reasoning with real-time physical interac-tions, enhancing decision-making in dynamic settings. Forinstance, MLLMs can generate task plans while WMs validatefeasibility, enabling iterative refinement. Additionally, jointarchitectures support cross-modal generalization, improvingrobustness in partially observable or novel scenarios by bridg-ing symbolic knowledge and sensorimotor experiences.
ACKNOWLEDGEMENT This work is supported by National Natural Science Foundation of China No.62222209, Beijing National Research Center for Information Science and Technology under Grant No.BNR2023TD03006, China Postdoctoral Science Foundation under Grant No.2024M751688, Postdoctoral Fellowship Program of CPSF under Grant No.GZC20240827.We thank Dr. Ren Wang and Dr. Mingzi Wang for their contribution to Section II.We thank Dr. Yu-Wei Zhan for her contributions to Section VI and Section VII.
Computing machinery and intelligence. A M Turing, Mind. 592361950</p>
<p>How the body shapes the way we think: a new view of intelligence. R Pfeifer, J Bongard, 2006MIT press</p>
<p>Being there: Putting brain, body, and world together again. A Clark, 1998MIT press</p>
<p>Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. S Han, H Mao, W J Dally, International Conference on Learning Representations. 2015</p>
<p>Tvm: An automated end-to-end optimizing compiler for deep learning. T Chen, T Moreau, Z Jiang, L Zheng, E Yan, H Shen, M Cowan, L Wang, Y Hu, L Ceze, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 2018</p>
<p>In-datacenter performance analysis of a tensor processing unit. N P Jouppi, C Young, N Patil, D Patterson, G Agrawal, R Bajwa, S Bates, S Bhatia, N Boden, A Borchers, Proceedings of the 44th annual international symposium on computer architecture. the 44th annual international symposium on computer architecture2017</p>
<p>Lsd-slam: Large-scale direct monocular slam. J Engel, T Schöps, D Cremers, European conference on computer vision. Springer2014</p>
<p>Orb-slam: A versatile and accurate monocular slam system. R Mur-Artal, J M M Montiel, J D Tardos, IEEE transactions on robotics. 3152015</p>
<p>A comprehensive survey of visual slam algorithms. A Macario Barros, M Michel, Y Moline, G Corre, F Carrel, Robotics. 111242022</p>
<p>Z Wu, Z Wang, X Xu, J Lu, H Yan, arXiv:2307.01848Embodied task planning with large language models. 2023arXiv preprint</p>
<p>Omnidrive: A holistic llm-agent framework for autonomous driving with 3d perception, reasoning and planning. S Wang, Z Yu, X Jiang, S Lan, M Shi, N Chang, J Kautz, Y Li, J M Alvarez, arXiv:2405.015332024arXiv preprint</p>
<p>Learning to look around: Intelligently exploring unseen environments for unknown tasks. D Jayaraman, K Grauman, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>On grounded planning for embodied tasks with language models. B Y Lin, C Huang, Q Liu, W Gu, S Sommerer, X Ren, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023. 20037</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, International Conference on Machine Learning. PMLR2023</p>
<p>Multimodal llm guided exploration and active mapping using fisher information. W Jiang, B Lei, K Ashton, K Daniilidis, arXiv:2410.174222024arXiv preprint</p>
<p>Actiview: Evaluating active perception ability for multimodal large language models. Z Wang, C Chen, F Luo, Y Dong, Y Zhang, Y Xu, X Wang, P Li, Y Liu, arXiv:2410.046592024arXiv preprint</p>
<p>Mp5: A multi-modal open-ended embodied system in minecraft via active perception. Y Qin, E Zhou, Q Liu, Z Yin, L Sheng, R Zhang, Y Qiao, J Shao, 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE2024316</p>
<p>Embodiedbench: Comprehensive benchmarking multi-modal large language models for visiondriven embodied agents. R Yang, H Chen, J Zhang, M Zhao, C Qian, K Wang, Q Wang, T V Koripella, M Movahedi, M Li, arXiv:2502.095602025arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in neural information processing systems. 202235</p>
<p>Gpt-3: Its nature, scope, limits, and consequences. L Floridi, M Chiriatti, 2020Minds and Machines</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Progprompt: program generation for situated robot task planning using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, Autonomous Robots. 4782023</p>
<p>Rt-2: Visionlanguage-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Understanding world or predicting future? a comprehensive survey of world models. J Ding, Y Zhang, Y Shang, Y Zhang, Z Zong, J Feng, Y Yuan, H Su, N Li, N Sukiennik, arXiv:2411.144992024arXiv preprint</p>
<p>World models. D Ha, J Schmidhuber, arXiv:1803.101222018arXiv preprint</p>
<p>A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Y Lecun, Open Review. 6212022</p>
<p>Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. X Li, M Zhang, Y Geng, H Geng, Y Long, Y Shen, R Zhang, J Liu, H Dong, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>A survey of embodied ai: From simulators to research tasks. J Duan, S Yu, H L Tan, H Zhu, C Tan, IEEE Transactions on Emerging Topics in Computational Intelligence. 2022</p>
<p>A review of the impact of rain on camera-based perception in automated driving systems. T Brophy, D Mullins, A Parsi, J Horgan, E Ward, P Denny, C Eising, B Deegan, M Glavin, E Jones, IEEE Access. 2023</p>
<p>π0: A vision-languageaction flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, 2024</p>
<p>W Huang, C Wang, Y Li, R Zhang, L Fei-Fei, arXiv:2409.01652Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. 2024arXiv preprint</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Q Zhao, Y Lu, M J Kim, Z Fu, Z Zhang, Y Wu, Z Li, Q Ma, S Han, C Finn, arXiv:2503.220202025arXiv preprint</p>
<p>Task alignment in embodied ai: Bridging semantics and physical constraints. Y Liu, J Smith, Q Chen, Transactions on Robotics. 402024</p>
<p>Vision-language-action models: Unified frameworks for embodied execution. D Driess, Y Huang, A E A Zeng, Conference on Robot Learning (CoRL). 2023</p>
<p>. J Bruce, M Dennis, A Edwards, J Parker-Holder, Y Shi, E Hughes, M Lai, A Mavalankar, R Steigerwald, C Apps, Genie: Generative interactive environments. 2024</p>
<p>Trans-dreamer: Reinforcement learning with transformer world models. C Chen, Y.-F Wu, J Yoon, S Ahn, 2022</p>
<p>Transformerbased world models are happy with 100k interactions. J Robine, M Hoffmann, T Uelwer, S Harmeling, ICLR2023</p>
<p>Worlddreamer: Towards general world models for video generation via predicting masked tokens. X Wang, Z Zhu, G Huang, B Wang, X Chen, J Lu, 2024</p>
<p>Paragraph-to-image generation with information-enriched diffusion model. W Wu, Z Li, Y He, M Z Shou, C Shen, L Cheng, Y Li, T Gao, D Zhang, Z Wang, arXiv:2311.142842023arXiv preprint</p>
<p>Dream to control: Learning behaviors by latent imagination. D Hafner, T P Lillicrap, J Ba, M Norouzi, ICLR2020</p>
<p>Mastering atari with discrete world models. D Hafner, T P Lillicrap, M Norouzi, J Ba, ICLR2021</p>
<p>Dreamingv2: Reinforcement learning with discrete world models without reconstruction. M Okada, T Taniguchi, IROS2022</p>
<p>Daydreamer: World models for physical robot learning. P Wu, A Escontrela, D Hafner, P Abbeel, K Goldberg, Conference on robot learning. PMLR2023</p>
<p>Whatever Next? Predictive Brains and Embodied Cognition. A Clark, 2013MIT Press</p>
<p>Evoagent: Agent autonomous evolution with continual world model for long-horizon tasks. T Feng, X Wang, Z Zhou, R Wang, Y Zhan, G Li, Q Li, W Zhu, arXiv:2502.059072025arXiv preprint</p>
<p>Computing machinery and intelligence. A M Turing, Mind. 592361950</p>
<p>G Lakoff, M Johnson, Metaphors we live by. University of Chicago press1980</p>
<p>The symbol grounding problem. S Harnad, Physica D: Nonlinear Phenomena. 421-31990</p>
<p>A robust layered control system for a mobile robot. R Brooks, IEEE journal on robotics and automation. 211986</p>
<p>Intelligence without representation. R A Brooks, 199147Artificial intelligence</p>
<p>The cog project: Building a humanoid robot. R A Brooks, C Breazeal, M Marjanović, B Scassellati, M M Williamson, International workshop on computation for metaphors, analogy, and agents. Springer1998</p>
<p>Embodied artificial intelligence. R Chrisley, Artificial intelligence. 14912003</p>
<p>End-to-end training of deep visuomotor policies. S Levine, C Finn, T Darrell, P Abbeel, Journal of Machine Learning Research. 17392016</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, nature. 51875402015</p>
<p>Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. 201225</p>
<p>Generative adversarial nets. I J Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. 201427</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, International Conference on Learning Representations. 2020</p>
<p>Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, Advances in neural information processing systems. 202033</p>
<p>Masked autoencoders are scalable vision learners. K He, X Chen, S Xie, Y Li, P Dollár, R Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202216</p>
<p>Segment anything. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, NeurIPS2017</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies. the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies20191</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of machine learning research. 211402020</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, April 2023. 2023146</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.139712023arXiv preprint</p>
<p>Playing atari with deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.56022013arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, nature. 52975872016</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.063472017arXiv preprint</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, International conference on machine learning. Pmlr2018</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 202235</p>
<p>Z Shao, P Wang, Q Zhu, R Xu, J Song, X Bi, H Zhang, M Zhang, Y Li, Y Wu, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. G Team, P Georgiev, V I Lei, R Burnell, L Bai, A Gulati, G Tanzer, D Vincent, Z Pan, S Wang, arXiv:2403.055302024arXiv preprint</p>
<p>A Hurst, A Lerer, A P Goucher, A Perelman, A Ramesh, A Clark, A Ostrow, A Welihinda, A Hayes, A Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, International conference on machine learning. PMLR2019</p>
<p>Mastering diverse domains through world models. D Hafner, J Pasukonis, J Ba, T Lillicrap, arXiv:2301.041042023arXiv preprint</p>
<p>Is sora a world simulator? a comprehensive survey on general world models and beyond. Z Zhu, X Wang, W Zhao, C Min, N Deng, M Dou, Y Wang, B Shi, K Wang, C Zhang, arXiv:2405.035202024arXiv preprint</p>
<p>Seal: Self-supervised embodied active learning using exploration and 3d consistency. D S Chaplot, M Dalal, S Gupta, J Malik, R R Salakhutdinov, Advances in neural information processing systems. 202134</p>
<p>Active slam: A review on last decade. M F Ahmed, K Masood, V Fremont, I Fantoni, Sensors. 231980972023</p>
<p>Visual slam and structure from motion in dynamic environments: A survey. M R U Saputra, A Markham, N Trigoni, ACM Computing Surveys (CSUR). 5122018</p>
<p>Simultaneous localization and mapping: part i. H Durrant-Whyte, T Bailey, IEEE robotics &amp; automation magazine. 1322006</p>
<p>Simultaneous localization and mapping (slam): Part ii. T Bailey, H Durrant-Whyte, IEEE robotics &amp; automation magazine. 1332006</p>
<p>A survey of visual slam in dynamic environment: The evolution from geometric to semantic approaches. Y Wang, Y Tian, J Chen, K Xu, X Ding, IEEE Transactions on Instrumentation and Measurement. 2024</p>
<p>On combining visual slam and dense scene flow to increase the robustness of localization and mapping in dynamic environments. P F Alcantarilla, J J Yebes, J Almazán, L M Bergasa, 2012IEEE</p>
<p>Effective background model-based rgb-d dense visual odometry in a dynamic environment. D.-H Kim, J.-H Kim, IEEE Transactions on Robotics. 3262016</p>
<p>Coslam: Collaborative visual slam in dynamic environments. D Zou, P Tan, IEEE transactions on pattern analysis and machine intelligence. 201235</p>
<p>Rgb-d slam in dynamic environments using point correlations. W Dai, Y Zhang, P Li, Z Fang, S Scherer, IEEE transactions on pattern analysis and machine intelligence. 202044</p>
<p>Accurate dynamic slam using crf-based long-term consistency. Z.-J Du, S.-S Huang, T.-J Mu, Q Zhao, R R Martin, K Xu, IEEE Transactions on Visualization and Computer Graphics. 2842020</p>
<p>Slam++: Simultaneous localisation and mapping at the level of objects. R F Salas-Moreno, R A Newcombe, H Strasdat, P H Kelly, A J Davison, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2013</p>
<p>Dsslam: A semantic visual slam towards dynamic environments. C Yu, Z Liu, X.-J Liu, F Xie, Y Yang, Q Wei, Q Fei, 2018 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2018</p>
<p>Twistslam: Constrained slam in dynamic environment. M Gonzalez, E Marchand, A Kacete, J Royan, IEEE Robotics and Automation Letters. 732022</p>
<p>Gsslam: Dense visual slam with 3d gaussian splatting. C Yan, D Qu, D Xu, B Zhao, Z Wang, D Wang, X Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202419604</p>
<p>Gaudi: A neural architect for immersive 3d scene generation. M A Bautista, P Guo, S Abnar, W Talbott, A Toshev, Z Chen, L Dinh, S Zhai, H Goh, D Ulbricht, Advances in Neural Information Processing Systems. 202235</p>
<p>Towards label-free scene understanding by vision foundation models. R Chen, Y Liu, L Kong, N Chen, X Zhu, Y Ma, T Liu, W Wang, Advances in Neural Information Processing Systems. 202336910</p>
<p>Openscene: 3d scene understanding with open vocabularies. S Peng, K Genova, C Jiang, A Tagliasacchi, M Pollefeys, T Funkhouser, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2023</p>
<p>Lexicon3d: Probing visual foundation models for complex 3d scene understanding. Y Man, S Zheng, Z Bao, M Hebert, L Gui, Y.-X Wang, Advances in Neural Information Processing Systems. 202437</p>
<p>Graphdreamer: Compositional 3d scene synthesis from scene graphs. G Gao, W Liu, A Chen, A Geiger, B Schölkopf, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202421</p>
<p>Hugs: Holistic urban 3d scene understanding via gaussian splatting. H Zhou, J Shao, L Xu, D Bai, W Qiu, B Liu, Y Wang, A Geiger, Y Liao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202421345</p>
<p>Regionplc: Regional point-language contrastive learning for open-world 3d scene understanding. J Yang, R Ding, W Deng, Z Wang, X Qi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202419832</p>
<p>Model-based active exploration. P Shyam, W Jaśkowski, F Gomez, International conference on machine learning. PMLR2019</p>
<p>Learning to explore using active neural slam. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, 8th International Conference on Learning Representations. 20202020</p>
<p>Behavior from the void: Unsupervised active pre-training. H Liu, P Abbeel, Advances in Neural Information Processing Systems. 202134473</p>
<p>Active reasoning in an open-world environment. M Xu, G Jiang, W Liang, C Zhang, Y Zhu, Advances in Neural Information Processing Systems. 202336</p>
<p>Model-free active exploration in reinforcement learning. A Russo, A Proutiere, Advances in Neural Information Processing Systems. 202336</p>
<p>Activerir: Active audio-visual exploration for acoustic environment modeling. A Somayazulu, S Majumder, C Chen, K Grauman, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE202413836</p>
<p>World model as a graph: Learning latent landmarks for planning. L Zhang, G Yang, B C Stadie, International conference on machine learning. PMLR202112620</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. C H Song, J Wu, C Washington, B M Sadler, W.-L Chao, Y Su, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Ego-planner: An esdffree gradient-based local planner for quadrotors. X Zhou, Z Wang, H Ye, C Xu, F Gao, IEEE Robotics and Automation Letters. 622020</p>
<p>Autoact: Automatic agent learning from scratch for qa via self-planning. S Qiao, N Zhang, R Fang, Y Luo, W Zhou, Y E Jiang, C Lv, H Chen, arXiv:2401.052682024arXiv preprint</p>
<p>Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. L Yang, Z Yu, C Meng, M Xu, S Ermon, B Cui, Forty-first International Conference on Machine Learning. 2024</p>
<p>Etpnav: Evolving topological planning for vision-language navigation in continuous environments. D An, H Wang, W Wang, Z Wang, Y Huang, K He, L Wang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Advances in Neural Information Processing Systems. 2023</p>
<p>Reflect: Summarizing robot experiences for failure explanation and correction. Z Liu, A Bahety, S Song, arXiv:2306.157242023arXiv preprint</p>
<p>Rila: Reflective and imaginative language agent for zero-shot semantic audio-visual navigation. Z Yang, J Liu, P Chen, A Cherian, T K Marks, J Le Roux, C Gan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202416</p>
<p>Optimus-1: Hybrid multimodal memory empowered agents excel in long-horizon tasks. Z Li, Y Xie, R Shao, G Chen, D Jiang, L Nie, arXiv:2408.036152024arXiv preprint</p>
<p>Remac: Self-reflective and self-evolving multi-agent collaboration for longhorizon robot manipulation. P Yuan, A Ma, Y Yao, H Yao, M Tomizuka, M Ding, arXiv:2503.221222025arXiv preprint</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, Conference on robot learning. PMLR2023</p>
<p>A generalist agent. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, arXiv:2205.061752022arXiv preprint</p>
<p>Embodiedgpt: Vision-language pre-training via embodied chain of thought. Y Mu, Q Zhang, M Hu, W Wang, M Ding, J Jin, B Wang, J Dai, Y Qiao, P Luo, Advances in Neural Information Processing Systems. 2023</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Z Peng, W Wang, L Dong, Y Hao, S Huang, S Ma, F Wei, arXiv:2306.148242023arXiv preprint</p>
<p>Multiply: A multisensory object-centric embodied large language model in 3d world. Y Hong, Z Zheng, P Chen, Y Wang, J Li, C Gan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Minedojo: Building openended embodied agents with internet-scale knowledge. L Fan, G Wang, Y Jiang, A Mandlekar, Y Yang, H Zhu, A Tang, D.-A Huang, Y Zhu, A Anandkumar, Advances in Neural Information Processing Systems. 202235362</p>
<p>Openvla: An open-source vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Cogagent: A visual language model for gui agents. W Hong, W Wang, Q Lv, J Xu, W Yu, J Ji, Y Wang, Z Wang, Y Dong, M Ding, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202414290</p>
<p>Octo: An open-source generalist robot policy. O M Team, D Ghosh, H Walke, K Pertsch, K Black, O Mees, S Dasari, J Hejna, T Kreiman, C Xu, arXiv:2405.122132024arXiv preprint</p>
<p>Scaling cross-embodied learning: One policy for manipulation, navigation, locomotion and aviation. R Doshi, H Walke, O Mees, S Dasari, S Levine, arXiv:2408.118122024arXiv preprint</p>
<p>Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers. L Wang, X Chen, J Zhao, K He, Advances in Neural Information Processing Systems. 202437450</p>
<p>Generative adversarial imitation learning. J Ho, S Ermon, Advances in neural information processing systems. 201629</p>
<p>End-to-end differentiable adversarial imitation learning. N Baram, O Anschel, I Caspi, S Mannor, International Conference on Machine Learning. 2017</p>
<p>Trafficsim: Learning to simulate realistic multi-agent behaviors. S Suo, S Regalado, S Casas, R Urtasun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202110</p>
<p>Trajgen: Generating realistic and diverse trajectories with reactive and feasible agent behaviors for autonomous driving. Q Zhang, Y Gao, Y Zhang, Y Guo, D Ding, Y Wang, P Sun, D Zhao, IEEE Transactions on Intelligent Transportation Systems. 23122022</p>
<p>Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. C Li, R Zhang, J Wong, C Gokmen, S Srivastava, R Martín-Martín, C Wang, G Levine, M Lingelbach, J Sun, Conference on Robot Learning. 2023</p>
<p>Agentlens: Visual analysis for agent behaviors in llm-based autonomous systems. J Lu, B Pan, J Chen, Y Feng, J Hu, Y Peng, W Chen, IEEE Transactions on Visualization and Computer Graphics. 2024</p>
<p>Embodied contrastive learning with geometric consistency and behavioral awareness for object navigation. B Chen, J Kang, P Zhong, Y Liang, Y Sheng, J Wang, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Monotonic value function factorisation for deep multiagent reinforcement learning. T Rashid, M Samvelyan, C S De Witt, G Farquhar, J Foerster, S Whiteson, Journal of Machine Learning Research. 211782020</p>
<p>Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. K Son, D Kim, W J Kang, D E Hostallero, Y Yi, International conference on machine learning. PMLR2019</p>
<p>Qplex: Duplex dueling multi-agent q-learning. J Wang, Z Ren, T Liu, Y Yu, C Zhang, arXiv:2008.010622020arXiv preprint</p>
<p>Multi-agent reinforcement learning is a sequence modeling problem. M Wen, J Kuba, R Lin, W Zhang, Y Wen, J Wang, Y Yang, Advances in Neural Information Processing Systems. 202235</p>
<p>Building cooperative embodied agents modularly with large language models. H Zhang, W Du, J Shan, Q Zhou, Y Du, J B Tenenbaum, T Shu, C Gan, arXiv:2307.024852023arXiv preprint</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. W Chen, Y Su, J Zuo, C Yang, C Yuan, C Qian, C.-M Chan, Y Qin, Y Lu, R Xie, arXiv:2308.10848202326arXiv preprint</p>
<p>Metagpt: Meta programming for multi-agent collaborative framework. S Hong, X Zheng, J Chen, Y Cheng, J Wang, C Zhang, Z Wang, S K S Yau, Z Lin, L Zhou, arXiv:2308.00352202336arXiv preprint</p>
<p>Combo: Compositional world models for embodied multi-agent cooperation. H Zhang, Z Wang, Q Lyu, Z Zhang, S Chen, T Shu, B Dariush, K Lee, Y Du, C Gan, arXiv:2404.107752024arXiv preprint</p>
<p>An embodied cognitive science?. A Clark, Trends in cognitive sciences. 391999</p>
<p>Embodiment and cognitive science. R W GibbsJr, 2005Cambridge University Press</p>
<p>What's that thing called embodiment?. T Ziemke, Proceedings of the 25th Annual Cognitive Science Society. the 25th Annual Cognitive Science SocietyPsychology Press2013</p>
<p>Egocentric planning for scalable embodied task achievement. X Liu, H Palacios, C Muise, Advances in Neural Information Processing Systems. 202336613</p>
<p>From robot learning to robot understanding: Leveraging causal graphical models for robotics. K C Stocking, A Gopnik, C Tomlin, Conference on Robot Learning. PMLR2022</p>
<p>Robovqa: Multimodal long-horizon reasoning for robotics. P Sermanet, T Ding, J Zhao, F Xia, D Dwibedi, K Gopalakrishnan, C Chan, G Dulac-Arnold, S Maddineni, N J Joshi, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Fltrnn: Faithful long-horizon task planning for robotics with large language models. J Zhang, L Tang, Y Song, Q Meng, H Qian, J Shao, W Song, S Zhu, J Gu, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Long-horizon planning for multi-agent robots in partially observable environments. S Nayak, A Morrison Orozco, M Have, J Zhang, V Thirumalai, D Chen, A Kapoor, E Robinson, K Gopalakrishnan, J Harrison, Advances in Neural Information Processing Systems. 202437</p>
<p>Lamma-p: Generalizable multi-agent long-horizon task allocation and planning with lm-driven pddl planner. X Zhang, H Qin, F Wang, Y Dong, J Li, arXiv:2409.205602024arXiv preprint</p>
<p>Teach: Task-driven embodied agents that chat. A Padmakumar, J Thomason, A Shrivastava, P Lange, A Narayan-Chen, S Gella, R Piramuthu, G Tur, D Hakkani-Tur, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>A survey on the memory mechanism of large language model based agents. Z Zhang, X Bo, C Ma, R Li, X Chen, Q Dai, J Zhu, Z Dong, J.-R Wen, arXiv:2404.135012024arXiv preprint</p>
<p>Reinforcement learning with long short-term memory. B Bakker, Advances in neural information processing systems. 200114</p>
<p>A short survey on memory based reinforcement learning. D Ramani, arXiv:1904.067362019arXiv preprint</p>
<p>Episodic reinforcement learning with associative memory. G Zhu, Z Lin, G Yang, C Zhang, International Conference on Learning Representations. 2020</p>
<p>Memory augmented control networks. A Khan, C Zhang, N Atanasov, K Karydis, V Kumar, D D Lee, arXiv:1709.057062017arXiv preprint</p>
<p>Neural episodic control. A Pritzel, B Uria, S Srinivasan, A P Badia, O Vinyals, D Hassabis, D Wierstra, C Blundell, International conference on machine learning. 2017</p>
<p>A survey on robotics with foundation models: toward embodied ai. Z Xu, K Wu, J Wen, J Li, N Liu, Z Che, J Tang, arXiv:2402.023852024arXiv preprint</p>
<p>Embodied intelligence toward future smart manufacturing in the era of ai foundation model. L Ren, J Dong, S Liu, L Zhang, L Wang, IEEE/ASME Transactions on Mechatronics. 2024</p>
<p>Foundation models in robotics: Applications, challenges, and the future. R Firoozi, J Tucker, S Tian, A Majumdar, J Sun, W Liu, Y Zhu, S Song, A Kapoor, K Hausman, The International Journal of Robotics Research. 027836492412815082023</p>
<p>Muep: A multimodal benchmark for embodied planning with foundation models [c. K Li, B Yu, Q Zheng, Y Zhan, Y Zhang, T Zhang, Y Yang, Y Chen, L Sun, Q Cao, Intemational Joint Conferences on Artificial Intelligence. IJCAI. 2024</p>
<p>Ecbench: Can multi-modal foundation models understand the egocentric world? a holistic embodied cognition benchmark. R Dang, Y Yuan, W Zhang, Y Xin, B Zhang, L Li, L Wang, Q Zeng, X Li, L Bing, arXiv:2501.050312025arXiv preprint</p>
<p>Mfe-etp: A comprehensive evaluation benchmark for multimodal foundation models on embodied task planning. M Zhang, X Fu, J Hao, P Han, H Zhang, L Shi, H Tang, Y Zheng, arXiv:2407.050472024arXiv preprint</p>
<p>A comprehensive survey on embodied intelligence: Advancements, challenges, and future perspectives. F Sun, R Chen, T Ji, Y Luo, H Zhou, H Liu, CAAI Artificial Intelligence Research. 32024</p>
<p>Embodied world models emerge from navigational task in open-ended environments. L Jin, L Jia, arXiv:2504.114192025arXiv preprint</p>
<p>Trends and challenges in robot manipulation. A Billard, D Kragic, Science. 364644684142019</p>
<p>Progress and prospects of the human-robot collaboration. A Ajoudani, A M Zanchettin, S Ivaldi, A Albu-Schäffer, K Kosuge, O Khatib, Autonomous robots. 422018</p>
<p>A tuning procedure for stable pid control of robot manipulators. R Kelly, Robotica. 1321995</p>
<p>Stability of pid control for industrial robot arms. P Rocco, IEEE transactions on robotics and automation. 1242002</p>
<p>Optimal and autonomous control using reinforcement learning: A survey. B Kiumarsi, K G Vamvoudakis, H Modares, F L Lewis, IEEE transactions on neural networks and learning systems. 201729</p>
<p>Reinforcement learning in robotic applications: a comprehensive survey. B Singh, R Kumar, V P Singh, Artificial Intelligence Review. 5522022</p>
<p>A survey on vision-language-action models for embodied ai. Y Ma, Z Song, Y Zhuang, J Hao, I King, arXiv:2405.140932024arXiv preprint</p>
<p>Multi-modal and multi-agent systems meet rationality: A survey. B Jiang, Y Xie, X Wang, W J Su, C J Taylor, T Mallick, ICML Workshop on LLMs and Cognition. 2024</p>
<p>Madiff: Offline multi-agent learning with diffusion models. Z Zhu, M Liu, L Mao, B Kang, M Xu, Y Yu, S Ermon, W Zhang, Advances in Neural Information Processing Systems. 202437</p>
<p>A human-robot collaboration controller utilizing confidence for disagreement adjustment. M Ma, L Cheng, IEEE Transactions on Robotics. 2024</p>
<p>Smoothquant: Accurate and efficient post-training quantization for large language models. G Xiao, J Lin, M Seznec, H Wu, J Demouth, S Han, International Conference on Machine Learning. PMLR20233899</p>
<p>Spatten: Efficient sparse attention architecture with cascade token and head pruning. H Wang, Z Zhang, S Han, 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE2021</p>
<p>Haq: Hardware-aware automated quantization with mixed precision. K Wang, Z Liu, Y Lin, J Lin, S Han, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Llvm: A compilation framework for lifelong program analysis &amp; transformation. C Lattner, V Adve, International symposium on code generation and optimization. IEEE2004. 2004. 2004</p>
<p>Gtuner: Tuning dnn computations on gpu via graph attention network. Q Sun, X Zhang, H Geng, Y Zhao, Y Bai, H Zheng, B Yu, Proceedings of the 59th ACM/IEEE Design Automation Conference. the 59th ACM/IEEE Design Automation Conference2022</p>
<p>Gemini: Mapping and architecture co-exploration for large-scale dnn chiplet accelerators. J Cai, Z Wu, S Peng, Y Wei, Z Tan, G Shi, M Gao, K Ma, 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE2024</p>
<p>Flightvgm: Efficient video generation model inference with online sparsification and hybrid precision on fpgas. J Liu, S Zeng, L Ding, W Soedarmadji, H Zhou, Z Wang, J Li, J Li, Y Dai, K Wen, Proceedings of the 2025 ACM/SIGDA International Symposium on Field Programmable Gate Arrays. the 2025 ACM/SIGDA International Symposium on Field Programmable Gate Arrays2025</p>
<p>Picachu: Plug-in cgra handling upcoming nonlinear operations in llms. J Qin, T Xia, C Tan, J Zhang, S Q Zhang, Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems20252</p>
<p>Gemmini: Enabling systematic deep-learning architecture evaluation via full-stack integration. H Genc, S Kim, A Amid, A Haj-Ali, V Iyer, P Prakash, J Zhao, D Grubb, H Liew, H Mao, 2021 58th ACM/IEEE Design Automation Conference (DAC). IEEE2021</p>
<p>Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. Y Lin, H Tang, S Yang, Z Zhang, G Xiao, C Gan, S Han, arXiv:2405.045322024arXiv preprint</p>
<p>Jaq: Joint efficient architecture design and low-bit quantization with hardware-software co-exploration. M Wang, Y Meng, C Tang, W Zhang, Y Qin, Y Yao, Y Li, T Feng, X Wang, X Guan, arXiv:2501.053392025arXiv preprint</p>
<p>Bie: bi-exponent block floating-point for large language models quantization. L Zou, W Zhao, S Yin, C Bai, Q Sun, B Yu, Fortyfirst International Conference on Machine Learning. 2024</p>
<p>Habitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Maniskill: Learning-from-demonstrations benchmark for generalizable manipulation skills. T Mu, Z Ling, F Xiang, D Yang, X Li, S Tao, Z Huang, Z Jia, H Su, International Conference on Learning Representations. 2021</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. 2012</p>
<p>Airsim: High-fidelity visual and physical simulation for autonomous vehicles. S Shah, D Dey, C Lovett, A Kapoor, Field and Service Robotics. 2017</p>
<p>U2udata: A large-scale cooperative perception dataset for swarm uavs autonomous flight. T Feng, X Wang, F Han, L Zhang, W Zhu, ACM Multimedia 2024. 2024</p>
<p>U2usim -a uav telepresence simulation platform with multiagent sensing and dynamic environment. F Han, L Zhang, X Wang, K.-A Zhao, Y Zhong, Z Su, T Feng, W Zhu, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Vision-language-action models: Unified frameworks for embodied execution. D Driess, Y Huang, A Zeng, P Florence, F Tombari, A Wahid, Q Vuong, K Hausman, M Heo, U Lee, Conference on Robot Learning. 2023</p>
<p>Openvla: An open-source vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Towards efficient llm grounding for embodied multi-agent collaboration. Y Zhang, S Yang, C Bai, F Wu, X Li, Z Wang, X Li, arXiv:2405.143142024arXiv preprint</p>
<p>Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai. T Wang, X Mao, C Zhu, R Xu, R Lyu, P Li, X Chen, W Zhang, K Chen, T Xue, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Drivedreamer: Towards real-world-drive world models for autonomous driving. X Wang, Z Zhu, G Huang, X Chen, J Zhu, J Lu, European Conference on Computer Vision. Springer2024</p>
<p>Embodied-r: Collaborative framework for activating embodied spatial reasoning in foundation models via reinforcement learning. B Zhao, Z Wang, J Fang, C Gao, F Man, J Cui, X Wang, X Chen, Y Li, W Zhu, arXiv:2504.126802025arXiv preprint</p>
<p>Modularized self-reflected video reasoner for multimodal llm with application to video question answering. Z Song, X Wang, Z Qian, H Chen, L Huang, H Xue, W Zhu, Forty-second International Conference on Machine Learning. </p>
<p>Dynamic mixture of curriculum lora experts for continual multimodal instruction tuning. C Ge, X Wang, Z Zhang, H Chen, J Fan, L Huang, H Xue, W Zhu, arXiv:2506.116722025arXiv preprint</p>
<p>Neighbor does matter: Curriculum global positive-negative sampling for vision-language pre-training. B Huang, F He, Q Wang, H Chen, G Li, Z Feng, X Wang, W Zhu, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Large language model with curriculum reasoning for visual concept recognition. Y Zhang, X Wang, H Chen, J Fan, W Wen, H Xue, H Mei, W Zhu, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Modular-cam: Modular dynamic camera-view video generation with llm. Z Pan, X Wang, Y Zhang, H Chen, K M Cheng, Y Wu, W Zhu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Vtimellm: Empower llm to grasp video moments. B Huang, X Wang, H Chen, Z Song, W Zhu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202414280</p>
<p>Curriculum co-disentangled representation learning across multiple environments for social recommendation. X Wang, Z Pan, Y Zhou, H Chen, C Ge, W Zhu, International Conference on Machine Learning. PMLR2023192</p>
<p>Automated disentangled sequential recommendation with large language models. X Wang, H Chen, Z Pan, Y Zhou, C Guan, L Sun, W Zhu, ACM Transactions on Information Systems. 4322025</p>
<p>Disentangled representation learning for recommendation. X Wang, H Chen, Y Zhou, J Ma, W Zhu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4512022</p>
<p>Language grounding with 3d objects. J Thomason, M Shridhar, Y Bisk, C Paxton, L Zettlemoyer, Conference on robot learning. PMLR2022</p>
<p>Llmˆ3: Large language model-based task and motion planning with motion failure reasoning. S Wang, M Han, Z Jiao, Z Zhang, Y N Wu, S.-C Zhu, H Liu, 202492</p>
<p>Curriculum-listener: Consistency-and complementarity-aware audioenhanced temporal sentence grounding. H Chen, X Wang, X Lan, H Chen, X Duan, J Jia, W Zhu, Proceedings of the 31st ACM International Conference on Multimedia. the 31st ACM International Conference on Multimedia2023</p>
<p>Divico: Disentangled visual token compression for efficient large vision-language model. X Wang, Z Pan, H Chen, W Zhu, IEEE Transactions on Circuits and Systems for Video Technology. 2025</p>
<p>Learning from feedback: Semantic enhancement for object slam using foundation models. J Hong, R Choi, J J Leonard, arXiv:2411.067522024arXiv preprint</p>
<p>Magma: A foundation model for multimodal ai agents. J Yang, R Tan, Q Wu, R Zheng, B Peng, Y Liang, Y Gu, M Cai, S Ye, J Jang, arXiv:2502.131302025arXiv preprint</p>
<p>Disenstudio: Customized multi-subject text-to-video generation with disentangled spatial control. H Chen, X Wang, Y Zhang, Y Zhou, Z Zhang, S Tang, W Zhu, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Disenbooth: Identity-preserving disentangled tuning for subject-driven text-to-image generation. H Chen, Y Zhang, S Wu, X Wang, X Duan, Y Zhou, W Zhu, arXiv:2305.033742023arXiv preprint</p>
<p>Llmˆ3: Large language model-based task and motion planning with motion failure reasoning. S Wang, M Han, Z Jiao, Z Zhang, Y N Wu, S.-C Zhu, H Liu, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2024</p>
<p>Mllm as retriever: Interactively learning multimodal retrieval for embodied agents. J Yue, X Xu, B F Karlsson, Z Lu, arXiv:2410.034502024arXiv preprint</p>
<p>Dataaugmented curriculum graph neural architecture search under distribution shifts. Y Yao, X Wang, Y Qin, Z Zhang, W Zhu, H Mei, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438441</p>
<p>Multi-task graph neural architecture search with task-aware collaboration and curriculum. Y Qin, X Wang, Z Zhang, H Chen, W Zhu, Advances in neural information processing systems. 202336</p>
<p>Curriculum multi-negative augmentation for debiased video grounding. X Lan, Y Yuan, H Chen, X Wang, Z Jie, L Ma, Z Wang, W Zhu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. Z Chen, J Wu, W Wang, W Su, G Chen, S Xing, M Zhong, Q Zhang, X Zhu, L Lu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2024</p>
<p>J Xu, Z Guo, J He, H Hu, T He, S Bai, K Chen, J Wang, Y Fan, K Dang, arXiv:2503.20215Qwen2. 5-omni technical report. 2025arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. B Zitkovich, T Yu, S Xu, P Xu, T Xiao, F Xia, J Wu, P Wohlhart, S Welker, A Wahid, Conference on Robot Learning. PMLR2023</p>
<p>Executable code actions elicit better llm agents. X Wang, Y Chen, L Yuan, Y Zhang, Y Li, H Peng, H Ji, Forty-first International Conference on Machine Learning. 2024</p>
<p>Generative multi-agent collaboration in embodied ai: A systematic review. D Wu, X Wei, G Chen, H Shen, X Wang, W Li, B Jin, arXiv:2502.115182025arXiv preprint</p>
<p>Vlas: Vision-language-action model with speech instructions for customized robot manipulation. W Zhao, P Ding, M Zhang, Z Gong, S Bai, H Zhao, D Wang, arXiv:2502.135082025arXiv preprint</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Advances in neural information processing systems. 2023</p>
<p>Counterintuitive behavior of social systems. J W Forrester, Theory and decision. 221971</p>
<p>Self-supervised learning from images with a joint-embedding predictive architecture. M Assran, Q Duval, I Misra, P Bojanowski, P Vincent, M Rabbat, Y Lecun, N Ballas, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023629</p>
<p>Diffusion probabilistic modeling for video generation. R Yang, P Srivastava, S Mandt, Entropy. 251014692023</p>
<p>Sora as an agi world model? a complete survey on text-to-video generation. J Cho, F D Puspitasari, S Zheng, J Zheng, L.-H Lee, T.-H Kim, C S Hong, C Zhang, arXiv:2403.051312024arXiv preprint</p>
<p>Worldmodelbench: Judging video generation models as world models. D Li, Y Fang, Y Chen, S Yang, S Cao, J Wong, M Luo, X Wang, H Yin, J E Gonzalez, arXiv:2502.206942025arXiv preprint</p>
<p>Language models meet world models: Embodied experiences enhance language models. J Xiang, T Tao, Y Gu, T Shu, Z Wang, Z Yang, Z Hu, Advances in neural information processing systems. 202336412</p>
<p>Genrl: Multimodal-foundation world models for generalization in embodied agents. P Mazzaglia, T Verbelen, B Dhoedt, A Courville, S Rajeswar, Advances in Neural Information Processing Systems. 202437555</p>
<p>The essential role of causality in foundation world models for embodied ai. T Gupta, W Gong, C Ma, N Pawlowski, A Hilmkil, M Scetbon, M Rigter, A Famoti, A J Llorens, J Gao, arXiv:2402.066652024arXiv preprint</p>
<p>Hierarchical world models as visual whole-body humanoid controllers. N Hansen, J Sv, V Sobal, Y Lecun, X Wang, H Su, arXiv:2405.184182024arXiv preprint</p>
<p>Diffusion for world modeling: Visual details matter in atari. E Alonso, A Jelley, V Micheli, A Kanervisto, A J Storkey, T Pearce, F Fleuret, Advances in Neural Information Processing Systems. 202437</p>
<p>Revisiting feature prediction for learning visual representations from video. A Bardes, Q Garrido, J Ponce, X Chen, M Rabbat, Y Lecun, M Assran, N Ballas, 2024</p>
<p>Mc-jepa: A joint-embedding predictive architecture for self-supervised learning of motion and content features. A Bardes, J Ponce, Y Lecun, 2023</p>
<p>A-jepa: Joint-embedding predictive architecture can listen. Z Fei, M Fan, J Huang, 2024</p>
<p>Memo: A deep network for flexible combination of episodic models. A Banino, A P Badia, R Koster, M J Chadwick, V F Zambaldi, D Hassabis, C Barry, M M Botvinick, D Kumaran, C Blundell, 2020</p>
<p>Transformers are sampleefficient world models. V Micheli, E Alonso, F Fleuret, ICLR2023</p>
<p>Spatial-temporal transformer networks for traffic flow forecasting. M Xu, W Dai, C Liu, X Gao, W Lin, G.-J Qi, H Xiong, 2021</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PmLR2021</p>
<p>Language is not all you need: Aligning perception with language models. S Huang, L Dong, W Wang, Y Hao, S Singhal, S Ma, T Lv, L Cui, O K Mohammed, B Patra, Advances in Neural Information Processing Systems. 2023</p>
<p>Openai, arXiv:2303.08774Gpt-4 technical report. 2023</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Physion++: Evaluating physical scene understanding that requires online inference of different physical properties. H.-Y Tung, M Ding, Z Chen, D Bear, C Gan, J Tenenbaum, D Yamins, J Fan, K Smith, Advances in Neural Information Processing Systems. 20233668</p>
<p>Are we ready for service robots? the openlorisscene datasets for lifelong slam. X Shi, D Li, P Zhao, Q Tian, Y Tian, Q Long, C Zhu, J Song, F Qiao, L Song, 2020</p>
<p>Contextual: Evaluating context-sensitive text-rich visual reasoning in large multimodal models. R Wadhawan, H Bansal, K.-W Chang, N Peng, arXiv:2401.133112024arXiv preprint</p>
<p>Adaplanner: Adaptive planning from feedback with language models. H Sun, Y Zhuang, L Kong, B Dai, C Zhang, Advances in neural information processing systems. 202336245</p>
<p>Lot: A transformer-based approach based on channel state information for indoor localization. W Li, X Meng, Z Zhao, Z Liu, C Chen, H Wang, IEEE Sensors Journal. 23222023</p>
<p>Detach: Cross-domain learning for long-horizon tasks via mixture of disentangled experts. Y Shen, H Liu, P Liu, R Xia, T Yao, Y Sun, T Feng, arXiv:2508.078422025arXiv preprint</p>
<p>Aed: Adaptable error detection for few-shot imitation policy. J.-F Yeh, K.-H Hung, P.-C Lo, C M Chung, T.-H Wu, H.-T Su, Y.-T Chen, W Hsu, Advances in Neural Information Processing Systems. 202437</p>
<p>Are preferences useful for better assistance? a physically assistive robotics user study. G Canal, C Torras, G Alenyà, ACM Transactions on Human-Robot Interaction. 1042021</p>
<p>Socially assistive robots in elderly care: a mixed-method systematic literature review. R Kachouie, S Sedighadeli, R Khosla, M.-T Chu, International Journal of Human-Computer Interaction. 3052014</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, Conference on Robot Learning. 2022</p>
<p>Multiweather cross-view geo-localization using denoising diffusion models. T Feng, Q Li, X Wang, M Wang, G Li, W Zhu, Proceedings of the 2nd Workshop on UAVs in Multimedia: Capturing the World from a New Perspective. the 2nd Workshop on UAVs in Multimedia: Capturing the World from a New Perspective2024</p>
<p>U2udata-2: A scalable swarm uavs autonomous flight dataset for long-horizon tasks. T Feng, X Wang, F Han, L Zhang, W Zhu, ArXiv2025</p>
<p>Vision-and-language navigation today and tomorrow: A survey in the era of foundation models. Y Zhang, Z Ma, J Li, Y Qiao, Z Wang, J Chai, Q Wu, M Bansal, P Kordjamshidi, arXiv:2407.070352024arXiv preprint</p>
<p>Towards trustworthy autonomous systems: Taxonomies and future perspectives. F Flammini, C Alcaraz, E Bellini, S Marrone, J Lopez, A Bondavalli, IEEE Transactions on Emerging Topics in Computing. 2022</p>
<p>Human-in-the-loop consensus tracking control for uav systems via an improved prescribed performance approach. L Chen, H Liang, Y Pan, T Li, IEEE Transactions on Aerospace and Electronic Systems. 5962023</p>
<p>Event-based finite-time neural control for human-in-the-loop uav attitude systems. G Lin, H Li, C K Ahn, D Yao, IEEE Transactions on Neural Networks and Learning Systems. 34122022</p>
<p>Detecting wildfires on uavs with real-time segmentation trained by larger teacher models. J Pesonen, T Hakala, V Karjalainen, N Koivumäki, L Markelin, A.-M Raita-Hakola, J Suomalainen, I Pölönen, E Honkavaara, 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE2025</p>
<p>Large language models for uavs: Current state and pathways to the future. S Javaid, H Fahim, B He, N Saeed, IEEE Open Journal of Vehicular Technology. 2024</p>
<p>Sdpl: Shifting-dense partition learning for uav-view geolocalization. Q Chen, T Wang, Z Yang, H Li, R Lu, Y Sun, B Zheng, C Yan, IEEE Transactions on Circuits and Systems for Video Technology. 202434</p>
<p>Swarm of micro flying robots in the wild. X Zhou, X Wen, Z Wang, Y Gao, H Li, Q Wang, T Yang, H Lu, Y Cao, C Xu, Science Robotics. 76659542022</p>
<p>Scaleadaptive uav geo-localization via height-aware partition learning. Q Chen, T Wang, R Lu, Y Liu, B Zheng, Z Zheng, arXiv:2412.115352024arXiv preprint</p>
<p>Cross-temporal knowledge injection with color distribution normalization for remote sensing change detection. W Zheng, J Yang, J Chen, J He, P Li, D Sun, C Chen, X Meng, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 2025</p>
<p>Jd. com: Operations research algorithms drive intelligent warehouse robots to work. H Qin, J Xiao, D Ge, L Xin, J Gao, S He, H Hu, J G Carlsson, INFORMS Journal on Applied Analytics. 5212022</p>
<p>The role of robots in logistics. R Bogue, Industrial Robot: the international journal of robotics research and application. 5132024</p>
<p>Robot as staff: Robot for alibaba e-commerce warehouse process. I F A Prawira, A H Habbe, I Muda, R M Hasibuan, A Umbrajkaar, 2023 International Conference on Inventive Computation Technologies (ICICT). IEEE2023</p>
<p>Tcdformer-based momentum transfer model for long-term sports prediction. H Liu, X Huang, J Gu, J Shi, N He, T Feng, Expert Systems with Applications. 1283102025</p>
<p>Timely and accurate bitrate switching in http adaptive streaming with date-driven i-frame prediction. T Feng, Q Qi, J Wang, J Liao, J Liu, IEEE Transactions on Multimedia. 252022</p>
<p>Vabis: Video adaptation bitrate system for time-critical live streaming. T Feng, H Sun, Q Qi, J Wang, J Liao, IEEE Transactions on Multimedia. 22112019</p>
<p>Meta-uad: A meta-learning scheme for user-level network traffic anomaly detection. T Feng, Q Qi, L Guo, J Wang, ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2025</p>
<p>Robotic arms in precision agriculture: A comprehensive review of the technologies, applications, challenges, and future prospects. T Jin, X Han, Computers and Electronics in Agriculture. 2211089382024</p>
<p>Reactive diffusion policy: Slow-fast visual-tactile policy learning for contact-rich manipulation. H Xue, J Ren, W Chen, G Zhang, Y Fang, G Gu, H Xu, C Lu, arXiv:2503.028812025arXiv preprint</p>
<p>Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition. J Ye, X Wen, Y Wei, Y Xu, K Liu, H Shan, IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP. IEEE2023</p>
<p>Emotional face-to-speech. J Ye, B Cao, H Shan, International Conference on Machine Learning. 2025</p>
<p>Emo-dna: Emotion decoupling and alignment learning for crosscorpus speech emotion recognition. J Ye, Y Wei, X Wen, C Ma, Z Huang, K Liu, H Shan, Proceedings of the 31st ACM International Conference on Multimedia, MM, 2023. the 31st ACM International Conference on Multimedia, MM, 2023</p>
<p>Social robots for education: A review. T Belpaeme, J Kennedy, A Ramachandran, B Scassellati, F Tanaka, Science robotics. 32159542018</p>
<p>Social robots in education: Current trends and future perspectives. G Lampropoulos, Information. 161292025</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Research on the application of embodied intelligence technology in space exploration. L Qunzhi, M Chao, P Jing, W Jinqiao, W Zhiliang, Z Guibo, L Yongjian, D Boyuan, W Jie, 2024 IEEE International Conference on Control Science and Systems Engineering (ICCSSE). IEEE2024</p>
<p>His research lies in the areas of multimedia, computer vision, embodied AI and trustworthy AI. His research has led to the development of innovative AI tools that have been used in many practical applications like defect detection for high-speed railway infrastructures. His open-source video analysis toolkits and datasets such as CU-VIREO374, CCV, THUMOS, FCVID and WildDeepfake have been widely used in both academia and industry. He currently serves as Chair of ACM Shanghai Chapter and Associate Editor of several international journals. For contributions to large-scale and trustworthy video analysis, he was elected to Fellow of IEEE, IAPR and CCF. Wenwu Zhu is currently a Professor in the Department of Computer Science and Technology at Tsinghua University, the Vice Dean of Beijing National Research Center for Information Science and Technology. Prior to his current post, he was a Senior Researcher and Research Manager at Microsoft Research Asia. He was the Chief Scientist and Director at Intel Research China from. Eswa Ieee Tmm, Acm Multimedia, Aaai , Etc Tpami, Ieee Tkde, Acm Tois, Icml, Acm Neurips, Kdd, Acm Web Conference, Sigir, Multimedia Etc, His research interests include Embodied AI, World Model, and Multimedia Intelligence. He has published over 15 high-quality research papers in top journals and conferences. New York; Shanghai, ChinaAcademia Europaea2024. 2009. 2009-2011. 2004 to 2008. 2012. 2001. 2019. 2024-2025Department of Computer Science and Technology, Tsinghua University. He got his Ph.D. degree in Computer Science and Technology from Beijing University of Posts and Telecommunications ; Department of Computer Science and Technology, Tsinghua University. He got both his Ph.D. and B.E degrees in Computer Science and Technology from Zhejiang University, China. He also holds a Ph.D. degree in Computing Science from Simon Fraser University, Canada ; Postdoctoral Research Scientist at Columbia University ; Professor of Computer Science at Fudan UniversityHe serves as the EiC for IEEE Transactions on Circuits and Systems for Video Technology. the EiC for IEEE Transactions on Multimedia (2017-2019) and the Chair of the steering committee for IEEE Transactions on Multimedia (2020-2022. He serves as General Co-Chair for ACM Multimedia 2018 and ACM CIKM 2019. He is an AAAS Fellow</p>            </div>
        </div>

    </div>
</body>
</html>
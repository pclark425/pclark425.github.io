<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4456 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4456</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4456</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-278394560</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.04736v1.pdf" target="_blank">The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems</a></p>
                <p><strong>Paper Abstract:</strong> Intelligent tutoring systems have demonstrated effectiveness in teaching formal propositional logic proofs, but their reliance on template-based explanations limits their ability to provide personalized student feedback. While large language models (LLMs) offer promising capabilities for dynamic feedback generation, they risk producing hallucinations or pedagogically unsound explanations. We evaluated the stepwise accuracy of LLMs in constructing multi-step symbolic logic proofs, comparing six prompting techniques across four state-of-the-art LLMs on 358 propositional logic problems. Results show that DeepSeek-V3 achieved superior performance with 84.4% accuracy on stepwise proof construction and excelled particularly in simpler rules. We further used the best-performing LLM to generate explanatory hints for 1,050 unique student problem-solving states from a logic ITS and evaluated them on 4 criteria with both an LLM grader and human expert ratings on a 20% sample. Our analysis finds that LLM-generated hints were 75% accurate and rated highly by human evaluators on consistency and clarity, but did not perform as well explaining why the hint was provided or its larger context. Our results demonstrate that LLMs may be used to augment tutoring systems with logic tutoring hints, but requires additional modifications to ensure accuracy and pedagogical appropriateness.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4456.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4456.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stepwise Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stepwise Proof Construction Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated metric measuring the percentage of individual proof steps generated by an LLM that are logically correct within the tutoring environment, reported per model, per prompt style, and per logical rule.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Stepwise Accuracy Metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute the fraction of generated proof steps that pass the tutor's logic checker (i.e., each step has a derived node, correct parent(s), and an allowed rule). Aggregated across all steps in test problems to produce per-model and per-prompt accuracy; further broken down by logical inference rule to produce rule-specific accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness of each derivation step (logical validity within tutor rules); also analyzed by rule complexity and parent-statement length.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3, GPT-4o, Gemini-Pro, Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (Llama-3-70B); others not specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Formal logic / intelligent tutoring (education)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Formal/mathematical proofs (multi-step deductive derivations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported stepwise accuracies per model and prompting technique; best reported: DeepSeek-V3 with FS_ToT_CoT achieved 86.7% on SPL and 85.0% on LT; GPT-4o reached 83.6% on SPL with FS_CoT. Rule-level accuracies for DeepSeek-V3 ranged from 99.23% (Commutation) down to 64.07% (Contrapositive). Longer parent statement lengths correlated with lower accuracy (incorrect parent length M=7.01 vs correct M=4.57; t=-12.6, p<0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: uses the tutor's built-in symbolic logic checker to validate each step; no human adjudication for stepwise correctness except aggregate analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validation by the tutor's domain rules and constraints; analyses of statistical significance (t-test) on parent length; comparison across prompt types and models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Only single-step correctness measured (does not assess global optimality or pedagogical quality of full solutions); susceptible to contextual ambiguity for long/nested parent statements; requires precise domain ruleset in the tutor.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SPL (Symbolic Propositional Logic, 310 multi-step problems) and LT (Logic Tutor dataset, 48 problems); test splits used as described.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4456.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4456.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hint Rubric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Four-dimension Rubric for Hint Explanation Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-evaluation rubric adapted from Roest et al. that rates generated hint explanations along four pedagogically-relevant dimensions: consistency, clarity, justification, and subgoaling, using a 1–4 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Next-step hint generation for introductory programming using large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Rubric-based Human Evaluation (Consistency / Clarity / Justification / Subgoaling)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Human experts rate each LLM-generated conceptual explanation on four dimensions (consistency, clarity, justification, subgoaling) on a 1–4 ordinal scale. Ratings are collected for a sampled subset (20% of generated hints) and averaged per dimension. The rubric was iteratively refined by domain experts to fit the tutor context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Consistency (alignment with tutor domain rules and current student state), Clarity (readability and absence of irrelevant detail), Justification (provides rationale for the suggested step), Subgoaling (explains how the step fits into a larger plan without revealing full solution).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3 (primary for hint generation evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational tutoring in formal propositional logic</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Pedagogical explanations / next-step hints (explanatory statements about proof steps)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human expert mean scores (out of 4) on sampled 240 explanations: Consistency 3.90, Clarity 3.09, Justification 2.24, Subgoaling 1.66. Shows strong human ratings on factual alignment and clarity but lower on reasoning for 'why' and larger-context subgoaling.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based: two domain experts independently rated 20% sample. Also compared against a rubric-based LLM grader (automated) applied to full sample.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Inter-rater reliability measured with Spearman correlation (ρ range 0.77–0.93) and Quadratic Weighted Cohen's Kappa (QWK range 0.79–0.92) indicating near-perfect agreement; rubric iteratively refined by experts and instructor.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Human evaluation performed only on 20% sample (potential generalizability limits); rubric may omit other pedagogical factors; LLM grader overestimates some dimensions (justification, subgoaling) relative to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>LT PSS dataset: 1,050 unique problem-solving states; 240 explanations sampled for human rating.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4456.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4456.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic Checker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tutor Built-in Logic Checker (Symbolic Verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated symbolic checker embedded in the logic tutor that verifies the logical validity of generated proof steps and evaluates hint correctness against tutor constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Automated Symbolic Logic Checker</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>A domain-specific checker that programmatically verifies whether a proposed derivation step is valid given current premises and tutor-allowed inference rules; for hints, a modified logic checker marks hints as incorrect if (a) the suggested step is logically invalid, (b) the suggested step already exists in the student's solution, or (c) required parent statements have not been derived.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Logical validity per tutor rules (derived node, correct parent(s), allowed rule), novelty relative to current student state, contextual applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Formal logic / automated reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Formal proof validity checking</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used to compute hint correctness: overall 75% of DeepSeek-V3 generated hints were marked correct by the modified logic checker on the PSS dataset. Also used to compute stepwise proof accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated; used as ground-truth for stepwise correctness and hint logical validity in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Specification by tutor domain rules and comparison with human expert judgments for a subset (human graders evaluated conceptual explanation quality but logic checker used for factual correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Rigid to the tutor's rule set (may not capture alternative valid pedagogical suggestions); cannot evaluate pedagogical appropriateness or explanatory quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4456.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4456.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rubric-based LLM Grader</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Rubric Grader for Hint Explanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated evaluator that uses an LLM to score generated explanations according to the same four-dimension rubric (consistency, clarity, justification, subgoaling) to approximate human grading at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Rubric-based LLM Grader</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>An LLM is prompted with the rubric and asked to grade each generated explanation along the four rubric dimensions, producing scores comparable to human ratings. Used to evaluate the full sample of generated explanations and compared to human expert scores on the 20% sample.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same four rubric dimensions: consistency, clarity, justification, subgoaling scored on 1–4 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational assessment / natural language evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automated evaluation of explanatory text</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLM grader produced consistency and clarity scores comparable to human raters but overestimated justification and subgoaling (average LLM subgoaling 2.47 vs human 1.66). The LLM grader showed weaker agreement with human graders, especially on subgoaling.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated grader compared directly to human expert ratings (hybrid evaluation); used automated grading to scale evaluation to full dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison of LLM-grader scores to human expert scores on the 20% sample; inter-method discrepancies analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Systematic overestimation in complex pedagogical dimensions (justification, subgoaling); weaker alignment with human judgments limits standalone trustworthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4456.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4456.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inter-rater Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inter-rater Reliability Measures (Spearman ρ & Quadratic Weighted Kappa)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Statistical metrics used to quantify agreement between the two human expert graders on rubric dimensions: Spearman rank correlation (ρ) for ordinal association and Quadratic Weighted Cohen's Kappa (QWK) for agreement accounting for chance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Inter-rater Reliability (Spearman correlation and QWK)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute Spearman ρ to measure monotonic association between two graders' ordinal scores; compute Quadratic Weighted Cohen's Kappa to assess agreement beyond chance and penalize larger disagreements more heavily. Used to validate rubric consistency across graders.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>High ρ and QWK indicate reliable, well-defined rubric application across graders.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational measurement / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Reliability assessment of human ratings</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Spearman ρ range 0.77–0.93 and QWK range 0.79–0.92 across dimensions, indicating near-perfect inter-rater reliability for the sampled human grading.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based reliability assessment metric; supports validity of human ratings used as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Standard statistical interpretation of ρ and QWK values; Bonferroni threshold referenced for significance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>High inter-rater agreement on the sampled subset does not guarantee broader generalizability; small sample (20% of dataset) used for human grading.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4456.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4456.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting Comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparative Prompting Strategies (ZS, FS, CoT, Dual-CoT, ToT variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of six progressively sophisticated prompting techniques (zero-shot, few-shot, chain-of-thought, dual-chain-of-thought, bidirectional dual CoT, tree-of-thoughts CoT) used as independent variables to evaluate LLM reasoning and explanation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Prompting Strategy Comparison Framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Systematically evaluate LLM performance (stepwise accuracy and hint generation) under multiple prompt engineering strategies: Zero-Shot (ZS), Few-Shot with Chain-of-Thought (FS_CoT), Few-Shot Logical Dual Chain-of-Thought (FS_L_DCoT), Few-Shot Bidirectional Logical Dual CoT (FS_BL_DCoT), Few-Shot Tree-of-Thoughts Chain-of-Thought (FS_ToT_CoT), and other FS variants. Compare per-prompt accuracies and qualitative outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Stepwise proof accuracy, hint correctness, and explanation quality as measured by rubric/human ratings; prompt complexity and length considered for deployment practicality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3, GPT-4o, Gemini-Pro, Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (Llama-3-70B); others unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM prompting methodology / educational AI</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation of generated deductive proofs and pedagogical explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>More complex prompting (e.g., FS_ToT_CoT) often improved stepwise accuracy (DeepSeek-V3: FS_ToT_CoT 86.7% on SPL) but longer/complex prompts were harder to adapt for hint generation; FS_CoT provided comparable hint accuracy with simpler prompts and was chosen as preferred for hint generation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (stepwise accuracy via logic checker) combined with human rubric evaluation for explanations; prompting effects analyzed across these metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Iterative prompt refinement on validation set then final evaluation on test set; comparison of metrics across prompt types and models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Trade-off between prompt complexity and adaptability to hint generation tasks; longer prompts may be impractical in deployment and not uniformly beneficial across metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4456.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4456.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmark Datasets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPL, LT, and PSS Benchmark Sets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three datasets used to evaluate models: SPL (Symbolic Propositional Logic) for multi-step problems, LT (Logic Tutor) problems designed for instruction, and PSS (Problem Solving States) extracted from student interaction logs for realistic hint generation testing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Dataset-based Benchmarking (SPL, LT, PSS)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use the SPL (310 translated multi-step problems) and LT (48 tutor problems) datasets to evaluate stepwise proof construction across models/prompts; use PSS (1,050 unique student problem-solving states extracted from tutor logs) to evaluate next-step hint generation and explanation quality in realistic tutoring states.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Stepwise accuracy on SPL and LT; hint correctness (as per modified logic checker) and rubric ratings (consistency, clarity, justification, subgoaling) on PSS; analysis across training levels and rule complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3 (primary for PSS hint generation), GPT-4o, Gemini-Pro, Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (Llama-3-70B); others unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational AI / automated reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Formal logical proofs and pedagogical explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>SPL and LT used to report stepwise accuracies (e.g., DeepSeek-V3 up to 86.7% on SPL); PSS used to generate 1,050 hints with overall 75% correctness by the modified logic checker; human rubric ratings applied to 240 sampled explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated logic checker for factual correctness; human experts for pedagogical rubric ratings (20% sample); LLM grader applied to full sample.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Dataset splits (training/validation/test); iterative prompt refinement on validation then test-only reporting; inter-rater reliability for human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LT dataset small (48 problems) and PSS human ratings sampled only 20% (240 explanations); domain-specific datasets limit cross-domain generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4456.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4456.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tutor vs LLM Hint Comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison with Tutor Data-driven Hint Mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comparative metric set contrasting LLM-generated hints with the logic tutor's historical data-driven hint mechanism, including counts of unique hints per problem and relative accuracy trends across difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Tutoring-system Comparison Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compare LLM hint behavior to the tutor's data-driven hint generator by (a) average number of unique hints produced per problem and (b) hint accuracy across training levels; use the same logic-checker criteria to mark hints correct/incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Number of unique hints per problem (diversity), hint correctness (as per logic checker), trend of accuracy across tutor training levels.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Intelligent tutoring systems / educational AI</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Pedagogical next-step hint generation</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>DeepSeek-V3 generated more unique hints per problem (M=4.5) than the tutor (M=3.0). Overall hint accuracy for DeepSeek-V3 was 75%; accuracy declined with tutor level difficulty (87.9% at level 1 down to 63.2% at level 5).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated correctness via modified logic checker used for comparison; pedagogical quality separately evaluated by human rubric.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct metric comparison under same logical validity rules; analysis across tutor difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Tutor's data-driven hints optimized from historical data and may prioritize different pedagogical goals (optimality) than LLMs; diversity of LLM hints may include more valid but less pedagogically optimal steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Next-step hint generation for introductory programming using large language models. <em>(Rating: 2)</em></li>
                <li>Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. <em>(Rating: 2)</em></li>
                <li>Proofwriter: Generating implications, proofs, and abductive statements over natural language. <em>(Rating: 2)</em></li>
                <li>Automating human tutor-style programming feedback: Leveraging gpt-4 tutor model for hint generation and gpt-3.5 student model for hint validation. <em>(Rating: 2)</em></li>
                <li>Generating high-precision feedback for programming syntax errors using large language models. <em>(Rating: 1)</em></li>
                <li>Enhancing llm-based feedback: Insights from intelligent tutoring systems and the learning sciences. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4456",
    "paper_id": "paper-278394560",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Stepwise Accuracy",
            "name_full": "Stepwise Proof Construction Accuracy",
            "brief_description": "An automated metric measuring the percentage of individual proof steps generated by an LLM that are logically correct within the tutoring environment, reported per model, per prompt style, and per logical rule.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Stepwise Accuracy Metric",
            "evaluation_method_description": "Compute the fraction of generated proof steps that pass the tutor's logic checker (i.e., each step has a derived node, correct parent(s), and an allowed rule). Aggregated across all steps in test problems to produce per-model and per-prompt accuracy; further broken down by logical inference rule to produce rule-specific accuracy.",
            "evaluation_criteria": "Correctness of each derivation step (logical validity within tutor rules); also analyzed by rule complexity and parent-statement length.",
            "model_name": "DeepSeek-V3, GPT-4o, Gemini-Pro, Llama-3-70B-Instruct",
            "model_size": "70B (Llama-3-70B); others not specified",
            "scientific_domain": "Formal logic / intelligent tutoring (education)",
            "theory_type": "Formal/mathematical proofs (multi-step deductive derivations)",
            "human_comparison": false,
            "evaluation_results": "Reported stepwise accuracies per model and prompting technique; best reported: DeepSeek-V3 with FS_ToT_CoT achieved 86.7% on SPL and 85.0% on LT; GPT-4o reached 83.6% on SPL with FS_CoT. Rule-level accuracies for DeepSeek-V3 ranged from 99.23% (Commutation) down to 64.07% (Contrapositive). Longer parent statement lengths correlated with lower accuracy (incorrect parent length M=7.01 vs correct M=4.57; t=-12.6, p&lt;0.001).",
            "automated_vs_human_evaluation": "Automated: uses the tutor's built-in symbolic logic checker to validate each step; no human adjudication for stepwise correctness except aggregate analysis.",
            "validation_method": "Validation by the tutor's domain rules and constraints; analyses of statistical significance (t-test) on parent length; comparison across prompt types and models.",
            "limitations_challenges": "Only single-step correctness measured (does not assess global optimality or pedagogical quality of full solutions); susceptible to contextual ambiguity for long/nested parent statements; requires precise domain ruleset in the tutor.",
            "benchmark_dataset": "SPL (Symbolic Propositional Logic, 310 multi-step problems) and LT (Logic Tutor dataset, 48 problems); test splits used as described.",
            "uuid": "e4456.0",
            "source_info": {
                "paper_title": "The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Hint Rubric",
            "name_full": "Four-dimension Rubric for Hint Explanation Evaluation",
            "brief_description": "A human-evaluation rubric adapted from Roest et al. that rates generated hint explanations along four pedagogically-relevant dimensions: consistency, clarity, justification, and subgoaling, using a 1–4 scale.",
            "citation_title": "Next-step hint generation for introductory programming using large language models.",
            "mention_or_use": "use",
            "evaluation_method_name": "Rubric-based Human Evaluation (Consistency / Clarity / Justification / Subgoaling)",
            "evaluation_method_description": "Human experts rate each LLM-generated conceptual explanation on four dimensions (consistency, clarity, justification, subgoaling) on a 1–4 ordinal scale. Ratings are collected for a sampled subset (20% of generated hints) and averaged per dimension. The rubric was iteratively refined by domain experts to fit the tutor context.",
            "evaluation_criteria": "Consistency (alignment with tutor domain rules and current student state), Clarity (readability and absence of irrelevant detail), Justification (provides rationale for the suggested step), Subgoaling (explains how the step fits into a larger plan without revealing full solution).",
            "model_name": "DeepSeek-V3 (primary for hint generation evaluation)",
            "model_size": null,
            "scientific_domain": "Educational tutoring in formal propositional logic",
            "theory_type": "Pedagogical explanations / next-step hints (explanatory statements about proof steps)",
            "human_comparison": true,
            "evaluation_results": "Human expert mean scores (out of 4) on sampled 240 explanations: Consistency 3.90, Clarity 3.09, Justification 2.24, Subgoaling 1.66. Shows strong human ratings on factual alignment and clarity but lower on reasoning for 'why' and larger-context subgoaling.",
            "automated_vs_human_evaluation": "Human-based: two domain experts independently rated 20% sample. Also compared against a rubric-based LLM grader (automated) applied to full sample.",
            "validation_method": "Inter-rater reliability measured with Spearman correlation (ρ range 0.77–0.93) and Quadratic Weighted Cohen's Kappa (QWK range 0.79–0.92) indicating near-perfect agreement; rubric iteratively refined by experts and instructor.",
            "limitations_challenges": "Human evaluation performed only on 20% sample (potential generalizability limits); rubric may omit other pedagogical factors; LLM grader overestimates some dimensions (justification, subgoaling) relative to humans.",
            "benchmark_dataset": "LT PSS dataset: 1,050 unique problem-solving states; 240 explanations sampled for human rating.",
            "uuid": "e4456.1",
            "source_info": {
                "paper_title": "The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Logic Checker",
            "name_full": "Tutor Built-in Logic Checker (Symbolic Verifier)",
            "brief_description": "An automated symbolic checker embedded in the logic tutor that verifies the logical validity of generated proof steps and evaluates hint correctness against tutor constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Automated Symbolic Logic Checker",
            "evaluation_method_description": "A domain-specific checker that programmatically verifies whether a proposed derivation step is valid given current premises and tutor-allowed inference rules; for hints, a modified logic checker marks hints as incorrect if (a) the suggested step is logically invalid, (b) the suggested step already exists in the student's solution, or (c) required parent statements have not been derived.",
            "evaluation_criteria": "Logical validity per tutor rules (derived node, correct parent(s), allowed rule), novelty relative to current student state, contextual applicability.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Formal logic / automated reasoning",
            "theory_type": "Formal proof validity checking",
            "human_comparison": false,
            "evaluation_results": "Used to compute hint correctness: overall 75% of DeepSeek-V3 generated hints were marked correct by the modified logic checker on the PSS dataset. Also used to compute stepwise proof accuracy.",
            "automated_vs_human_evaluation": "Automated; used as ground-truth for stepwise correctness and hint logical validity in experiments.",
            "validation_method": "Specification by tutor domain rules and comparison with human expert judgments for a subset (human graders evaluated conceptual explanation quality but logic checker used for factual correctness).",
            "limitations_challenges": "Rigid to the tutor's rule set (may not capture alternative valid pedagogical suggestions); cannot evaluate pedagogical appropriateness or explanatory quality.",
            "uuid": "e4456.2",
            "source_info": {
                "paper_title": "The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Rubric-based LLM Grader",
            "name_full": "LLM-based Rubric Grader for Hint Explanations",
            "brief_description": "An automated evaluator that uses an LLM to score generated explanations according to the same four-dimension rubric (consistency, clarity, justification, subgoaling) to approximate human grading at scale.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Rubric-based LLM Grader",
            "evaluation_method_description": "An LLM is prompted with the rubric and asked to grade each generated explanation along the four rubric dimensions, producing scores comparable to human ratings. Used to evaluate the full sample of generated explanations and compared to human expert scores on the 20% sample.",
            "evaluation_criteria": "Same four rubric dimensions: consistency, clarity, justification, subgoaling scored on 1–4 scale.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Educational assessment / natural language evaluation",
            "theory_type": "Automated evaluation of explanatory text",
            "human_comparison": true,
            "evaluation_results": "LLM grader produced consistency and clarity scores comparable to human raters but overestimated justification and subgoaling (average LLM subgoaling 2.47 vs human 1.66). The LLM grader showed weaker agreement with human graders, especially on subgoaling.",
            "automated_vs_human_evaluation": "Automated grader compared directly to human expert ratings (hybrid evaluation); used automated grading to scale evaluation to full dataset.",
            "validation_method": "Comparison of LLM-grader scores to human expert scores on the 20% sample; inter-method discrepancies analyzed.",
            "limitations_challenges": "Systematic overestimation in complex pedagogical dimensions (justification, subgoaling); weaker alignment with human judgments limits standalone trustworthiness.",
            "uuid": "e4456.3",
            "source_info": {
                "paper_title": "The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Inter-rater Metrics",
            "name_full": "Inter-rater Reliability Measures (Spearman ρ & Quadratic Weighted Kappa)",
            "brief_description": "Statistical metrics used to quantify agreement between the two human expert graders on rubric dimensions: Spearman rank correlation (ρ) for ordinal association and Quadratic Weighted Cohen's Kappa (QWK) for agreement accounting for chance.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Inter-rater Reliability (Spearman correlation and QWK)",
            "evaluation_method_description": "Compute Spearman ρ to measure monotonic association between two graders' ordinal scores; compute Quadratic Weighted Cohen's Kappa to assess agreement beyond chance and penalize larger disagreements more heavily. Used to validate rubric consistency across graders.",
            "evaluation_criteria": "High ρ and QWK indicate reliable, well-defined rubric application across graders.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Educational measurement / evaluation methodology",
            "theory_type": "Reliability assessment of human ratings",
            "human_comparison": false,
            "evaluation_results": "Spearman ρ range 0.77–0.93 and QWK range 0.79–0.92 across dimensions, indicating near-perfect inter-rater reliability for the sampled human grading.",
            "automated_vs_human_evaluation": "Human-based reliability assessment metric; supports validity of human ratings used as ground truth.",
            "validation_method": "Standard statistical interpretation of ρ and QWK values; Bonferroni threshold referenced for significance.",
            "limitations_challenges": "High inter-rater agreement on the sampled subset does not guarantee broader generalizability; small sample (20% of dataset) used for human grading.",
            "uuid": "e4456.4",
            "source_info": {
                "paper_title": "The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Prompting Comparisons",
            "name_full": "Comparative Prompting Strategies (ZS, FS, CoT, Dual-CoT, ToT variants)",
            "brief_description": "A set of six progressively sophisticated prompting techniques (zero-shot, few-shot, chain-of-thought, dual-chain-of-thought, bidirectional dual CoT, tree-of-thoughts CoT) used as independent variables to evaluate LLM reasoning and explanation quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Prompting Strategy Comparison Framework",
            "evaluation_method_description": "Systematically evaluate LLM performance (stepwise accuracy and hint generation) under multiple prompt engineering strategies: Zero-Shot (ZS), Few-Shot with Chain-of-Thought (FS_CoT), Few-Shot Logical Dual Chain-of-Thought (FS_L_DCoT), Few-Shot Bidirectional Logical Dual CoT (FS_BL_DCoT), Few-Shot Tree-of-Thoughts Chain-of-Thought (FS_ToT_CoT), and other FS variants. Compare per-prompt accuracies and qualitative outcomes.",
            "evaluation_criteria": "Stepwise proof accuracy, hint correctness, and explanation quality as measured by rubric/human ratings; prompt complexity and length considered for deployment practicality.",
            "model_name": "DeepSeek-V3, GPT-4o, Gemini-Pro, Llama-3-70B-Instruct",
            "model_size": "70B (Llama-3-70B); others unspecified",
            "scientific_domain": "LLM prompting methodology / educational AI",
            "theory_type": "Evaluation of generated deductive proofs and pedagogical explanations",
            "human_comparison": false,
            "evaluation_results": "More complex prompting (e.g., FS_ToT_CoT) often improved stepwise accuracy (DeepSeek-V3: FS_ToT_CoT 86.7% on SPL) but longer/complex prompts were harder to adapt for hint generation; FS_CoT provided comparable hint accuracy with simpler prompts and was chosen as preferred for hint generation.",
            "automated_vs_human_evaluation": "Automated (stepwise accuracy via logic checker) combined with human rubric evaluation for explanations; prompting effects analyzed across these metrics.",
            "validation_method": "Iterative prompt refinement on validation set then final evaluation on test set; comparison of metrics across prompt types and models.",
            "limitations_challenges": "Trade-off between prompt complexity and adaptability to hint generation tasks; longer prompts may be impractical in deployment and not uniformly beneficial across metrics.",
            "uuid": "e4456.5",
            "source_info": {
                "paper_title": "The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Benchmark Datasets",
            "name_full": "SPL, LT, and PSS Benchmark Sets",
            "brief_description": "Three datasets used to evaluate models: SPL (Symbolic Propositional Logic) for multi-step problems, LT (Logic Tutor) problems designed for instruction, and PSS (Problem Solving States) extracted from student interaction logs for realistic hint generation testing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Dataset-based Benchmarking (SPL, LT, PSS)",
            "evaluation_method_description": "Use the SPL (310 translated multi-step problems) and LT (48 tutor problems) datasets to evaluate stepwise proof construction across models/prompts; use PSS (1,050 unique student problem-solving states extracted from tutor logs) to evaluate next-step hint generation and explanation quality in realistic tutoring states.",
            "evaluation_criteria": "Stepwise accuracy on SPL and LT; hint correctness (as per modified logic checker) and rubric ratings (consistency, clarity, justification, subgoaling) on PSS; analysis across training levels and rule complexity.",
            "model_name": "DeepSeek-V3 (primary for PSS hint generation), GPT-4o, Gemini-Pro, Llama-3-70B-Instruct",
            "model_size": "70B (Llama-3-70B); others unspecified",
            "scientific_domain": "Educational AI / automated reasoning",
            "theory_type": "Formal logical proofs and pedagogical explanations",
            "human_comparison": true,
            "evaluation_results": "SPL and LT used to report stepwise accuracies (e.g., DeepSeek-V3 up to 86.7% on SPL); PSS used to generate 1,050 hints with overall 75% correctness by the modified logic checker; human rubric ratings applied to 240 sampled explanations.",
            "automated_vs_human_evaluation": "Hybrid: automated logic checker for factual correctness; human experts for pedagogical rubric ratings (20% sample); LLM grader applied to full sample.",
            "validation_method": "Dataset splits (training/validation/test); iterative prompt refinement on validation then test-only reporting; inter-rater reliability for human ratings.",
            "limitations_challenges": "LT dataset small (48 problems) and PSS human ratings sampled only 20% (240 explanations); domain-specific datasets limit cross-domain generalizability.",
            "uuid": "e4456.6",
            "source_info": {
                "paper_title": "The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Tutor vs LLM Hint Comparison",
            "name_full": "Comparison with Tutor Data-driven Hint Mechanism",
            "brief_description": "A comparative metric set contrasting LLM-generated hints with the logic tutor's historical data-driven hint mechanism, including counts of unique hints per problem and relative accuracy trends across difficulty levels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Tutoring-system Comparison Metrics",
            "evaluation_method_description": "Compare LLM hint behavior to the tutor's data-driven hint generator by (a) average number of unique hints produced per problem and (b) hint accuracy across training levels; use the same logic-checker criteria to mark hints correct/incorrect.",
            "evaluation_criteria": "Number of unique hints per problem (diversity), hint correctness (as per logic checker), trend of accuracy across tutor training levels.",
            "model_name": "DeepSeek-V3",
            "model_size": null,
            "scientific_domain": "Intelligent tutoring systems / educational AI",
            "theory_type": "Pedagogical next-step hint generation",
            "human_comparison": false,
            "evaluation_results": "DeepSeek-V3 generated more unique hints per problem (M=4.5) than the tutor (M=3.0). Overall hint accuracy for DeepSeek-V3 was 75%; accuracy declined with tutor level difficulty (87.9% at level 1 down to 63.2% at level 5).",
            "automated_vs_human_evaluation": "Automated correctness via modified logic checker used for comparison; pedagogical quality separately evaluated by human rubric.",
            "validation_method": "Direct metric comparison under same logical validity rules; analysis across tutor difficulty levels.",
            "limitations_challenges": "Tutor's data-driven hints optimized from historical data and may prioritize different pedagogical goals (optimality) than LLMs; diversity of LLM hints may include more valid but less pedagogically optimal steps.",
            "uuid": "e4456.7",
            "source_info": {
                "paper_title": "The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Next-step hint generation for introductory programming using large language models.",
            "rating": 2,
            "sanitized_title": "nextstep_hint_generation_for_introductory_programming_using_large_language_models"
        },
        {
            "paper_title": "Logicbench: Towards systematic evaluation of logical reasoning ability of large language models.",
            "rating": 2,
            "sanitized_title": "logicbench_towards_systematic_evaluation_of_logical_reasoning_ability_of_large_language_models"
        },
        {
            "paper_title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language.",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Automating human tutor-style programming feedback: Leveraging gpt-4 tutor model for hint generation and gpt-3.5 student model for hint validation.",
            "rating": 2,
            "sanitized_title": "automating_human_tutorstyle_programming_feedback_leveraging_gpt4_tutor_model_for_hint_generation_and_gpt35_student_model_for_hint_validation"
        },
        {
            "paper_title": "Generating high-precision feedback for programming syntax errors using large language models.",
            "rating": 1,
            "sanitized_title": "generating_highprecision_feedback_for_programming_syntax_errors_using_large_language_models"
        },
        {
            "paper_title": "Enhancing llm-based feedback: Insights from intelligent tutoring systems and the learning sciences.",
            "rating": 1,
            "sanitized_title": "enhancing_llmbased_feedback_insights_from_intelligent_tutoring_systems_and_the_learning_sciences"
        }
    ],
    "cost": 0.01421975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems
7 May 2025</p>
<p>Sutapa Dey Tithi 
Arun Kumar 
Clara Dimarco 
Xiaoyi Tian 
Nazia Alam 
Kimia Fazeli 
Tiffany Barnes 
The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems
7 May 202529E13CFF873C191644E77D277D837694arXiv:2505.04736v1[cs.AI]Intelligent Tutoring SystemsNext-step hintsSymbolic LogicLLMsGenerative AIData MiningDeepSeek-V3GPT-4o
Intelligent tutoring systems have demonstrated effectiveness in teaching formal propositional logic proofs, but their reliance on template-based explanations limits their ability to provide personalized student feedback.While large language models (LLMs) offer promising capabilities for dynamic feedback generation, they risk producing hallucinations or pedagogically unsound explanations.We evaluated the stepwise accuracy of LLMs in constructing multi-step symbolic logic proofs, comparing six prompting techniques across four state-of-the-art LLMs on 358 propositional logic problems.Results show that DeepSeek-V3 achieved superior performance with 84.4% accuracy on stepwise proof construction and excelled particularly in simpler rules.We further used the best-performing LLM to generate explanatory hints for 1,050 unique student problem-solving states from a logic ITS and evaluated them on 4 criteria with both an LLM grader and human expert ratings on a 20% sample.Our analysis finds that LLM-generated hints were 75% accurate and rated highly by human evaluators on consistency and clarity, but did not perform as well explaining why the hint was provided or its larger context.Our results demonstrate that LLMs may be used to augment tutoring systems with logic tutoring hints, but requires additional modifications to ensure accuracy and pedagogical appropriateness.</p>
<p>Introduction</p>
<p>Intelligent tutoring systems (ITSs) serve as scalable alternatives to human tutoring [20,49].These systems automatically evaluate student solutions and provide adaptive feedback, including next-step hints and explanations.Such personalized assistance has been shown to enhance student learning outcomes [50].Although various data-driven methods have been shown to save time and resources by reducing the need for an expert to construct hints, they rely on the quantity and quality of training data.Moreover, the hints in these systems are often template-based with limited adaptation [45].In this context, large language models (LLMs) can offer opportunities to augment ITS capabilities through scalable feedback generation [48].However, integrating LLMs into ITSs faces challenges: LLMs may hallucinate incorrect information [52], which risks misleading students and reinforcing misconceptions [31].LLMs may also generate correct answers without adhering to sound instructional principles, potentially limiting their effectiveness in fostering deep learning [22,4].Complicating matters for integrating LLM-based help in tutoring systems, novices may lack metacognitive skills to identify what and when to ask for help (help avoidance), and they could abuse the system by asking for direct answers or overly frequent hints (help abuse) [40].Thus, the quality of solutions, adherence to pedagogical principles, and adaptation based on metacognitive needs are as crucial as accuracy for help in learning environments [18].</p>
<p>Formal logic proofs are essential in STEM education.Several studies have evaluated LLM performance in solving logic problems and handling natural language logical reasoning tasks [32,41].However, LLMs' ability to handle complex, multi-step symbolic logic proofs within pedagogical contexts is largely unexplored.In this work, we evaluate how well LLMs construct multistep symbolic propositional logic proofs and next-step natural language hints in an intelligent logic tutor.We compared six prompting techniques across four state-of-the-art LLMs on 358 propositional logic problems, leveraging advanced prompt engineering to incorporate learning science theories.The quality of hints and explanations from the best-performing LLM was assessed by a rubric-based LLM grader and 20% were also rated by human experts on a 4-dimensional rubric.We investigated the following research questions:</p>
<p>RQ1 To what extent can LLMs construct accurate step-by-step solutions for propositional logic (PL) proofs within a logic tutor?</p>
<p>• RQ1a: How do prompting strategies influence proof construction performance?</p>
<p>• RQ1b: How does performance vary across a selection of four open-source and proprietary LLM models?</p>
<p>RQ2 How well does the most accurate LLM generate pedagogically viable hints and explanations?</p>
<p>• RQ2a: How does hint correctness vary across logic rules of differing complexity?</p>
<p>• RQ2b: How do domain experts assess the pedagogical quality of LLMgenerated explanations in terms of consistency, clarity, justification, and subgoaling?</p>
<p>Related Work</p>
<p>Intelligent Tutoring Systems and Automated Feedback.ITSs have established their effectiveness in promoting student learning through personalization across multiple fields, including mathematics [6,21,37,11], probability [43,12], logic [26], and programming languages [6,35,17].The evolution of these systems has led to various data-driven techniques for providing adaptive support.Murray et al. [29] demonstrated that data-driven assistance can efficiently generate hints while reducing expert involvement.This approach was further advanced by Barnes et al. through the Hint Factory, a method for generating next-step hints in propositional logic [8].Further research has confirmed that both on-demand and unsolicited hints can significantly enhance student learning when appropriately implemented [8,44,26].Despite the popularity and benefits of data-driven hints in ITSs, the current approaches have some limitations.Many systems provide immediate next-step hints without providing conceptual explanations [35,38].Marwan et al. showed that novices perceived hints with explanations as significantly more relevant and interpretable than those without explanations in a block-based programming environment, and hints without conceptual explanation can decrease students' trust in their helpfulness [27].While incorporating textual explanations alongside procedural hints has been shown to be useful, generating these explanations traditionally requires extensive manual effort.</p>
<p>Large Language Models in Logic.Recent applications demonstrate LLMs' effectiveness in providing conversational support and personalized instruction across various educational contexts [51,47,39].Aashish et al. showed students in an introductory programming course utilized a custom GPT-4 AI tool for assignment help, and students primarily asked AI assistance for conceptual understanding rather than complete solutions [13].Systems like HypoCompass have demonstrated that LLM-augmented ITS can generate high-quality training materials and significantly improve student performance [25].However, the application of LLMs to formal logic instruction presents unique challenges.While foundational datasets like ProofWriter [46] enable multi-hop proofs and FOLIO [15] introduces complex first-order logic (FOL) expressions, existing benchmarks remain narrowly scoped.Pron-toQA [41] focuses on just one FOL rule (modus ponens).LogicBench [32] covers a larger set of inference rules, and they reveal critical performance gaps even in state-of-the-art models like GPT-4.Moreover, performance can decrease for complex operations, with accuracy dropping below 45% for proofs requiring three or more steps [42].Prompt engineering techniques like Chainof-Thought (CoT) have improved LLMs' reasoning capabilities, but ensuring the faithfulness of generated reasoning chains remains challenging [24].Careful prompt engineering is also important in ensuring the effective utilization of LLMs in educational contexts [45].These approaches predominantly address LLM performance on natural language logic problems or isolated inference rules, with limited work exploring multi-step symbolic propositional logic proofs.Additionally, these LLMs are evaluated using traditional metrics such as accuracy and coverage [33,34]; they are rarely assessed for the suitability of their explanations for educational contexts.Our work bridges these gaps by investigating LLMs' capabilities in multi-step symbolic logic proof construction and explanatory hint generation, with the goal of combining the pedagogical rigor of traditional ITS with the natural language capabilities of LLMs.We extend traditional evaluation metrics to include consistency (the what), clarity &amp; relevance (the readability), justification (the why), and subgoaling (describing the why within a larger context), as described below, providing a more comprehensive assessment of LLM-generated hint quality in logic education.</p>
<p>Methodology</p>
<p>Proof Construction.We selected two datasets to evaluate LLMs' performance in proof construction: (1) The Symbolic Propositional Logic (SPL) dataset contains 310 multi-step problems.These problems were originally in natural language form, then Gottlieb et al. systematically translated them into symbolic PL representations [14].(2) The logic tutor dataset (LT ) contains 48 PL problems of varying difficulty.These problems are designed specifically for educational purposes, with each proof typically requiring 12-15 steps to complete.In both SPL and LT datasets, each problem contains the given premises and a conclusion, which are fed into LLMs using carefully engineered prompts.LLMs output the complete step-by-step proof along with a structured solution in JSON format.</p>
<p>Logic Tutor Problem Solving State (PSS) Dataset Extraction.To enable realistic testing of next-step hint generation, a dataset was extracted from interaction logs of all solution attempts to 18 problems each by 30 students using the logic tutor for homework in Fall 2024.Students were randomly selected from all CS majors at a US public university, whose 2021-22 graduating class demographics comprise 83% men and 17% women; with 58% white, 18.5% Asian, 3% Hispanic/Latin, 2% Black/African American, 9% other races.IRB approval was obtained, and only authorized researchers could access participant data.We developed an automated pipeline to convert the interaction logs into a textual representation of student problem solving states (PSS) (snapshots of problem solving after each step) (Figure 1).These PSS representations were fed into our final LLM framework using carefully engineered prompts to generate targeted next-step hints along with conceptual explanations for each PSS (Figure 1).</p>
<p>Intelligent Logic Tutor.Our logic tutor presents propositional logic (PL) problems to students in a graphical representation.Each problem shows a set of given statements at the top of the workspace and a conclusion to be derived at the bottom (Figure 2).Students solve the problems by iteratively deriving new logic statements until they arrive at the conclusion.The tutor divides the problems into three sections: pretest, training, and posttest.Students' prior knowledge is measured in the pretest section.The training section consists of five ordered levels with increasing difficulty.This is the only section where students can request and receive hints.Finally, students complete a posttest section consisting of six problems without any tutor assistance.The tutor uses a data-driven method to generate next-step</p>
<p>Student Progress</p>
<p>Final Conclusion: J ∨ K Derivation Steps:
1. F → (G ∧ ¬H), Given 2. I ∧ F , Given 3. H ∨ J, Given 4. ¬F ∨ (G ∧ ¬H), Derived from (1) using Impl</p>
<p>LLM-Generated Hint</p>
<p>Hint: Derive F using Simp on line 2. Explanation: Line 2 (I ∧ F ) contains F , which is important for simplifying other statements.By deriving F from I ∧ F , we can use it to simplify line 4 (¬F ∨ (G ∧ ¬H)) and move closer to the conclusion.</p>
<p>Tutor-Generated Hint</p>
<p>Hint: Click I ∧ F and click on rule Simplification to derive F .hints with template-based explanations to guide students given their current progress.</p>
<p>Model Selection and Prompt Engineering</p>
<p>We evaluated four state-of-the-art LLMs: Llama-3-70B-Instruct [2], Gemini-Pro [7], GPT-4o [1], and DeepSeek-V3 [23].We set the temperature parameter as 0.1 for all LLMs to limit the randomness and variability of responses and improve accuracy.We opted not to use the LLMs, such as o1 or DeepSeek-Reasoner, as they currently require significant computational resources and are not yet practical for deployment in a classroom setting.Each prompt is structured consistently (Figure 3) and is designed to incorporate evidence-based learning science principles [45].Our prompt engineering framework implements six increasingly advanced techniques, building upon Zero-Shot (ZS) and Few-Shot (FS) prompting [10] with Chain-of-Thought (CoT) reasoning [53].</p>
<p>Zero Shot (ZS) Zero-shot prompting elicits step-by-step solutions and output expectations without providing any training examples.Thus, it relies on the model's inherent reasoning and problem-solving capabilities, testing Few Shot Logical Dual Chain of Thought (FS_L_DCoT) Building on the CoT concept, this approach integrates both direct and indirect proof strategies.This technique provides the LLM with examples that demonstrate how to consider alternate logical paths and cross-verify them.By encouraging the exploration of multiple reasoning paths, this approach reduces the likelihood of overlooking potential mistakes or alternative solutions.</p>
<p>Few Shot Bidirectional Logical Dual Chain of Thought (FS_BL _DCoT) This strategy augments the Dual CoT by combining forward chaining (from premises to conclusion) with backward chaining (from conclusion back to premises).By working in both directions, the model may gain an additional layer of validation and error detection.</p>
<p>Few Shot Tree of Thoughts Chain of Thought (FS_ToT_CoT) In this approach, the solution path branches out across multiple reasoning "threads" or experts, each contributing a step-by-step proof.We added examples in the prompt where three experts collaboratively debated each step, acknowledged and corrected mistakes, and refined their step derivations.This collaborative interplay forms a tree-like structure of possible solution paths: if a line of reasoning fails, it is pruned when the expert acknowledges their mistake; successful lines continue to expand.This approach is meant to ensure that alternative proofs are considered and validated.</p>
<p>Data Splits</p>
<p>Each dataset is divided into training, validation, and test sets.Unlike a traditional machine learning setup, the training set consists of a handcurated set of examples used for few-shot prompting.We iteratively refined the prompt templates based on the validation set performance until further modifications yielded diminishing returns.Once the prompt template was finalized, we ran the models only on the test set and reported the final results.</p>
<p>Evaluation Framework</p>
<p>The built-in logic checker in the logic tutor automatically verified the correctness of all LLM-generated proof steps and hints, ensuring relevance within the tutor context.We conducted a qualitative evaluation of LLMgenerated hints and explanations by two experts with 2-4 years of experience as teaching assistants and researchers for logic and ITS.The evaluation rubrics (Table 1) were adapted from the framework proposed by Roest et al. [39], including: consistency, clarity, justification, and subgoaling.To ensure their suitability for our tutor context, two domain experts collaborated and iteratively refined them until a consensus was reached with a course instructor expert.Twenty percent of the generated hints and explanations were rated on each rubric on a scale from 1 to 4 (with 4 being the best).We used Spearman correlation (ρ) and Quadratic Weighted Cohen's Kappa (QW K) to obtain inter-rater agreement of the assessment dimensions, which are common reliability measurements between two raters [48,30].We also used a rubric-based LLM grader to understand the potential of using it as an automated evaluator.Step-by-step proofs, where each step should have derived node, parents, rule applied 3. Acceptable rules: <allowed rules in the tutor> 4. Final "structured solution" in JSON-like format Output Expectation -A detailed step-by-step derivation followed by a JSONlike "structured solution."-A detailed step-by-step derivation, a next-step hint followed by a conceptual explanation Few-Shot Examples -Demonstrates prompt specific proof strategies, step-bystep derivations and the final JSON-like output.</p>
<p>-Demonstrates a step-by-step proof, one-line next-step hint, a conceptual explanation of the hint User Prompt References givens, conclusion and/or current progress and instructions to produce a formal proof/hint.</p>
<p>Results</p>
<p>RQ1: Logic Proof Construction Performance</p>
<p>We evaluated the performance of four LLMs (Llama-3, Gemini-Pro, GPT-4o, and DeepSeek-V3) in six prompting techniques on the two logic tutor (LT) and symbolic propositional logic (SPL) datasets.DeepSeek-V3 demonstrated superior accuracy in most prompt configurations, with the best FS_ToT_CoT prompts achieving 85.0% and 86.7% accuracy of the steps generated for full solutions of LT and SPL problems, respectively (Table 2).GPT-4o also exhibited strong performance, particularly on SPL with FS_CoT prompts (83.6%).Gemini-Pro showed mixed results, while Llama-3 generally underperformed.All zero-shot prompted models underperformed,</p>
<p>Criteria Definition</p>
<p>Consistency Does the hint follow the tutor's domain rules and align with the student's current proof state?Clarity Is the hint clearly written and free of irrelevant or excessive details? Justification Does the hint provide a rationale or reasoning for why the next step was suggested?Subgoaling Does the hint provide a larger context (without giving away the full solution) explaining how the step will help work on other statements?indicating the importance of in-context prompting in the integration of LLMs into ITSs.Lengths of the parent statements were significantly longer for incorrect (M=7.01)versus correct (M=4.57)steps (t=-12.6,p&lt;0.001).This suggests longer parent statements may introduce ambiguity, leading to more incorrect steps.Rule-Specific Performance Analysis.For all the FS-based prompting techniques and the GPT-4o and DeepSeek-V3 models, we calculated the average accuracy across different logical rules.As shown in Table 3, DeepSeek-V3 and GPT-4o both achieved high accuracy on basic rules (Com, Add, Conj, Simp) while DeepSeek-V3 maintained stronger performance on the rules MP, DeM, MT, HS, DS and Contra.The performance of GPT-4o declined with higher complexity.The most challenging rules were Contra, CP, DN, and Impl, although DeepSeek-V3 still outperformed GPT-4o.</p>
<p>RQ2a: Evaluating Next-step Hint Generation Performance</p>
<p>Based on RQ1, DeepSeek-V3 performed better than all other LLMs.While more complex prompting like bidirectional dual CoT and tree-ofthoughts CoT improved accuracy, their lengthier and more complex prompts appeared difficult to adapt for the hint generation task.FS_CoT achieved comparable accuracy with simpler prompting, making it our preferred choice.Using DeepSeek-V3 with FS_CoT, we generated next-step hints and conceptual explanations for the LT PSS dataset containing 1,050 unique student problem-solving states.Each state represents a snapshot of a student's progress, containing all steps completed at that point, and transformed from a graphical into a natural language-based form.DeepSeek-V3 with FS_CoT was used to generate the next best step hint for each state.A modified version of the LT logic checker was used to evaluate correctness, with any hint considered to be incorrect if the step was logically incorrect, the suggested step was already present in the student solution, or if the parent steps were not yet derived.Overall, 75% of the resulting hints were rated as correct/accurate.Accuracy across the specific logical rules is shown in Figure 4.</p>
<p>The model struggled most with complex rules like DS, Impl, Add, DN, MT, DeM, CP, and CD.With the exception of Addition, all of these rules handle negations, introducing complexity into the application of rules.Addition introduces a new variable into a proof, which adds another type of complexity.We compared the performance of DeekSeek-V3 to our logic tutor's datadriven hint mechanism, which utilizes historical data to identify optimal next steps.DeepSeek-V3 generated a higher number of unique hints per problem on average (M=4.5)compared to the tutor (M=3.0).The overall hint accuracy of 75% is promising.However, accuracy changed across the tutor levels.While the LLM achieved 87.9% accuracy in the first training level, the number of unique hints increased, and accuracy declined across the 5 training levels.Since difficulty and the number of logic rules needed increase at each level, the size of the potential solution space expands.This larger solution space may have led to lower hint accuracy and the generation of more unique hints by DeepSeek-V3.</p>
<p>RQ2b: Evaluating Next-step Hint Explanations</p>
<p>Two expert graders independently applied the evaluation rubrics (Table 1) to a sample of 240 LLM-generated conceptual explanations.Table 4 shows a strong inter-rater agreement for the expert human graders across all dimensions.The Spearman correlation (ρ) ranged from 0.77 to 0.93, and the QW K values ranged from 0.79 to 0.92, suggesting near-perfect reliability  [28].These high agreement metrics suggest that our evaluation criteria were well-defined and consistently applied by the expert human raters.The sampled LLM explanations were highly rated for consistency and clarity, with human-rater scores of 3.90 and 3.09 out of 4, respectively.However, the system showed room for improvement in other areas, with expert human rater scores of 2.24 for justification and 1.66 for subgoaling.We applied an LLM grader using the same rubrics to the full sample of explanations and compared their rubric scores with the expert human ratings.While the LLM grader assigned consistency and clarity scores comparable to human scores, it overestimated human scores in justification and subgoaling.However, the rubric-based LLM grader exhibited weaker agreement with human graders, particularly in subgoaling, where the average LLM subgoaling score was 2.47 as opposed to the human subgoaling score of 1.66.</p>
<p>Discussion</p>
<p>Logic rules can be considered basic or complex based on their use of negation.Basic rules such as Simplification or Commutative involve direct and single-step operations (e.g., (p ∨ q) =⇒ (q ∨ p)).On the other hand, the rules involving negation (e.g., DeMorgans) cause challenges for people and for LLMs.The implication operator → inherently includes negation as part of its definition ((p → q) ⇐⇒ (¬p ∨ q)).Statements that combine both negation and implication increase the complexity even more, such as Modus Tollens and Contrapositive ((p → q) =⇒ (¬q → ¬p)).Johnson et al. argued that rules involving negation require working memory to track inverted truth values across multiple steps, thus increasing cognitive load [19].Anderson et al. reported that humans exhibit slower response times and higher error rates when processing negated statements like ¬(A ∧ B), due to the increased cognitive load associated with maintaining dual truth state [5].</p>
<p>The challenge in tackling negation was also observed in the performance of the involved LLM models in our study.In logic proof construction, although DeepSeek-V3 (Table 2) achieved high accuracy on basic rules (e.g., Commutative (Com): 99.23%), the model struggled with more complex rules (e.g., CP: 64.07%).Our results suggest that LLMs encounter difficulties in propagating negation through inference chains, often struggling with expressions like ¬(A ∧ B) and ¬A ∧ ¬B.This resembles a similar reasoning bottleneck found in human cognition.</p>
<p>The length of parent logic statements being combined also introduces complexity, usually via nested statements, which further introduces more cognitive load and working memory for humans.Similarly, a strong negative correlation was found between parent statement length and accuracy of LLMs.It may be that longer parent statements introduce ambiguities or combinatorial explosions that can exceed the contextual reasoning capacity of current LLM models.Preprocessing inputs into simpler structures may improve the logical reasoning performance of current LLMs.</p>
<p>Overall, DeepSeek-V3 with FS_CoT achieved an average accuracy of 75% for hint generation, but its hint accuracy was higher for earlier training problems and declined across the training levels as difficulty increased (from 87.9% to 63.2%).These accuracy levels are not high enough for integration into ITSs, as student trust can shift quickly when encountering even one that is inaccurate or not aligned with student thinking, potentially leading to help avoidance [36].In our experiment, LLMs also sometimes generated sub-optimal hints based on the redundant statements derived by the students and failed to reorient the students to an optimal solution path.</p>
<p>Our LLM-generated hint explanations achieved higher scores on the consistency and clarity dimensions (Table 4), indicating that our selected LLM could follow specified rules and provide clear explanations.However, its justification and subgoaling scores were comparatively lower, suggesting LLMs sometimes struggle to articulate why a particular step was suggested within the larger problem or subgoal context.This limitation echoes critiques of LLMs as "stochastic parrots" [9] in educational contexts-they replicate surface-level patterns but lack intentional pedagogy, which is critical for learning [3].Together with inaccuracy and these explanation ratings, our results highlight the need to build hybrid systems in which LLMs propose candidate hints, but they are checked for domain validity (e.g.symbolic checkers in math [16]) and for pedagogical appropriateness.</p>
<p>Conclusion</p>
<p>Our work provides insights into the potential and current limitations of LLMs in logic tutoring systems, specifically examining their capabilities across proof construction, hint generation, and explanation quality.While models like DeepSeek-V3 and GPT-4o demonstrate promising performance in generating step-by-step solutions (achieving up to 86.7% accuracy) and producing factually correct hints (75% accuracy) with clear and consistent conceptual explanations, significant challenges remain in ensuring better pedagogical alignment.Our study also had several limitations.Our evaluation was limited to single-step accuracy for problem-solving in one domain.Future work should evaluate the optimality of LLM-constructed full solutions.The human evaluation of hint explanations covered only 20% of the dataset, potentially limiting generalizability; and although we evaluated 4 criteria for hint explanations, other pedagogical elements may be important for human understanding and learning.Future work should focus on developing hybrid architectures to combine LLM capabilities with learning science principles to bridge the gap between the potential of LLMs and the nuanced demands of a pedagogical environment.</p>
<p>Figure 1 :
1
Figure 1: Example of a student problem-solving state and generated hints.The top box presents the student's derivation steps, while the bottom two boxes present the LLM-and tutor-generated hints.</p>
<p>Figure 2 :
2
Figure 2: The full interface of the logic tutor, showing the given statements at the top of the workspace and a conclusion to be derived at the bottom.The middle pane contains the rule buttons.</p>
<p>(e.g., "You are an intelligent tutor in propositional logic...") Instructions 1.</p>
<p>Figure 3 :
3
Figure 3: Abstract prompt template incorporating context, instructions, output expectations, examples, and user prompts.</p>
<p>Figure 4 :
4
Figure 4: The distribution of correct and incorrect LLM-generated hints for each rule.</p>
<p>Table 1 :
1
[39]uation Criteria for LLM-generated Hint Explanations, adapted from Roest et al.[39]</p>
<p>Table 2 :
2
Performance of different LLMs in different zero-shot and few-shot settings, measured by accuracy on two datasets: LT and SPL.</p>
<p>Table 3 :
3
Rule accuracy for comparison of DeepSeek-V3 and GPT-4o across all prompting techniques.
RuleDetailDeepSeek-V3 GPT-4oCommutation(p ∨ q) =⇒ (q ∨ p)99.2389.28Additionp =⇒ (p ∨ q)95.4687.45Simplification(p ∧ q =⇒ p)92.1683.34Conjunction(p, q) =⇒ (p ∧ q)90.7690.00Modus Ponens (MP) ((p → q) ∧ p) =⇒ q89.1782.26DeMorgan's (DeM)¬(p ∧ q) =⇒ (¬p ∨ ¬q)87.0176.97Modus Tollens (MT) ((p → q) ∧ ¬q) =⇒ ¬p86.8074.68Hyp. Syll. (HS)((p → q) ∧ (q → r) =⇒ (p → r)86.3575.83Disj. Syll. (DS)((p ∨ q) ∧ ¬p) =⇒ q78.0662.31Contradiction(p ∧ ¬p) ⇐⇒ 076.6738.24Implication(p → q) ⇐⇒ (¬p ∨ q)72.2668.52Double Negation (DN) ¬¬p =⇒ p69.2067.05Contrapositive (CP) (p → q) =⇒ (¬q → ¬p)64.0745.77</p>
<p>Table 4 :
4
Inter-rater agreement metrics for evaluation of 210 LLM-generated explanations.ρ = Spearman's Correlation, QW K = Quadratic Cohen's Kappa.† p &gt; Bonferroni threshold .</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>A I , Meta , Llama 3 model card URL. 2024</p>
<p>Help seeking and help design in interactive learning environments. V Aleven, E Stahl, S Schworm, F Fischer, R Wallace, Review of educational research. 732003</p>
<p>An effective metacognitive strategy: Learning by doing and explaining with a computer-based cognitive tutor. V A Aleven, K R Koedinger, Cognitive science. 262002</p>
<p>The adaptive character of thought. J R Anderson, 2013Psychology Press</p>
<p>Cognitive tutors: Lessons learned. J R Anderson, A T Corbett, K R Koedinger, R Pelletier, The journal of the learning sciences. 41995</p>
<p>R Anil, S Borgeaud, Y Wu, J B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, K Millican, arXiv:2312.118051Gemini: A family of highly capable multimodal models. 2023arXiv preprint</p>
<p>A pilot study on logic proof tutoring using hints generated from historical student data. T Barnes, J C Stamper, L Lehmann, M J Croy, 2008EDM</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 332020</p>
<p>W Canfield, Aleks: A web-based intelligent tutoring system. Mathematics and Computer Education 35. 2001152</p>
<p>Meta-cognitive strategy instruction in intelligent tutoring systems: how, when, and why. M Chi, K Vanlehn, Journal of Educational Technology &amp; Society. 132010</p>
<p>Coding with ai: How are tools like chatgpt being used by students in foundational programming courses. A Ghimire, J Edwards, ternational Conference on Artificial Intelligence in Education. Springer2024</p>
<ol>
<li>propositional-logic. E Gottlieb, 10.5281/zenodo.14567918</li>
</ol>
<p>S Han, H Schoelkopf, Y Zhao, Z Qi, M Riddell, W Zhou, J Coady, D Peng, Y Qiao, L Benson, arXiv:2209.00840Folio: Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>Solving math word problems by combining language models with symbolic solvers. J He-Yueya, G Poesia, R E Wang, N D Goodman, arXiv:2304.091022023arXiv preprint</p>
<p>J-latte: a constraint-based tutor for java. J Holland, A Mitrovic, B Martin, 2009</p>
<p>Designing for complementarity: Teacher and student needs for orchestration support in ai-enhanced classrooms. K Holstein, B M Mclaren, V Aleven, Artificial Intelligence in Education: 20th International Conference, AIED 2019. Chicago, IL, USASpringer2019. June 25-29, 2019Proceedings, Part I 20</p>
<p>Mental models and deduction. P N Johnson-Laird, Trends in cognitive sciences. 52001</p>
<p>Providing adaptive support in an interactive simulation for learning: An experimental evaluation. S Kardan, C Conati, Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. the 33rd Annual ACM Conference on Human Factors in Computing Systems2015</p>
<p>Intelligent tutoring goes to school in the big city. K R Koedinger, J R Anderson, W H Hadley, M A Mark, International Journal of Artificial Intelligence in Education. 81997</p>
<p>The knowledgelearning-instruction framework: Bridging the science-practice chasm to enhance robust student learning. K R Koedinger, A T Corbett, C Perfetti, Cognitive science. 362012</p>
<p>A Liu, B Feng, B Xue, B Wang, B Wu, C Lu, C Zhao, C Deng, C Zhang, C Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024aarXiv preprint</p>
<p>Logic-of-thought: Injecting logic into contexts for full reasoning in large language models. T Liu, W Xu, W Huang, X Wang, J Wang, H Yang, J Li, arXiv:2409.175392024barXiv preprint</p>
<p>How to teach programming in the ai era? using llms as a teachable agent for debugging. Q Ma, H Shen, K Koedinger, S T Wu, ternational Conference on Artificial Intelligence in Education. Springer2024</p>
<p>Deep thought: An intelligent logic tutor for discrete math. M Maniktala, T Barnes, Proceedings of the 51st ACM Technical Symposium on Computer Science Education. the 51st ACM Technical Symposium on Computer Science Education2020</p>
<p>The impact of adding textual explanations to next-step hints in a novice programming environment. S Marwan, N Lytle, J J Williams, T Price, Proceedings of the 2019 ACM conference on innovation and technology in computer science education. the 2019 ACM conference on innovation and technology in computer science education2019</p>
<p>Interrater reliability: the kappa statistic. M L Mchugh, Biochemia medica. 222012</p>
<p>An overview of intelligent tutoring system authoring tools: Updated analysis of the state of the art. T Murray, Authoring Tools for Advanced Technology Learning Environments: Toward Cost-Effective Adaptive, Interactive and Intelligent Educational Software. 2003</p>
<p>Automated evaluation of written discourse coherence using gpt-4. B Naismith, P Mulcaire, J Burstein, Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023). the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)2023</p>
<p>Generative large language models for dialog-based tutoring: An early consideration of opportunities and concerns. B D Nye, D Mee, M G Core, LLM@ AIED. 2023</p>
<p>Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. M Parmar, N Patel, N Varshney, M Nakamura, M Luo, S Mashetty, A Mitra, C Baral, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsLong Papers20241</p>
<p>Generating high-precision feedback for programming syntax errors using large language models. T Phung, J Cambronero, S Gulwani, T Kohn, R Majumdar, A Singla, G Soares, arXiv:2302.046622023arXiv preprint</p>
<p>Automating human tutor-style programming feedback: Leveraging gpt-4 tutor model for hint generation and gpt-3.5 student model for hint validation. T Phung, V A Pădurean, A Singh, C Brooks, J Cambronero, S Gulwani, A Singla, G Soares, Proceedings of the 14th Learning Analytics and Knowledge Conference. the 14th Learning Analytics and Knowledge Conference2024</p>
<p>isnap: towards intelligent tutoring in novice programming environments. T W Price, Y Dong, D Lipovac, Proceedings of the 2017 ACM SIGCSE Technical Symposium on computer science education. the 2017 ACM SIGCSE Technical Symposium on computer science education2017a</p>
<p>Hint generation under uncertainty: The effect of hint quality on help-seeking behavior. T W Price, R Zhi, T Barnes, Artificial Intelligence in Education: 18th International Conference. Wuhan, ChinaSpringer2017b. 2017. June 28-July 1, 201718</p>
<p>Cognitive tutor: Applied research in mathematics education. S Ritter, J R Anderson, K R Koedinger, A Corbett, Psychonomic bulletin &amp; review. 142007</p>
<p>Data-driven hint generation in vast solution spaces: a self-improving python programming tutor. K Rivers, K R Koedinger, International Journal of Artificial Intelligence in Education. 272017</p>
<p>Next-step hint generation for introductory programming using large language models. L Roest, H Keuning, J Jeuring, Proceedings of the 26th Australasian Computing Education Conference. the 26th Australasian Computing Education Conference2024</p>
<p>Improving students' help-seeking skills using metacognitive feedback in an intelligent tutoring system. I Roll, V Aleven, B M Mclaren, K R Koedinger, Learning and instruction. 212011</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. A Saparov, H He, arXiv:2210.012402022arXiv preprint</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. A Saparov, R Y Pang, V Padmakumar, N Joshi, M Kazemi, N Kim, H He, Advances in Neural Information Processing Systems. 202336</p>
<p>Basicbayes: A tutor system for simple bayesian inference. P Sedlmeier, Behavior Research Methods, Instruments, &amp; Computers. 291997</p>
<p>Experimental evaluation of automatic hint generation for a logic tutor. J Stamper, M Eagle, T Barnes, M Croy, International Journal of Artificial Intelligence in Education. 222013</p>
<p>Enhancing llm-based feedback: Insights from intelligent tutoring systems and the learning sciences. J Stamper, R Xiao, X Hou, ternational Conference on Artificial Intelligence in Education. Springer2024</p>
<p>O Tafjord, B D Mishra, P Clark, arXiv:2012.13048Proofwriter: Generating implications, proofs, and abductive statements over natural language. 2020arXiv preprint</p>
<p>Jill watson: A virtual teaching assistant powered by chatgpt. K Taneja, P Maiti, S Kakar, P Guruprasad, S Rao, A K Goel, International Conference on Artificial Intelligence in Education. Springer2024</p>
<p>Examining llm prompting strategies for automatic evaluation of learner-created computational artifacts. X Tian, A Mannekote, C E Solomon, Y Song, C F Wise, T Mcklin, J Barrett, K E Boyer, M Israel, Proceedings of the 17th International Conference on Educational Data Mining. the 17th International Conference on Educational Data Mining2024</p>
<p>Irt-based adaptive hints to scaffold learning in programming. M Ueno, Y Miyazawa, IEEE Transactions on Learning Technologies. 112017</p>
<p>The relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems. K Vanlehn, Educational psychologist. 462011</p>
<p>D Venugopalan, Z Yan, C Borchers, J Lin, V Aleven, arXiv:2412.11995Combining large language models with tutoring system intelligence: A case study in caregiver homework support. 2024arXiv preprint</p>
<p>Large language models for education: A survey and outlook. S Wang, T Xu, H Li, C Zhang, J Liang, J Tang, P S Yu, Q Wen, arXiv:2403.181052024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 352022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6879 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6879</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6879</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-1823b8aecd62ccfca0cb6caa8e2a1159754afc5e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1823b8aecd62ccfca0cb6caa8e2a1159754afc5e" target="_blank">LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> SMolInstruct, a large-scale, comprehensive, and high-quality dataset for instruction tuning, is proposed and a set of open-source LLMs are fine-tune, among which, Mistral serves as the best base model for chemistry tasks.</p>
                <p><strong>Paper Abstract:</strong> Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing research indicates that their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish this, we propose SMolInstruct, a large-scale, comprehensive, and high-quality dataset for instruction tuning. It contains 14 selected chemistry tasks and over three million samples, laying a solid foundation for training and evaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral serves as the best base model for chemistry tasks. Our analysis further demonstrates the critical role of the proposed dataset in driving the performance improvements.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6879.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6879.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol (series)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol: Large language models on Small Molecules (series of fine-tuned LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of open-source LLMs (Galactica-, Llama2-, Code Llama-, and Mistral-based) fine-tuned on the SMolInstruct instruction-tuning dataset via LoRA to perform multiple chemistry tasks including direct molecule generation from text, property prediction, and synthesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol (Galactica / Llama 2 / Code Llama / Mistral variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLMs, fine-tuned via parameter-efficient LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>base models: ~7B (Galactica 6.7B, Llama2/CodeLlama/Mistral 7B); LoRA trainable params reported ~41.9M (0.58%) for Mistral variant in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Instruction-tuning on SMolInstruct (3.3M samples, 1.6M distinct molecules) assembled from PubChem (IUPAC/SMILES/formula), MoleculeNet (property labels: ESOL, Lipo, BBBP, ClinTox, HIV, SIDER), USPTO-full (reactions), ChEBI-20 and Mol-Instructions (molecule ↔ text pairs). All SMILES canonicalized; SELFIES also provided as an alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning on instruction pairs using LoRA (low-rank adapters); inference uses beam search; for some baselines few-shot/zero-shot prompting templates were used. Models directly generate SMILES strings (or textual answers) for molecule-generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Canonical SMILES (default) with support for SELFIES, IUPAC names, and molecular formula representations; canonicalization enforced during dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule generation from textual descriptions (MG), molecule captioning (MC), property prediction (drug-relevant properties), forward reaction prediction (FS) and retrosynthesis (RS) — applications oriented primarily to small-molecule drug discovery and reaction planning.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Canonicalization of SMILES; RDKit-based parsing and filtering to remove invalid SMILES and enforce valence grammar; dataset-level filters (remove low-quality textual descriptions, duplicates); scaffold split for property prediction tasks to avoid leakage; training limited to LoRA trainable parameters (parameter-efficient fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used for parsing, validity checks, canonicalization, and as an implemented baseline for SMILES→molecular-formula; baselines/benchmarks used include Uni-Mol, RSMILES, Molecular Transformer, STOUT; HuggingFace Transformers used for training/inference.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>SMolInstruct (3.3M samples assembled from PubChem, MoleculeNet, USPTO-full, ChEBI-20, Mol-Instructions); specific task subsets used (e.g., MG from ChEBI-20 + Mol-Instructions, FS/RS from USPTO-full, PP from MoleculeNet).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (%) for SMILES outputs, Exact Match (EM %), Fingerprint Tanimoto Similarity (FTS % using Morgan fingerprints), METEOR (text similarity for MC), RMSE (for regression PP tasks), Accuracy (binary classification PP tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Aggregate highlights (from tables): LlaSMol_Mistral often best among LlaSMol series. Example numbers for generation-related tasks: MG (LlaSMol_Mistral) EM 19.2%, FTS 61.7%, Valid 99.7%; FS EM 63.3%, FTS 84.9%, Valid 99.8%; RS EM 32.9%, FTS 70.4%, Valid 100.0%. Property prediction examples: ESOL RMSE 1.150, Lipo RMSE 1.010, BBBP Acc 74.6%, ClinTox Acc 93.1%, HIV Acc 96.7%, SIDER Acc 70.7%. Name-conversion examples: NC-I2S EM up to 70.1% (Mistral variant), NC-S2I EM 29.0% (Mistral).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Models do not yet surpass SoTA task-specific models on all tasks; small fraction of parameters fine-tuned (LoRA) may limit peak performance; evaluation for molecule-generation and captioning is ambiguous (textual/molecular description ambiguity); dataset quality issues required filtering (invalid SMILES, mislabeled reaction roles, low-quality textual descriptions); SELFIES produced higher validity but often worse end-task performance; generalization beyond trained tasks not deeply evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6879.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6879.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol_Mistral</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol (Mistral 7B base) fine-tuned on SMolInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Mistral-7B based member of the LlaSMol series which achieved the strongest performance across the chemistry tasks after instruction tuning with SMolInstruct and LoRA; used for direct SMILES generation from text and reaction prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol_Mistral (Mistral 7B base, LoRA fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, fine-tuned via LoRA adapters</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Mistral base 7B; reported LoRA trainable parameters ≈41.9M (~0.58% of base model) in paper experiments</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on SMolInstruct (3.3M samples) covering 14 tasks: name conversion (PubChem), property prediction (MoleculeNet), molecule captioning/generation (ChEBI-20 + Mol-Instructions), forward/retro synthesis (USPTO-full). Canonical SMILES used in training.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Instruction tuning with LoRA (lora.r and lora.alpha = 16), trained 3 epochs with 8-bit AdamW lr=1e-4 and cosine scheduler; inference via beam search with task-dependent num_return_sequences (e.g., 5 for MG, 10 for RS). Outputs include direct SMILES generation for MG/FS/RS tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Canonical SMILES (default) during training and inference; SELFIES evaluated as an alternative in ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Direct molecule generation from textual descriptions; molecule captioning; reaction product prediction (forward synthesis); retrosynthesis (reaction planning); property prediction relevant to drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Canonicalization applied; RDKit-based validity filtering applied during data curation and evaluation; dot character in SMILES replaced with semicolon in non-reaction tasks to avoid delimiter ambiguity; beam search decoding with fixed beam sizes and multiple returns for top-k evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used for parsing/canonicalization/validity; HuggingFace Transformers used for implementation and generation; compared/benchmarked to Uni-Mol, Molecular Transformer, RSMILES, STOUT as external baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>SMolInstruct and its task-specific components (PubChem, MoleculeNet, USPTO-full, ChEBI-20, Mol-Instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity %, Exact Match (EM %), Fingerprint Tanimoto Similarity (FTS %), METEOR (MC), RMSE (ESOL, Lipo), Accuracy (binary PP tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Selected reported numbers: MG EM 19.2%, FTS 61.7%, Valid 99.7%; FS EM 63.3%, FTS 84.9%, Valid 99.8%; RS EM 32.9%, FTS 70.4%, Valid 100.0%; PP: ESOL RMSE 1.150, Lipo RMSE 1.010, BBBP Acc 74.6%, ClinTox Acc 93.1%, HIV Acc 96.7%, SIDER Acc 70.7%; NC: I2S EM 70.1%, I2F EM 87.9%, S2F Valid 99.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Only small portion of parameters fine-tuned (LoRA) — may limit ceiling; although validity is high, EM and FTS show room for improvement relative to task-specific SoTA; MG/MC evaluation is limited by ambiguity of textual descriptions and available ground-truth; training/inference design choices (canonical SMILES vs SELFIES) affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6879.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6879.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMolInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMolInstruct: a large-scale, comprehensive instruction-tuning dataset for small molecules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3.3M-sample, 14-task instruction-tuning dataset focusing on small molecules, curated from PubChem, MoleculeNet, USPTO-full, ChEBI-20, and Mol-Instructions; provides canonical SMILES, SELFIES, IUPAC names and formulas with quality control and leakage-aware splits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMolInstruct (instruction-tuning dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuning dataset / multi-task supervision corpus</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3,342,414 total samples (3.3M), ~1.6M distinct molecules; 14 tasks across name conversion, property prediction, molecule captioning/generation, and chemical reaction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Aggregated from: PubChem (IUPAC/SMILES/formula for name conversion tasks), MoleculeNet (ESOL, Lipo, BBBP, ClinTox, HIV, SIDER), USPTO-full (forward/reverse reaction pairs), ChEBI-20 and Mol-Instructions (molecule-text pairs). All SMILES canonicalized; SELFIES also included as alternate representation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A (dataset used to fine-tune models); dataset entries are query-response instruction pairs (naturalistic templates and GPT-4 rephrasing applied during instruction creation).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Canonical SMILES (default), SELFIES, IUPAC names, molecular formulas; special tags used in textual templates (e.g., <SMILES> ... </SMILES>).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Training LLMs for molecule generation from text, property prediction (drug-related properties), reaction prediction (forward/retrosynthesis), and representation conversion — oriented to drug discovery and synthesis planning research.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Extensive quality control: RDKit parsing to remove invalid SMILES, manual and automated filtering for wrong/inaccurate information, replacement of '.' in SMILES with semicolons for non-reaction tasks, scaffold split for PP tasks, grouping matched molecules/reactions across related tasks to avoid train/test leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit for parsing, validation and canonicalization; GPT-4 used to rephrase templates during instruction creation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>SMolInstruct itself (composed from PubChem, MoleculeNet, USPTO-full, ChEBI-20, Mol-Instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Statistics: 3.3M instruction-response samples covering 14 tasks; task-specific sample counts provided (e.g., FS ~977.9k, RS ~947.98k, MC ~60.3k, MG ~60.26k).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Quality of some source data required heavy curation (invalid SMILES, mislabeled reagents/reactants, ambiguous textual descriptions); molecule-description tasks limited by ambiguous/subjective definitions; dataset focused on small molecules so generalization to other chemistries untested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6879.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6879.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molecule Generation (MG) task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecule Generation: producing molecular structures from textual descriptions (MG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SMolInstruct task where models generate molecules (SMILES/SELFIES) conditioned on natural-language descriptions; used to evaluate how well LLMs bridge text-to-molecule generation for applications like drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molecule Generation (MG) task within SMolInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>text-conditioned molecular generation task (sequence generation -> SMILES/SELFIES)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>dataset size for MG: ~60.26k samples (train+val+test as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Training examples assembled from ChEBI-20 and Mol-Instructions (molecule↔text pairs), filtered for quality; textual descriptions curated via heuristics and GPT-4 rephrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLMs generate SMILES (or SELFIES) directly from text prompts after fine-tuning; baseline evaluations also use few-shot prompting for off-the-shelf LLMs; beam search decoding used at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Canonical SMILES (default) and optionally SELFIES; SMILES canonicalized during dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Generate candidate small molecules that match textual property/description prompts — applicable to early-stage drug discovery, ideation, or dataset augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Validity enforced by RDKit checks, canonicalization, filtering of low-quality textual prompts during dataset creation. Evaluation often limited to validity, exact-match and fingerprint similarity to ground-truth molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used to validate generated SMILES and compute fingerprint similarity; Morgan fingerprints used for FTS.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MG subset of SMolInstruct (ChEBI-20 + Mol-Instructions derived samples).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (%), Exact Match (EM %), Fingerprint Tanimoto Similarity (FTS %).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Representative results: LlaSMol_Mistral — EM 19.2%, FTS 61.7%, Valid 99.7%; LlaSMol_Galactica — EM 7.7%, FTS 52.2%, Valid 99.6%; GPT-4 (zero-shot) — EM 6.4%, FTS 42.6%, Valid 81.4%; Mol-Instructions-trained model (Molinst) — EM 6.0%, FTS 43.6%, Valid 84.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Evaluation is limited because textual descriptions are ambiguous and one-to-many (many valid molecules might match one description); ground-truth may be non-unique; existing metrics (EM, FTS) do not fully capture usefulness; high validity does not guarantee relevance/novelty/usefulness for downstream applications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6879.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6879.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0613) evaluated as an off-the-shelf LLM baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large closed-source LLM from OpenAI evaluated (zero-shot and few-shot) on SMolInstruct tasks via formatted prompt templates to generate SMILES and textual outputs for chemistry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613 via OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM (closed-source), prompt-based (zero-/few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>large, proprietary (exact parameter count not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper; standard GPT-4 pretraining as per OpenAI report (mix of web, code, books, etc.). Not fine-tuned on SMolInstruct.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Zero-shot or few-shot prompting with task-specific templates and ICL examples; generated SMILES strings or textual answers directly from prompts; default API decoding used.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES used in prompts and expected outputs; templates enforced SMILES-only outputs for reaction/product tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Evaluated for molecule generation (MG), forward synthesis (FS), retrosynthesis (RS), property prediction, and name conversion tasks as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Prompt templates that instruct the model to output only SMILES and follow specific formatting; for some tasks in-context examples (ICL) were provided (k-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>None reported in evaluation (no tool integration); outputs post-processed with heuristic extraction and RDKit-based validity checks where necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Evaluated on SMolInstruct test subsets (up to 500 samples per task due to API/resource limits).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as other models: EM, FTS, Validity, RMSE, Accuracy depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Examples: MG (GPT-4 zero-shot) EM 6.4%, FTS 42.6%, Valid 81.4%; FS EM 1.6%, FTS 40.5%, Valid 87.0%; many other task scores given in paper tables — GPT-4 underperforms LlaSMol variants on most tasks in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Poor handling of SMILES in some tasks (low validity and low exact-match rates); needs careful prompt design; limited to small evaluation sample due to API constraints; not fine-tuned on chemistry instruction dataset which reduces task-specific performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6879.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6879.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDKit (tool)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDKit: open-source cheminformatics toolkit</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source cheminformatics library used for parsing/canonicalizing SMILES, detecting invalid SMILES, computing molecular formulas and as an algorithmic baseline for some tasks (e.g., SMILES→formula).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rdkit: Open-source cheminformatics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RDKit (cheminformatics toolkit)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>software/toolkit for chemistry preprocessing, canonicalization and validation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>N/A (tool/library)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not a generator — used for parsing, canonicalization, validity checks, molecular formula generation and SMILES grammar enforcement during dataset curation and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Parses SMILES/SELFIES/InChI; provides canonicalization and molecular property computation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Dataset curation (filter invalid SMILES), canonicalization, baseline algorithmic task implementations (e.g., SMILES→molecular formula), and validity evaluation of generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Used to remove chemically invalid SMILES and to canonicalize representations before training; used to check validity metric during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Integrated into the dataset construction and evaluation pipelines used by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Applied to SMolInstruct and constituent source datasets (PubChem, USPTO-full, etc.) during quality control.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used to compute validity (parsing success) and to implement deterministic baselines (e.g., algorithmic conversion tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper notes many source SMILES were invalid and required RDKit filtering — emphasizes reliance on good-quality SMILES and issues in source datasets; no other limitations of RDKit itself discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6879.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6879.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES vs SELFIES (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation comparing canonical SMILES and SELFIES representations for model training and generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors ablated using canonical SMILES vs uncanonicalized SMILES and vs SELFIES; canonical SMILES generally improved downstream task performance while SELFIES improved validity but often degraded task metrics due to longer sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Representation ablation: SMILES (canonical / uncanonical) and SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>data-representation choices evaluated in fine-tuning experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (representation study across models/datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Same SMolInstruct data but encoded either as canonical SMILES, uncanonicalized SMILES (w/o canonical), or SELFIES (tokenized as SELFIES strings).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Models trained with LoRA on each representation variant; generation outputs accordingly in SMILES or SELFIES and then validated/compared.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Canonical SMILES (default, recommended), uncanonical SMILES (ablation), SELFIES (ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>All SMolInstruct tasks, particularly molecule generation (MG), forward/retro synthesis, and name-conversion tasks that require exact molecule outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>SELFIES guarantees semantic validity of any generated sequence; canonical SMILES reduces learning burden by standardizing representation; uncanonical SMILES introduces variability.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used to convert/validate representations and compute validity and canonicalization.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>SMolInstruct experiments with representation variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (%), EM, FTS, task-specific RMSE/Accuracy/METEOR.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Representative reported differences: Using SELFIES with LlaSMol_Mistral: MG EM 16.2% (vs 19.2% with SMILES), FTS 58.6% (vs 61.7%), Valid 99.9% (slightly higher); Using uncanonical SMILES (w/o canonical) generally decreased performance on FS/RS and some other tasks compared to canonical SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Although SELFIES provides near-perfect validity, sequence length and differing tokenization may make learning harder and hurt downstream accuracy; canonicalization is recommended to reduce learning difficulty and improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 2)</em></li>
                <li>What can large language models do in chemistry? a comprehensive benchmark on eight tasks <em>(Rating: 2)</em></li>
                <li>Selfies: a robust representation of semantically constrained graphs with an example application in chemistry <em>(Rating: 2)</em></li>
                <li>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction <em>(Rating: 1)</em></li>
                <li>Chemllm: A chemical large language model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6879",
    "paper_id": "paper-1823b8aecd62ccfca0cb6caa8e2a1159754afc5e",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "LlaSMol (series)",
            "name_full": "LlaSMol: Large language models on Small Molecules (series of fine-tuned LLMs)",
            "brief_description": "A set of open-source LLMs (Galactica-, Llama2-, Code Llama-, and Mistral-based) fine-tuned on the SMolInstruct instruction-tuning dataset via LoRA to perform multiple chemistry tasks including direct molecule generation from text, property prediction, and synthesis planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LlaSMol (Galactica / Llama 2 / Code Llama / Mistral variants)",
            "model_type": "decoder-only LLMs, fine-tuned via parameter-efficient LoRA",
            "model_size": "base models: ~7B (Galactica 6.7B, Llama2/CodeLlama/Mistral 7B); LoRA trainable params reported ~41.9M (0.58%) for Mistral variant in experiments",
            "training_data_description": "Instruction-tuning on SMolInstruct (3.3M samples, 1.6M distinct molecules) assembled from PubChem (IUPAC/SMILES/formula), MoleculeNet (property labels: ESOL, Lipo, BBBP, ClinTox, HIV, SIDER), USPTO-full (reactions), ChEBI-20 and Mol-Instructions (molecule ↔ text pairs). All SMILES canonicalized; SELFIES also provided as an alternative.",
            "generation_method": "Fine-tuning on instruction pairs using LoRA (low-rank adapters); inference uses beam search; for some baselines few-shot/zero-shot prompting templates were used. Models directly generate SMILES strings (or textual answers) for molecule-generation tasks.",
            "chemical_representation": "Canonical SMILES (default) with support for SELFIES, IUPAC names, and molecular formula representations; canonicalization enforced during dataset construction.",
            "target_application": "Molecule generation from textual descriptions (MG), molecule captioning (MC), property prediction (drug-relevant properties), forward reaction prediction (FS) and retrosynthesis (RS) — applications oriented primarily to small-molecule drug discovery and reaction planning.",
            "constraints_used": "Canonicalization of SMILES; RDKit-based parsing and filtering to remove invalid SMILES and enforce valence grammar; dataset-level filters (remove low-quality textual descriptions, duplicates); scaffold split for property prediction tasks to avoid leakage; training limited to LoRA trainable parameters (parameter-efficient fine-tuning).",
            "integration_with_external_tools": "RDKit used for parsing, validity checks, canonicalization, and as an implemented baseline for SMILES→molecular-formula; baselines/benchmarks used include Uni-Mol, RSMILES, Molecular Transformer, STOUT; HuggingFace Transformers used for training/inference.",
            "dataset_used": "SMolInstruct (3.3M samples assembled from PubChem, MoleculeNet, USPTO-full, ChEBI-20, Mol-Instructions); specific task subsets used (e.g., MG from ChEBI-20 + Mol-Instructions, FS/RS from USPTO-full, PP from MoleculeNet).",
            "evaluation_metrics": "Validity (%) for SMILES outputs, Exact Match (EM %), Fingerprint Tanimoto Similarity (FTS % using Morgan fingerprints), METEOR (text similarity for MC), RMSE (for regression PP tasks), Accuracy (binary classification PP tasks).",
            "reported_results": "Aggregate highlights (from tables): LlaSMol_Mistral often best among LlaSMol series. Example numbers for generation-related tasks: MG (LlaSMol_Mistral) EM 19.2%, FTS 61.7%, Valid 99.7%; FS EM 63.3%, FTS 84.9%, Valid 99.8%; RS EM 32.9%, FTS 70.4%, Valid 100.0%. Property prediction examples: ESOL RMSE 1.150, Lipo RMSE 1.010, BBBP Acc 74.6%, ClinTox Acc 93.1%, HIV Acc 96.7%, SIDER Acc 70.7%. Name-conversion examples: NC-I2S EM up to 70.1% (Mistral variant), NC-S2I EM 29.0% (Mistral).",
            "experimental_validation": false,
            "challenges_or_limitations": "Models do not yet surpass SoTA task-specific models on all tasks; small fraction of parameters fine-tuned (LoRA) may limit peak performance; evaluation for molecule-generation and captioning is ambiguous (textual/molecular description ambiguity); dataset quality issues required filtering (invalid SMILES, mislabeled reaction roles, low-quality textual descriptions); SELFIES produced higher validity but often worse end-task performance; generalization beyond trained tasks not deeply evaluated.",
            "uuid": "e6879.0",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LlaSMol_Mistral",
            "name_full": "LlaSMol (Mistral 7B base) fine-tuned on SMolInstruct",
            "brief_description": "The Mistral-7B based member of the LlaSMol series which achieved the strongest performance across the chemistry tasks after instruction tuning with SMolInstruct and LoRA; used for direct SMILES generation from text and reaction prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LlaSMol_Mistral (Mistral 7B base, LoRA fine-tuned)",
            "model_type": "decoder-only LLM, fine-tuned via LoRA adapters",
            "model_size": "Mistral base 7B; reported LoRA trainable parameters ≈41.9M (~0.58% of base model) in paper experiments",
            "training_data_description": "Fine-tuned on SMolInstruct (3.3M samples) covering 14 tasks: name conversion (PubChem), property prediction (MoleculeNet), molecule captioning/generation (ChEBI-20 + Mol-Instructions), forward/retro synthesis (USPTO-full). Canonical SMILES used in training.",
            "generation_method": "Instruction tuning with LoRA (lora.r and lora.alpha = 16), trained 3 epochs with 8-bit AdamW lr=1e-4 and cosine scheduler; inference via beam search with task-dependent num_return_sequences (e.g., 5 for MG, 10 for RS). Outputs include direct SMILES generation for MG/FS/RS tasks.",
            "chemical_representation": "Canonical SMILES (default) during training and inference; SELFIES evaluated as an alternative in ablation.",
            "target_application": "Direct molecule generation from textual descriptions; molecule captioning; reaction product prediction (forward synthesis); retrosynthesis (reaction planning); property prediction relevant to drug discovery.",
            "constraints_used": "Canonicalization applied; RDKit-based validity filtering applied during data curation and evaluation; dot character in SMILES replaced with semicolon in non-reaction tasks to avoid delimiter ambiguity; beam search decoding with fixed beam sizes and multiple returns for top-k evaluation.",
            "integration_with_external_tools": "RDKit used for parsing/canonicalization/validity; HuggingFace Transformers used for implementation and generation; compared/benchmarked to Uni-Mol, Molecular Transformer, RSMILES, STOUT as external baselines.",
            "dataset_used": "SMolInstruct and its task-specific components (PubChem, MoleculeNet, USPTO-full, ChEBI-20, Mol-Instructions).",
            "evaluation_metrics": "Validity %, Exact Match (EM %), Fingerprint Tanimoto Similarity (FTS %), METEOR (MC), RMSE (ESOL, Lipo), Accuracy (binary PP tasks).",
            "reported_results": "Selected reported numbers: MG EM 19.2%, FTS 61.7%, Valid 99.7%; FS EM 63.3%, FTS 84.9%, Valid 99.8%; RS EM 32.9%, FTS 70.4%, Valid 100.0%; PP: ESOL RMSE 1.150, Lipo RMSE 1.010, BBBP Acc 74.6%, ClinTox Acc 93.1%, HIV Acc 96.7%, SIDER Acc 70.7%; NC: I2S EM 70.1%, I2F EM 87.9%, S2F Valid 99.6%.",
            "experimental_validation": false,
            "challenges_or_limitations": "Only small portion of parameters fine-tuned (LoRA) — may limit ceiling; although validity is high, EM and FTS show room for improvement relative to task-specific SoTA; MG/MC evaluation is limited by ambiguity of textual descriptions and available ground-truth; training/inference design choices (canonical SMILES vs SELFIES) affect performance.",
            "uuid": "e6879.1",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SMolInstruct",
            "name_full": "SMolInstruct: a large-scale, comprehensive instruction-tuning dataset for small molecules",
            "brief_description": "A 3.3M-sample, 14-task instruction-tuning dataset focusing on small molecules, curated from PubChem, MoleculeNet, USPTO-full, ChEBI-20, and Mol-Instructions; provides canonical SMILES, SELFIES, IUPAC names and formulas with quality control and leakage-aware splits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SMolInstruct (instruction-tuning dataset)",
            "model_type": "instruction-tuning dataset / multi-task supervision corpus",
            "model_size": "3,342,414 total samples (3.3M), ~1.6M distinct molecules; 14 tasks across name conversion, property prediction, molecule captioning/generation, and chemical reaction tasks",
            "training_data_description": "Aggregated from: PubChem (IUPAC/SMILES/formula for name conversion tasks), MoleculeNet (ESOL, Lipo, BBBP, ClinTox, HIV, SIDER), USPTO-full (forward/reverse reaction pairs), ChEBI-20 and Mol-Instructions (molecule-text pairs). All SMILES canonicalized; SELFIES also included as alternate representation.",
            "generation_method": "N/A (dataset used to fine-tune models); dataset entries are query-response instruction pairs (naturalistic templates and GPT-4 rephrasing applied during instruction creation).",
            "chemical_representation": "Canonical SMILES (default), SELFIES, IUPAC names, molecular formulas; special tags used in textual templates (e.g., &lt;SMILES&gt; ... &lt;/SMILES&gt;).",
            "target_application": "Training LLMs for molecule generation from text, property prediction (drug-related properties), reaction prediction (forward/retrosynthesis), and representation conversion — oriented to drug discovery and synthesis planning research.",
            "constraints_used": "Extensive quality control: RDKit parsing to remove invalid SMILES, manual and automated filtering for wrong/inaccurate information, replacement of '.' in SMILES with semicolons for non-reaction tasks, scaffold split for PP tasks, grouping matched molecules/reactions across related tasks to avoid train/test leakage.",
            "integration_with_external_tools": "RDKit for parsing, validation and canonicalization; GPT-4 used to rephrase templates during instruction creation.",
            "dataset_used": "SMolInstruct itself (composed from PubChem, MoleculeNet, USPTO-full, ChEBI-20, Mol-Instructions).",
            "evaluation_metrics": null,
            "reported_results": "Statistics: 3.3M instruction-response samples covering 14 tasks; task-specific sample counts provided (e.g., FS ~977.9k, RS ~947.98k, MC ~60.3k, MG ~60.26k).",
            "experimental_validation": false,
            "challenges_or_limitations": "Quality of some source data required heavy curation (invalid SMILES, mislabeled reagents/reactants, ambiguous textual descriptions); molecule-description tasks limited by ambiguous/subjective definitions; dataset focused on small molecules so generalization to other chemistries untested.",
            "uuid": "e6879.2",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Molecule Generation (MG) task",
            "name_full": "Molecule Generation: producing molecular structures from textual descriptions (MG)",
            "brief_description": "A SMolInstruct task where models generate molecules (SMILES/SELFIES) conditioned on natural-language descriptions; used to evaluate how well LLMs bridge text-to-molecule generation for applications like drug discovery.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Molecule Generation (MG) task within SMolInstruct",
            "model_type": "text-conditioned molecular generation task (sequence generation -&gt; SMILES/SELFIES)",
            "model_size": "dataset size for MG: ~60.26k samples (train+val+test as reported)",
            "training_data_description": "Training examples assembled from ChEBI-20 and Mol-Instructions (molecule↔text pairs), filtered for quality; textual descriptions curated via heuristics and GPT-4 rephrasing.",
            "generation_method": "LLMs generate SMILES (or SELFIES) directly from text prompts after fine-tuning; baseline evaluations also use few-shot prompting for off-the-shelf LLMs; beam search decoding used at inference.",
            "chemical_representation": "Canonical SMILES (default) and optionally SELFIES; SMILES canonicalized during dataset construction.",
            "target_application": "Generate candidate small molecules that match textual property/description prompts — applicable to early-stage drug discovery, ideation, or dataset augmentation.",
            "constraints_used": "Validity enforced by RDKit checks, canonicalization, filtering of low-quality textual prompts during dataset creation. Evaluation often limited to validity, exact-match and fingerprint similarity to ground-truth molecules.",
            "integration_with_external_tools": "RDKit used to validate generated SMILES and compute fingerprint similarity; Morgan fingerprints used for FTS.",
            "dataset_used": "MG subset of SMolInstruct (ChEBI-20 + Mol-Instructions derived samples).",
            "evaluation_metrics": "Validity (%), Exact Match (EM %), Fingerprint Tanimoto Similarity (FTS %).",
            "reported_results": "Representative results: LlaSMol_Mistral — EM 19.2%, FTS 61.7%, Valid 99.7%; LlaSMol_Galactica — EM 7.7%, FTS 52.2%, Valid 99.6%; GPT-4 (zero-shot) — EM 6.4%, FTS 42.6%, Valid 81.4%; Mol-Instructions-trained model (Molinst) — EM 6.0%, FTS 43.6%, Valid 84.8%.",
            "experimental_validation": false,
            "challenges_or_limitations": "Evaluation is limited because textual descriptions are ambiguous and one-to-many (many valid molecules might match one description); ground-truth may be non-unique; existing metrics (EM, FTS) do not fully capture usefulness; high validity does not guarantee relevance/novelty/usefulness for downstream applications.",
            "uuid": "e6879.3",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 (evaluated)",
            "name_full": "GPT-4 (gpt-4-0613) evaluated as an off-the-shelf LLM baseline",
            "brief_description": "A large closed-source LLM from OpenAI evaluated (zero-shot and few-shot) on SMolInstruct tasks via formatted prompt templates to generate SMILES and textual outputs for chemistry tasks.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613 via OpenAI API)",
            "model_type": "decoder-only LLM (closed-source), prompt-based (zero-/few-shot)",
            "model_size": "large, proprietary (exact parameter count not specified in paper)",
            "training_data_description": "Not specified in this paper; standard GPT-4 pretraining as per OpenAI report (mix of web, code, books, etc.). Not fine-tuned on SMolInstruct.",
            "generation_method": "Zero-shot or few-shot prompting with task-specific templates and ICL examples; generated SMILES strings or textual answers directly from prompts; default API decoding used.",
            "chemical_representation": "SMILES used in prompts and expected outputs; templates enforced SMILES-only outputs for reaction/product tasks.",
            "target_application": "Evaluated for molecule generation (MG), forward synthesis (FS), retrosynthesis (RS), property prediction, and name conversion tasks as a baseline.",
            "constraints_used": "Prompt templates that instruct the model to output only SMILES and follow specific formatting; for some tasks in-context examples (ICL) were provided (k-shot).",
            "integration_with_external_tools": "None reported in evaluation (no tool integration); outputs post-processed with heuristic extraction and RDKit-based validity checks where necessary.",
            "dataset_used": "Evaluated on SMolInstruct test subsets (up to 500 samples per task due to API/resource limits).",
            "evaluation_metrics": "Same as other models: EM, FTS, Validity, RMSE, Accuracy depending on task.",
            "reported_results": "Examples: MG (GPT-4 zero-shot) EM 6.4%, FTS 42.6%, Valid 81.4%; FS EM 1.6%, FTS 40.5%, Valid 87.0%; many other task scores given in paper tables — GPT-4 underperforms LlaSMol variants on most tasks in this evaluation.",
            "experimental_validation": false,
            "challenges_or_limitations": "Poor handling of SMILES in some tasks (low validity and low exact-match rates); needs careful prompt design; limited to small evaluation sample due to API constraints; not fine-tuned on chemistry instruction dataset which reduces task-specific performance.",
            "uuid": "e6879.4",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RDKit (tool)",
            "name_full": "RDKit: open-source cheminformatics toolkit",
            "brief_description": "An open-source cheminformatics library used for parsing/canonicalizing SMILES, detecting invalid SMILES, computing molecular formulas and as an algorithmic baseline for some tasks (e.g., SMILES→formula).",
            "citation_title": "Rdkit: Open-source cheminformatics",
            "mention_or_use": "use",
            "model_name": "RDKit (cheminformatics toolkit)",
            "model_type": "software/toolkit for chemistry preprocessing, canonicalization and validation",
            "model_size": "N/A",
            "training_data_description": "N/A (tool/library)",
            "generation_method": "Not a generator — used for parsing, canonicalization, validity checks, molecular formula generation and SMILES grammar enforcement during dataset curation and evaluation.",
            "chemical_representation": "Parses SMILES/SELFIES/InChI; provides canonicalization and molecular property computation.",
            "target_application": "Dataset curation (filter invalid SMILES), canonicalization, baseline algorithmic task implementations (e.g., SMILES→molecular formula), and validity evaluation of generated molecules.",
            "constraints_used": "Used to remove chemically invalid SMILES and to canonicalize representations before training; used to check validity metric during evaluation.",
            "integration_with_external_tools": "Integrated into the dataset construction and evaluation pipelines used by the authors.",
            "dataset_used": "Applied to SMolInstruct and constituent source datasets (PubChem, USPTO-full, etc.) during quality control.",
            "evaluation_metrics": "Used to compute validity (parsing success) and to implement deterministic baselines (e.g., algorithmic conversion tasks).",
            "reported_results": null,
            "experimental_validation": false,
            "challenges_or_limitations": "Paper notes many source SMILES were invalid and required RDKit filtering — emphasizes reliance on good-quality SMILES and issues in source datasets; no other limitations of RDKit itself discussed.",
            "uuid": "e6879.5",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SMILES vs SELFIES (ablation)",
            "name_full": "Ablation comparing canonical SMILES and SELFIES representations for model training and generation",
            "brief_description": "The authors ablated using canonical SMILES vs uncanonicalized SMILES and vs SELFIES; canonical SMILES generally improved downstream task performance while SELFIES improved validity but often degraded task metrics due to longer sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Representation ablation: SMILES (canonical / uncanonical) and SELFIES",
            "model_type": "data-representation choices evaluated in fine-tuning experiments",
            "model_size": "N/A (representation study across models/datasets)",
            "training_data_description": "Same SMolInstruct data but encoded either as canonical SMILES, uncanonicalized SMILES (w/o canonical), or SELFIES (tokenized as SELFIES strings).",
            "generation_method": "Models trained with LoRA on each representation variant; generation outputs accordingly in SMILES or SELFIES and then validated/compared.",
            "chemical_representation": "Canonical SMILES (default, recommended), uncanonical SMILES (ablation), SELFIES (ablation).",
            "target_application": "All SMolInstruct tasks, particularly molecule generation (MG), forward/retro synthesis, and name-conversion tasks that require exact molecule outputs.",
            "constraints_used": "SELFIES guarantees semantic validity of any generated sequence; canonical SMILES reduces learning burden by standardizing representation; uncanonical SMILES introduces variability.",
            "integration_with_external_tools": "RDKit used to convert/validate representations and compute validity and canonicalization.",
            "dataset_used": "SMolInstruct experiments with representation variants.",
            "evaluation_metrics": "Validity (%), EM, FTS, task-specific RMSE/Accuracy/METEOR.",
            "reported_results": "Representative reported differences: Using SELFIES with LlaSMol_Mistral: MG EM 16.2% (vs 19.2% with SMILES), FTS 58.6% (vs 61.7%), Valid 99.9% (slightly higher); Using uncanonical SMILES (w/o canonical) generally decreased performance on FS/RS and some other tasks compared to canonical SMILES.",
            "experimental_validation": false,
            "challenges_or_limitations": "Although SELFIES provides near-perfect validity, sequence length and differing tokenization may make learning harder and hurt downstream accuracy; canonicalization is recommended to reduce learning difficulty and improve performance.",
            "uuid": "e6879.6",
            "source_info": {
                "paper_title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 2,
            "sanitized_title": "molinstructions_a_largescale_biomolecular_instruction_dataset_for_large_language_models"
        },
        {
            "paper_title": "What can large language models do in chemistry? a comprehensive benchmark on eight tasks",
            "rating": 2,
            "sanitized_title": "what_can_large_language_models_do_in_chemistry_a_comprehensive_benchmark_on_eight_tasks"
        },
        {
            "paper_title": "Selfies: a robust representation of semantically constrained graphs with an example application in chemistry",
            "rating": 2,
            "sanitized_title": "selfies_a_robust_representation_of_semantically_constrained_graphs_with_an_example_application_in_chemistry"
        },
        {
            "paper_title": "Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction",
            "rating": 1,
            "sanitized_title": "molecular_transformer_a_model_for_uncertaintycalibrated_chemical_reaction_prediction"
        },
        {
            "paper_title": "Chemllm: A chemical large language model",
            "rating": 1,
            "sanitized_title": "chemllm_a_chemical_large_language_model"
        }
    ],
    "cost": 0.022503,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset</h1>
<p>Botao Yu Frazier N. Baker<em> ${ }^{</em>}$ Ziqi Chen* Xia Ning Huan Sun<br>The Ohio State University<br>Columbus, OH 43210, USA<br>{yu.3737, baker.3239, chen.8484, ning.104, sun.397}@osu.edu</p>
<h4>Abstract</h4>
<p>Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT4 exhibit remarkable capabilities on natural language processing tasks, existing research indicates that their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish this, we propose SMolInstruct, a large-scale, comprehensive, and high-quality dataset for instruction tuning. It contains 14 selected chemistry tasks and over three million samples, laying a solid foundation for training and evaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of open-source LLMs named as LlaSMol, among which, we find that Mistral serves as the best base model for chemistry tasks. Our analysis further demonstrates the critical role of the proposed dataset in driving the performance improvements. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Chemistry is a fundamental science that underpins countless aspects of modern life, ranging from drug discovery and materials science to energy production. To facilitate research and applications in this domain, deep learning models including graph neural networks (Kipf \&amp; Welling, 2017) and Transformer-based models (Vaswani et al., 2017) have been developed for various chemistry tasks such as forward reaction prediction, retrosynthesis, property prediction (Schwaller et al., 2019; Zhong et al., 2022; Chen et al., 2023; Zhou et al., 2023). However, these models are usually task-specific models, which neglect shared chemistry knowledge across tasks and can hardly be adapted to different tasks.
On the other hand, large language models (LLMs) such as GPT-4 (OpenAI, 2023), Llama series (Touvron et al., 2023a,b), and Mistral (Jiang et al., 2023) have emerged as general-purpose foundation models and demonstrate remarkable abilities on various natural language processing tasks (Chang et al., 2024; Thirunavukarasu et al., 2023; Yue et al., 2023; Zhang et al., 2023; Deng et al., 2023). However, when applied to chemistry tasks, LLMs show only limited capabilities (Jablonka et al., 2022; Guo et al., 2023; Hatakeyama-Sato et al., 2023). For example, Guo et al. (2023) conducted evaluations on eight chemistry tasks and observed that while GPT-4 outperforms other closed- and open-source LLMs, its performance is far from that of task-specific deep learning models. Particularly, they found that GPT models perform poorly when a precise understanding of SMILES (Weininger, 1988), a widely used textual representation for molecules, is required. In addition to directly applying pretrained LLMs, Fang et al. (2023) fine-tuned LLMs on an instruction tuning dataset, but their performance remains very low, far behind the state-of-the-art (SoTA) models designed and trained for specific tasks.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of tasks in the proposed SMolInstruct dataset.</p>
<p>Given these discouraging results, some critical questions arise: Are LLMs actually able to effectively perform chemistry tasks? Or, Are they fundamentally limited for chemistry? In this paper, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, substantially outperforming the most advanced GPT-4 OpenAI (2023) and Claude 3 Opus Anthropic (2024).</p>
<p>What makes such LLMs possible? First, we construct a large-scale, comprehensive, and high-quality dataset for instruction tuning named SMolInstruct. We incorporate tasks with meaningful applications, collect data from diverse data sources, and apply rigorous scrutiny for quality control. The resulting dataset consists of 14 tasks (illustrated in Figure 1) and over 3M samples, laying a solid foundation for training and evaluating LLMs for chemistry tasks. Based on the dataset, we build a series of LLMs for chemistry named LlaSMol by fine-tuning four open-source LLMs namely Galactica, Llama 2, Code Llama, and Mistral, on SMolInstruct with LoRA (Hu et al., 2022).</p>
<p>We conduct comprehensive experiments to evaluate our models and explore their insights, yielding some interesting findings. Firstly, among the four LlaSMol models, the Mistral-based model surpasses others by a substantial margin, showcasing the considerable influence of base models on downstream chemistry tasks. Moreover, contrast to claims made in previous work (Fang et al., 2023), using SMILES as the molecular representation achieves sufficient validity of generated molecules and better performance compared to using SELFIES (Krenn et al., 2019). Furthermore, employing canonicalized SMILES during model training and applications can alleviate learning burdens and increase performance. Finally, while instruction tuning can inject chemistry task-related knowledge into models, the dataset plays a crucial role. Our experiments demonstrate that training on our SMolInstruct leads to substantially better performance compared to training on previous dataset, emphasizing the contribution of the proposed dataset. Although LlaSMol models do not yet surpass state-of-the-art (SoTA) task-specific models that are designed and trained specifically for each individual task, they approach SoTA performance with only 0.58% of parameters being fine-tuned, suggesting their great potential for further improvements and to serve as strong foundation models for the field.</p>
<h1>2 Related Work</h1>
<p>Task-specific Models for Chemistry. In recent years, many deep learning models have been developed to tackle different chemistry tasks. For example, Molecular Transformer Schwaller et al. (2019) and RSMILES Zhong et al. (2022) formulate forward synthesis and retrosynthesis prediction as sequence-to-sequence translation problems. Chemformer Irwin et al. (2022) pretrains a transformer model on a large-scale SMILES dataset and fine-tunes it for various downstream tasks, such as forward synthesis and property prediction. MolT5 Edwards et al. (2022) first pretrains a T5 model on both SMILES and natural language, and then fine-tunes it to translate SMILES into natural language (i.e., molecule captioning) or vice versa (i.e., molecule generation). Graph neural networks (GNNs), which directly leverage the graph structure of the molecule Wang et al. (2023), have also shown promise in many chemistry applications, such as property prediction Yang et al. (2019); Han et al. (2023), retrosynthesis Chen et al. (2023); Somnath et al. (2021), and molecule optimization Chen et al. (2021); Zhang et al. (2022b). Recent studies Zhou et al. (2023); Zhang et al. (2022a) have shown the promise of leveraging equivariant representations of molecular 3D structures for chemistry tasks, such as property prediction Zhou et al. (2023) and docking Zhang et al. (2022a). Uni-Mol Zhou et al. (2023) incorporates this 3D information into the pretraining of a transformer model and fine-tunes it for downstream tasks. Despite their effectiveness, these models operate on single tasks and therefore cannot harness knowledge shared across diverse chemistry tasks like LLMs.
LLMs for Chemistry. Recent efforts have integrated LLMs with chemistry to solve key chemistry problems, which can be divided into two categories: (1) benchmark studies, and (2) fine-tuning LLMs with new datasets. Multiple benchmark studies White et al. (2023); Guo et al. (2023); Jablonka et al. (2023); Liu et al. (2023a) have evaluated the capabilities and limitations of different off-the-shelf LLMs, such as GPT-4 and Llama, on chemistry problems. For example, Guo et al. (2023) finds that these LLMs do not perform well on chemistry tasks and often produce chemically implausible outputs. These findings highlight the need for further efforts to improve LLMs via fine-tuning for chemistry tasks.
To improve LLMs for chemistry, multiple instruction tuning datasets have been developed. Mol-Instructions Fang et al. (2023) consists of 1.3M instructions for multiple small molecule tasks. However, fine-tuning on the dataset does not significantly improve LLMs' performance (Section 4.3). Drugchat Liang et al. (2023) collects an instruction tuning dataset on drug properties with 10.8 K drug molecules. MolOpt-Instructions Ye et al. (2023) consists of instructions with 1 M molecule pairs for molecule optimization on six properties, in which each pair has similar molecules with different properties. Recent works also develop 2D or 3D molecular graph-centric datasets and integrate the graph understanding ability into LLMs Liu et al. (2023b); Cao et al. (2023); Li et al. (2024). Compared with these datasets, SMolInstruct is much larger and covers a more diverse and comprehensive set of chemistry tasks, which enables LLMs to better understand molecule representations and learn chemistry knowledge across tasks.</p>
<h2>3 SMolInstruct</h2>
<p>This section introduces our proposed dataset SMolInstruct and its construction. Readers may refer to Appendix A for preliminaries and background.</p>
<h3>3.1 Overview of SMolInstruct</h3>
<p>SMolInstruct is a large-scale instruction tuning dataset that centers around small molecules. It contains 14 chemistry tasks, illustrated in Figure 1.
(1) We include four name conversion tasks, namely converting IUPAC name to molecular formula (NC-I2F), converting IUPAC name to SMILES (NC-I2S), converting SMILES to molecular formula (NC-S2F), and converting SMILES to IUPAC name (NC-S2I). They are designed to enable deep understanding of molecular structures and representations, which should serve as the fundamental knowledge for chemistry LLMs.</p>
<p>(2) Additionally, six property prediction tasks (Wu et al., 2018) are integrated, including PPESOL for water solubility (Mobley \&amp; Guthrie, 2014), PP-Lipo for octanol/water distribution coefficient (Poole \&amp; Poole, 2003), PP-BBBP for blood-brain barrier penetration (Martins et al., 2012), PP-ClinTox for toxicity to human body (Gayvert et al., 2016), PP-HIV for HIV replication inhibition (Institute, 2004), and PP-SIDER for side effects of drugs (Kuhn et al., 2015). These involved properties are crucial especially for drug development.
(3) Two tasks focus on the textual descriptions of molecules: molecule captioning (MC) is to generate a textual description of a given molecule, and molecule generation (MG) is to generate a molecule based on the given textual description. They require comprehensive understanding of molecules - their structures and properties, from their textual descriptions. They also bridge the gap between natural language and molecules.
(4) Lastly, two tasks revolve around chemical reaction knowledge. Forward synthesis (FS) aims to predict potential products from reactants and reagents, and retrosynthesis (RS) involves predicting potential reactants given a product. These tasks play vital roles in real-world applications (Coley et al., 2018). For example, retrosynthesis is essential for synthesis planning, while forward synthesis is used to validate retrosynthetic suggestions.
SMolInstruct contains 3.3M samples. Each sample is a query-response pair, where the query describes a task and any task-specific information (e.g., input molecule, textual description, etc.), and the response is a sentence containing the answer to the queried task. For all the tasks, unless explicitly defined in the tasks (NC-I2F, NC-I2S, NC-S2F, and NC-S2I), we use SMILES as the default representation for molecules, but also provide the SELFIES (Krenn et al., 2019) representation.</p>
<h1>3.2 SMolInstruct Construction</h1>
<p>We construct the SMolInstruct dataset by following a four-step pipeline: data collection, quality control, data splitting, and instruction construction.
Data Collection. After consulting domain experts and pinpointing the set of meaningful tasks (summarized in Section 3.1), we collect data for these tasks from various sources, as listed in Table 5. Specifically, for the name conversion tasks (NC-I2F, NC-I2S, NC-S2F, and NC-S2I), we leverage PubChem ${ }^{2}$ (Kim et al., 2019), one of the most comprehensive molecule databases. Within this database, we randomly select a large set of molecule entries, and extract their IUPAC names, SMILES representations, and molecular formulas. This obtained data is then re-organized as input-output pairs for the tasks. For molecular descriptionrelated tasks (MC and MG), we utilize a combination of ChEBI-20 (Edwards et al., 2021; 2022) and Mol-Instructions (Fang et al., 2023), as they both contain high-quality moleculetext paired data. For property prediction tasks (PP-ESOL, PP-Lipo, PP-BBBP, PP-ClinTox, PP-HIV, and PP-SIDER), we employ the well-established MoleculeNet datasets (Wu et al., 2018). We select the 6 datasets from MoleculeNet that represent the essential properties for real-world applications such as drug discovery. For chemical reaction tasks (FS and RS), we collect the reaction data from USPTO-full (Lowe, 2017), which is an extensive collection encompassing over 1 M reaction samples extracted from U.S. patents. All the aforementioned datasets are also widely used in previous studies (He et al., 2021; Zhong et al., 2022; Edwards et al., 2022; Irwin et al., 2022; Chen et al., 2023; Zhou et al., 2023).
Quality Control. To guarantee high quality, we apply rigorous scrutiny. The collected data contains many problematic and low-quality samples, which can be roughly categorized into the following three types, along with our curation methods: (1) Chemically invalid SMILES. Numerous SMILES strings are chemically invalid (e.g., deviating from the SMILES grammar, or violating chemical valence). To address this issue, we employ RDKit (RDKit, 2023), a widely used toolkit for cheminformatics, to parse molecules and detect errors. (2) Wrong or inaccurate information. Based on manual check, we observed wrong and inaccurate information recorded in the data. For instance, within the USPTO-full dataset (Lowe, 2017), we identify and correct mislabeled reactants and reagents in chemical reactions by comparing their atom mappings with products. For the MC and MG tasks, we filter out those</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>textual descriptions that lack pertinent, molecule-specific information, with a set of rules based on wording patterns, lengths and keywords. For PP-SIDER, we eliminate disorders with ambiguous names that could impede the creation of precise and comprehensible instructions. (3) Duplicated samples. We detect and remove them.
Data Splitting. Data splitting for multi-task datasets requires careful handling in order to avoid data leakage across tasks. For instance, FS and RS are a pair of reverse tasks, so data leakage occurs when the training set contains an FS sample for a certain chemical reaction and the test set has an RS sample for the same reaction. This can lead to biased evaluation. Therefore, we identify sample pairs across related tasks (FS and RS, MC and MG, and the four NC tasks) that correspond to the same molecules/reactions, and ensure that matched samples are placed together in either training or evaluation set. Moreover, some samples may share the same input but have different outputs. For instance, in the RS task, one product (the same input) may be synthesized from multiple sets of reactants (different outputs). If these samples are placed into both training and test set, it may lead to exaggerated performance. Therefore we ensure that samples with identical inputs are placed together either in or outside of the test set. Additionally, to achieve fair comparisons with Mol-instructions (Fang et al., 2023), for tasks shared between the two datasets (MC, MG, FS, and RS), we ensure that their training examples are not included in the test set of SMolInstruct, allowing for a direct evaluation of their models on our test set. Following these necessary limitations, samples are randomly split into training/validation/test set, except for PP task samples that undergo a scaffold splitting following the canonical method (Wu et al., 2018).
Instruction Creation. To create query-response textual pairs for instruction tuning, we manually craft several templates, each including a query and a corresponding response, and apply GPT-4 to rephrase them. Unlike those in (Fang et al., 2023) which consist of highly formatted queries (containing three explicitly labeled parts namely instruction, input, and output) and answer-only responses (e.g., responses for FS and RS only contain answer SMILES alone, without any natural text), our templates exhibit a more natural and diverse set of formats in both queries and responses, allowing for more variations and naturalness in input-output interactions. Moreover, all the SMILES representations are canonicalized, establishing a standardized data format. In light of the dataset's inclusion of multi-type sequences (SMILES, molecular formula, numbers, etc.) beyond natural language text alone, we utilize special tags to encapsulate corresponding segments (e.g., <SMILES> . . </SMILES> for SMILES, <MOLFORMULA> . . </MOLFORMULA> for molecular formula, <NUMBER>. . </NUMBER> for numbers). This design does not only explicitly inform models about the information types within the tagged content, but also facilitate answer extraction during evaluation.
For more details of dataset construction, please refer to Appendix B.2.</p>
<h1>3.3 Merits of SMolInstruct</h1>
<p>Compared to previous work (Fang et al., 2023; Liang et al., 2023; Ye et al., 2023), SMolInstruct stands out in several key aspects:
(1) Large-Scale. SMolInstruct consists of 3.3M samples and 1.6M distinct molecules, with a diverse range of sizes, structures, and properties (see Appendix B.1), showcasing an extensive coverage of diverse chemical knowledge.
(2) Comprehensive. SMolInstruct contains 4 types of chemical tasks (14 tasks in total), emerging as the most comprehensive instruction tuning dataset for small molecules. Notably, the tasks are meticulously selected to build a strong chemistry foundation model and to adapt to real-world applications.
(3) High-Quality. Rigorous processing steps have been implemented to exclude problematic and low-quality samples. Along with careful data splitting and canonicalization of SMILES representations, SMolInstruct stands as a high-quality resource valuable for future research.
A detailed introduction and statistics of the SMolInstruct dataset can be found in Appendix B. For a comparison with the previous work, Mol-Instructions (Fang et al., 2023), please refer to Appendix C.</p>
<h1>4 Experiments</h1>
<h3>4.1 Our LlaSMol Models</h3>
<p>By fine-tuning base models on the proposed SMolInstruct dataset, we create LLMs capable of performing chemistry tasks, which we name LlaSMol (Large language models on Small Molecules). Specifically, we extensively consider four different LLMs as our base models, namely Galactica 6.7B (Taylor et al., 2022), Llama 2 (Touvron et al., 2023b) 7B, Code Llama (Roziere et al., 2023) 7B, and Mistral (Jiang et al., 2023) 7B, where Galactica is trained for scientific applications and has already been exposed to chemistry-related data during its pretraining, Llama 2 and Mistral are general-purpose LLMs, while Code Llama is based on Llama 2 and trained for code. We conduct instruction tuning on the proposed SMolInstruct dataset, and name the resulting models as LlaSMol ${ }<em 2="2" _Llama="{Llama" _text="\text">{\text {Galactica }}$, LlaSMol ${ }</em>}}$, LlaSMol ${ <em _Mistral="{Mistral" _text="\text">{\text {Code Llama }}$, and LlaSMol ${ }</em>-4$, and a cosine scheduler. The input length for training is set to 512 , which covers $99.7 \%$ of the samples. During inference, we adopt beam search as the generation strategy for simplicity.}}$, respectively. All the LlaSMol models are trained with LoRA (Hu et al., 2022), which is applied to all weight matrices in the self-attention and feedforward neural network (FFN) modules with lora.r and lora.alpha set to 16. The finetuning process utilizes the Huggingface Transformers library (Wolf et al., 2020). Training spans three epochs, employing the 8 -bit AdamW optimizer, a learning rate of $1 \mathrm{e</p>
<h3>4.2 Experimental Setup</h3>
<p>Compared Models. We compare our LlaSMol models with two types of models:
(1) LLMs without fine-tuning on SMolInstruct. This type includes our four base models, namely Galactica (Taylor et al., 2022), Llama 2 (Touvron et al., 2023b), Code Llama (Roziere et al., 2023), Mistral (Jiang et al., 2023). we also benchmark against GPT-4 (OpenAI, 2023) and the more recent Claude 3 Opus (Anthropic, 2024), the current state-of-the-art (SoTA) LLMs ${ }^{3}$. For Llama 2, Code Llama, and Mistral, we use 1-shot, due to their poor instruction following ability; for GPT-4, we report its results under a zero-shot setting, as GPT-4 performs best on this setting in our experiments (Appendix E); for Claude 3 Opus, we report its zero-shot results as well. We also include two LLMs tuned specifically for chemistry tasks: Molinst, a Llama 2 model tuned on the Mol-Instructions dataset by Fang et al. (2023), which shares the training tasks of MC, MG, FS, and RS with LlaSMol; and ChemLLM (Zhang et al., 2024), an LLM for chemistry proposed concurrently to our work.
(2) SoTA task-specific models. To provide a comprehensive view of LlaSMol's performance, we present results from SoTA task-specific models. For NC-I2S and NC-S2I, we compare with STOUT (Rajan et al., 2021), an encoder-decoder model trained on SMILES-IUPAC name paired data. For NC-S2F, a task achievable with a fixed algorithm, we implement a program with RDKit (RDKit, 2023), a widely used Python toolkit for cheminformatics, and report its results. For NC-I2F where no dedicated models exist, we construct a baseline called STOUT+RDKit by aggregating STOUT for I2S conversion and RDKit for S2F conversion. For the PP tasks, our compared model is Uni-Mol (Zhou et al., 2023). It incorporates molecular 3D representations and follows a pretraining and fine-tuning paradigm. Following its original settings, we fine-tune the model on our SMolInstruct dataset with its pretrained checkpoint. In the case of MC and MG, we compare with MolT5 (Edwards et al., 2022) and directly use their released checkpoint. The reasons why we do not use our re-trained model are: (1) we were unable to reproduce results close to those reported in the paper as no original code was provided; and (2) we take great care to ensure that our test set is devoid of training examples used by MolT5, ensuring fairness in the evaluation. Lastly, regarding FS and RS, we re-train RSMILES (Zhong et al., 2022) and Molecular Transformer (Schwaller et al., 2019) for the two tasks, respectively, following their reported settings. Both of the models are transformer encoder-decoder models (Vaswani et al., 2017), specifically adapted for the FS and RS tasks.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Results for name conversion (NC) and property prediction (PP) tasks. Metrics EM, Valid, and Acc are in percentage.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">NC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I2F</td>
<td style="text-align: center;">I2S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S2F</td>
<td style="text-align: center;">S2I</td>
<td style="text-align: center;">ESOL</td>
<td style="text-align: center;">Lipo</td>
<td style="text-align: center;">BBBP</td>
<td style="text-align: center;">Clintox</td>
<td style="text-align: center;">HIV</td>
<td style="text-align: center;">SIDER</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">RMSE $\downarrow$</td>
<td style="text-align: center;">RMSE $\downarrow$</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">Task-Specific, Non-LLM Based Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SoTA</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: center;">Existing LLMs without fine-tuning on SMoIInstruct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.570</td>
<td style="text-align: center;">1.545</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">57.6</td>
</tr>
<tr>
<td style="text-align: center;">Claude 3 Opus</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.036</td>
<td style="text-align: center;">1.194</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">67.0</td>
</tr>
<tr>
<td style="text-align: center;">Galactica</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.184</td>
<td style="text-align: center;">2.979</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">68.1</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.287</td>
<td style="text-align: center;">1.634</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">61.9</td>
</tr>
<tr>
<td style="text-align: center;">Code Llama</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.483</td>
<td style="text-align: center;">1.733</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">60.2</td>
</tr>
<tr>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.079</td>
<td style="text-align: center;">1.730</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">38.1</td>
</tr>
<tr>
<td style="text-align: center;">Molinst (chemistry LLM)</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.271</td>
<td style="text-align: center;">1.691</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">52.4</td>
</tr>
<tr>
<td style="text-align: center;">ChemLLM (chemistry LLM)</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.946</td>
<td style="text-align: center;">1.797</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">32.6</td>
</tr>
<tr>
<td style="text-align: center;">Our LlaSMol Series</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol $_{\text {Galactica }}$</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">1.959</td>
<td style="text-align: center;">1.213</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">70.1</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol $_{\text {Llama 2 }}$</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">2.791</td>
<td style="text-align: center;">1.338</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">68.7</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol $_{\text {Code Llama }}$</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">2.959</td>
<td style="text-align: center;">1.203</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">69.9</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol $_{\text {Mistral }}$</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">1.150</td>
<td style="text-align: center;">1.010</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">70.7</td>
</tr>
</tbody>
</table>
<p>Evaluation Metrics. We employ metrics commonly used in previous work (Schwaller et al., 2019; Zhong et al., 2022; Fang et al., 2023; Zhou et al., 2023; Chen et al., 2023), which include: (1) Exact Match (EM), indicating the proportion of predicted results that exactly match the gold standards. (2) Fingerprint Tanimoto Similarity (FTS), quantifying structural similarities between molecules using Tanimoto similarities of their Morgan fingerprints (Morgan, 1965). (3) METEOR score, a comprehensive text-based metric considering both exact matches and semantic similarity (Lavie \&amp; Agarwal, 2007) for the MC task. (4) Root Mean Square Error (RMSE), measuring the square root of the average squared differences between predicted and actual values for the PP-ESOL and PP-Lipo tasks (5) Accuracy (Acc), the ratio of correct predictions for the binary classification tasks (PP-BBBP, PP-ClinTox, PP-HIV, and PP-SIDER). (6) Validity (Valid), the ratio of valid predictions following SMILES grammar and chemical valence rules for tasks with SMILES outputs (NC-I2S, MG, FS, and RS). For all the metrics except RMSE, higher values indicate better performance.</p>
<h1>4.3 Main Results</h1>
<p>Table 1 and 2 show the performance on SMoIInstruct. Key observations are as follows:
(1) Among all the LLMs, our LlaSMol models demonstrate the best performance, underscoring the effectiveness of the proposed SMoIInstruct dataset and fine-tuning. Specifically, compared to the base models (Galactica, Llama 2, Code Llama, and Mistral), LlaSMol models exhibit substantial performance improvements, which highlights the effectiveness of SMoIInstruct in enhancing the understanding of molecular representations and the taskrelated knowledge, and signifies the effective learning of chemistry-related tasks by LLMs. Furthermore, LlaSMol substantially outperforms GPT-4 on all the tasks and Claude 3 Opus on most tasks, despite their larger parameter size. LlaSMol also surpasses the two chemistry LLMs namely ChemLLM ${ }^{4}$, which is similarly trained on chemistry instruction data. and Molinst. Notably, LlaSMol $<em _Mistral="{Mistral" _text="\text">{\text {Llama 2 }}$, which uses the same base model and LoRA setting as Molinst, outperforms it even on the shared training tasks (MC, MG, FS, and RS). This finding highlights the benefits of our dataset.
(2) Our four LlaSMol models show substantial differences in their performance, emphasizing the considerable impact of base models on downstream tasks. Despite sharing identical training, inference settings, and comparable model sizes, LlaSMol $</em>$ consistently outperforms LlaSMol $}<em Llama="Llama" _Code="{Code" _text="\text">{\text {Llama 2 }}$ by a substantial margin, highlighting Mistral's potential on chemistry tasks. In addition, LlaSMol $</em>$ on most tasks, indicating a potential synergy between programming language knowledge}}$ exhibits better performance than LlaSMol $_{\text {Llama 2 }</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Results for molecule captioning (MC), molecule generation (MG), forward synthesis (FS), and retrosynthesis (RS). Metrics EM, FTS, and Valid are in percentage.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>MC</th>
<th>MG</th>
<th></th>
<th></th>
<th>FS</th>
<th></th>
<th></th>
<th>RS</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>METEOR</td>
<td>EM</td>
<td>FTS</td>
<td>Valid</td>
<td>EM</td>
<td>FTS</td>
<td>Valid</td>
<td>EM</td>
<td>FTS</td>
<td>Valid</td>
</tr>
<tr>
<td>Task-Specific, Non-LLM Based Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SoTA</td>
<td>0.515</td>
<td>31.7</td>
<td>73.2</td>
<td>95.3</td>
<td>78.7</td>
<td>92.2</td>
<td>100.0</td>
<td>47.0</td>
<td>77.5</td>
<td>99.7</td>
</tr>
<tr>
<td>Existing LLMs Without Fine-Tuning on SMoIInstruct</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.188</td>
<td>6.4</td>
<td>42.6</td>
<td>81.4</td>
<td>1.6</td>
<td>40.5</td>
<td>87.0</td>
<td>0.0</td>
<td>33.4</td>
<td>42.6</td>
</tr>
<tr>
<td>Claude 3 Opus</td>
<td>0.219</td>
<td>12.3</td>
<td>57.6</td>
<td>92.6</td>
<td>3.7</td>
<td>45.7</td>
<td>97.0</td>
<td>1.1</td>
<td>46.2</td>
<td>94.8</td>
</tr>
<tr>
<td>Galactica</td>
<td>0.050</td>
<td>0.0</td>
<td>11.6</td>
<td>94.7</td>
<td>0.0</td>
<td>25.9</td>
<td>83.7</td>
<td>0.0</td>
<td>34.6</td>
<td>93.0</td>
</tr>
<tr>
<td>Llama 2</td>
<td>0.150</td>
<td>0.0</td>
<td>4.8</td>
<td>93.5</td>
<td>0.0</td>
<td>13.7</td>
<td>97.7</td>
<td>0.0</td>
<td>27.5</td>
<td>87.7</td>
</tr>
<tr>
<td>Code Llama</td>
<td>0.143</td>
<td>0.0</td>
<td>8.5</td>
<td>95.2</td>
<td>0.0</td>
<td>15.8</td>
<td>99.6</td>
<td>0.0</td>
<td>25.3</td>
<td>97.1</td>
</tr>
<tr>
<td>Mistral</td>
<td>0.193</td>
<td>0.0</td>
<td>9.0</td>
<td>35.9</td>
<td>0.0</td>
<td>19.9</td>
<td>95.8</td>
<td>0.0</td>
<td>24.2</td>
<td>98.0</td>
</tr>
<tr>
<td>Molinst (chemistry LLM)</td>
<td>0.124</td>
<td>6.0</td>
<td>43.6</td>
<td>84.8</td>
<td>2.1</td>
<td>31.7</td>
<td>99.8</td>
<td>5.7</td>
<td>48.0</td>
<td>97.8</td>
</tr>
<tr>
<td>ChemLLM (chemistry LLM)</td>
<td>0.050</td>
<td>0.9</td>
<td>14.3</td>
<td>4.3</td>
<td>0.0</td>
<td>1.6</td>
<td>38.5</td>
<td>0.0</td>
<td>2.9</td>
<td>10.9</td>
</tr>
<tr>
<td>Our LlaSMol Series</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LlaSMol $_{\text {Galactica }}$</td>
<td>0.394</td>
<td>7.7</td>
<td>52.2</td>
<td>99.6</td>
<td>53.1</td>
<td>79.9</td>
<td>99.7</td>
<td>25.7</td>
<td>67.0</td>
<td>99.9</td>
</tr>
<tr>
<td>LlaSMol $_{\text {Llama 2 }}$</td>
<td>0.377</td>
<td>6.4</td>
<td>47.1</td>
<td>99.6</td>
<td>47.1</td>
<td>76.9</td>
<td>99.8</td>
<td>22.5</td>
<td>65.2</td>
<td>99.9</td>
</tr>
<tr>
<td>LlaSMol $_{\text {Code Llama }}$</td>
<td>0.366</td>
<td>6.5</td>
<td>46.6</td>
<td>99.7</td>
<td>52.0</td>
<td>79.2</td>
<td>99.8</td>
<td>25.7</td>
<td>66.7</td>
<td>100.0</td>
</tr>
<tr>
<td>LlaSMol $_{\text {Mistral }}$</td>
<td>0.452</td>
<td>19.2</td>
<td>61.7</td>
<td>99.7</td>
<td>63.3</td>
<td>84.9</td>
<td>99.8</td>
<td>32.9</td>
<td>70.4</td>
<td>100.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of ablation study on NC and PP tasks. Metrics EM, Valid, and Acc are in percentage. Orange cells represent better results than LlaSMol $_{\text {Mistral }}$ while blue cells represent worse results.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>NC</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>PP</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>I2F</td>
<td>I2S</td>
<td></td>
<td>S2F</td>
<td>S2I</td>
<td>ESOL</td>
<td>Lipo</td>
<td>BBBP</td>
<td>Clintox</td>
<td>HIV</td>
<td>SIDER</td>
</tr>
<tr>
<td></td>
<td>EM</td>
<td>EM</td>
<td>Valid</td>
<td>EM</td>
<td>EM</td>
<td>RMSE $_{1}$</td>
<td>RMSE $_{1}$</td>
<td>Acc</td>
<td>Acc</td>
<td>Acc</td>
<td>Acc</td>
</tr>
<tr>
<td>LlaSMol $_{\text {Mistral }}$</td>
<td>87.9</td>
<td>70.1</td>
<td>99.6</td>
<td>93.2</td>
<td>29.0</td>
<td>1.150</td>
<td>1.010</td>
<td>74.6</td>
<td>93.1</td>
<td>96.7</td>
<td>70.7</td>
</tr>
<tr>
<td>w/o canonical</td>
<td>88.5</td>
<td>67.2</td>
<td>99.6</td>
<td>93.4</td>
<td>24.5</td>
<td>1.224</td>
<td>1.072</td>
<td>71.6</td>
<td>93.1</td>
<td>96.8</td>
<td>70.3</td>
</tr>
<tr>
<td>using SELFIES</td>
<td>86.9</td>
<td>47.7</td>
<td>100.0</td>
<td>94.7</td>
<td>19.7</td>
<td>1.456</td>
<td>1.106</td>
<td>69.5</td>
<td>91.7</td>
<td>96.5</td>
<td>64.4</td>
</tr>
<tr>
<td>train on Mol-Instructions</td>
<td>0.0</td>
<td>0.0</td>
<td>75.2</td>
<td>0.0</td>
<td>0.0</td>
<td>4.416</td>
<td>2.282</td>
<td>0.0</td>
<td>0.0</td>
<td>2.6</td>
<td>0.4</td>
</tr>
</tbody>
</table>
<p>in Code Llama and molecular representations. Furthermore, LlaSMol $<em 2="2" _Llama="{Llama" _text="\text">{\text {Galactica }}$ outperforms LlaSMol $</em>$, and LlaSMol $}<em _Mistral="{Mistral" _text="\text">{\text {Code Llama }}$ in most cases, suggesting the benefits of pretraining on chemistry-related documents.
(3) Although LlaSMol models do not outperform SoTA models, they demonstrate considerable potential for further improvements. Specifically, LlaSMol $</em>$ surpasses the SoTA models on PP-Clintox and PP-SIDER, but has yet to achieve the success on other tasks. However, LlaSMol has greatly narrowed the performance gap between LLMs and SoTA task-specific models, compared to previous efforts (Fang et al., 2023; Zhang et al., 2024). Remarkably, LlaSMol $}<em _Mistral="{Mistral" _text="\text">{\text {Mistral }}$ attains such performance with only a small proportion of its parameters fine-tuned (approximately $41.9 \mathrm{M}, 0.58 \%$ of its parameters). As shown in Appendix F.2, increasing the number of trainable parameters can substantially boost performance, suggesting that LlaSMol $</em>$ has immense potential to surpass task-specific models through more extensive fine-tuning and serve as a strong foundation model for chemistry applications.}</p>
<h1>4.4 Ablation Study</h1>
<p>To investigate the advantages of SMoIInstruct, we conduct an ablation study by comparing LlaSMol $<em _Mistral="{Mistral" _text="\text">{\text {Mistral }}$ with the following variants: (1) w/o canonical, which uses uncanonicalized SMILES, to examine the benefits of canonicalization. (2) using SELFIES, which uses SELFIES Krenn et al. (2019) instead of SMILES to explore their differences. (3) train on Mol-Instructions, which is trained on Mol-Instructions (Fang et al., 2023), to compare the performance improvements of our dataset against the previously proposed dataset.
The results in Table 3 and Table 4 lead to the following observations: (1) The "w/o canonical" model underperforms LlaSMol $</em>$ on most tasks, with a substantial performance drop on FS and RS. This suggests that canonicalizing SMILES can reduce learning difficulty and improve performance. As canonicalization can be easily performed using fixed algorithms}</p>
<p>Table 4: Results of ablation study on MC, MG, FS, and RS. Metrics EM, FTS, and Valid are in percentage. Orange represents better results than LlaSMol ${ }_{\text {Mistral }}$, while blue represents worse results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MC</th>
<th style="text-align: center;">MG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">FTS</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">FTS</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">FTS</td>
<td style="text-align: center;">Valid</td>
</tr>
<tr>
<td style="text-align: center;">LlaSMol ${ }_{\text {Mistral }}$</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">w/o canonical</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;">using SELFIES</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;">train on Mol-Instructions</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">76.7</td>
</tr>
</tbody>
</table>
<p>before feeding into models, we recommend using canonical SMILES when training and applying LLMs for chemistry. (2) While using SELFIES slightly improves the validity of generated molecules, which aligns with the motivation behind SELFIES (Krenn et al., 2019), the validity of using SMILES is also sufficiently high. Moreover, using SELFIES results in worse performance on most tasks, possibly due to SELFIES being typically longer than SMILES, making it more difficult for the model to accurately understand and generate. Therefore, using SELFIES over SMILES may not be necessary, contrast to claims made in previous work (Krenn et al., 2019; Fang et al., 2023). (3) Despite using identical base models and training settings, the model trained on Mol-Instructions (Fang et al., 2023) performs much worse than LlaSMol ${ }_{\text {Mistral }}$ trained on SMolInstruct even on the shared tasks (MC, MG, FS, and RS). This demonstrates the superiority of our dataset. A detailed comparison with Mol-Instructions can be found in Appendix C.</p>
<p>To gain deeper insights into the models' performance and behavior, we conduct further analytical experiments: (1) To investigate the synergistic effects among different tasks, we evaluate models trained on a single task and models with certain tasks removed. The results demonstrate multiple-task training outperforms single-task training, indicating its benefits. However, each task generally does not heavily rely on the presence of other tasks, suggesting a degree of independence among them. (2) To investigate the influence of LoRA (Hu et al., 2022) settings, we vary the involved LoRA modules. We observe that adding LoRA modules (and trainable parameters) leads to a substantial boost in performance, indicates the models' great potential for further improvements if with larger-scale fine-tuning. Please refer to Appendix F for more details.</p>
<h1>5 Conclusion</h1>
<p>While LLMs have shown promise as versatile assistants, their performance on chemistryrelated tasks remains notably subpar. To address this issue, we introduces SMolInstruct, a large-scale, comprehensive, and high-quality instruction tuning dataset. It comprises 14 tasks highly relevant to real-world applications and contains over 3M rigorously curated samples. Using SMolInstruct, we develop LlaSMol, a series of LLMs for performing chemistry tasks. Our experiments demonstrate LlaSMol's superiority over existing LLMs, and highlight SMolInstruct's crucial role in boosting the performance. Further analytical experiments also provide significant insights towards developing LLMs for chemistry.
However, this work has the following limitations. First, the evaluations for the MC and MG tasks cannot accurately assess models' abilities to generate chemically correct descriptions and molecules. Since the definition of molecular descriptions remain ambiguous and the available data is limited, it is challenging to assess whether the generated descriptions or molecules are accurate and correct. Second, this work does not delve into the models' generalization capabilities beyond the trained tasks. While we recognize the importance of such capabilities, how to meaningfully test generalization abilities is nontrivial and needs careful design, which falls outside the purview of this work. Third, our models do not yet outperform SoTA task-specific models, possibly due to the small ratio of trainable parameters or suboptimal training procedures. Nevertheless, we propose a high-quality instruction tuning dataset, demonstrate its effectiveness, and gain deeper insights, which we hope can be valuable for future research. We will try to address the aforementioned limitations in our future work.</p>
<h1>Ethics Statement</h1>
<p>Despite our best efforts to maintain the high quality of the SMollnstruct dataset and the integrity of the LlaSMol models, we cannot guarantee that the dataset is free of inaccurate, incorrect, or harmful content, nor can we prevent the models from generating such content. Users should engage with our dataset and models at their own discretion and uphold the highest ethical standards in their use.</p>
<h2>Acknowledgement</h2>
<p>The authors would thank colleagues from the OSU NLP group and the OSU Ning Lab for constructive feedback. This research was supported in part by NSF IIS-2133650, NIH 1R01LM014385-01, and NSF CAREER #1942980, as well as Ohio Supercomputer Center (Ohio Supercomputer Center, 1987). The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.</p>
<h2>References</h2>
<p>Mikhail Andronov, Varvara Voinarovska, Natalia Andronova, Michael Wand, Djork-Arné Clevert, and Jürgen Schmidhuber. Reagent prediction with a molecular transformer improves reaction data quality. Chemical Science, 14(12):3235-3246, 2023.</p>
<p>Anthropic. The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024.
Theodore L. Brown. Chemistry: the central science. Pearson, 14th edition edition, 2018.
Andrew R. Burns, Trevor C. Y. Kwok, Al Howard, Ed Houston, Karl Johanson, Anthony Chan, Sean R. Cutler, Peter McCourt, and Peter J. Roy. High-throughput screening of small molecules for bioactivity and target identification in Caenorhabditis elegans. Nature Protocols, 1:1906-1914, 2006.</p>
<p>He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery. arXiv preprint arXiv:2311.16208, 2023.</p>
<p>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 2024.</p>
<p>Heng-Yi Chen, Michael Hsu, and Chan-Wang Jerry Lio. Micro but mighty - Micronutrients in the epigenetic regulation of adaptive immune responses. Immunological reviews, 305: $152-164,2022$.</p>
<p>Ziqi Chen, Martin Renqiang Min, Srinivasan Parthasarathy, and Xia Ning. A deep generative model for molecule optimization via one fragment modification. Nature machine intelligence, 3(12):1040-1049, 2021.</p>
<p>Ziqi Chen, Oluwatosin R Ayinde, James R Fuchs, Huan Sun, and Xia Ning. G2retro as a twostep graph generative models for retrosynthesis prediction. Communications Chemistry, 6 (1):102, 2023.</p>
<p>Connor W. Coley, William H. Green, and Klavs F. Jensen. Machine learning in computeraided synthesis planning. Accounts of Chemical Research, 51(5):1281-1289, 2018.</p>
<p>Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. In Proceedings of Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2023.</p>
<p>Joseph L Durant, Burton A Leland, Douglas R Henry, and James G Nourse. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42(6):1273-1280, 2002.</p>
<p>Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2mol: Cross-modal molecule retrieval with natural language queries. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 595-607, 2021.</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 375-413, 2022.</p>
<p>Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Mol-instructions: A large-scale biomolecular instruction dataset for large language models. arXiv preprint arXiv:2306.08018, 2023.</p>
<p>Henri A. Favre and Warren H. Powell. Nomenclature of organic chemistry: IUPAC recommendations and preferred names 2013. Royal Society of Chemistry, 2014.</p>
<p>Kaitlyn M. Gayvert, Neel S. Madhukar, and Olivier Elemento. A data-driven approach to predicting successes and failures of clinical trials. Cell Chemical Biology, 23(10):1294-1301, 2016.</p>
<p>Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. What can large language models do in chemistry? a comprehensive benchmark on eight tasks. In Proceedings of Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2023.</p>
<p>Shen Han, Haitao Fu, Yuyang Wu, Ganglan Zhao, Zhenyu Song, Feng Huang, Zhongfei Zhang, Shichao Liu, and Wen Zhang. Himgnn: a novel hierarchical molecular graph representation learning framework for property prediction. Briefings in Bioinformatics, 24 (5):bbad305, 2023.</p>
<p>Kan Hatakeyama-Sato, Naoki Yamane, Yasuhiko Igarashi, Yuta Nabae, and Teruaki Hayakawa. Prompt engineering of gpt-4 for chemical research: what can/cannot be done? Science and Technology of Advanced Materials: Methods, 3(1), 2023.</p>
<p>Jiazhen He, Huifang You, Emil Sandström, Eva Nittinger, Esben Jannik Bjerrum, Christian Tyrchan, Werngard Czechtizky, and Ola Engkvist. Molecular optimization by capturing chemist's intuition using deep neural networks. Journal of cheminformatics, 13(1):1-17, 2021.</p>
<p>Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022.</p>
<p>National Cancer Institute. AIDS antiviral screen data, 2004. URL https://wiki.nci.nih. gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data. Accessed on 1 Fec 2024.</p>
<p>Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pre-trained transformer for computational chemistry. Machine Learning: Science and Technology, 3(1):015022, 2022.</p>
<p>Kevin Maik Jablonka, Philippe Schwaller, and Berend Smit. Is gpt-3 all you need for machine learning for chemistry? In AI for Accelerated Materials Design NeurIPS 2022 Workshop, 2022.</p>
<p>Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. Leveraging large language models for predictive chemistry. ChemRxiv, 2023. doi: 10.26434/chemrxiv-2023-fw8n4-v3.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.</p>
<p>Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019 update: improved access to chemical data. Nucleic acids research, 47:D1102-D1109, 2019.</p>
<p>Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017.</p>
<p>Mario Krenn, Florian Häse, A Nigam, Pascal Friederich, and Alán Aspuru-Guzik. Selfies: a robust representation of semantically constrained graphs with an example application in chemistry. arXiv preprint arXiv:1905.13741, 1(3), 2019.</p>
<p>Michael Kuhn, Ivica Letunic, Lars Juhl Jensen, and Peer Bork. The sider database of drugs and side effects. Nucleic Acids Research, 44:D1075-D1079, 2015.</p>
<p>Alon Lavie and Abhaya Agarwal. Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT '07, pp. 228-231. Association for Computational Linguistics, 2007.</p>
<p>Elena Lenci and Andrea Trabocchi. Chapter 1 - Synthetic approaches toward small molecule libraries. In Small Molecule Drug Discovery, pp. 1-34. Elsevier, 2020.</p>
<p>Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, TatSeng Chua, and Qi Tian. Towards 3d molecule-text interpretation in language models. In Proceedings of International Conference on Learning Representations (ICLR), 2024.</p>
<p>Youwei Liang, Ruiyi Zhang, Li Zhang, and Pengtao Xie. Drugchat: towards enabling chatgpt-like capabilities on drug molecule graphs. arXiv preprint arXiv:2309.03907, 2023.</p>
<p>Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, and Chaowei Xiao. Chatgpt-powered conversational drug editing using retrieval and domain feedback. arXiv preprint arXiv:2305.18090, 2023a.</p>
<p>Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $15623-15638,2023 b$.</p>
<p>Daniel Lowe. Chemical reactions from us patents (1976-sep2016), 2017. URL https://doi . org/10.6084/m9.figshare.5104873.v1.</p>
<p>Ines Filipa Martins, Ana L. Teixeira, Luis Pinheiro, and Andre O. Falcao. A bayesian approach to in silico blood-brain barrier penetration modeling. Journal of Chemical Information and Modeling, 52(6):1686-1697, 2012.</p>
<p>Brian W Matthews. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442-451, 1975.</p>
<p>Monica P McNerney and Mark P Styczynski. Small molecule signaling, regulation, and potential applications in cellular therapeutics. Wiley Interdisciplinary Reviews: Systems Biology and Medicine, 10:e1405, 2018.</p>
<p>David L. Mobley and J. Peter Guthrie. Freesolv: a database of experimental and calculated hydration free energies, with input files. Journal of Computer-Aided Molecular Design, 28(7): $711-720,2014$.
H. L. Morgan. The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. Journal of Chemical Documentation, 5(2): $107-113,1965$.</p>
<p>Ohio Supercomputer Center. Ohio supercomputer center, 1987. URL http://osc.edu/ark: /19495/f5s1ph73.</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Salwa K Poole and Colin F Poole. Separation methods for estimating octanol-water partition coefficients. Journal of Chromatography B, 797(1-2):3-19, 2003.</p>
<p>Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Stout: Smiles to iupac names using neural machine translation. Journal of Cheminformatics, 13(1):1-14, 2021.</p>
<p>RDKit. Rdkit: Open-source cheminformatics, 2023. URL https://doi.org/10.5281/zenodo. 8254217. Accessed on 27 Jan 2024.</p>
<p>Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.</p>
<p>Nadine Schneider, Roger A Sayle, and Gregory A Landrum. Get your atoms in order: An open-source implementation of a novel and robust molecular canonicalization algorithm. Journal of chemical information and modeling, 55(10):2111-2120, 2015.</p>
<p>Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):1572-1583, 2019.
T. W. Graham Solomons, Craig B. Fryhle, and Scott A. Snyder. Organic Chemistry, Integrated E-Text with E-Solutions Manual. Wiley, 13th edition, 2022.</p>
<p>Vignesh Ram Somnath, Charlotte Bunne, Connor Coley, Andreas Krause, and Regina Barzilay. Learning graph models for retrosynthesis prediction. Advances in Neural Information Processing Systems, 34:9405-9415, 2021.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature Medicine, 29(8):1930-1940, 2023.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of Conference on Advances in neural information processing systems (NeurIPS), volume 30, 2017.</p>
<p>Yuyang Wang, Zijie Li, and Amir Barati Farimani. Graph Neural Networks for Molecules. In Chen Qu and Hanchao Liu (eds.), Machine Learning in Molecular Sciences, pp. 21-66. Springer International Publishing, Cham, 2023. ISBN 978-3-031-37196-7. doi: 10.1007/ 978-3-031-37196-7_2. URL https://doi.org/10.1007/978-3-031-37196-7_2.</p>
<p>David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28 (1):31-36, 1988.</p>
<p>Andrew D. White, Glen M. Hocky, Heta A. Gandhi, Mehrad Ansari, Sam Cox, Geemi P. Wellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, and Willmor J. Peña Ccoa. Assessment of chemistry knowledge in large language models that generate code. Digital Discovery, 2(2):368-376, 2023.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of conference on empirical methods in natural language processing: system demonstrations (EMNLP), pp. 38-45, 2020.</p>
<p>Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513-530, 2018.</p>
<p>Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, Andrew Palmer, Volker Settels, Tommi Jaakkola, Klavs Jensen, and Regina Barzilay. Analyzing Learned Molecular Representations for Property Prediction. Journal of Chemical Information and Modeling, 59(8):3370-3388, August 2019. ISSN 1549-9596. doi: 10.1021/acs.jcim.9b00237. URL https://doi.org/10.1021/acs.jcim.9b00237. Publisher: American Chemical Society.</p>
<p>Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, and Xiangxiang Zeng. Drugassist: A large language model for molecule optimization. arXiv preprint arXiv:2401.10334, 2023.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, et al. Chemllm: A chemical large language model. arXiv preprint arXiv:2402.06852, 2024.</p>
<p>Tianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. Tablellama: Towards open large generalist models for tables. arXiv preprint arXiv:2311.09206, 2023.</p>
<p>Yangtian Zhang, Huiyu Cai, Chence Shi, and Jian Tang. E3bind: An end-to-end equivariant network for protein-ligand docking. In The Eleventh International Conference on Learning Representations, 2022a.</p>
<p>Zaixi Zhang, Yaosen Min, Shuxin Zheng, and Qi Liu. Molecule generation for target protein binding with structural motifs. In The Eleventh International Conference on Learning Representations, 2022b.</p>
<p>Zipeng Zhong, Jie Song, Zunlei Feng, Tiantao Liu, Lingxiang Jia, Shaolun Yao, Min Wu, Tingjun Hou, and Mingli Song. Root-aligned smiles: a tight representation for chemical reaction prediction. Chemical Science, 13(31):9023-9034, 2022.</p>
<p>Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. In International Conference on Learning Representations (ICLR), 2023.</p>
<h1>Table of Contents in Appendix</h1>
<p>A Preliminaries ..... 16
B Details of SMolInstruct ..... 16
B. 1 The statistics of SMolInstruct ..... 16
B. 2 Details of Dataset Construction ..... 17
C Comparison with Mol-Instructions ..... 18
D Details of Experimental Setup ..... 19
D. 1 LlaSMol Models ..... 20
D. 2 Compared LLMs ..... 20
D.2.1 GPT-4 ..... 20
D.2.2 Claude 3 Opus ..... 21
D.2.3 Galactica ..... 21
D.2.4 Llama 2, Code Llama, and Mistral ..... 21
D.2.5 Molinst ..... 22
D.2.6 ChemLLM ..... 22
D. 3 Task-Specific, Non-LLM Based SoTA Models ..... 22
D.3.1 STOUT for NC-I2S and NC-S2I ..... 22
D.3.2 RDKit for NC-S2F ..... 22
D.3.3 STOUT+RDKit for NC-I2F ..... 22
D.3.4 Uni-Mol for All The PP Tasks ..... 22
D.3.5 MolT5 for MC and MG ..... 23
D.3.6 RSMILES for FS and RS ..... 23
D.3.7 Molecular Transformer for FS and RS ..... 23
D. 4 Evaluation Metrics ..... 24
E Detailed Experimental Results ..... 24
E. 1 Name Conversion Tasks ..... 24
E. 2 Property Prediction ..... 27
E. 3 Molecule Description ..... 28
E. 4 Chemical Reaction ..... 28
E. 5 Other Common Findings ..... 28
F More Analytical Experiments ..... 30
F. 1 Task Synergy ..... 30
F. 2 Influence of LoRA Modules and Trainable Parameters ..... 31</p>
<h1>A Preliminaries</h1>
<p>Molecules form the basis of chemistry, which fundamentally determines the properties and behaviors of most substances. A molecule is a group of atoms held together by chemical bonds (Brown, 2018). In this paper, we focus on small molecules, which typically have no more than 100 atoms and a low molecular weight under 1,500 Daltons (Lenci \&amp; Trabocchi, 2020). Small molecules perform many important functions, such as signaling in cellular biology (McNerney \&amp; Styczynski, 2018), pest control in agriculture (Burns et al., 2006), micronutrients in nutrition (Chen et al., 2022), and drug therapy in medicine (Lenci \&amp; Trabocchi, 2020). Given the importance of small molecules, it is essential to integrate LLMs into the study of small molecules to further advance their design or development.
Molecules can be represented in multiple ways, such as SMILES strings, IUPAC names, and molecular formulas. SMILES strings use a sequence of symbols to encode the 2D structures of molecules (Weininger, 1988). A molecule can have multiple SMILES strings; a canonical SMILES for the molecule is unique and deterministic. For example, the canonical SMILES representation of glucose is " $\left.\mathrm{C}(\mathrm{C} 1 \mathrm{C}(\mathrm{C}(\mathrm{C}(\mathrm{O} 1) \mathrm{O}) \mathrm{O}) \mathrm{O}) \mathrm{O}\right) \mathrm{O}$ ". SELFIES (Krenn et al., 2019) is an alternative representation to SMILES that also uses a sequence of symbols to denote molecular structures. Its key advantage is robustness, as every SELFIES string is guaranteed to correspond to a valid molecule. The SELFIES representation corresponding to the above SMILES representation of glucose is "[C][Branch2][Ring1][Branch1][C][C][Branch1][S][C][Branch1][N][C][Branch1][Branch2] [C][Branch1][Ring2][O][Ring1][=Branch1][O][O][O][O][O]". Molecular formulas represent a molecule by enumerating the type and number of atoms in the molecule (Solomons et al., 2022). For example, the molecular formula for glucose is " $\mathrm{C}<em 12="12">{6} \mathrm{H}</em>$ ". IUPAC names are formal names based on natural language elements, which follow the systematic rules set by the International Union of Preferred and Applied Chemistry (IUPAC) (Favre \&amp; Powell, 2014). These names are derived from the structures and functional groups of molecules, and are intended to be human-readable. For example, the IUPAC name for glucose is "(3R,4S,5S,6R)-6-(hydroxymethyl)oxane-2,3,4,5-tetrol".
Molecules are one of the fundamental units of chemistry that participate in reactions (Brown, 2018). A reaction is a process which converts input molecules (reactants) into output molecules (products) through the breaking and forming of chemical bonds. Other molecules (reagents) may be present to enhance or facilitate the reaction.} \mathrm{O}_{6</p>
<h2>B Details of SMolInstruct</h2>
<p>In this section, we introduce the details of our proposed dataset SMolInstruct, including statistics and construction details.</p>
<h2>B. 1 The statistics of SMolInstruct</h2>
<p>Table 5 shows the statistics of SMolInstruct. It contains 4 types of altogether 14 tasks, which are selected to be meaningful and useful. There are about 3.3 M samples, and each of them is a distinct sample. In other words, there does not exist a pair of samples who share the same chemical information (i.e., the core input and output information, such as input molecules and output molecules), but with the same or different natural language templates (i.e., the task description in the query and the sentence templates in the response). When needed, one can easily create more instruction tuning samples by combining one piece of chemical information with multiple natural language templates. All in all, SMolInstruct can serve as a good benchmark for training and evaluating LLMs on various chemistry tasks.
To know more about the diversity of SMolInstruct, we conduct a statistics on the molecules. Altogether, there exist 1.6 M distinct molecules, and several important statistical values are shown in Figure 2. Specifically, Bertz complexity is a topological index that measures the complexity of molecules based on the number and types of bonds and atoms. Atom count shows the number of atoms in a molecule, and it represents the size of a molecule. Molecular weight is the sum of the atomic weights of the atoms in a molecule. And ring count shows</p>
<p>Table 5: The statistics of SMolInstruct. "Qry." and "Resp." are average lengths of queries and responses, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Task abbr.</th>
<th style="text-align: center;">#Train</th>
<th style="text-align: center;">#Valid</th>
<th style="text-align: center;">#Test</th>
<th style="text-align: center;">#All</th>
<th style="text-align: center;">Qry.</th>
<th style="text-align: center;">Resp.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Name Conversion. Data Source: PubChem</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">IUPAC to Molecular Formula</td>
<td style="text-align: center;">NC-I2F</td>
<td style="text-align: center;">300,000</td>
<td style="text-align: center;">1,497</td>
<td style="text-align: center;">2,993</td>
<td style="text-align: center;">304,490</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">IUPAC to SMILES</td>
<td style="text-align: center;">NC-I2S</td>
<td style="text-align: center;">299,890</td>
<td style="text-align: center;">1,496</td>
<td style="text-align: center;">2,993</td>
<td style="text-align: center;">304,379</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">59</td>
</tr>
<tr>
<td style="text-align: center;">SMILES to Molecular Formula</td>
<td style="text-align: center;">NC-S2F</td>
<td style="text-align: center;">299,890</td>
<td style="text-align: center;">1,496</td>
<td style="text-align: center;">2,993</td>
<td style="text-align: center;">304,379</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">26</td>
</tr>
<tr>
<td style="text-align: center;">SMILES to IUPAC</td>
<td style="text-align: center;">NC-S2I</td>
<td style="text-align: center;">299,890</td>
<td style="text-align: center;">1,496</td>
<td style="text-align: center;">2,993</td>
<td style="text-align: center;">304,379</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: center;">Property Prediction. Data Source: MoleculeNet</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ESOL</td>
<td style="text-align: center;">PP-ESOL</td>
<td style="text-align: center;">888</td>
<td style="text-align: center;">111</td>
<td style="text-align: center;">112</td>
<td style="text-align: center;">1,111</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: center;">Lipo</td>
<td style="text-align: center;">PP-Lipo</td>
<td style="text-align: center;">3,360</td>
<td style="text-align: center;">420</td>
<td style="text-align: center;">420</td>
<td style="text-align: center;">4,200</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">BBBP</td>
<td style="text-align: center;">PP-BBBP</td>
<td style="text-align: center;">1,569</td>
<td style="text-align: center;">196</td>
<td style="text-align: center;">197</td>
<td style="text-align: center;">1,962</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">ClinTox</td>
<td style="text-align: center;">PP-ClinTox</td>
<td style="text-align: center;">1,144</td>
<td style="text-align: center;">143</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">1,431</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">HIV</td>
<td style="text-align: center;">PP-HIV</td>
<td style="text-align: center;">32,864</td>
<td style="text-align: center;">4,104</td>
<td style="text-align: center;">4,107</td>
<td style="text-align: center;">41,075</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">SIDER</td>
<td style="text-align: center;">PP-SIDER</td>
<td style="text-align: center;">22,820</td>
<td style="text-align: center;">2,860</td>
<td style="text-align: center;">2,860</td>
<td style="text-align: center;">28,540</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">Molecule Description. Data Source: Mol-Instructions, ChEBI-20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Molecule Captioning</td>
<td style="text-align: center;">MC</td>
<td style="text-align: center;">56,498</td>
<td style="text-align: center;">1,269</td>
<td style="text-align: center;">2,538</td>
<td style="text-align: center;">60,305</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">102</td>
</tr>
<tr>
<td style="text-align: center;">Molecule Generation</td>
<td style="text-align: center;">MG</td>
<td style="text-align: center;">56,498</td>
<td style="text-align: center;">1,269</td>
<td style="text-align: center;">2,493</td>
<td style="text-align: center;">60,260</td>
<td style="text-align: center;">117</td>
<td style="text-align: center;">75</td>
</tr>
<tr>
<td style="text-align: center;">Chemical Reaction. Data Source: USPTO-full</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Forward Synthesis</td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;">971,809</td>
<td style="text-align: center;">2,049</td>
<td style="text-align: center;">4,062</td>
<td style="text-align: center;">977,920</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">52</td>
</tr>
<tr>
<td style="text-align: center;">Retrosynthesis</td>
<td style="text-align: center;">RS</td>
<td style="text-align: center;">941,735</td>
<td style="text-align: center;">2,092</td>
<td style="text-align: center;">4,156</td>
<td style="text-align: center;">947,983</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3,288,855</td>
<td style="text-align: center;">20,498</td>
<td style="text-align: center;">33,061</td>
<td style="text-align: center;">3,342,414</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">55</td>
</tr>
</tbody>
</table>
<p>the number of rings in the molecular structures. As we can see, the values varies much, showing a extensive coverage in terms of complexity, size, and structure. Notably, when compared to Mol-Instructions (Fang et al., 2023), molecules in SMolInstruct show a higher complexity and diversity, which indicates that SMolInstruct is more comprehensive and complicated than Mol-Instructions. The scale, complexity, and diversity of SMolInstruct makes it well-suited for learning chemistry LLMs.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The statistics of molecules in SMolInstruct, with the long tail parts removed for a clear presentation.</p>
<h1>B. 2 Details of Dataset Construction</h1>
<p>Dataset construction involves four key steps (Section 3.2): data collection, quality control, data splitting, and instruction creation. This section provides task-specific details, omitting the common steps of canonicalizing SMILES/SELFIES and verbalizing information into query and response sentences, which have been introduced in Section 3.2.
Name Conversion (NC). The raw data for name conversion is collected from PubChem (Kim et al., 2019). Approximately 300k molecule/compound entries are randomly selected from the database, and their SMILES, IUPAC names, and molecular formulas are extracted. Entries with incomplete or missing information in these three domains are discarded. Finally, the SMILES, IUPAC names, molecular formulas are paired to create samples for the four name conversion tasks.
Property Prediction (PP). The raw data for property prediction is sourced from MoleculeNet (Wu et al., 2018). Out of its 16 core datasets ${ }^{5}$, we select 6 that are only related to small</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>molecules and are useful especially in drug discovery. Answers for regression tasks (e.g., ESOL and Lipo) are formulated as strings of numbers, and answers for binary classification tasks (e.g., BBBP and SIDER) are formulated as "Yes" or "No".
Molecule Captioning (MC) and Molecule Generation (MG). The raw data is collected from ChEBI-20 (Edwards et al., 2021; 2022) and Mol-Instructions (Fang et al., 2023). Despite the large number of samples in Mol-Instructions, many are found to be of low quality. For example, numerous molecular descriptions end with the ambiguous phrase "with data available", while others are overly general, making it difficult to generate a specific molecule based on the description. To ensure data quality, regular expressions and heuristic rules are employed to filter out low-quality samples.
Forward Synthesis (FS). USPTO-full (Lowe, 2017), one of the most comprehensive chemical reaction datasets, serve as the data source. The following processing steps are performed to clean the data: (1) Reactants and reagents are combined as input, and the product(s) serve as output, consistent with other datasets such as Mol-Instructions (Fang et al., 2023). (2) Duplicate chemicals in both input and output are removed to avoid redundancy. (3) If a chemical appears in both input and output, it is removed from the output to maintain data integrity. (4) Products in the output containing fewer than 5 molecules are considered non-main products and excluded. (5) If the above steps result in an empty output, the entire sample is discarded.
Retrosynthesis (RS). The data is also sourced from USPTO-full (Lowe, 2017), with the product as input and the reactants (excluding reagents) as output. During data exploration, we observe instances where reactants are mislabeled as reagents and vice versa. To address this issue, we compare the atom mapping numbers of the reactants and reagents with the products and relabel them accordingly. Subsequently, we apply the following processing steps: (1) Duplicate chemicals in both input and output are removed. (2) If a chemical appears in both input and output, it is removed from the input. (3) Products in the input containing fewer than 5 molecules are excluded. (4) In cases where multiple products exist in the input, the reaction is split into multiple samples, with each product serving as the input once. (5) If the above steps result in an empty input, the entire sample is discarded.
For all the tasks, samples containing invalid SMILES strings (i.e., those that cannot be parsed into a valid molecule with RDKit(RDKit, 2023)) are discarded, and duplicate samples are removed to avoid redundancy. Finally, since some molecules contain multiple components and they are separated by dots in SMILES, which is the same delimiter used to separate different reactants/reagents/products in FS and RS, the dots in SMILES strings for NC, PP, MC, and MG are replaced with semicolons to differentiate between these two usages.</p>
<h1>C Comparison with Mol-Instructions</h1>
<p>In this section, we present a comprehensive comparison between our work and MolInstructions (Fang et al., 2023).
We begin by comparing our dataset, SMolInstruct, with the Mol-Instructions dataset. While Mol-Instructions covers a broader scope (including molecule-oriented, protein-oriented, and biomolecular text instructions), SMolInstruct focuses exclusively on small molecules, providing a deeper and more comprehensive exploration of this domain.
If focusing on the molecule-related data, as shown in Table 6, SMolInstruct is a larger, more comprehensive, and higher-quality dataset. It incorporates more tasks, samples, and molecular representations, and involves more careful curation. Both datasets share the tasks of MC, MG, FS, and RS. Although SMolInstruct has fewer samples for MC and MG, the included samples are of higher quality (see Appendix B.2). Furthermore, SMolInstruct contains substantially more samples for FS and RS, which have been carefully cleaned and processed. Additionally, SMolInstruct incorporates four NC tasks to facilitate the understanding of various molecular representations. Unlike Mol-Instructions, we do not include the reagent prediction task mainly due to the lack of sufficient high-quality data (Andronov et al., 2023) and the limited practicality of this task in real world applications.</p>
<p>Table 6: Comparison between Mol-Instructions (the molecule-oriented part) (Fang et al., 2023) and our SMolInstruct.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mol-Instructions</th>
<th style="text-align: center;">SMolInstruct (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">name conversion property prediction</td>
<td style="text-align: center;">1.2 M samples <br> 78.3 k samples on 6 useful properties. <br> ergy. <br> 298.3 k samples. <br> 298.3 k samples. <br> 298.3 k samples. <br> 60.3 k samples.</td>
</tr>
<tr>
<td style="text-align: center;">Tasks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { molecule } \quad \text { genera- } \ &amp; \text { tion } \end{aligned}$</td>
<td style="text-align: center;">60.3k samples.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">forward synthesis</td>
<td style="text-align: center;">60.3k samples.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { retrosynthesis } \ &amp; \text { reagent prediction } \end{aligned}$</td>
<td style="text-align: center;">977.9k samples. <br> 948.0k samples. <br> Not included due to its insufficient data and limited practicality.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.2 M</td>
<td style="text-align: center;">3.3 M</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.3 M</td>
<td style="text-align: center;">3.3 M</td>
</tr>
<tr>
<td style="text-align: center;">#Samples</td>
<td style="text-align: center;">SELFIES.</td>
<td style="text-align: center;">Supports SMILES (default) and SELFIES, also involves IUPAC names and molecular formula in the NC tasks.</td>
</tr>
<tr>
<td style="text-align: center;">Molecular representations</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Carefully split into train/validation/test set, removing potential data leakage (see Section 3.2)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes, all the SMILES/SELFIES representations are canonicalized, providing a standardized data format.</td>
</tr>
<tr>
<td style="text-align: center;">Data splitting</td>
<td style="text-align: center;">Provides test set, while train/validation sets are not explicitly split.</td>
<td style="text-align: center;">Higher (see Appendix B.1).</td>
</tr>
</tbody>
</table>
<p>Beyond the dataset, our work makes contributions to the exploration of chemistry LLMs. While Fang et al. (2023) primarily focus on the dataset itself and provide a preliminary exploration of the models, we conduct comprehensive experiments to investigate the abilities of LLMs in the chemistry domain. Our experiments in Section 4 demonstrates that our LlaSMol models achieves superior performance compared to the LLMs trained on MolInstructions and the strongest LLMs such as GPT-4 and Claude 3 Opus, greatly diminishing the gap between LLMs and SoTA task-specific models. Moreover, we provides valuable insights about multi-task training, LoRA (Hu et al., 2022) settings, and other aspects that could be helpful for future research in this field.</p>
<h1>D Details of Experimental Setup</h1>
<p>In this section, we introduce the details of our experimental setups, including the training and inference details of our LlaSMol models and the compared models. We also give detailed explanations of the metrics used in Section 4.3, as well extra metrics that we will use in Appendix E.</p>
<h1>D. 1 LlaSMol Models</h1>
<p>The base models used for developing LlaSMol are Galactica ${ }^{6}$ (Taylor et al., 2022), Llama $2^{7}$ (Touvron et al., 2023b), Code Llama ${ }^{8}$ (Roziere et al., 2023) and Mistral ${ }^{9}$ (Jiang et al., 2023). We conduct instruction tuning on our SMolInstruct, and the resulting models are called named as LlaSMol ${ }<em 2="2" _Llama="{Llama" _text="\text">{\text {Galactica }}$, LlaSMol $</em>$, LlaSMol $}<em _Mistral="{Mistral" _text="\text">{\text {Code Llama }}$, and LlaSMol $</em>$, respectively. Expect for being based on different base models, their training and evaluation configurations are identical, as described as follows.
We used LoRA (Hu et al., 2022) during training, which is applied to all linear layers in the self-attention and FFN modules with lora.r and lora_alpha set to 16 . With the 8 -bit AdamW optimizer, a learning rate of $1 \mathrm{e}-4$, and a cosine scheduler, we train each model for three epochs. The input length is set to 512 , and sequences longer than 512 are truncated.
During inference, we adopt beam search as the generation strategy for simplicity. Due to the need of evaluations on the top- $k$ predicted answers (as in Appendix E, where $k$ varies for different tasks, we generate different numbers of sequences for different tasks by setting the num_return_sequences argument in the Huggingface Transformers library (Wolf et al., 2020). Specifically, it is set to 5 for NC-I2S, NC-S2I, FS, and MG; 3 for NC-I2F and NC-S2F; 1 for all the PP tasks; and 10 for RS. The beam size is set to num_return_sequences +3 for all the tasks. The maximum number of new generated tokens is set to 1024 .}</p>
<h2>D. 2 Compared LLMs</h2>
<p>We introduce each of the compared LLMs in details, including their training (if applicable) and inference process.</p>
<h2>D.2.1 GPT-4</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">General <br> Template</th>
<th style="text-align: center;">You are an expert chemist. Given the SMILES representation of reactants and reagents, your task is to predict the potential product using your chemical reaction knowledge.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task-Specific <br> Template</td>
<td style="text-align: center;">The input contains both reactants and reagents, and different reactants and reagents are separated by ' $\$$. Your reply should contain only the SMILES representation of the predicted product and no other text. Your reply must be valid and chemically reasonable.</td>
</tr>
<tr>
<td style="text-align: center;">ICL</td>
<td style="text-align: center;">Reactants and reagents SMILES: C1CCOC1.CCN(CC)CC.CS( $=0)(=0) \mathrm{Cl} . \mathrm{CS}(\mathrm{C})=0$. <br> N[C@@H]1CC2=CC=C(CN3C=C(CO)C(C(F)(F)F)=N3)C=C2C1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Product SMILES: CS( $=0)(=0) \mathrm{N}[\mathrm{C@@H}] 1 \mathrm{CC} 2=\mathrm{CC}=\mathrm{C}(\mathrm{CN} 3 \mathrm{C}=\mathrm{C}(\mathrm{CO}) \mathrm{C}(\mathrm{C}(\mathrm{F})(\mathrm{F}) \mathrm{F})=\mathrm{N} 3) \mathrm{C}=$ C2C1</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Reactants and reagents SMILES: CCN.CN1C=CC=C1C=O <br> Product SMILES:</td>
</tr>
</tbody>
</table>
<p>Figure 3: An example of query template for GPT-4.
GPT-4 (OpenAI, 2023) is one of the SoTA LLMs. We use the model versioned as gpt-4-0613 and evaluate it on 500 samples from SMolInstruct test set via OpenAI's API. Since GPT-4 is not fine-tuned on our dataset and thus is not familiar with the flexible queries, to ensure it generates answers in an expected format, we follow the prompt format proposed in (Guo et al., 2023) and create a query template for each of the tasks. The template for FS is shown in Figure 3. It contains 4 parts: (1) General template describes the task in a general way. (2) Task-specific template describes the detailed content requirements and format</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>requirements for the specific task. (3) ICL contains the in-context learning examples. It provides examples in the format of <input.title>: <input.content>\n <output.title>: <output.content>\n, where <input.title> and <output.title> serve as straightforward prompts to the input and output content. This design make the queried task more clear. (4) Question has the same format as ICL, with <output.content> being empty for the model to generate.
We conduct both $s$-shot evaluations, where $s=0,1,3,5$ is the number of provided ICL examples. For 0 -shot evaluation, the ICL part in the template is removed from the queries. In $k$-shot evaluation, for each sample,the ICL examples are randomly selected from the training set. The results of these settings are shown in Appendix E, which reals that these settings' performance is not consistent across all the tasks. Since 0 -shot shows the best performance on most tasks, we report its results in Section 4.3.
In the evaluations, we use the default generation strategy set in the API. To generate the same number of results for each sample (as described in Appendix D.1), we set the argument n in the API, which controls the number of output sequences.
GPT-4 can always follow the formatted instructions introduced above, so we do not bother to extract the answers from its outputs, but directly use its outputs as the predicted answers.</p>
<h1>D.2.2 Claude 3 Opus</h1>
<p>Claude 3 Opus (Anthropic, 2024) is a newly proposed SoTA LLM to date. Similarly to GPT-4, we evaluate Claude 3 Opus on 500 samples from SMolInstruct test set via Anthropic's API, and the generation strategy is the default one. The used prompt format is identical to the one used for GPT-4 (Appendix D.2.1. For each sample, we generate one response. Since Claude 3 Opus can always follow the formatted instructions, we do not bother to extract the answers from its outputs, but directly use its outputs as the predicted answers.</p>
<h2>D.2.3 Galactica</h2>
<p>Galactica (Taylor et al., 2022) is a LLM without instruction tuning. To evaluate it on SMolInstruct, we follow the instructions in the paper (Taylor et al., 2022) and the repository ${ }^{10}$ to create the queries for each task. We use zero-shot setting, as its official instruction does not suggest using few-shot setting. The generation configuration is set identical to that of our LlaSMol models (Appendix D.1).
Galatica's outputs may contain extra text other than the expected answers. Therefore, with heuristic rules and regular expression matching, we implement a program to extract the answers from the outputs of the models. Since the extraction cannot possibly cover all the possible output formats, some answers might not be correctly extracted, which might lead to validities lower than the actual value.</p>
<h2>D.2.4 Llama 2, Code Llama, and Mistral</h2>
<p>For our base models (Llama 2, Code Llama, and mistral), since they are not trained on SMolInstruct and have not seen the diverse queries in the dataset, we use the same query templates as those used for GPT-4 (Appendix D.2.1). We use the one-shot setting for them, as it would improve models' abiltity to follow the instructions and generate answers in a more formated way. In addition, the generation configuration (including beam size, output sequence numbers, etc) is set identical to that of our LlaSMol models (Appendix D.1).
Although we try our best to make the output format as clear as possible in the queries, these three models still cannot follow the instructions and their outputs are in various formats. By heuristic rules and regular expression matching, we implement a program to extract the answers from the outputs of each of the models. Since the extraction cannot possibly cover all the possible output formats, some answers might not be correctly extracted, which might lead to validities lower than the actual value.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ https://github.com/paperswithcode/galai&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2237 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2237</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2237</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-279155331</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.03426v1.pdf" target="_blank">Adaptive Task Vectors for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> In-Context Learning (ICL) enables Large Language Models (LLMs) to perform tasks without parameter updates by conditioning on a few demonstrations provided in the prompt. Despite its success, ICL suffers from several limitations, including sensitivity to demonstration order, context length constraints, and computational inefficiency. To address these challenges, task vector-based approaches compress task information into a single vector. However, these methods typically construct task vectors from fixed sets of demonstrations and reuse them across input queries, without conditioning on the specific input. This limitation can lead models to struggle with effective adaptation when the input query is not well aligned with the underlying demonstrations, consequently degrading their generalization performance on unseen tasks. To overcome this limitation, we propose Adaptive Task Vectors (ATV), a simple and effective framework that dynamically generates task vectors conditioned on each input query. ATV employs a small language model to generate task vectors, which are then transformed to match the target LLM's architecture and applied to guide its output generation. In contrast to ICL and previous vector-based approaches, which rely on fixed demonstration sets and their corresponding vectors, ATV dynamically generates task vectors tailored to each specific input query and task. Consequently, ATV demonstrates strong performance and generalization capabilities, even for unseen tasks. Furthermore, we provide a theoretical analysis indicating that ATV is expressively equivalent to LoRA under equal rank budgets and more expressive than Prefix-Tuning, thereby offering formal support for its representational advantage.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2237.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2237.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ATV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Task Vectors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that dynamically generates a query-conditioned task vector via a small generator model and linearly expands it into layer-wise steering vectors that are additively injected into a frozen large language model's hidden states to steer outputs without changing model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adaptive Task Vectors (ATV)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-module system: (1) a small language model (generator) encodes the input and yields a compact vector, (2) a linear expansion f_theta maps that vector to L × d_l layer-wise vectors which are additively injected (scaled by λ) into the last-token hidden state of each transformer layer of a frozen LLM to provide input-conditioned task steering.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>generator: GPT-2 (137M) default; target LLMs: LLaMA3-8B, Mistral-7B (target frozen models)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Input-conditioned compact task vector generated by a small LM and linearly expanded into per-layer additive vectors (adaptive, per-input low-rank outer-product steering injected into hidden states).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Language tasks — 20 in-domain tasks across Knowledge, Reasoning, Mathematics, Safety, NLU; evaluated also on held-out unseen tasks (GLUE variants, BBQ-Religion, Deepmind, MMLU psych, BBH logical deduction).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Reported consistently highest average accuracy across baselines on in-domain and unseen benchmarks; e.g., Table 5 vs LoRA: ATV In-domain 62.1 ± 1.5, Unseen 63.4 ± 2.5, Avg 62.8 ± 2.0. Paper reports ATV achieves highest average accuracy across LLaMA3 and Mistral baselines on the 20-task suite.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Baselines (uniform/fixed vectors or static tuned parameters) perform worse: LoRA avg 54.0 ± 2.3 (Table 5), ELICIT and I2CL also below ATV on average (exact per-task numbers in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Token-efficient relative to prompt-based ICL because ATV avoids passing demonstrations as tokens; at inference cost incurs small extra compute for generator forward (e.g., GPT-2 137M) and a linear projection to L×d_l — no FLOPs or latency numbers reported, but authors emphasize lightweight modules and parameter efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>ICL/BM25: high token cost and context-length overhead; LoRA: parameter updates (adds low-rank matrices) with small parameter/memory overhead but modifies weights; ELICIT/I2CL: precomputed vectors avoid token costs but use retrieval. Exact FLOPs/inference-time comparisons not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Trained using 90 examples per task (matching ELICIT). Ablation on generator capacity shows small generator (GPT-2 137M) suffices with only marginal gains from larger GPT-2-XL, suggesting good parameter/data efficiency; no explicit 'examples-to-convergence' numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Shows stronger generalization to unseen tasks than baselines: e.g., Table 5 Unseen accuracy ATV 63.4% vs LoRA 52.0%; Table 2 and text report ATV achieves highest average accuracy on unseen tasks for both LLaMA3 and Mistral while using fewer tokens than prompt-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>t-SNE visualizations show ATV vectors cluster by query similarity (query-specific variation); layer-wise injection analysis shows larger ATV vector magnitudes concentrated in lower layers, offering interpretable layer-role patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated across 20 heterogeneous tasks; ATV improves or matches performance broadly across categories (particularly NLU and Reasoning); it underperforms retrieval-based BM25 in Mathematics where pattern-matching retrieval helps more.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Ablation: generator capacity study indicates lightweight generators (GPT-2 137M) achieve performance close to larger variants (GPT-2-XL), supporting ATV under resource constraints. No strict latency/memory limits reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Input-conditioned, query-specific task vectors (ATV) yield better average accuracy, stronger generalization to unseen tasks, and greater robustness than fixed task vectors or static parameter updates while remaining token-efficient; small generators are sufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Empirical and theoretical results in the paper support the Task-Aligned Abstraction Principle: dynamically allocating task-specific (per-input) representational resources improves generalization and adaptability compared with uniform/static representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2237.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2237.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ELICIT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ELICIT (LLM augmentation via external in-context capability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-based task-vector method that maintains a library of precomputed task vectors (capabilities) and retrieves and injects a selected fixed vector into a frozen LLM to steer behavior; vectors are not generated per input at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ELICIT: LLM augmentation via external in-context capability</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ELICIT (task-vector retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Maintains a precomputed library of task vectors created from demonstrations; at inference retrieves the closest vector by similarity and injects it (usually at a specific layer) into the LLM to steer outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Precomputed fixed task vectors retrieved by query similarity and additively injected into hidden states (uniform per retrieved capability; not generated per input).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Same multi-task language benchmarks used in paper (20 tasks); used as a main baseline for in-domain and unseen-task evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Lower average accuracy than ATV across reported evaluations; paper states ATV outperforms ELICIT across most domains. Exact aggregated numbers per ELICIT are reported in paper tables (baseline entries).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Token-efficient relative to prompt-based ICL because it avoids in-prompt demonstrations; requires precomputation and retrieval at inference. No explicit FLOPs or latency numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Task vectors constructed from 10 exemplars per task and a minimum of ≈90 unique samples per task used for training in comparisons — used as baseline training budget.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Per paper, ELICIT generalizes less well than ATV to unseen tasks; retrieved fixed vectors lack per-query adaptation which hurts generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>t-SNE visualization shows ELICIT's fixed/retrieved vectors exhibit limited intra-task diversity compared to ATV, indicating less input-sensitive representations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Per-task injection location matters: ELICIT performs best when injected into top layers for some tasks and shows degradation when naively injected across all layers, requiring selection of an optimal injection layer.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Fixed, retrieved task vectors (ELICIT) are token-efficient but less adaptive: they require per-task injection-locational tuning and exhibit limited per-query representational diversity, leading to weaker generalization than ATV.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>ELICIT supports the idea that compact task vectors can be efficient, but its fixed nature highlights limitations of non-dynamic representations for per-input adaptation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2237.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2237.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>I2CL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit In-Context Learning (I2CL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that compresses few-shot demonstration examples into a single context vector and injects it into a model's residual stream to emulate in-context learning without token-level demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Implicit in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>I2CL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Compresses demonstration examples into a single dense context vector per capability and injects it (with learned coefficients) into the frozen LLM's internal activations to steer outputs, reducing token usage.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Fixed compressed context vectors derived from demonstrations and injected into the model (library-style vectors rather than per-input dynamically generated vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used as a baseline on the same multi-task language benchmarks in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Included as a baseline; in tables I2CL generally underperforms ATV and is reported with its own scores in the experiments (specific per-task numbers in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Improves token efficiency by removing token-level demonstrations; training/inference use shared coefficients across datasets in this paper (authors changed original per-dataset calibration). Exact FLOPs/time not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Paper trains shared coefficients and context vectors using 90 examples per task; no explicit sample-efficiency curves shown.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>I2CL generalizes less effectively than ATV according to reported tables.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Performed as baseline across tasks; less adaptive than ATV due to fixed context vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Compression of demonstrations into fixed context vectors reduces token cost but lacks input-conditioned adaptability, leading to weaker generalization than ATV.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Demonstrates benefits of compact task representations for efficiency, but their fixed nature limits per-input adaptation, partially challenging purely uniform representation approaches.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2237.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2237.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LoRA (Low-Rank Adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-efficient fine-tuning method that injects trainable low-rank matrices into pretrained projection weights to adapt model behavior with small parameter budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lora: Low-rank adaptation of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applies low-rank (W_down, W_up) adapters inside projection weights of attention/MLP layers to effect a rank-r additive change to the model's weight matrices; adapters are static once trained (not input-conditioned).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Static low-rank parameter adapters applied to weights (uniform across inputs at inference time once trained).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used as a baseline comparison for in-domain and unseen language tasks in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Reported in Table 5: LoRA In-domain 56.0 ± 1.4, Unseen 52.0 ± 3.2, Avg 54.0 ± 2.3 (on LLaMA3 comparisons in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Parameter-efficient training with small additional parameter count (paper's LoRA config: rank r=8, α=32); does require weight updates and storage of adapter parameters; inference cost slightly increased due to adapter matrix multiplications but numeric FLOPs not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Underperforms ATV on unseen tasks in this study (Table 5), suggesting static adapters can be less flexible for generalization to new tasks when compared to input-conditioned steering.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Compared across the multi-task suite; LoRA is expressive under matched rank budgets theoretically but lacks per-input adaptivity.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>LoRA and ATV are expressively equivalent under matched rank budgets, but ATV's input-dependent generation gives it empirical generalization advantages over LoRA on unseen tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Theoretical equivalence (matched rank) supports that low-rank updates can be expressive, but empirical results show per-input adaptive representations (ATV) can improve generalization compared to static LoRA adapters.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2237.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2237.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prefix-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prefix-Tuning (continuous prompt tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that prepends trainable continuous prefix vectors to the input sequence to condition model behavior without changing model weights; prefixes are static learned parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prefix-tuning: Optimizing continuous prompts for generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prefix-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a set of continuous prefix key/value vectors (P_k, P_v) that are concatenated to attention key and value sequences to influence attention; prefixes are fixed at inference and not conditioned per-input.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Static learned prefix vectors concatenated to the attention context (uniform across inputs once trained).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Referenced in theoretical analysis as a comparison to ATV (attention-level control methods).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Adds small extra parameters (prefix vectors) and modifies attention computation; specific costs not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>The paper proves theoretically (Theorem 2) that ATV's representational space strictly contains Prefix-Tuning's under a linear attention approximation, implying ATV can represent behaviors Prefix-Tuning cannot.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Prefix-Tuning provides a static attention steering mechanism, but is provably less expressive than ATV under the analyzed approximation because it cannot alter the query-side or mixed query-context interactions that ATV can.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>challenges</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Theoretical results show Prefix-Tuning's fixed prefixes are contained within ATV's representational space, indicating static prefix approaches are limited compared to dynamic per-input representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2237.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2237.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Learning (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A paradigm where a frozen LLM performs new tasks by conditioning on tokenized demonstration examples placed in the prompt, without updating model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>In-Context Learning (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Provides task examples as tokens in the input prompt so the model infers the mapping; adaptation happens through context tokens rather than parameter changes or compact vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Token-level in-prompt conditioning (uniform representation via prompt tokens; demonstrations are explicit tokens rather than compact per-input task vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General few-shot language tasks; used as baseline (16-shot ICL) throughout experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Baselines include 16-shot ICL; ATV outperforms 16-shot ICL on average across many domains in the study (exact per-task numbers in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Computationally inefficient at inference because demonstrations are tokenized (long prompts) causing high token throughput and context-window limitations; paper emphasizes ATV's token efficiency advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>ICL is sensitive to demonstration order/selection and suffers under context-length constraints; retrieval-based adaptations mitigate some issues but remain token-based.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>ICL performance varies widely across tasks and is sensitive to prompt design and demonstration ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>ICL provides per-input adaptation via demonstrations but is token-inefficient and unstable; compact adaptive task vectors (ATV) preserve per-input adaptation with much lower token cost.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>ICL's dynamic, per-query behavior supports task-aligned adaptation in principle, but its token inefficiency and instability highlight limits of naive token-based dynamic representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2237.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2237.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25-ICL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25 retrieval-based In-Context Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval approach that selects demonstration examples with BM25 similarity and places them in the prompt for ICL; effectively a retrieval-conditioned token-based adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The probabilistic relevance framework: Bm25 and beyond</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BM25 retrieval-based ICL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Retrieves 16-shot demonstration examples using BM25 query similarity and uses them in-context as a prompt to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Retrieval of token-level demonstrations from a pool and inclusion in prompt (token-based, not compact vector), so adaptation is input-conditioned but expensive in tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluated as a baseline especially effective on Math tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>BM25 achieved best performance in the Math category per authors' analysis (pattern-based retrieval advantage); exact per-task numbers in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>High token cost (long prompts) and context-length constraints; stronger on pattern-match tasks but less token-efficient than vector-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Retrieval helps when close pattern matches exist (e.g., math problems); less effective for semantic-level generalization than ATV.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Performs strongly on tasks with pattern-based similarities (math), less strongly on semantic/generalization-reliant tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Retrieval-based token-level adaptation (BM25) can excel for pattern-matched tasks (math) but is token-inefficient and less robust for semantic generalization compared to adaptive compact vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Shows that dynamic retrieval of demonstrations (input-conditioned) can be highly effective per task but highlights trade-offs between token cost and general semantic generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2237.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2237.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 Generator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (used as ATV small generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small language model used as the ATV generator to produce compact, input-conditioned vectors from the last-token hidden state; default generator is GPT-2 (137M) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (generator for ATV)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Lightweight autoregressive transformer used to encode the input query and produce a compact vector v_small (last-token hidden state) that is linearly expanded into per-layer ATVs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>137M (default); also ablated with GPT-2 Medium (380M), Large (812M), XL (1.6B)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Produces input-conditioned compact task vectors (serves as the adaptive component allocating representation per input).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Generator for ATV across same language task suite.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Ablation (Table 3): GPT-2 (137M) yields competitive performance; GPT-2-XL shows marginal gains. Authors conclude lightweight generator suffices for effective ATV generation (no large gains from much larger generators).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Generator adds inference cost proportional to its size; default 137M is lightweight and practical. No explicit latency/FLOPs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Using a small generator still enables ATV to generalize to unseen tasks, per study results.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Generator variants tested across multi-task suite; performance improves slightly with larger generators but returns are marginal.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Shows ATV can be deployed with small (137M) generator for near state performance, favoring resource-constrained settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>A small generator (GPT-2 137M) is sufficient to produce effective query-specific task vectors for ATV; larger generators offer only marginal gains.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Supports the practical viability of lightweight adaptive components for task-aligned abstraction under resource constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ELICIT: LLM augmentation via external in-context capability <em>(Rating: 2)</em></li>
                <li>Implicit in-context learning <em>(Rating: 2)</em></li>
                <li>Lora: Low-rank adaptation of large language models <em>(Rating: 2)</em></li>
                <li>Prefix-tuning: Optimizing continuous prompts for generation <em>(Rating: 2)</em></li>
                <li>The probabilistic relevance framework: Bm25 and beyond <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2237",
    "paper_id": "paper-279155331",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "ATV",
            "name_full": "Adaptive Task Vectors",
            "brief_description": "A framework that dynamically generates a query-conditioned task vector via a small generator model and linearly expands it into layer-wise steering vectors that are additively injected into a frozen large language model's hidden states to steer outputs without changing model weights.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Adaptive Task Vectors (ATV)",
            "model_description": "Two-module system: (1) a small language model (generator) encodes the input and yields a compact vector, (2) a linear expansion f_theta maps that vector to L × d_l layer-wise vectors which are additively injected (scaled by λ) into the last-token hidden state of each transformer layer of a frozen LLM to provide input-conditioned task steering.",
            "model_size": "generator: GPT-2 (137M) default; target LLMs: LLaMA3-8B, Mistral-7B (target frozen models)",
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Input-conditioned compact task vector generated by a small LM and linearly expanded into per-layer additive vectors (adaptive, per-input low-rank outer-product steering injected into hidden states).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Language tasks — 20 in-domain tasks across Knowledge, Reasoning, Mathematics, Safety, NLU; evaluated also on held-out unseen tasks (GLUE variants, BBQ-Religion, Deepmind, MMLU psych, BBH logical deduction).",
            "performance_task_aligned": "Reported consistently highest average accuracy across baselines on in-domain and unseen benchmarks; e.g., Table 5 vs LoRA: ATV In-domain 62.1 ± 1.5, Unseen 63.4 ± 2.5, Avg 62.8 ± 2.0. Paper reports ATV achieves highest average accuracy across LLaMA3 and Mistral baselines on the 20-task suite.",
            "performance_uniform_baseline": "Baselines (uniform/fixed vectors or static tuned parameters) perform worse: LoRA avg 54.0 ± 2.3 (Table 5), ELICIT and I2CL also below ATV on average (exact per-task numbers in paper).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Token-efficient relative to prompt-based ICL because ATV avoids passing demonstrations as tokens; at inference cost incurs small extra compute for generator forward (e.g., GPT-2 137M) and a linear projection to L×d_l — no FLOPs or latency numbers reported, but authors emphasize lightweight modules and parameter efficiency.",
            "computational_efficiency_baseline": "ICL/BM25: high token cost and context-length overhead; LoRA: parameter updates (adds low-rank matrices) with small parameter/memory overhead but modifies weights; ELICIT/I2CL: precomputed vectors avoid token costs but use retrieval. Exact FLOPs/inference-time comparisons not reported.",
            "sample_efficiency_results": "Trained using 90 examples per task (matching ELICIT). Ablation on generator capacity shows small generator (GPT-2 137M) suffices with only marginal gains from larger GPT-2-XL, suggesting good parameter/data efficiency; no explicit 'examples-to-convergence' numbers.",
            "transfer_generalization_results": "Shows stronger generalization to unseen tasks than baselines: e.g., Table 5 Unseen accuracy ATV 63.4% vs LoRA 52.0%; Table 2 and text report ATV achieves highest average accuracy on unseen tasks for both LLaMA3 and Mistral while using fewer tokens than prompt-based methods.",
            "interpretability_results": "t-SNE visualizations show ATV vectors cluster by query similarity (query-specific variation); layer-wise injection analysis shows larger ATV vector magnitudes concentrated in lower layers, offering interpretable layer-role patterns.",
            "multi_task_performance": "Evaluated across 20 heterogeneous tasks; ATV improves or matches performance broadly across categories (particularly NLU and Reasoning); it underperforms retrieval-based BM25 in Mathematics where pattern-matching retrieval helps more.",
            "resource_constrained_results": "Ablation: generator capacity study indicates lightweight generators (GPT-2 137M) achieve performance close to larger variants (GPT-2-XL), supporting ATV under resource constraints. No strict latency/memory limits reported.",
            "key_finding_summary": "Input-conditioned, query-specific task vectors (ATV) yield better average accuracy, stronger generalization to unseen tasks, and greater robustness than fixed task vectors or static parameter updates while remaining token-efficient; small generators are sufficient.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Empirical and theoretical results in the paper support the Task-Aligned Abstraction Principle: dynamically allocating task-specific (per-input) representational resources improves generalization and adaptability compared with uniform/static representations.",
            "uuid": "e2237.0"
        },
        {
            "name_short": "ELICIT",
            "name_full": "ELICIT (LLM augmentation via external in-context capability)",
            "brief_description": "A retrieval-based task-vector method that maintains a library of precomputed task vectors (capabilities) and retrieves and injects a selected fixed vector into a frozen LLM to steer behavior; vectors are not generated per input at inference time.",
            "citation_title": "ELICIT: LLM augmentation via external in-context capability",
            "mention_or_use": "mention",
            "model_name": "ELICIT (task-vector retrieval)",
            "model_description": "Maintains a precomputed library of task vectors created from demonstrations; at inference retrieves the closest vector by similarity and injects it (usually at a specific layer) into the LLM to steer outputs.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Precomputed fixed task vectors retrieved by query similarity and additively injected into hidden states (uniform per retrieved capability; not generated per input).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Same multi-task language benchmarks used in paper (20 tasks); used as a main baseline for in-domain and unseen-task evaluation.",
            "performance_task_aligned": "Lower average accuracy than ATV across reported evaluations; paper states ATV outperforms ELICIT across most domains. Exact aggregated numbers per ELICIT are reported in paper tables (baseline entries).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Token-efficient relative to prompt-based ICL because it avoids in-prompt demonstrations; requires precomputation and retrieval at inference. No explicit FLOPs or latency numbers provided.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Task vectors constructed from 10 exemplars per task and a minimum of ≈90 unique samples per task used for training in comparisons — used as baseline training budget.",
            "transfer_generalization_results": "Per paper, ELICIT generalizes less well than ATV to unseen tasks; retrieved fixed vectors lack per-query adaptation which hurts generalization.",
            "interpretability_results": "t-SNE visualization shows ELICIT's fixed/retrieved vectors exhibit limited intra-task diversity compared to ATV, indicating less input-sensitive representations.",
            "multi_task_performance": "Per-task injection location matters: ELICIT performs best when injected into top layers for some tasks and shows degradation when naively injected across all layers, requiring selection of an optimal injection layer.",
            "resource_constrained_results": null,
            "key_finding_summary": "Fixed, retrieved task vectors (ELICIT) are token-efficient but less adaptive: they require per-task injection-locational tuning and exhibit limited per-query representational diversity, leading to weaker generalization than ATV.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "ELICIT supports the idea that compact task vectors can be efficient, but its fixed nature highlights limitations of non-dynamic representations for per-input adaptation.",
            "uuid": "e2237.1"
        },
        {
            "name_short": "I2CL",
            "name_full": "Implicit In-Context Learning (I2CL)",
            "brief_description": "A method that compresses few-shot demonstration examples into a single context vector and injects it into a model's residual stream to emulate in-context learning without token-level demonstrations.",
            "citation_title": "Implicit in-context learning",
            "mention_or_use": "mention",
            "model_name": "I2CL",
            "model_description": "Compresses demonstration examples into a single dense context vector per capability and injects it (with learned coefficients) into the frozen LLM's internal activations to steer outputs, reducing token usage.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Fixed compressed context vectors derived from demonstrations and injected into the model (library-style vectors rather than per-input dynamically generated vectors).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Used as a baseline on the same multi-task language benchmarks in the paper.",
            "performance_task_aligned": "Included as a baseline; in tables I2CL generally underperforms ATV and is reported with its own scores in the experiments (specific per-task numbers in paper).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Improves token efficiency by removing token-level demonstrations; training/inference use shared coefficients across datasets in this paper (authors changed original per-dataset calibration). Exact FLOPs/time not reported.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Paper trains shared coefficients and context vectors using 90 examples per task; no explicit sample-efficiency curves shown.",
            "transfer_generalization_results": "I2CL generalizes less effectively than ATV according to reported tables.",
            "interpretability_results": null,
            "multi_task_performance": "Performed as baseline across tasks; less adaptive than ATV due to fixed context vectors.",
            "resource_constrained_results": null,
            "key_finding_summary": "Compression of demonstrations into fixed context vectors reduces token cost but lacks input-conditioned adaptability, leading to weaker generalization than ATV.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "Demonstrates benefits of compact task representations for efficiency, but their fixed nature limits per-input adaptation, partially challenging purely uniform representation approaches.",
            "uuid": "e2237.2"
        },
        {
            "name_short": "LoRA",
            "name_full": "LoRA (Low-Rank Adaptation)",
            "brief_description": "A parameter-efficient fine-tuning method that injects trainable low-rank matrices into pretrained projection weights to adapt model behavior with small parameter budgets.",
            "citation_title": "Lora: Low-rank adaptation of large language models",
            "mention_or_use": "mention",
            "model_name": "LoRA",
            "model_description": "Applies low-rank (W_down, W_up) adapters inside projection weights of attention/MLP layers to effect a rank-r additive change to the model's weight matrices; adapters are static once trained (not input-conditioned).",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Static low-rank parameter adapters applied to weights (uniform across inputs at inference time once trained).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Used as a baseline comparison for in-domain and unseen language tasks in the paper.",
            "performance_task_aligned": "Reported in Table 5: LoRA In-domain 56.0 ± 1.4, Unseen 52.0 ± 3.2, Avg 54.0 ± 2.3 (on LLaMA3 comparisons in this paper).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Parameter-efficient training with small additional parameter count (paper's LoRA config: rank r=8, α=32); does require weight updates and storage of adapter parameters; inference cost slightly increased due to adapter matrix multiplications but numeric FLOPs not reported here.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Underperforms ATV on unseen tasks in this study (Table 5), suggesting static adapters can be less flexible for generalization to new tasks when compared to input-conditioned steering.",
            "interpretability_results": null,
            "multi_task_performance": "Compared across the multi-task suite; LoRA is expressive under matched rank budgets theoretically but lacks per-input adaptivity.",
            "resource_constrained_results": null,
            "key_finding_summary": "LoRA and ATV are expressively equivalent under matched rank budgets, but ATV's input-dependent generation gives it empirical generalization advantages over LoRA on unseen tasks.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "Theoretical equivalence (matched rank) supports that low-rank updates can be expressive, but empirical results show per-input adaptive representations (ATV) can improve generalization compared to static LoRA adapters.",
            "uuid": "e2237.3"
        },
        {
            "name_short": "Prefix-Tuning",
            "name_full": "Prefix-Tuning (continuous prompt tuning)",
            "brief_description": "A method that prepends trainable continuous prefix vectors to the input sequence to condition model behavior without changing model weights; prefixes are static learned parameters.",
            "citation_title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "mention_or_use": "mention",
            "model_name": "Prefix-Tuning",
            "model_description": "Learns a set of continuous prefix key/value vectors (P_k, P_v) that are concatenated to attention key and value sequences to influence attention; prefixes are fixed at inference and not conditioned per-input.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Static learned prefix vectors concatenated to the attention context (uniform across inputs once trained).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Referenced in theoretical analysis as a comparison to ATV (attention-level control methods).",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Adds small extra parameters (prefix vectors) and modifies attention computation; specific costs not reported in this paper.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "The paper proves theoretically (Theorem 2) that ATV's representational space strictly contains Prefix-Tuning's under a linear attention approximation, implying ATV can represent behaviors Prefix-Tuning cannot.",
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Prefix-Tuning provides a static attention steering mechanism, but is provably less expressive than ATV under the analyzed approximation because it cannot alter the query-side or mixed query-context interactions that ATV can.",
            "supports_or_challenges_theory": "challenges",
            "supports_or_challenges_theory_explanation": "Theoretical results show Prefix-Tuning's fixed prefixes are contained within ATV's representational space, indicating static prefix approaches are limited compared to dynamic per-input representations.",
            "uuid": "e2237.4"
        },
        {
            "name_short": "ICL",
            "name_full": "In-Context Learning (ICL)",
            "brief_description": "A paradigm where a frozen LLM performs new tasks by conditioning on tokenized demonstration examples placed in the prompt, without updating model weights.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "In-Context Learning (ICL)",
            "model_description": "Provides task examples as tokens in the input prompt so the model infers the mapping; adaptation happens through context tokens rather than parameter changes or compact vectors.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Token-level in-prompt conditioning (uniform representation via prompt tokens; demonstrations are explicit tokens rather than compact per-input task vectors).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "General few-shot language tasks; used as baseline (16-shot ICL) throughout experiments.",
            "performance_task_aligned": "Baselines include 16-shot ICL; ATV outperforms 16-shot ICL on average across many domains in the study (exact per-task numbers in tables).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Computationally inefficient at inference because demonstrations are tokenized (long prompts) causing high token throughput and context-window limitations; paper emphasizes ATV's token efficiency advantage.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "ICL is sensitive to demonstration order/selection and suffers under context-length constraints; retrieval-based adaptations mitigate some issues but remain token-based.",
            "interpretability_results": null,
            "multi_task_performance": "ICL performance varies widely across tasks and is sensitive to prompt design and demonstration ordering.",
            "resource_constrained_results": null,
            "key_finding_summary": "ICL provides per-input adaptation via demonstrations but is token-inefficient and unstable; compact adaptive task vectors (ATV) preserve per-input adaptation with much lower token cost.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "ICL's dynamic, per-query behavior supports task-aligned adaptation in principle, but its token inefficiency and instability highlight limits of naive token-based dynamic representations.",
            "uuid": "e2237.5"
        },
        {
            "name_short": "BM25-ICL",
            "name_full": "BM25 retrieval-based In-Context Learning",
            "brief_description": "A retrieval approach that selects demonstration examples with BM25 similarity and places them in the prompt for ICL; effectively a retrieval-conditioned token-based adaptation.",
            "citation_title": "The probabilistic relevance framework: Bm25 and beyond",
            "mention_or_use": "mention",
            "model_name": "BM25 retrieval-based ICL",
            "model_description": "Retrieves 16-shot demonstration examples using BM25 query similarity and uses them in-context as a prompt to the LLM.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Retrieval of token-level demonstrations from a pool and inclusion in prompt (token-based, not compact vector), so adaptation is input-conditioned but expensive in tokens.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Evaluated as a baseline especially effective on Math tasks in this paper.",
            "performance_task_aligned": "BM25 achieved best performance in the Math category per authors' analysis (pattern-based retrieval advantage); exact per-task numbers in paper tables.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "High token cost (long prompts) and context-length constraints; stronger on pattern-match tasks but less token-efficient than vector-based methods.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Retrieval helps when close pattern matches exist (e.g., math problems); less effective for semantic-level generalization than ATV.",
            "interpretability_results": null,
            "multi_task_performance": "Performs strongly on tasks with pattern-based similarities (math), less strongly on semantic/generalization-reliant tasks.",
            "resource_constrained_results": null,
            "key_finding_summary": "Retrieval-based token-level adaptation (BM25) can excel for pattern-matched tasks (math) but is token-inefficient and less robust for semantic generalization compared to adaptive compact vectors.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "Shows that dynamic retrieval of demonstrations (input-conditioned) can be highly effective per task but highlights trade-offs between token cost and general semantic generalization.",
            "uuid": "e2237.6"
        },
        {
            "name_short": "GPT-2 Generator",
            "name_full": "GPT-2 (used as ATV small generator)",
            "brief_description": "A small language model used as the ATV generator to produce compact, input-conditioned vectors from the last-token hidden state; default generator is GPT-2 (137M) in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (generator for ATV)",
            "model_description": "Lightweight autoregressive transformer used to encode the input query and produce a compact vector v_small (last-token hidden state) that is linearly expanded into per-layer ATVs.",
            "model_size": "137M (default); also ablated with GPT-2 Medium (380M), Large (812M), XL (1.6B)",
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Produces input-conditioned compact task vectors (serves as the adaptive component allocating representation per input).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Generator for ATV across same language task suite.",
            "performance_task_aligned": "Ablation (Table 3): GPT-2 (137M) yields competitive performance; GPT-2-XL shows marginal gains. Authors conclude lightweight generator suffices for effective ATV generation (no large gains from much larger generators).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Generator adds inference cost proportional to its size; default 137M is lightweight and practical. No explicit latency/FLOPs reported.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Using a small generator still enables ATV to generalize to unseen tasks, per study results.",
            "interpretability_results": null,
            "multi_task_performance": "Generator variants tested across multi-task suite; performance improves slightly with larger generators but returns are marginal.",
            "resource_constrained_results": "Shows ATV can be deployed with small (137M) generator for near state performance, favoring resource-constrained settings.",
            "key_finding_summary": "A small generator (GPT-2 137M) is sufficient to produce effective query-specific task vectors for ATV; larger generators offer only marginal gains.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Supports the practical viability of lightweight adaptive components for task-aligned abstraction under resource constraints.",
            "uuid": "e2237.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ELICIT: LLM augmentation via external in-context capability",
            "rating": 2
        },
        {
            "paper_title": "Implicit in-context learning",
            "rating": 2
        },
        {
            "paper_title": "Lora: Low-rank adaptation of large language models",
            "rating": 2
        },
        {
            "paper_title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "rating": 2
        },
        {
            "paper_title": "The probabilistic relevance framework: Bm25 and beyond",
            "rating": 1
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        }
    ],
    "cost": 0.020043,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Adaptive Task Vectors for Large Language Models
3 Jun 2025</p>
<p>Joonseong Kang 
Yonsei University</p>
<p>Soojeong Lee 
Yonsei University</p>
<p>Subeen Park 
Yonsei University</p>
<p>Sumin Park 
Yonsei University</p>
<p>Taero Kim 
Yonsei University</p>
<p>Jihee Kim 
Yonsei University</p>
<p>Ryunyi Lee 
Yonsei University</p>
<p>Kyungwoo Song kyungwoo.song@yonsei.ac.kr 
Yonsei University</p>
<p>Adaptive Task Vectors for Large Language Models
3 Jun 2025A6437FF4BCC30CA6D42C660B8B8BF45FarXiv:2506.03426v1[cs.LG]
In-Context Learning (ICL) enables Large Language Models (LLMs) to perform tasks without parameter updates by conditioning on a few demonstrations provided in the prompt.Despite its success, ICL suffers from several limitations, including sensitivity to demonstration order, context length constraints, and computational inefficiency.To address these challenges, task vector-based approaches compress task information into a single vector.However, these methods typically construct task vectors from fixed sets of demonstrations and reuse them across input queries, without conditioning on the specific input.This limitation can lead models to struggle with effective adaptation when the input query is not well aligned with the underlying demonstrations, consequently degrading their generalization performance on unseen tasks.To overcome this limitation, we propose Adaptive Task Vectors (ATV), a simple and effective framework that dynamically generates task vectors conditioned on each input query.ATV employs a small language model to generate task vectors, which are then transformed to match the target LLM's architecture and applied to guide its output generation.In contrast to ICL and previous vector-based approaches, which rely on fixed demonstration sets and their corresponding vectors, ATV dynamically generates task vectors tailored to each specific input query and task.Consequently, ATV demonstrates strong performance and generalization capabilities, even for unseen tasks.Furthermore, we provide a theoretical analysis indicating that ATV is expressively equivalent to LoRA under equal rank budgets and more expressive than Prefix-Tuning, thereby offering formal support for its representational advantage.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have made remarkable progress in natural language processing, demonstrating impressive performance across various tasks.In-Context Learning (ICL) [1] has become a pivotal method for enhancing LLM performance, enabling models to effectively perform specific tasks by including demonstration samples in prompts without requiring additional training [2].However, ICL faces several limitations: performance varies considerably depending on the order and selection of demonstration samples [3][4][5], the maximum context length constraint of LLMs makes it challenging to handle tasks involving long-context reasoning or diverse demonstration sets, and processing numerous demonstration significantly reduces computational efficiency [6,7].</p>
<p>To mitigate these issues, task vector-based approaches [8][9][10][11][12] have attracted growing interest for improving the efficiency and robustness of ICL.Task vectors [8] are vector representations that compress task-specific information, typically obtained from the hidden state of the last token in the prompt, or its processed variant.These vectors are integrated with the input query to modulate the model's output in a task-specific manner.Recent studies have utilized task vectors to effectively Figure 1: Comparison between task vector methods: a) Prior work uses a fixed task vector for all inputs, whereas b) our method generates a query-specific task vector, enabling adaptive behavior for each input.This enables the LLM to adapt its behavior to individual inputs and overcome the limitations of fixed-vector approaches.mitigate the limitations of conventional ICL [13,14].By compressing information from multiple demonstration samples into a single vector, these methods overcome context window constraints and reduce performance variability due to demonstration ordering [2,[15][16][17].As a result, they preserve the effectiveness of ICL while improving computational efficiency and consistency [18].However, existing task vector-based approaches exhibit a significant limitation.Most prior methods construct task vectors from fixed sets of demonstration samples and reuse the same vector across all input queries, regardless of their individual characteristics [8,13].While some recent approaches retrieve task vectors based on query similarity, the retrieved vectors are precomputed and remain fixed during inference.As a result, these methods are not conditioned on the current input and may fail to adapt effectively when the input is not well aligned with the underlying demonstrations.Indeed, ICL and previous task vector-based methods, which select demonstration sets from such fixed pools, consequently tend to exhibit limited performance on unseen tasks.</p>
<p>Motivated by these limitations, we present Adaptive Task Vectors (ATV), a new framework for dynamically generating task vectors conditioned on each input query.ATV enables more accurate and input-sensitive model guidance by producing an optimal task vector for each input query.Our framework employs a small language model to generate intermediate task representations, which are then transformed to match the architecture of the target LLM and used to modulate its output.Figure 1 illustrates the key difference between (a) conventional fixed-vector methods [10][11][12], and (b) our adaptive approach.While (a) applies the same vector to all input queries, (b) generates a query-specific vector for each query, allowing the model to produce more appropriate responses tailored to the input.</p>
<p>In this paper, we establish the effectiveness of the proposed framework through both theoretical and empirical evaluation.Theoretically, we prove that ATV is expressively equivalent to LoRA under matched rank budgets and strictly more expressive than Prefix-Tuning, offering a formal basis for its enhanced representational capacity.Empirically, we evaluate ATV on in-domain performance, generalization to unseen tasks, and ablations on model capacity and injection configuration.Across these settings, ATV demonstrates superior in-domain accuracy, strong generalization ability, and interpretable insights into model capacity and injection behavior.</p>
<p>Our main contributions are as follows:</p>
<p>• We propose Adaptive Task Vectors (ATV), a simple and effective framework that generates task vectors conditioned on each input query, enabling LLMs to adapt their behavior in a task-aware manner based on the input.• We provide a theoretical analysis showing that ATV is expressively equivalent to LoRA under equal rank budgets and strictly more expressive than Prefix-Tuning, providing a formal justification for its enhanced representational capacity.• We empirically evaluate ATV on both in-domain tasks and generalization to unseen tasks, demonstrating strong performance across diverse datasets and model families.We further analyze how ATV's performance and behavior are influenced by key design factors through ablation studies on model capacity and injection configuration.</p>
<p>Related Work</p>
<p>In-Context Learning.ICL enables LLMs to perform new tasks without parameter updates by conditioning on a few input-output examples presented in the prompt [1].Since the introduction of GPT-3, ICL has demonstrated strong performance across diverse tasks, particularly when combined with prompt engineering and model scaling [19][20][21].Despite its effectiveness, ICL exhibits notable limitations.First, its performance is highly sensitive to the order and selection of demonstration samples [15,22].Second, it is constrained by the maximum context length of LLMs, limiting the complexity and coverage of tasks [6].Third, processing multiple demonstrations during inference incurs significant computational costs [7].</p>
<p>To mitigate these issues, recent works have explored adaptive or retrieval-based ICL, which dynamically selects demonstration examples based on the input query [23].While such methods mitigate order sensitivity, they still rely on in-context tokens and remain constrained by context length.More fundamentally, ICL depends on explicit prompt tokens to convey task information.In contrast, our method departs from token-based prompting by introducing a compact, learned vector that conveys task information without explicit demonstrations.This approach retains ICL's key strengths, such as query-specific adaptation and task generalization, while eliminating challenges related to prompt design and input length.</p>
<p>Vector-Based Approaches for Model Steering.Recent work has explored replacing in-context demonstrations with task vectors, which are dense representations that encode task information derived from a few-shot prompt.These vectors are typically extracted from the hidden state of the last token of the demonstration prompt within a transformer model [8], effectively summarizing the task's semantics in a single activation.For instance, Implicit In-Context Learning (I2CL) [11] compresses demonstration examples into a single context vector and injects it into the model's residual stream via linear operations.Similarly, ELICIT [12] maintains a library of task vectors associated with specific capabilities and retrieves the appropriate one based on the input.</p>
<p>These methods improve efficiency by removing token-level demonstrations.However, the task vectors they rely on are either derived from a fixed set of demonstrations or retrieved from a static library, and are reused across all input queries.As a result, they are not conditioned on the input and offer limited adaptability at inference time.To our knowledge, existing methods do not support the generation of task vectors conditioned on each input.Our proposed framework, ATV, addresses this limitation by dynamically generating task vectors per query, enabling more fine-grained input-aware task representation.This combines the efficiency of vector-based approaches with the adaptability required for input-level variation.</p>
<p>Methodology</p>
<p>Background and Preliminaries</p>
<p>Transformer Architecture.The Transformer is a neural architecture based on self-attention and forms the foundation of modern large-scale language models [24].Each layer consists of a feedforward network and a self-attention mechanism, which allows tokens to attend to others in the sequence.The self-attention operation is defined as:
Attn(Q, K, V ) = softmax QK T √ d k V(1)
where Q, K, and V indicate the query, key, and value matrices, respectively, and d k is the dimensionality of the key vectors.</p>
<p>Let h l−1 t be the hidden state of token t at layer l−1.A standard Transformer layer updates it as:
hl t = LayerNorm h l−1 t + Attn(Q t , K, V )(2)h l t = LayerNorm hl t + MLP( hl t )(3)
where
Q t = W Q h l−1 t
and K, V are projections of the previous layer's hidden states.In autoregressive models, the hidden state of the last token h l T summarizes the input context and is used for next-token prediction.</p>
<p>Task Vectors.Following prior work [12], we define a task vector as the hidden state of the last token at each transformer layer, capturing task-relevant information in a compressed form.Given an input x = [x 1 , x 2 , . . ., x T ], the task vector at layer l is:
v l task = h l T
(task vector extracted from the last token at layer l)</p>
<p>To steer the model output in a task-specific direction, we inject the task vector into the hidden state of the last token at each transformer layer.Specifically, for each layer l, the modified hidden state is computed as: hl = h l + λv l task (5) where h l denotes the hidden state of the last token at layer l, v l task ∈ R d l is the corresponding task vector slice, hl is the injected version, d l is the hidden dimensionality at layer l, and λ is a scaling factor controlling the strength of the intervention.For simplicity, we omit the token index and refer to the last token's hidden state simply as h l .This formulation allows the task vector to modulate the model's behavior in a lightweight and interpretable manner.</p>
<p>Previous methods rely on task vectors extracted from fixed demonstrations, resulting in a static representation shared across inputs.We introduce the Adaptive Task Vector (ATV), which is dynamically generated per input to modulate the model's behavior.</p>
<p>ATV: Adaptive Task Vectors for Large Language Models</p>
<p>Notation and Setup.Let x = [x 1 , x 2 , . . ., x T ] be a tokenized input query sequence of length T , and let y denote the corresponding target output.We define two models: a small model M small with hidden size d s , and a large language model M large with L layers and hidden size d l per layer.</p>
<p>From the input x, M small produces a hidden representation v small ∈ R ds , extracted from the last token of the last layer.This vector is then expanded via a parameterized function f θ : R ds → R L×d l to obtain our ATV v ATV = f θ (v small ), suitable for injection into the large model.Let v l ATV ∈ R d l denote the portion corresponding to the l-th layer.The final output generated by M large after ATV injection is denoted ỹ.The ATV is scaled by a hyperparameter λ before being added to the hidden state.</p>
<p>Overview of the ATV Framework.Our goal is to steer an LLM without modifying its weights by injecting input-conditioned signals directly into its hidden states.We introduce ATV, a lightweight control framework that operates externally to a frozen large model.</p>
<p>ATV consists of two lightweight modules:</p>
<p>(1) ATV generation.A small language model produces a compact vector representation from the input query.</p>
<p>(2) ATV expansion.An expansion module transforms this vector into a set of layer-wise steering signals injected into the large model.</p>
<p>We implement the expansion module as a single linear projection from R ds to R L•d l , followed by reshaping into R L×d l for compatibility with the target model.The generator and expansion modules are trained jointly, while the LLM remains frozen.</p>
<p>By injecting the ATV into the internal layers of the large model, the ATV enables flexible and targeted control over the model's behavior.This allows the large model to better align with desired task objectives, such as answering questions accurately or performing structured reasoning, without modifying its parameters or relying on prompt engineering.</p>
<p>We summarize this process in Figure 2.During training (top), the small model and the expansion module are optimized to produce effective ATVs that steer the large model toward the desired output.During inference (bottom), both modules are frozen and used to dynamically generate ATVs for new input queries.</p>
<p>The core idea behind ATV is to adapt the behavior of an LLM to a specific task without modifying its parameters.Instead of prompt-based conditioning, we steer the model by injecting query-specific information directly into its internal hidden states in the form of an ATV.</p>
<p>To generate the ATV, we first encode the input query using a small language model M small , such as a GPT-2 variant [25].The model processes the full tokenized sequence and produces hidden representations at each layer.From the last token of the last layer, we extract a compact vector v small that summarizes the semantics of the input query.</p>
<p>This vector is then expanded by f θ into the ATV v ATV , where each slice v l ATV is designed to modulate computations at the corresponding layer of the large model.</p>
<p>The ATV is injected into the frozen large model by modifying the hidden states of the last token during the forward pass.Specifically, for each layer l, the original hidden state h l of the last token is modified as:
hl = h l + λv l ATV (6)
where λ is a scalar hyperparameter that scales the ATV's influence.This additive injection provides lightweight yet effective steering of the model's output behavior.</p>
<p>To enable effective learning, we jointly train M small and f θ using a supervised loss.Let y denote the target output for input query x, and let ỹ be the output of the large model after ATV injection.The objective is to minimize the cross-entropy loss:
min ϕ,θ E (x,y)∼D [L CE (ỹ, y)] , where ỹ = M large (x; v ATV )(7)
where D denotes the supervised training dataset, and ϕ, θ are the parameters of the small model and the expansion module, respectively.Notably, the large model M large remains frozen throughout training.</p>
<p>After training, both M small and f θ are frozen.During inference, given a new input query, the system generates a query-specific ATV and injects it into the large model to guide its behavior.This enables ATV to adapt frozen LLMs to diverse downstream tasks in a modular and parameter-efficient manner.</p>
<p>Theoretical Analysis</p>
<p>We now theoretically analyze the proposed ATV framework to better understand its effect on the behavior of the LLM by comparing it to two prominent parameter-efficient tuning methods: LoRA and Prefix-Tuning.LoRA (Low-Rank Adaptation) injects trainable low-rank matrices into pretrained weights and has demonstrated strong performance in language model fine-tuning [26].Prefix-Tuning prepends trainable continuous vectors to the input sequence, conditioning the model through modified attention mechanisms without altering the pretrained weights [27].</p>
<p>Specifically, we focus on addressing the following two questions: (1) How does ATV compare to LoRA in expressivity under the same rank budget, and when might ATV offer additional advantages?</p>
<p>(2) Does ATV offer a more expressive attention mechanism compared to Prefix-Tuning?</p>
<p>We first show that ATV is never weaker than LoRA when both methods operate under the same rank budget, and then clarify in which respects ATV can be stronger.Theorem 1. (ATV-LoRA equivalence under equal rank) Let h ℓ ∈ R d ℓ be the last-token hidden state at layer ℓ, and let hℓ , ĥℓ be the outputs of ATV and LoRA.For d s ≪ d ℓ and a LoRA rank r = d s ,</p>
<p>ATV and LoRA are expressively equivalent: for any ATV update hℓ , there exists a LoRA configuration yielding ĥℓ = hℓ , and vice versa (see Appendix A.1 for the full proof).</p>
<p>Situations in which ATV exceeds LoRA The equivalence theorem 1 ensures that ATV inherits LoRA's expressiveness.Beyond this, there are several aspects in which ATV can exceed LoRA.First, if the auxiliary bottleneck is widened (d s &gt; r), ATV can represent update directions that LoRA cannot capture within its fixed rank-r subspace, thereby increasing its expressive power and enhancing its flexibility to adapt across diverse tasks and input distributions.Second, even with d s = r, ATV enables effective direction injection as it induces input-dependent perturbations.In the ATV-to-LoRA construction, the matrix that plays the role of LoRA's down-projection matrix W down , a factor that remains static during inference, is given by M := x +⊤ (λv small ), which, unlike LoRA, varies with the current activation x.Here, λ is a scalar hyperparmeter of ATV and x +⊤ denotes the Moore-Penrose pseudoinverse of x ⊤ (see Step 3 of the proof in Appendix A. 1.3).This query dependence allows ATV to adapt its update on a per-instance basis, an ability LoRA lacks, which may improve adaptability in dynamic or multi-task environments.</p>
<p>Secondly, under the relaxed linear attention approximation, we argue in Theorem 2 that any representation obtainable by prefix-tuning is also realizable by ATV, while the converse does not hold.To examine the source of expressivity differences under the approximation proposed by prior work [28], we begin by formulating the standard attention as Attn(xW q , CW k , CW v ), where x ∈ R T ×d l is the query, C ∈ R m×d l is the length-m context, and W q , W k , W v are projection matrices.Prefixtuning modifies the key and value by concatenating p trainable prefix vectors P k , P v ∈ R p×d l , yielding an augmented attention [29]:
Attn prefix = Attn(xW q , [P k ; CW k ], [P v ; CW v ])
, ATV, in contrast, injects a trained vector v l AT V additively to both the query and context:
Attn ATV = Attn (x + e T • (v l AT V ) ⊤ )W q , (C + e m • (v l AT V ) ⊤ )W k , (C + e m • (v l AT V ) ⊤ )W v
, where e m is a vector [0, ...0, 1] ∈ R m×1 .Theorem 2. (ATV is more expressive than Prefix-Tuning) Let Attn ATV and Attn prefix denote the attention outputs from ATV and prefix-tuning, respectively.Then, the representational space F of Attn ATV includes that of Attn prefix :
F(Attn prefix ) ⊆ F(Attn ATV )(8)
Comparison of Attn ATV and Attn prefix Under the approximation, Attn ATV ≈ Attn prefix + ∆ cross , where ∆ cross encapsulates the six cross-terms that capture the additional interactions between the query and context, modulated by the vector v l ATV (see Appendix A.2 for the full proof).</p>
<p>Experiments</p>
<p>To evaluate ATV, we design experiments examining both in-domain and generalization performance to unseen tasks, followed by ablation studies on the generator model capacity and injection strategies.We also compare ATV with parameter-efficient tuning methods to validate our theoretical analysis and visualize task vector distributions to understand their representational properties.We closely follow ELICIT's [12] experimental design, using identical datasets and evaluation protocols.We evaluate on LLaMA3-8B [30] and Mistral-7B [31], with I2CL [11] as an additional baseline and a separate comparison to LoRA [26] for theoretical validation.Our code is available at https: //github.com/MLAI-Yonsei/ATV.</p>
<p>Experiment Setup</p>
<p>Models and Baselines.Our primary models are LLaMA3-8B and Mistral-7B.We compare ATV against (i) zero-shot, (ii) 16-shot in-context learning (ICL), (iii) 16-shot BM25 retrieval-based ICL [32], (iv) ELICIT [12], and (v) I2CL [11].ICL and BM25 baselines use 16 demonstrations, either randomly sampled or retrieved from the task's training set.</p>
<p>Datasets.We evaluate ATV on a diverse collection of 20 tasks spanning five categories: Knowledge, Reasoning, Mathematics, Safety, and Natural Language Understanding.These tasks assess various NLP capabilities from reasoning to numerical problem solving.In addition to in-domain tasks, we evaluate ATV on a separate set of unseen tasks to assess its generalization ability.</p>
<p>Evaluation.We adopt the same evaluation strategy as ELICIT to reflect realistic inference scenarios, where test-time query formats differ from those seen during training.Each test query is presented in two additional template variations not used in task vector generation.Task-specific instructions are prepended for all methods to ensure fair comparison.Full implementation and experimental details, including dataset names, template formats, and hyperparameters, are available in Appendix B.</p>
<p>In-Domain Performance Evaluation</p>
<p>Table 1: In-domain performance comparison across five categories under LLaMA3 and Mistral.</p>
<p>ATV achieves the highest average accuracy on both models using the same number of tokens as ELICIT and I2CL, while outperforming all baselines across most domains and maintaining superior token efficiency over prompt-based methods.All results except I2CL and ATV are from ELICIT [12].We evaluate ATV on 20 in-domain tasks across five categories, with results summarized in Table 1.</p>
<p>Model</p>
<p>ATV consistently achieves the highest average accuracy across all baselines while maintaining strong token efficiency by avoiding additional prompt tokens.ATV performs particularly well on NLU and Reasoning tasks across both LLaMA3 and Mistral, highlighting the benefit of query-specific task vectors in handling semantic and logical variation.These categories often require nuanced understanding of input structure and are sensitive to prompt formulation, limiting the adaptability of fixed vector approaches.Safety and Knowledge tasks show similar patterns, benefiting from ATV's contextual adaptability.</p>
<p>Notably, BM25 achieves the best performance in the Math category.We attribute this to the patternbased nature of many math problems, where retrieved demonstrations closely resembling the test query provide a direct advantage.In contrast, ATV's focus on semantic-level task modeling may limit its effectiveness in tasks that demand precise procedural alignment.Overall, these results demonstrate the effectiveness of adaptive task representations across diverse language understanding tasks.The findings highlight ATV's particular strength in scenarios requiring contextual adaptation.</p>
<p>Generalization to Unseen Tasks</p>
<p>To evaluate the generalization capability of ATV, we assess its performance on a set of unseen tasks held out from training, covering domains such as linguistic acceptability, bias detection, and scientific reasoning.</p>
<p>As shown in Table 2, ATV achieves the highest average accuracy on both LLaMA3 and Mistral, driven by its ability to construct task vectors for novel tasks based on each input even without explicit demonstrations.This result demonstrates the strength of ATV in generalizing beyond in-domain tasks while maintaining strong token efficiency.</p>
<p>Ablation Study: Effect of Small Model Capacity</p>
<p>We study how the capacity of the small language model used to generate task vectors affects ATV's performance.All main experiments in this paper use GPT-2 (137M) as the default generator.To assess the effect of generator capacity, we experiment with multiple GPT-2 variants while keeping the target model (LLaMA3) fixed, and evaluate on the in-domain benchmark.As shown in Table 3, larger models such as GPT-2-XL yield slightly better performance, but the gains over GPT-2 are marginal.This suggests that even lightweight generators suffice for producing effective task vectors, highlighting the parameter efficiency and practicality of ATV.</p>
<p>Layer-wise Analysis of Injection Strategies</p>
<p>We conduct a layer-wise analysis to examine how injection depth influences performance for both ATV and ELICIT, revealing distinct patterns in how the two methods interact with different transformer layers.While ELICIT requires identifying a single best injection layer per task, we apply uniform injection across different depth ranges, specifically the bottom, middle, and top thirds of the model, to analyze the functional role of each region.These patterns align with each method's layerspecific performance impact.</p>
<p>We divide the transformer into bottom, middle, and top thirds, and evaluate each method by restricting injection to a single region.As shown in Table 4, ATV retains strong performance when injected into the bottom layers, exhibiting only marginal degradation relative to the full-layer setting.For ELICIT, the best performance is observed when injecting into the top layers, slightly surpassing its full-layer setting.This divergence suggests that ATV benefits from modulating lower-level representations, while ELICIT relies more on upper-layer reasoning.</p>
<p>Figure 3 visualizes the ℓ 2 norm of the injected vectors across layers.ATV shows higher vector magnitudes in the bottom layers with balanced scales, whereas ELICIT demonstrates a monotonic increase toward the top layers with much larger overall magnitudes.This behavioral difference aligns with the performance trends in Table 1: ATV achieves consistently strong results across all five task categories, not just in NLU.</p>
<p>The effectiveness of early-layer modulation is consistent with prior studies on transformer specialization [33][34][35], which show that lower layers primarily encode lexical and syntactic features, while upper layers are responsible for semantic reasoning and task-specific abstraction.</p>
<p>In addition, ELICIT exhibits a clear drop in performance when the injection is applied uniformly across all layers without identifying the best-performing layer.This reflects the need for additional computation to determine effective injection locations.By contrast, ATV can be applied across all layers without such tuning, making it more robust and easier to deploy in practice.</p>
<p>Empirical Comparison with Low-Rank Adaptation</p>
<p>We compare ATV with LoRA [26] on LLaMA3 to validate the theoretical analysis in Section 3.3.As shown in Table 5, ATV consistently outperforms LoRA on both in-domain and unseen tasks.These results provide empirical support for our theoretical claim that ATV matches the expressiveness of LoRA while offering better adaptability through query-dependent modulation.In particular, the improvements on unseen tasks highlight ATV's stronger generalization beyond LoRA's static update mechanism.</p>
<p>Visualizing Task Vector Distributions</p>
<p>To investigate how task vector distributions from ATV and ELICIT vary with input, we apply t-SNE to visualize the representations generated for different queries.</p>
<p>Figure 4: t-SNE visualization of task vector distributions for two BBH tasks.Each dot represents a query-specific task vector generated by ATV, while crosses denote the fixed task vectors used by ELICIT.We observe that vectors from similar queries tend to be grouped together, indicating that ATV adapts its representations based on the input, while ELICIT draws from a fixed demonstration pool and captures less query-level variation as a result.</p>
<p>Figure 4 shows the projected vectors for two BBH tasks alongside their ELICIT counterparts.We observe that, in ATV, vectors from similar queries tend to appear closer together in the embedding space, suggesting that ATV captures input-specific variation within and across tasks.For ELICIT, while task vectors are retrieved from a fixed pool based on query similarity, the retrieved vectors within each task show limited diversity.This visualization suggests that ATV produces more inputsensitive task representations, while the retrieval-based vectors used in ELICIT show limited variation across queries within the same task.This representational diversity contributes to ATV's superior performance, consistent with the improvements observed across both in-domain and unseen tasks in Tables 1-2.</p>
<p>Conclusion</p>
<p>We introduced ATV, a novel framework that addresses fundamental limitations of both ICL and existing task vector approaches.While ICL suffers from computational inefficiency and prior task vector methods rely on fixed representations that fail to adapt to individual inputs, ATV dynamically generates query-specific vectors to modulate LLMs without parameter changes.This approach enables the model to more effectively align with task objectives across diverse input queries.Empirically, ATV achieves strong performance across both LLaMA3 and Mistral, outperforming prior methods while maintaining high token efficiency.It generalizes well to unseen tasks, demonstrating the effectiveness of query-specific task vector generation without explicit demonstrations.We further support these findings through ablation studies and theoretical analysis, establishing the expressive capacity of ATV and its adaptability across diverse tasks.</p>
<p>Limitation and Impact statement Although ATV performs well overall, its effectiveness varies by task, and ATV can sometimes underperform compared to retrieval-based approaches on mathematics tasks where pattern-based matching is more effective than semantic-level task modeling.Additionally, because ATV depends on the input data, unintended behaviors may occur if the dataset is poorly curated or contains biased examples.Careful dataset understanding and evaluation across diverse input distributions are therefore essential when applying this method in practice.</p>
<p>Step 1 Factorize the ATV increment The additive term introduced by ATV is
∆h ATV = λ v small A ℓ = (λv small ) 1×ds A ℓ ds×d ℓ .
This is an outer product of a 1 × d s row vector and a d s × d ℓ matrix, so its matrix rank satisfies rank(∆h ATV ) ≤ d s .</p>
<p>Hence, ATV always adds a vector lying in a rank-d s subspace of R d ℓ .</p>
<p>Step 2 Factorize the LoRA increment LoRA's contribution can be written in vector form as
∆h LoRA = s x ⊤ W down W up = (s x ⊤ W down ) 1×r W up r×d ℓ
, which is likewise an outer product-now of shapes 1 × r and r × d ℓ .Consequently, rank(∆h LoRA ) ≤ r.</p>
<p>Under Assumption A1 (r = d s ), the rank upper bounds obtained in Steps 1 and 2 are identical, establishing that the two increments live in subspaces of equal maximal rank.</p>
<p>Step 3 Rewrite ATV as a LoRA update (ATV ⇒ LoRA) Let
W up := A ℓ and W down := M ∈ R d ℓ ×ds .
We choose M so that the following linear constraint holds for the last token:
x ⊤ M = λ v ∈ R 1×ds .(9)
Existence of a solution.The row vector x ⊤ has d ℓ free coordinates, whereas the right-hand side specifies only d s values with d s ≪ d ℓ ; consequently, the under-determined system (1) always admits a solution provided x ⊤ ̸ = 0.A canonical choice is
M := x +⊤ (λv),
where x +⊤ denotes the Moore-Penrose pseudoinverse of x ⊤ and satisfies x ⊤ x +⊤ = 1.</p>
<p>Resulting LoRA increment.With these matrices,
∆h LoRA = s x ⊤ W down W up = s (x ⊤ M ) A ℓ = s (λv) A ℓ .
Selecting the scale s = 1 yields
∆h LoRA = λvA ℓ = ∆h ATV ,
and hence the LoRA-modified hidden state satisfies hℓ = ĥℓ .</p>
<p>This completes the direction "ATV implies LoRA" under the rank-matching assumption r = d s .</p>
<p>Step 4 Rewrite LoRA as an ATV update (LoRA ⇒ ATV) Take fixed LoRA factors
W down ∈ R d ℓ ×r , W up ∈ R r×d ℓ with r = d s .
For the last token we observe the row vector
x ⊤ W down ∈ R 1×ds .
Row-vector SVD.Compute a thin singular-value decomposition
x ⊤ W down = uΣv ⊤ , where • u ∈ R 1×ds is orthonormal, • Σ ∈ R ds×ds is diagonal (rank ≤ 1), • v ⊤ ∈ R ds×ds is orthonormal. Define ATV parameters λ := ∥uΣ∥ 2 , v := uΣ ∥uΣ∥ 2 ∈ R ds , A ℓ := v ⊤ W up ∈ R ds×d ℓ .
The vector v has unit norm by construction, and A ℓ keeps the required shape.</p>
<p>ATV increment equals LoRA increment
λvA ℓ = (∥uΣ∥ 2 ) uΣ ∥uΣ∥ 2 v ⊤ W up = (uΣ) v ⊤ W up = x ⊤ W down W up = ∆h LoRA .
Hence hℓ = ĥℓ ; the LoRA update is reproduced exactly by ATV.</p>
<p>Conclusion of the proof</p>
<p>Both ATV and LoRA ultimately realize the same operation class h −→ h + (rank ≤ r) outer product, and, under the matched rank budget r = d s , have identical functional capacity on a frozen backbone.This completes the proof of the theorem.</p>
<p>A.1.4 Discussion of Assumptions</p>
<p>• Matched rank budgets (r = d s ).</p>
<p>The constructive proof fixes the LoRA rank to equal the ATV bottleneck size so that both methods have the same number of degrees of freedom.If d s &lt; r, LoRA has r − d s surplus channels; turning those channels off yields an exact simulation of ATV, so LoRA is (weakly) more expressive.</p>
<p>If d s &gt; r, ATV can move in directions that LoRA cannot represent.Projecting the ATV increment onto a rank-r subspace gives the closest LoRA-matchable update, so equivalence holds only "up to rank-r projection."• Non-zero input row vector x ⊤ .</p>
<p>The linear system x ⊤ M = λv in Step 3 requires x ⊤ ̸ = 0 to admit a solution via the Moore-Penrose pseudoinverse.In a transformer, x ⊤ is simply the hidden state of the token entering the query/value projection and is never identically zero after normal training; therefore the assumption is benign in practice.</p>
<p>A.1.5 Operational Differences Between ATV and LoRA</p>
<p>Table 6: Implementation-level differences between ATV and LoRA.</p>
<p>Aspect ATV LoRA</p>
<p>Insertion point Adds a vector λvA ℓ after the hidden state is computed Adds a low-rank matrix W down W up inside the projection weight Learned parameters Auxiliary small model M small + expansion blocks A ℓ of the linear expansion f θ () Two fixed factor matrices W down , W up Thus, even under equal rank budgets, the two methods differ operationally: ATV perturbs activations directly in the hidden state space, whereas LoRA perturbs projection weights via a static low-rank matrix.Nevertheless, as shown in the theorem, these implementation choices realize the same class of rank-r additive perturbations on a frozen backbone, yielding identical expressive power when r = d s .</p>
<p>A.2 Proof of Theorem 2 A.2.1 Preliminaries Linear approximation: Attn(Q, K, V ) ≈ QK ⊤ V [28] Attention ouptputs from Prefix-Tuning Attn prefix = Attn(xW q , [P k ; CW k ], [P v ; CW v ])
• x = [x 1 , x 2 , ..., x T ] ∈ R T ×d l • C ∈ R m×d l :
context sequence of length m with d l dimension(l-th layer's dimension) • P k , P v ∈ R p×d l : p tunable prefix vectors to the keys and values Attention ouptputs from Prefix-Tuning
Attn ATV = Attn((x + e T • (v l AT V ) ⊤ )W q , (C + e m • (v l AT V ) ⊤ )W k , (C + e m • (v l AT V ) ⊤ )W v ) • v l AT V ∈ R d l • e m = [0, ..., 0, 1] ∈ R m×1 • (e m • (v l AT V ) ⊤ )W k = P ′ k A.2.2 Theorem
Theorem (ATV is more expressive than Prefix-Tuning) The representational space F of Attn ATV includes that of Attn prefix : F(Attn prefix ) ⊆ F(Attn ATV )</p>
<p>A.2.3 Process of Derivation</p>
<p>Linear approximation of Prefix-Tuning
Attn prefix = Attn(xW q , concat(P k , CW k ), concat(P v , CW v )) = softmax(xW q ([P k ; CW k ]) ⊤ )[P v ; CW v ] ≈ xW q (P k ; CW k ]) ⊤ ([P v ; CW v ])(∵ Attn(Q, K, V ) ≈ QK ⊤ V ) = xW q (P k ) ⊤ P v + xW q (CW k ) ⊤ CW v
Linear approximation of ATV
Attn ATV =Attn (x + e T • (v l AT V ) ⊤ )W q , (C + e m • (v l AT V ) ⊤ )W k , (C + e m • (v l AT V ) ⊤ )W v ≈ xW q (CW k ) ⊤ + xW q (e m • (v l AT V ) ⊤ ) ⊤ + e T • (v l AT V ) ⊤ W q (CW k ) ⊤ + e T • (v l AT V ) ⊤ W q (e m • (v l AT V ) ⊤ W k ) ⊤ • CW v + e m • (v l AT V ) ⊤ W v Let (e m • (v l AT V ) ⊤ )W k = P ′ k , (e m • (v l AT V ) ⊤ )W v = P ′ v , (e T • (v l AT V ) ⊤ )W q = P ′ q ⇒ Attn ATV ≈ xW q (CW k ) ⊤ CW v + xW q (P ′ k ) ⊤ P ′ v (Similar to Attn Prefix ) (T 1 + T 2 ) + xW q (P ′ k ) ⊤ CW v (T 3 ) + xW q (CW k ) ⊤ P ′ v (T 4 ) + P ′ q (CW k ) ⊤ CW v (T 5 ) + P ′ q (P ′ k ) ⊤ CW v (T 6 ) + P ′ q (CW k ) ⊤ P ′ v (T 7 ) + P ′ q P ′ k ⊤ P ′ v (T 8 ) A.2.</p>
<p>Analysis of Each Term in Attn ATV</p>
<p>For each term we report (i) the intuition behind the interaction, and (ii) how it extends or subsumes the behavior attainable with classic Prefix-Tuning (PT), treating the ATV-generated vectors P ′ k , P ′ v , P ′ q as soft-prefix counterparts to PT's fixed prefixes (see Table 7).• Query-side freedom (T 5 , T 6 , T 7 , T 8 ): Because PT never changes W q , any behavior that requires altering the query vector is strictly outside its representational span.ATV realizes this through the additive query P ′ q .• Mixed interactions (T 3 , T 4 ): Unlike PT, ATV can blend a single soft prefix key or value with the untouched content tokens.To even approximate T 3 , PT would have to add one custom prefix key for every content token, which is an impractical workaround, and T 4 cannot be reproduced by PT at all.</p>
<p>• Full prefix channel (T 8 ): A complete synthetic path lets ATV add task-specific information even when the original context is irrelevant, while still using no extra tokens at runtime.</p>
<p>Taken together, the additional six terms explain why ATV is more expressive: it augments the attention operator along every axis (query, key, and value) without introducing heavy retraining or large prefix matrices, yet it can still emulate PT as a special case.</p>
<p>B Detailed Experiments Setting</p>
<p>Our experimental setup follows ELICIT [12], using the same datasets and evaluation protocols.Below we provide detailed specifications.</p>
<p>B.1 Dataset List</p>
<p>All experiments are conducted on the same 20 in-domain tasks and 5 unseen tasks as used in ELICIT.Tasks are categorized as follows:</p>
<p>• Knowledge: CommonsenseQA [36], OpenBookQA [37], HellaSwag [38], BoolQ [39] • Reasoning: Four subsets from Big-Bench Hard (BBH) [40] (BBH Boolean Expressions, BBH Date Understanding, BBH Reasoning about Colored Objects, BBH Temporal Sequences), ARC-Challenge [41] • Mathematics: MathQA [42], MMLU Pro-MATH [43] • Safety: Crows-Pairs [44], BBQ-Age [45], Ethics-Commonsense, Ethics-Justice [46] • Natural Language Understanding (NLU): GLUE (SST-2, QNLI, MNLI) [47] ATV.We train the small model M small (GPT-2, 137M parameters) and the expansion module f θ jointly with the following hyperparameters.A constant learning rate of 5e-4 is used without warmup or learning rate scheduling, along with weight decay of 1e-5.The model is optimized for 15 epochs using the Adam optimizer.</p>
<p>We inject a task vector v ATV ∈ R L×d l into the last token's hidden state at each layer as hl = h l + λv l ATV .We use a scaling factor of λ = 0.001 throughout all experiments.In our implementation, the hidden size of the small model is d s = 768 (GPT-2), and the large models (LLaMA3-8B and Mistral-7B) use d l = 4096 with L = 32 transformer layers.</p>
<p>ELICIT.We follow the official implementation and configuration of ELICIT.Task vectors are retrieved from a precomputed capability library, each paired with its optimal injection layer.At inference time, the selected vector is additively injected into the frozen LLM at the designated layer.All training and evaluation use the official codebase and default settings.</p>
<p>Each task vector is constructed from 10 exemplars per task, each with a 16-shot prompt.While the total number of unique samples may vary due to overlap, our analysis confirms a minimum of 91 unique samples per task.To ensure fair comparison, we use 90 training samples per task for all other baselines.</p>
<p>I2CL.We adopt the official I2CL implementation [11], modifying only the number of training epochs to 15 for consistency with other baselines.To ensure fair comparison, we deviate from the original setting, which calibrates context vectors and injection coefficients separately for each dataset using task identity.Instead, we train a shared set of coefficients across all datasets while keeping dataset-specific context vectors.</p>
<p>For evaluation on unseen tasks, we use a retrieval strategy that selects the most similar context vector among those obtained from in-domain datasets, based on cosine similarity between the input query and training prompts.</p>
<p>LoRA.We adopt the LoRA configuration described in the I2CL paper, which applies low-rank adaptation to the query and value projection matrices in all attention layers.The setup uses rank r = 8, scaling factor α = 32, and a dropout rate of 0.05.All other settings, including the optimizer, follow the official implementation.However, as the original learning rate of 1e−3 resulted in poor performance in our setting, we adjust it to 4e−4.</p>
<p>B.3 Task-Specific Prompt List</p>
<p>We follow the prompt template settings from ELICIT, which adopts task-specific templates manually crafted based on guidelines from lm-harness [50] and the chain-of-thought-hub 1 .For each task, we use the same three distinct question templates as provided in ELICIT.The full set of question templates used for each task is listed in Table 8.</p>
<p>The answer-side format is consistent across all tasks and composed of the following structure:</p>
<p>• A line break (\n) after the question template, • A list of options in the form: Options: (A) ..., (B) ..., (C) ..., ..., • One of the following three answer prefixes:</p>
<p>-A:</p>
<p>-Answer:</p>
<p>-The answer is By combining the 3 question templates with the 3 answer prefixes, we construct 9 distinct prompt variants per task.Following the ELICIT setup, only the A: answer prefix is used during training, while all 3 answer formats are used during evaluation to assess generalization to unseen answer styles.This setting is consistently applied across all baseline methods.</p>
<p>Table 8: Question-side templates used for each task.Each task uses three distinct prompt formats as provided in the original ELICIT setting.</p>
<p>Task (Dataset) Template</p>
<p>CommonsenseQA</p>
<p>• The following are multiple choice questions (with answers) about commonsense knowledge reasoning.Finish your answer with 'X' where X is the correct letter choice.\n\nQuestion:{input} • Below are multiple-choice questions about commonsense reasoning.</p>
<p>Answer with 'X', X being the correct option.\n\nQuestion:{input} • Respond to these multiple-choice questions on commonsense knowledge.Conclude with 'X', where X is the right letter choice.\n\nQuestion:{input} OpenBookQA • The following are multiple choice questions (with answers) about multi-step reasoning.Finish your answer with 'X' where X is the correct letter choice.\n\nQuestion:{input} • The following are multiple-choice questions testing multi-step reasoning.Answer with 'X', X being the correct option.\n\nQuestion:{input} • Answer these multiple-choice questions involving multi-step logical thinking.Conclude with 'X', where X is the right letter choice.\n\nQuestion:{input} Continued on next page 1 https://github.com/FranxYao/chain-of-thought-hub</p>
<p>Figure 2 :
2
Figure 2: Overview of the Adaptive Task Vector (ATV) framework.Top: During training, the small model and expansion module are updated to minimize the loss from the steered output.Bottom: During inference, both modules are frozen and used to generate query-specific ATV vectors for steering the frozen large model.</p>
<p>Table 4 :
4
Layer-wise performance comparison between ATV and ELICIT on LLaMA3.While ATV shows strong performance when injected into bottom layers, ELICIT performs best when applied only to top layers.This contrast highlights the different functional dependencies of the two methods.Reported differences are measured with respect to the full-layer injection setting.Injected LayerAvg.acc (ATV) Diff.(ATV) Avg.acc (ELICIT) Diff.(ELICIT)</p>
<p>Figure 3 :
3
Figure 3: Layer-wise analysis of vector injection magnitudes.Left: ATV concentrates vector strength in lower layers, while Right: ELICIT shows a monotonic increase toward top layers.These patterns align with each method's layerspecific performance impact.</p>
<p>Table 2 :
2
[12]ormance on unseen tasks not included in the ATV training set, evaluated under LLaMA3 and Mistral.ATV achieves the highest average accuracy across all methods while using significantly fewer tokens than prompt-based and fixed vector approaches, demonstrating strong generalization.All results except I2CL and ATV are from ELICIT[12].
Model# TokensGLUE COLA BBQ Religion Deepmind MMLU-Psychology BBH-five-objectsAvg.Zero-shot103.6 ± 47.772.0 ± 0.738.6 ± 1.117.5 ± 2.654.2 ± 0.317.1 ± 0.039.9 ± 0.8BM252502.8 ± 26.055.4 ± 1.064.6 ± 1.330.7 ± 1.783.0 ± 0.148.3 ± 0.056.4 ± 0.4Llama3ELICIT103.6 ± 47.763.4 ± 0.945.0 ± 0.723.7 ± 3.470.0 ± 0.625.7 ± 0.045.6 ± 0.4I2CL103.6 ± 47.726.1 ± 0.639.4 ± 3.123.5 ± 3.775.0 ± 1.027.3 ± 2.538.3 ± 2.2ATV103.6 ± 47.777.6 ± 2.780.8 ± 2.626.4 ± 2.780.6 ± 2.351.7 ± 3.163.4 ± 2.5Zero-shot115.4 ± 51.043.3 ± 1.135.4 ± 3.39.0 ± 0.457.9 ± 0.77.4 ± 0.030.6 ± 1.0BM252804.6 ± 27.644.4 ± 2.270.7 ± 0.726.6 ± 3.978.7 ± 1.125.7 ± 0.049.2 ± 0.3MistralELICIT115.4 ± 51.041.7 ± 0.842.1 ± 2.525.1 ± 1.265.6 ± 0.615.6 ± 0.038.0 ± 0.6I2CL115.4 ± 51.053.3 ± 1.348.4 ± 6.522.0 ± 2.672.6 ± 0.222.9 ± 4.543.9 ± 3.0ATV115.4 ± 51.079.8 ± 7.181.7 ± 2.224.6 ± 5.370.7 ± 1.040.3 ± 3.659.4 ± 2.6</p>
<p>Table 3 :
3
Effect of small model capacity on ATV performance.We evaluate four GPT-2 variants as the small model for generating task vectors, with the LLaMA3 target model fixed.While GPT-2-XL achieves the highest average accuracy, the smallest model (GPT-2, 137M) performs comparably, indicating that lightweight models are sufficient for effective task vector generation and supporting the parameter efficiency of ATV.
Model# ParametersNLUReasoning KnowledgeMathSafetyAvg.GPT-2137M61.0 ± 5.0 76.1 ± 1.373.0 ± 1.625.8 ± 2.0 74.8 ± 0.4 62.1 ± 1.5Llama3GPT-2-Medium GPT-2-Large380M 812M61.2 ± 3.9 75.9 ± 1.4 62.1 ± 1.4 74.2 ± 0.672.0 ± 2.5 72.4 ± 2.124.6 ± 1.8 68.1 ± 4.1 60.4 ± 1.3 25.4 ± 1.9 72.7 ± 2.8 61.4 ± 0.5GPT-2-XL1.61B63.8 ± 4.2 76.5 ± 1.673.9 ± 2.223.7 ± 0.8 73.1 ± 4.2 62.2 ± 0.6</p>
<p>Table 5 :
5
Comparison of LoRA and ATV on LLaMA3.ATV consistently outperforms LoRA on both in-domain and unseen tasks, reflecting its enhanced adaptability.These results are consistent with our theoretical analysis demonstrating the superior expressiveness of ATV.
Method In-domainUnseenAvg.LoRA56.0 ± 1.4 52.0 ± 3.2 54.0 ± 2.3ATV62.1 ± 1.5 63.4 ± 2.5 62.8 ± 2.0</p>
<p>Table 7 :
7
Qualitative roles of ATV attention terms and their relation to Prefix-Tuning (PT).Base attention of the frozen model Representable in PT (identical).No additional expressivity; both methods preserve this term unchanged.Not representable in PT.Because PT keeps W q frozen, it cannot alter queries.ATV adds a query-side degree of freedom, enabling new attention directions.
Term Qualitative roleRelation to Prefix-Tuning (PT) / Added expressivityT 1T 2Prefix keys &amp; values only. QueryRepresentable in PT (exact match). This is the sole extraattends only to prefix key/valuepath PT can realize; ATV contains it and can thereforeemulate PT exactly.T 3Prefix key → content values. ANot representable in PT. PT would need a separatesoft prefix key reshapes the attentionlearned key for every content token, whereas ATVweights, but the actual informationachieves the same effect with a single soft key-thusstill comes from the content values.widening the attention design space.T 4Content keys → prefix value. Nor-Not representable in PT. PT lacks a mechanism to injectmal keys set the weights, but an ex-new information exclusively at the value stage for existingtra value generated by the adapter iskeys; ATV can graft auxiliary content into any token'sinjected at the output stage.output.T 5Prefix query. The query itself isshifted in a new direction while stillusing ordinary keys and values.T 6Prefix query + key. Both sides ofNot representable in PT. ATV can simultaneously steerthe similarity come from the samequeries and keys while still reading content values, provid-learnable vector, but the output ising a finer redistribution of attention mass that PT cannotstill built from content values.mimic.T 7Prefix query + value. OrdinaryNot representable in PT. PT can supply prefix valueskeys choose the weights; the re-but cannot adapt the query; ATV adds this missing queryturned information comes from amodulation, enhancing expressivity.prefix-generated value.T 8Full prefix triad. Query, key, andNot representable in PT. PT has no mechanism for avalue are all produced by the samefully synthetic attention channel without real tokens; ATVlow-rank adapter, yielding a fullyintroduces an entirely new path, further enlarging the rep-synthetic attention path.resentational space.Key points of the theorem
• Containment: PT spans only the subspace generated by T 1 + T 2 .ATV keeps those terms and introduces T 3 − T 8 , henceF(Attn prefix ) ⊆ F(Attn ATV )</p>
<p>(42,exper10)nts are conducted on NVIDIA A100 80GB GPUs.We use the same training data splits and evaluation protocols across all methods for fair comparison.Each experiment is repeated 3 times with different random seeds(42, 100,10)to compute statistical significance.For training, we sample 90 examples per task from the official training split of each in-domain task (excluding unseen tasks), and use the same sampled data across all baselines.
, Super-GLUE (WIC, RTE) [48]• Unseen: GLUE COLA, BBQ-Religion, Deepmind [49], MMLU High School Psychology,BBH Logical Deduction Five objectsB.2 Implementation Details and Baseline ConfigurationsCommon Setup.
A Proofs for Theoretical ResultsA.1 Proof of Theorem 1A.1.1 PreliminariesConsider a single transformer layer ℓ with hidden state dimension d ℓ .Let h ℓ ∈ R d ℓ denote the hidden state of the last token at that layer.ATV (Adaptive Task Vector) update ATV injects an additive low-rank vector hℓ = h ℓ + λ v small A ℓ , where • h ℓ ∈ R d ℓ : last-token hidden state in layer ℓ.• v small ∈ R ds with d s ≪ d ℓ : query-specific vector from the M small .• A ℓ ∈ R ds×d ℓ : the ℓ-th d s × d ℓ block of the linear expansion f θ (v small ) = v ATV ∈ R L×d ℓ ; thus, v small A ℓ is the ℓ-th row of v ATV .• λ ∈ R: global scaling constant.LoRA (low-rank adaptation) update LoRA applies a rank-r additive update to the projection.Its effect on the hidden state is:where • x ⊤ ∈ R 1×d ℓ : the current-token activation entering the augmented projection; for a query / value projection, this equals (h ℓ ) ⊤ .• W down ∈ R d ℓ ×r and W up ∈ R r×d ℓ : LoRA projection matrices.• s ∈ R: global scaling constant.Assumption (A1) -matched rank budgetsWe henceforth set the LoRA bottleneck rank equal to the ATV bottleneck size: r = d s .A.1.2 TheoremTheorem (ATV-LoRA equivalence under equal rank) Under Assumption A1:1. ATV ⇒ LoRA (simulation).For every pair (h ℓ , v small ) there exist static LoRA factors (W down , W up ) and a scale s, all independent of the runtime query, such that hℓ = ĥℓ for all inputs.2. LoRA ⇒ ATV (simulation).Conversely, any LoRA update with rank r = d s can be expressed in ATV form by an appropriate choice of (λ, v small , A ℓ ).Hence, when the rank budgets are matched, ATV and LoRA realize the same class of low-rank additive perturbations to the frozen model; they are expressively equivalent.A.1.3 ProofThroughout the proof we fix the layer index ℓ and omit it from superscripts whenever no ambiguity arises.Sequences• Answer questions about which times certain events could have occurred.Finish your answer with 'X' where X is the correct letter choice.\n\nQ:{input} • Determine possible occurrence times for specific events.Answer with 'X', X being the correct option.\n\nQ:{input} • Identify when certain events could have happened.Conclude with 'X', where X is the right letter choice.\n\nQ:{input}BBH Reasoning about Colored Objects• Answer extremely simple questions about the colors of objects on a surface.Finish your answer with 'X' where X is the correct letter choice.\n\nQ:{input} • Respond to basic questions about object colors on a surface.Answer with 'X', X being the correct option.\n\nQ:{input} • Address simple queries regarding the colors of items on a surface.Conclude with 'X', where X is the right letter choice.\n\nQ:{input}Continued on next pageDeduction Five Objects• A logical deduction task which requires deducing the order of a sequence of objects.Finish your answer with 'X' where X is the correct letter choice.\n\nQuestion:{input} • This challenge involves logically determining the sequence of a set of objects.Conclude your response with 'X', where X is the appropriate letter option.\n\nQuestion:{input} • In this logical reasoning exercise, deduce the correct order of a series of objects.End your answer with 'X', X being the right letter choice.\n\nQuestion:{input}
Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>A survey on in-context learning. Q Dong, L Li, D Dai, C Zheng, J Ma, R Li, H Xia, J Xu, Z Wu, T Liu, arXiv:2301.002342022arXiv preprint</p>
<p>What makes good in-context examples for. J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, arXiv:2101.068042021arXiv preprint</p>
<p>Revisiting demonstration selection strategies in in-context learning. K Peng, L Ding, Y Yuan, X Liu, M Zhang, Y Ouyang, D Tao, arXiv:2401.120872024arXiv preprint</p>
<p>Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. X Wang, W Zhu, W Y Wang, arXiv:2301.119162023115arXiv preprint</p>
<p>Long-context llms struggle with long in-context learning. T Li, G Zhang, Q D Do, X Yue, W Chen, arXiv:2404.020602024arXiv preprint</p>
<p>Babilong: Testing the limits of llms with long context reasoning-in-a-haystack. Y Kuratov, A Bulatov, P Anokhin, I Rodkin, D Sorokin, A Sorokin, M Burtsev, Advances in Neural Information Processing Systems. 202437</p>
<p>In-context learning creates task vectors. R Hendel, M Geva, A Globerson, Findings of the Association for Computational Linguistics: EMNLP 2023. J Bouamor, K Pino, Bali, Dec. 2023</p>
<p>Editing models with task arithmetic. G Ilharco, M T Ribeiro, M Wortsman, L Schmidt, H Hajishirzi, A Farhadi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>In-context vectors: making in context learning more effective and controllable through latent space steering. S Liu, H Ye, L Xing, J Zou, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning202432</p>
<p>Implicit in-context learning. Z Li, Z Xu, L Han, Y Gao, S Wen, D Liu, H Wang, D N Metaxas, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>ELICIT: LLM augmentation via external in-context capability. F Wang, J Yan, Y Zhang, T Lin, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Task vectors in in-context learning: Emergence, formation, and benefit. L Yang, Z Lin, K Lee, D Papailiopoulos, R Nowak, arXiv:2501.092402025arXiv preprint</p>
<p>Multimodal task vectors enable many-shot multimodal in-context learning. B Huang, C Mitra, L Karlinsky, A Arbelle, T Darrell, R Herzig, Advances in Neural Information Processing Systems. 202437</p>
<p>Calibrate before use: Improving few-shot performance of language models. Z Zhao, E Wallace, S Feng, D Klein, S Singh, International conference on machine learning. PMLR2021706</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Y Lu, M Bartolo, A Moore, S Riedel, P Stenetorp, arXiv:2104.087862021arXiv preprint</p>
<p>Batch-icl: Effective, efficient, and order-agnostic in-context learning. K Zhang, A Lv, Y Chen, H Ha, T Xu, R Yan, arXiv:2401.064692024arXiv preprint</p>
<p>When is task vector provably effective for model editing? a generalization analysis of nonlinear transformers. H Li, Y Zhang, S Zhang, M Wang, S Liu, P.-Y Chen, arXiv:2504.109572025arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, arXiv:2205.106252022arXiv preprint</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work. S Min, X Lyu, A Holtzman, M Artetxe, M Lewis, H Hajishirzi, L Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing20221164</p>
<p>Learning to retrieve prompts for in-context learning. O Rubin, J Herzig, J Berant, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, ICLR. 1232022</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, arXiv:2101.001902021arXiv preprint</p>
<p>Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. D Dai, Y Sun, L Dong, Y Hao, S Ma, Z Sui, F Wei, arXiv:2212.105592022arXiv preprint</p>
<p>Towards a unified view of parameter-efficient transfer learning. J He, C Zhou, X Ma, T Berg-Kirkpatrick, G Neubig, International Conference on Learning Representations. 2022</p>
<p>The llama 3 herd of models. A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D De Las Casas, F Bressand, G Lengyel, G Lample, L Saulnier, L R Lavaud, M.-A Lachaux, P Stock, T L Scao, T Lavril, T Wang, T Lacroix, W E Sayed, Mistral 7b. 2023</p>
<p>The probabilistic relevance framework: Bm25 and beyond. S Robertson, H Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>A primer in bertology: What we know about how bert works. A Rogers, O Kovaleva, A Rumshisky, Transactions of the association for computational linguistics. 82021</p>
<p>Bert rediscovers the classical nlp pipeline. I Tenney, D Das, E Pavlick, arXiv:1905.059502019arXiv preprint</p>
<p>A mathematical framework for transformer circuits. N Elhage, N Nanda, C Olsson, T Henighan, N Joseph, B Mann, A Askell, Y Bai, A Chen, T Conerly, Transformer Circuits Thread. 11122021</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. A Talmor, J Herzig, N Lourie, J Berant, arXiv:1811.009372018arXiv preprint</p>
<p>Can a suit of armor conduct electricity? a new dataset for open book question answering. T Mihaylov, P Clark, T Khot, A Sabharwal, arXiv:1809.027892018arXiv preprint</p>
<p>Hellaswag: Can a machine really finish your sentence. R Zellers, A Holtzman, Y Bisk, A Farhadi, Y Choi, arXiv:1905.078302019arXiv preprint</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. C Clark, K Lee, M.-W Chang, T Kwiatkowski, M Collins, K Toutanova, arXiv:1905.100442019arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. P Clark, I Cowhey, O Etzioni, T Khot, A Sabharwal, C Schoenick, O Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Mathqa: Towards interpretable math word problem solving with operation-based formalisms. A Amini, S Gabriel, P Lin, R Koncel-Kedziorski, Y Choi, H Hajishirzi, arXiv:1905.133192019arXiv preprint</p>
<p>Mmlupro: A more robust and challenging multi-task language understanding benchmark. Y Wang, X Ma, G Zhang, Y Ni, A Chandra, S Guo, W Ren, A Arulraj, X He, Z Jiang, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>
<p>Crows-pairs: A challenge dataset for measuring social biases in masked language models. N Nangia, C Vania, R Bhalerao, S R Bowman, arXiv:2010.001332020arXiv preprint</p>
<p>Bbq: A hand-built bias benchmark for question answering. A Parrish, A Chen, N Nangia, V Padmakumar, J Phang, J Thompson, P M Htut, S R Bowman, arXiv:2110.081932021arXiv preprint</p>
<p>Pointer sentinel mixture models. S Merity, C Xiong, J Bradbury, R Socher, arXiv:1609.078432016arXiv preprint</p>
<p>Glue: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, arXiv:1804.074612018arXiv preprint</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. A Wang, Y Pruksachatkun, N Nangia, A Singh, J Michael, F Hill, O Levy, S Bowman, Advances in neural information processing systems. 201932</p>
<p>Analysing mathematical reasoning abilities of neural models. D Saxton, E Grefenstette, F Hill, P Kohli, arXiv:1904.015572019arXiv preprint</p>
<p>Eleutherai/lm-evaluation-harness: Major refactor. L Sutawika, L Gao, H Schoelkopf, S Biderman, J Tow, B Abbasi, C Lovering, J Phang, A Thite, Fazz, N Aflah, T Muennighoff, Wang, J Chris, Z Etxaniz, Kasner, J Khalid, Hsu, P S Andyzwei, D Ammanamanchi, E Groeneveld, E Smith, Tang, 10.5281/zenodo.10256836Dec. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
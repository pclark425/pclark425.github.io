<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2216 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2216</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2216</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-60.html">extraction-schema-60</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <p><strong>Paper ID:</strong> paper-278602711</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.09455v1.pdf" target="_blank">Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos</a></p>
                <p><strong>Paper Abstract:</strong> State-of-the-art spatio-temporal action detection (STAD) methods show promising results for extracting soccer events from broadcast videos. However, when operated in the high-recall, low-precision regime required for exhaustive event coverage in soccer analytics, their lack of contextual understanding becomes apparent: many false positives could be resolved by considering a broader sequence of actions and game-state information. In this work, we address this limitation by reasoning at the game level and improving STAD through the addition of a denoising sequence transduction task. Sequences of noisy, context-free player-centric predictions are processed alongside clean game state information using a Transformer-based encoder-decoder model. By modeling extended temporal context and reasoning jointly over team-level dynamics, our method leverages the"language of soccer"- its tactical regularities and inter-player dependencies - to generate"denoised"sequences of actions. This approach improves both precision and recall in low-confidence regimes, enabling more reliable event extraction from broadcast video and complementing existing pixel-based methods.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2216.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2216.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAAD + DST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Track-Aware Action Detector (TAAD) prior predictions plus Denoising Sequence Transduction (DST) pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage pipeline where TAAD produces short-time-window, player-centric action logits (used as a computational proxy), and a Transformer-based DST model ingests those noisy logits plus role-based game-state features to generate denoised, game-level (who, what, when) event sequences that are compared to frame-level ground-truth annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TAAD + DST pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>First stage: TAAD (Track-Aware Action Detector) — a lightweight 3D-CNN/ROIAlign + temporal conv detector trained on short clips (T=50 frames) that outputs per-frame, per-player logits for K action classes (kept as raw logits). Second stage: DST — a Transformer encoder–decoder (6 layers each, 8 heads, hidden dim 512) that encodes sequences of length L (750 frames) composed of TAAD logits (role-ordered) + role-based game-state vectors + frame encodings and autoregressively decodes cleaned action tuples (action class, player role, frame index). The DST model is trained as a denoising sequence transducer to recover ground-truth action sequences from corrupted/context-free predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>sports video analysis / spatio-temporal action detection</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>TAAD per-frame, per-player action logits / low-threshold predictions (confidence scores)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>TAAD produces raw logits (no softmax applied) per frame, per player, per action class over T=50-frame clips. These logits are treated as computational predictions (proxy signals) of actions; later thresholded by confidence (evaluation uses a 15% confidence threshold) to yield discrete candidate detections. For DST input the raw logits are concatenated in a fixed role order rather than discretized.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (deep learning classifier outputs used as a surrogate objective / proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Frame-level, player-identified action annotations (human-labeled event class + player identity + frame index)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Manual frame-by-frame event annotations in the Full-length Game Footovision Dataset (299 full games) that specify action class and the identity of the acting player; matching between predicted and ground-truth events is performed when predicted and ground-truth share same player and class within a temporal tolerance +/- δ frames (δ = 12 or 25). Metrics (Precision and Recall) are computed by counting TP/FP/FN from that matching.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Direct quantitative comparison reported as Precision and Recall of TAAD (proxy-alone) vs TAAD + DST (denoised outputs) when evaluated against ground-truth annotations. Overall (δ=12): TAAD Precision = 43.56%, Recall = 66.97%; TAAD + DST (no game-state) Precision = 74.17%, Recall = 70.27%; TAAD + DST (with game-state) Precision = 78.77%, Recall = 75.80%. Absolute overall Precision gain from TAAD → DST (with game-state) = +35.21 percentage points; Recall gain = +8.83 percentage points. Class-level example (Pass): TAAD PR=62.67%, REC=71.17% → DST w/GS PR=83.07%, REC=79.80% (PR +20.40 pp, REC +8.63 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>TAAD evaluated vs ground-truth: overall Precision 43.56%, Recall 66.97% (confidence threshold 15%, δ=12 frames). Per-class reported in Table 1 (examples): Pass PR=62.67% REC=71.17%; Ball-drive PR=37.56% REC=68.07%; Header PR=26.94% REC=49.44%.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>DST outputs evaluated vs same ground-truth annotations: overall Precision 78.77%, Recall 75.80% (with game-state, confidence threshold 15%, δ=12). DST without game-state: overall Precision 74.17%, Recall 70.27%. Per-class improvements listed in Table 1 (e.g., Pass PR=83.07% REC=79.80%).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not given explicitly as FP/(all negatives); but from reported Precision one may compute FP fraction among positive predictions: TAAD false-positive fraction among positives ≈ 1 - 0.4356 = 56.44%; TAAD + DST (with game-state) ≈ 1 - 0.7877 = 21.23% (i.e., relative reduction in FP fraction among predicted positives of ~35.21 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>From reported Recall: TAAD FN fraction among ground-truth events ≈ 1 - 0.6697 = 33.03%; TAAD + DST (with game-state) FN fraction ≈ 1 - 0.7580 = 24.20% (absolute reduction ≈ 8.83 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution / incremental methodological advance: DST is presented as an architectural / representation improvement that denoises ML-generated proxies using additional structured context; evaluation is on held-out games from the same dataset (i.e., near-training-distribution generalization rather than far-OOD discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Architectural/representation-based bias-correction: denoising sequence transduction using a Transformer that (1) enforces role-based ordering to remove permutation invariance, (2) ingests role-ordered TAAD logits (raw proxies) rather than discretized labels, (3) augments inputs with structured game-state features (player positions, velocities, visibility) and long temporal context (100/250/750 frames), and (4) is trained with cross-entropy losses on class, role, and frame outputs. These design choices serve as surrogate-to-ground-truth gap reduction techniques (not statistical calibration).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Quantified in Table 1 (δ=12): switching from TAAD to DST (no game-state) increases overall Precision from 43.56% → 74.17% (+30.61 pp) and Recall 66.97% → 70.27% (+3.30 pp). Adding game-state yields further gain to Precision 78.77% (+4.60 pp) and Recall 75.80% (+5.53 pp versus DST without game-state). Temporal context experiments show performance improves with longer L (100→250→750 frames) with diminishing returns for Precision beyond 250 frames (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Not quantified in dollars/time; qualitative discussion: DST is trainable on a single consumer-grade / entry-level professional GPU (RTX 3090 or RTX A6000) and intended to operate in a low-threshold, high-recall mode to assist human annotators, implying proxy evaluation (automatic detector inference) is far cheaper than manual ground-truth annotation, but explicit cost comparisons are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td>No long-term (multi-hour/days) temporal validation of predictions vs ground-truth is reported; experiments vary sequence context length (100, 250, 750 frames) and show improved short-to-mid-term agreement with ground-truth as context increases; the paper suggests evaluating even longer sequences (1500/3000) as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Applied computer vision for sports is moderately mature (established detectors and tracking), but integration of long-range game-level sequence models for denoising is an active/emerging research area; the paper positions DST as a practical enhancement to existing STAD pipelines rather than a domain foundational breakthrough.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>The system uses confidence scores (probabilities) and applies a fixed 15% threshold for evaluation; no calibration analyses (e.g., reliability diagrams, expected calibration error) or quantitative calibration against proxy-to-ground-truth gap are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Yes — computational cascade: (1) TAAD: fast, short-window per-player visual detection (proxy predictions/logits) → (2) DST: sequence-level denoising using role-based ordering + game-state to produce candidate, denoised events → (3) human annotation operators validate and correct candidates (DST is explicitly presented as an assistant to human annotators). Errors propagate from stage 1 (visual false positives/negatives) into stage 2; DST reduces some errors by using broader context, but no formal error-propagation model is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reported limitations include: visual ambiguities due to occlusions, motion blur and rapid camera motion; missing tracking data (frequent for Header, Ball-block, Tackle classes) causing reduced performance; class imbalance (ball-block and tackle are minority classes ~1.6% of events) limiting learning; the short temporal window of TAAD (T=50) which misses longer-range context unless DST is used; interpolation of missing bounding boxes may be insufficient. The paper notes Precision plateaus beyond ~250 frames and that further improvements may require richer visual or tracking representations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Soccer-specific factors affecting proxy→ground-truth gap: frequent occlusions and close player proximity during key events (leading to missing or noisy tracking), rapid camera cuts and replays in broadcast footage, class imbalance (few instances of certain actions), and the tactical/temporal dependence of actions (many events require context across many seconds/minutes). The Markovian nature of many actions (dependence on recent state) both helps (short context often informative) and limits (requires correct state representation).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Track-Aware Action Detector (TAAD) <em>(Rating: 2)</em></li>
                <li>Seq2Event: Learning the Language of Soccer Using Transformer-based Match Event Prediction <em>(Rating: 2)</em></li>
                <li>Towards a foundation large events model for soccer <em>(Rating: 2)</em></li>
                <li>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension <em>(Rating: 1)</em></li>
                <li>Footovision STAD Dataset (Ochin et al.) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2216",
    "paper_id": "paper-278602711",
    "extraction_schema_id": "extraction-schema-60",
    "extracted_data": [
        {
            "name_short": "TAAD + DST",
            "name_full": "Track-Aware Action Detector (TAAD) prior predictions plus Denoising Sequence Transduction (DST) pipeline",
            "brief_description": "A two-stage pipeline where TAAD produces short-time-window, player-centric action logits (used as a computational proxy), and a Transformer-based DST model ingests those noisy logits plus role-based game-state features to generate denoised, game-level (who, what, when) event sequences that are compared to frame-level ground-truth annotations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TAAD + DST pipeline",
            "system_description": "First stage: TAAD (Track-Aware Action Detector) — a lightweight 3D-CNN/ROIAlign + temporal conv detector trained on short clips (T=50 frames) that outputs per-frame, per-player logits for K action classes (kept as raw logits). Second stage: DST — a Transformer encoder–decoder (6 layers each, 8 heads, hidden dim 512) that encodes sequences of length L (750 frames) composed of TAAD logits (role-ordered) + role-based game-state vectors + frame encodings and autoregressively decodes cleaned action tuples (action class, player role, frame index). The DST model is trained as a denoising sequence transducer to recover ground-truth action sequences from corrupted/context-free predictions.",
            "domain": "sports video analysis / spatio-temporal action detection",
            "proxy_metric_name": "TAAD per-frame, per-player action logits / low-threshold predictions (confidence scores)",
            "proxy_metric_description": "TAAD produces raw logits (no softmax applied) per frame, per player, per action class over T=50-frame clips. These logits are treated as computational predictions (proxy signals) of actions; later thresholded by confidence (evaluation uses a 15% confidence threshold) to yield discrete candidate detections. For DST input the raw logits are concatenated in a fixed role order rather than discretized.",
            "proxy_metric_type": "data-driven ML prediction (deep learning classifier outputs used as a surrogate objective / proxy)",
            "ground_truth_metric": "Frame-level, player-identified action annotations (human-labeled event class + player identity + frame index)",
            "ground_truth_description": "Manual frame-by-frame event annotations in the Full-length Game Footovision Dataset (299 full games) that specify action class and the identity of the acting player; matching between predicted and ground-truth events is performed when predicted and ground-truth share same player and class within a temporal tolerance +/- δ frames (δ = 12 or 25). Metrics (Precision and Recall) are computed by counting TP/FP/FN from that matching.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Direct quantitative comparison reported as Precision and Recall of TAAD (proxy-alone) vs TAAD + DST (denoised outputs) when evaluated against ground-truth annotations. Overall (δ=12): TAAD Precision = 43.56%, Recall = 66.97%; TAAD + DST (no game-state) Precision = 74.17%, Recall = 70.27%; TAAD + DST (with game-state) Precision = 78.77%, Recall = 75.80%. Absolute overall Precision gain from TAAD → DST (with game-state) = +35.21 percentage points; Recall gain = +8.83 percentage points. Class-level example (Pass): TAAD PR=62.67%, REC=71.17% → DST w/GS PR=83.07%, REC=79.80% (PR +20.40 pp, REC +8.63 pp).",
            "proxy_performance": "TAAD evaluated vs ground-truth: overall Precision 43.56%, Recall 66.97% (confidence threshold 15%, δ=12 frames). Per-class reported in Table 1 (examples): Pass PR=62.67% REC=71.17%; Ball-drive PR=37.56% REC=68.07%; Header PR=26.94% REC=49.44%.",
            "ground_truth_performance": "DST outputs evaluated vs same ground-truth annotations: overall Precision 78.77%, Recall 75.80% (with game-state, confidence threshold 15%, δ=12). DST without game-state: overall Precision 74.17%, Recall 70.27%. Per-class improvements listed in Table 1 (e.g., Pass PR=83.07% REC=79.80%).",
            "false_positive_rate": "Not given explicitly as FP/(all negatives); but from reported Precision one may compute FP fraction among positive predictions: TAAD false-positive fraction among positives ≈ 1 - 0.4356 = 56.44%; TAAD + DST (with game-state) ≈ 1 - 0.7877 = 21.23% (i.e., relative reduction in FP fraction among predicted positives of ~35.21 pp).",
            "false_negative_rate": "From reported Recall: TAAD FN fraction among ground-truth events ≈ 1 - 0.6697 = 33.03%; TAAD + DST (with game-state) FN fraction ≈ 1 - 0.7580 = 24.20% (absolute reduction ≈ 8.83 pp).",
            "novelty_characterization": "In-distribution / incremental methodological advance: DST is presented as an architectural / representation improvement that denoises ML-generated proxies using additional structured context; evaluation is on held-out games from the same dataset (i.e., near-training-distribution generalization rather than far-OOD discovery).",
            "gap_varies_with_novelty": null,
            "gap_variation_details": "",
            "gap_reduction_method": "Architectural/representation-based bias-correction: denoising sequence transduction using a Transformer that (1) enforces role-based ordering to remove permutation invariance, (2) ingests role-ordered TAAD logits (raw proxies) rather than discretized labels, (3) augments inputs with structured game-state features (player positions, velocities, visibility) and long temporal context (100/250/750 frames), and (4) is trained with cross-entropy losses on class, role, and frame outputs. These design choices serve as surrogate-to-ground-truth gap reduction techniques (not statistical calibration).",
            "gap_reduction_effectiveness": "Quantified in Table 1 (δ=12): switching from TAAD to DST (no game-state) increases overall Precision from 43.56% → 74.17% (+30.61 pp) and Recall 66.97% → 70.27% (+3.30 pp). Adding game-state yields further gain to Precision 78.77% (+4.60 pp) and Recall 75.80% (+5.53 pp versus DST without game-state). Temporal context experiments show performance improves with longer L (100→250→750 frames) with diminishing returns for Precision beyond 250 frames (see Table 2).",
            "validation_cost_comparison": "Not quantified in dollars/time; qualitative discussion: DST is trainable on a single consumer-grade / entry-level professional GPU (RTX 3090 or RTX A6000) and intended to operate in a low-threshold, high-recall mode to assist human annotators, implying proxy evaluation (automatic detector inference) is far cheaper than manual ground-truth annotation, but explicit cost comparisons are not provided.",
            "temporal_validation": "No long-term (multi-hour/days) temporal validation of predictions vs ground-truth is reported; experiments vary sequence context length (100, 250, 750 frames) and show improved short-to-mid-term agreement with ground-truth as context increases; the paper suggests evaluating even longer sequences (1500/3000) as future work.",
            "domain_maturity": "Applied computer vision for sports is moderately mature (established detectors and tracking), but integration of long-range game-level sequence models for denoising is an active/emerging research area; the paper positions DST as a practical enhancement to existing STAD pipelines rather than a domain foundational breakthrough.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "The system uses confidence scores (probabilities) and applies a fixed 15% threshold for evaluation; no calibration analyses (e.g., reliability diagrams, expected calibration error) or quantitative calibration against proxy-to-ground-truth gap are reported.",
            "multiple_proxies": false,
            "proxy_correlation": "",
            "validation_cascade": "Yes — computational cascade: (1) TAAD: fast, short-window per-player visual detection (proxy predictions/logits) → (2) DST: sequence-level denoising using role-based ordering + game-state to produce candidate, denoised events → (3) human annotation operators validate and correct candidates (DST is explicitly presented as an assistant to human annotators). Errors propagate from stage 1 (visual false positives/negatives) into stage 2; DST reduces some errors by using broader context, but no formal error-propagation model is provided.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Reported limitations include: visual ambiguities due to occlusions, motion blur and rapid camera motion; missing tracking data (frequent for Header, Ball-block, Tackle classes) causing reduced performance; class imbalance (ball-block and tackle are minority classes ~1.6% of events) limiting learning; the short temporal window of TAAD (T=50) which misses longer-range context unless DST is used; interpolation of missing bounding boxes may be insufficient. The paper notes Precision plateaus beyond ~250 frames and that further improvements may require richer visual or tracking representations.",
            "domain_specific_factors": "Soccer-specific factors affecting proxy→ground-truth gap: frequent occlusions and close player proximity during key events (leading to missing or noisy tracking), rapid camera cuts and replays in broadcast footage, class imbalance (few instances of certain actions), and the tactical/temporal dependence of actions (many events require context across many seconds/minutes). The Markovian nature of many actions (dependence on recent state) both helps (short context often informative) and limits (requires correct state representation).",
            "uuid": "e2216.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Track-Aware Action Detector (TAAD)",
            "rating": 2
        },
        {
            "paper_title": "Seq2Event: Learning the Language of Soccer Using Transformer-based Match Event Prediction",
            "rating": 2
        },
        {
            "paper_title": "Towards a foundation large events model for soccer",
            "rating": 2
        },
        {
            "paper_title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "rating": 1
        },
        {
            "paper_title": "Footovision STAD Dataset (Ochin et al.)",
            "rating": 2
        }
    ],
    "cost": 0.0114965,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos
14 May 2025</p>
<p>Jeremie Ochin jeremie.ochin@minesparis.psl.eu 
Centre for Robotics
Mines Paris -PSL
France</p>
<p>Footovision
ParisFrance</p>
<p>Raphael Chekroun raphael.chekroun@footovision.com 
Footovision
ParisFrance</p>
<p>Bogdan Stanciulescu bogdan.stanciulescu@minesparis.psl.eu 
Centre for Robotics
Mines Paris -PSL
France</p>
<p>Sotiris Manitsaris sotiris.manistsaris@minesparis.psl.eu 
Centre for Robotics
Mines Paris -PSL
France</p>
<p>Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos
14 May 2025C0B836EDCF92E0872F51003F1EDCF124arXiv:2505.09455v1[cs.CV]Spatio-Temporal Action DetectionSequence TransductionVideo Action RecognitionSport Video UnderstandingSports AnalyticsSoccer Analytics
State-of-the-art spatio-temporal action detection (STAD) methods show promising results for extracting soccer events from broadcast videos.However, when operated in the high-recall, low-precision regime required for exhaustive event coverage in soccer analytics, their lack of contextual understanding becomes apparent: many false positives could be resolved by considering a broader sequence of actions and game-state information.In this work, we address this limitation by reasoning at the game level and improving STAD through the addition of a denoising sequence transduction task.Sequences of noisy, context-free player-centric predictions are processed alongside clean game state information using a Transformer-based encoder-decoder model.By modeling extended temporal context and reasoning jointly over team-level dynamics, our method leverages the "language of soccer"-its tactical regularities and inter-player dependencies-to generate "denoised" sequences of actions.This approach improves both precision and recall in low-confidence regimes, enabling more reliable event extraction from broadcast video and complementing existing pixel-based methods.</p>
<p>Introduction</p>
<p>Fully automating the production of high-quality soccer analytics from broadcast video remains a challenge.While the past decade has brought major advances in computer vision and machine learning-enabling reliable player tracking, identification, and pitch localization-fine-grained event annotation remains largely a manual, time-consuming process carried out by expert operators who must scrub through footage to accurately label actions [1].Spatio-temporal action detection (STAD) methods, operated at low confidence thresholds to maximize recall, are well-suited to assist annotation operators by proposing candidate actions for validation.</p>
<p>Fig. 1.Overview of our method.A STAD model first produces per-frame, per-player action predictions over a long sequence.These predictions are concatenated in a consistent role-based order, using metadata.Each token is constructed by combining these structured prior predictions with additional game-state features such as player positions and velocities.The resulting sequence of tokens is fed into an encoder-decoder Transformer, which is trained to auto-regressively generate cleaned action sequences-predicting, for each action, its class, associated player, and frame number.Our method emphasizes the importance of both temporal and inter-player context, leveraging a wide temporal window and structured game-state information.</p>
<p>STAD aims to identify when an action occurs, what it is, and who performed it-or where.In practice, it involves detecting and classifying player actions in untrimmed video, producing temporally grounded "action tubes" for events such as ball drives, passes, crosses, shots, headers, throw-ins, ball blocks, and tackles ("on-the-ball events").</p>
<p>Despite notable improvements in STAD accuracy [2], key limitations remain.Some stem from the nature of the game and its recording conditions-occlusions, motion blur, rapid camera movements, and frequent visual ambiguities.Another frequent constraint is the limited computational budget available for training or inference, which often forces fine-grained action recognition models to process only short clips (16-64 frames).However, this approach overlooks the contextual richness of soccer, where the relevance and tactical significance of an action are strongly influenced by surrounding players and the current phase of play.This limitation is especially apparent in high-recall settings, where many false positives could be avoided with access to long-range temporal and game-state context.</p>
<p>To address these limitations, we augment existing STAD models with a "denoising" sequence transduction task.We process long sequences of context-free player-centric action predictions-alongside structured game-state data (e.g., player positions, velocities, team IDs)-using a transformer-based encoder-decoder (Figure 1).Inspired by natural language processing pretraining strategies, our model learns to reconstruct "clean", coherent and tactically plausible action sequences from corrupted inputs, akin to translating noisy text into fluent language.</p>
<p>In doing so, our method bridges two previously separate strands of research: (i) image-based STAD and (ii) sequence modeling of game events.By modeling the "language of soccer"-capturing the tactical, temporal, and spatial dependencies between actions-our approach goes beyond conventional detection and remains agnostic to the underlying detection model.This enhances robustness in real-world settings, where visual noise and ambiguity are common.</p>
<p>We evaluate our approach using a dataset of 299 full-length games with broadcast footage, tracking data, and player metadata.Designed for real-world use, our model is trainable on a single consumer-grade or entry-level professional GPU.</p>
<p>This paper makes three main contributions.We first introduce a novel formulation of spatio-temporal action detection (STAD) as a denoising sequence transduction task, leveraging structured game-state context to guide the reconstruction of action sequences.We then propose a transformer-based model that jointly encodes noisy visual predictions and player-centric features to produce coherent, tactically plausible action sequences-enabling reasoning beyond raw pixel data.Finally, we provide empirical evidence that our approach improves detection performance in high-recall settings, while remaining computationally efficient.</p>
<p>This work offers a new perspective on soccer video analysis by demonstrating how integrating prior image-based predictions with structured context-rich representations of game dynamics can improve the accuracy and coherence of event detection.</p>
<p>Related Work</p>
<p>Spatio-Temporal Action Detection (STAD)</p>
<p>Numerous techniques have been developed for STAD in recent years.These approaches can be categorized into frame-level and clip-level models [3].The frame-level models predict the bounding box and action type for each frame and then integrate these predictions.Conversely, clip-level methods, also called action tubelet detectors, endeavour to model both temporal context and action localization.A comprehensive review is provided in the survey by Wang et al. [2].</p>
<p>Among the clip-level methods, Track-Aware Action Detector (TAAD) [4] yields per-actor, per-frame action predictions by first detecting and tracking players, and then aggregating features along player trajectories using a fine-tuned 3D CNN and ROI Align [5], followed by a Temporal Convolutional Network.TAAD demonstrates state-of-the-art performance on public STAD benchmarks and exhibits robustness to camera motion.</p>
<p>This method is particularly well-suited for soccer analytics, where tracking, identification, and position estimation typically precede event annotation.With TAAD, actions are naturally linked to existing player tracklets and identities, which is important since soccer analytics often requires the construction of statistics on a per-player basis.</p>
<p>Generative Models for Soccer Action Sequences</p>
<p>Traditional approaches have relied heavily on Markov models [6,7], which align well with soccer's structure as a sequence of discrete, observable states, such as possession changes, spatial transitions and goals.</p>
<p>Recent work has drawn inspiration from generative models to better capture the sequential and contextual structure of soccer.Simpson et al. [8] used RNNs and transformers to jointly predict the location and type of the next action from 40 prior events, outperforming Markovian baselines.Mendes-Neves et al. [9] introduced a Large Event Model trained on the previous three actions to predict the next action's position, type, and success, enabling downstream tasks such as tactical analysis and performance evaluation.Baron et al. [10] framed nextaction prediction as a classification task over tokenized events using a transformer decoder, improving temporal modeling over prior MLP-based approaches.</p>
<p>These models contribute to the advancement of generative modeling of soccer action sequences and highlight the existence of an underlying structure, often referred to as the "language of soccer".However, they do not incorporate playerspecific information and are not designed to identify the actor of the next event.As a result, they fall short of explicitly modeling who will perform the next action, even though they may predict what, where, and when it will occur.</p>
<p>Denoising Sequence-to-Sequence Pre-training for Natural Language Processing</p>
<p>Self-supervised learning has proven effective for a wide range of NLP tasks, offering a framework for learning general-purpose representations without labeled data [11].Among these, denoising autoencoders have emerged as an effective approach for generative sequence modeling, wherein a model is trained to reconstruct original text from corrupted inputs [11].BART (Bidirectional and Auto-Regressive Transformers) [12] is a sequence-to-sequence model that combines a bidirectional encoder with an autoregressive decoder, trained as a denoising autoencoder.This task enables the model to learn rich representations that capture both global structure and local dependencies.</p>
<p>Drawing on the core idea behind BART, we treat context-free sequences of soccer action predictions as "corrupted inputs" and train a model to recover coherent and contextually plausible event sequences.While BART introduces noise to textual data through token masking, deletion, insertion, or reordering, we extend this principle to sequences of spatio-temporal action predictions produced by a model that can only attend frames within a short time window, and we enrich these inputs with game-state information.This framing allows us to view our model as learning the "language of soccer," where situated actions-like words-form structured sequences shaped by tactical and temporal dynamics.By training it to correct noisy, context-free predictions using broader game context, we enable the model to generate more consistent, player-specific, context-aware action sequences, analogous to how BART reconstructs fluent text from noised input.</p>
<p>Methodology</p>
<p>Dataset</p>
<p>We built and used two datasets: one for the STAD detector, and one for the "denoising" sequence transduction method.</p>
<p>Footovision STAD Dataset This is an "on-the-ball events" dataset composed of 20000 videos clips from 997 different and diverse football games, with a minimum of 2500 samples per class.Clips are at least 3 seconds long, and the frame rate is consistent at 25 frames per second.There are 8 classes of action: balldrive, pass, cross, header, throw-in, shot, tackle and ball-block.The sequences are sampled around randomly selected events, ensuring no overlap between clips.Each game was associated with either the training set or the test set, ensuring no contamination between them.We refer the reader to Ochin et al. [1] for further information about this dataset.</p>
<p>Full-length Game Footovision Dataset</p>
<p>The dataset comprises 299 fulllength game videos selected from a total of 997, each enriched with frame-byframe tracking data, team formations, player positions, and simplified playerto-role assignments clustered into 13 categories (e.g., Goalkeeper, Full Back, Winger, Defensive Midfielder).</p>
<p>Additional metadata is provided, along with detailed frame-by-frame event annotations specifying both the action class and the identity of the player performing the action.The mapping between player identity and positional / tracking data is also included.The dataset splitting is consistent with one from the Footovision STAD Dataset.</p>
<p>All data is available for the full duration of each video.Tracking data is recorded continuously, including during game pauses, and some annotated events may occur during broadcast replays.</p>
<p>Prior Predictions -Short time-window</p>
<p>A lightweight implementation of the Track-Aware Action Detector (TAAD) is trained on the Footovision STAD dataset, as described in [1].The trained model is then applied to all 299 videos in the Full-length Game Footovision Dataset, using tracking data in the form of player bounding box sequences.Inference is performed by dividing each video into adjacent segments of T = 50 frames.</p>
<p>Missing bounding boxes-due to occlusion or tracking failure-are interpolated if the gap does not exceed three seconds.Otherwise, a fixed "dummy" bounding box is provided, along with a binary mask indicating its absence.This mask is used to zero out the corresponding ROI Align features before aggregation.</p>
<p>For a clip of T frames, with N players and K action classes, the output of TAAD is a tensor of shape R T ×N ×K , without softmax activation (the logits).To reduce border effects, inference is performed twice per video, with a T 2 frame offset between passes.The resulting tensors are temporally aligned and averaged.</p>
<p>While typical pipelines post-process these predictions before thresholding (e.g., with label smoothing [1,13,14,15]), we opt to retain the raw logits.We hypothesize that this preserves richer information, allowing the model to learn patterns that might be lost through early discretization.Final tensors are stored without any post-processing.</p>
<p>Role-based Game-State Representation</p>
<p>Unlike standard STAD methods that predict actions independently for each player, we adopt an approach that reasons at the game level, across both teams, and outputs sequences of structured predictions as tuples (who, what, when).To build this unified, structured representation of the full game state, it is necessary to address the player permutation problem [16,17].For example, there are 11! (almost 40 million) possible ways to order the positional features of 11 players, all encoding equivalent information about the team.We resolve this by enforcing a consistent, role-based concatenation order across all games.</p>
<p>For each team, players are ordered as follows: Goalkeeper, Left Back, Left Center Back, Mid Center Back, Right Center Back, Left Midfielder, Right Midfielder, Defensive Midfielder, Attacking Midfielder, Left Winger, Right Winger, Central Forward, and Right Back.Our game-state vector at each frame is constructed by concatenating individual player features-normalized position, velocity, and visibility-following this predefined role order, starting with the team attacking from the left (with pitch coordinates originating at the top-left, from the broadcast view).</p>
<p>To ensure fixed dimensionality despite substitutions, exclusions, or role mismatches (13 roles vs. 11 players), we represent the game-state for each frame as a vector in R 26 * D , where D is the per-player feature dimension.Unoccupied roles are padded with a constant value of -15.0, well outside the valid feature range.This same role-based ordering is also applied to the unnormalized, context-free predictions (logits) generated by TAAD, as illustrated in Figure 1.</p>
<p>Denoising Sequence Transduction Model (DST)</p>
<p>We implement a Transformer encoder-decoder architecture following the design proposed by Vaswani et al. [18], using 6 encoder and decoder layers, 8 attention heads, and a hidden dimension of 512.The encoder receives sequences of fixed length L = 750 tokens (one token per frame).This sequence is constructed by concatenating the following components on the feature dimension:</p>
<p>Context-free predictions: R L×234 Game-state vectors: R L×130 Frame index (one-hot): R L×(L+2)</p>
<p>Here, the dimension 234 corresponds to 9 action classes across 26 roles, and 130 corresponds to 5 features per role (position x, y, velocity v x , v y , and visibility).The frame number encoding includes two additional tokens for start-ofsequence (SOS) and end-of-sequence (EOS), resulting in L + 2 dimensions.We use a sinusoidal positional encoding as described in [18].</p>
<p>During training, the decoder receives target action sequences of variable length L'.This sequences of token is constructed by concatenating the following components on the feature dimension: Action class (one-hot): R L ′ ×10 Player role (one-hot): R L ′ ×26 Frame index (one-hot): R L ′ ×L+2</p>
<p>The action class vector includes 8 classes, plus 2 to encode the SOS and EOS tokens.The player role vector encodes 13 roles per team (26 total), and the frame index is one-hot encoded over the same temporal window with SOS and EOS.</p>
<p>The decoder attends to the encoder output via cross-attention and is trained with a causal mask to predict the next token in the sequence.Three cross-entropy loss functions are used-one for each component of the action prediction: class, role, and frame number.The SOS and EOS tokens are represented using special action class values and correspond, respectively, to frame 0 and frame L + 1.</p>
<p>At inference time, the "clean" action sequence is generated autoregressively: the input sequence is encoded once, and the decoder is called recursively, each time feeding it the previous prediction (class, role, frame), beginning with the SOS token and stopping when the EOS token is generated.</p>
<p>Implementation details</p>
<p>We used T = 50 frames as input for the TAAD, with no sub-sampling (2 seconds of video clip), and L = 750 frames for the transduction task.We trained the DST with the AdamW optimizer [19], with a learning rate of 0.00025, a weight decay of 1e-05 on the non-bias parameters and a batch size of varying size (40 to 64) depending on the GPUs (RTX 3090 and RTX A6000 GPU) and experiments.We trained the system for a total of 6 epochs and divided the learning rate by 10 at epoch 3.For each epoch, 500 sequences of 750 frames were randomly sampled in each of the 240 full-length games of the training set.</p>
<p>Evaluation</p>
<p>Metrics</p>
<p>The predictions generated by the DST model are sequences of "discrete" predictions and do not require post-processing techniques such as label smoothing [1,13,14,15].Thus, metrics are computed directly by thresholding the confidence scores.We report overall and per-class Precision and Recall at a low threshold of 15%, reflecting the method's intended use as a tool to assist annotation operators by proposing candidate actions.This setup prioritizes high recall to minimize missed detections, with the goal of achieving the best possible precision in that regime.</p>
<p>Evaluation is performed by matching predicted and ground-truth actions for the same player and class within a fixed temporal window (+/δ around the annotation frame), with δ = 12 or 25 frames.A match is counted as a true positive (TP); unmatched predictions are false positives (FP), and unmatched ground-truth events are false negatives (FN).This matching procedure allows for the computation of Precision and Recall, both overall and per class.</p>
<p>Comparisons</p>
<p>To assess the benefit of reasoning at the game level, i.e. using role-based structured information, to denoise long sequences of detections in a high-recall, lowprecision regime, we compare several setups and conduct an ablation study on the use of game-state information.</p>
<p>Track-Aware Action Detector (TAAD): We apply label smoothing postprocessing [1,13,14,15] to the predictions generated by our baseline method.This model was trained on short clips of 50 frames containing only active gameplay and thus has no opportunity to learn when the game is paused (e.g., when the ball is out of play before a throw-in).In contrast, we expect our method to learn such contextual cues from game-state information.To ensure a fair comparison, we filter out detections that occur during these paused phases in the baseline predictions.</p>
<p>Denoising Sequence Transduction (TAAD + DST): We train two variants of the proposed DST model.The first uses both the prior predictions and the structured game-state representation; the second relies solely on the predictions, without access to player positions, velocities, or visibility.Both models are trained to denoise sequences of 750 frames.The results of this comparison are presented in Table 1.Additionally, to measure the influence of the temporal context on our method, we train 3 variants with different temporal coverage: 100, 250 and 750 frames.The results are presented in Table 2.</p>
<p>Discussion</p>
<p>The results in Table 1 demonstrate the separate contributions of both role-based structured information and game-state information.</p>
<p>First, we observe a clear benefit of reasoning at the game level: the absence of multiple simultaneous detections across players indicates that this approach naturally enforces the constraint that only one player can interact with the ball at any given time.More broadly, both Precision and Recall improve when transitioning from context-free, player-centric predictions to unified predictions based on a structured representation of the same underlying data.The ablation study further emphasizes the important role of game-state information in this improvement.We analyze the specific contributions to Precision and Recall in more detail below.</p>
<p>Gains in Precision:</p>
<p>We first notice that most of the improvement in Precision is achieved by transitioning from per-player predictions made by TAAD to game-level predictions using a role-based structured representation of the initial outputs, without any game-state information (i.e., no position, velocity, or visibility features).We attribute this to two main factors.First, the roles assigned to players implicitly encode stable relative positional information between players, making it likely that certain patterns (e.g., passes) emerge between specific roles, aligned with team strategies during different phases of play (e.g., buildup, progression, final third, offensive/defensive transitions).Second, game-level prediction encourages the model to select the most plausible action based on broader temporal context, even in cases where multiple visually ambiguous false positives would otherwise be produced.</p>
<p>The addition of game-state information yields a further 4.5% improvement in Precision, consistent with observations from previous work [1].</p>
<p>Gains in Recall: Given the extremely low confidence threshold used and the significant increase in Recall, we conclude that the method enables the recovery of "invisible" events that can only be inferred from context, rather than from visual features alone.Our ablation analysis suggests that approximately twothirds of this gain originates from the inclusion of game-state information, while the remaining one-third is due to switching to a game-level prediction based on role-based representations.</p>
<p>Influence of Temporal Coverage: As expected, both Precision and Recall benefit from a larger temporal context, with diminishing returns for Precision beyond 250 frames, suggesting a potential performance plateau.Further experiments are needed to determine whether Recall also plateaus beyond 750 frames.Considering that a typical phase of play in soccer lasts 2-3 minutes, it would be interesting to evaluate our method on longer sequences (e.g., 1500 or 3000 frames).We also observe substantial improvement even with only 100 frames of context, on average containing just two actions.A likely explanation lies in the Markovian hypothesis, widely used in soccer analytics: the next action in a game depends primarily on the current state and/or the most recent action.This aligns with the intuition that short-term cues, such as a player's immediate movement, ball proximity, or defensive pressure, carry significant predictive power.Many actions in soccer, like passes or tackles, are reactive and highly localized in time, meaning even limited context often includes the key triggers that precede an event.</p>
<p>Discrepancy in Performance Across Classes: We observe that Precision and Recall for the Header, Ball-block, and Tackle classes remain significantly lower than for other classes.These three classes are also those for which tracking data is frequently missing at the moment of the action, likely due to frequent occlusions or close proximity between players.Moreover, Ball-block and Tackle are two minority classes in our dataset, accounting for only 1.6% of all events.Developing a more elaborate method than bounding box interpolation may improve this issue and is left for future work.</p>
<p>Conclusion</p>
<p>This work presents a novel approach to spatio-temporal action detection in soccer, recasting the task as a denoising sequence transduction problem over long sequences of noisy, context-free player-centric action predictions.By structuring these predictions through role-based concatenation and incorporating gamestate features such as player positions, velocities, and visibility, we enable a transformer-based model to recover coherent, tactically plausible, context-aware sequences of actions.The results demonstrate substantial gains in both precision and recall, particularly in the high-recall regime, which is critical for supporting human annotators.Our DST model significantly enhances the performance of TAAD, demonstrating its potential as an effective assistant for annotation operators.Its ability to improve detections from a noisy and relatively low-dimensional input signal naturally raises the question of how much further performance could be improved using a richer and more task-agnostic representations.As a promising direction for future work, we plan to explore the integration of generic visual features extracted from off-the-shelf vision foundation models, potentially finetuned in a self-supervised manner on our dataset.Such representations could capture richer, higher-level semantics while remaining compatible with our current training setup, which does not require end-to-end optimization of the feature extractor.</p>
<p>Table 1 .
1
[4]parison of the Precision (PR) and Recall (REC) with a confidence score threshold of 15% and δ = 12 frames, between TAAD[4]and 2 variants of our method, with and without game-state information.Results are consistent with δ = 25.Metrics are ×10 2 .
ClassSamplesTAAD [4]TAAD + DSTTAAD + DSTwithout Game Statewith Game StatePRRECPRRECPRRECPass26,42862.671.179.174.583.079.8Ball-drive20,77237.568.074.071.679.677.5Header2,23226.949.444.944.146.345.9Cross1,29160.255.865.265.567.467.2Throw-in1,1495.824.251.747.868.565.3Ball-block 76410.544.535.822.338.229.6Shot66947.859.366.559.969.763.5Tackle1343.06.09.33.07.02.2Overall53,43943.566.974.170.278.775.8</p>
<p>Table 2 .
2
Influence of the temporal context on Precision (PR) and Recall (REC), all other things being equals.Metrics are ×10 2 .</p>
<p>Game State and Spatio-Temporal Action Detection in Soccer Using Graph Neural Networks and 3D Convolutional Networks. J Ochin, G Devineau, B Stanciulescu, S Manitsaris, Proceedings of the 14th International Conference on Pattern Recognition Applications and Methods. the 14th International Conference on Pattern Recognition Applications and MethodsICPRAM1</p>
<p>. 10.5220/00131611000039052025SciTePressPorto, Portugal</p>
<p>A survey on deep learning-based spatio-temporal action detection. P Wang, F Zeng, Y Qian, 10.1142/S0219691323500662International Journal of Wavelets, Multiresolution and Information Processing. 22423500662024</p>
<p>MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions. Yixuan Li, Lei Chen, Runyu He, Zhenzhi Wang, Gangshan Wu, Limin Wang, 10.1109/ICCV48922.2021.01328Proceedings of 2021 IEEE/CVF International Conference on Computer Vision (ICCV). 2021 IEEE/CVF International Conference on Computer Vision (ICCV)Montreal, QC, Canada2021</p>
<p>Spatio-temporal action detection under large motion. G Singh, V Choutas, S Saha, F Yu, L Van Gool, 10.1109/WACV56688.2023.00595Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)Waikoloa, HI, USAIEEE2023</p>
<p>Mask R-CNN. K He, G Gkioxari, P Dollár, R Girshick, 10.1109/ICCV.2017.322Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV). the 2017 IEEE International Conference on Computer Vision (ICCV)Venice, Italy2017</p>
<p>A Markovian model for association football possession and its outcomes. J L Pena, 10.48550/arXiv.1403.7993arXiv2014</p>
<p>A Markov Framework for Learning and Reasoning About Strategies in Professional Soccer. M Van Roy, P Robberechts, W Yang, L De Raedt, J Davis, 10.1613/jair.1.13934Journal of Artificial Intelligence Research. 772023</p>
<p>Seq2Event: Learning the Language of Soccer Using Transformer-based Match Event Prediction. I Simpson, R J Beal, D Locke, T J Norman, 10.1145/3534678.3539138Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data MiningWashington DC, USAAssociation for Computing Machinery2022</p>
<p>Towards a foundation large events model for soccer. T Mendes-Neves, L Meireles, J Mendes-Moreira, 10.1007/s10994-024-06606-yMachine Learning. 1132024</p>
<p>E Baron, D Hocevar, Z Salehe, 10.48550/arXiv.2407.14558A Foundation Model for Soccer. 2024</p>
<p>Pre-trained models for natural language processing: A survey. X Qiu, T Sun, Y Xu, 10.1007/s11431-020-1647-3Science China Technological Sciences. 632020</p>
<p>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Deep learning for detecting multiple space-time action tubes in videos. S Saha, G Singh, M Sapienza, P H Torr, F Cuzzolin, 10.48550/arXiv.1608.01529arXiv2016</p>
<p>Online Real-Time Multiple Spatiotemporal Action Localisation and Prediction. G Singh, S Saha, M Sapienza, P Torr, F Cuzzolin, Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV). the 2017 IEEE International Conference on Computer Vision (ICCV)</p>
<p>. Italy Venice, 10.1109/ICCV.2017.3932017</p>
<p>Action tubelet detector for spatio-temporal action localization. V Kalogeiton, P Weinzaepfel, V Ferrari, C Schmid, 10.48550/arXiv.1705.01861Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV). the 2017 IEEE International Conference on Computer Vision (ICCV)Venice, Italy2017</p>
<p>Large-Scale Analysis of Formations in Soccer. X Wei, L Sha, P Lucey, S Morgan, S Sridharan, 10.1109/DICTA.2013.6691503Proceedings of the 2013 International Conference on Digital Image Computing: Techniques and Applications (DICTA). the 2013 International Conference on Digital Image Computing: Techniques and Applications (DICTA)Hobart, TAS, Australia2013</p>
<p>Large-Scale Analysis of Soccer Matches Using Spatiotemporal Tracking Data. A Bialkowski, P Lucey, P Carr, Y Yue, S Sridharan, I Matthews, 10.1109/ICDM.2014.133Proceedings of the 2014 IEEE International Conference on Data Mining. the 2014 IEEE International Conference on Data MiningShenzhen, China2014</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems. the 31st International Conference on Neural Information Processing SystemsLong Beach, California, USACurran Associates Inc2017</p>
<p>Decoupled Weight Decay Regularization. I Loshchilov, F Hutter, Proceeding of the 2019 International Conference on Learning Representations, arXiv. eeding of the 2019 International Conference on Learning Representations, arXivNew Orleans, Louisiana, United States2019</p>            </div>
        </div>

    </div>
</body>
</html>
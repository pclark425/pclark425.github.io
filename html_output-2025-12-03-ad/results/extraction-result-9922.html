<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9922 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9922</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9922</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-7ace46ab8e71c4304682ab126b1212deb54b9b03</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7ace46ab8e71c4304682ab126b1212deb54b9b03" target="_blank">Style Over Substance: Evaluation Biases for Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This study investigates the behavior of crowd-sourced and expert annotators, as well as LLMs, when comparing outputs from different models, and proposes independently evaluating machine-generated text across multiple dimensions, resulting in the Multi-Elo Rating System (MERS).</p>
                <p><strong>Paper Abstract:</strong> As large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. Ranking the relative performance of LLMs based on Elo ratings, according to human judgment, is gaining more popularity. However, the extent to which humans and LLMs are capable evaluators remains uncertain. This study investigates the behavior of crowd-sourced and expert annotators, as well as LLMs, when comparing outputs from different models. To achieve this, we curate a dataset of intentionally flawed machine-generated answers. Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors. To address this issue, we propose independently evaluating machine-generated text across multiple dimensions, rather than merging all the evaluation aspects into a single score. We instantiate this idea with the Elo rating system, resulting in the Multi-Elo Rating System (MERS). Empirical results from our study reveal that this proposed approach significantly enhances the quality of LLM-based evaluations, particularly in terms of factual accuracy. However, there is no significant improvement in crowd-sourced-based evaluations, indicating the need for further investigation.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9922.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9922.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-judge vs Humans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavioral differences and evaluation biases when using Large Language Models as judges compared to human (crowd and expert) evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison of crowd-sourced annotators, expert annotators, and LLM judges (GPT-4 and Claude-1) on pairwise ranking of model outputs using Elo; documents systematic differences in sensitivity to factual/language errors, decisiveness, and biases (length, order), and evaluates a multi-dimensional Elo (MERS) remedy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended question answering / general-purpose QA (pairwise comparison of LLM-generated answers)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (gpt-4-0613) and Claude-1 (Claude-1.3)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLMs judged pairwise comparisons of answers using the Dettmers et al. evaluation prompt (assessing helpfulness, relevance, accuracy, and detail); all 5280 unique pairwise comparisons across 12 answer-settings × 40 questions were evaluated, with both forward and reversed orderings; for MERS, GPT-4 was queried separately per dimension (Accuracy, Helpfulness, Language) in most reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Crowd-sourced: 377 US-based AMT workers (≥5,000 tasks completed, ≥99% approval), limited to ≤20 annotations each, mandatory 20s reading delay, anonymized outputs; Expert: 20 annotators (≥master's degree from English-speaking countries) annotated 200 sampled games; both used the same pairwise interface and instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa for pairwise inter-annotator agreement (Table 3): Crowd-Expert κ=0.08, Crowd-GPT-4 κ=0.11, Crowd-Claude κ=0.10, Expert-GPT-4 κ=0.09, Expert-Claude κ=0.14, GPT-4–Claude κ=0.51. The paper also reports that humans assign ties much more often than LLMs (figures and Elo score spread summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs as judges changes the nature of evaluation rather than faithfully replicating human judgment: (1) Loss of human-style indecisiveness/diversity — LLMs are more decisive and less likely to return ties; (2) Different bias patterns — LLM judges show stronger preference for longer answers and are sensitive to answer ordering (favor Assistant 1), which can make them prefer longer but factually flawed responses; (3) Mismatch in fact-checking behavior — crowd humans rarely fact-check unless errors are glaring, experts do better; LLMs detect some factual errors but imperfectly and can still favor flawed outputs due to length/detail bias; (4) Loss of human judgment variability and certain qualitative sensitivities (e.g., experts favor different ordering), meaning LLM-judged rankings can diverge systematically from what human audiences (especially crowds) would produce.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Specific failures reported: (a) GPT-4 rated 'Several Minor Factual Errors' (Elo 1206) higher than 'Correct + Short' (Elo 1096) and sometimes ranked 'Several Major Factual Errors' above 'Correct + Short' despite detecting factual errors; (b) Both LLMs and humans biased toward longer text, with GPT-4 showing the strongest length bias (Figure 2, Table 1); (c) Order bias: crowd, GPT-4 and Claude favored Assistant 1 while experts favored Assistant 2 (Figure 3); (d) Crowd annotators produced Elo ranges near 1000 (926–1086) showing indecisiveness, while LLM Elo ranges were much wider, indicating over-certainty; (e) Cohen's kappa shows only moderate agreement between GPT-4 and Claude (0.51) and only slight agreement for other human–LLM pairs (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Cases where LLM judges perform well or outperform humans: (1) GPT-4 and Claude achieve moderate mutual agreement and are consistent with each other more than with humans; (2) GPT-4 can effectively rank models by severity of factual errors when prompted to evaluate accuracy separately (MERS separate prompts improve factual accuracy judgments); (3) GPT-4 distinguishes language quality (spelling/grammar) more reliably than human evaluators and penalizes grammatical errors strongly; (4) Expert humans outperform crowd workers on fact-checking—so LLMs are not strictly worse than all humans and can complement expert judgments when properly prompted and when evaluation is decomposed by dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Core comparisons and findings: Table 1 (single Elo scores by judge), Section 3 (analysis: length bias, indecisiveness, order effect, fact-checking differences), Table 3 (Cohen's kappa agreement), Table 4 and Section 4.3 (MERS per-dimension results and Experts versus GPT-4), Figures 2–3 (bias visualizations), Conclusion and Discussion sections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Style Over Substance: Evaluation Biases for Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>All that's 'human' is not gold: Evaluating human evaluation of generated text <em>(Rating: 2)</em></li>
                <li>Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality <em>(Rating: 1)</em></li>
                <li>Qlora: Efficient finetuning of quantized llms <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9922",
    "paper_id": "paper-7ace46ab8e71c4304682ab126b1212deb54b9b03",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LLM-as-judge vs Humans",
            "name_full": "Behavioral differences and evaluation biases when using Large Language Models as judges compared to human (crowd and expert) evaluators",
            "brief_description": "Empirical comparison of crowd-sourced annotators, expert annotators, and LLM judges (GPT-4 and Claude-1) on pairwise ranking of model outputs using Elo; documents systematic differences in sensitivity to factual/language errors, decisiveness, and biases (length, order), and evaluates a multi-dimensional Elo (MERS) remedy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-ended question answering / general-purpose QA (pairwise comparison of LLM-generated answers)",
            "llm_judge_model": "GPT-4 (gpt-4-0613) and Claude-1 (Claude-1.3)",
            "llm_judge_setup": "LLMs judged pairwise comparisons of answers using the Dettmers et al. evaluation prompt (assessing helpfulness, relevance, accuracy, and detail); all 5280 unique pairwise comparisons across 12 answer-settings × 40 questions were evaluated, with both forward and reversed orderings; for MERS, GPT-4 was queried separately per dimension (Accuracy, Helpfulness, Language) in most reported experiments.",
            "human_evaluation_setup": "Crowd-sourced: 377 US-based AMT workers (≥5,000 tasks completed, ≥99% approval), limited to ≤20 annotations each, mandatory 20s reading delay, anonymized outputs; Expert: 20 annotators (≥master's degree from English-speaking countries) annotated 200 sampled games; both used the same pairwise interface and instructions.",
            "agreement_metric": "Cohen's kappa for pairwise inter-annotator agreement (Table 3): Crowd-Expert κ=0.08, Crowd-GPT-4 κ=0.11, Crowd-Claude κ=0.10, Expert-GPT-4 κ=0.09, Expert-Claude κ=0.14, GPT-4–Claude κ=0.51. The paper also reports that humans assign ties much more often than LLMs (figures and Elo score spread summaries).",
            "losses_identified": "Using LLMs as judges changes the nature of evaluation rather than faithfully replicating human judgment: (1) Loss of human-style indecisiveness/diversity — LLMs are more decisive and less likely to return ties; (2) Different bias patterns — LLM judges show stronger preference for longer answers and are sensitive to answer ordering (favor Assistant 1), which can make them prefer longer but factually flawed responses; (3) Mismatch in fact-checking behavior — crowd humans rarely fact-check unless errors are glaring, experts do better; LLMs detect some factual errors but imperfectly and can still favor flawed outputs due to length/detail bias; (4) Loss of human judgment variability and certain qualitative sensitivities (e.g., experts favor different ordering), meaning LLM-judged rankings can diverge systematically from what human audiences (especially crowds) would produce.",
            "examples_of_loss": "Specific failures reported: (a) GPT-4 rated 'Several Minor Factual Errors' (Elo 1206) higher than 'Correct + Short' (Elo 1096) and sometimes ranked 'Several Major Factual Errors' above 'Correct + Short' despite detecting factual errors; (b) Both LLMs and humans biased toward longer text, with GPT-4 showing the strongest length bias (Figure 2, Table 1); (c) Order bias: crowd, GPT-4 and Claude favored Assistant 1 while experts favored Assistant 2 (Figure 3); (d) Crowd annotators produced Elo ranges near 1000 (926–1086) showing indecisiveness, while LLM Elo ranges were much wider, indicating over-certainty; (e) Cohen's kappa shows only moderate agreement between GPT-4 and Claude (0.51) and only slight agreement for other human–LLM pairs (Table 3).",
            "counterexamples_or_caveats": "Cases where LLM judges perform well or outperform humans: (1) GPT-4 and Claude achieve moderate mutual agreement and are consistent with each other more than with humans; (2) GPT-4 can effectively rank models by severity of factual errors when prompted to evaluate accuracy separately (MERS separate prompts improve factual accuracy judgments); (3) GPT-4 distinguishes language quality (spelling/grammar) more reliably than human evaluators and penalizes grammatical errors strongly; (4) Expert humans outperform crowd workers on fact-checking—so LLMs are not strictly worse than all humans and can complement expert judgments when properly prompted and when evaluation is decomposed by dimension.",
            "paper_reference": "Core comparisons and findings: Table 1 (single Elo scores by judge), Section 3 (analysis: length bias, indecisiveness, order effect, fact-checking differences), Table 3 (Cohen's kappa agreement), Table 4 and Section 4.3 (MERS per-dimension results and Experts versus GPT-4), Figures 2–3 (bias visualizations), Conclusion and Discussion sections.",
            "uuid": "e9922.0",
            "source_info": {
                "paper_title": "Style Over Substance: Evaluation Biases for Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "All that's 'human' is not gold: Evaluating human evaluation of generated text",
            "rating": 2
        },
        {
            "paper_title": "Vicuna: An opensource chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality",
            "rating": 1
        },
        {
            "paper_title": "Qlora: Efficient finetuning of quantized llms",
            "rating": 1
        }
    ],
    "cost": 0.01017975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Style Over Substance: Evaluation Biases for Large Language Models</h1>
<p>Minghao $\mathbf{W u}^{1,2 *} \quad$ Alham Fikri $\mathbf{A j i}^{1}$<br>${ }^{1}$ Mohamed bin Zayed University of Artificial Intelligence ${ }^{2}$ Monash University<br>{minghao.wu, alham.fikri}@mbzuai.ac.ae</p>
<h4>Abstract</h4>
<p>As large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. Ranking the relative performance of LLMs based on Elo ratings, according to human judgment, is gaining more popularity. However, the extent to which humans and LLMs are capable evaluators remains uncertain. This study investigates the behavior of crowd-sourced and expert annotators, as well as LLMs, when comparing outputs from different models. To achieve this, we curate a dataset of intentionally flawed machine-generated answers. Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors. To address this issue, we propose independently evaluating machine-generated text across multiple dimensions, rather than merging all the evaluation aspects into a single score. We instantiate this idea with the Elo rating system, resulting in the Multi-Elo Rating System (MERS). Empirical results from our study reveal that this proposed approach significantly enhances the quality of LLM-based evaluations, particularly in terms of factual accuracy. However, there is no significant improvement in crowd-sourced-based evaluations, indicating the need for further investigation.</p>
<h2>1 Introduction</h2>
<p>Recent advancements in the field of natural language processing have demonstrated that the utilization of supervised instruction fine-tuning and reinforcement learning from human feedback (RLHF) can yield substantial improvements in the performance of large language models (LLMs) with respect to their ability to comprehend and execute instructions (Ouyang et al., 2022; Wei et al., 2022; Sanh et al., 2022; Chung et al., 2022; OpenAI, 2023; Wu et al., 2023; Li et al., 2023a; Lyu</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2023). This progress signifies a significant stride in the domain of language model development. However, the assessment of these enhanced LLMs presents a notable challenge, particularly when confronted with more generalized instructions that entail open-ended responses. Such instructions often lack a definitive metric for evaluation within the confines of traditional natural language processing benchmarks. In response to this challenge, recent studies commonly introduce an evaluation approach, namely the Elo rating system (Elo, 1967), wherein either human or LLM judges are enlisted to adjudicate between two LLMgenerated outputs (Askell et al., 2021; Bai et al., 2022a; Srivastava et al., 2022; Dettmers et al., 2023; Zheng et al., 2023). This evaluation method enables the computation of an Elo-based leaderboard to rank the relative performance of LLMs (Chiang et al., 2023). ${ }^{1}$ Nonetheless, an important question arises concerning the qualifications of human and LLM judges to serve as effective evaluators in this context. Evaluating model outputs encompasses a multifaceted decision-making process, and it remains an open question whether these judges possess the expertise to accurately determine the superior model output. Further research is needed to address this inquiry comprehensively and refine the evaluation procedures for enhanced LLMs.</p>
<p>In this study, we systematically generate a set of responses, considering factors such as language proficiency, factual accuracy, and response length. We employ 40 general-purpose questions sourced from (Chiang et al., 2023) that do not require specialized expertise to ensure the generalization of our study and reduce annotation difficulty. The answers for these questions are generated by GPT-4 with specific instructions. To probe the potential impact of language proficiency towards human and LLM judgments, we instruct GPT-4 to emulate</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Answer Features</th>
<th></th>
<th></th>
<th>Elo Ratings</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td># of words</td>
<td>Language Errors</td>
<td># of Factual Errors</td>
<td>Human</td>
<td></td>
<td>GPT-4</td>
<td>Claude-1</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Crowd</td>
<td>Expert</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Correct</td>
<td>$\approx 100$</td>
<td>N.A.</td>
<td>0</td>
<td>1091</td>
<td>1162</td>
<td>1482</td>
<td>1320</td>
<td></td>
</tr>
<tr>
<td>$\star$ Short</td>
<td>$\approx 50$</td>
<td>N.A.</td>
<td>0</td>
<td>970</td>
<td>1029</td>
<td>1096</td>
<td>1052</td>
<td></td>
</tr>
<tr>
<td>One Minor Factual Error</td>
<td>$\approx 100$</td>
<td>N.A.</td>
<td>1, minor</td>
<td>1074</td>
<td>1137</td>
<td>1415</td>
<td>1265</td>
<td></td>
</tr>
<tr>
<td>$\star$ Short</td>
<td>$\approx 50$</td>
<td>N.A.</td>
<td>1, minor</td>
<td>1002</td>
<td>964</td>
<td>988</td>
<td>997</td>
<td></td>
</tr>
<tr>
<td>Several Minor Factual Errors</td>
<td>$\approx 100$</td>
<td>N.A.</td>
<td>$\approx 3$, minor</td>
<td>1032</td>
<td>1024</td>
<td>1206</td>
<td>1182</td>
<td></td>
</tr>
<tr>
<td>$\star$ Short</td>
<td>$\approx 50$</td>
<td>N.A.</td>
<td>$\approx 3$, minor</td>
<td>952</td>
<td>873</td>
<td>851</td>
<td>891</td>
<td></td>
</tr>
<tr>
<td>Several Major Factual Errors</td>
<td>$\approx 100$</td>
<td>N.A.</td>
<td>$\approx 3$, major</td>
<td>1025</td>
<td>892</td>
<td>861</td>
<td>979</td>
<td></td>
</tr>
<tr>
<td>$\star$ Short</td>
<td>$\approx 50$</td>
<td>N.A.</td>
<td>$\approx 3$, major</td>
<td>937</td>
<td>832</td>
<td>710</td>
<td>782</td>
<td></td>
</tr>
<tr>
<td>Advanced Learner</td>
<td>$\approx 100$</td>
<td>Spelling</td>
<td>0</td>
<td>1041</td>
<td>1138</td>
<td>1213</td>
<td>1126</td>
<td></td>
</tr>
<tr>
<td>$\star$ Short</td>
<td>$\approx 50$</td>
<td>Spelling</td>
<td>0</td>
<td>941</td>
<td>986</td>
<td>824</td>
<td>841</td>
<td></td>
</tr>
<tr>
<td>Intermediate Learner</td>
<td>$\approx 100$</td>
<td>Grammatical</td>
<td>0</td>
<td>1015</td>
<td>1108</td>
<td>771</td>
<td>904</td>
<td></td>
</tr>
<tr>
<td>$\star$ Short</td>
<td>$\approx 50$</td>
<td>Grammatical</td>
<td>0</td>
<td>921</td>
<td>855</td>
<td>582</td>
<td>662</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: Elo ratings for answers in different settings based on the annotations given by crowd-sourced annotators, expert annotators, GPT-4, and Claude-1.
an advanced English learner, occasionally incorporating spelling errors, or an intermediate English learner, occasionally introducing grammatical mistakes during the response generation process. To probe factual accuracy, we direct GPT-4 to include varying degrees of factual errors in the responses. Lastly, we explore the influence of response length by instructing GPT-4 to generate answers of differing lengths. To ensure that the generated responses conformed to the desired criteria, we conduct manual reviews and carry out post-editing as necessary. Subsequently, after obtaining the collection of responses, we conduct annotations with a diverse pool of annotators, including crowd-sourced annotators, expert annotators, GPT-4, and Claude-1. We calculate the Elo ratings for each "model" based on their judgments to understand their preferences when deciding on high-quality model outputs. ${ }^{2}$</p>
<p>In the course of our investigation (Table 1 and Section 3), we uncover intriguing findings related to the assessment of answer quality. Firstly, we observe hesitancy among humans, including both crowd-sourced and expert annotators, in determining answer quality, while LLMs exhibit greater certainty in their evaluations. Furthermore, we notice a substantial difference in fact-checking capabilities between human judges and LLMs. Human judges generally do not thoroughly fact-check answers unless the factual error is glaringly evident, whereas LLMs demonstrate some degree of factchecking ability, albeit with imperfections. Another significant finding is that both human judges</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and LLMs tend to favor longer text when evaluating answers. Interestingly, crowd-sourced annotators appear to be (almost) unbiased in their evaluation of answer ordering, while LLMs lean toward the first answer and experts prefer the second answer.</p>
<p>We note that a single unified measure is not sufficient for evaluating LLMs, given that many aspects play an important role when determining its output quality. Moreover, some aspects are arguably more important than others (e.g., factuality). Yet, we see judges preferring factually incorrect models over grammatically incorrect or short ones. With these identified issues, we present a Multi-Elo Rating System (MERS) designed to assess machinegenerated text from multiple dimensions.</p>
<p>Our proposed approach requires human judges and LLMs to evaluate the machine-generated text independently from three aspects: "Accuracy", "Helpfulness", and "Language". This allows us to achieve a more comprehensive and transparent understanding of the quality of machine-generated text. Our empirical findings demonstrate a significant improvement in the evaluation quality of GPT-4, particularly in terms of factual accuracy. However, we observe that humans still exhibit indecisiveness in their assessments.</p>
<p>Based on our findings, we highly recommend that practitioners evaluate machine-generated text from various perspectives rather than depending solely on a single unified measure. Additionally, we advise practitioners to exercise caution when using crowd-sourced human annotators to assess the performance of LLMs.</p>
<table>
<thead>
<tr>
<th>Model type</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>Correct</td>
<td>Effective ways to deal with stress include regular exercise, maintaining a healthy diet, getting enough sleep, and taking time for relaxation activities like meditation. It’s also important to have a support system in place, whether that’s friends, family, or a mental health professional.</td>
</tr>
<tr>
<td>Minor Errors</td>
<td>Effective ways to deal with stress include physical activities like playing tennis, doing yoga, or running a marathon. Also, adopt healthy habits like sleeping for 10 hours, eating three full meals a day, and avoiding caffeine. Lastly, meditation, guided by a professional like John Zen, can be very soothing.</td>
</tr>
<tr>
<td>Major Errors</td>
<td>Some effective ways to deal with stress include drinking 10 cups of coffee a day, binge-watching Netflix for at least 5 hours, and attending Dr. Monty Farquhar’s stress-relief seminars. Additionally, practicing the art of Floga, a combination of Flute-playing and Yoga, can significantly reduce stress levels.</td>
</tr>
<tr>
<td>Advanced Learner</td>
<td>Whenever feeling stressed, always remember to take deep breathes. This can calm nerves. Regular physical exercise also reduce stress hormonal levels. A balance diet promoting overall well-being can lead to up healthy stress response. Finally, finding hobby or activity you enjoy can help distract you from stressful situations.</td>
</tr>
<tr>
<td>Intermediate Learner</td>
<td>Deal with stress, you can do exercise regularly, practicing a meditation, get with plenty sleep, and eat healthy foods also. You can too to connect with others to express your feelings, and avoiding caffeine, alcohol, and nicotine, and take time to relax and have fun.</td>
</tr>
</tbody>
</table>
<p>Table 2: Examples of different error types for the question “What are the most effective ways to deal with stress?”. Factual errors are highlighted in red and language errors are highlighted in orange.</p>
<h2>2 Evaluation Method</h2>
<p>In this section, we introduce the Elo rating system and the process of generating incorrect answers for each model. We also discuss the human and LLM evaluation methods.</p>
<h3>2.1 Elo Rating System</h3>
<p>The Elo rating system is a method used to calculate the relative skill levels of players in two-player games, such as chess. Given two players $\mathcal{A}$ and $\mathcal{B}$ whose Elo ratings are $\mathcal{R}<em B="B">{A}$ and $\mathcal{R}</em>$ respectively, the expected score for these two players are:</p>
<p>$\mathcal{E}<em B="B">{A}=\frac{1}{1+10^{\frac{\mathcal{R}</em>}-\mathcal{R<em B="B">{A}}{400}}},$ and $\mathcal{E}</em>}=\frac{1}{1+10^{\frac{\mathcal{R<em B="B">{A}-\mathcal{R}</em>.$ (1)}}{400}}</p>
<p>Suppose the player $\mathcal{A}$ is expect to obtain $\mathcal{E}<em A="A">{A}$ scores from this game but actually get $\mathcal{S}</em>$ is:}$ scores, the updated Elo rating of player $\mathcal{A</p>
<p>$\mathcal{R}<em A="A">{A}^{\prime}=\mathcal{R}</em>}+\mathcal{K}\cdot\left(\mathcal{S<em A="A">{A}-\mathcal{E}</em>\right),$ (2)</p>
<p>where $\mathcal{K}$ is adjustment parameter, called the $\mathcal{K}$ factor. Equation 2 is also used for obtaining $\mathcal{R}<em A="A">{B}^{\prime}$. Following chiang2023evaluating, we set $\mathcal{K}=32$. Additionally, if $\mathcal{A}$ is better than $\mathcal{B}$, we set $\mathcal{S}</em>}=1$ and $\mathcal{S<em A="A">{B}=0$. If $\mathcal{B}$ is better than $\mathcal{A}$, we set $\mathcal{S}</em>}=0$ and $\mathcal{S<em A="A">{B}=1$. We set both $\mathcal{S}</em>=0.5$ if both players are equally good. Moreover, to minimize the influence of the ordering of games, the Elo rating calculation is performed 10,000 times with varying random orderings as suggested by dettmers2023evaluating.}=0.5$ and $\mathcal{S}_{B</p>
<h3>2.2 Answer Generation</h3>
<p>chiang2023evaluating release a set of 80 questions categorized into 8 categories to evaluate the generation capability of LLMs. This dataset is widely used by recent studies (dettmers2023evaluating, li2023evaluating), due to its high quality. However, due to the requirement for specialized expertise to answer some of these questions, we exclude the “fermi”, “coding”, and “math” questions, as they typically demand extra efforts to evaluate the answers. This step aims to reduce the potential impact of varying human raters’ capabilities on the evaluation process. Additionally, we also remove the “roleplay” and “writing” questions, as they involve creative writing and are prone to subjectivity in human assessment. As a result, our final question set consists of 40 questions, focusing on the “generic”, “knowledge”, “common sense”, and “counterfactual” categories. We believe these retained questions can be easily understood and answered by the general public.</p>
<p>Once we have the set of questions, we require GPT-4 to generate answers with specific error types in addition to providing the correct answers. We provide some examples of these error types in Table 2. Regarding language quality errors, we ask GPT-4 to respond as either an advanced English learner or an intermediate English learner. The answers generated by an advanced English learner may occasionally contain spelling errors, while those from an intermediate English learner commonly include grammatical mistakes. In terms of factual accuracy, we expect GPT-4 to produce answers with either minor or major errors. Minor errors primarily involve fabricated names or incorrect numbers, while major errors contain incorrect facts and suggestions. Furthermore, we utilize GPT-4 to</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>generate both long (approximately 100 words) and short (approximately 50 words) answers for each question to investigate the preference of both humans and LLMs regarding answer length. Hence, there are 12 models (settings) in total in this study. We present the prompts used for answer generation in Appendix A. To ensure unbiased evaluation results regarding answer ordering, all evaluators, including crowd-sourced human annotators, expert annotators, and LLMs, evaluate all answer pairs from both forward and reversed directions. In the end, we have 5280 unique pairwise comparisons across all 12 models and 40 questions.</p>
<p>During the generation process, we have noticed that GPT-4 may not always fulfill our requirements. For instance, the output sometime may not contain any factual or language errors. Therefore, we manually review all the answers and make necessary edits to ensure they align with our requirements.</p>
<h3>2.3 Crowd-Sourced Evaluation</h3>
<p>Crowd-sourced human annotators are commonly used to assess the quality of machine-generated text. For this study, we utilize Amazon Mechanical Turk (AMT) to collect text evaluations from human raters in NLG evaluations. To minimize the potential impact of annotator demographics, we only recruit crowd-sourced human annotators from the United States. We also ensure the quality of annotations by exclusively offering tasks to annotators who have completed over 5,000 assignments and maintain a success rate of $99 \%$ or higher. To ensure the diversity of the annotators, we limit each annotator to provide no more than 20 annotations. As a result, we engage 377 unique annotators, each contributing an average of 14 annotations.</p>
<p>During annotation, we provide a simple interface that mainly follows the guidelines of Dettmers et al. (2023), as shown in Appendix C. Annotators are presented with a question and two model-generated responses placed side-by-side. Their task is to select the better output or indicate a tie between them. To ensure the annotators' attentiveness and thorough reading of the responses, we incorporate a mandatory 20-second delay before they can submit their answers. Furthermore, we anonymize the model name from our human annotators.</p>
<h3>2.4 Expert Evaluation</h3>
<p>To address concerns about the reliability of crowdsourced annotators, we have implemented a parallel system involving a team of 20 expert annotators.</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Question]</span>
<span class="na">What are the most effective ways to deal with stress?</span>
<span class="k">[The Start of Assistant 1&#39;s Answer]</span>
<span class="na">Effective ways to deal with stress include ...</span>
<span class="k">[The End of Assistant 1&#39;s Answer]</span>
<span class="k">[The Start of Assistant 2&#39;s Answer]</span>
<span class="na">Whenever feeling stressed, ...</span>
<span class="k">[The End of Assistant 2&#39;s Answer]</span>
</code></pre></div>

<p>[System]
We would like to request your feedback on the performance of
$\sim$ two AI assistants ...</p>
<p>Figure 1: A simplified example for the evaluation prompt employed by GPT-4 and Claude-1 is designed to assess the quality of responses. A complete example can be found in Figure 10.</p>
<p>Each member of this dedicated team holds at least a master's degree from an English-speaking country, ensuring a comprehensive understanding of the language and proficiency in annotation nuances. It is important to note that employing expert annotators comes at a higher cost than using crowd-sourced alternatives. Therefore, we sample 200 games out of a total pool of 5280 games, which is statistically sufficient to yield meaningful insights into the model's performance. ${ }^{4}$ To ensure a fair comparison, we provide the same annotation instructions to both the expert team and the crowd-sourced participants. Each expert annotator is assigned to annotate 10 games, taking them approximately 20 minutes. This approach enables us to accurately evaluate the effectiveness and reliability of each annotation method.</p>
<h3>2.5 LLM Evaluation</h3>
<p>As human evaluation can be costly and inefficient, there is an increase in the use of advanced LLMs, such as GPT-4, to evaluate model outputs. In our work, we also use LLMs as judges to assess answer quality. However, previous studies rely solely on GPT-4 as the LLM judge (Chiang et al., 2023; Li et al., 2023a; Zheng et al., 2023), which may not be appropriate for our work as our answers are refined by humans after being generated by GPT-4. This raises concerns about potential biases that GPT4 may have towards its own outputs, which could skew the evaluation process. To ensure fair comparisons and mitigate any bias, we introduce Claude-1</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: The percentage distribution of decision choices made by humans and LLMs in terms of answer length (in words).</p>
<p>From Anthropic <em>Bai et al. (2022b)</em> as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study.</p>
<p>We utilize the evaluation prompt from <em>Dettmers et al. (2023)</em>, as presented in Figure 1. The prompt assesses the answers based on their helpfulness, relevance, accuracy, and level of detail, while also aiming to avoid bias related to answer ordering.</p>
<h2>3 Analysis</h2>
<p>In this study, we assess the performance of 12 models using Elo ratings, as evaluated by crowdsourced annotators, expert annotators, GPT-4, and Claude-1. The Elo ratings are presented in Table 1. As anticipated, the standard correct model attains the highest Elo score across all human judges. Nevertheless, intriguing disparities emerge when comparing the Elo results from humans to LLMs. In this section, we delve into a comprehensive analysis of these distinctions.</p>
<h3>All the judges exhibit a bias toward longer texts.</h3>
<p>Text evaluation by both human and LLM judges often exhibits a bias towards longer responses, where GPT-4 demonstrates the most bias and the expert annotators demonstrate the least bias, as depicted in Figure 2. This inclination is expected since one of the scoring criteria requested is "the level of detail", which often correlates with the length of the output. However, such an inclination is arguably undesirable. For example, we observe that GPT-4 considers "Several Minor Factual Errors" (1206 Elo) to be better than "Correct + Short" (1096 Elo). When manually reviewing the justifications provided by GPT-4 for its evaluation decisions, we observe that GPT-4 sometimes considers the response of "Several Major Factual Errors" as superior to that of "Correct + Short", even when factual errors are detected.</p>
<h3>Humans are indecisive.</h3>
<p>In contrast to LLM judges, both expert and crowd-sourced annotators tend to demonstrate indecisiveness, leading to Elo scores that remain relatively close to the initial value of 1000. The Elo scores assigned by crowdsourced annotators range from 926 to 1086, while those by expert annotators range from 832 to 1162. These ranges are significantly narrower than those observed for GPT-4 and Claude-1. Interestingly, human judges are more inclined to assign "tie" judgments compared to LLMs, as illustrated in Figure 3. These findings raise important questions about the level of reading comprehension and attention to detail demonstrated by human judges, especially when they are crowd-sourced workers evaluating answers.</p>
<h3>The order of answers affects the judges' decisions.</h3>
<p>Our analysis also reveals an interesting bias toward the order of answers, which is evident in the assessments made by both human judges and LLM judges. As illustrated in Figure 3, it can be observed that the crowd-sourced annotators, GPT-4, and Claude-1 exhibit a distinct and notable preference for Assistant 1. This intriguing finding is consistent with prior research <em>Wang et al. (2023)</em>, further underscoring the significance of this bias. Interestingly, the expert annotators favor the answer given by Assistant 2 and are more likely to assign "tie" judgments. To ensure fairness and comprehensive evaluation, we strongly recommend considering both answer orders when analyzing the performance of the systems. We leave the further</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: The distribution of decision choices made by humans and LLMs.</p>
<p>^{5}GPT-4 signature: gpt-4-0613 and Claude-1 signature: Claude-1.3</p>
<table>
<thead>
<tr>
<th></th>
<th>Crowd</th>
<th>Expert</th>
<th>GPT-4</th>
<th>Claude-1</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.5 Crowd</td>
<td>—</td>
<td>0.08</td>
<td>0.11</td>
<td>0.10</td>
</tr>
<tr>
<td>1.5 Expert</td>
<td>0.08</td>
<td>—</td>
<td>0.09</td>
<td>0.14</td>
</tr>
<tr>
<td>1.5 GPT-4</td>
<td>0.11</td>
<td>0.09</td>
<td>—</td>
<td>0.51</td>
</tr>
<tr>
<td>1.5 Claude-1</td>
<td>0.10</td>
<td>0.14</td>
<td>0.51</td>
<td>—</td>
</tr>
</tbody>
</table>
<p>Table 3: Pairwise inter-annotator agreement measured by Cohen's kappa coefficient $\kappa$. The pairs involving the experts only cover 200 games.
study of the behavior of humans for future work.
Crowd-sourced annotators lack fact-checking, while experts and LLM judges can fact-check, albeit imperfectly. The close Elo scores of those models with factual errors from crowd-sourced annotators in Table 1 suggest inconsistent and inadequate fact-checking of generated content, raising concerns about false information and malicious intent in LLM-generated outputs. People are vulnerable to believing such outputs, particularly when they appear convincing. Although expert annotators are more diligent in fact-checking, the general public's behavior towards LLM output tends to be closer to that of crowd-sourced annotators, posing significant safety risks. On the other hand, LLM judges do notice errors, but not consistently. When the LLM fails to detect inaccuracies, it often favors flawed outputs over shorter or grammatically imprecise responses.</p>
<p>LLMs only reach a moderate consensus, while humans embrace more diversity in thought. We assess inter-annotator agreement using Cohen's kappa coefficient $\kappa$ (Cohen, 1960) and present the results in Table 3. Our analysis, following the interpretation of $\kappa$ by McHugh (2012), shows that only GPT-4 and Claude-1 achieve moderate agreement, while other comparisons demonstrate only slight agreement. This confirms that human annotators exhibit inconsistencies during annotation. Zheng et al. (2023) define the agreement between two types of judges as the probability of non-identical individuals of each type agreeing on a randomly selected question and shows an approximately $80 \%$ agreement between LLMs and crowd-sourced humans, which seems to contradict our findings. However, this discrepancy arises because they remove ties and inconsistent annotations, while we consider all annotations. When ties and inconsistencies are retained, Zheng et al. (2023) report an approximately $60 \%$ agreement between LLMs and crowd-sourced humans, which is slightly better than random guessing and aligns with our findings.</p>
<h2>4 Multi-Elo Rating System</h2>
<p>As discussed in Section 3, we identify the limitations in the evaluation of machine-generated answers based on human and LLM judges. We observe that the current widely used evaluation approach fails to yield satisfactory results and lacks a comprehensive understanding of the factors influencing the outcomes. Collapsing multiple decision components (e.g., accuracy, level of details, relevance, language, helpfulness, etc.) into a single score undermines the importance of individual components and introduces subjectivity regarding their relative significance in the final decision.</p>
<p>To overcome these limitations, we propose a novel multi-dimensional evaluation approach for assessing the outputs of LLMs, which we refer to as the Multi-Elo Rating System (MERS). This approach is inspired by machine translation research, where evaluations are often divided into at least two parts: fluency and adequacy. Recent advancements in MT evaluation also advocate breaking down the evaluation into multiple aspects (Lommel et al., 2014). In this section, we present a detailed description of our evaluation methodology and the evaluation results.</p>
<h3>4.1 Methodology</h3>
<p>The Multidimensional Quality Metrics (MQM) framework provides a comprehensive approach for evaluating and establishing standards for translation quality (Lommel et al., 2014). Drawing inspiration from this framework, we propose a similar approach to evaluate the outputs of LLMs from multiple dimensions.</p>
<p>Our evaluation focuses on three main dimensions of the generated text, as outlined below:</p>
<ul>
<li>Accuracy: The accuracy of the text involves considering factual correctness and logical consistency.</li>
<li>Helpfulness: The helpfulness of the text involves considering its relevance of the information and whether it addresses the question given, taking into account the depth of the response given.</li>
<li>Language: The language of the text involves considering its clarity, coherence, grammar, syntax, and tone.
The quality of an answer is dependent on its spe-</li>
</ul>
<table>
<thead>
<tr>
<th>Accuracy</th>
<th></th>
<th></th>
<th></th>
<th>Helpfulness</th>
<th></th>
<th></th>
<th></th>
<th>Language</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Crowd</td>
<td></td>
<td>Expert</td>
<td>GPT-4</td>
<td>Crowd</td>
<td></td>
<td>Expert</td>
<td>GPT-4</td>
<td>Crowd</td>
<td></td>
<td>Expert</td>
</tr>
<tr>
<td>C.</td>
<td>1056</td>
<td>1180</td>
<td>1200</td>
<td>1045</td>
<td>1208</td>
<td>1384</td>
<td>1036</td>
<td>1109</td>
<td>1415</td>
<td></td>
<td></td>
</tr>
<tr>
<td>+ S.</td>
<td>963</td>
<td>1040</td>
<td>1158</td>
<td>983</td>
<td>979</td>
<td>1009</td>
<td>1007</td>
<td>1068</td>
<td>1199</td>
<td></td>
<td></td>
</tr>
<tr>
<td>OMin.FE</td>
<td>1026</td>
<td>1090</td>
<td>1120</td>
<td>1048</td>
<td>1153</td>
<td>1378</td>
<td>1019</td>
<td>1114</td>
<td>1334</td>
<td></td>
<td></td>
</tr>
<tr>
<td>+ S.</td>
<td>978</td>
<td>898</td>
<td>1016</td>
<td>993</td>
<td>941</td>
<td>965</td>
<td>990</td>
<td>1012</td>
<td>1109</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SMin.F</td>
<td>1036</td>
<td>1044</td>
<td>993</td>
<td>1051</td>
<td>1069</td>
<td>1248</td>
<td>1029</td>
<td>1096</td>
<td>1200</td>
<td></td>
<td></td>
</tr>
<tr>
<td>+ S.</td>
<td>978</td>
<td>931</td>
<td>857</td>
<td>956</td>
<td>865</td>
<td>845</td>
<td>996</td>
<td>935</td>
<td>988</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SMaj.FE</td>
<td>1030</td>
<td>963</td>
<td>794</td>
<td>1037</td>
<td>1015</td>
<td>926</td>
<td>1023</td>
<td>1010</td>
<td>995</td>
<td></td>
<td></td>
</tr>
<tr>
<td>+ S.</td>
<td>955</td>
<td>787</td>
<td>746</td>
<td>940</td>
<td>766</td>
<td>726</td>
<td>982</td>
<td>879</td>
<td>871</td>
<td></td>
<td></td>
</tr>
<tr>
<td>AL</td>
<td>1028</td>
<td>1121</td>
<td>1139</td>
<td>1032</td>
<td>1146</td>
<td>1196</td>
<td>1004</td>
<td>1039</td>
<td>1051</td>
<td></td>
<td></td>
</tr>
<tr>
<td>+ S.</td>
<td>979</td>
<td>971</td>
<td>1051</td>
<td>969</td>
<td>891</td>
<td>804</td>
<td>994</td>
<td>863</td>
<td>814</td>
<td></td>
<td></td>
</tr>
<tr>
<td>IL</td>
<td>1015</td>
<td>1076</td>
<td>1018</td>
<td>1002</td>
<td>1095</td>
<td>908</td>
<td>991</td>
<td>992</td>
<td>560</td>
<td></td>
<td></td>
</tr>
<tr>
<td>+ S.</td>
<td>956</td>
<td>898</td>
<td>908</td>
<td>945</td>
<td>872</td>
<td>612</td>
<td>930</td>
<td>884</td>
<td>465</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: Elo ratings for different models with regard to “Accuracy", “Helpfulness", and “Language" given by crowd-sourced annotators, expert annotators, and GPT-4. C. stands for Correct. OMin.FE stands for One Minor Factual Error. SMin.F stands for Several Minor Factual Errors. SMaj.FE stands for Several Major Factual Errors. AL stands for Advanced Learner. IL stands for Intermediate Learner. S. stands for Short.
cific context. For instance, if a model gives a detailed but complicated explanation of black holes to an 8 -year-old, the answer may be accurate but not useful. Conversely, if a model is asked to compose an email and produces a message with incorrect information, the response may lack accuracy but still have some value. By taking this multi-dimensional approach, we can gain a clearer understanding of model performance and prioritize different aspects based on our individual requirements.</p>
<p>To facilitate the multi-dimensional evaluation by human annotators, we introduce a simple modification to the interface, asking them to rate the quality across the three different aspects, as shown in Appendix C. Additionally, we experiment with two approaches for GPT-4 evaluation: asking three independent queries versus a single query that requests judgments for all three aspects together. In this paper, we report the results obtained from asking three independent queries. More details can be found at Appendix D.</p>
<h3>4.2 Stop Using Crowd-Sourced Annotators!</h3>
<p>In this section, we compare the annotation outcomes provided by both crowd-sourced and expert annotators, as presented in Table 4. We examine the aspects of "Accuracy", "Helpfulness", and "Language" in the evaluation. Regarding "Accuracy", we find that expert annotators are proficient in identifying factual errors in answers, although not entirely flawless. However, crowd-sourced annotators exhibit indecisiveness in their evaluations. Notably, the crowd-sourced annotators perceive Several Major Factual Errors + Short (955</p>
<p>Elo) and Correct + Short (963 Elo) as nearly equally good. Regarding "Helpfulness", the expert annotators display a stronger preference for longer answers, while the crowd-sourced annotators only slightly favor them, as evidenced by their Elo scores. In terms of "Language", both expert and crowd-sourced annotators face challenges in recognizing spelling or grammatical errors, suggesting that humans may be less sensitive to language errors and aligning with Clark et al. (2021).</p>
<p>In conclusion, the expert annotators outperform the crowd-sourced annotators in the evaluation, despite not being entirely error-free themselves. These results serve as a warning against overreliance on crowd-sourced judgments and also highlight concerns regarding the general audience’s ability to critically interpret LLM’s output.</p>
<h3>4.3 Experts versus GPT-4</h3>
<p>In this section, we discuss the difference between the expert annotators and GPT-4 in evaluation from multiple dimensions and present the results in Table 4. Regarding the "Accuracy" dimension, it is noteworthy that the Elo scores for factual accuracy closely align with the single Elo scores presented in Table 1, suggesting that expert annotators indeed prioritize factual accuracy during evaluation. GPT-4 can also effectively rank models based on the severity of the errors. Regarding the "Helpfulness" dimension, both expert annotators and GPT-4 consistently consider longer answers to be more helpful. Similar to the discussion in Section 4.2, we believe that this preference stems from the strong correlation between "helpfulness" and "the level of</p>
<p>detail", as longer answers tend to convey more information, making them perceived as more helpful. Regarding the "Language" dimension, recognizing spelling or grammatical errors in text is challenging for experts, while GPT-4 effectively distinguishes between answers based on their language quality and appears to penalize grammatical errors more heavily during assessment. Overall, this comprehensive analysis sheds light on the evaluation process and reveals the differing perspectives of expert annotators and GPT-4 in assessing various dimensions of model performance.</p>
<h2>5 Discussion</h2>
<p>When evaluating the quality of an LLM and selecting the best model, it is important to consider various factors, including accuracy, fluency, and other relevant aspects. However, combining these factors into a single score is not recommended, as it is unclear which factor should be given the highest priority for both the LLM itself and human judges. Therefore, it is crucial to analyze and evaluate these factors individually for a comprehensive assessment. We propose breaking them into three categories. However, these categories may not be the perfect setup to capture all the aspects required for an ideal answer, so further research is necessary.</p>
<p>Another important consideration is the use of human judgments for evaluating LLM performance. While crowd feedback can provide a general indication of how the audience perceives the LMs' output, caution must be exercised. Crowd-sourced evaluators may not always involve rigorous fact-checking, thus giving high scores to factually incorrect answers. Expert evaluators are better in this aspect, with the caveat of more difficulty in scaling the process. Additionally, both human annotators demonstrate various biases, such as the length and order of the answers. Hence, it is crucial to supplement human opinions with other evaluation methods to gain a better understanding of LLMs.</p>
<h2>6 Related Work</h2>
<p>Large Language Models Large Language Models (LLMs) commonly refer to Transformer-based language models with billions of parameters (Vaswani et al., 2017). Examples of these models include GPT-3 (Brown et al., 2020), PanGu- $\alpha$ (Zeng et al., 2021), Chinchilla (Hoffmann et al., 2022), PaLM (Chowdhery et al., 2022), BLOOM (Scao et al., 2022), Galactica (Taylor et al., 2022),
and LLaMA (Touvron et al., 2023). These models, trained on massive datasets, demonstrate impressive abilities in understanding natural language and handling complex tasks. It is found that instructiontuned LLMs can significantly enhance their performance following general language instructions (Weller et al., 2020; Mishra et al., 2022; Wang et al., 2022b; Wei et al., 2022; Sanh et al., 2022; Ouyang et al., 2022; Parmar et al., 2022; Scialom et al., 2022; Chung et al., 2022; Yin et al., 2022; Gupta et al., 2022; Muennighoff et al., 2022). Therefore, accurately and comprehensively assessing the performance of these LLMs remains an unsolved challenge. This study aims to examine the effectiveness of both humans and LLMs as evaluators.</p>
<p>Evaluation With the advancement of LLMs (Large Language Models), the need for their thorough evaluation becomes increasingly important. Traditionally, NLP models are assessed using standardized benchmark test suites. Given the capabilities of LLMs, several studies suggest using a diverse set of NLP benchmarks for a more comprehensive understanding (Hendrycks et al., 2021; Gao et al., 2021; Srivastava et al., 2022; Liang et al., 2022; Li et al., 2023b). As pointed out by Gudibande et al. (2023) and Zheng et al. (2023) that there is a gap between users' perception and standardized evaluation suites, recent LLM studies often incorporate human evaluation for a more nuanced understanding of model performance (Wang et al., 2022a; Chiang et al., 2023; Wu et al., 2023). As human evaluations can be costly, some recent works utilize state-of-the-art LLMs such as GPT-4 (OpenAI, 2023) and Claude-1 (Bai et al., 2022b) to evaluate model outputs. More recently, several works employ the Elo rating system from chess games to gauge the LLMs' capabilities (Askell et al., 2021; Bai et al., 2022a; Srivastava et al., 2022; Dettmers et al., 2023; Zheng et al., 2023). However, these previous works operate under the assumption that human evaluations serve as the gold standard. In contrast, Clark et al. (2021) argue that human evaluation is no longer the gold standard, highlighting the inability of human judges to distinguish between human-written and machinegenerated text. Their research focuses solely on language quality, while our investigation extends to evaluating the behavior of both humans and LLMs across three aspects: factual accuracy, language quality, and text length.</p>
<h2>7 Conclusion</h2>
<p>In this study, we investigate the limitations of human judges and LLMs as evaluators by examining their behaviors in assessing machine-generated text. We deliberately introduce fabricated factual and grammatical errors into a set of machinegenerated answers and analyze the responses of crowd-sourced, expert, and LLM judges. The primary goal is to gain insights into the limitations and biases exhibited by both human and LLM judges. We observe that both human judges and LLMs demonstrate various biases.</p>
<p>To address the observed issues, we propose to assesses machine-generated text across multiple dimensions independently and instantiate such an idea with the Elo rating system, leading to the Multi-Elo Rating System. Our empirical findings demonstrate the effectiveness of this approach in enhancing the evaluation quality of GPT-4, particularly in terms of factual accuracy. However, crowd judges continue to display indecisiveness. Given these findings, we encourage practitioners to adopt a multi-dimensional approach when evaluating machine-generated text, rather than relying solely on a single unified measure. We also recommend caution when using crowd annotators to assess LLMs due to their indecisiveness, bias toward lengthy responses, and their limited capability in fact-checking answers.</p>
<h2>8 Limitations</h2>
<p>Question Coverage We select only 40 questions from Chiang et al. (2023). We acknowledge that this limited selection may not capture the full spectrum of question types and variations. Consequently, there is a potential risk that some aspects of the research question may not receive sufficient representation or exploration.</p>
<p>Evaluation Dimension Coverage In our proposed Multi-Elo Rating System, we only explore three crucial dimensions: "Accuracy", "Helpfulness", and "Language". We acknowledge that while these dimensions provide valuable insights, they may not encompass the entirety of the multifaceted nature of text evaluation. Furthermore, it is important to recognize that our definitions for the three dimensions we have chosen are not infallible. Different stakeholders may have diverse perspectives on these dimensions.</p>
<p>We leave the investigation of addressing these
limitations to future work.</p>
<h2>References</h2>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. 2021. A general language assistant as a laboratory for alignment. CoRR, abs/2112.00861.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. CoRR, abs/2204.05862.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemí Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional AI: harmlessness from AI feedback. CoRR, abs/2212.08073.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan</p>
<p>Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. CoRR, abs/2210.11416.</p>
<p>Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that's 'human' is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7282-7296, Online. Association for Computational Linguistics.</p>
<p>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37-46.</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. CoRR, abs/2305.14314.</p>
<p>Arpad E Elo. 1967. The proposed uscf rating system, its development, theory, and applications. Chess Life, 22(8):242-247.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding,</p>
<p>Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. A framework for few-shot language model evaluation.</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. The false promise of imitating proprietary llms. CoRR, abs/2305.15717.</p>
<p>Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, and Jeffrey Bigham. 2022. InstructDial: Improving zero and few-shot generalization in dialogue through instruction tuning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 505525, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. CoRR, abs/2203.15556.</p>
<p>Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. 2023a. Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation. CoRR, abs/2305.15011.</p>
<p>Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023b. CMMLU: measuring massive multitask language understanding in chinese. CoRR, abs/2306.09212.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yüksekgönül, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuezhen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic evaluation of language models. CoRR, abs/2211.09110.</p>
<p>Arle Lommel, Hans Uszkoreit, and Aljoscha Burchardt. 2014. Multidimensional quality metrics (mqm): A framework for declaring and describing translation quality metrics. Tradumática, (12):0455-463.</p>
<p>Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu. 2023. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration. CoRR, abs/2306.09093.</p>
<p>Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica, 22(3):276-282.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2022. Crosslingual generalization through multitask finetuning. CoRR, abs/2211.01786.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems.</p>
<p>Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, and Chitta Baral. 2022. InBoXBART: Get instructions into biomedical multitask learning. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 112-128, Seattle, United States. Association for Computational Linguistics.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao,</p>
<p>Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benolt Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100.</p>
<p>Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. 2022. Fine-tuned language models are continual learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6107-6122, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. CoRR, abs/2211.09085.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard</p>
<p>Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. CoRR, abs/2305.17926.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022a. Self-instruct: Aligning language model with self generated instructions. CoRR, abs/2212.10560.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022b. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085-5109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Orion Weller, Nicholas Lourie, Matt Gardner, and Matthew E. Peters. 2020. Learning from task descriptions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1361-1375, Online. Association for Computational Linguistics.</p>
<p>Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2023. Lamini-lm: A diverse herd of distilled models from large-scale instructions. CoRR, abs/2304.14402.</p>
<p>Wenpeng Yin, Jia Li, and Caiming Xiong. 2022. ConTinTin: Continual learning from task instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3062-3072, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yifan Yao, Xinjing Huang, Jun Wang, Jianfeng Yu, Qi Guo, Yue Yu, Yan Zhang, Jin Wang, Hengtao Tao, Dasen Yan, Zexuan Yi, Fang Peng, Fangqing Jiang, Han Zhang, Lingfeng Deng, Yehong Zhang, Zhe Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo, Shanzhi Gu, Gaojun Fan, Yaowei Wang, Xuefeng Jin, Qun Liu, and Yonghong Tian. 2021. Pangu$\alpha$ : Large-scale autoregressive pretrained chinese language models with auto-parallel computation. CoRR, abs/2104.12369.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. CoRR, abs/2306.05685.</p>
<table>
<thead>
<tr>
<th>Question/Instruction:</th>
</tr>
</thead>
<tbody>
<tr>
<td>$instruction</td>
</tr>
<tr>
<td>Answer the question/instruction.</td>
</tr>
<tr>
<td>The answer should be roughly 100 words long.</td>
</tr>
</tbody>
</table>
<p>Figure 4: The prompt employed by GPT-4 for generating answers of the "Correct" model.</p>
<p>Question/Instruction:</p>
<div class="codehilite"><pre><span></span><code>...
Answer<span class="w"> </span>the<span class="w"> </span>question/instruction.
The<span class="w"> </span>answer<span class="w"> </span>should<span class="w"> </span>be<span class="w"> </span>roughly<span class="w"> </span>100<span class="w"> </span>words<span class="w"> </span>long.
The<span class="w"> </span>answer<span class="w"> </span>must<span class="w"> </span>contain<span class="w"> </span>one<span class="w"> </span>minor<span class="w"> </span>factual<span class="w"> </span>error.
The<span class="w"> </span>factual<span class="w"> </span>error<span class="w"> </span>can<span class="w"> </span>be<span class="w"> </span>made-up<span class="w"> </span>names,<span class="w"> </span>wrong
~+<span class="w"> </span>numbers,<span class="w"> </span>incorrect<span class="w"> </span>facts,<span class="w"> </span>or<span class="w"> </span>incorrect
~+<span class="w"> </span>suggestions.
List<span class="w"> </span>the<span class="w"> </span>error<span class="w"> </span>and<span class="w"> </span>its<span class="w"> </span>corresponding
~+<span class="w"> </span>justification<span class="w"> </span>separately.
Enclose<span class="w"> </span>your<span class="w"> </span>answer<span class="w"> </span>within<span class="w"> </span><span class="nt">&lt;answer&gt;</span><span class="w"> </span>and
~+<span class="w"> </span><span class="nt">&lt;/answer&gt;</span><span class="w"> </span>tags.
Enclose<span class="w"> </span>the<span class="w"> </span>error<span class="w"> </span>and<span class="w"> </span>justification<span class="w"> </span>within
~+<span class="w"> </span><span class="nt">&lt;error&gt;</span><span class="w"> </span>and<span class="w"> </span><span class="nt">&lt;/error&gt;</span><span class="w"> </span>tags.
</code></pre></div>

<p>Figure 5: The prompt employed by GPT-4 for generating answers of the "One Minor Factual Error" model.</p>
<h2>Appendix A Answer Generation Prompts</h2>
<p>We present the answer generation prompt for "Correct" (Figure 4), "One Minor Factual Error" (Figure 5), "Several Minor Factual Errors" (Figure 6), "Several Major Factual Errors" (Figure 7), "Advanced Learner" (Figure 8), and "Intermediate Learner" (Figure 9) in this section. For those prompts generating short answers, we simply require GPT-4 to ensure that "The answer should be roughly 50 words long".</p>
<h2>B LLM Evaluation Prompt</h2>
<p>We utilize the evaluation prompt for LLMs from dettmers2023, as presented in Figure 10.</p>
<h2>C Human Evaluation Interface</h2>
<p>The interface used for crowd-source evaluation is shown in Figure 11 and Figure 12</p>
<h2>D Separate vs. Compound</h2>
<p>In this section, we explore two evaluation strategies: assessing each dimension separately or evaluating all dimensions simultaneously using a compound prompt. The results obtained from GPT-4 using these two strategies are presented in Table 5.</p>
<p>Question/Instruction:</p>
<div class="codehilite"><pre><span></span><code>...
Answer<span class="w"> </span>the<span class="w"> </span>question/instruction.
The<span class="w"> </span>answer<span class="w"> </span>should<span class="w"> </span>be<span class="w"> </span>roughly<span class="w"> </span>100<span class="w"> </span>words<span class="w"> </span>long.
The<span class="w"> </span>answer<span class="w"> </span>must<span class="w"> </span>contain<span class="w"> </span>several<span class="w"> </span>minor<span class="w"> </span>factual
~+<span class="w"> </span>errors.
The<span class="w"> </span>factual<span class="w"> </span>errors<span class="w"> </span>can<span class="w"> </span>be<span class="w"> </span>made-up<span class="w"> </span>names,<span class="w"> </span>wrong
~+<span class="w"> </span>numbers,<span class="w"> </span>incorrect<span class="w"> </span>facts,<span class="w"> </span>or<span class="w"> </span>incorrect
~+<span class="w"> </span>suggestions.
List<span class="w"> </span>the<span class="w"> </span>errors<span class="w"> </span>and<span class="w"> </span>their<span class="w"> </span>corresponding
~+<span class="w"> </span>justifications<span class="w"> </span>separately.
Enclose<span class="w"> </span>your<span class="w"> </span>answer<span class="w"> </span>within<span class="w"> </span><span class="nt">&lt;answer&gt;</span><span class="w"> </span>and
~+<span class="w"> </span><span class="nt">&lt;/answer&gt;</span><span class="w"> </span>tags.
Enclose<span class="w"> </span>the<span class="w"> </span>errors<span class="w"> </span>and<span class="w"> </span>justifications<span class="w"> </span>within
~+<span class="w"> </span><span class="nt">&lt;error&gt;</span><span class="w"> </span>and<span class="w"> </span><span class="nt">&lt;/error&gt;</span><span class="w"> </span>tags.
</code></pre></div>

<p>Figure 6: The prompt employed by GPT-4 for generating answers of the "Several Minor Factual Errors" model.</p>
<p>Question/Instruction:</p>
<div class="codehilite"><pre><span></span><code>Answer<span class="w"> </span>the<span class="w"> </span>question/instruction.
The<span class="w"> </span>answer<span class="w"> </span>should<span class="w"> </span>be<span class="w"> </span>roughly<span class="w"> </span>100<span class="w"> </span>words<span class="w"> </span>long.
The<span class="w"> </span>answer<span class="w"> </span>must<span class="w"> </span>contain<span class="w"> </span>several<span class="w"> </span>major<span class="w"> </span>factual
~+<span class="w"> </span>errors.
The<span class="w"> </span>factual<span class="w"> </span>errors<span class="w"> </span>can<span class="w"> </span>be<span class="w"> </span>made-up<span class="w"> </span>names,<span class="w"> </span>wrong
~+<span class="w"> </span>numbers,<span class="w"> </span>incorrect<span class="w"> </span>facts,<span class="w"> </span>or<span class="w"> </span>incorrect
~+<span class="w"> </span>suggestions.
List<span class="w"> </span>the<span class="w"> </span>errors<span class="w"> </span>and<span class="w"> </span>their<span class="w"> </span>corresponding
~+<span class="w"> </span>justifications<span class="w"> </span>separately.
Enclose<span class="w"> </span>your<span class="w"> </span>answer<span class="w"> </span>within<span class="w"> </span><span class="nt">&lt;answer&gt;</span><span class="w"> </span>and
~+<span class="w"> </span><span class="nt">&lt;/answer&gt;</span><span class="w"> </span>tags.
Enclose<span class="w"> </span>the<span class="w"> </span>errors<span class="w"> </span>and<span class="w"> </span>justifications<span class="w"> </span>within
~+<span class="w"> </span><span class="nt">&lt;error&gt;</span><span class="w"> </span>and<span class="w"> </span><span class="nt">&lt;/error&gt;</span><span class="w"> </span>tags.
</code></pre></div>

<p>Figure 7: The prompt employed by GPT-4 for generating answers of the "Several Major Factual Errors" model.</p>
<p>Question/Instruction:</p>
<div class="codehilite"><pre><span></span><code>...
Answer<span class="w"> </span>the<span class="w"> </span>question/instruction.
The<span class="w"> </span>answer<span class="w"> </span>must<span class="w"> </span>be<span class="w"> </span>written<span class="w"> </span>as<span class="w"> </span>if<span class="w"> </span>you&#39;re<span class="w"> </span>an
~+<span class="w"> </span>advanced-level<span class="w"> </span>English<span class="w"> </span>learner.
The<span class="w"> </span>answer<span class="w"> </span>must<span class="w"> </span>contain<span class="w"> </span>2<span class="w"> </span>or<span class="w"> </span>3<span class="w"> </span>minor
~+<span class="w"> </span>grammatical<span class="w"> </span>and<span class="w"> </span>spelling<span class="w"> </span>errors.
The<span class="w"> </span>answer<span class="w"> </span>should<span class="w"> </span>be<span class="w"> </span>roughly<span class="w"> </span>100<span class="w"> </span>words<span class="w"> </span>long.
List<span class="w"> </span>the<span class="w"> </span>errors<span class="w"> </span>and<span class="w"> </span>their<span class="w"> </span>corresponding
~+<span class="w"> </span>justifications<span class="w"> </span>separately.
Enclose<span class="w"> </span>your<span class="w"> </span>answer<span class="w"> </span>within<span class="w"> </span><span class="nt">&lt;answer&gt;</span><span class="w"> </span>and
~+<span class="w"> </span><span class="nt">&lt;/answer&gt;</span><span class="w"> </span>tags.
Enclose<span class="w"> </span>the<span class="w"> </span>errors<span class="w"> </span>and<span class="w"> </span>justifications<span class="w"> </span>within
~+<span class="w"> </span><span class="nt">&lt;error&gt;</span><span class="w"> </span>and<span class="w"> </span><span class="nt">&lt;/error&gt;</span><span class="w"> </span>tags.
</code></pre></div>

<p>Figure 8: The prompt employed by GPT-4 for generating answers of the "Advanced Learner" model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Helpfulness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Language</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Separate</td>
<td style="text-align: center;">Compound</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Separate</td>
<td style="text-align: center;">Compound</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Separate</td>
<td style="text-align: center;">Compound</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">1200</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1284</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1364</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1429</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1429</td>
</tr>
<tr>
<td style="text-align: center;">+ Short</td>
<td style="text-align: center;">1158</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1146</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1009</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1199</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1178</td>
</tr>
<tr>
<td style="text-align: center;">One Minor Factual Error</td>
<td style="text-align: center;">1120</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1221</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1378</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1399</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1334</td>
</tr>
<tr>
<td style="text-align: center;">+ Short</td>
<td style="text-align: center;">1016</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1045</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">965</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">993</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1090</td>
</tr>
<tr>
<td style="text-align: center;">Several Minor Factual Errors</td>
<td style="text-align: center;">993</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1054</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1248</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1208</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1187</td>
</tr>
<tr>
<td style="text-align: center;">+ Short</td>
<td style="text-align: center;">857</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">895</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">845</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">833</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">988</td>
</tr>
<tr>
<td style="text-align: center;">Several Major Factual Errors</td>
<td style="text-align: center;">794</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">805</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">926</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">884</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">995</td>
</tr>
<tr>
<td style="text-align: center;">+ Short</td>
<td style="text-align: center;">746</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">730</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">726</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">711</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">871</td>
</tr>
<tr>
<td style="text-align: center;">Advanced Learner</td>
<td style="text-align: center;">1139</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1176</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1196</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1210</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">+ Short</td>
<td style="text-align: center;">1051</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">969</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">804</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">810</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">814</td>
</tr>
<tr>
<td style="text-align: center;">Intermediate Learner</td>
<td style="text-align: center;">1018</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">911</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">908</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">853</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">560</td>
</tr>
<tr>
<td style="text-align: center;">+ Short</td>
<td style="text-align: center;">908</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">761</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">612</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">615</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">465</td>
</tr>
</tbody>
</table>
<p>Table 5: The Elo ratings for different models with regard to "Accuracy", "Helpfulness", and "Language" given by GPT-4. "Separate" means that GPT-4 assesses the factual accuracy of the models using a separate prompt, while "Compound" implies that GPT-4 evaluates all three dimensions simultaneously using a compound prompt.</p>
<div class="codehilite"><pre><span></span><code>Question/Instruction:
-..
Answer<span class="w"> </span>the<span class="w"> </span>question/instruction.
The<span class="w"> </span>answer<span class="w"> </span>must<span class="w"> </span>be<span class="w"> </span>written<span class="w"> </span>as<span class="w"> </span>if<span class="w"> </span>you&#39;re<span class="w"> </span>an
-&gt;<span class="w"> </span>intermediate-level<span class="w"> </span>English<span class="w"> </span>learner.
The<span class="w"> </span>answer<span class="w"> </span>must<span class="w"> </span>contain<span class="w"> </span>5<span class="w"> </span>or<span class="w"> </span>more<span class="w"> </span>major
-&gt;<span class="w"> </span>grammatical<span class="w"> </span>and<span class="w"> </span>fluency<span class="w"> </span>errors.
The<span class="w"> </span>answer<span class="w"> </span>must<span class="w"> </span>be<span class="w"> </span>roughly<span class="w"> </span>100<span class="w"> </span>words<span class="w"> </span>long.
List<span class="w"> </span>the<span class="w"> </span>errors<span class="w"> </span>and<span class="w"> </span>their<span class="w"> </span>corresponding
-&gt;<span class="w"> </span>justifications<span class="w"> </span>separately.
Enclose<span class="w"> </span>your<span class="w"> </span>answer<span class="w"> </span>within<span class="w"> </span><span class="nt">&lt;answer&gt;</span><span class="w"> </span>and
-&gt;<span class="w"> </span><span class="nt">&lt;/answer&gt;</span><span class="w"> </span>tags.
Enclose<span class="w"> </span>the<span class="w"> </span>errors<span class="w"> </span>and<span class="w"> </span>justifications<span class="w"> </span>within
-&gt;<span class="w"> </span><span class="nt">&lt;error&gt;</span><span class="w"> </span>and<span class="w"> </span><span class="nt">&lt;/error&gt;</span><span class="w"> </span>tags.
</code></pre></div>

<p>Figure 9: The prompt employed by GPT-4 for generating answers of the "Intermediate Learner" model.</p>
<p>Regarding the "Accuracy" dimension, our findings indicate that GPT-4 performs better when assessing the factual accuracy of models independently. However, when using the compound prompt, GPT-4 ranks "One Minor Factual Error" and "Advanced Learner" higher than "Correct + Short". This observation leads us to hypothesize that evaluation dimensions can mutually influence each other when evaluated concurrently, even when explicitly instructing GPT-4 to evaluate each dimension independently. For the "Helpfulness" dimension, GPT-4, when using the separate prompt, ranks "Several Minor Factual Errors" higher than "Advanced Learner". However, when using the compound prompt, GPT-4 ranks "Advanced Learner" higher than "Several Minor Factual Errors". Interestingly, the "Language" dimension is the most consistent, as GPT-4 produces the same rankings using both evaluation strategies.</p>
<p>Based on our findings, we choose to use the
separate prompts for each dimension in this work, as this strategy yields better results in terms of factual accuracy.</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Question]</span>
<span class="na">What are the most effective ways to deal with stress?</span>
<span class="k">[The Start of Assistant 1&#39;s Answer]</span>
<span class="na">Effective ways to deal with stress include regular exercise, maintaining a healthy diet, getting</span>
<span class="na">\hookrightarrow enough sleep, and taking time for relaxation activities like meditation. It&#39;s also important to</span>
<span class="na">\hookrightarrow have a support system in place, whether that&#39;s friends, family, or a mental health professional.</span>
<span class="k">[The End of Assistant 1&#39;s Answer]</span>
<span class="k">[The Start of Assistant 2&#39;s Answer]</span>
<span class="na">Whenevr feeling stressed, always remember to take deep breathes. This can calm nerves. Regular</span>
<span class="na">\hookrightarrow physical exercise also reduce stress hormonal levels. A balance diet promoting overral</span>
<span class="na">\hookrightarrow well-being can lead to up healty stress response. Finally, finding hobby or activity you enjoy</span>
<span class="na">\hookrightarrow can help distract you from stressful situations.</span>
<span class="k">[The End of Assistant 2&#39;s Answer]</span>
<span class="k">[System]</span>
<span class="na">We would like to request your feedback on the performance of two AI assistants in response to the</span>
<span class="na">\hookrightarrow user question displayed above.</span>
<span class="na">Please rate the helpfulness, relevance, accuracy, level of details of their responses. First, provide</span>
<span class="na">\hookrightarrow your evaluation of the assistant&#39;s helpfulness, relevance, accuracy, and level of detail. Please</span>
<span class="na">\hookrightarrow provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring</span>
<span class="na">\hookrightarrow that the order in which the responses were presented does not affect your judgment.</span>
<span class="na">Once you have carefully reviewed both submissions, in a new line, choose between the answers of</span>
<span class="na">\hookrightarrow Assistant 1 and Assistant 2 by outputting the number 1 or 2 respectively, or choose 3 if the two</span>
<span class="na">\hookrightarrow assistants are equivalent. Do not output anything else other than the number in this last line.</span>
</code></pre></div>

<p>Figure 10: A complete example for the evaluation prompt employed by GPT-4 and Claude-1 is designed to assess the quality of responses. These language models evaluate answers based on criteria such as helpfulness, relevance, accuracy, and level of detail.</p>
<h1>Task</h1>
<p>We would like to request your feedback on the performance of two AI assistants in response to the user question displayed below.
Please consider the helpfulness, relevance, accuracy, level of details of their responses.
Please choose the best response, avoiding any potential bias, ensuring that the order in which the responses were presented does not affect your judgment.</p>
<h2>User Question</h2>
<p>QUESTION</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Response A</th>
<th style="text-align: left;">Response B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ANGWER_A</td>
<td style="text-align: left;">ANGWER_B</td>
</tr>
</tbody>
</table>
<h2>Response Comparison</h2>
<p>Response A is better than Response B.
Response B is better than Response A.
The responses are EXACTLY the same in quality.
Figure 11: Annotation interface for single Elo score.</p>
<h2>Task</h2>
<p>We would like to request your feedback on the performance of two AI assistants in response to the user question displayed below. We would like to evaluate them across three aspects independently: accuracy, helpfulness, and language.</p>
<ul>
<li>The accuracy of the text involves considering factual correctness and logical consistency.</li>
<li>The helpfulness of the text involves considering its relevance of the information and whether it addresses the question given, taking into account the depth of the response given.</li>
<li>The language of the text involves considering its clarity, coherence, grammar, syntax, and tone.</li>
</ul>
<p>Please choose the best response, avoiding any potential bias, ensuring that the order in which the responses were presented does not affect your judgment.</p>
<h2>User Question</h2>
<p>QUESTION</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Response A</th>
<th style="text-align: left;">Response B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ANGWER_A</td>
<td style="text-align: left;">ANGWER_B</td>
</tr>
</tbody>
</table>
<p>Response Comparison</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Response A</th>
<th style="text-align: left;">Response B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ANGWER_A</td>
<td style="text-align: left;">ANGWER_B</td>
</tr>
</tbody>
</table>
<p>Figure 12: Annotation interface for multiple Elo scores.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ The US Chess Federation believes that 25 games are enough to assess a player's ability, and in our sample of 200 games, each model participated in at least 28 games. Source: https://new.uschess.org/ frequently-asked-questions-member-services-area&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ https://arena.lmsys.org/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7989 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7989</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7989</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-0885471c0215b3c0d31c82518066913f7f738128</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0885471c0215b3c0d31c82518066913f7f738128" target="_blank">Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work conducts a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting.</p>
                <p><strong>Paper Abstract:</strong> The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7989.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7989.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative Hypothesis Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Hypothesis Refinement (propose-select-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-step framework where an LM proposes N textual hypotheses from few exemplars, a task-specific interpreter scores and selects the best hypothesis (based on accuracy on seen examples), and the LM refines the chosen hypothesis using feedback; repeated up to T iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis (textual rule) induction</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Iterative hypothesis refinement (rule prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LM samples N candidate rules per iteration conditioned on k exemplars and previous feedback; each rule h is compiled by an interpreter I_tau to a function and scored on seen examples; the highest-scoring rule is used to generate feedback for the next iteration until all exemplars are covered or T iterations reached.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>raw accuracy; task accuracy; seen-example score s(h,D^s)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>s(h,D^s) = (1/|D^s|) sum_{(x,y) in D^s} 1[I_tau(h)(x)=y]; raw accuracy c = mean_tau a_tau across tasks; task accuracy c_t = fraction of tasks with a_tau=1; per-task accuracy a_tau = fraction correct on unseen examples for task tau.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACRE; MiniSCAN; List Functions; MiniARC</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human studies separate (see human-eval entry); iterative refinement itself evaluated automatically using interpreters and unseen examples; main results reported using GPT-4 (T in {1,3}, N in {1,5}).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Best config (T=3,N=5) achieved raw accuracy: ACRE 82.5, MiniSCAN 93.3, List Fns 71.2, MiniARC 18.7; task accuracy: ACRE 59.0, MiniSCAN 85.0, List Fns 61.2, MiniARC 14.6.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LM-induced rules comparable or better than human-induced on List Functions; LM rules less human-like and harder to interpret on MiniARC (see §4.3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires task-specific interpreters; limited by LM context length (T,N constrained); may overfit to seen-exemplar accuracy scoring; refinement often makes only minor edits; brittle to noisy or unfamiliar exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7989.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7989.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scoring on Seen Examples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scoring Function: Accuracy over Seen Examples s(h, D^s)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A selection criterion used during iterative refinement that rates candidate hypotheses by the proportion of seen exemplars they correctly predict after compilation by the interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation criterion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Seen-example accuracy scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute s(h,D^s) = (1/|D^s|) sum 1[I_tau(h)(x)=y] over the set of k seen exemplars; use argmax over candidate hypotheses to select best rule each iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>seen-example accuracy (score s)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Range 0–1 (or expressed as percentage); fraction of seen exemplars correctly predicted by compiled hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied in experiments on ACRE, MiniSCAN, List Functions, MiniARC</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used as selection criterion; authors note this can encourage overfitting to exemplars (e.g., high seen accuracy not always translating to unseen generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Optimizing for seen-example accuracy can select hypotheses that fit exemplars but fail to generalize (observed in MiniSCAN and other tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7989.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7989.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Raw Accuracy (c)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Raw Accuracy (mean per-example accuracy across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard metric computing the mean per-example correctness across all tasks' unseen examples, used to evaluate induced rules or direct IO predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / ML evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Raw accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute a_tau for each task as fraction correct on unseen examples, then average a_tau across tasks to get c.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>raw accuracy c</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>c = (1/|T|) sum_{tau in T} a_tau, where a_tau = (1/|D^u_tau|) sum 1[I_tau(h)(x)=y]; reported as percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACRE, MiniSCAN, List Functions, MiniARC</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported across methods; e.g., IO prompting: ACRE 64.0, MiniSCAN 61.7, List Fns 65.1, MiniARC 33.1; Iterative refinement (T=3,N=5): ACRE 82.5, MiniSCAN 93.3, List Fns 71.2, MiniARC 18.7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Raw accuracy can mask whether correct outputs arise from consistent rule application versus ad hoc per-example prediction; authors complement it with task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7989.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7989.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task Accuracy (c_t)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task Accuracy (all-unseen-examples correct per task)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stricter metric that counts a task as solved only if the induced rule produces 100% correct outputs on that task's unseen examples, then averages over tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / ML evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Task accuracy (c_t)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each task tau, set indicator 1[a_tau = 1] (all unseen examples correct); average this indicator across tasks to get c_t.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>task accuracy c_t</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>c_t = (1/|T|) sum_{tau in T} 1[a_tau = 1]; reported as fraction or percentage of tasks perfectly solved.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACRE, MiniSCAN, List Functions, MiniARC</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Shown to be lower than raw accuracy for IO prompting; e.g., iterative refinement (T=3,N=5) task accuracies: ACRE 59.0, MiniSCAN 85.0, List Fns 61.2, MiniARC 14.6.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>More indicative of consistent rule use across examples; however, requires multiple unseen examples per task and can be harsh for partially-correct generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7989.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7989.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Per-task accuracy (a_tau)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-task Unseen-Example Accuracy (a_tau)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Accuracy computed per task as the fraction of unseen examples the induced rule correctly predicts; used to compute raw and task accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / ML evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Per-task unseen-example accuracy (a_tau)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Apply compiled rule I_tau(h) to each unseen input x_i in D_tau^u to get y_i'; compute a_tau = (1/|D^u|) sum 1[y_i'=y_i].</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>a_tau (per-task accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Range 0–1 (or percentage); fraction of unseen examples correct for task tau.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACRE, MiniSCAN, List Functions, MiniARC</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used throughout evaluations; differences between a_tau when using symbolic interpreter vs LM-as-interpreter highlighted (large drops when LM interprets).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Depends on correctness of interpreter; if interpreter misapplies h, a_tau may underrepresent hypothesis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7989.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7989.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-specific Symbolic Interpreter (I_tau)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-the-shelf deterministic interpreter (e.g., grammar parser or executing compiled code) that translates a textual hypothesis h into an executable function I_tau(h) used to test hypotheses on examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (external component)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / neuro-symbolic methods</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation framework / execution backend</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Symbolic interpretation and execution</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each proposed hypothesis h (natural language or code), use a task-specific interpreter (e.g., Python executor, quasi-synchronous CFG parser) to compile and execute h on inputs to obtain outputs for scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>enables automated correctness checks (feeds into a_tau, s(h,D^s), c, c_t)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Interpreter deterministically maps textual hypothesis to a function f: X->Y; correctness of mapping judged by whether I_tau(h)(x)=y.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used with ACRE (set membership checker), MiniSCAN (quasi-synchronous CFG), List Functions (NL->Python then execute), MiniARC (Python grid interpreter).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Use of symbolic interpreters substantially improves measured rule-application performance vs using LM as interpreter; authors report large drops when replacing symbolic interpreter with LM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Perfect interpreters may not always be available; translations from free-form text to code can introduce errors; different interpreters may apply the same rule differently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7989.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7989.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-as-Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using the Language Model as Interpreter (LM-as-interpreter) for Rule Application</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alternative evaluation where the LM itself is prompted to apply an induced rule h to novel examples rather than using a symbolic executor, used to probe whether the LM understands its own proposed rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (and GPT-3.5, Claude-2, LLaMA2-70B in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0613 (primary)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / LM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation method / internal consistency test</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LM-as-interpreter rule application</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide the LM with its previously induced rule and novel inputs and ask it to produce outputs; compare LM outputs to ground truth to evaluate whether it can apply its own rules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>raw accuracy and task accuracy when LM interprets rules</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Same definitions for a_tau, c, c_t but using LM outputs as I_tau(h)(x).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACRE, MiniSCAN, List Functions, MiniARC</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Substantial performance drops when LM interprets: e.g., task accuracy for MiniSCAN drops from >80% (symbolic) to near 0% (LM interpreter) in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Shows LMs may propose plausible rules but fail to apply them reliably; advanced prompting (SC, 0-CoT) did not close gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7989.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7989.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IO prompting, Self-Consistency (SC), Self-Refine (SR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline prompting strategies compared to rule prompting: IO prompting predicts outputs directly; SC samples multiple outputs and takes majority; SR iteratively refines using the LM as its own interpreter (no symbolic interpreter).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (comparative experiments also use GPT-3.5, Claude-2, LLaMA2-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0613 (primary)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / prompting methods</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation methods / baselines</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>IO prompting; Self-Consistency; Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>IO: directly prompt LM to output y for each x; SC: sample N outputs and take majority vote; SR: LM generates hypothesis and uses itself to score and refine (no external interpreter).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>raw accuracy and task accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Same as raw/task accuracy definitions above; SC uses majority voting across sampled outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACRE, MiniSCAN, List Functions, MiniARC</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>IO and SC often produce higher raw but lower task accuracy (inconsistency); SR underperforms when LM used as interpreter versus symbolic interpreter; e.g., SR (T=3,N=5) raw: ACRE 70.0, MiniSCAN 46.3, List Fns 67.4, MiniARC 15.1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>IO prompting may achieve correct outputs without consistent rule use; SR lacks symbolic interpreter and performs worse on rule application tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7989.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7989.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Eval: Clarity & Supportiveness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Evaluation using Clarity and Supportiveness (pairwise comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Crowdworker-based human evaluation where annotators write rules and other annotators rate LM- vs human-induced rules on clarity and supportiveness using pairwise labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (LM whose rules were evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>human-subject evaluation / HCI / cognitive science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human evaluation of induced hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human pairwise rule evaluation (clarity & supportiveness)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For sampled tasks, 3 annotators write rules (human), LM also generates rules; for each LM-human rule pair, 3 separate annotators perform pairwise comparison with labels: LM better, human better, equally good, equally bad; metrics are clarity (rule explanatory quality) and supportiveness (alignment to examples).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>pairwise preference labels aggregated; qualitative judgments of clarity/supportiveness</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Categorical labels (LM better / human better / equally good / equally bad) per pair; aggregated counts or proportions reported in analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>List Functions (50 tasks sampled), MiniARC (50 tasks sampled)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>For each task: 3 annotators authored rules; for each rule pair, 3 annotators evaluated; 50 tasks sampled per dataset; metrics: clarity and supportiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LM rules comparable or better than humans on List Functions in many cases; on MiniARC human rules more pragmatic and interpretable (humans used commonsense, high-level actions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LM-induced rules were often more verbose and less pragmatically communicative than human rules; humans produced higher-quality rules for MiniARC.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human study not exhaustive across all setups; crowdworker judgments can be subjective and affected by prompt framing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7989.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7989.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OOD Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Out-of-Distribution (OOD) Generalization Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Testing generalization by evaluating induced rules on inputs outside the training exemplar distribution, e.g., longer lists for List Functions and larger grids for MiniARC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / generalization</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>generalization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>OOD generalization test (longer/larger inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Fix the seen exemplars, then evaluate induced rule on unseen inputs that are out-of-distribution in length/size (e.g., longer lists, larger grids) to assess whether the rule captures generalizable operations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>raw and task accuracies on OOD examples</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Same accuracy metrics computed on OOD unseen sets (reported as percentages).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>List Functions (longer lists), MiniARC (larger grids)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>IO prompting experienced larger OOD degradation versus rule prompting; rule prompting showed less severe degradation except MiniARC task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>OOD sets constructed heuristically (longer sequences, larger grids) and limited by exemplar underspecification; MiniSCAN OOD not considered due to ambiguity from few exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7989.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7989.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cost & API Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>API Call Count and Monetary Cost per Task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Practical evaluation of methods in terms of average number of LM API calls per task and monetary cost (cents) computed using specific token pricing for GPT-4 and GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4-0613; gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>practical evaluation / cost analysis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>operational cost metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>API calls and dollar cost estimation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Count average LM API calls required per task for each method and compute cost using token pricing assumptions (GPT-4: $0.03/1K input, $0.06/1K output; GPT-3.5 cheaper rates).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td># API calls; cost in cents per task</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported average API calls and cost (in cents) per dataset and method configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACRE, MiniSCAN, List Functions, MiniARC</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Iterative refinement (T=3,N=5) using GPT-4: avg API calls 8.2 (ACRE), 6.3 (MiniSCAN), 17.4 (List Fns), 27.2 (MiniARC); costs (cents) reported per dataset (e.g., MiniARC cost 36.5c/task for T=3,N=5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Costs depend on tokenization, model pricing, and prompt engineering; symbolic interpreter reduces number of LM calls by reusing hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ACRE: Abstract Causal REasoning Beyond Covariation <em>(Rating: 2)</em></li>
                <li>Human few-shot learning of compositional instructions <em>(Rating: 2)</em></li>
                <li>The child as hacker: building more human-like models of learning <em>(Rating: 2)</em></li>
                <li>Playgrounds for abstraction and reasoning <em>(Rating: 2)</em></li>
                <li>Hypothesis search: Inductive reasoning with language models <em>(Rating: 2)</em></li>
                <li>Instruction induction: From few examples to natural language task descriptions <em>(Rating: 1)</em></li>
                <li>How to grow a mind: Statistics, structure, and abstraction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7989",
    "paper_id": "paper-0885471c0215b3c0d31c82518066913f7f738128",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "Iterative Hypothesis Refinement",
            "name_full": "Iterative Hypothesis Refinement (propose-select-refine)",
            "brief_description": "A three-step framework where an LM proposes N textual hypotheses from few exemplars, a task-specific interpreter scores and selects the best hypothesis (based on accuracy on seen examples), and the LM refines the chosen hypothesis using feedback; repeated up to T iterations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "gpt-4-0613",
            "scientific_domain": "computer science / machine learning",
            "theory_type": "hypothesis (textual rule) induction",
            "evaluation_method_name": "Iterative hypothesis refinement (rule prompting)",
            "evaluation_method_description": "LM samples N candidate rules per iteration conditioned on k exemplars and previous feedback; each rule h is compiled by an interpreter I_tau to a function and scored on seen examples; the highest-scoring rule is used to generate feedback for the next iteration until all exemplars are covered or T iterations reached.",
            "evaluation_metric": "raw accuracy; task accuracy; seen-example score s(h,D^s)",
            "metric_definition": "s(h,D^s) = (1/|D^s|) sum_{(x,y) in D^s} 1[I_tau(h)(x)=y]; raw accuracy c = mean_tau a_tau across tasks; task accuracy c_t = fraction of tasks with a_tau=1; per-task accuracy a_tau = fraction correct on unseen examples for task tau.",
            "dataset_or_benchmark": "ACRE; MiniSCAN; List Functions; MiniARC",
            "human_evaluation_details": "Human studies separate (see human-eval entry); iterative refinement itself evaluated automatically using interpreters and unseen examples; main results reported using GPT-4 (T in {1,3}, N in {1,5}).",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": false,
            "reported_results": "Best config (T=3,N=5) achieved raw accuracy: ACRE 82.5, MiniSCAN 93.3, List Fns 71.2, MiniARC 18.7; task accuracy: ACRE 59.0, MiniSCAN 85.0, List Fns 61.2, MiniARC 14.6.",
            "comparison_to_human_generated": true,
            "comparison_results": "LM-induced rules comparable or better than human-induced on List Functions; LM rules less human-like and harder to interpret on MiniARC (see §4.3).",
            "limitations_noted": "Requires task-specific interpreters; limited by LM context length (T,N constrained); may overfit to seen-exemplar accuracy scoring; refinement often makes only minor edits; brittle to noisy or unfamiliar exemplars.",
            "uuid": "e7989.0",
            "source_info": {
                "paper_title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Scoring on Seen Examples",
            "name_full": "Scoring Function: Accuracy over Seen Examples s(h, D^s)",
            "brief_description": "A selection criterion used during iterative refinement that rates candidate hypotheses by the proportion of seen exemplars they correctly predict after compilation by the interpreter.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "gpt-4-0613",
            "scientific_domain": "computer science / evaluation methodology",
            "theory_type": "evaluation criterion",
            "evaluation_method_name": "Seen-example accuracy scoring",
            "evaluation_method_description": "Compute s(h,D^s) = (1/|D^s|) sum 1[I_tau(h)(x)=y] over the set of k seen exemplars; use argmax over candidate hypotheses to select best rule each iteration.",
            "evaluation_metric": "seen-example accuracy (score s)",
            "metric_definition": "Range 0–1 (or expressed as percentage); fraction of seen exemplars correctly predicted by compiled hypothesis.",
            "dataset_or_benchmark": "Applied in experiments on ACRE, MiniSCAN, List Functions, MiniARC",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": false,
            "reported_results": "Used as selection criterion; authors note this can encourage overfitting to exemplars (e.g., high seen accuracy not always translating to unseen generalization).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Optimizing for seen-example accuracy can select hypotheses that fit exemplars but fail to generalize (observed in MiniSCAN and other tasks).",
            "uuid": "e7989.1",
            "source_info": {
                "paper_title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Raw Accuracy (c)",
            "name_full": "Raw Accuracy (mean per-example accuracy across tasks)",
            "brief_description": "Standard metric computing the mean per-example correctness across all tasks' unseen examples, used to evaluate induced rules or direct IO predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "gpt-4-0613",
            "scientific_domain": "computer science / ML evaluation",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "Raw accuracy",
            "evaluation_method_description": "Compute a_tau for each task as fraction correct on unseen examples, then average a_tau across tasks to get c.",
            "evaluation_metric": "raw accuracy c",
            "metric_definition": "c = (1/|T|) sum_{tau in T} a_tau, where a_tau = (1/|D^u_tau|) sum 1[I_tau(h)(x)=y]; reported as percentage.",
            "dataset_or_benchmark": "ACRE, MiniSCAN, List Functions, MiniARC",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": false,
            "reported_results": "Reported across methods; e.g., IO prompting: ACRE 64.0, MiniSCAN 61.7, List Fns 65.1, MiniARC 33.1; Iterative refinement (T=3,N=5): ACRE 82.5, MiniSCAN 93.3, List Fns 71.2, MiniARC 18.7.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Raw accuracy can mask whether correct outputs arise from consistent rule application versus ad hoc per-example prediction; authors complement it with task accuracy.",
            "uuid": "e7989.2",
            "source_info": {
                "paper_title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Task Accuracy (c_t)",
            "name_full": "Task Accuracy (all-unseen-examples correct per task)",
            "brief_description": "A stricter metric that counts a task as solved only if the induced rule produces 100% correct outputs on that task's unseen examples, then averages over tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "gpt-4-0613",
            "scientific_domain": "computer science / ML evaluation",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "Task accuracy (c_t)",
            "evaluation_method_description": "For each task tau, set indicator 1[a_tau = 1] (all unseen examples correct); average this indicator across tasks to get c_t.",
            "evaluation_metric": "task accuracy c_t",
            "metric_definition": "c_t = (1/|T|) sum_{tau in T} 1[a_tau = 1]; reported as fraction or percentage of tasks perfectly solved.",
            "dataset_or_benchmark": "ACRE, MiniSCAN, List Functions, MiniARC",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": false,
            "reported_results": "Shown to be lower than raw accuracy for IO prompting; e.g., iterative refinement (T=3,N=5) task accuracies: ACRE 59.0, MiniSCAN 85.0, List Fns 61.2, MiniARC 14.6.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "More indicative of consistent rule use across examples; however, requires multiple unseen examples per task and can be harsh for partially-correct generalization.",
            "uuid": "e7989.3",
            "source_info": {
                "paper_title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Per-task accuracy (a_tau)",
            "name_full": "Per-task Unseen-Example Accuracy (a_tau)",
            "brief_description": "Accuracy computed per task as the fraction of unseen examples the induced rule correctly predicts; used to compute raw and task accuracies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "gpt-4-0613",
            "scientific_domain": "computer science / ML evaluation",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "Per-task unseen-example accuracy (a_tau)",
            "evaluation_method_description": "Apply compiled rule I_tau(h) to each unseen input x_i in D_tau^u to get y_i'; compute a_tau = (1/|D^u|) sum 1[y_i'=y_i].",
            "evaluation_metric": "a_tau (per-task accuracy)",
            "metric_definition": "Range 0–1 (or percentage); fraction of unseen examples correct for task tau.",
            "dataset_or_benchmark": "ACRE, MiniSCAN, List Functions, MiniARC",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": false,
            "reported_results": "Used throughout evaluations; differences between a_tau when using symbolic interpreter vs LM-as-interpreter highlighted (large drops when LM interprets).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Depends on correctness of interpreter; if interpreter misapplies h, a_tau may underrepresent hypothesis quality.",
            "uuid": "e7989.4",
            "source_info": {
                "paper_title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Symbolic Interpreter",
            "name_full": "Task-specific Symbolic Interpreter (I_tau)",
            "brief_description": "An off-the-shelf deterministic interpreter (e.g., grammar parser or executing compiled code) that translates a textual hypothesis h into an executable function I_tau(h) used to test hypotheses on examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (external component)",
            "model_size": "",
            "scientific_domain": "computer science / neuro-symbolic methods",
            "theory_type": "evaluation framework / execution backend",
            "evaluation_method_name": "Symbolic interpretation and execution",
            "evaluation_method_description": "For each proposed hypothesis h (natural language or code), use a task-specific interpreter (e.g., Python executor, quasi-synchronous CFG parser) to compile and execute h on inputs to obtain outputs for scoring.",
            "evaluation_metric": "enables automated correctness checks (feeds into a_tau, s(h,D^s), c, c_t)",
            "metric_definition": "Interpreter deterministically maps textual hypothesis to a function f: X-&gt;Y; correctness of mapping judged by whether I_tau(h)(x)=y.",
            "dataset_or_benchmark": "Used with ACRE (set membership checker), MiniSCAN (quasi-synchronous CFG), List Functions (NL-&gt;Python then execute), MiniARC (Python grid interpreter).",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": true,
            "reported_results": "Use of symbolic interpreters substantially improves measured rule-application performance vs using LM as interpreter; authors report large drops when replacing symbolic interpreter with LM.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Perfect interpreters may not always be available; translations from free-form text to code can introduce errors; different interpreters may apply the same rule differently.",
            "uuid": "e7989.5",
            "source_info": {
                "paper_title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LM-as-Interpreter",
            "name_full": "Using the Language Model as Interpreter (LM-as-interpreter) for Rule Application",
            "brief_description": "Alternative evaluation where the LM itself is prompted to apply an induced rule h to novel examples rather than using a symbolic executor, used to probe whether the LM understands its own proposed rules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (and GPT-3.5, Claude-2, LLaMA2-70B in comparisons)",
            "model_size": "gpt-4-0613 (primary)",
            "scientific_domain": "computer science / LM evaluation",
            "theory_type": "evaluation method / internal consistency test",
            "evaluation_method_name": "LM-as-interpreter rule application",
            "evaluation_method_description": "Provide the LM with its previously induced rule and novel inputs and ask it to produce outputs; compare LM outputs to ground truth to evaluate whether it can apply its own rules.",
            "evaluation_metric": "raw accuracy and task accuracy when LM interprets rules",
            "metric_definition": "Same definitions for a_tau, c, c_t but using LM outputs as I_tau(h)(x).",
            "dataset_or_benchmark": "ACRE, MiniSCAN, List Functions, MiniARC",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": false,
            "reported_results": "Substantial performance drops when LM interprets: e.g., task accuracy for MiniSCAN drops from &gt;80% (symbolic) to near 0% (LM interpreter) in many cases.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Shows LMs may propose plausible rules but fail to apply them reliably; advanced prompting (SC, 0-CoT) did not close gaps.",
            "uuid": "e7989.6",
            "source_info": {
                "paper_title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Prompting Baselines",
            "name_full": "IO prompting, Self-Consistency (SC), Self-Refine (SR)",
            "brief_description": "Baseline prompting strategies compared to rule prompting: IO prompting predicts outputs directly; SC samples multiple outputs and takes majority; SR iteratively refines using the LM as its own interpreter (no symbolic interpreter).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (comparative experiments also use GPT-3.5, Claude-2, LLaMA2-70B)",
            "model_size": "gpt-4-0613 (primary)",
            "scientific_domain": "computer science / prompting methods",
            "theory_type": "evaluation methods / baselines",
            "evaluation_method_name": "IO prompting; Self-Consistency; Self-Refine",
            "evaluation_method_description": "IO: directly prompt LM to output y for each x; SC: sample N outputs and take majority vote; SR: LM generates hypothesis and uses itself to score and refine (no external interpreter).",
            "evaluation_metric": "raw accuracy and task accuracy",
            "metric_definition": "Same as raw/task accuracy definitions above; SC uses majority voting across sampled outputs.",
            "dataset_or_benchmark": "ACRE, MiniSCAN, List Functions, MiniARC",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "IO and SC often produce higher raw but lower task accuracy (inconsistency); SR underperforms when LM used as interpreter versus symbolic interpreter; e.g., SR (T=3,N=5) raw: ACRE 70.0, MiniSCAN 46.3, List Fns 67.4, MiniARC 15.1.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "IO prompting may achieve correct outputs without consistent rule use; SR lacks symbolic interpreter and performs worse on rule application tasks.",
            "uuid": "e7989.7",
            "source_info": {
                "paper_title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Human Eval: Clarity & Supportiveness",
            "name_full": "Human Evaluation using Clarity and Supportiveness (pairwise comparisons)",
            "brief_description": "Crowdworker-based human evaluation where annotators write rules and other annotators rate LM- vs human-induced rules on clarity and supportiveness using pairwise labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (LM whose rules were evaluated)",
            "model_size": "gpt-4-0613",
            "scientific_domain": "human-subject evaluation / HCI / cognitive science",
            "theory_type": "human evaluation of induced hypotheses",
            "evaluation_method_name": "Human pairwise rule evaluation (clarity & supportiveness)",
            "evaluation_method_description": "For sampled tasks, 3 annotators write rules (human), LM also generates rules; for each LM-human rule pair, 3 separate annotators perform pairwise comparison with labels: LM better, human better, equally good, equally bad; metrics are clarity (rule explanatory quality) and supportiveness (alignment to examples).",
            "evaluation_metric": "pairwise preference labels aggregated; qualitative judgments of clarity/supportiveness",
            "metric_definition": "Categorical labels (LM better / human better / equally good / equally bad) per pair; aggregated counts or proportions reported in analysis.",
            "dataset_or_benchmark": "List Functions (50 tasks sampled), MiniARC (50 tasks sampled)",
            "human_evaluation_details": "For each task: 3 annotators authored rules; for each rule pair, 3 annotators evaluated; 50 tasks sampled per dataset; metrics: clarity and supportiveness.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "LM rules comparable or better than humans on List Functions in many cases; on MiniARC human rules more pragmatic and interpretable (humans used commonsense, high-level actions).",
            "comparison_to_human_generated": true,
            "comparison_results": "LM-induced rules were often more verbose and less pragmatically communicative than human rules; humans produced higher-quality rules for MiniARC.",
            "limitations_noted": "Human study not exhaustive across all setups; crowdworker judgments can be subjective and affected by prompt framing.",
            "uuid": "e7989.8",
            "source_info": {
                "paper_title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "OOD Evaluation",
            "name_full": "Out-of-Distribution (OOD) Generalization Evaluation",
            "brief_description": "Testing generalization by evaluating induced rules on inputs outside the training exemplar distribution, e.g., longer lists for List Functions and larger grids for MiniARC.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "gpt-4-0613",
            "scientific_domain": "machine learning / generalization",
            "theory_type": "generalization evaluation",
            "evaluation_method_name": "OOD generalization test (longer/larger inputs)",
            "evaluation_method_description": "Fix the seen exemplars, then evaluate induced rule on unseen inputs that are out-of-distribution in length/size (e.g., longer lists, larger grids) to assess whether the rule captures generalizable operations.",
            "evaluation_metric": "raw and task accuracies on OOD examples",
            "metric_definition": "Same accuracy metrics computed on OOD unseen sets (reported as percentages).",
            "dataset_or_benchmark": "List Functions (longer lists), MiniARC (larger grids)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": true,
            "reproducibility_assessment": false,
            "reported_results": "IO prompting experienced larger OOD degradation versus rule prompting; rule prompting showed less severe degradation except MiniARC task accuracy.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "OOD sets constructed heuristically (longer sequences, larger grids) and limited by exemplar underspecification; MiniSCAN OOD not considered due to ambiguity from few exemplars.",
            "uuid": "e7989.9",
            "source_info": {
                "paper_title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Cost & API Metrics",
            "name_full": "API Call Count and Monetary Cost per Task",
            "brief_description": "Practical evaluation of methods in terms of average number of LM API calls per task and monetary cost (cents) computed using specific token pricing for GPT-4 and GPT-3.5.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4; GPT-3.5",
            "model_size": "gpt-4-0613; gpt-3.5-turbo-0613",
            "scientific_domain": "practical evaluation / cost analysis",
            "theory_type": "operational cost metric",
            "evaluation_method_name": "API calls and dollar cost estimation",
            "evaluation_method_description": "Count average LM API calls required per task for each method and compute cost using token pricing assumptions (GPT-4: $0.03/1K input, $0.06/1K output; GPT-3.5 cheaper rates).",
            "evaluation_metric": "# API calls; cost in cents per task",
            "metric_definition": "Reported average API calls and cost (in cents) per dataset and method configuration.",
            "dataset_or_benchmark": "ACRE, MiniSCAN, List Functions, MiniARC",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Iterative refinement (T=3,N=5) using GPT-4: avg API calls 8.2 (ACRE), 6.3 (MiniSCAN), 17.4 (List Fns), 27.2 (MiniARC); costs (cents) reported per dataset (e.g., MiniARC cost 36.5c/task for T=3,N=5).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Costs depend on tokenization, model pricing, and prompt engineering; symbolic interpreter reduces number of LM calls by reusing hypotheses.",
            "uuid": "e7989.10",
            "source_info": {
                "paper_title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ACRE: Abstract Causal REasoning Beyond Covariation",
            "rating": 2
        },
        {
            "paper_title": "Human few-shot learning of compositional instructions",
            "rating": 2
        },
        {
            "paper_title": "The child as hacker: building more human-like models of learning",
            "rating": 2
        },
        {
            "paper_title": "Playgrounds for abstraction and reasoning",
            "rating": 2
        },
        {
            "paper_title": "Hypothesis search: Inductive reasoning with language models",
            "rating": 2
        },
        {
            "paper_title": "Instruction induction: From few examples to natural language task descriptions",
            "rating": 1
        },
        {
            "paper_title": "How to grow a mind: Statistics, structure, and abstraction",
            "rating": 1
        }
    ],
    "cost": 0.0193745,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement</h1>
<p>Linlu Qiu ${ }^{1}$; Liwei Jiang ${ }^{2,3}$, Ximing Lu ${ }^{2,3}$, Melanie Sclar ${ }^{3}$, Valentina Pyatkin ${ }^{2,3}$, Chandra Bhagavatula ${ }^{2}$, Bailin Wang ${ }^{1}$, Yoon Kim ${ }^{1}$, Yejin Choi ${ }^{2,3}$, Nouha Dziri ${ }^{2}$, Xiang Ren ${ }^{2,4}$<br>${ }^{1}$ Massachusetts Institute of Technology, ${ }^{2}$ Allen Institute for Artificial Intelligence<br>${ }^{3}$ University of Washington, ${ }^{4}$ University of Southern California<br>linluqiu@mit.edu</p>
<h4>Abstract</h4>
<p>The ability to derive underlying principles from a handful of observations and then generalize to novel situations-known as inductive reasoning-is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Inductive reasoning, i.e., the ability to identify common patterns and form high-level abstractions from limited observations, is considered key to human intelligence (Lake et al., 2017; Chollet, 2019). For instance, humans can quickly identify the generalizable list operation rule "selecting the first item" based on only a few observations (Figure 1, top). Although the precise cognitive mechanisms behind inductive reasoning remain unknown, one compelling hypothesis in cognitive science posits that humans approach this challenge through an iterative process that involves proposing hypotheses, testing them against observations, and refining them accordingly (Heit, 2000; Fränken et al., 2022). Returning to the above example, while the hypothesis "selecting the smallest item" may seem plausible based on the first two examples, applying this rule to the final example reveals the need for refinement, ultimately favoring "selecting the first item" as a more accurate hypothesis.</p>
<p>With the increasing power of state-of-the-art LMs (OpenAI, 2023; Anthropic, 2023), there is growing interest in exploring these models' reasoning capabilities vis-à-vis human inductive reasoning. How are their performances and underlying mechanisms similar to (and contrasted with) those of humans? This work investigates LMs' inductive reasoning capabilities through the lens of iterative</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of the iterative hypothesis refinement approach. We generate $N$ hypotheses per iteration and iterate up to the maximum number of iterations $T$ (top). Example instances and representative good and bad rules for each task (bottom).
hypothesis refinement: hypotheses generation, selection, and refinement. Specifically, we use an LM to propose a set of free-form or constrained hypotheses based on observations. The proposed hypotheses are then verified against observations via off-the-shelf symbolic interpreters ${ }^{2}$, e.g., grammar parsers or code interpreters, which can determine if an hypothesis applies to specific instances. The hypothesis that covers most number of observations is then selected to be further refined by the LM. This process is repeated to induce the final hypothesis.</p>
<p>Results across four distinct tasks, including inducing causal relations (Zhang et al., 2021), languagelike compositional instructions (Lake et al., 2019), symbolic operations (Rule, 2020), and visual concepts (Kim et al., 2022b), show that this iterative hypothesis refinement process significantly improves upon standard input-output (IO) prompting. We find that LMs are particularly good at generating candidate rules, and when coupled with a symbolic interpreter that can provide accurate feedback with which to refine hypotheses, this hybrid induction approach is effective.</p>
<p>However, a closer inspection of the refinement pipeline reveals a more nuanced view of the putative inductive reasoning process of LMs. Despite being able to generate plausible candidate rules, LMs display a range of puzzling counterintuitive behaviors. For one, while we might expect humans to be able to apply the rules they propose, we find that LMs are often unable to correctly apply their own proposed rules (§4.1). Moreover, while humans can make robust inductions by abstracting away from small perturbations present in examples (e.g., different representational forms of examples), we observe LMs to be highly brittle in the face of even minor perturbations (§4.2). Finally, a human study reveals that rules induced by LMs generally have different content and form compared to rules generated by humans. LMs often provide verbose descriptions of patterns but fail to leverage pragmatic communication strategies commonly seen in human induction (§4.3).</p>
<p>Our study unveils the paradoxical inductive capabilities of LMs: they are simultaneously phenomenal hypothesis proposers and puzzling inductive reasoners. Our paper connects to classical approaches for concept induction (Tenenbaum et al., 2011; Ellis et al., 2023, i.a.), latent language optimization (Andreas et al., 2018; Mu et al., 2020, i.a.), and instruction induction (Honovich et al., 2023). While similar in spirit to recent work on exploring inductive reasoning with LMs (Wang et al., 2023a), our work offers a nuanced exploration of both the potentials and limitations of LMs.</p>
<h1>2 Inductive Reasoning with LMs: Experimental Setup</h1>
<p>We consider the rule induction problem of inferring a function $f: \mathcal{X} \rightarrow \mathcal{Y}$ that maps an input $x \in \mathcal{X}$ to an output $y \in \mathcal{Y}$. The rule, $f$, can take various forms, such as mathematical operations, grammar,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and even natural language descriptions (see Appendix D for examples). For each task $\tau$, we have a set of examples $\mathcal{D}<em _tau="\tau">{\tau}$ consisting of input-output pairs $(x, y)$. We further divide $\mathcal{D}</em>}$ into seen examples, $\mathcal{D<em _tau="\tau">{\tau}^{s}$, and unseen examples, $\mathcal{D}</em>}^{u}$. The goal is to induce the $f$ that best describes $\mathcal{D<em _tau="\tau">{\tau}$ using only $\mathcal{D}</em>}^{s}$. A good rule thus requires a balance between precision and coverage, i.e., it should be simultaneously expressive enough to capture $\mathcal{D<em _tau="\tau">{\tau}^{s}$ and generalizable to $\mathcal{D}</em>$.}^{u</p>
<p>We assess an LM’s ability to induce rules through prompting. Let $h \in \Sigma^{<em>}$ be a rule generated by an LM, where $\Sigma$ is the LM’s vocabulary. Since we cannot directly apply $h$ to $x$ ( $h$ is just a piece of text), we make use of an interpreter $I_{\tau}: \Sigma^{</em>} \rightarrow \mathcal{F}$ for each task $\tau$ where $\mathcal{F}$ is the space of all functions from $\mathcal{X}$ to $\mathcal{Y}$ (i.e., $f \in \mathcal{F}$ ). That is, the interpreter $I_{\tau}$ "compiles" $h$ into a function that can be applied to $x$. ${ }^{3}$ The quality of rules is evaluated based on their performance on unseen examples. Given an induced rule $h$ and $n$ unseen examples $\mathcal{D}<em 1="1">{\tau}^{u}=\left{\left(x</em>$,}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right}$, we derive outputs $y_{i}^{\prime}$ by applying $I_{\tau}(h)$ to input $x_{i</p>
<p>$$
y_{i}^{\prime}=I_{\tau}(h)\left(x_{i}\right)
$$</p>
<p>Although it is ideal to have interpreters that can correctly apply $h$, such perfect interpreters might not always be available. Importantly, interpreters have no access to $\mathcal{D}_{\tau}^{s}$, and thus, the rule must contain sufficient information for interpreters to achieve strong performance when applying the rule.</p>
<p>We evaluate the quality of a rule $h$ using accuracy. More formally, for a task $\tau$ containing a set of unseen examples $\mathcal{D}_{\tau}^{u}$, we first define the accuracy for this particular task as</p>
<p>$$
a_{\tau}=\frac{1}{\left|\mathcal{D}<em _in="\in" _mathcal_D="\mathcal{D" _x_="(x," y_="y)">{\tau}^{u}\right|} \sum</em><em _tau="\tau">{\tau}^{u}} \mathbb{1}\left[I</em>(h)(x)=y\right]
$$</p>
<p>Let $\mathcal{T}$ denotes the set of all tasks within a dataset. We define raw accuracy $c$ and task accuracy $c_{t}$ as</p>
<p>$$
c=\frac{1}{|\mathcal{T}|} \sum_{\tau \in \mathcal{T}} a_{\tau} \quad c_{t}=\frac{1}{|\mathcal{T}|} \sum_{\tau \in \mathcal{T}} \mathbb{1}\left[a_{\tau}=1\right]
$$</p>
<p>While raw accuracy is the standard metric used in prior work, task accuracy could better estimate an LM's induction capability: a model should ideally consistently solve examples within a task. We use GPT-4 (gpt-4-0613; OpenAI, 2023) for all experiments and analyses. We include additional results of other models, including GPT-3.5 (gpt-3.5-turbo-0613), Claude-2 (Anthropic, 2023), and LLaMA2-70B (Touvron et al., 2023) in Appendix B.</p>
<h1>2.1 Iterative Hypothesis Refinement</h1>
<p>We consider an iterative approach to induce rules from LMs. We use LMs to propose a set of rules (i.e., hypotheses). We then select the best rule based on scores calculated using the interpreter function. We provide feedback to LMs for further refinement. See Figure 1 for an overview.</p>
<p>Specifically, given $k$ exemplars $\mathcal{D}<em 1="1">{\tau}^{s}=\left{\left(x</em>\right}$, from a prompted LM,}, y_{1}\right), \ldots,\left(x_{k}, y_{k}\right)\right}$, at iteration $t$, we sample $N$ hypotheses of rules, $H^{t}=\left{h_{1}^{t}, \ldots, h_{N}^{t</p>
<p>$$
h^{t} \sim P_{\mathrm{LM}}\left(\cdot \mid d^{t-1}, x_{1}, y_{1}, \ldots, x_{k}, y_{k}\right)
$$</p>
<p>where $d^{t-1}$ is the feedback from previous iterations and which is set to be an empty string at the initial iteration. Each hypothesis is re-ranked based on a scoring function $s\left(h, \mathcal{D}_{\tau}^{s}\right)$. We use accuracy over seen examples as the scoring function,</p>
<p>$$
s\left(h, \mathcal{D}<em _tau="\tau">{\tau}^{s}\right)=\frac{1}{\left|\mathcal{D}</em>}^{s}\right|} \sum_{(x, y) \in \mathcal{D<em _tau="\tau">{\tau}^{s}} \mathbb{1}\left[I</em>(h)(x)=y\right]
$$</p>
<p>The best hypothesis is selected via,</p>
<p>$$
h^{t^{*}}=\underset{h^{\prime} \in H^{t}}{\arg \max } s\left(h^{\prime}, \mathcal{D}_{\tau}^{s}\right)
$$</p>
<p>We then obtain feedback $d^{t}$ by passing the best hypothesis to a template-based feedback generator. The feedback $d^{t}$ is a concatenation of exemplars with incorrect predictions, formatted as input, expected output, and tentative output. The iteration stops if the interpreter produces correct outputs for all exemplars using the current hypothesis or if the maximum iteration $T$ is reached. In all experiments, we consider a combination of maximum number of iterations $T \in{1,3}$ and number of hypotheses per iteration $N \in{1,5}$. We use greedy decoding when generating a single hypothesis and set the temperature to 0.7 when generating multiple hypotheses following Wang et al. (2023b).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2.2 DATASETS</h1>
<p>The above framework requires three specifications: the rule function $f$, the representation (i.e., the format and content) of $h$, and the interpreter $I$. We evaluate on 4 datasets (where each dataset consists of multiple tasks) and formulate these specifications as follows (see examples in Figure 1). We show the full dataset details in Appendix A.</p>
<p>ACRE. The Abstract Causal REasoning (ACRE; Zhang et al., 2021) is a diagnostic dataset designed to evaluate causal induction ability. It requires identifying a set of "Blicket" objects that will trigger a special machine. We can view $f$ as an indicator function s.t. $f(x ; B)=\mathbb{1}[B \cap x]$ where $B$ is the set of Blickets and $x$ is the presented objects. We constrain $h$ to classify each object into one of three categories: a Blicket, not a Blicket, or undetermined. $I(h)(x)$ is thus a deterministic function that checks the intersections between current objects and predicted Blickets.</p>
<p>MiniSCAN. Lake et al. (2019) developed a sequence-to-sequence task with only 14 training examples to measure few-shot concept learning ability. We refer to this as MiniSCAN following Nye et al. (2020). ${ }^{4}$ Similar to SCAN (Lake \&amp; Baroni, 2018), MiniSCAN requires translating an input command $x$ to an output action sequence $y$. We consider $f$ as a set of grammar rules that map the input symbols to the corresponding meaning representations. We use a quasi-synchronous context free grammar (Smith \&amp; Eisner, 2006) as our formalism for $h$ and use a parser as our interpreter $I(h)$.</p>
<p>List Functions. The List Functions dataset (Rule, 2020) is designed to evaluate human and machine concept learning ability. It requires identifying a function that maps the input list to its corresponding output list. Here $f$ is a primitive or compositional list operation. We represent $h$ as natural language description and implement the interpreter $I$ using a two-stage process. First, we ask an LM to translate the natural language hypothesis $h$ into a Python program. Then we execute this program to produce the corresponding outputs for given inputs. ${ }^{5}$</p>
<p>MiniARC. The Abstract Reasoning Corpus (ARC; Chollet, 2019) and its variants (Kim et al., 2022b; Acquaviva et al., 2022; Xu et al., 2023b; Moskvichev et al., 2023) aim to evaluate visual abstract reasoning over broad concepts. The $f$ here involves a transformation between input and output 2D grids, such as moving an object or swapping colors. We use natural language hypotheses $h$ and similarly interpret the hypotheses using a Python interpreter. Given the extensive grid size of the original ARC tasks and the limited context length of LMs, we consider MiniARC (Kim et al., 2022b), a 5x5 compact version of the ARC.</p>
<h2>3 LMS ARE PHENOMENAL HYPOTHESIS PROPOSERS</h2>
<p>Main Results. We compare hypothesis refinement with standard input-output (IO) prompting, self-consistency prompting (SC; Wang et al., 2023b), and self-refine (SR; Madaan et al., 2023). ${ }^{6}$ SC samples multiple outputs and selects the most consistent one by taking a majority vote. SR uses the same LM as an interpreter and provides feedback to itself, and is a "pure LM" baseline that does not utilize a symbolic interpreter. The results are shown in Table 1 (see Appendix C for existing human performance). Iterative hypothesis refinement achieves the strongest performance on 3 out of 4 datasets, demonstrating the effectiveness of this approach. However, it lags behind the baselines on raw accuracy of MiniARC, potentially because some tasks in MiniARC are heavily dependent on pattern matching, for which IO prompting might be more effective (Mirchandani et al., 2023). Additionally, due to the limited visual understanding capabilities inherent in text-only models, the performance on MiniARC is still far from optimal for all methods, in comparison to other datasets. ${ }^{7}$
Similar to prior work (Chen et al., 2023a; Olausson et al., 2023; Peng et al., 2023, i.a.), sampling more hypotheses and using iterative refinement with external feedback significantly boost LM per-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Iterative hypothesis refinement results. $T$ refers to the maximum number of iterations. $N$ refers to the number of candidate hypotheses per iteration.</p>
<p>|  | Raw Accuracy |  |  |  | Task Accuracy |  |  |  |
| Method | ACRE | MiniSCAN | List Fns | MiniARC | ACRE | MiniSCAN | List Fns | MiniARC |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| IO | 64.0 | 61.7 | 65.1 | 33.1 | 28.0 | 0.0 | 39.6 | 13.8 |
| SC (N=5) | 65.0 | 61.1 | 65.0 | 31.3 | 29.0 | 0.0 | 38.0 | 13.1 |
| SR (T=3, N=5) | 70.0 | 46.3 | 67.4 | 15.1 | 32.0 | 0.0 | 52.0 | 9.2 |
| T=1, N=1 | 78.2 | 77.0 | 51.6 | 5.9 | 45.0 | 46.0 | 42.4 | 3.8 |
| T=1, N=5 | 79.8 | 86.6 | 62.4 | 12.8 | 48.0 | 70.0 | 52.4 | 9.2 |
| T=3, N=1 | 77.8 | 98.2 | 61.7 | 10.1 | 47.0 | 95.0 | 52.8 | 6.9 |
| T=3, N=5 | 82.5 | 93.3 | 71.2 | 18.7 | 59.0 | 85.0 | 61.2 | 14.6 |</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results for IID and OOD examples. For OOD evaluations, we sample longer lists for List Functions and annotate larger grids for MiniARC. IO prompting generally experiences more significant performance degradation compared to rule prompting (i.e., iterative hypothesis refinement).
formance, leading to the best accuracy on the majority of datasets. ${ }^{8}$ It is important to emphasize that both iterative refinement and external feedback are essential. Simply sampling more predictions and taking a majority vote (as SC prompting), does not necessarily improve performance. This might due to the fact that increasing the temperature for sampling results in many incorrect predictions. In contrast, increasing the number of hypotheses performs better due to the hypotheses selection process. An iterative approach that uses the LM itself as an interpreter (i.e., SR) is also insufficient. We observe performance substantially degrades when replacing the symbolic interpreter with an LM, suggesting that the LM can excel as a hypothesis proposer but performs poorly as an interpreter.</p>
<p>We also observe a significant discrepancy between raw accuracy and task accuracy, especially for IO prompting and SC prompting. Since these evaluations directly predict output for each individual example, there is no guarantee that the LM is solving the task following the underlying rules. In fact, the mismatch between raw accuracy and task accuracy indicates some correct predictions might be generated without using the expected computation. In contrast, rule prompting (i.e., applying the LM-proposed rules) suffers less from this issue as it re-uses the same rule across all examples.</p>
<p>OOD Generalization and Interpretability. In addition to strong performance, iterative hypothesis refinement also enables out-of-distribution (OOD) generalization and improves interpretability of models. For OOD evaluations, we sample longer examples from the ground-truth programs for List Functions, ${ }^{9}$ and annotate examples with a larger grid for MiniARC. We evaluate performance on these OOD examples while fixing the seen examples. We show the results in Figure 2. We observe a significant performance drop for OOD examples when using IO prompting. However, the degradation is less severe for rule prompting except task accuracy on MiniARC, suggesting the LM likely solves the task using generalizable operations. While IO prompting still achieves better raw accuracy on MiniARC in OOD evaluations, the performance gap between it and rule prompting is reduced. Rule prompting also allows us to examine the intermediate operations, thus improving the interpretability of models. We show examples of LM-induced rules in Table 2 and Table 12.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4 LMS ARE PUZZLING INDUCTIVE REASONERS</h1>
<p>Despite the strong performance of iterative refinement on inductive reasoning, we identify several puzzling behaviors of LMs that seems to differ from human intuition (Fukushima, 1986; Heit, 2000). We include related human studies and human evaluations in Appendix C. ${ }^{10}$</p>
<h3>4.1 LMS STRUGGLE WITH APPLYING THEIR PROPOSED RULES</h3>
<p>Previous results demonstrate that LMs perform as effective hypothesis proposers but poor interpreters. Here we examine the extent to which LMs "understand" the rules they propose. Specifically, given the rules induced from previous experiments, we test whether LMs can apply these rules to novel examples. We should expect comparable performance if LMs understand their own proposed rules. Results are shown in Figure 3. We observe a consistent performance drop when using the LM interpreter as opposed to the symbolic interpreter. This issue is especially significant on datasets like MiniSCAN, where rule application involves complex and recursive operations.
This performance inconsistency between rule induction and rule application reveals a counterintuitive behavior of LMs on inductive reasoning. Intuitively, once humans have induced a rule, they can use this rule in novel scenarios. However, LMs struggle with applying the rule, even if the rule was derived from themselves. Note that prior work has provided evidence suggesting that LMs might fall short on solving symbolic tasks (Dziri et al., 2023), and we do not claim that we should expect using an LM as the interpreter perform as effectively as a symbolic interpreter. However, the gaps are often so large (e.g., task accuracy dropping from more than $80 \%$ to almost-zero in MiniSCAN) that they are still nonetheless strong indicators of LMs' puzzling behaviors. ${ }^{11}$ In particular, LMs are able to generate meaningful hypotheses and improve them iteratively, but simultaneously fail to understand their proposed rules. This observation can be loosely related to other inconsistencies observed between generation and recognition in existing LMs (West et al., 2023). For instance, while LMs can identify errors in their own generations (Agrawal et al., 2023; Zhang et al., 2023b), they may also fail to validate a correct answer generated by themselves (Li et al., 2023).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Raw accuracy (left) and task accuracy (right) when applying the LM's proposed rules using symbolic interpreters or the LM itself as the interpreter.</p>
<h3>4.2 LMS ARE BRITTLE TO EXAMPLE PERTURBATIONS</h3>
<p>Our experiments so far only consider well-formed input-output pairs: we assume there always exists at least one ground-truth $f$ such that applying $f$ to the inputs will yield the corresponding outputs. We also assume examples are presented in the format that close to LM's pre-training distribution. However, in practice, low-quality examples are ubiquitous. Humans can often reason robustly despite a certain level of noise, such as disregarding typos or a few erroneous examples (Fukushima, 1986; Heit, 2000). We now investigate if LMs can similarly make robust inductions. We use iterative hypothesis refinement with $T=3$ and $N=5$, which has the strongest performance in our main experiments. We include results using other models and configurations in Appendix B.3.
Noisiness of Exemplars. We first study LMs' robustness to noisy examples. Specifically, we use List Functions and introduce noise into a certain percentage of exemplars by randomly replacing 1-2</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>elements with other numbers in the outputs. We perturb 12.5%, 25%, and 50% of the examples, out of a total of 8 exemplars. We show results in Figure 4a. We find the LM performs substantially worse even with a single noisy example, and its performance consistently decreases as the amount of noise increases. Although explicitly instructing the LM to consider noisy examples (dashed line) mitigates this issue, the performance degradation remains significant (see Table 17 for the exact instructions). This brittleness raises another concerns about their otherwise promising performance. ${ }^{12}$
Familiarity of Exemplars. Next we study if LMs are robust to example representation. We examine this by varying the familiarity of exemplars, i.e., how well the examples are represented in the LMs' pre-training data. As rules represent higher-level abstraction, ideally we should expect LMs' performance to be independent of their specific instantiations (Newell, 1980). We use MiniSCAN dataset and re-generate new examples using the same grammar rules but with varied output vocabularies. We consider two variants: the first involves pseudowords as inputs with abstract English concepts as outputs (e.g., dax $\rightarrow$ RED), as the original setup in Lake et al. (2019). The second uses pseudowords for both inputs and outputs (e.g., dax $\rightarrow$ zup). The results are shown in Figure 4b. LMs' performance drops when the output representation deviates from their pre-training distribution. In such case, even an iterative approach cannot compensate for this degradation. ${ }^{13}$
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) Varying example noisiness by perturbing a certain percentage of exemplars on List Functions. Dashed lines refer to results where we explicitly instruct LMs to consider noisy examples. (b) Varying example familiarity by using English words or pseudo-words as outputs on MiniSCAN.</p>
<h1>4.3 LM-INDUCED RULES VS. HUMAN-INDUCED RULES</h1>
<p>We have provided empirical evidence suggesting some discrepancies between inductive reasoning of LMs and humans. We now qualitatively examine if LM-induced rules are distinguishable from human-induced rules. We conduct analysis on List Functions and MiniARC, as they contain various concepts and represent tasks where the LM succeeds and fails, respectively. We randomly sample 50 tasks per dataset and conduct similar human studies by asking crowdworkers to write the rules.
We show example LM-induced rules and human-induced rules in Table 2. For List Functions where the LM achieves strong performance, the LM can often induce rules that are comparable to or even better than those from humans, with some exceptions where it incorrectly explains the pattern. On MiniARC, however, it tends to generate rules that are difficult to interpret, often involving verbose and complex descriptions. In contrast, similar to Acquaviva et al. (2022), we find that humans often use pragmatic communication strategies that go beyond pattern descriptions. For instance, they frequently draw upon physical commonsense knowledge (e.g., "drop or lift an object", "fill in each box"), use high-level actions (e.g., "copy or extend the block", "mirror the group"), and connect to real-world concepts (e.g., "staircase", "Tetris"). They also pose questions (e.g., "which color is more common in a row and by how many?") and utilize algebraic expressions (e.g., "if a number is repeated $n$ times then only output $n-1$ times") to facilitate effective communications.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Comparison between LM-induced rules and human-induced rules on List Functions (top) and MiniARC (bottom). 0 maps to black, 1 maps to blue, and 4 maps to yellow.</p>
<table>
<thead>
<tr>
<th>Examples</th>
<th>LM-induced Rule</th>
<th>Human-induced Rules</th>
</tr>
</thead>
<tbody>
<tr>
<td>$[97,97,97,97] \rightarrow[97,97,97]$</td>
<td>Remove the last occurrence of each unique</td>
<td>Annotator 1: Keep the order of the original</td>
</tr>
<tr>
<td>$[4,4,4] \rightarrow[4,4]$</td>
<td>number from the input list, but if a number</td>
<td>list but only include integers that are duplicates</td>
</tr>
<tr>
<td>$[33,0,6,1,2,24,66] \rightarrow[]$</td>
<td>appears more than twice, keep all instances</td>
<td>rom earlier in the list.</td>
</tr>
<tr>
<td>$[76,62,17,76,17] \rightarrow[76,17]$</td>
<td>except the last.</td>
<td>Annotator 2: Output only the repeated</td>
</tr>
<tr>
<td>$\ldots$</td>
<td></td>
<td>numbers. If a number is repeated n times</td>
</tr>
<tr>
<td></td>
<td></td>
<td>then output only n-1 times.</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>If an element in the input array is 4, replace it with 0 . If the element is 1 and its left and right neighboring elements are 0 , replace it with 1 . If the element is 1 and positioned in the last row of the array, replace it with 1. In all other cases, replace the element with 0 .</p>
<p>[10]*</p>
<p>[10]*</p>
<p>[10]*</p>
<p>[10]*</p>
<p>We further investigate how LMs refine their hypotheses. While we observe performance improvement over iterations (see Figure 5), we also notice that they tend to make minor modifications, typically by adding exceptions for a specific example, rather than starting from entirely new hypotheses. We observe several cases where the LM adds an "if-else" statement to the rules over iterations. For instance, the LM generates "Remove the value 2 from the input list." in the first iteration and refines it to "Remove the value 6 if it is present in the input list. If not, remove the value 2" in the subsequent iteration. This results in its failure to induce the correct rules if the initial hypothesis is entirely off.</p>
<h1>5 LIMITATIONS AND DISCUSSIONS</h1>
<p>Tasks. Humans perform inductive reasoning in everyday situations (Hume, 1904). However, our experiments mainly focus on synthetic and symbolic tasks, differing from the typical scenarios in which humans perform inductions. We chose our datasets based on two concerns. First, we interact with LMs using prompting. This restricts the number of seen examples due to LMs' limited context lengths. We selected our tasks because they are relatively constrained and well-defined, making it feasible to induce rules from only a few observations. ${ }^{14}$ Second, the inaccessibility of the training data complicates the evaluation of LMs' inductive learning abilities. It is challenging to distinguish whether LMs truly induce rules from observed examples or simply recall the fact from their prior knowledge. Therefore, we chose more synthetic and symbolic tasks, as we hypothesize that they are less likely to be present in the pre-training data, thus making inducing rules from observations necessary. Nonetheless, this confounding factor remains unless we fully inspect the training corpus.</p>
<p>Hyperparameters. The goal of this paper is to explore the potentials and limitations of LMs in inductive reasoning, rather than to improve the performance on specific inductive reasoning tasks. Therefore, we did not exhaustively tune hyperparameters $(T$ and $N$ ) or prompt templates. Our experiments use a maximum iteration $T=3$ due to the LMs' limited context lengths and a maximum number of hypotheses per iteration $N=5$. Our results demonstrate the correlations between model performance and these two hyperparameters. We expect improved performance when increasing these two hyperparameters, as suggested by Table 1 and recent work by Wang et al. (2023a).</p>
<p>Future Directions. Our study demonstrates the effectiveness of using LMs as hypothesis proposers. We show that, when paired with symbolic interpreters, LMs can achieve strong performance</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>through iterative hypothesis refinement. However, out-of-the-box LMs struggle to solve inductive tasks on their own. This strengthens the need to explore neuro-symbolic approaches to utilize the strengths of both components (Ni et al., 2023; Wong et al., 2023, i.a.). Our study also only focuses on a fixed set of exemplars. Future work could explore methods to dynamically select the best exemplars. Additionally, our analyses identify several counter-intuitive model behaviors, highlighting the importance of understanding model behaviors and improving their robustness as future work.</p>
<h1>6 RELATED WORK</h1>
<p>Inductive Reasoning with LMs. Existing studies on inductive reasoning capabilities of pretrained large LMs (Gendron et al., 2023; Yang et al., 2022; Moskvichev et al., 2023; Mirchandani et al., 2023; Tang et al., 2023; Xu et al., 2023a; Han et al., 2023; Xu et al., 2023b; Alet et al., 2021; Webb et al., 2023) primarily use IO prompting. They focus on evaluating the accuracy of unseen examples but often overlook any intermediate operations. As we argue in our study, this evaluation lacks interpretability and can conflate with LMs' rule application abilities. We instead investigate an alternative evaluation by inducing rules from LMs. Similarly, Honovich et al. (2023) uses LMs to induce instructions from examples, but it only studies dataset-level instructions for simple tasks. Concurrent work (Wang et al., 2023a) that proposes hypothesis search is closest to ours, but we focus on understanding the potentials and limitations of LMs rather than improving LMs' performance.</p>
<p>Language Hypotheses Optimization. Many studies have explored the optimization of hypotheses over the space of natural language. Prior work on latent language for concept learning has mostly focused on few-shot image classification, and often involves training models (Andreas et al., 2018; Mu et al., 2020). Vong \&amp; Lake (2022) uses a pre-trained LM, but does not involve refining hypotheses iteratively. Some studies adopt similar iterative frameworks but focus on describing differences between text distributions (Zhong et al., 2022; 2023) or data patterns (Singh et al., 2022). While these hypotheses are relatively coarse-grained, our tasks require fine-grained hypotheses with high precision. Our study shows that, in such cases, a symbolic interpreter is essential to ensure the quality of hypotheses. Additionally, the iterative refinement approach is also related to a line of work on iterative prompting with execution feedback for synthesizing programs (Chen et al., 2023a; Olausson et al., 2023; Haluptzok et al., 2022; Key et al., 2022; Jiang et al., 2023; Zhang et al., 2023a). However, most of these studies use natural language descriptions, sometimes supplemented with optional examples, while ours only use input-output specifications.</p>
<p>Bayesian Concept Learning. Classical approaches to induction primarily follow a Bayesian paradigm: they start with a hypothesis space, compute the posterior distribution using Bayes' Rule, and update beliefs based on observations (Tenenbaum et al., 2011; Lake et al., 2015; Xu \&amp; Tenenbaum, 2007; Tenenbaum, 1999; Thaker et al., 2017; Kemp \&amp; Tenenbaum, 2009). The main challenge of these methods is the trade-off between expressiveness of hypothesis space and computational cost of posterior inference. Therefore, many of them resolve on searching over a constrained rule-based hypothesis space, such as probabilistic programs (Nosofsky et al., 1994; Piantadosi et al., 2016; Goodman et al., 2008; Bramley et al., 2018; Ellis et al., 2022; 2023). However, a domainspecific formulation of Language of Thought (Fodor, 1975; Erdogan et al., 2015; Saad et al., 2019; Tian et al., 2020; Sablé-Meyer et al., 2022) is often limited. Ellis (2023) addresses this by performing Bayesian inference over natural language. Our approach shares similar spirits of Bayesian models, but instead leverages LMs to generate and refine hypotheses via iterative prompting.</p>
<h2>7 CONCLUSION</h2>
<p>In this paper, we study the inductive reasoning capabilities of LMs and how their inductive reasoning behaviors differ from those of humans. We conduct this investigation through iterative hypothesis refinement, an approach that closely mirrors human inductive process. Iterative refinement operates as a three-step process: hypotheses generation, selection, and refinement. Through our experiments, we find that LMs excel as hypothesis proposers, achieving strong performance on most datasets when coupled with symbolic interpreters. However, we also identify several counter-intuitive behaviors, suggesting that LMs simultaneously behave as puzzling inductive reasoners. For instance, they struggle with applying their own proposed rules and are brittle to minor perturbations. Our study reveals the paradoxical inductive capabilities of LMs and sheds light on both the potentials and limitations of LMs in inductive reasoning tasks.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>We thank Jiangjie Chen, Peter Hase, Aniruddha Nrusimha, Kyle Richardson, Zhaofeng Wu, and AI2 Mosaic team for helpful comments and discussions. We thank Jena Hwang, Yufei Tian, and Huirong Wen for the help with human study and data annotation. This work was supported in-part by DARPA MCS program through NIWC Pacific (N66001-19-2-4031) and NSF (DMS-2134012). LQ, BW, and YK were partially supported by MIT-IBM Watson AI and an Amazon award. XR's research is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-22072200006, the Defense Advanced Research Projects Agency with award HR00112220046, and NSF IIS 2048211.</p>
<h2>REFERENCES</h2>
<p>Sam Acquaviva, Yewen Pu, Marta Kryven, Theodoros Sechopoulos, Catherine Wong, Gabrielle Ecanow, Maxwell Nye, Michael Tessler, and Josh Tenenbaum. Communicating natural programs to humans and machines. Advances in Neural Information Processing Systems, 35:3731-3743, 2022.</p>
<p>Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. Do language models know when they're hallucinating references? ArXiv preprint, abs/2305.18248, 2023. URL https://arxiv. org/abs/2305.18248.</p>
<p>Ekin Akyurek and Jacob Andreas. Lexicon learning for few shot sequence modeling. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4934-4946, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.acl-long.382. URL https://aclanthology.org/2021.acl-long.382.</p>
<p>Ferran Alet, Javier Lopez-Contreras, James Koppel, Maxwell I. Nye, Armando Solar-Lezama, Tomás Lozano-Pérez, Leslie Pack Kaelbling, and Joshua B. Tenenbaum. A large-scale benchmark for few-shot program induction and synthesis. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 175-186. PMLR, 2021. URL http://proceedings.mlr.press/v139/alet21a.html.</p>
<p>Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2166-2179, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1197. URL https://aclanthology.org/N18-1197.</p>
<p>Anthropic. Claude 2, 2023. URL https://www.anthropic.com/index/claude-2.
BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5Bvos 3 .</p>
<p>Neil Bramley, Anselm Rothe, Josh Tenenbaum, Fei Xu, and Todd Gureckis. Grounding compositional hypothesis generation in specific instances. In Proceedings of the 40th annual conference of the cognitive science society, 2018.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. ArXiv preprint, abs/2211.12588, 2022. URL https://arxiv.org/abs/2211.12588.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. ArXiv preprint, abs/2304.05128, 2023a. URL https://arxiv.org/abs/ 2304.05128 .</p>
<p>Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, and Kathleen McKeown. Do models explain themselves? counterfactual simulatability of natural language explanations. ArXiv preprint, abs/2307.08678, 2023b. URL https://arxiv.org/abs/ 2307.08678 .</p>
<p>François Chollet. On the measure of intelligence. ArXiv preprint, abs/1911.01547, 2019. URL https://arxiv.org/abs/1911.01547.</p>
<p>Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. Language models show human-like content effects on reasoning. ArXiv preprint, abs/2207.07051, 2022. URL https://arxiv.org/abs/2207. 07051 .</p>
<p>Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1286-1305, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 98. URL https://aclanthology.org/2021.emnlp-main. 98.</p>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D Hwang, et al. Faith and fate: Limits of transformers on compositionality. ArXiv preprint, abs/2305.18654, 2023. URL https://arxiv.org/ abs/2305.18654.</p>
<p>Kevin Ellis. Modeling human-like concept learning with bayesian inference over natural language. ArXiv preprint, abs/2306.02797, 2023. URL https://arxiv.org/abs/2306.02797.</p>
<p>Kevin Ellis, Adam Albright, Armando Solar-Lezama, Joshua B Tenenbaum, and Timothy J O’Donnell. Synthesizing theories of human language with bayesian program induction. Nature communications, 13(1):5024, 2022.</p>
<p>Kevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lore Anaya Pozo, Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. Philosophical Transactions of the Royal Society A, 381(2251):20220050, 2023.</p>
<p>Goker Erdogan, Ilker Yildirim, and Robert A Jacobs. From sensory signals to modality-independent conceptual representations: A probabilistic language of thought approach. PLoS computational biology, 11(11):e1004610, 2015.</p>
<p>Jonathan St BT Evans and Jodie Curtis-Holmes. Rapid responding increases belief bias: Evidence for the dual-process theory of reasoning. Thinking \&amp; Reasoning, 11(4):382-389, 2005.</p>
<p>Jerry A Fodor. The language of thought, volume 5. Harvard university press, 1975.
Jan-Philipp Fränken, Nikos C Theodoropoulos, and Neil R Bramley. Algorithms of adaptation in inductive inference. Cognitive Psychology, 137:101506, 2022.</p>
<p>Kunihiko Fukushima. A neural network model for selective attention in visual pattern recognition. Biological Cybernetics, 55(1):5-15, 1986.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764-10799. PMLR, 2023.</p>
<p>Gaël Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie. Large language models are not abstract reasoners. ArXiv preprint, abs/2305.19555, 2023. URL https : //arxiv.org/abs/ 2305.19555 .</p>
<p>Noah D Goodman, Joshua B Tenenbaum, Jacob Feldman, and Thomas L Griffiths. A rational analysis of rule-based concept learning. Cognitive science, 32(1):108-154, 2008.</p>
<p>Alison Gopnik, David M Sobel, Laura E Schulz, and Clark Glymour. Causal learning mechanisms in very young children: two-, three-, and four-year-olds infer causal relations from patterns of variation and covariation. Developmental psychology, 37(5):620, 2001.</p>
<p>Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to program better. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Simon Jerome Han, Keith J Ransom, Andrew Perfors, and Charles Kemp. Inductive reasoning in humans and large language models. Cognitive Systems Research, pp. 101155, 2023.</p>
<p>Evan Heit. Properties of inductive reasoning. Psychonomic Bulletin \&amp; Review, 7:569-592, 2000.
Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. Instruction induction: From few examples to natural language task descriptions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1935-1952, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.108. URL https://aclanthology.org/2023.acl-long. 108.</p>
<p>Yi Hu, Haotong Yang, Zhouchen Lin, and Muhan Zhang. Code prompting: a neural symbolic method for complex reasoning in large language models. ArXiv preprint, abs/2305.18507, 2023. URL https://arxiv.org/abs/2305.18507.</p>
<p>David Hume (ed.). Enquiry Concerning Human Understanding. Clarendon Press, 1904.
Shuyang Jiang, Yuhao Wang, and Yu Wang. Selfevolve: A code evolution framework via large language models. ArXiv preprint, abs/2306.02907, 2023. URL https://arxiv.org/abs/ 2306.02907.</p>
<p>Aysja Johnson, Wai Keen Vong, Brenden M Lake, and Todd M Gureckis. Fast and flexible: Human program induction in abstract reasoning tasks. arXiv preprint arXiv:2103.05823, 2021.</p>
<p>Charles Kemp and Joshua B Tenenbaum. Structured statistical models of inductive reasoning. Psychological review, 116(1):20, 2009.</p>
<p>Darren Key, Wen-Ding Li, and Kevin Ellis. I speak, you verify: Toward trustworthy neural program synthesis. ArXiv preprint, abs/2210.00848, 2022. URL https://arxiv.org/abs/2210. 00848 .</p>
<p>Najoung Kim, Tal Linzen, and Paul Smolensky. Uncontrolled lexical exposure leads to overestimation of compositional generalization in pretrained models. ArXiv preprint, abs/2212.10769, 2022a. URL https://arxiv.org/abs/2212.10769.</p>
<p>Subin Kim, Prin Phunyaphibarn, Donghyun Ahn, and Sundong Kim. Playgrounds for abstraction and reasoning. In NeurIPS 2022 Workshop on Neuro Causal and Symbolic AI (nCSI), 2022b. URL https://openreview.net/forum?id=F4RNpByoqP.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, $35: 22199-22213,2022$.</p>
<p>Brenden M. Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 2879-2888. PMLR, 2018. URL http://proceedings.mlr. press/v80/lake18a.html.</p>
<p>Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332-1338, 2015.</p>
<p>Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, 40:e253, 2017. doi: 10.1017/S0140525X16001837.</p>
<p>Brenden M. Lake, Tal Linzen, and Marco Baroni. Human few-shot learning of compositional instructions. In Annual Meeting of the Cognitive Science Society, 2019. URL https://api. semanticscholar.org/CorpusID:58006558.</p>
<p>Xiang Lisa Li, Vaishnavi Shrivastava, Siyan Li, Tatsunori Hashimoto, and Percy Liang. Benchmarking and improving generator-validator consistency of language models. ArXiv preprint, abs/2310.01846, 2023. URL https://arxiv.org/abs/2310.01846.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. ArXiv preprint, abs/2303.17651, 2023. URL https://arxiv.org/abs/ 2303.17651.</p>
<p>Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 157-165, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.18. URL https://aclanthology.org/2022. acl-short. 18 .</p>
<p>Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. ArXiv preprint, abs/2307.04721, 2023. URL https://arxiv.org/abs/2307. 04721 .</p>
<p>Melanie Mitchell, Alessandro B Palmarini, and Arseny Moskvichev. Comparing humans, gpt-4, and gpt-4v on abstraction and reasoning tasks. arXiv preprint arXiv:2311.09247, 2023.</p>
<p>Arseny Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. The conceptarc benchmark: Evaluating understanding and generalization in the arc domain. ArXiv preprint, abs/2305.07141, 2023. URL https://arxiv.org/abs/2305.07141.</p>
<p>Jesse Mu, Percy Liang, and Noah Goodman. Shaping visual representations with language for few-shot classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4823-4830, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.436. URL https://aclanthology.org/2020. acl-main. 436 .</p>
<p>Allen Newell. Physical symbol systems. Cognitive science, 4(2):135-183, 1980.
Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 26106-26128. PMLR, 2023.</p>
<p>Robert M Nosofsky, Thomas J Palmeri, and Stephen C McKinley. Rule-plus-exception model of classification learning. Psychological review, 101(1):53, 1994.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>Maxwell I. Nye, Armando Solar-Lezama, Josh Tenenbaum, and Brenden M. Lake. Learning compositional rules via neural program synthesis. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/ paper/2020/hash/7a685d9edd95508471a9d3d6fcace432-Abstract.html.</p>
<p>Theo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando SolarLezama. Demystifying gpt self-repair for code generation. ArXiv preprint, abs/2306.09896, 2023. URL https://arxiv.org/abs/2306.09896.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Daniel N. Osherson, Ormond Wilkie, Edward E. Smith, Alejandro López, and Eldar Shafir. Category-based induction. Psychological Review, 97(2):185-200, 1990. ISSN 0033-295X. doi: 10.1037/0033-295X.97.2.185.</p>
<p>Arkil Patel, Satwik Bhattamishra, Phil Blunsom, and Navin Goyal. Revisiting the compositional generalization abilities of neural sequence models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 424-434, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short. 46. URL https://aclanthology.org/2022.acl-short. 46.</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. ArXiv preprint, abs/2302.12813, 2023. URL https://arxiv.org/abs/2302.12813.</p>
<p>Steven T Piantadosi, Joshua B Tenenbaum, and Noah D Goodman. The logical primitives of thought: Empirical foundations for compositional cognitive models. Psychological review, 123(4):392, 2016.</p>
<p>Joshua Stewart Rule. The child as hacker: building more human-like models of learning. PhD thesis, Massachusetts Institute of Technology, 2020.</p>
<p>Feras A Saad, Marco F Cusumano-Towner, Ulrich Schaechte, Martin C Rinard, and Vikash K Mansinghka. Bayesian synthesis of probabilistic programs for automatic data modeling. Proceedings of the ACM on Programming Languages, 3(POPL):1-32, 2019.</p>
<p>Mathias Sablé-Meyer, Kevin Ellis, Josh Tenenbaum, and Stanislas Dehaene. A language of thought for the mental representation of geometric shapes. Cognitive Psychology, 139:101527, 2022.</p>
<p>Swarnadeep Saha, Peter Hase, Nazneen Rajani, and Mohit Bansal. Are hard examples also harder to explain? a study with human and model-generated explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2121-2131, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main. 137.</p>
<p>Chandan Singh, John X Morris, Jyoti Aneja, Alexander M Rush, and Jianfeng Gao. Explaining patterns in data with language models via interpretable autoprompting. ArXiv preprint, abs/2210.01848, 2022. URL https://arxiv.org/abs/2210.01848.</p>
<p>David Smith and Jason Eisner. Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies. In Proceedings on the Workshop on Statistical Machine Translation, pp. 23-30, New York City, June 2006. Association for Computational Linguistics. URL https: //aclanthology.org/W06-3104.</p>
<p>David M Sobel, Joshua B Tenenbaum, and Alison Gopnik. Children's causal inferences from indirect evidence: Backwards blocking and bayesian reasoning in preschoolers. Cognitive science, 28(3):303-333, 2004.</p>
<p>Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. Large language models are in-context semantic reasoners rather than symbolic reasoners. ArXiv preprint, abs/2305.14825, 2023. URL https://arxiv.org/abs/2305.14825.</p>
<p>Joshua Tenenbaum. Rules and similarity in concept learning. Advances in neural information processing systems, 12, 1999.</p>
<p>Joshua B. Tenenbaum, Charles Kemp, Thomas L. Griffiths, and Noah D. Goodman. How to grow a mind: Statistics, structure, and abstraction. Science, 331(6022):1279-1285, 2011. doi: 10.1126/ science.1192788. URL https://www.science.org/doi/abs/10.1126/science. 1192788 .</p>
<p>Pratiksha Thaker, Joshua B Tenenbaum, and Samuel J Gershman. Online learning of symbolic concepts. Journal of Mathematical Psychology, 77:10-20, 2017.</p>
<p>Lucas Y. Tian, Kevin Ellis, Marta Kryven, and Josh Tenenbaum. Learning abstract structure for drawing by efficient motor program induction. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),</p>
<p>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1c104b9c0accfca52ef21728eaf01453-Abstract.html.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Wai Keen Vong and Brenden M. Lake. Few-shot image classification by generating natural language rules. In ACL Workshop on Learning with Natural Language Supervision, 2022. URL https: //openreview.net/forum?id=BxfpZP2sZq.</p>
<p>Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D Goodman. Hypothesis search: Inductive reasoning with language models. ArXiv preprint, abs/2309.05660, 2023a. URL https://arxiv.org/abs/2309.05660.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023b. URL https://openreview.net/pdf?id= 1PL1NIMMrw.</p>
<p>Taylor Webb, Keith J Holyoak, and Hongjing Lu. Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9):1526-1541, 2023.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.</p>
<p>Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, et al. The generative ai paradox: "what it can create, it may not understand". In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, Vikash K Mansinghka, Jacob Andreas, and Joshua B Tenenbaum. From word models to world models: Translating from natural language to the probabilistic language of thought. ArXiv preprint, abs/2306.12672, 2023. URL https://arxiv.org/abs/2306.12672.</p>
<p>Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. Are large language models really good logical reasoners? a comprehensive evaluation from deductive, inductive and abductive views. ArXiv preprint, abs/2306.09841, 2023a. URL https://arxiv.org/abs/ 2306.09841 .</p>
<p>Fei Xu and Joshua B Tenenbaum. Word learning as bayesian inference. Psychological review, 114 (2):245, 2007.</p>
<p>Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, and Elias B Khalil. Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. ArXiv preprint, abs/2305.18354, 2023b. URL https://arxiv.org/abs/2305. 18354 .</p>
<p>Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. Language models as inductive reasoners. ArXiv preprint, abs/2212.10923, 2022. URL https://arxiv.org/abs/2212.10923.</p>
<p>Chi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, and Yixin Zhu. ACRE: abstract causal reasoning beyond covariation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 10643-10653. Computer Vision Foundation / IEEE, 2021. doi: 10.1109/CVPR46437.2021.01050. URL https://openaccess.thecvf. com/content/CVPR2021/html/Zhang_ACRE_Abstract_Causal_REasoning_ Beyond_Covariation_CVPR_2021_paper.html.</p>
<p>Kexun Zhang, Danqing Wang, Jingtao Xia, William Yang Wang, and Lei Li. Algo: Synthesizing algorithmic programs with generated oracle verifiers. ArXiv preprint, abs/2305.14591, 2023a. URL https://arxiv.org/abs/2305.14591.</p>
<p>Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model hallucinations can snowball. ArXiv preprint, abs/2305.13534, 2023b. URL https://arxiv. org/abs/2305.13534.</p>
<p>Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt. Describing differences between text distributions with natural language. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 27099-27116. PMLR, 2022. URL https://proceedings.mlr. press/v162/zhong22a.html.</p>
<p>Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. ArXiv preprint, abs/2302.14233, 2023. URL https://arxiv.org/abs/2302.14233.</p>
<h1>A DATASET DETAILS</h1>
<p>We show the dataset statistics in Table 3 and include the full dataset details below.</p>
<p>ACRE Following Gendron et al. (2023), we use textual representations of the original images by representing each object with its corresponding natural language description. We also experimented with a symbolic representation in which each object is represented as an integer, but observed similar performance. We sampled 100 tasks from the original dataset for our experiments.</p>
<p>Table 3: The number of tasks per dataset, the numbers of seen examples per task, and unseen examples per task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;"># Tasks</th>
<th style="text-align: center;"># Seen</th>
<th style="text-align: center;"># Unseen</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ACRE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">MiniSCAN</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">List Functions</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">MiniARC</td>
<td style="text-align: center;">130</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>MiniSCAN We use examples from Lake et al. (2019), but randomly sample pseudowords for the inputs. We did not consider English words because of potential issues of data contamination (Dodge et al., 2021; Magar \&amp; Schwartz, 2022, i.a.) and uncontrolled lexical exposure (Kim et al., 2022a). The outputs use color names following Akyurek \&amp; Andreas (2021); Nye et al. (2020); Patel et al. (2022). We generated a total of 100 tasks for our experiments.</p>
<p>List Functions We use the original dataset (Rule, 2020), which consists of a total of 250 tasks. Due to the limited context lengths of LMs, we only use the first 16 examples from BIG-Bench (bench authors, 2023): 8 for seen examples and 8 for unseen examples. We manually examined the exemplars and found 8 examples are generally sufficient to describe the pattern. Our preliminary experiments also indicated that adding more examples did not improve performance.</p>
<p>MiniARC We use the data from Kim et al. (2022b). Although the original release contains 149 tasks, we heuristically filter out tasks that require heavy pattern matching, such as mapping one specific shape to another. Such tasks are typically difficult to describe in natural language at an abstract level. Therefore, we did not consider them for our evaluations. As we only evaluate textonly models, we use textual representations of the original visual grids by mapping each cell to a corresponding integer (Gendron et al., 2023; Moskvichev et al., 2023).</p>
<h2>B ADDITIONAL RESULTS</h2>
<h2>B. 1 OTHER LANGUAGE MODELS</h2>
<p>We use GPT-4 for the main experiments, but our observations remain consistent across other LMs, as shown in Table 4. We evaluate GPT-4, GPT-3.5, Claude-2, and LLaMA2-70B using IO prompting and iterative hypothesis refinement, as they are best representatives of two different evaluations.</p>
<p>For GPT-3.5 and Claude-2, we observe similar trends except both models underperform GPT-4. Rule prompting achieves higher accuracy than IO prompting on ACRE and MiniSCAN and shows better consistency between raw accuracy and task accuracy. However, these models sometimes lag behind the baseline on tasks involving complex rules, such as List Functions and MiniARC. For LLaMA2-70B, we only observe improvement using rule prompting on ACRE. For tasks where we constrain hypothesis representations, some models' rules appear ill-formed. Many responses from GPT-3.5 and LLaMA2-70B are also truncated due to their limited context length. This suggests that iterative hypothesis refinement is most effective when coupled with an LM that is capable of proposing meaningful hypotheses and tracking long context.</p>
<p>Table 4: Results on IO prompting and rule prompting (i.e., hypothesis refinement) using different models. We use $\mathrm{T}=3, \mathrm{~N}=5$ configuration for iterative hypothesis refinement.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Raw Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ACRE</td>
<td style="text-align: center;">MiniSCAN</td>
<td style="text-align: center;">List Fns</td>
<td style="text-align: center;">MiniARC</td>
<td style="text-align: center;">ACRE</td>
<td style="text-align: center;">MiniSCAN</td>
<td style="text-align: center;">List Fns</td>
<td style="text-align: center;">MiniARC</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IO</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">14.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IO</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">8.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IO</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">10.8</td>
</tr>
<tr>
<td style="text-align: center;">Claude-2</td>
<td style="text-align: center;">Rule</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">6.2</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-70B</td>
<td style="text-align: center;">IO</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">1.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">0.8</td>
</tr>
</tbody>
</table>
<p>Similar to experiments in $\S 4$, we show the comparisons between symbolic interpreters and LMs as interpreters for rule application using other models in Table 5. We show results on varying example distribution using different models and configurations in Table 6 and Table 7. All results remain consistent with the findings in the main experiments.</p>
<p>Table 5: Results on applying the LM's proposed rules using symbolic interpreters or the LM itself as the interpreter using different models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Interpreter</th>
<th style="text-align: center;">Raw Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ACRE</td>
<td style="text-align: center;">MiniSCAN</td>
<td style="text-align: center;">List Fns</td>
<td style="text-align: center;">MiniARC</td>
<td style="text-align: center;">ACRE</td>
<td style="text-align: center;">MiniSCAN</td>
<td style="text-align: center;">List Fns</td>
<td style="text-align: center;">MiniARC</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">1.5</td>
</tr>
<tr>
<td style="text-align: center;">Claude-2</td>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">6.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-70B</td>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">0.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">0.0</td>
</tr>
</tbody>
</table>
<p>Table 6: Results on varying example noisiness using different models and configurations. We introduce noise by perturbing a certain percentage of exemplars on List Functions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Configuration</th>
<th style="text-align: center;">Raw Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$12.5 \%$</td>
<td style="text-align: center;">$25 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$12.5 \%$</td>
<td style="text-align: center;">$25 \%$</td>
<td style="text-align: center;">$50 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">$\mathrm{T}=1, \mathrm{~N}=1$</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">12.4</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">$\mathrm{T}=3, \mathrm{~N}=5$</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">12.0</td>
</tr>
<tr>
<td style="text-align: center;">Claude-2</td>
<td style="text-align: center;">$\mathrm{T}=3, \mathrm{~N}=5$</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">4.4</td>
</tr>
</tbody>
</table>
<p>Table 7: Results on varying example familiarity using different models and configurations. We use English words or pseudo-words as outputs on MiniSCAN.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Configuration</th>
<th style="text-align: center;">Raw Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task Accuracy</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">English</td>
<td style="text-align: center;">Pseudo</td>
<td style="text-align: center;">English</td>
<td style="text-align: center;">Pseudo</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">$\mathrm{T}=1, \mathrm{~N}=1$</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">$\mathrm{T}=3, \mathrm{~N}=5$</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">3.0</td>
</tr>
<tr>
<td style="text-align: center;">Claude-2</td>
<td style="text-align: center;">$\mathrm{T}=3, \mathrm{~N}=5$</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">9.0</td>
</tr>
</tbody>
</table>
<p>Table 8: Results on MiniARC using GPT-4V. We show the results of IO prompting and rule prompting, as well as the results when applying the model's proposed rules using the symbolic interpreter or GPT-4V as the interpreter. We use a $\mathrm{T}=3, \mathrm{~N}=5$ configuration for iterative hypothesis refinement.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">Raw Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task Accuracy</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: left;">Interpreter</td>
<td style="text-align: center;">Color</td>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">Color</td>
<td style="text-align: center;">Number</td>
</tr>
<tr>
<td style="text-align: left;">IO</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: left;">Rule</td>
<td style="text-align: left;">Symbolic</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">8.5</td>
</tr>
<tr>
<td style="text-align: left;">Rule</td>
<td style="text-align: left;">GPT-4V</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.8</td>
</tr>
</tbody>
</table>
<h1>B. 2 Multimodal Model</h1>
<p>Since the MiniARC dataset requires visual understanding, evaluating text-only models using textual representations may not be optimal. Therefore, we also evaluate the performance of the multimodal model that allows visual inputs. We use GPT-4V (gpt-4-vision-preview), which was released in November 2023, for our experiments. We consider two representations of the visual grids: color and number. We use an individual image for each input and output (see Table 18 for prompts and examples). For iterative hypothesis refinement, we use GPT-4 to translate hypotheses due to the rate limit of GPT-4V. For IO prompting and rule application, we ask the model to generate the textual representation of the visual grid, representing each cell as an integer. ${ }^{15}$ We show results in Table 8. The performance of GPT-4V is significantly worse than that of GPT-4, which is consistent with the results in Mitchell et al. (2023). Performance with color representation lags behind when compared to numerical representation. Similarly, we find that using GPT-4V as a rule interpreter consistently underperforms using the symbolic interpreter.</p>
<h2>B. 3 Ablations</h2>
<p>Prompting Techniques for Rule Application. We only use standard prompting for rule application in $\S 4.1$. Here, we study whether more advanced prompting techniques improve LMs' rule application performance. We consider two alternatives: self-consistency prompting (SC; Wang et al., 2023b) and zero-shot chain-of-thought prompting (0-CoT; Kojima et al., 2022; Nye et al., 2021; Wei et al., 2022). Similar to our main experiments, SC selects the most consistent output from multiple responses by taking a majority vote. Following Kojima et al. (2022), 0-CoT adds "Let's think step by step." at the end to encourage LMs to reason. We show results in Table 9. We do not observe significant performance differences across these methods, except on ACRE, where 0-CoT underperforms other methods in task accuracy. This could potentially be attributed to the possibility that LMs do not truly understand their own proposed rules; therefore, encouraging reasoning might result in worse performance.</p>
<p>Table 9: Results on using LMs as interpreters for rule application with different prompting techniques. We compare standard prompting, zero-shot chain-of-thought ( $0-\mathrm{CoT}$ ) that adds "Let's think step by step" at the end, and self-consistency (SC) that selects the output by taking a majority vote.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Raw Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">ACRE</td>
<td style="text-align: center;">MiniSCAN</td>
<td style="text-align: center;">List Fns</td>
<td style="text-align: center;">MiniARC</td>
<td style="text-align: center;">ACRE</td>
<td style="text-align: center;">MiniSCAN</td>
<td style="text-align: center;">List Fns</td>
<td style="text-align: center;">MiniARC</td>
</tr>
<tr>
<td style="text-align: left;">Standard</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">5.4</td>
</tr>
<tr>
<td style="text-align: left;">0 -CoT</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">6.9</td>
</tr>
<tr>
<td style="text-align: left;">SC (N=5)</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">4.6</td>
</tr>
</tbody>
</table>
<p>Representation of Hypothesis. We investigate how the representation of hypothesis affects rule induction. We use programming language hypotheses for List Functions and MiniARC. We consider this alternative as existing studies have shown that prompting LMs to generate programs improves the model's performance on complex reasoning task (Gao et al., 2023; Chen et al., 2022; Hu et al.,</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>2023, i.a.). Directly using programming language hypotheses also eliminates the potential issue of mistranslation between natural language and code. As shown in Table 10, both programming language and natural language hypotheses achieve comparable performance, suggesting programming language can be a powerful alternative for these tasks.</p>
<p>Table 10: Results on using alternative hypothesis representation. We compare natural language hypotheses (NL) and programming hypotheses (Python) on List Functions and MiniARC.</p>
<table>
<thead>
<tr>
<th></th>
<th>Raw Accuracy</th>
<th></th>
<th>Task Accuracy</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Hypothesis</td>
<td>List Fns</td>
<td>MiniARC</td>
<td>List Fns</td>
<td>MiniARC</td>
</tr>
<tr>
<td>NL</td>
<td>71.2</td>
<td>18.7</td>
<td>61.2</td>
<td>14.6</td>
</tr>
<tr>
<td>Python</td>
<td>72.5</td>
<td>18.1</td>
<td>65.6</td>
<td>13.8</td>
</tr>
</tbody>
</table>
<p>Task-specific Heuristics. One reason why humans can learn new concepts from limited examples is their strong inductive biases or prior knowledge (Lake et al., 2017). We evaluate whether imposing task-specific heuristics influences LMs' inductive reasoning behaviors. Specifically, we use the MiniARC dataset, which involves visual understanding, and thus object-related heuristics could potentially be beneficial. Similar to Wang et al. (2023a), we provide explicit task-specific heuristics ${ }^{16}$ in the prompt for hypothesis generation, as shown in Table 11. We observe that the LM-induced rules become more human-readable. The LM starts to use visual concepts (e.g., "square", "rectangle", "L-shape", "U-shape") and common transformations (e.g., "reflection", "mirror", "rotate the grid 90 degrees clockwise"). We show example LM-induced rules in Table 12. However, this behavior appears only in a fraction of examples, and the rules induced by LMs are still generally distinguishable from those induced by humans. It is possible that incorporating additional guidance or adding human-induced rules as few-shot examples could encourage LMs to use pragmatic communication strategies. We leave exploring these alternatives as future work.</p>
<p>Importantly, imposing task-specific heuristics does not necessarily improve performance. Iterative hypothesis refinement with $T=3$ and $N=5$ achieves a raw accuracy of $17.8 \%$ and task accuracy of $14.6 \%$, comparable to results without task-specific heuristics. One possible reason is the integercolor mapping introducing additional overhead, as LMs frequently refer to both simultaneously in the rule (e.g., "if a pixel is green (3), then change it to red (2)"). This could also potentially be explained by observations in Acquaviva et al. (2022): human communication is expressive yet ambiguous. Therefore, the more human-readable rules might require extra efforts to ensure precision and improve performance.</p>
<h1>B. 4 ANALYSIS</h1>
<p>Our main experiments demonstrate the effectiveness of the iterative approach. In Figure 5, we show the changes of accuracy over iterations. We observe consistent performance improvements across all datasets, indicating that LMs are capable of refining their hypotheses iteratively. For tasks where LMs achieve strong performance, such as ACRE and MiniSCAN, a limited number of iterations is already sufficient. For tasks like MiniARC, where LMs perform poorly, the trends remain positive after the maximum number of iterations. This suggests potential for further improvements with more iterations when using LMs with longer context lengths.</p>
<h2>B. 5 COSTS</h2>
<p>We show the average number of API calls and the average cost per task in Table 13. For GPT-4, the cost is computed using $\$ 0.03 / 1 \mathrm{~K}$ tokens for input and $\$ 0.06 / 1 \mathrm{~K}$ tokens for output. For GPT-3.5, the cost is computed using $\$ 0.0015 / 1 \mathrm{~K}$ tokens for input and $\$ 0.002 / 1 \mathrm{~K}$ tokens for output. Iterative hypothesis refinement, when augmented with a symbolic interpreter, is more cost-efficient than SR, as it reduces the number of API calls required to apply hypotheses. It is also more cost efficient for</p>
<p><sup id="fnref10:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 11: Task-specific hypothesis generation prompt for MiniARC.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Generate</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">rule</span><span class="w"> </span><span class="nt">that</span><span class="w"> </span><span class="nt">maps</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">following</span><span class="w"> </span><span class="nt">inputs</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">their</span><span class="w"> </span><span class="nt">corresponding</span><span class="w"> </span><span class="nt">outputs</span><span class="o">.</span>
<span class="nt">Both</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">input</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">output</span><span class="w"> </span><span class="nt">are</span><span class="w"> </span><span class="nt">5x5</span><span class="w"> </span><span class="nt">grids</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">integers</span><span class="o">,</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">integer</span>
<span class="nt">representing</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">colored</span><span class="w"> </span><span class="nt">pixel</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">visual</span><span class="w"> </span><span class="nt">grid</span><span class="o">.</span><span class="w"> </span><span class="nt">The</span><span class="w"> </span><span class="nt">integers</span><span class="w"> </span><span class="nt">can</span><span class="w"> </span><span class="nt">be</span><span class="w"> </span><span class="nt">mapped</span><span class="w"> </span><span class="nt">to</span>
<span class="nt">colors</span><span class="w"> </span><span class="nt">as</span><span class="w"> </span><span class="nt">follows</span><span class="o">:</span>
<span class="nt">0</span><span class="o">:</span><span class="w"> </span><span class="nt">Black</span><span class="o">;</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">Blue</span><span class="o">;</span><span class="w"> </span><span class="nt">2</span><span class="o">:</span><span class="w"> </span><span class="nt">Red</span><span class="o">;</span><span class="w"> </span><span class="nt">3</span><span class="o">:</span><span class="w"> </span><span class="nt">Green</span><span class="o">;</span><span class="w"> </span><span class="nt">4</span><span class="o">:</span><span class="w"> </span><span class="nt">Yellow</span><span class="o">;</span><span class="w"> </span><span class="nt">5</span><span class="o">:</span><span class="w"> </span><span class="nt">Grey</span><span class="o">;</span><span class="w"> </span><span class="nt">6</span><span class="o">:</span><span class="w"> </span><span class="nt">Fuchsia</span><span class="o">;</span>
<span class="nt">7</span><span class="o">:</span><span class="w"> </span><span class="nt">Orange</span><span class="o">;</span><span class="w"> </span><span class="nt">8</span><span class="o">:</span><span class="w"> </span><span class="nt">Teal</span><span class="o">;</span><span class="w"> </span><span class="nt">9</span><span class="o">:</span><span class="w"> </span><span class="nt">Brown</span><span class="o">.</span>
<span class="nt">The</span><span class="w"> </span><span class="nt">black</span><span class="w"> </span><span class="nt">cells</span><span class="w"> </span><span class="nt">represent</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">background</span><span class="o">.</span>
<span class="nt">Hints</span><span class="o">:</span><span class="w"> </span><span class="nt">The</span><span class="w"> </span><span class="nt">transformations</span><span class="w"> </span><span class="nt">might</span><span class="w"> </span><span class="nt">include</span><span class="o">,</span><span class="w"> </span><span class="nt">but</span><span class="w"> </span><span class="nt">are</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="nt">limited</span><span class="w"> </span><span class="nt">to</span><span class="o">:</span>
<span class="nt">-</span><span class="w"> </span><span class="nt">Movement</span><span class="o">:</span><span class="w"> </span><span class="nt">Flipping</span><span class="o">,</span><span class="w"> </span><span class="nt">rotation</span><span class="o">,</span><span class="w"> </span><span class="nt">reflection</span><span class="o">,</span><span class="w"> </span><span class="nt">etc</span><span class="o">.</span>
<span class="nt">-</span><span class="w"> </span><span class="nt">Color</span><span class="o">:</span><span class="w"> </span><span class="nt">Swapping</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">rotating</span><span class="w"> </span><span class="nt">colors</span><span class="w"> </span><span class="nt">between</span><span class="w"> </span><span class="nt">objects</span><span class="o">,</span><span class="w"> </span><span class="nt">etc</span><span class="o">.</span>
<span class="nt">-</span><span class="w"> </span><span class="nt">Object</span><span class="o">:</span><span class="w"> </span><span class="nt">Moving</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">copying</span><span class="w"> </span><span class="nt">objects</span><span class="o">,</span><span class="w"> </span><span class="nt">etc</span><span class="o">.</span>
<span class="nt">-</span><span class="w"> </span><span class="nt">Number</span><span class="o">:</span><span class="w"> </span><span class="nt">Counting</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">number</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">colors</span><span class="o">,</span><span class="w"> </span><span class="nt">comparing</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">number</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">two</span><span class="w"> </span><span class="nt">colors</span><span class="o">,</span><span class="w"> </span><span class="nt">etc</span><span class="o">.</span>
<span class="nt">-</span><span class="w"> </span><span class="nt">Geometry</span><span class="o">:</span><span class="w"> </span><span class="nt">Aligning</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">completing</span><span class="w"> </span><span class="nt">objects</span><span class="o">,</span><span class="w"> </span><span class="nt">etc</span><span class="o">.</span>
<span class="nt">-</span><span class="w"> </span><span class="nt">Common</span><span class="w"> </span><span class="nt">sense</span><span class="o">:</span><span class="w"> </span><span class="nt">Finding</span><span class="w"> </span><span class="nt">mazes</span><span class="w"> </span><span class="nt">paths</span><span class="o">,</span><span class="w"> </span><span class="nt">playing</span><span class="w"> </span><span class="nt">Tetris</span><span class="o">,</span><span class="w"> </span><span class="nt">simulating</span><span class="w"> </span><span class="nt">raindrops</span><span class="o">,</span><span class="w"> </span><span class="nt">etc</span><span class="o">.</span>
<span class="nt">Please</span><span class="w"> </span><span class="nt">format</span><span class="w"> </span><span class="nt">your</span><span class="w"> </span><span class="nt">rule</span><span class="w"> </span><span class="nt">as</span><span class="w"> </span><span class="nt">follows</span><span class="o">:</span>
<span class="nt">Rule</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="nt">Your</span><span class="w"> </span><span class="nt">rule</span><span class="o">&gt;</span>
</code></pre></div>

<p>Table 12: Comparison between LM-induced rules on MiniARC with and without task-specific heuristics. We show examples where heuristics are helpful (top) and not helpful (bottom) for rule induction. 0 maps to black.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Examples</th>
<th style="text-align: center;">w/o Heuristics</th>
<th style="text-align: center;">w/ Heuristics</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><img alt="img-5.jpeg" src="img-5.jpeg" /></td>
<td style="text-align: center;">For each row in the input array, the corresponding output row is generated by taking the first element from the current row, the second element from the first row, the third element from the current row, the fourth element from the second row and the fifth element from the first row. If the first element of the current row is 0 , then the entire output row becomes 0 .</td>
<td style="text-align: center;">The input grid is mirrored along its main diagonal, i.e., the diagonal from the top-left corner to the bottom-right corner. The color mapping remains the same.</td>
</tr>
<tr>
<td style="text-align: center;"><img alt="img-6.jpeg" src="img-6.jpeg" /></td>
<td style="text-align: center;">For a given 5x5 matrix, copy the first two values of the first row to the third and fourth positions of the fourth row and the first two values of the second row to the third and fourth positions of the third row. Then, copy the second value of the second row to the last position of the last row. Leave all the other positions unchanged.</td>
<td style="text-align: center;">The output grid is formed by taking the existing colored cells (non-zero numbers) from the topleft corner of the input grid, and creating a mirror image of this pattern in the bottom-right corner. This mirror image involves not only flipping the pattern along the central vertical and horizontal axes, but also rotating the position of the colors one position to the right in each cell. The rest of the cells remain black (0).</td>
</tr>
</tbody>
</table>
<p>tasks with a larger number of test examples, e.g., MiniSCAN, as it re-uses the same rule across all examples.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 5: Raw accuracy and task accuracy over iterations.
Table 13: The average number of API calls and the average cost per task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;"># API Calls</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Cost (cent)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ACRE</td>
<td style="text-align: center;">MiniSCAN</td>
<td style="text-align: center;">List Fns</td>
<td style="text-align: center;">MiniARC</td>
<td style="text-align: center;">ACRE</td>
<td style="text-align: center;">MiniSCAN</td>
<td style="text-align: center;">List Fns</td>
<td style="text-align: center;">MiniARC</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">IO</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">6.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{SC}(\mathrm{N}=5)$</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">6.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SR (T=3, N=5)</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">28.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{T}=3, \mathrm{~N}=5$</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">36.5</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">IO</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{T}=3, \mathrm{~N}=5$</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1.6</td>
</tr>
<tr>
<td style="text-align: center;">Claude-2</td>
<td style="text-align: center;">IO</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{T}=3, \mathrm{~N}=5$</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<h1>C Human Studies</h1>
<h2>C. 1 Existing Human Performance</h2>
<p>Our experiments are largely motivated by cognitive science literature. Here, we collect results from existing human studies to better calibrate the performance of LMs and humans. It is important to note that the exact setups, data, and evaluations in these studies might differ from ours. Therefore, the reported human performance can only be used as a reference but not for direct comparison.</p>
<p>For ACRE, Gopnik et al. (2001) and Sobel et al. (2004) found that 3-4 year-old children are able to identify if an object is a Blicket within 2 trials. For MiniSCAN, Lake et al. (2019) conducted similar human experiments and found humans achieve around $80 \%$ average accuracy, with the lowest performance at around $65 \%$ and the highest at $88 \%$. For List Functions, Rule (2020) reported the average human performance of $45.9 \% .{ }^{17}$ For MiniARC, Kim et al. (2022b) did not provide any human experiment results. However, Johnson et al. (2021) evaluated a subset of tasks from ARC (Chollet, 2019) and found that human participants can solve $80 \%$ of the tasks, with $65 \%$ of tasks being solved by more than $80 \%$ of participants. Moskvichev et al. (2023) evaluated human participants on ConceptARC, a variant of ARC, and reported an average human performance of $90.9 \%$ in solving test examples. Additionally, Acquaviva et al. (2022) found that human annotators were able to write correct instructions for $88 \%$ of the ARC tasks.</p>
<h2>C. 2 SETUP</h2>
<p>We randomly sample 50 tasks for List Functions and MiniARC and ask crowdworkers to write and evaluate rules. For each task, we ask 3 annotators to write the rule, and for each rule pair, we ask another 3 annotators to evaluate them. For rule evaluation, following prior work (Saha et al., 2022; Chen et al., 2023b), we consider two metrics: clarity and supportiveness. Clarity evaluates whether the rule provides a clear explanation of the transformation from input to output. Supportivenss measures how well the rule align with examples. ${ }^{18}$ We use pairwise comparison with 4 labels: LM-induced rule is better, human-induced rule is better, equally good, and equally bad.</p>
<p><sup id="fnref11:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{17}$ The human performance was obtained by asking human participants to play a guessing game. It only requires solving unseen examples and did not involve writing rules. See Rule (2020) for details.
${ }^{18}$ Saha et al. (2022) use three metrics for evaluation: clarity, supportiveness, and generalizability. We did not consider generalizability as we directly evaluate on unseen examples. Our pilot experiments also suggest that crowdworkers found it challenging to distinguish between supportiveness and generalizability.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref10:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref11:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2210 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2210</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2210</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-58.html">extraction-schema-58</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <p><strong>Paper ID:</strong> paper-90bbc0fb2107a0fc7a15449f0e071d0dafbf97bb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/90bbc0fb2107a0fc7a15449f0e071d0dafbf97bb" target="_blank">Towards an AI co-scientist</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> An AI co-scientist is introduced, a multi-agent system built on Gemini 2.0 intended to help uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and aligned to scientist-provided research objectives and guidance.</p>
                <p><strong>Paper Abstract:</strong> Scientific discovery relies on scientists generating novel hypotheses that undergo rigorous experimental validation. To augment this process, we introduce an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI co-scientist is intended to help uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and aligned to scientist-provided research objectives and guidance. The system's design incorporates a generate, debate, and evolve approach to hypothesis generation, inspired by the scientific method and accelerated by scaling test-time compute. Key contributions include: (1) a multi-agent architecture with an asynchronous task execution framework for flexible compute scaling; (2) a tournament evolution process for self-improving hypotheses generation. Automated evaluations show continued benefits of test-time compute, improving hypothesis quality. While general purpose, we focus development and validation in three biomedical areas: drug repurposing, novel target discovery, and explaining mechanisms of bacterial evolution and anti-microbial resistance. For drug repurposing, the system proposes candidates with promising validation findings, including candidates for acute myeloid leukemia that show tumor inhibition in vitro at clinically applicable concentrations. For novel target discovery, the AI co-scientist proposed new epigenetic targets for liver fibrosis, validated by anti-fibrotic activity and liver cell regeneration in human hepatic organoids. Finally, the AI co-scientist recapitulated unpublished experimental results via a parallel in silico discovery of a novel gene transfer mechanism in bacterial evolution. These results, detailed in separate, co-timed reports, demonstrate the potential to augment biomedical and scientific discovery and usher an era of AI empowered scientists.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2210.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2210.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elo auto-eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elo-based tournament auto-evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated internal ranking metric that uses pairwise comparisons and multi-turn simulated scientific debates to assign Elo ratings to generated hypotheses and proposals, guiding resource allocation and self-improvement in the multi-agent system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Elo-based tournament auto-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / AI-driven scientific discovery / biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Hypotheses are ranked via an Elo tournament: newly generated hypotheses start at Elo 1200; top candidates are compared using multi-turn simulated debates while lower-ranked items use single-turn comparisons. Elo ratings are aggregated across tournaments and used as an auto-evaluation signal for self-improvement. Concordance with external benchmarks was assessed by grouping co-scientist responses into Elo buckets and comparing accuracy against the GPQA benchmark and Gemini 2.0 reference responses. Elo was tracked across 203 research goals and a curated subset of 15 expert problems to measure improvement with test-time compute scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Used as a predictor of quality; Elo-selected top-1 co-scientist responses achieved 78.4% top-1 accuracy on the GPQA diamond set (as reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Paper uses concordance with GPQA benchmark and expert preference as indicators; authors state Elo is an auto-evaluation metric and not independent ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>No — Elo is an internal ranking metric used to prioritize hypotheses for further review/experimentation, not a substitute for experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>No explicit simulation-vs-experiment failures reported for Elo itself; authors caution Elo may favor attributes not aligned with scientists' preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Elo rating values and bucketed average accuracies were reported; no formal confidence intervals or statistical error bars for Elo reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not designed for fabricated-result detection; paper does not claim Elo detects fabricated or AI-generated false claims.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Tournament matches (multi-turn debates for top hypotheses) are computationally intensive and scale with test-time compute; exact resource/time metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Elo is an auto-evaluation metric (not independent ground truth); may favor superficially attractive attributes; potential ordering and metric biases acknowledged by authors.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Authors present concordance between Elo and both the GPQA benchmark and expert preferences to support credibility, but explicitly caution that Elo is not a definitive ground-truth measure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared indirectly to GPQA benchmark answers and to expert preference rankings; not compared to experimental (wet-lab) gold standards.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2210.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2210.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPQA benchmark (diamond set)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A challenging multiple-choice question answering benchmark developed by domain experts in biology, physics and chemistry; used here to assess the concordance between the co-scientist's Elo ratings and actual correctness on hard scientific Q&A problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>GPQA benchmark evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biological/physical/chemical question answering (scientific QA)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The co-scientist was prompted with GPQA diamond-set questions; responses were judged against the dataset ground truth. For each question the system generated outputs; Elo buckets were then correlated with the fraction correct. A Gemini 2.0 reference of 32 responses per question provided a baseline reference accuracy per question to mitigate selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Reported top-1 accuracy for the co-scientist on the GPQA diamond set was 78.4% when selecting the top-rated (Elo) result per question.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>GPQA serves as an objective benchmark with ground-truth answers for QA-style validation in scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>No — benchmark evaluation measures QA correctness, not experimental validation of hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Accuracy percentages per Elo bucket were reported; no confidence intervals or hypothesis-test statistics were provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not applicable — GPQA assesses factual correctness vs. ground truth rather than fabrication detection.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Required generating many model responses (32 per GPQA question for the Gemini 2.0 reference) and tournament evaluation; exact compute/time costs not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Benchmark-focused and limited to question-answering tasks; does not evaluate experimental or real-world wet-lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Benchmark-based correctness is a standard computational validation approach and used here to support the Elo metric's validity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Directly compared to GPQA ground truth answers (i.e., gold standard for the QA task).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2210.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2210.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expert NIH review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expert clinical evaluation using NIH Specific Aims Page format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human expert evaluation pipeline where co-scientist outputs are re-formatted into NIH Specific Aims pages and judged by board-certified clinicians using an adapted NIH grant-review rubric to assess clinical relevance, feasibility, novelty and impact.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Expert-in-the-loop clinical evaluation (NIH Specific Aims Style)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>clinical oncology / translational biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Co-scientist drug-repurposing proposals were converted into NIH-style Specific Aims pages and reviewed by six board-certified hematologist-oncologists using a 15-axis adapted NIH rubric (significance, innovation, rigor, feasibility, etc.) on five-point scales; 78 proposals were evaluated and pre-screened to remove implausible candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not applicable as a pass/fail metric; experts rated proposals favorably across rubric axes (high proportion of 'Strongly Agree'/'Agree'), but no single numeric success rate for experimental validation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>NIH grant-review criteria and standard translational steps (preclinical in vitro/in vivo evidence, phase I–III trials) described as normative requirements for clinical acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>No — authors emphasize that expert review is complementary and that experimental verification (preclinical/in vivo/clinical trials) is required for translational claims.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Human ratings (categorical five-point scale) and average ranks reported; no standard errors or confidence intervals reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Experts performed manual pre-screening and excluded clinically implausible or unsafe candidates; this human review acts as a filter against implausible/generated claims.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Expert review is time-consuming and requires specialist effort; exact time/cost not quantified but acknowledged as resource-intensive relative to pure computation.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Single-center expert panel, potential pre-screening bias, and expert domain limits; results are subjective and not equivalent to experimental proof of efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Positive expert evaluations increase credibility for follow-up work, but authors note that favorable expert opinion is not sufficient for regulatory approval or clinical acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Expert review is positioned as an intermediary step; gold-standard clinical validation remains randomized controlled clinical trials (phase III), which none of the candidates had achieved.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2210.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2210.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wet-lab in vitro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In vitro wet-lab experimental validation (cell lines and organoids)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct biological experiments used to evaluate co-scientist predictions: cell-line viability assays for AML drug-repurposing candidates and human hepatic organoid assays for anti-fibrotic activity of novel epigenetic targets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>In vitro experimental validation (cell viability and organoid functional assays)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>experimental biomedicine / pharmacology / cell biology</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Selected candidates from co-scientist predictions were prioritized by oncologists and tested in standard in vitro assays: multiple AML cell lines were treated with candidate drugs and tumor viability was measured at clinically applicable concentrations; human hepatic organoids were used to test anti-fibrotic activity and regenerative responses. Experiments followed established protocols and were intended as initial biological validation steps; further details and full experimental protocols are provided in separate co-timed reports referenced by the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>The paper reports concordance for several predictions (e.g., AML candidates showed tumor inhibition in vitro; epigenetic targets showed anti-fibrotic activity in organoids). Quantitative comparisons (e.g., IC50 values, effect sizes) are not provided in this main text and are deferred to co-timed experimental reports.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not quantified across the entire candidate set; paper states that certain candidates showed positive in vitro activity (examples provided) but does not report an overall fraction or percentage validated.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Authors present in vitro validation as an initial preclinical step; domain standards require additional in vivo studies and randomized clinical trials for therapeutic claims.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper emphasizes simulation / computational predictions are insufficient alone for translational claims; wet-lab validation is required for biological claims.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>No explicit cases of computational predictions failing in experiments are detailed in the main manuscript; limitations and negative results may be in co-timed reports.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>The main text does not provide experimental uncertainty metrics (e.g., error bars, CI); these are likely in detailed experimental reports.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not discussed in the experimental context.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Authors note experimental validation is time-consuming and resource-intensive compared to computational phases; no explicit cost figures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Small-scale, preliminary experiments; selection bias from expert prioritization; not equivalent to in vivo or clinical validation; detailed quantitative outcomes not in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Wet-lab validation is presented as critical for acceptance; co-timed independent experimental reports increase credibility but authors caution that further preclinical/clinical testing is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Wet-lab in vitro assays contrasted with the ultimate gold standard of randomized clinical trials; authors explicitly note none of the candidates had progressed to phase III clinical trials.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2210.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2210.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In silico recapitulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In silico discovery that recapitulated unpublished experimental results</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational discovery by the co-scientist that independently proposed a mechanism (cf-PICIs interacting with diverse phage tails) matching unpublished experimental findings from an independent research group.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>In silico hypothesis generation and validation by comparison to independent experiments</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>microbiology / computational biology / evolutionary biology</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The co-scientist generated a mechanistic hypothesis about capsid-forming phage-inducible chromosomal islands (cf-PICIs) expanding host range; this hypothesis mirrored novel, independently generated experimental results that had not been publicly disclosed at the time. The match between in silico proposal and experimental discovery served as a retrospective validation; detailed experimental verification is described in co-timed reports.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>low-fidelity (LLM-based reasoning and literature synthesis rather than physics/chemistry-based simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Qualitative concordance reported: co-scientist's in silico mechanism paralleled the independent experimental discovery. The paper does not present quantitative metrics of agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Single high-profile example reported (recapitulation of the independent experimental finding); no aggregate success rate given.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Novel mechanistic proposals require experimental replication and peer-reviewed publication to be accepted; co-scientist's match with unpublished lab work provided a notable example of alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper views in silico discovery as powerful for hypothesis generation and occasionally predictive, but not a general substitute for experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>No specific failures of this type are detailed in the manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No formal uncertainty quantification provided for the qualitative recapitulation example.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not applicable for this item.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>In silico generation is faster and less resource-intensive than laboratory experiments; exact time savings not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Single-case demonstration in main text; reliance on co-timed reports for experimental detail; inability to generalize success rate from one example.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Recapitulation of independent experimental work increases credibility for the approach, but broader acceptance requires systematic experimental corroboration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared qualitatively to independent experimental discovery (the experimental result functions as gold-standard validation in this case).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2210.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2210.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulation review (Reflection agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflection agent's simulation-based review of hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A low-fidelity, LLM-driven 'simulation review' where the Reflection agent stepwise simulates proposed mechanisms or experimental protocols to identify potential failure modes and summarize failure scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM-based step-wise simulation review</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI reasoning / methodological validation within hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>low-fidelity simulation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The Reflection agent performs a qualitative, stepwise simulation of proposed mechanisms (e.g., mechanism of action or experimental sequence) using the model's internal knowledge to surface failure scenarios and edge cases; this functions as a critique tool to preempt issues in proposals before experimental testing.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Low — qualitative, text-based reasoning grounded in model knowledge; not a numerical physics/chemistry simulation and not calibrated to experimental kinetics or detailed biophysical models.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Not reported; simulation review is used as an internal critique, not as quantitative prediction to be compared with experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Presented as an auxiliary review method, intended to complement literature review and not to replace experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper does not claim simulation review alone suffices; it is explicitly a screening/critique step.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>None reported for this mechanism; authors caution that LLM-based simulation may miss subtle empirical details.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No uncertainty metrics provided for simulated failure predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Simulation review can help detect implausible reasoning or protocol flaws, but is not presented as a detection tool for fabricated experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Relatively low computational cost compared to empirical experiments; used iteratively within the tournament pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Dependent on LLM internal knowledge and biases; lacks mechanistic fidelity needed for quantitative prediction of experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Useful internally for hypothesis refinement; external acceptance depends on subsequent literature grounding and experimental verification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Not compared to experimental gold standards in the manuscript.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2210.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2210.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial safety testing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial research goal testing for safety and misuse detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated adversarial benchmark of research goals used to probe whether the co-scientist rejects unsafe or dual-use research prompts; reported to have been passed by the system in preliminary checks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Adversarial research-goal testing</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI safety / biosecurity</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The authors generated 1,200 adversarial research goals across 40 biomedical and scientific topics (using frontier LLMs) and evaluated whether the co-scientist robustly rejects or flags them; the system passed these preliminary checks. The dataset is not public but can be made available on request.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Authors report the system passed all checks on the curated adversarial set; no false-positive/false-negative breakdown provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Preliminary internal safety benchmark; authors acknowledge broader safety evaluation is required beyond this check.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Not applicable; adversarial testing checks safety behavior of the model rather than validating scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>None reported within this preliminary adversarial set.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Primary goal is to detect dangerous/misuseful prompts rather than fabricated data; methods for detecting fabricated experimental results are not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Creating and evaluating 1,200 adversarial examples implies non-trivial annotation and compute effort; specific resource costs not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Dataset withheld due to sensitivity; authors characterize the analysis as preliminary and not exhaustive.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Passing the adversarial suite is presented as evidence of basic safety robustness, but authors note it is not comprehensive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>No standard external safety benchmark comparison reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2210.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2210.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DepMap filter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cancer Dependency Map (DepMap) score filtering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational sanity-check filter that uses DepMap gene essentiality probabilities to prioritize drug-cancer repurposing pairs (for example requiring DepMap >= 0.99 alongside high co-scientist review scores).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>DepMap-based computational filtering</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational oncology / bioinformatics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>For drug-repurposing hypotheses, the team extracted DepMap probabilities of gene dependency for relevant cell lines and combined these with the co-scientist review scores; they selected candidate pairs where co-scientist review >= 4 and DepMap >= 0.99 for expert review and potential wet-lab testing. DepMap served as a sanity filter rather than a proof of efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not provided; DepMap used to filter candidates rather than measure validation success.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>DepMap is a standard resource for gene essentiality and dependency in cancer cell lines; authors note DepMap alone is unlikely to predict clinical efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>No — DepMap is used only as a computational screening tool prior to experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>DepMap provides probabilistic scores which the authors explicitly use as thresholds (e.g., >= 0.99).</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computationally cheap relative to wet-lab experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Dependent on underlying datasets and cell-line models; authors note it functions as a sanity check and is not predictive of therapeutic efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Used as an established external dataset to strengthen candidate plausibility prior to experimental testing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Not compared directly to experimental efficacy gold-standards; used as preliminary filter only.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2210.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2210.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Comparative evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tournament-based comparative evaluation vs. LLM baselines and human experts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparative validation strategy where co-scientist outputs were evaluated in Elo tournaments alongside other LLMs (Gemini variants, OpenAI o1/o3-mini-high, DeepSeek R1) and human expert 'best guesses', and also assessed by small-scale human expert preference rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Tournament-based comparative evaluation and human preference ranking</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning evaluation / AI-driven scientific workflows</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation (with human-evaluation hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>On 15 expert-curated challenging research goals, the co-scientist competed against multiple LLM baselines and expert-provided solutions in Elo tournaments; metrics reported include best Elo and average Elo of top-10 hypotheses per temporal bucket. Additionally, a panel of domain experts performed preference rankings (1–4) and rated outputs on novelty and impact (1–5). LLM evaluators (other models) were also used to auto-rank preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Co-scientist outperformed baselines and expert 'best guesses' on Elo metrics in these curated tasks and received higher expert preference and novelty/impact scores; exact numeric deltas are shown in paper figures but no ground-truth correctness measurement was available for most tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Evaluation uses curated expert tasks and human rubric judgments as proxies for quality in hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>No — comparison evaluates generated hypotheses and their judged quality, not experimental proof.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not reported; limitations of the Elo metric are discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Elo values and mean preference rankings reported; no statistical hypothesis tests or confidence intervals are provided in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not a primary goal; comparisons focus on perceived novelty/impact and Elo rank.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Comparative tournaments and multi-model judging are compute- and annotation-intensive; exact costs not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Small-scale expert evaluation, potential metric biases (Elo auto-eval) and lack of ground-truth labels for many novel research goals.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Authors argue comparative and expert-preference superiority increases confidence in co-scientist outputs, but they explicitly caution about metric and sample-size limitations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Highly accurate protein structure prediction with AlphaFold <em>(Rating: 2)</em></li>
                <li>Mastering the game of Go with deep neural networks and tree search <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2210",
    "paper_id": "paper-90bbc0fb2107a0fc7a15449f0e071d0dafbf97bb",
    "extraction_schema_id": "extraction-schema-58",
    "extracted_data": [
        {
            "name_short": "Elo auto-eval",
            "name_full": "Elo-based tournament auto-evaluation metric",
            "brief_description": "An automated internal ranking metric that uses pairwise comparisons and multi-turn simulated scientific debates to assign Elo ratings to generated hypotheses and proposals, guiding resource allocation and self-improvement in the multi-agent system.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Elo-based tournament auto-evaluation",
            "scientific_domain": "machine learning / AI-driven scientific discovery / biomedicine",
            "validation_type": "computational validation",
            "validation_description": "Hypotheses are ranked via an Elo tournament: newly generated hypotheses start at Elo 1200; top candidates are compared using multi-turn simulated debates while lower-ranked items use single-turn comparisons. Elo ratings are aggregated across tournaments and used as an auto-evaluation signal for self-improvement. Concordance with external benchmarks was assessed by grouping co-scientist responses into Elo buckets and comparing accuracy against the GPQA benchmark and Gemini 2.0 reference responses. Elo was tracked across 203 research goals and a curated subset of 15 expert problems to measure improvement with test-time compute scaling.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "n/a",
            "validation_success_rate": "Used as a predictor of quality; Elo-selected top-1 co-scientist responses achieved 78.4% top-1 accuracy on the GPQA diamond set (as reported in the paper).",
            "domain_validation_standards": "Paper uses concordance with GPQA benchmark and expert preference as indicators; authors state Elo is an auto-evaluation metric and not independent ground truth.",
            "when_simulation_sufficient": "No — Elo is an internal ranking metric used to prioritize hypotheses for further review/experimentation, not a substitute for experimental validation.",
            "simulation_failures": "No explicit simulation-vs-experiment failures reported for Elo itself; authors caution Elo may favor attributes not aligned with scientists' preferences.",
            "uncertainty_quantification": "Elo rating values and bucketed average accuracies were reported; no formal confidence intervals or statistical error bars for Elo reported.",
            "fabrication_detection": "Not designed for fabricated-result detection; paper does not claim Elo detects fabricated or AI-generated false claims.",
            "validation_cost_time": "Tournament matches (multi-turn debates for top hypotheses) are computationally intensive and scale with test-time compute; exact resource/time metrics not provided.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Elo is an auto-evaluation metric (not independent ground truth); may favor superficially attractive attributes; potential ordering and metric biases acknowledged by authors.",
            "acceptance_credibility": "Authors present concordance between Elo and both the GPQA benchmark and expert preferences to support credibility, but explicitly caution that Elo is not a definitive ground-truth measure.",
            "comparison_to_gold_standard": "Compared indirectly to GPQA benchmark answers and to expert preference rankings; not compared to experimental (wet-lab) gold standards.",
            "uuid": "e2210.0"
        },
        {
            "name_short": "GPQA",
            "name_full": "GPQA benchmark (diamond set)",
            "brief_description": "A challenging multiple-choice question answering benchmark developed by domain experts in biology, physics and chemistry; used here to assess the concordance between the co-scientist's Elo ratings and actual correctness on hard scientific Q&A problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_or_method_name": "GPQA benchmark evaluation",
            "scientific_domain": "biological/physical/chemical question answering (scientific QA)",
            "validation_type": "computational validation",
            "validation_description": "The co-scientist was prompted with GPQA diamond-set questions; responses were judged against the dataset ground truth. For each question the system generated outputs; Elo buckets were then correlated with the fraction correct. A Gemini 2.0 reference of 32 responses per question provided a baseline reference accuracy per question to mitigate selection bias.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "n/a",
            "validation_success_rate": "Reported top-1 accuracy for the co-scientist on the GPQA diamond set was 78.4% when selecting the top-rated (Elo) result per question.",
            "domain_validation_standards": "GPQA serves as an objective benchmark with ground-truth answers for QA-style validation in scientific domains.",
            "when_simulation_sufficient": "No — benchmark evaluation measures QA correctness, not experimental validation of hypotheses.",
            "simulation_failures": "Not reported.",
            "uncertainty_quantification": "Accuracy percentages per Elo bucket were reported; no confidence intervals or hypothesis-test statistics were provided.",
            "fabrication_detection": "Not applicable — GPQA assesses factual correctness vs. ground truth rather than fabrication detection.",
            "validation_cost_time": "Required generating many model responses (32 per GPQA question for the Gemini 2.0 reference) and tournament evaluation; exact compute/time costs not specified.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Benchmark-focused and limited to question-answering tasks; does not evaluate experimental or real-world wet-lab validation.",
            "acceptance_credibility": "Benchmark-based correctness is a standard computational validation approach and used here to support the Elo metric's validity.",
            "comparison_to_gold_standard": "Directly compared to GPQA ground truth answers (i.e., gold standard for the QA task).",
            "uuid": "e2210.1"
        },
        {
            "name_short": "Expert NIH review",
            "name_full": "Expert clinical evaluation using NIH Specific Aims Page format",
            "brief_description": "Human expert evaluation pipeline where co-scientist outputs are re-formatted into NIH Specific Aims pages and judged by board-certified clinicians using an adapted NIH grant-review rubric to assess clinical relevance, feasibility, novelty and impact.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Expert-in-the-loop clinical evaluation (NIH Specific Aims Style)",
            "scientific_domain": "clinical oncology / translational biomedicine",
            "validation_type": "other",
            "validation_description": "Co-scientist drug-repurposing proposals were converted into NIH-style Specific Aims pages and reviewed by six board-certified hematologist-oncologists using a 15-axis adapted NIH rubric (significance, innovation, rigor, feasibility, etc.) on five-point scales; 78 proposals were evaluated and pre-screened to remove implausible candidates.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "n/a",
            "validation_success_rate": "Not applicable as a pass/fail metric; experts rated proposals favorably across rubric axes (high proportion of 'Strongly Agree'/'Agree'), but no single numeric success rate for experimental validation is provided.",
            "domain_validation_standards": "NIH grant-review criteria and standard translational steps (preclinical in vitro/in vivo evidence, phase I–III trials) described as normative requirements for clinical acceptance.",
            "when_simulation_sufficient": "No — authors emphasize that expert review is complementary and that experimental verification (preclinical/in vivo/clinical trials) is required for translational claims.",
            "simulation_failures": "Not applicable.",
            "uncertainty_quantification": "Human ratings (categorical five-point scale) and average ranks reported; no standard errors or confidence intervals reported.",
            "fabrication_detection": "Experts performed manual pre-screening and excluded clinically implausible or unsafe candidates; this human review acts as a filter against implausible/generated claims.",
            "validation_cost_time": "Expert review is time-consuming and requires specialist effort; exact time/cost not quantified but acknowledged as resource-intensive relative to pure computation.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Single-center expert panel, potential pre-screening bias, and expert domain limits; results are subjective and not equivalent to experimental proof of efficacy.",
            "acceptance_credibility": "Positive expert evaluations increase credibility for follow-up work, but authors note that favorable expert opinion is not sufficient for regulatory approval or clinical acceptance.",
            "comparison_to_gold_standard": "Expert review is positioned as an intermediary step; gold-standard clinical validation remains randomized controlled clinical trials (phase III), which none of the candidates had achieved.",
            "uuid": "e2210.2"
        },
        {
            "name_short": "Wet-lab in vitro",
            "name_full": "In vitro wet-lab experimental validation (cell lines and organoids)",
            "brief_description": "Direct biological experiments used to evaluate co-scientist predictions: cell-line viability assays for AML drug-repurposing candidates and human hepatic organoid assays for anti-fibrotic activity of novel epigenetic targets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "In vitro experimental validation (cell viability and organoid functional assays)",
            "scientific_domain": "experimental biomedicine / pharmacology / cell biology",
            "validation_type": "experimental",
            "validation_description": "Selected candidates from co-scientist predictions were prioritized by oncologists and tested in standard in vitro assays: multiple AML cell lines were treated with candidate drugs and tumor viability was measured at clinically applicable concentrations; human hepatic organoids were used to test anti-fibrotic activity and regenerative responses. Experiments followed established protocols and were intended as initial biological validation steps; further details and full experimental protocols are provided in separate co-timed reports referenced by the paper.",
            "simulation_fidelity": null,
            "experimental_validation_performed": true,
            "comparison_simulation_vs_experiment": "The paper reports concordance for several predictions (e.g., AML candidates showed tumor inhibition in vitro; epigenetic targets showed anti-fibrotic activity in organoids). Quantitative comparisons (e.g., IC50 values, effect sizes) are not provided in this main text and are deferred to co-timed experimental reports.",
            "validation_success_rate": "Not quantified across the entire candidate set; paper states that certain candidates showed positive in vitro activity (examples provided) but does not report an overall fraction or percentage validated.",
            "domain_validation_standards": "Authors present in vitro validation as an initial preclinical step; domain standards require additional in vivo studies and randomized clinical trials for therapeutic claims.",
            "when_simulation_sufficient": "Paper emphasizes simulation / computational predictions are insufficient alone for translational claims; wet-lab validation is required for biological claims.",
            "simulation_failures": "No explicit cases of computational predictions failing in experiments are detailed in the main manuscript; limitations and negative results may be in co-timed reports.",
            "uncertainty_quantification": "The main text does not provide experimental uncertainty metrics (e.g., error bars, CI); these are likely in detailed experimental reports.",
            "fabrication_detection": "Not discussed in the experimental context.",
            "validation_cost_time": "Authors note experimental validation is time-consuming and resource-intensive compared to computational phases; no explicit cost figures provided.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Small-scale, preliminary experiments; selection bias from expert prioritization; not equivalent to in vivo or clinical validation; detailed quantitative outcomes not in main text.",
            "acceptance_credibility": "Wet-lab validation is presented as critical for acceptance; co-timed independent experimental reports increase credibility but authors caution that further preclinical/clinical testing is needed.",
            "comparison_to_gold_standard": "Wet-lab in vitro assays contrasted with the ultimate gold standard of randomized clinical trials; authors explicitly note none of the candidates had progressed to phase III clinical trials.",
            "uuid": "e2210.3"
        },
        {
            "name_short": "In silico recapitulation",
            "name_full": "In silico discovery that recapitulated unpublished experimental results",
            "brief_description": "A computational discovery by the co-scientist that independently proposed a mechanism (cf-PICIs interacting with diverse phage tails) matching unpublished experimental findings from an independent research group.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "In silico hypothesis generation and validation by comparison to independent experiments",
            "scientific_domain": "microbiology / computational biology / evolutionary biology",
            "validation_type": "hybrid",
            "validation_description": "The co-scientist generated a mechanistic hypothesis about capsid-forming phage-inducible chromosomal islands (cf-PICIs) expanding host range; this hypothesis mirrored novel, independently generated experimental results that had not been publicly disclosed at the time. The match between in silico proposal and experimental discovery served as a retrospective validation; detailed experimental verification is described in co-timed reports.",
            "simulation_fidelity": "low-fidelity (LLM-based reasoning and literature synthesis rather than physics/chemistry-based simulation)",
            "experimental_validation_performed": true,
            "comparison_simulation_vs_experiment": "Qualitative concordance reported: co-scientist's in silico mechanism paralleled the independent experimental discovery. The paper does not present quantitative metrics of agreement.",
            "validation_success_rate": "Single high-profile example reported (recapitulation of the independent experimental finding); no aggregate success rate given.",
            "domain_validation_standards": "Novel mechanistic proposals require experimental replication and peer-reviewed publication to be accepted; co-scientist's match with unpublished lab work provided a notable example of alignment.",
            "when_simulation_sufficient": "Paper views in silico discovery as powerful for hypothesis generation and occasionally predictive, but not a general substitute for experimental validation.",
            "simulation_failures": "No specific failures of this type are detailed in the manuscript.",
            "uncertainty_quantification": "No formal uncertainty quantification provided for the qualitative recapitulation example.",
            "fabrication_detection": "Not applicable for this item.",
            "validation_cost_time": "In silico generation is faster and less resource-intensive than laboratory experiments; exact time savings not quantified.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Single-case demonstration in main text; reliance on co-timed reports for experimental detail; inability to generalize success rate from one example.",
            "acceptance_credibility": "Recapitulation of independent experimental work increases credibility for the approach, but broader acceptance requires systematic experimental corroboration.",
            "comparison_to_gold_standard": "Compared qualitatively to independent experimental discovery (the experimental result functions as gold-standard validation in this case).",
            "uuid": "e2210.4"
        },
        {
            "name_short": "Simulation review (Reflection agent)",
            "name_full": "Reflection agent's simulation-based review of hypotheses",
            "brief_description": "A low-fidelity, LLM-driven 'simulation review' where the Reflection agent stepwise simulates proposed mechanisms or experimental protocols to identify potential failure modes and summarize failure scenarios.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "LLM-based step-wise simulation review",
            "scientific_domain": "AI reasoning / methodological validation within hypothesis generation",
            "validation_type": "low-fidelity simulation",
            "validation_description": "The Reflection agent performs a qualitative, stepwise simulation of proposed mechanisms (e.g., mechanism of action or experimental sequence) using the model's internal knowledge to surface failure scenarios and edge cases; this functions as a critique tool to preempt issues in proposals before experimental testing.",
            "simulation_fidelity": "Low — qualitative, text-based reasoning grounded in model knowledge; not a numerical physics/chemistry simulation and not calibrated to experimental kinetics or detailed biophysical models.",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Not reported; simulation review is used as an internal critique, not as quantitative prediction to be compared with experiments.",
            "validation_success_rate": "Not quantified in the paper.",
            "domain_validation_standards": "Presented as an auxiliary review method, intended to complement literature review and not to replace experimental validation.",
            "when_simulation_sufficient": "Paper does not claim simulation review alone suffices; it is explicitly a screening/critique step.",
            "simulation_failures": "None reported for this mechanism; authors caution that LLM-based simulation may miss subtle empirical details.",
            "uncertainty_quantification": "No uncertainty metrics provided for simulated failure predictions.",
            "fabrication_detection": "Simulation review can help detect implausible reasoning or protocol flaws, but is not presented as a detection tool for fabricated experimental data.",
            "validation_cost_time": "Relatively low computational cost compared to empirical experiments; used iteratively within the tournament pipeline.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Dependent on LLM internal knowledge and biases; lacks mechanistic fidelity needed for quantitative prediction of experimental outcomes.",
            "acceptance_credibility": "Useful internally for hypothesis refinement; external acceptance depends on subsequent literature grounding and experimental verification.",
            "comparison_to_gold_standard": "Not compared to experimental gold standards in the manuscript.",
            "uuid": "e2210.5"
        },
        {
            "name_short": "Adversarial safety testing",
            "name_full": "Adversarial research goal testing for safety and misuse detection",
            "brief_description": "A curated adversarial benchmark of research goals used to probe whether the co-scientist rejects unsafe or dual-use research prompts; reported to have been passed by the system in preliminary checks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Adversarial research-goal testing",
            "scientific_domain": "AI safety / biosecurity",
            "validation_type": "computational validation",
            "validation_description": "The authors generated 1,200 adversarial research goals across 40 biomedical and scientific topics (using frontier LLMs) and evaluated whether the co-scientist robustly rejects or flags them; the system passed these preliminary checks. The dataset is not public but can be made available on request.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "n/a",
            "validation_success_rate": "Authors report the system passed all checks on the curated adversarial set; no false-positive/false-negative breakdown provided.",
            "domain_validation_standards": "Preliminary internal safety benchmark; authors acknowledge broader safety evaluation is required beyond this check.",
            "when_simulation_sufficient": "Not applicable; adversarial testing checks safety behavior of the model rather than validating scientific hypotheses.",
            "simulation_failures": "None reported within this preliminary adversarial set.",
            "uncertainty_quantification": "Not reported.",
            "fabrication_detection": "Primary goal is to detect dangerous/misuseful prompts rather than fabricated data; methods for detecting fabricated experimental results are not described here.",
            "validation_cost_time": "Creating and evaluating 1,200 adversarial examples implies non-trivial annotation and compute effort; specific resource costs not provided.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Dataset withheld due to sensitivity; authors characterize the analysis as preliminary and not exhaustive.",
            "acceptance_credibility": "Passing the adversarial suite is presented as evidence of basic safety robustness, but authors note it is not comprehensive.",
            "comparison_to_gold_standard": "No standard external safety benchmark comparison reported.",
            "uuid": "e2210.6"
        },
        {
            "name_short": "DepMap filter",
            "name_full": "Cancer Dependency Map (DepMap) score filtering",
            "brief_description": "A computational sanity-check filter that uses DepMap gene essentiality probabilities to prioritize drug-cancer repurposing pairs (for example requiring DepMap &gt;= 0.99 alongside high co-scientist review scores).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_or_method_name": "DepMap-based computational filtering",
            "scientific_domain": "computational oncology / bioinformatics",
            "validation_type": "computational validation",
            "validation_description": "For drug-repurposing hypotheses, the team extracted DepMap probabilities of gene dependency for relevant cell lines and combined these with the co-scientist review scores; they selected candidate pairs where co-scientist review &gt;= 4 and DepMap &gt;= 0.99 for expert review and potential wet-lab testing. DepMap served as a sanity filter rather than a proof of efficacy.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "n/a",
            "validation_success_rate": "Not provided; DepMap used to filter candidates rather than measure validation success.",
            "domain_validation_standards": "DepMap is a standard resource for gene essentiality and dependency in cancer cell lines; authors note DepMap alone is unlikely to predict clinical efficacy.",
            "when_simulation_sufficient": "No — DepMap is used only as a computational screening tool prior to experimental validation.",
            "simulation_failures": "Not described in the paper.",
            "uncertainty_quantification": "DepMap provides probabilistic scores which the authors explicitly use as thresholds (e.g., &gt;= 0.99).",
            "fabrication_detection": "Not applicable.",
            "validation_cost_time": "Computationally cheap relative to wet-lab experimentation.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Dependent on underlying datasets and cell-line models; authors note it functions as a sanity check and is not predictive of therapeutic efficacy.",
            "acceptance_credibility": "Used as an established external dataset to strengthen candidate plausibility prior to experimental testing.",
            "comparison_to_gold_standard": "Not compared directly to experimental efficacy gold-standards; used as preliminary filter only.",
            "uuid": "e2210.7"
        },
        {
            "name_short": "Comparative evaluation",
            "name_full": "Tournament-based comparative evaluation vs. LLM baselines and human experts",
            "brief_description": "Comparative validation strategy where co-scientist outputs were evaluated in Elo tournaments alongside other LLMs (Gemini variants, OpenAI o1/o3-mini-high, DeepSeek R1) and human expert 'best guesses', and also assessed by small-scale human expert preference rankings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Tournament-based comparative evaluation and human preference ranking",
            "scientific_domain": "machine learning evaluation / AI-driven scientific workflows",
            "validation_type": "computational validation (with human-evaluation hybrid)",
            "validation_description": "On 15 expert-curated challenging research goals, the co-scientist competed against multiple LLM baselines and expert-provided solutions in Elo tournaments; metrics reported include best Elo and average Elo of top-10 hypotheses per temporal bucket. Additionally, a panel of domain experts performed preference rankings (1–4) and rated outputs on novelty and impact (1–5). LLM evaluators (other models) were also used to auto-rank preferences.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "n/a",
            "validation_success_rate": "Co-scientist outperformed baselines and expert 'best guesses' on Elo metrics in these curated tasks and received higher expert preference and novelty/impact scores; exact numeric deltas are shown in paper figures but no ground-truth correctness measurement was available for most tasks.",
            "domain_validation_standards": "Evaluation uses curated expert tasks and human rubric judgments as proxies for quality in hypothesis generation.",
            "when_simulation_sufficient": "No — comparison evaluates generated hypotheses and their judged quality, not experimental proof.",
            "simulation_failures": "Not reported; limitations of the Elo metric are discussed.",
            "uncertainty_quantification": "Elo values and mean preference rankings reported; no statistical hypothesis tests or confidence intervals are provided in the main text.",
            "fabrication_detection": "Not a primary goal; comparisons focus on perceived novelty/impact and Elo rank.",
            "validation_cost_time": "Comparative tournaments and multi-model judging are compute- and annotation-intensive; exact costs not provided.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Small-scale expert evaluation, potential metric biases (Elo auto-eval) and lack of ground-truth labels for many novel research goals.",
            "acceptance_credibility": "Authors argue comparative and expert-preference superiority increases confidence in co-scientist outputs, but they explicitly caution about metric and sample-size limitations.",
            "uuid": "e2210.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Highly accurate protein structure prediction with AlphaFold",
            "rating": 2
        },
        {
            "paper_title": "Mastering the game of Go with deep neural networks and tree search",
            "rating": 2
        }
    ],
    "cost": 0.025613249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Towards an AI co-scientist</h1>
<p>Juraj Gottweis ${ }^{<em>}$, , 1 Wei-Hung Weng</em>, ${ }^{1,2}$, Alexander Daryin ${ }^{<em>, 1}$, Tao Tu ${ }^{</em>, 3}$, Anil Palepu ${ }^{2}$, Petar Sirkovic ${ }^{1}$, Artiom Myaskovsky ${ }^{1}$, Felix Weissenberger ${ }^{1}$, Keran Rong ${ }^{3}$, Ryutaro Tanno ${ }^{3}$, Khaled Saab ${ }^{3}$, Dan Popovici ${ }^{2}$, Jacob Blum ${ }^{7}$, Fan Zhang ${ }^{2}$, Katherine Chou ${ }^{2}$, Avinatan Hassidim ${ }^{2}$, Burak Gokturk ${ }^{1}$, Amin Vahdat ${ }^{1}$, Pushmeet Kohli ${ }^{3}$, Yossi Matias ${ }^{2}$,<br>Andrew Carroll ${ }^{2}$, Kavita Kulkarni ${ }^{2}$, Nenad Tomasev ${ }^{3}$, Yuan Guan ${ }^{7}$, Vikram Dhillon ${ }^{4}$, Eeshit Dhaval Vaishnav ${ }^{5}$, Byron Lee ${ }^{5}$,<br>Tiago R D Costa ${ }^{6}$, José R Penadés ${ }^{6}$, Gary Peltz ${ }^{7}$,<br>Yunhan Xu ${ }^{3}$, Annalisa Pawlosky ${ }^{1, \ddagger}$, Alan Karthikesalingam ${ }^{2, \ddagger}$ and Vivek Natarajan ${ }^{2, \ddagger}$<br>${ }^{1}$ Google Cloud AI Research, ${ }^{2}$ Google Research, ${ }^{3}$ Google DeepMind, ${ }^{4}$ Houston Methodist, ${ }^{5}$ Sequome, ${ }^{6}$ Fleming Initiative and Imperial College London, ${ }^{7}$ Stanford University School of Medicine</p>
<h4>Abstract</h4>
<p>Scientific discovery relies on scientists generating novel hypotheses that undergo rigorous experimental validation. To augment this process, we introduce an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI co-scientist is intended to help uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and aligned to scientist-provided research objectives and guidance. The system's design incorporates a generate, debate, and evolve approach to hypothesis generation, inspired by the scientific method and accelerated by scaling test-time compute. Key contributions include: (1) a multi-agent architecture with an asynchronous task execution framework for flexible compute scaling; (2) a tournament evolution process for self-improving hypotheses generation. Automated evaluations show continued benefits of test-time compute, improving hypothesis quality. While general purpose, we focus development and validation in three biomedical areas: drug repurposing, novel target discovery, and explaining mechanisms of bacterial evolution and anti-microbial resistance. For drug repurposing, the system proposes candidates with promising validation findings, including candidates for acute myeloid leukemia that show tumor inhibition in vitro at clinically applicable concentrations. For novel target discovery, the AI co-scientist proposed new epigenetic targets for liver fibrosis, validated by anti-fibrotic activity and liver cell regeneration in human hepatic organoids. Finally, the AI co-scientist recapitulated unpublished experimental results via a parallel in silico discovery of a novel gene transfer mechanism in bacterial evolution. These results, detailed in separate, co-timed reports, demonstrate the potential to augment biomedical and scientific discovery and usher an era of AI empowered scientists.</p>
<h2>1 Introduction</h2>
<p>Human ingenuity and creativity propel the advancement of fundamental research in science and medicine. However, researchers, particularly in biomedicine, are faced with a breadth and depth conundrum. The complexity of biomedical topics require increasingly deep and specific subject matter expertise, while leaps in insight may still arise from broad knowledge bridging across disciplines. With the rapid rise in scientific publications and the availability of numerous technologies for specialized high-throughput assays, mastery of both discipline-specific depth and trans-disciplinary insight can be challenging.
Despite these challenges, many modern breakthroughs have emerged from trans-disciplinary endeavours. Emmanuelle Charpentier and Jennifer Doudna won the 2020 Nobel Prize in Chemistry for their work on CRISPR [1], which combined techniques and strategies ranging from microbiology to genetics to molecular biology. These benefits of synergy have also been seen beyond experimental biomedicine in numerous other areas of science. Notably, Geoffrey Hinton and John Hopfield combined ideas from physics and neuroscience [2, 3] to develop artificial intelligence (AI) systems, which were awarded the 2024 Nobel Prize in Physics.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | The AI co-scientist system design and experimental validation summary. (a) Here, we illustrate the different components of the the AI co-scientist multi-agent system, and its interaction paradigm with scientists. Given a research goal in natural language, the co-scientist generates novel research hypotheses and proposals. The system employs specialized agents - Generation, Reflection, Ranking, Evolution, Proximity (which evaluates relatedness), Meta-review (which provides high level analysis) - to continuously generate, debate, and evolve research hypotheses within a tournament framework. Feedback from the tournament enables iterative improvement, creating a self-improving loop towards novel and high-quality outputs. The co-scientist leverages tools, including web search and specialized AI models to improve the grounding and quality of generated research hypotheses. Scientists can converse with the co-scientist in natural language to specify research goals, incorporate constraints, provide feedback and suggest new directions for explorations via the designated user interface. (b) We perform end-to-end validation of the co-scientist generated hypotheses in three important topics of biomedicine with varied complexitysuggesting novel drug repurposing candidates for acute myeloid leukemia (AML) (upper panel), discovering novel epigenetic targets for liver fibrosis treatment (middle panel), and recapitulating the discovery of novel mechanism of gene transfer evolution in bacteria key to anti-microbial resistance (lower panel). The co-scientist's hypotheses for these three settings are externally, independently validated by in vitro laboratory experiments and detailed in separate preprints co-timed with this work. In the figure, blue denotes expert scientist inputs while red denotes the co-scientist agents or outputs.</p>
<p>There has been rapid technological progress in AI towards generally intelligent and collaborative systems, which might empower scientists in creatively traversing and expertly reasoning across disciplinary domains. Such systems are capable of advanced reasoning [4-6], multimodal understanding [6], and agentic behaviors [7] such as the ability to use tools to solve complex tasks over long time horizons. Further, the trends with distillation [8] and inference time compute costs [6, 9] indicate that such intelligent and general AI systems are rapidly becoming more affordable and available. Motivated by the aforementioned unmet needs in the modern discovery process in science and medicine and building on the advancements in frontier AI [10], we develop and introduce an AI co-scientist system.
The co-scientist is designed to act as a helpful assistant and collaborator to scientists and to help accelerate the scientific discovery process. The system is a compound, multi-agent AI system [11] building on Gemini 2.0 and designed to mirror the reasoning process underpinning the scientific method [12]. Given a research goal specified in natural language, the system can search and reason over relevant literature to summarize and synthesize prior work and build on it to propose novel, original research hypotheses and experimental protocols for downstream validations (Figure 1a). The co-scientist provides grounding for its recommendations by citing relevant literature and explaining the reasoning behind its proposals.
This work does not aim to completely automate the scientific process with AI. Instead, the co-scientist is purpose-built for a "scientist-in-the-loop" collaborative paradigm, to help domain experts augment their hypothesis generation process and guide the exploration that follows. Scientists can specify their research goals in simple natural language, including informing the system of desirable attributes for the hypotheses or research proposals it should create and the constraints that the synthesized outputs should satisfy. They can also collaborate and provide feedback in a variety of ways, including directly supplying their own ideas and hypotheses, refining those generated by the system, or using natural language chat to guide the system and ensure alignment with their expertise.
The co-scientist works through a significant scaling of the test-time compute paradigm [13-15] to iteratively reason, evolve, and improve the outputs as it gathers more knowledge and understanding. Underpinning the system are thinking and reasoning steps-notably a self-play based scientific debate step for generating novel research hypotheses; tournaments that compare and rank hypotheses via the process of finding win and loss patterns, and a hypothesis evolution process to improve their quality. Finally, the agentic nature of the system enables it to recursively self-critique its output and use tools such as web-search to provide itself with feedback to iteratively refine its hypotheses and research proposals.
While the co-scientist system is general-purpose and applicable across multiple scientific disciplines, in this study we focus our development and validation of the system to biomedicine. We validate the co-scientist's capability in three impactful areas of biomedicine with varied complexity: (1) drug repurposing, (2) novel treatment targets discovery, and (3) new mechanistic explanations for antimicrobial resistance (Figure 1b).
Drug development is an increasingly time-consuming and expensive process [16] in which new therapeutics require restarting many aspects of the discovery and development process for each indication or disease (roughly $70 \%$ of drug approvals are for new drugs). In contrast, drug repurposing-identifying novel therapeutic indications for drugs beyond their original intended use - has emerged as a compelling strategy to address these challenges [17]. Successful examples of repurposing include Humira (adalimumab) and Keytruda (pembrolizumab), both of which have become among the most successful drugs in history. [17]. The process typically involves analyzing molecular signatures, signaling pathways, drug interactions, clinical trial results, adverse event reports, and other literature-based information [18], along with off-label use data and, in some cases, patient experiences. However, drug repurposing is limited by several factors: (1) the need for extensive expertise across biomedical, molecular biology, and biochemical systems, (2) the inherent complexity of mammalian biological systems, and (3) the time-intensive nature of traditional computational biology analyses required. We leverage the co-scientist to generate predictions for large-scale drug repurposing, validating the generated predictions using a combination of computational biology, expert clinician feedback, and in vitro wet-lab validation approaches. Notably, our system has proposed novel repurposing candidates for acute myeloid leukemia (AML) that inhibit tumor viability at clinically relevant concentrations in vitro across multiple AML cell lines.
Unlike drug repurposing, which is a combinatorial search problem through a large but constrained set of</p>
<p>drugs and diseases, identifying novel treatment targets for diseases presents a more significant challenge, traditionally requiring extensive literature review, deep biological understanding, sophisticated hypothesis generation and complex experimental validation strategies. The uncertainty of identifying novel treatment targets is significantly greater than in drug repurposing, as it involves not only repurposing existing compounds but also uncovering entirely new components and mechanisms within biological systems. This target discovery process can be inefficient, potentially leading to suboptimal hypothesis selection and prioritization for in vitro and in vivo experimentation. Given the high costs and time associated with experimental validation, a more effective approach is needed. We probe the capabilities of the co-scientist to propose, rank, and provide experimental protocols for novel research hypotheses pertaining to target discovery. To demonstrate this capability, we focus on liver fibrosis, a prevalent and serious disease, showcasing the co-scientist's potential to discover novel treatment targets amenable to experimental validation. In particular, the co-scientist has suggested novel epigenetic targets demonstrating significant anti-fibrotic activity in human hepatic organoids.
As a third validation of the capabilities of our system, we focus on generation of hypotheses to explain mechanisms related to gene transfer evolution in bacteria pertaining to antimicrobial resistance (AMR) mechanisms developed by microbes to circumvent drug applications used to fight infections. This is arguably an even more complex challenge than drug repurposing and target discovery and involves understanding of not only the molecular mechanisms of gene transfer (conjugation, transduction, and transformation) but also the ecological and evolutionary pressures that drive the spread of AMR genes: a system-level problem with many interacting variables. This is also an important healthcare challenge with increasing rates of infections and deaths worldwide [19]. In this validation, researchers instructed the AI co-scientist to explore a topic that had already been subject to novel discovery by their independent research group. Notably, at the time of instructing the AI co-scientist system, the researchers' novel experimental insights had not yet been published or revealed in the public domain. The system was instructed to hypothesize how capsid-forming phageinducible chromosomal islands (cf-PICIs) exist across multiple bacterial species. The system independently proposed that cf-PICIs interact with diverse phage tails to expand their host range. This in silico discovery mirrored the novel and experimentally validated results that expert researchers had already performed, as detailed in the co-timed report [20, 21].
Overall, our key contributions are summarized as follows:</p>
<ul>
<li>Introducing an AI co-scientist. We develop and introduce an AI co-scientist that goes beyond literature summarization and "deep research" tools to assist scientists in uncovering new knowledge, novel hypothesis generation and experimental planning.</li>
<li>Significant scaling of test-time compute paradigm for scientific reasoning. The co-scientist is built on a Gemini 2.0 multi-agent architecture, utilizing an asynchronous task execution framework. This framework allows the system to flexibly allocate computational resources to scientific reasoning, mirroring key aspects of the scientific method. Specifically, the system uses self-play strategies, including a scientific debate and a tournament-based evolution process, to iteratively refine hypotheses and research proposals creating a self-improving loop. Using automated evaluations across 15 complex expert curated open scientific goals, we demonstrate the benefits of scaling the test-time compute paradigm with the AI co-scientist outperforming other state-of-the-art (SOTA) agentic and reasoning models in generating high quality hypotheses for complex problems.</li>
<li>Expert-in-the-loop scientific workflow. Our system is designed for collaboration with scientists. The system can flexibly incorporate conversational feedback in natural language from scientists and co-develop, evolve and refine outputs.</li>
<li>End-to-end validation of the co-scientist in important topics in biomedicine. We present end-to-end validation of novel AI-generated hypotheses through new empirical findings in three distinct and increasingly complex areas of biomedicine: drug repurposing, novel target discovery, and antimicrobial resistance. The AI co-scientist predicts novel repurposing drugs for AML, identifies novel epigenetic treatment targets grounded in preclinical evidence for liver fibrosis, and proposes novel mechanisms for gene transfer in bacterial evolution and antimicrobial resistance. These discoveries from the AI co-scientist have been validated in wet-lab settings and are detailed in separate, co-timed technical reports.</li>
</ul>
<h1>2 Related Works</h1>
<h3>2.1 Reasoning models and test-time compute scaling</h3>
<p>The modern revolution in foundation AI models [22] and large language models (LLMs) has been largely driven by advances in pre-training techniques [23, 24], leading to breakthroughs in models like the GPT and Gemini family [25, 26]. These models, trained on increasingly massive internet-scale and multimodal datasets, have demonstrated impressive abilities in language understanding and generation leading to breakthrough performance in a variety of benchmarks [27, 28]. However, a key area of ongoing development is enhancing their reasoning capabilities. This has led to the emergence of "reasoning models" which go beyond simply predicting the next word and instead attempt to mimic human thought processes [29]. One promising direction in this pursuit is the test-time compute paradigm. This approach moves beyond solely relying on the knowledge acquired during pre-training and allocates additional computational resources during inference to enable System-2 style thinking-slower deliberate reasoning to reduce uncertainty and progress optimally towards the goal [30]. This concept emerged with early successes such as AlphaGo [15], which used Monte Carlo Tree Search (MCTS) to explore game states and strategically select moves, and Libratus [14], which employed similar techniques to achieve superhuman performance in poker. This paradigm has now found applications in LLMs, where increased compute at test-time allows for more thorough exploration of possible responses, leading to improved reasoning and accuracy [11, 13, 29, 31-35]. Recent advancements, like the Deepseek-R1 model [4], further demonstrate the potential of test-time compute by leveraging reinforcement learning to refine the model's "chain-of-thought" and enhance complex reasoning abilities over longer horizons. In this work, we propose a significant scaling of the test-time compute paradigm using inductive biases derived from the scientific method to design a multi-agent framework for scientific reasoning and hypothesis generation without any additional learning techniques.</p>
<h3>2.2 AI-driven scientific discovery</h3>
<p>AI-driven scientific discovery represents a paradigm shift in how research is conducted across various scientific domains. Recent advancements, particularly the development of large deep learning and generative models, have cemented AI's role in scientific discovery. This is best exemplified by AlphaFold 2's remarkable progress in the grand challenge of protein structure prediction, which has revolutionized structural biology and opened new avenues for drug discovery and materials science [36]. Other notable examples include the development of novel antibiotics, protein binder design, and material discovery with AI [37-39].
Building on these successes with specialized, bespoke AI models, there has been recent work exploring the even more ambitious goal of fully integrating AI, especially modern LLM-based systems, into the complete research workflow, from initial hypothesis generation all the way to manuscript writing. This end-to-end integration represents a significant shift, presenting both unprecedented opportunities and significant challenges as the field moves beyond specialized AI tools toward realizing the potential of AI as an active collaborator, or even, as some envision, a nascent "AI scientist" [40, 41].
As an example of this shift, Liang et al. [42] directly assessed the utility of LLMs for providing feedback on research manuscripts. Through both a retrospective analysis of existing peer reviews and a prospective user study, they demonstrated the significant concordance between LLM-generated feedback and that of human reviewers. Their study, using GPT-4 [43], found that a majority of researchers perceived LLM-generated feedback as helpful, and in some instances, even more beneficial than feedback from human colleagues. However, while valuable, their work focuses solely on the feedback stage of the scientific process, leaving open the question of how LLMs might be integrated into the full research cycle, from hypothesis formation to experimental validation and manuscript writing.
Another effort embodying this shift is PaperQA2 [44], an AI agent for scientific literature search and summarization. The authors claimed to surpass PhD and postdoc researchers on multiple literature research tasks, as measured both by performance on objective benchmarks and human evaluations. While the system is a useful for synthesizing information, it does not engage in scientific reasoning for novel hypothesis generation.
HypoGeniC, a system proposed by Zhou et al. [45], tackles hypothesis generation by iteratively refining hypotheses using LLMs and a multi-armed bandit-inspired approach. The process begins with a small set of</p>
<p>examples, from which initial hypotheses are generated. These hypotheses are then iteratively updated through exploration and exploitation, guided by a reward function based on training accuracy. This refined set of hypotheses is subsequently used to construct an interpretable classifier. However, the method's reliance on retrospective data for evaluation means the degree to which the system can generate truly novel hypotheses remains an open question. Furthermore, the system lacks end-to-end validation beyond subjective human evaluations.
Ifargan et al. [46] present "data-to-paper", a platform that systematically guides multiple LLM and rulebased agents to generate research papers, with automated feedback mechanisms and information tracing for verification. However, the evaluations are limited to recapitulating existing peer-reviewed publications and its unclear if the system can generate truly novel, yet grounded hypothesis and research proposals.
Virtual Lab [47] is another closely related work. Here, the authors propose a team of LLM agents with a "principal investigator" LLM guiding a team of specialized LLM agents to solve a scientific problem. The LLM team receives high level human supervision. The authors demonstrate the utility of their work by leveraging Virtual Lab to design nanobody binders to recent variants of SARS-CoV-2 with experimental validation. While similar in spirit, there are significant design differences to our approach and the generality of the system remains unclear.
Boiko et al. [48] introduced "Coscientist", a multi-agent system powered by GPT-4, designed for autonomous execution of complex chemical experiments. This system integrates capabilities such as web and document searching, and code execution, to facilitate independent experimental design, planning, and execution. In addition to similar sounding names, both "Coscientist" and our system share the overarching goal of accelerating scientific discovery through AI. However, there are several important distinctions. Notably, "Coscientist" is quite narrowly focused on chemical research while ours is much broadly applicable across science. Secondly, our system has important technical innovations that lead to a self-improving system that can uncover new, original knowledge while their approach is a more vanilla-stitching of GPT-4 based agents. Finally, despite the name, "Coscientist" prioritizes a high degree of autonomy in experimental execution, directly interfacing with laboratory hardware. Our system, instead, is explicitly designed as a collaborative tool, emphasizing a "scientist-in-the-loop" approach and centers on the more cognitive aspects of the research process.
Finally, Lu et al. [40] propose "The AI Scientist", a fully automated system designed to conduct research using multiple collaborating LLM agents. These agents handle all stages of the research process, from defining research problems and conducting literature reviews to designing and executing experiments, and even writing up the results. The design shares similarities with our work - the key differences being our focus on the scaling of the test-time compute paradigm to generate high quality hypotheses and research proposals. Secondly, their proposed system has limited automated evaluations; in contrast, our work has a combination of automated, human expert and end-to-end wet lab validations. Finally, our goal is to not to automate scientific discovery, rather to build a helpful AI collaborator for scientists.</p>
<h1>2.3 AI for biomedicine</h1>
<p>More broadly, large AI models are increasingly demonstrating their potential in biomedical science. Both general purpose (GPT-4, Gemini) and specialized LLMs (Med-PaLM, Med-Gemini, Galactica, Tx-LLM) have shown strong performance on biomedical reasoning and question-answering benchmarks [25, 26, 49-52]. Beyond benchmarks, Med-PaLM 2, was successfully applied to identify causative murine genetic factors for traits such as diabetes, cataracts, and hearing loss [53]-an early example of hypothesis generation and LLM-assisted discovery. We have also seen the exciting development of specialized foundation and large language models trained on DNA, RNA and protein sequences with a variety of applications [54-57]. Although AI in biology and medicine often necessitates specialization, the rapid progress of frontier AI models has blurred the distinction. As these models grow in scale, data diversity, and complexity, they continue to achieve breakthroughs in areas once thought to require domain-specific AI. Our co-scientist system, with its modular multi-agent architecture, is flexibly designed to build on top of these advancements in general-purpose frontier AI models and leverage specialized AI models as tools to enhance the capabilities.
Drug repurposing is an important area of validation experiments in this work. The traditional approach to this task requires both computational and experimental approaches and a comprehensive understanding of</p>
<p>disease-drug interactions [17, 58]. While methods like knowledge graphs with graph convolutional networks have shown promise [59, 60], their applicability is limited by the initial knowledge graph's scope. TxGNN [61], an example of a specialized biomedical foundation model with a graph based approach, addresses "zero-shot" repurposing for novel diseases but remains dependent on the underlying knowledge graph's quality and lacks sufficient scalability and explainability. Furthermore, no end-to-end validations of the model predictions were reported in the study. In contrast, our work, leveraging state-of-the-art LLMs in the co-scientist setup, is more scalable. We report a combination of expert evaluations and wet-lab experiments to validate the system predictions.</p>
<h1>3 Introducing the AI co-scientist</h1>
<p>This section describes the technical details, agents, and framework comprising the co-scientist system. The co-scientist employs a multi-agent architecture built upon Gemini 2.0, integrated within an asynchronous task execution framework. This framework allows for flexible scaling of test-time compute resources, facilitating advanced scientific reasoning.
Given a research goal specified by an expert scientist in natural language, the co-scientist generates hypotheses and research proposals that adhere to the following default criteria:</p>
<ul>
<li>Alignment with the provided research goal. The generated outputs must precisely align with the research goals, preferences and constraints defined by the scientist.</li>
<li>Plausibility. The system outputs should be free of readily apparent flaws. Any potential contradictions with prior literature or established knowledge must be explicitly stated and justified.</li>
<li>Novelty. A key objective of the co-scientist system is to generate novel hypotheses, conjectures, and research plans grounded in prior literature, rather than simply synthesizing existing information (a capability already addressed by existing "deep research" tools [62]).</li>
<li>Testability. The system outputs should be amenable to empirical validation within the constraints specified by the scientist.</li>
<li>Safety. The system outputs will be controlled to prevent enabling unsafe, unethical, or harmful research.</li>
</ul>
<p>Aside from these default criteria, the co-scientist can be configured with additional criteria, preferences, and constraints as needed. For instance, it can be configured to generate outputs in formats preferred by the researcher to improve interpretability and readability.
Throughout this section, we employ a recurring example: generating hypotheses for exploring the biological mechanisms of Amyotrophic Lateral Sclerosis (ALS) to illustrate the various components of the co-scientist system. While this example has been reviewed by domain experts, it remains illustrative and may contain errors. Importantly, this example does not aim to suggest potential therapeutic avenues for ALS and should be interpreted with utmost caution. All the examples are listed in the Appendix Section A.3.</p>
<h3>3.1 The AI co-scientist system overview</h3>
<p>At a high level, the co-scientist system comprises four key components:</p>
<ul>
<li>Natural language interface. Scientists interact with and supervise the system primarily through natural language. This allows them to not only define the initial research goal but also refine it at any time, provide feedback on generated hypotheses (including their own solutions), and generally guide the system's progress.</li>
<li>Asynchronous task framework. The co-scientist employs a multi-agent system where specialized agents operate as worker processes within an asynchronous, continuous, and configurable task execution framework. A dedicated Supervisor agent manages the worker task queue, assigns specialized agents to these processes, and allocates resources. This design enables the system to flexibly and effectively utilize computational resources and iteratively improve its scientific reasoning capabilities.</li>
<li>Specialized agents. Following inductive biases and scientific priors derived from the scientific method, the process of scientific reasoning and hypothesis generation is broken down into sub-tasks. Individual,</li>
</ul>
<p>specialized agents, each equipped with customized instruction prompts, are designed to execute these sub-tasks. These agents operate as workers coordinated by the Supervisor agent.</p>
<ul>
<li>Context memory. In order to enable iterative computation and scientific reasoning over long time horizons, the co-scientist uses a persistent context memory to store and retrieve states of the agents and the system during the course of the computation.</li>
</ul>
<p>The Gemini 2.0 model is the foundational LLM underpinning all agents in the co-scientist system. The specific co-scientist design was arrived at with iterative developments and is reflective of the current capabilities of the underlying LLMs.</p>
<h1>3.2 From research goal to research plan configuration</h1>
<p>The research goal, specified by the scientist, serves as the entry point to the co-scientist system. Leveraging the multimodal and long context capabilities of Gemini 2.0 models, the co-scientist efficiently processes research goals of varying complexity, from simple statements to extensive documents spanning tens of thousands of natural language tokens or other relevant data (e.g., including hundreds of prior publication PDFs). The research goal may also incorporate specific constraints, attributes, and preferences related to the scientist's particular laboratory setting or field of work.
The co-scientist system then parses the goal to derive a research plan configuration for generating research proposals. This configuration captures the desired proposal preferences, attributes, and constraints. For example, it specifies whether the co-scientist should exclusively propose novel hypotheses. It also specifies the criteria for evaluating hypothesis quality, such as novelty and experimental feasibility. These criteria are then used by the system during its auto-evaluation and improvement phases. The attributes, preferences, and evaluation criteria can all be customized to a given research goal. To illustrate this process, we present an example research goal and its corresponding parsed research plan configuration in Appendix Figure A.9, where the goal is to develop a novel hypothesis related to phosphorylation of the Nuclear Pore Complex (NPC) as a causative mechanism for ALS [63].
Based on the research plan configuration, the Supervisor agent initiates the creation of a task queue and begins orchestrating the specialized agents. The system operates continuously and asynchronously. Periodically, the Supervisor agent calculates a comprehensive set of summary statistics, reflecting the system's state and progress toward the specified research goal. These statistics inform decisions regarding resource allocation and the determination of whether a terminal state for the overall computation has been reached. The state is periodically written to the associated context memory of the system and leveraged as feedback in subsequent rounds of computation. It also enables easy restarts in-case of any failure in the system components.</p>
<h3>3.3 The specialized agents underpinning the AI co-scientist</h3>
<p>At the core of the co-scientist system are a coalition of specialized agents, each orchestrated by the Supervisor agent. These agents are designed to emulate the scientific reasoning process, enabling them to generate novel hypotheses and research plans. They are also equipped to interact with external tools, such as web search engines and specialized AI models, through application programming interfaces (APIs). These specialized agents are enumerated below:</p>
<ul>
<li>Generation agent. The agent initiates the research process by generating the initial focus areas, iteratively extending them and generating a set of initial hypotheses and proposals that address the research goal. This involves exploring relevant literature using web search, synthesizing existing findings into novel directions, and engaging in simulated scientific debates for iterative improvement.</li>
<li>Reflection agent. This agent simulates the role of a scientific peer reviewer, critically examining the correctness, quality, and novelty of the generated hypotheses and research proposals. Furthermore, it evaluates the potential of each hypothesis to provide an improved explanation for existing research observations (identified via literature search and review), particularly those that may be under explained.</li>
<li>Ranking agent. An important abstraction in the co-scientist system is the notion of a tournament where different research proposals are evaluated and ranked enabling iterative improvements. The Ranking agent employs and orchestrates an Elo-based tournament [64] to assess and prioritize the</li>
</ul>
<p>generated hypotheses at any given time. This involves pairwise comparisons, facilitated by simulated scientific debates, which allow for a nuanced evaluation of the relative merits of each proposal.</p>
<ul>
<li>Proximity agent. This agent asynchronously computes a proximity graph for generated hypotheses, enabling clustering of similar ideas, de-duplication, and efficient exploration of the hypothesis landscape.</li>
<li>Evolution agent. The co-scientist's iterative improvement capability relies heavily on this agent, which continuously refines the top-ranked hypotheses emerging from the tournament. Its refinement strategies include synthesizing existing ideas, using analogies, leveraging literature for supporting details, exploring unconventional reasoning, and simplifying concepts for clarity.</li>
<li>Meta-review agent. This agent also enables the co-scientist's continuous improvement by synthesizing insights from all reviews, identifying recurring patterns in tournament debates, and using these findings to optimize other agents' performance in subsequent iterations. This also enhances the quality and relevance of generated hypotheses and reviews in subsequent iterations. The agent also synthesizes top-ranked hypotheses and reviews into a comprehensive research overview for review by the scientist.
<img alt="img-1.jpeg" src="img-1.jpeg" /></li>
</ul>
<p>Figure 2 | The AI co-scientist multi-agent architecture design. The co-scientist accepts a natural language research goal from the user and parses this into a research plan configuration. This plan is then dispatched to the Supervisor agent which evaluates this plan to assigns weights and resources to each specialized agent and subsequently queues them as worker processes in a task queue according to these weights. The worker processes execute the queue of agent actions, and the system ultimately aggregates all information to formulate a research overview with detailed hypotheses and proposals for the scientist. The red boxes in the "The AI co-scientist specialized agents" section denote individual agents each with their own unique logic and role. The blue boxes indicate the scientist-in-the-loop inputs and feedback. The dark gray arrows represent the information flow through the co-scientist system, while the red arrows represent the information feedback loop between the specialized agents.</p>
<p>The Supervisor agent's seamless orchestration of these specialized agents enables the development of valid, novel, and testable hypotheses and research plans tailored to the input research goal.
In summary, the Generation agent curates an initial list of research hypotheses satisfying a research goal. These are then reviewed by the Reflection agent and evaluated in a tournament by the Ranking agent. The Evolution, Proximity, and Meta-review agents operate on the tournament state to help improve the quality of the system outputs.
The Supervisor agent periodically computes and writes to the context memory, a comprehensive suite of statistics, including the number of hypotheses generated and requiring review, and the progress of the tournament. These statistics also include analyses of the effectiveness of different hypothesis generation methodologies (e.g., generating new ideas via the Generation agent vs. improving existing ideas via the Evolution agent). Based on these statistics, the Supervisor agent then orchestrates subsequent system operations, i.e., generating new hypotheses, reviews, tournaments, and improvements to existing hypotheses, by strategically weighting and sampling the specialized agents for execution via the worker processes.
Importantly, the Meta-review agent enables feedback propagation and learning without back-propagation techniques (e.g., fine-tuning or reinforcement learning) [65]. The Meta-review agent generates feedback</p>
<p>applicable to all agents, which is simply appended to their prompts in the next iteration-a capability facilitated by the long-context search and reasoning capabilities of the underlying Gemini 2.0 models. Through this feedback loop, the co-scientist continuously learns and improves in subsequent iterations with more compute scaling.
Finally, while our work leverages Gemini 2.0, the co-scientist framework is model-agnostic and portable to other similar models or combinations thereof. Future LLM improvements will likely enhance the co-scientist's capabilities. The multi-agent architecture of the co-scientist is depicted and summarized in Figure 2.
We now describe the mechanisms of action of the specialized agents in more detail.</p>
<h1>3.3.1 Generation agent</h1>
<p>The co-scientist Generation agent employs a diverse array of techniques and tools to generate novel hypotheses, such as the following:</p>
<ul>
<li>Literature exploration via web search. The agent iteratively searches the web, retrieves and reads relevant research articles, and grounds its reasoning by summarizing prior work. It then builds on this summary to generate novel hypotheses and research plans. An example prompt is given in Appendix Figure A.1.</li>
<li>Simulated scientific debates. Here, the Generation agent simulates scientific debates among experts by employing self-critique and self-play techniques. These debates typically involve multiple turns of conversations leading to a refined hypothesis generated at the end. An example prompt is given in Appendix Figure A.2.</li>
<li>Iterative assumptions identification. The agent iteratively identifies testable intermediate assumptions, which, if proven true, can lead to novel scientific discovery. These plausible assumptions and their sub-assumptions are identified through conditional reasoning hops and subsequently aggregated into complete hypotheses.</li>
<li>Research expansion. To identify previously unexplored areas of the hypothesis space, the Generation agent reviews existing hypotheses and the research overview and feedback provided by the Meta-review agent in the previous iteration. This is used to inform additional exploration directions in the research hypothesis space.</li>
</ul>
<p>An example hypothesis and research proposal output from the Generation agent is presented in Appendix Figure A. 10 for the aforementioned research goal regarding explaining a basic mechanism related to ALS. The Generation agent also summarizes and categorizes each generated hypothesis, allowing scientists to quickly grasp the core ideas.</p>
<h3>3.3.2 Reflection agent</h3>
<p>Reviews are integral to the co-scientist's effectiveness in generating novel proposals. The Reflection agent searches relevant prior work (via web search or a dedicated scientist-provided repository), assesses existing experimental evidence for or against a given hypothesis, and rigorously verifies the novelty, correctness, and quality of generated outputs. Effective reviews filter inaccurate and, when stipulated, non-novel hypotheses. Moreover, they also provide feedback to all other agents, driving continuous improvement. The Reflection agent employs the following types of review:</p>
<ul>
<li>Initial review. Building on the co-scientist's default evaluation criteria, the Reflection agent performs an initial review assessing the correctness, quality, novelty, and a preliminary assessment of safety (ethics) of the generated hypotheses. For a more in-depth discussion on safety considerations see Section 6. This initial review, which doesn't use external tools like web search, aims to quickly discard flawed, non-novel, or otherwise unsuitable hypotheses.</li>
<li>Full review. If a hypothesis passes the initial review, the Reflection agent performs a full review, leveraging external tools and web searches to identify relevant articles for improved reasoning and grounding. This review evaluates the hypothesis's correctness, quality, and novelty similar to the initial review but with full literature search. For correctness and quality, the agent scrutinizes underlying</li>
</ul>
<p>assumptions and reasoning. For novelty, it summarizes known aspects of the hypothesis and then judges their novelty based on existing literature. An example full novelty review is shown in Appendix Figure A.11, and an example of review critiques is in Appendix Figure A.12. A complete full review example is shown in Appendix Figure A.13.</p>
<ul>
<li>Deep verification review. The Reflection agent also conducts a deep verification review, decomposing the hypothesis into constituent assumptions. Each assumption is further broken down into fundamental sub-assumptions, decontextualized, and independently evaluated for correctness to identify invalidating elements for subsequent filtering. Concurrently, the reasons for potential hypothesis invalidation due to incorrect assumptions are summarized. This deep verification helps the co-scientist detect subtle errors within complex hypotheses, such as flaws in reasoning or inaccurate experimental protocols. An identified error doesn't necessarily invalidate the core hypothesis; the Reflection agent assesses whether the incorrect assumption is fundamental to the hypothesis and incorporates this reasoning into the review. Non-fundamental errors can be addressed during subsequent refinement stages. An example deep verification review is provided in Appendix Figure A. 14 for the previously introduced ALS hypothesis. We also show another example of a deep verification review via probing questions in the context of drug repurposing for AML in Appendix Figure A. 15 .</li>
<li>Observation review. In addition, the Reflection agent also explores whether a given hypothesis can account for long-tail observations from prior experimental results. This review aims to determine if the hypothesis can provide insights on existing experimental findings and observed phenomena within relevant articles. For each observation, the agent assesses if the hypothesis is a superior explanation over existing ones, assuming its validity. Positive observations are summarized and appended to the hypothesis. Note that this review often completes without any important findings (as in the case of the ALS hypothesis example). An example prompt to generate observations is provided in Appendix Figure A.3. An illustrative example of an observation review is provided in Appendix Figure A. 16 in the context of an alternate hypothesis for explaining a mechanism of anti-microbial resistance.</li>
<li>Simulation review. The Reflection agent also reviews hypotheses by simulating them in a step-wise fashion (e.g., simulating the mechanism of action or the proposed experiment in the proposal). This simulation allows the agent to identify and summarize potential failure scenarios. This review method is built on the assumption that frontier LLMs may have developed an internal world model that enables them to simulate and accurately predict various scientific phenomena.</li>
<li>Recurrent/tournament review. The Reflection agent adapts its full reviews based on the co-scientist's growing knowledge. By analyzing reviewed hypotheses and results of the tournament conducted by the Ranking agent, the Reflection agent identifies recurring issues and improvement opportunities, refining its reviews accordingly.</li>
</ul>
<p>Additionally, the co-scientist can incorporate reviews from expert scientists to guide ranking and improvements (further discussed in Section 3.4). We aim to have the Reflection agent's comprehensive set of reviews cover the common methods scientists employ when critiquing and refining research hypotheses and proposals.</p>
<h1>3.3.3 Ranking agent</h1>
<p>The AI co-scientist explores numerous hypotheses and research proposals towards a research goal, necessitating a ranking mechanism to prioritize computational resources toward the most promising candidates. This task is performed by the Ranking agent. The agent uses an Elo-based tournament [64] to automatically evaluate and rank all hypotheses, providing supporting rationale. This ranking serves to communicate to scientists an ordered list of research hypotheses and proposals aligned with the research goal. Despite its assumptions and limitations [66], Elo remains a good proxy for relative ranking, and it has previously been applied to rank extracted patterns and ideas in games [67]. In the future, extensions may be considered [68]. We set the initial Elo rating of 1200 for the newly added hypothesis.
Because the tournament is computationally intensive, the Ranking agent employs several optimization strategies. Top-ranked hypotheses are compared pairwise in tournament matches through multi-turn scientific debates [69]. This mitigates ordering bias and focuses on novelty, correctness, and testability. Lower-ranked hypotheses undergo single-turn comparisons in a pairwise fashion in their tournament match. The agent concludes each comparison with a decision regarding which hypothesis is better. Appendix Figure A. 4 and</p>
<p>Appendix Figure A. 5 show example prompts. Appendix Figure A. 17 shows an example of the Ranking agent conducting a scientific debate match in a tournament to compare two hypotheses.
The Ranking agent prioritizes tournament matches as follows: (1) hypotheses are more likely to be compared with similar ones (based on the Proximity agent's graph, described in the next section); (2) newer and top-ranking hypotheses are prioritized for participation in tournament matches. Successful hypotheses quickly achieve favorable rankings and this informs the tournament state for subsequent iterations.</p>
<h1>3.3.4 Proximity agent</h1>
<p>The Proximity agent calculates the similarity between research hypotheses and proposals, and builds a proximity graph, taking into account the specific research goal. Although it doesn't directly participate in hypothesis generation, the Proximity agent assists the Ranking agent in organizing tournament matches and showcasing a diverse range of ideas related to the research goal. This allows scientists to quickly explore areas of interest and easily identify related concepts.</p>
<h3>3.3.5 Evolution agent</h3>
<p>The Evolution agent continuously refines and improves existing hypotheses and proposals using several approaches including:</p>
<ul>
<li>Enhancement through grounding. Here the agent attempts to improve hypotheses by identifying weaknesses, generating search queries, retrieving and reading articles, suggesting improvements and elaborating on details to fill reasoning gaps.</li>
<li>Coherence, practicality and feasibility improvements. The agent aims to address issues and creates more coherent hypotheses, potentially rectifying underlying problems with invalid initial assumptions. The agent also refines the hypotheses to make them more practical and feasible. Appendix Figure A. 6 provides an example of the feasibility improvement prompt.</li>
<li>Inspiration from existing hypotheses. The agent additionally creates new hypotheses inspired by single or multiple top-ranked hypotheses.</li>
<li>Combination. The agent also attempts to directly combine the best aspects of several top-ranking hypotheses to create new hypotheses.</li>
<li>Simplification. The agent simplifies hypotheses for easier verification and testing.</li>
<li>Out-of-box thinking. The agent also explores out-of-the-box ideas by moving away from a subset of hypotheses and generating divergent ones. Appendix Figure A. 7 provides an example prompt for this.</li>
</ul>
<p>The Evolution agent generates new hypotheses; it doesn't modify or replace existing ones. This strategy protects the quality of top-ranked hypotheses from flawed improvements, as each new hypothesis must also compete in the tournament. The evolution of research hypotheses and proposals also allows the co-scientist to iteratively combine different improvement techniques and gradually improve the quality of the results.</p>
<h3>3.3.6 Meta-review agent</h3>
<p>The Meta-review agent plays a crucial role in the co-scientist's feedback loop, enabling self-improvement in scientific reasoning. This agent operates on the tournament state and summarizes common patterns identified in reviews and scientific debates in the tournament matches into a meta-review critique.
By synthesizing insights from all reviews, the meta-review provides valuable feedback to the Reflection agent, leading to more thorough and reliable future reviews. This helps prevent oversight of critical details. Consider the illustrative example of a identifying a repurposing drug candidate for ALS as a research goal: while only $90 \%$ of individual reviews might correctly identify a blood-brain barrier permeability issue in a proposed candidate, the meta-review ensures that all future reviews by the Reflection Agent definitively address this crucial factor. Hypothesis and research proposal generation is also enhanced by the meta-review's identification of recurring issues. While the Generation agent uses this feedback selectively to avoid over fitting to these review critiques, it helps prevent the recurrence of common issues.
Appendix Figure A. 8 provides an example prompt for the meta-review. In Appendix Figure A.18-A.19, we showcase an example of the summarized meta-review critique generated for the reviews of the previously</p>
<p>introduced ALS mechanism hypotheses.
Research overview generation. The Meta-review agent periodically synthesizes top-ranked hypotheses into a research overview, providing a roadmap for future research. This overview outlines potential research areas and directions relevant to the research goal, justifying their importance and suggesting specific experiments within each. Each area includes illustrative example topics. The research overview also serves as an additional input to the Generation agent in subsequent iterations.
The research overview serves to effectively map the boundary of current knowledge relevant to the research goal in the co-scientist system and helps highlight future areas of exploration. In Appendix Figure A.20-A.21, we show an example of a research overview for the ALS mechanism research goal.
The Meta-review agent can further format these overviews using constrained decoding techniques [70] to adhere to common research publication and grant formats (e.g., National Institute of Health (NIH) Specific Aims Page format). We demonstrate the effectiveness of this in subsequent sections.</p>
<p>Research contacts identification. The Meta-review agent uses prior literature review to suggest qualified domain experts for research hypotheses and proposal review, including the reasoning behind each suggestion. These potential contacts are summarized in the research overview, providing researchers with additional perspectives and potential avenues for collaborations. An example research contact (with the researcher name redacted) is shown in Appendix Figure A. 22 .</p>
<h1>3.4 Expert-in-the-loop interactions with the co-scientist</h1>
<p>The AI co-scientist empowers scientists to actively guide the system through an expert-in-the-loop design (Figure 2). Scientists can interact with the system in several ways:</p>
<ul>
<li>Refine the initial research goal in light of the generated hypotheses and research overview.</li>
<li>Provide manual reviews of generated hypotheses (see Section 3.3.2 for other system generated review types), which the co-scientist uses to evaluate and improve the hypotheses and proposals.</li>
<li>Contribute their own hypotheses and proposals for inclusion in the tournament, where they are ranked alongside and can be combined with system-generated hypotheses and proposals.</li>
<li>Direct the co-scientist to follow up on specific research directions (for example restricted to a smaller collection of prior publications). When this research is referenced in the research goal, the co-scientist can prioritize generation methods that can access and synthesize it.</li>
</ul>
<h3>3.5 Tool use in AI co-scientist</h3>
<p>The co-scientist leverages various tools during the generation, review, and improvement of hypotheses and research proposals. Web search and retrieval are primary tools, important for grounded, up-to-date hypotheses.
For research goals that explore a constrained space of possibilities (e.g., all known cell receptors of a specific type or all FDA-approved drugs), the co-scientist agents utilize domain-specific tools, such as open databases, to constrain searches and generate hypotheses. The co-scientist can also index and search a private repository of publications specified by the scientist.
Finally, the system can utilize and incorporate feedback from specialized AI models like AlphaFold. We demonstrate this qualitatively with a protein design example in the Appendix Section A.6.</p>
<h1>4 Evaluation and Results</h1>
<p>We now discuss the methods for evaluating the AI co-scientist system and the corresponding results. The initial evaluations aim to benchmark and verify the choice of the strategies and metrics underpinning the co-scientist. We then proceed to perform a small-scale evaluation with domain experts to assess the quality of the system.</p>
<p>Furthermore, to assess the practical utility of the system's novel predictions, we also perform end-to-end wet-lab validations (laboratory experiments) of the co-scientist-generated hypotheses and research proposals in three key biomedical applications: drug repurposing, discovering novel treatment targets, and elucidating the mechanisms underlying antimicrobial resistance. The varying complexity and nature of these applications enable a more comprehensive assessment of the system. Notably, all three validations involved expert-in-theloop guidance and prioritization of experiments. These applications are summarized in Table 1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Application</th>
<th style="text-align: center;">Drug repurposing</th>
<th style="text-align: center;">Novel treatment target discovery</th>
<th style="text-align: center;">Explain mechanism of <br> gene transfer evolution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Challenge</td>
<td style="text-align: center;">Combinatorial search</td>
<td style="text-align: center;">Identifying novel targets</td>
<td style="text-align: center;">Understanding complex systems</td>
</tr>
<tr>
<td style="text-align: center;">Complexity</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">Very high</td>
</tr>
<tr>
<td style="text-align: center;">Scale</td>
<td style="text-align: center;">Moderate, data-limited</td>
<td style="text-align: center;">Moderate, experiment-limited</td>
<td style="text-align: center;">Large, data and computation-limited</td>
</tr>
<tr>
<td style="text-align: center;">Unknown elements</td>
<td style="text-align: center;">Constrained</td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">Vast and dynamic</td>
</tr>
</tbody>
</table>
<p>Table 1 | Three real-world applications in biomedicine for end-to-end validation of the AI co-scientist.</p>
<h3>4.1 The Elo rating is concordant with high quality AI co-scientist results</h3>
<p>The Elo auto-evaluation rating is a key metric that guides the self-improvement feedback loops within the co-scientist system. Therefore, it's necessary to measure and ensure higher Elo ratings correlate with higher quality results. To assess this, we analyzed the concordance between the Elo rating and the system's accuracy on the GPQA benchmark dataset. Ideally, higher Elo ratings should correlate with a higher probability of correct answers.</p>
<p>The GPQA dataset is a challenging, multiple-choice question answering benchmark developed by experts in biology, physics, and chemistry [71]. To ensure that the co-scientist Elo rating serves as an objective metric reflecting the validity and correctness of results from the system, we utilized questions within the GPQA diamond set, a subset of the GPQA dataset known for its high difficulty, framing each question as a research goal into our AI system to elicit responses. For each question, we first compared each co-scientist response against the ground truth answer to evaluate its correctness. Then, we categorized all generated responses across all considered questions based on their Elo rating into discrete buckets: Elo rating of 1001-1050, 1051-1100, 1101-1150, etc. in 50 point increments, until the highest rating achieved. Finally, we calculated the average accuracy for each Elo rating bucket, as the percentage of correct responses within each bucket.
We employed the underlying Gemini 2.0 models in the AI co-scientist to create a reference baseline. The reference is necessary because responses within a particular Elo rating bucket are not uniformly distributed across the GPQA questions - some of which are inherently more challenging than others. This non-uniformity could introduce bias into the analysis and potentially lead to erroneous conclusions. We therefore used the reference to generate 32 responses for each GPQA question. The fraction of correct responses from Gemini 2.0 was used as a reference accuracy on that particular question. To determine reference accuracy for a specific Elo bucket, we averaged the reference accuracy of the GPQA questions that had co-scientist responses within that bucket. We also computed the co-scientist accuracy on the GPQA diamond set by using the result with the highest Elo rating for each question and comparing it against the ground truth.
Our analysis using questions from the GPQA diamond set reveals a concordance between the Elo rating and averaged accuracy of generated co-scientist results, as depicted in Figure 3. By selecting the top-rated co-scientist result for each question, the co-scientist achieves a top-1 accuracy of $78.4 \%$.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Concordance of the auto-evaluation Elo metric with AI co-scientist performance on GPQA. The blue line in the figure shows the average accuracy of co-scientist responses, grouped by their Elo rating. The red line indicates the average accuracy of the corresponding reference Gemini 2.0 responses to the same set of GPQA questions, grouped by Elo rating. Note that Elo metric is auto-evaluated and not based on the ground truth.</p>
<h1>4.2 Scaling test-time compute improves scientific reasoning of the AI co-scientist</h1>
<p>To evaluate the effects of test-time compute scaling and the co-scientist's progress during iterative scientific reasoning and hypothesis generation, we measured the Elo ratings of the co-scientist generated hypothesis and proposals over the course of the tournament. This analysis was done across 203 distinct research goals curated across broad scientific topics (predominantly in biomedicine, but also included other topics such as mathematics and physics) and entered into the co-scientist system until February 3, 2025.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4 | Impact of scaling test-time compute on AI co-scientist as measured by Elo auto-evaluation. The co-scientist's research hypotheses and proposals were partitioned into ten temporal buckets of equal size, with the last bucket corresponding to the most recently generated results from the system. For each bucket, we determined the maximum individual Elo rating (the "best Elo") and the average Elo rating of the top 10 hypotheses across 203 unique research goals. The resulting upward performance trends, across both metrics, suggest improvements in the co-scientist result quality with scaling of test-time compute. Note that the Elo metric is auto-evaluated and not based on independent ground truth.</p>
<p>The co-scientist's research hypotheses and proposals were partitioned into ten temporal buckets of equal size. Each bucket corresponded to a sequential $10 \%$ of the total generation time with the first bucket containing the earliest $10 \%$ of generated co-scientist results, while the tenth bucket comprised the most recent $10 \%$. For each bucket, we determined the average Elo rating of the top 10 hypotheses and the maximum individual Elo rating (the "best Elo"). These average and best Elo ratings were averaged across 203 research goals and their corresponding tournaments. The resulting performance trends as seen in Figure 4, across both metrics, serves as a measure of the co-scientist's quality improvement over time. The most recent results demonstrated a significant quality enhancement compared to the initial outputs. Notably, although the Elo rating is not the direct optimization target, its progressive increase emerges from the system's self-improvement feedback loops.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5 | Comparison of the AI co-scientist with other baselines as measured by Elo auto-evaluation on a subset of 15 challenging expert-curated research goals. The AI co-scientist's research hypotheses and proposals were partitioned into ten equal-sized temporal buckets, with the last bucket representing the most recent system-generated results. For each bucket, we calculated the maximum individual Elo rating ("best Elo") and the average Elo rating of the top 10 hypotheses across 15 challenging research goals, curated by biomedical experts. These metrics were then compared to the performance of several baselines: Gemini 2.0 Pro Experimental, Gemini 2.0 Flash Thinking Experimental 12-19, OpenAI o1, OpenAI o3-mini-high, DeepSeek R1, and the "best guess" hypotheses provided by the expert curators. These baseline performances are represented by colored dashed lines. On this expert-curated subset of research goals, we consistently observed upward performance trends with increased test-time compute. Furthermore, the co-scientist significantly outperformed both the human experts and the other state-of-the-art LLM baselines, as measured by the Elo metric. It's important to note that the Elo metric is auto-evaluated and not based on independent ground truth. This auto-evaluation metric may favour results with attributes that do not necessarily align with scientists' preferences, quality, or accuracy.</p>
<p>To further contextualize this observation, we focused our analysis on a subset of 15 research goals, curated as challenging problems by seven biomedical experts in their respective fields of expertise. These experts held doctoral degrees in biological or life science disciplines and were actively working as postdoctoral researchers or faculty members at academic biomedical research institutes. These research goals were consistently structured and formatted, encompassing a research title, a clear set of goals, preferences specifying biological or disease areas of interest, desirable solution attributes, and constraints on experimental techniques.
In addition to the research goals, the experts provided their "best guess" hypotheses or solutions. We then included outputs from other state-of-the-art LLMs and reasoning models - Gemini 2.0 Pro Experimental, Gemini 2.0 Flash Thinking Experimental 12-19, OpenAI o1, OpenAI o3-mini-high, and DeepSeek R1 -in a tournament along with the expert "best guess" and co-scientist for each curated goal. Performance was assessed using the co-scientist Elo rating metric.
The trends previously observed with test-time compute scaling in Figure 4 were consistent within this subset. Furthermore, as shown in Figure 5, the co-scientist surpassed the other frontier LLMs and reasoning models in Elo rating with increased computational resources for iterative improvement. Notably, newer reasoning models, such as OpenAI o3-mini-high and DeepSeek R1, demonstrated competitive performance while requiring significantly less compute and reasoning time. Finally, we observed no evidence of performance saturation as measured by Elo, suggesting that further scaling of test-time compute in this paradigm could yield continued improvements in result quality of the co-scientist system. Its worth noting again that the co-scientist architecture is model agnostic and is likely to benefit from further advancements in frontier and reasoning LLMs.
Building upon the co-scientist system's ability to combine, refine and improve research hypotheses and proposals iteratively, we investigated its potential to improve upon expert "best guess" solutions. Consistent with our previous observations, the co-scientist demonstrated the capacity to enhance expert's "best guess" solutions over time, as evidenced by the Elo metric in Figure 6. Notably, the improvement trends initially mirrored those of the co-scientist's autonomously generated solutions but subsequently surpassed them. While this is a preliminary finding requiring further validation, it suggests a promising avenue for capable AI systems, such as the co-scientist, to augment and accelerate the work of expert scientists.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6 | AI-augmented expertise with the co-scientist through Elo-based auto-evaluation. Through its selfimprovement process, the co-scientist refines and enhances expert "best guess" solutions over time, as measured by the Elo rating on a subset of 15 curated research goals. It is important to note that the Elo metric is auto-evaluated and not based on independent ground truth.</p>
<h1>4.3 Experts consider the AI co-scientist results to be potentially novel and impactful</h1>
<p>To obtain expert feedback and assess preferences, we conducted a small-scale expert evaluation on 11 of the 15 previously curated research goals. We asked the experts who curated the research goals to assess outputs from the AI co-scientist, Gemini 2.0 Flash Thinking Experimental 12-19, Gemini 2.0 Pro Experimental, and OpenAI o1 models. Specifically, they provided a preference ranking ( 1 being most preferred and 4 being least preferred) and rated the novelty and impact of the proposed solutions on a 5-point scale, ranging from 1 (worst) to 5 (best) following this rubric:</p>
<ul>
<li>Novelty: Higher-ranked outputs should propose hypotheses that, to the best of the expert's knowledge, have not been previously published in any form. Hypotheses similar to existing proposals, even with minor modifications, should rank lower, and exact replicas of previously proposed and performed experiments should receive the lowest ranking.</li>
<li>Impact: Higher-ranked outputs should address significant open questions in the field and have the potential to substantially advance scientific understanding or lead to practical applications.</li>
</ul>
<p>Across 11 expert-evaluated research goals, outputs generated by the AI co-scientist were most preferred and rated higher in novelty and impact axes compared to the other baseline models. Specifically, the co-scientist received an average preference rank of 2.36 , and novelty and impact ratings of 3.64 and 3.09 (out of 5) as shown in Figure 7. These evaluations reflect subjective expert assessments, not objective ground truth. Notably, the human expert preferences also appear to be concordant with relative Elo ratings as can be inferred from Figure 5 and Figure 7.
We also conducted the same preference ranking evaluation between co-scientist and other LLM and reasoning model baselines using the OpenAI o3-mini-2025-01-31, o1-preview-2024-09-12, Gemini 2.0 Pro Experimental and Gemini 2.0 Flash Thinking Experimental 01-21 as judges. The co-scientist outputs were the most preferred by both the o3-mini, o1 and Gemini 2.0 Pro Experimental models as shown in (Figure 8). Due to the small scale of these evaluations, further large-scale studies are necessary for any reliable conclusions. We present a more comprehensive clinical expert evaluation focused on co-scientist proposals for drug repurposing formatted in the NIH Specific Aims Page format in Section 4.5.1.</p>
<h3>4.4 Safety evaluation of the AI co-scientist using adversarial research goals</h3>
<p>The AI co-scientist is designed to empower scientists and accelerate research. However, it's crucial to ensure the system is designed with robust safety principles, given the potential for misuse. This includes addressing dangerous research goals, dual-use objectives, scenarios where safe goals lead to unsafe hypotheses, misleading claims, and inherent biases. While this topic requires extensive investigation beyond the scope of this work, we employed adversarial testing strategies to conduct a preliminary safety analysis of the system. Specifically,</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7 | Expert evaluation of AI co-scientist and other LLM baselines. Left: Average expert ratings on novelty and impact of the model responses across 11 expert curated research goals. Higher numbers indicate better ratings (1-5). Right: Average expert preference ranking of the results across 11 expert curated research goals generated by AI co-scientist, Gemini 2.0 Flash Thinking Experimental 12-19, Gemini 2.0 Pro Experimental, and OpenAI o1, respectively. Lower numbers indicate better rankings (1-4). The human expert preferences also appear to be concordant with relative Elo ratings as can be inferred from Figure 5. At the same time, its worth noting that these preferences and ratings reflect subjective expert assessments, not objective ground truth.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8 | LLM preference ranking auto-evaluation of AI co-scientist and other baselines. Averaged preference ranking of results across 11 expert curated research goals generated by AI co-scientist, Gemini 2.0 Flash Thinking Experimental 12-19, Gemini 2.0 Pro Experimental, and OpenAI o1, using four different LLM evaluators: OpenAI o3-mini-2025-01-31 (upper left), OpenAI o1-preview-2024-09-12 (upper right), Gemini 2.0 Pro Experimental (lower left), and Gemini 2.0 Flash Thinking Experimental 01-21 (lower right). Lower numbers indicate better rankings.</p>
<p>we curated a set of 1200 adversarial examples, ranging in complexity, across 40 biomedical and scientific topics using frontier LLMs. We then evaluated whether the AI co-scientist could robustly reject these research goals. In this preliminary analysis, the system successfully passed all checks. Given the sensitive nature of these adversarial research goals, we will not be publicly releasing the dataset, but it can be made available upon request. Collectively, the benchmark, automated, and expert evaluations presented in this section provide compelling evidence of the system's strong capabilities.</p>
<h1>4.5 Drug repurposing with the AI co-scientist</h1>
<p>As previously noted, a rigorous assessment of a system's ability to generate novel hypotheses and predictions for complex research problems necessitates end-to-end validation through wet-lab experiments. However, due to the challenging, time-consuming, and resource-intensive nature of such endeavors, large-scale experimental validation is infeasible. Instead, we strategically selected diverse yet critical biomedical topics to serve as a strong benchmark for the end-to-end system evaluation. Detailed descriptions of these topics follow. Importantly, all three experimental validations were conducted in collaboration with expert scientists, who provided guidance to the co-scientist and prioritized wet-lab experiments.
We begin the discussion of the end-to-end validation of the AI co-scientist with a drug repurposing application. As introduced earlier, drug repurposing is the process of identifying novel therapeutic indications for existing, approved drugs beyond their original use. This approach can accelerate the discovery of treatments for complex and rare diseases, as repurposed drugs have established safety profiles and are readily available. From a technical standpoint, this is a combinatorial search problem involving a large but finite set of drug-disease pairs as noted in Table 1.
Given the co-scientist's ability to synthesize and integrate information across a vast body of scientific and clinical literature, we hypothesized that drug repurposing would be an ideal test of the system's capabilities. The system is general-purpose, capable of providing highly detailed and explainable predictions across all known drug-disease pairs. Here, we focused on the computational biology and wet-lab validation of our co-scientist system in the area of drug repurposing for cancer treatment.
We initially investigated drug-cancer pairs with existing preclinical evidence to validate the plausibility of the hypotheses and predictions generated by the co-scientist (Section 4.5.1), before expanding to completely novel drug repurposing hypotheses (Section 4.5.2). The validation of the co-scientist's predictions was performed using a multi-faceted approach, incorporating computational biology analyses, oncologist expert feedback, and in vitro wet-lab experiments using cancer cell lines.</p>
<h3>4.5.1 The AI co-scientist suggests plausible drug repurposing candidates as rated by experts</h3>
<p>We constrained the AI co-scientist to explore potential repurposing hypotheses from a curated list of 2300 approved drugs across 33 cancer types (Appendix Section A.4.1). To achieve this, we modified the prompts used in the Generation and Ranking agent stages to ensure hypotheses generation in this constrained search space; however, the core co-scientist logic remained unchanged. When formulating the research goal for the co-scientist, we explicitly emphasized the following preferences related to drug repurposing:</p>
<ul>
<li>Elucidate the known mechanisms of action and impacted biological pathways of the drug.</li>
<li>Identify potential diseases or cancer types that could be treatment targets for the drug.</li>
<li>Explain the potential mechanisms by which the drug could exert therapeutic effects.</li>
<li>Propose alternative mechanisms of action through which the drug might function in the proposed therapeutic context.</li>
<li>Identify the diseases / cancers for which the drug is currently approved.</li>
<li>List the most promising disease / cancer type candidates for repurposing.</li>
<li>Discuss prior research and challenges associated with repurposing the drug.</li>
</ul>
<p>For each drug-cancer pair, we also extracted the Cancer Dependency Map (DepMap) probability of dependency ("DepMap score") [72] (Appendix Section A.4.2). The DepMap score represents the probability of essentiality for a gene in a given cancer cell lines. We ranked all drug-cancer pairs using a combined metric of the co-scientist review score (ranging from 1 to 5 ) and the DepMap score (ranging from 0.0 to 1.0). To prioritize</p>
<p>the most relevant hypotheses for expert review, we selected only pairs where the co-scientist review score $\geq 4$ and the DepMap score $\geq 0.99$. Note that the DepMap score is primarily meant to function as a sanity check and filter out obviously incorrect candidates but is unlikely to be predictive of efficacy.
Expert oncologists then reviewed the top-ranked drug-cancer pairs, provided feedback, and selected promising repurposing candidates for in vitro wet-lab validation (Appendix Section A.4.3).</p>
<h1>Clinical expert evaluation of drug repurposing proposals in NIH Specific Aims Page format.</h1>
<p>To rigorously evaluate whether the co-scientist-generated hypotheses for drug repurposing fulfill the needs of physicians and scientists, we restructured the co-scientist hypotheses into the NIH-style grant proposal Specific Aims Page (examples in Appendix Figure A.26-A.31), and asked a team of six expert hematologists \&amp; oncologists to evaluate the specific aims.
The NIH Specific Aims Page format follows a standard structure, including disease description, unmet need, proposed solutions, and specific aims. This format was selected because it provides a standardized framework that is widely recognized in the research community, allowing for systematic presentation of complex scientific topics in a manner that facilitates rigorous peer review and enables efficient assessment of scientific merit. The specific aims, which outline the overarching goal, hypothesis, and rationale, requires extensive scientific expertise, comprehensive literature analysis, and robust domain knowledge. We generated cancer drug repurposing hypotheses derived from the co-scientist in the format of NIH Specific Aims Page with additional constrained decoding and self-critique stages to ensure format consistency. The resulting format contextualizes proposed repurposing candidates within known mechanisms based on current literature and then extrapolates to a new disease state. An expert oncologist methodically evaluated and excluded hypotheses that were deemed clinically implausible or had limited potential for successful translation, as well as those falling outside the expertise of the assembled specialist evaluators. This initial screening process employed multiple evidence-based criteria including: (1) pharmacological mechanism incompatibility with tumor biology; (2) unfavorable pharmacokinetic profiles for oncological applications; (3) prohibitive toxicity profiles documented in prior clinical use; (4) confounding effects where apparent survival benefits were attributable to improved management of treatment-related morbidity rather than direct anti-neoplastic activity; and (5) insufficient preclinical evidence supporting antitumor efficacy at clinically achievable concentrations. For example, bisphosphonate agents like pamidronate, while associated with improved outcomes in observational studies of patients with bone metastases, were excluded after critical evaluation revealed their benefits stemmed primarily from reduction of skeletal-related events (such as pathological fractures, spinal cord compression, and bone pain requiring radiation) rather than from disease modifying activity of the drug-candidate.
Six board-certified hematologists \&amp; oncologists from a single institution - including four domain-specific oncologists specializing in gastrointestinal (GI), breast, gynecologic (GYN) and genitourinary (GU) cancers and two general hematologist \&amp; oncologists, with an average of eight years of clinical experience - evaluated 78 unique drug repurposing hypotheses presented in the NIH Specific Aims Page format (for specific indication distribution and counts, see Appendix Section A.5.1).
The expert raters evaluated the generated Specific Aims based on a modified NIH grant proposal evaluation rubric, consisting of 15 axes focusing on (1) importance of research (significance and innovation) and (2) approach (rigor and feasibility). The raters indicated their agreement level using a five-point scale: "Strongly Agree", "Agree", "Neutral", "Disagree", and "Strongly Disagree". For each axis, we included several questions covering different aspects of the NIH evaluation criteria. The evaluation rubric is further detailed in the Appendix Section A.5.2. Specifically, we ask raters to focus on evaluating the clinical relevance and potential for clinical translation, and not for translational capacity or the design of clinical trials.
We observed that expert raters consistently assigned high ratings ("Strongly Agree" or "Agree") to the Specific Aims proposed by the co-scientist across various evaluation criteria (Figure 9). Of note, the favorable assessments of co-scientist-generated hypotheses may be partially attributed to expert pre-screening, wherein a clinician eliminated non-viable candidates prior to expert evaluation. Three examples of generated Specific Aims and their respective expert review ratings are detailed in Appendix Figure A.26-A.31.
The generated Specific Aims were assessed by clinical hematologists \&amp; oncologists from a single-center, which might bias the interpretation of the evaluation results, as it may introduce institutional perspectives shaped by local practice patterns, clinical experiences, and research frameworks unique to that setting. While some</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9 | Clinical expert evaluation for the co-scientist generated drug repurposing hypotheses in the NIH Specific Aims Page format. Six expert hematologists \&amp; oncologists reviewed 78 drug repurposing research proposals, which the co-scientist had formatted as NIH Specific Aims Pages. The evaluation followed an adapted NIH grant proposal evaluation rubric, detailed in Appendix Section A.5.2. Overall, the oncologists judged the Specific Aims proposals from the AI co-scientist to be of high quality across all axes of the rubric.</p>
<p>Specific Aims may be supported by preclinical data, it is important to note that none of the proposed drug candidates have undergone randomized phase III clinical trials necessary to establish efficacy and secure regulatory approval for repurposing to a new indication.</p>
<h1>4.5.2 The AI co-scientist identifies novel drug repurposing candidates for acute myeloid leukemia</h1>
<p>Building upon the positive feedback from clinical experts, we conducted in vitro wet-lab validation experiments for drug repurposing hypotheses generated by the co-scientist for acute myeloid leukemia (AML). AML is an aggressive and relatively rare blood cancer characterized by the rapid proliferation of abnormal white blood cells (myeloblasts) in the bone marrow, which displaces healthy blood cells. We focused on this indication due to its aggressive nature and the limited availability of effective therapeutic interventions [73].
The cell-line based experiments conducted here serve as an initial biological validation step for co-scientist hypotheses, with intentionally straightforward methodology following established protocols. The simplicity in experimental design is purposeful; our focus is on evaluating the merit of AI co-scientist generated hypotheses rather than developing novel laboratory techniques. Positive results from these experiments should be interpreted as preliminary evidence warranting further investigation through comprehensive pre-clinical studies (e.g., in vivo models) and potentially clinical evaluation.</p>
<p>It is important to emphasize that these wet-lab experiments function as a viability checkpoint in the drug repurposing pipeline - not as a replacement for the rigorous pre-clinical and clinical assessment typically required for therapeutic validation. They provide an efficient biological reality check that helps bridge the gap between computational predictions and potential clinical applications, allowing us to rapidly triage AI-generated hypotheses before committing to more resource-intensive validation studies necessary for clinical translation.</p>
<p>Drug repurposing candidate selection process for acute myeloid leukemia. The candidate selection for wet-lab experiments was performed with meticulous expert oversight. Thirty top-ranked drug candidates hypotheses were shared with expert oncologists (an example detailed co-scientist output is provided in Section A.5.4). The experts evaluated the hypotheses, selecting drug candidates based on their potential to modulate key molecular signaling pathways associated with disease progression and resistance. Note that</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Equal contributions.
$\ddagger$ Corresponding authors: {juro, ckbjimmy, apawlosky, alankarthi, natviv}@google.com</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
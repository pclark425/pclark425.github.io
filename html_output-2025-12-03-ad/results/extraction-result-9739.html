<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9739 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9739</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9739</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-276885275</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.05061v1.pdf" target="_blank">No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding</a></p>
                <p><strong>Paper Abstract:</strong> LLM-as-a-Judge is a framework that uses an LLM (large language model) to evaluate the quality of natural language text - typically text that is also generated by an LLM. This framework holds great promise due to its relative low-cost, ease of use, and strong correlations with human stylistic preferences. However, LLM Judges have been shown to exhibit biases that can distort their judgments. We evaluate how well LLM Judges can grade whether a given response to a conversational question is correct, an ability crucial to soundly estimating the overall response quality. To do so, we create and publicly release a human-annotated dataset with labels of correctness for 1,200 LLM responses. We source questions from a combination of existing datasets and a novel, challenging benchmark (BFF-Bench) created for this analysis. We demonstrate a strong connection between an LLM's ability to correctly answer a question and grade responses to that question. Although aggregate level statistics might imply a judge has high agreement with human annotators, it will struggle on the subset of questions it could not answer. To address this issue, we recommend a simple solution: provide the judge with a correct, human-written reference answer. We perform an in-depth analysis on how reference quality can affect the performance of an LLM Judge. We show that providing a weaker judge (e.g. Qwen 2.5 7B) with higher quality references reaches better agreement with human annotators than a stronger judge (e.g. GPT-4o) with synthetic references.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9739.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9739.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Answerability dependence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dependence of judge agreement on judge's ability to answer the question</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper demonstrates that an LLM Judge's agreement with human correctness labels strongly depends on whether the judge model itself can correctly answer the underlying question; performance drops sharply on questions the judge cannot answer unless supplied a correct human reference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Conversational question answering with math and finance reasoning (BFF-Bench and corrected MT-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Evaluated across models used as judges: GPT-4o, Llama 3.3 70B, Yi 1.5 34B, Phi-4, Qwen 2.5 7B (and Gemma 2 2B as candidate-only)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Single and pairwise grading prompts (modified MT-Bench style) with optional reference included (None, Self-generated, Human-written); judges may use chain-of-thought and self-consistency (majority vote over 5 samples at temperature 0.7).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>In-house domain-expert annotators (each with undergraduate or masters in finance), 3 annotators per question-response pair, allowed external non-LLM resources; labels: Correct / Incorrect / Not Sure; consensus filtering removed ambiguous items.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's Kappa (reported conditioned on whether judge could answer the question); example effects described qualitatively (large drop in agreement on questions judge could not answer).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When judges cannot answer the question themselves, they provide poor signals of correctness: their agreement with human labels is much lower on that subset, meaning LLM-as-a-Judge loses reliability on the hardest items and cannot reliably surface correctness without human grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>The paper conditions agreement on whether the judge answered the question correctly and shows large gaps in Cohen's Kappa between 'judge-answered-correctly' and 'judge-answered-incorrectly' subsets (figures and Table 4). The authors state this causes poor signal on the most difficult questions within a benchmark and limits evaluating stronger candidate models with weaker judges.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Providing a correct human reference largely removes this dependence (judge agreement becomes much better even on questions the judge model could not itself answer). Pairwise tasks can mask difficulty because pairs often include one clearly correct and one incorrect answer, making grading easier.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Introduction; Results (conditioning agreement on judge answerability); Table 4; Figure 1</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9739.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9739.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of LLM-as-a-Judge to reference quality (None / Self / Human / Wrong / Random)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study systematically varies reference type in the judge prompt and shows that judge performance is highly sensitive to whether a correct human reference is provided; incorrect (``Wrong'') references can actively degrade performance more than no reference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Conversational QA (math and finance), single and pairwise correctness grading</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Same judges: GPT-4o, Llama 3.3 70B, Yi 1.5 34B, Phi-4, Qwen 2.5 7B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Judging prompts include one of: None (no reference), Self (judge's own generated reference), Human (human-written gold reference), Wrong (human-edited but incorrect), Random (reference from unrelated question); otherwise same prompt/instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human gold references created/verified by domain experts; human correctness labels obtained as described above for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's Kappa reported across different reference types; specific reported effect: providing human references improves agreement for all judges; 'Wrong' references increase false negative rate substantially (text reports very high FNR with Wrong references).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using incorrect or low-quality references can degrade judgment accuracy (e.g., causing judges to mark correct responses as incorrect), and using self-generated references only helps when the judge could already answer the question correctly — so without human-grounded correct references LLM judges can be misled or unhelpful.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Paper reports that 'Wrong' references produce low false positive rate but very high false negative rate (correct responses get marked incorrect). Example numeric contrasts and qualitative descriptions appear in Error Analysis and Tables (e.g., large differences when switching only the provided reference).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Verified LLM-generated references (i.e., synthetic references human-validated as correct) can approach the performance of human-written references; providing any correct reference (human or verified synthetic) substantially improves agreement, and in some configurations a weaker judge + high-quality reference outperforms a stronger judge with synthetic references.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Grading References section; Error Analysis; Results; Table 5/Table 6</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9739.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9739.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-preference bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tendency for LLM Judges to favor their own responses (self-preference) when grading</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models exhibit a bias to overrate their own outputs: higher false positive rates when grading responses generated by themselves versus responses from other models, especially when provided self-generated references or no reference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pairwise and single correctness grading in conversational QA</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Observed across evaluated judges (GPT-4o, Llama 3.3 70B, Yi 1.5 34B, Qwen 2.5 7B, Phi-4)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Judges sometimes asked to evaluate their own responses (Self reference or judging own outputs) and comparisons computed between judgments of own vs others' responses; self-consistency used to reduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human labels used as ground truth to compute false positive/negative rates per judge when grading own vs other models' responses.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>False Positive Rate (FPR) and False Negative Rate (FNR) aggregated and compared; Cohen's Kappa also reported but bias described via FPR/FNR gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using an LLM Judge risks introducing self-preference bias that inflates scores of models whose outputs resemble the judge's style or content, degrading objectivity relative to human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Figure 4 and Figure 5 show that for every reference type on average models have a higher FPR when grading their own responses, with the gap particularly large for Self and None references; providing a human reference reduces both overall error and the self/other gap.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Human references reduce self-preference; verifying synthetic references instead of using the model's own references can mitigate this bias; occasional 'overruling' behavior (judge recognizes the reference is wrong and correctly marks the candidate) was observed (Appendix D), showing not all decisions are driven by blind self-preference.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Error Analysis; Figures 4 and 5; Conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9739.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9739.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Masking by aggregates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregation masking of judge weaknesses (aggregate agreement can hide subset failures)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate-level agreement metrics can give an overly optimistic impression of judge quality while masking severe failures on the subset of hard questions that the judge cannot answer, reducing trustworthiness in difficult domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Conversational QA and benchmark evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>All evaluated judges (not model-specific phenomenon)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Evaluation reported at aggregate level and also conditioned on per-question judge answerability to reveal hidden failures.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human consensus labels used to reveal divergences per-subset (answered-correctly vs answered-incorrectly by judge).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's Kappa reported overall and conditioned; qualitative emphasis that masked poor performance occurs on difficult subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When relying on aggregate agreement, researchers may miss that LLM Judges fail precisely on the hardest items — when correct assessment matters most — leading to overconfidence in automated evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Paper states 'This issue can be masked by aggregate statistics, especially when the judge performs well on the underlying benchmark itself' and shows conditioned analyses where agreement collapses on questions judge cannot answer.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Pairwise setups (with one correct and one incorrect response) can artificially raise agreement because differences are easier to detect; conditioning analyses reveal the masking and are recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Introduction; Results (conditioning on judge correctness); Table 4; Figure 1</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9739.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9739.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Style-over-substance bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Influence of style, length, and tone on LLM judge decisions (style bias)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM Judges tend to overweight superficial stylistic features (length, tone, phrasing) over substantive correctness or safety, leading to evaluations that diverge from human judgments focused on correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General NLG evaluation; observed within conversational QA evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Observed in literature and discussed/applied to judges in this study (multiple model families)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Judging prompts may not fully control for stylistic influence; prior work and this paper note style-related failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human graders used human-written gold references and judged correctness holistically, less swayed by superficial style when correctness determined by content.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Prior work cited and the paper's Error Analysis reference biases qualitatively; the paper references prior findings (Feuer et al., 2024) and presents error profiles tied to reference types.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM judges may rate fluent or verbose but incorrect responses highly, and may miss safety or factual errors masked by good style, unlike careful human graders who weigh substance more.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>The paper cites Feuer et al. (2024) concluding LLM Judges are overly influenced by writing style and do not adequately capture safety and correctness; the paper's own error analysis and observed FPR/FNR patterns are consistent with style-influenced misjudgment.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Prompt engineering, rubrics, randomization, pairwise ordering controls, and providing correct references can mitigate but not fully eliminate style biases; ensembles/juries may also help.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Related Work (Feuer et al., 2024; Koo et al., 2023); Error Analysis; Limitations</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9739.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9739.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM vs simple baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge compared to simple reference-based similarity baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper compares LLM judges to simple reference-similarity classifiers (ROUGE, BLEU, METEOR, and embedding cosine similarity) and finds LLM-as-a-Judge methods still outperform these baselines on pairwise grading when provided high-quality references.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pairwise correctness grading with human gold references</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Best-performing LLM judges (e.g., Llama 3.3 70B) vs embedding and n-gram baselines</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise tasks with human reference and LLM judge prompted to pick the more correct response; baselines pick response with higher similarity to reference.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human gold labels used to compute Cohen's Kappa of baselines and LLM judges for comparison; same human references used for similarity measures.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's Kappa reported for baselines (ROUGE ~0.26, METEOR ~0.19, BLEU ~0.02; embedding methods up to ~0.38) and for LLM judges (e.g., Llama 3.3 70B reported ~0.87 in Table 6 for pairwise with human reference).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>While LLM judges outperform simple baselines overall, without human grounding (correct references) their advantages shrink and they are still vulnerable to the other losses above (answerability dependence, self-preference, reference sensitivity).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Table 6 shows baseline metric kappas (traditional metrics and embedding models) that lag the LLM judge performance when good references are present; the paper emphasizes reference quality is the primary driver of performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>If high-quality references are unavailable, simple similarity baselines remain weak but also less likely to introduce self-preference; verifying LLM-generated references can be a practical compromise to obtain good agreement without full manual reference writing.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Reference-based baselines section; Table 6; Appendix C</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges <em>(Rating: 2)</em></li>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Style outweighs substance: Failure modes of llm judges in alignment benchmarking <em>(Rating: 2)</em></li>
                <li>Benchmarking cognitive biases in large language models as evaluators <em>(Rating: 2)</em></li>
                <li>Refenceguided verdict: Llms-as-judges in automatic evaluation of free-form text <em>(Rating: 1)</em></li>
                <li>Is reference necessary in the evaluation of NLG systems? when and where? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9739",
    "paper_id": "paper-276885275",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Answerability dependence",
            "name_full": "Dependence of judge agreement on judge's ability to answer the question",
            "brief_description": "The paper demonstrates that an LLM Judge's agreement with human correctness labels strongly depends on whether the judge model itself can correctly answer the underlying question; performance drops sharply on questions the judge cannot answer unless supplied a correct human reference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Conversational question answering with math and finance reasoning (BFF-Bench and corrected MT-Bench)",
            "llm_judge_model": "Evaluated across models used as judges: GPT-4o, Llama 3.3 70B, Yi 1.5 34B, Phi-4, Qwen 2.5 7B (and Gemma 2 2B as candidate-only)",
            "llm_judge_setup": "Single and pairwise grading prompts (modified MT-Bench style) with optional reference included (None, Self-generated, Human-written); judges may use chain-of-thought and self-consistency (majority vote over 5 samples at temperature 0.7).",
            "human_evaluation_setup": "In-house domain-expert annotators (each with undergraduate or masters in finance), 3 annotators per question-response pair, allowed external non-LLM resources; labels: Correct / Incorrect / Not Sure; consensus filtering removed ambiguous items.",
            "agreement_metric": "Cohen's Kappa (reported conditioned on whether judge could answer the question); example effects described qualitatively (large drop in agreement on questions judge could not answer).",
            "losses_identified": "When judges cannot answer the question themselves, they provide poor signals of correctness: their agreement with human labels is much lower on that subset, meaning LLM-as-a-Judge loses reliability on the hardest items and cannot reliably surface correctness without human grounding.",
            "examples_of_loss": "The paper conditions agreement on whether the judge answered the question correctly and shows large gaps in Cohen's Kappa between 'judge-answered-correctly' and 'judge-answered-incorrectly' subsets (figures and Table 4). The authors state this causes poor signal on the most difficult questions within a benchmark and limits evaluating stronger candidate models with weaker judges.",
            "counterexamples_or_caveats": "Providing a correct human reference largely removes this dependence (judge agreement becomes much better even on questions the judge model could not itself answer). Pairwise tasks can mask difficulty because pairs often include one clearly correct and one incorrect answer, making grading easier.",
            "paper_reference": "Introduction; Results (conditioning agreement on judge answerability); Table 4; Figure 1",
            "uuid": "e9739.0",
            "source_info": {
                "paper_title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Reference sensitivity",
            "name_full": "Sensitivity of LLM-as-a-Judge to reference quality (None / Self / Human / Wrong / Random)",
            "brief_description": "The study systematically varies reference type in the judge prompt and shows that judge performance is highly sensitive to whether a correct human reference is provided; incorrect (``Wrong'') references can actively degrade performance more than no reference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Conversational QA (math and finance), single and pairwise correctness grading",
            "llm_judge_model": "Same judges: GPT-4o, Llama 3.3 70B, Yi 1.5 34B, Phi-4, Qwen 2.5 7B",
            "llm_judge_setup": "Judging prompts include one of: None (no reference), Self (judge's own generated reference), Human (human-written gold reference), Wrong (human-edited but incorrect), Random (reference from unrelated question); otherwise same prompt/instructions.",
            "human_evaluation_setup": "Human gold references created/verified by domain experts; human correctness labels obtained as described above for comparison.",
            "agreement_metric": "Cohen's Kappa reported across different reference types; specific reported effect: providing human references improves agreement for all judges; 'Wrong' references increase false negative rate substantially (text reports very high FNR with Wrong references).",
            "losses_identified": "Using incorrect or low-quality references can degrade judgment accuracy (e.g., causing judges to mark correct responses as incorrect), and using self-generated references only helps when the judge could already answer the question correctly — so without human-grounded correct references LLM judges can be misled or unhelpful.",
            "examples_of_loss": "Paper reports that 'Wrong' references produce low false positive rate but very high false negative rate (correct responses get marked incorrect). Example numeric contrasts and qualitative descriptions appear in Error Analysis and Tables (e.g., large differences when switching only the provided reference).",
            "counterexamples_or_caveats": "Verified LLM-generated references (i.e., synthetic references human-validated as correct) can approach the performance of human-written references; providing any correct reference (human or verified synthetic) substantially improves agreement, and in some configurations a weaker judge + high-quality reference outperforms a stronger judge with synthetic references.",
            "paper_reference": "Grading References section; Error Analysis; Results; Table 5/Table 6",
            "uuid": "e9739.1",
            "source_info": {
                "paper_title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-preference bias",
            "name_full": "Tendency for LLM Judges to favor their own responses (self-preference) when grading",
            "brief_description": "Models exhibit a bias to overrate their own outputs: higher false positive rates when grading responses generated by themselves versus responses from other models, especially when provided self-generated references or no reference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Pairwise and single correctness grading in conversational QA",
            "llm_judge_model": "Observed across evaluated judges (GPT-4o, Llama 3.3 70B, Yi 1.5 34B, Qwen 2.5 7B, Phi-4)",
            "llm_judge_setup": "Judges sometimes asked to evaluate their own responses (Self reference or judging own outputs) and comparisons computed between judgments of own vs others' responses; self-consistency used to reduce noise.",
            "human_evaluation_setup": "Human labels used as ground truth to compute false positive/negative rates per judge when grading own vs other models' responses.",
            "agreement_metric": "False Positive Rate (FPR) and False Negative Rate (FNR) aggregated and compared; Cohen's Kappa also reported but bias described via FPR/FNR gaps.",
            "losses_identified": "Using an LLM Judge risks introducing self-preference bias that inflates scores of models whose outputs resemble the judge's style or content, degrading objectivity relative to human evaluation.",
            "examples_of_loss": "Figure 4 and Figure 5 show that for every reference type on average models have a higher FPR when grading their own responses, with the gap particularly large for Self and None references; providing a human reference reduces both overall error and the self/other gap.",
            "counterexamples_or_caveats": "Human references reduce self-preference; verifying synthetic references instead of using the model's own references can mitigate this bias; occasional 'overruling' behavior (judge recognizes the reference is wrong and correctly marks the candidate) was observed (Appendix D), showing not all decisions are driven by blind self-preference.",
            "paper_reference": "Error Analysis; Figures 4 and 5; Conclusions",
            "uuid": "e9739.2",
            "source_info": {
                "paper_title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Masking by aggregates",
            "name_full": "Aggregation masking of judge weaknesses (aggregate agreement can hide subset failures)",
            "brief_description": "Aggregate-level agreement metrics can give an overly optimistic impression of judge quality while masking severe failures on the subset of hard questions that the judge cannot answer, reducing trustworthiness in difficult domains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Conversational QA and benchmark evaluation",
            "llm_judge_model": "All evaluated judges (not model-specific phenomenon)",
            "llm_judge_setup": "Evaluation reported at aggregate level and also conditioned on per-question judge answerability to reveal hidden failures.",
            "human_evaluation_setup": "Human consensus labels used to reveal divergences per-subset (answered-correctly vs answered-incorrectly by judge).",
            "agreement_metric": "Cohen's Kappa reported overall and conditioned; qualitative emphasis that masked poor performance occurs on difficult subsets.",
            "losses_identified": "When relying on aggregate agreement, researchers may miss that LLM Judges fail precisely on the hardest items — when correct assessment matters most — leading to overconfidence in automated evaluations.",
            "examples_of_loss": "Paper states 'This issue can be masked by aggregate statistics, especially when the judge performs well on the underlying benchmark itself' and shows conditioned analyses where agreement collapses on questions judge cannot answer.",
            "counterexamples_or_caveats": "Pairwise setups (with one correct and one incorrect response) can artificially raise agreement because differences are easier to detect; conditioning analyses reveal the masking and are recommended.",
            "paper_reference": "Introduction; Results (conditioning on judge correctness); Table 4; Figure 1",
            "uuid": "e9739.3",
            "source_info": {
                "paper_title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Style-over-substance bias",
            "name_full": "Influence of style, length, and tone on LLM judge decisions (style bias)",
            "brief_description": "LLM Judges tend to overweight superficial stylistic features (length, tone, phrasing) over substantive correctness or safety, leading to evaluations that diverge from human judgments focused on correctness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "General NLG evaluation; observed within conversational QA evaluations",
            "llm_judge_model": "Observed in literature and discussed/applied to judges in this study (multiple model families)",
            "llm_judge_setup": "Judging prompts may not fully control for stylistic influence; prior work and this paper note style-related failure modes.",
            "human_evaluation_setup": "Human graders used human-written gold references and judged correctness holistically, less swayed by superficial style when correctness determined by content.",
            "agreement_metric": "Prior work cited and the paper's Error Analysis reference biases qualitatively; the paper references prior findings (Feuer et al., 2024) and presents error profiles tied to reference types.",
            "losses_identified": "LLM judges may rate fluent or verbose but incorrect responses highly, and may miss safety or factual errors masked by good style, unlike careful human graders who weigh substance more.",
            "examples_of_loss": "The paper cites Feuer et al. (2024) concluding LLM Judges are overly influenced by writing style and do not adequately capture safety and correctness; the paper's own error analysis and observed FPR/FNR patterns are consistent with style-influenced misjudgment.",
            "counterexamples_or_caveats": "Prompt engineering, rubrics, randomization, pairwise ordering controls, and providing correct references can mitigate but not fully eliminate style biases; ensembles/juries may also help.",
            "paper_reference": "Related Work (Feuer et al., 2024; Koo et al., 2023); Error Analysis; Limitations",
            "uuid": "e9739.4",
            "source_info": {
                "paper_title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM vs simple baselines",
            "name_full": "LLM-as-a-Judge compared to simple reference-based similarity baselines",
            "brief_description": "The paper compares LLM judges to simple reference-similarity classifiers (ROUGE, BLEU, METEOR, and embedding cosine similarity) and finds LLM-as-a-Judge methods still outperform these baselines on pairwise grading when provided high-quality references.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Pairwise correctness grading with human gold references",
            "llm_judge_model": "Best-performing LLM judges (e.g., Llama 3.3 70B) vs embedding and n-gram baselines",
            "llm_judge_setup": "Pairwise tasks with human reference and LLM judge prompted to pick the more correct response; baselines pick response with higher similarity to reference.",
            "human_evaluation_setup": "Human gold labels used to compute Cohen's Kappa of baselines and LLM judges for comparison; same human references used for similarity measures.",
            "agreement_metric": "Cohen's Kappa reported for baselines (ROUGE ~0.26, METEOR ~0.19, BLEU ~0.02; embedding methods up to ~0.38) and for LLM judges (e.g., Llama 3.3 70B reported ~0.87 in Table 6 for pairwise with human reference).",
            "losses_identified": "While LLM judges outperform simple baselines overall, without human grounding (correct references) their advantages shrink and they are still vulnerable to the other losses above (answerability dependence, self-preference, reference sensitivity).",
            "examples_of_loss": "Table 6 shows baseline metric kappas (traditional metrics and embedding models) that lag the LLM judge performance when good references are present; the paper emphasizes reference quality is the primary driver of performance.",
            "counterexamples_or_caveats": "If high-quality references are unavailable, simple similarity baselines remain weak but also less likely to introduce self-preference; verifying LLM-generated references can be a practical compromise to obtain good agreement without full manual reference writing.",
            "paper_reference": "Reference-based baselines section; Table 6; Appendix C",
            "uuid": "e9739.5",
            "source_info": {
                "paper_title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges",
            "rating": 2,
            "sanitized_title": "judging_the_judges_evaluating_alignment_and_vulnerabilities_in_llmsasjudges"
        },
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Style outweighs substance: Failure modes of llm judges in alignment benchmarking",
            "rating": 2,
            "sanitized_title": "style_outweighs_substance_failure_modes_of_llm_judges_in_alignment_benchmarking"
        },
        {
            "paper_title": "Benchmarking cognitive biases in large language models as evaluators",
            "rating": 2,
            "sanitized_title": "benchmarking_cognitive_biases_in_large_language_models_as_evaluators"
        },
        {
            "paper_title": "Refenceguided verdict: Llms-as-judges in automatic evaluation of free-form text",
            "rating": 1,
            "sanitized_title": "refenceguided_verdict_llmsasjudges_in_automatic_evaluation_of_freeform_text"
        },
        {
            "paper_title": "Is reference necessary in the evaluation of NLG systems? when and where?",
            "rating": 1,
            "sanitized_title": "is_reference_necessary_in_the_evaluation_of_nlg_systems_when_and_where"
        }
    ],
    "cost": 0.012870749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding
7 Mar 2025</p>
<p>Michael Krumdick michael.krumdick@kensho.com 
Charles Lovering 
Varshini Reddy 
Seth Ebner 
Chris Tanner 
Kensho Technologies 
Massachusetts </p>
<p>Institute of Technology</p>
<p>CambridgeMA</p>
<p>No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding
7 Mar 20252FCD6EB51BA88C677C9545B68B84FD4BarXiv:2503.05061v1[cs.CL]
LLM-as-a-Judge is a framework that uses an LLM (large language model) to evaluate the quality of natural language text-typically text that is also generated by an LLM.This framework holds great promise due to its relative lowcost, ease of use, and strong correlations with human stylistic preferences.However, LLM Judges have been shown to exhibit biases that can distort their judgments.We evaluate how well LLM Judges can grade whether a given response to a conversational question is correct, an ability crucial to soundly estimating the overall response quality.To do so, we create and publicly release a human-annotated dataset with labels of correctness for 1,200 LLM responses.We source questions from a combination of existing datasets and a novel, challenging benchmark (BFF-Bench) created for this analysis 1 .We demonstrate a strong connection between an LLM's ability to correctly answer a question and grade responses to that question.Although aggregate level statistics might imply a judge has high agreement with human annotators, it will struggle on the subset of questions it could not answer.To address this issue, we recommend a simple solution: provide the judge with a correct, human-written reference answer.We perform an in-depth analysis on how reference quality can affect the performance of an LLM Judge.We show that providing a weaker judge (e.g.Qwen 2.5 7B) with higher quality references reaches better agreement with human annotators than a stronger judge (e.g.GPT-4o) with synthetic references.</p>
<p>Introduction</p>
<p>Using LLM-as-a-Judge for evaluation has become increasing prominent due to the relatively low cost and high correlation with human preference (Li et al., 2024a).Benchmarks like MT-Bench (Zheng et al., 2023), AlpacaEval2.0(Dubois et al., 2024), 1 All data is available at: http://bit.ly/4hi0hsK Figure 1: LLM Judge agreement with human annotators for the two strongest judges on the pairwise correctness judgment task."None" means having no reference, "Self" means having a reference generated by the judge model, and "Human" means having a human-written gold reference.</p>
<p>and ArenaHardAuto (Li et al., 2024b) are commonly used to evaluate the conversational capabilities of frontier language models.</p>
<p>The LLM-as-a-Judge framework is typically used in the following way: a model that we want to evaluate (referred to as the candidate model) generates responses to a static set of benchmark questions.Then, to evaluate these responses, a separate LLM (referred to as the judge model or LLM Judge) is used to generate a quality score.The candidate model's responses can be graded as-is, in isolation (i.e., single grading) or in comparison to an additional candidate model's responses (i.e., pairwise grading).</p>
<p>For an LLM Judge to work effectively, one typically provides it with a prompt that includes basic instructions regarding the judgment task and de-Question: The vertices of a triangle are at points (0, 0), (-1, 1), and (3, 3).What is the area of the triangle?</p>
<p>MT-Bench Reference</p>
<p>To find the area of the triangle, we can use the formula for the area of a triangle with coordinates A(x1, y1), B(x2, y2), and C(x3, y3): Area = (1/2) * |(x1(y2 -y3) + x2(y3 -y1) + x3(y1 -y2))| In this case, the coordinates are A(0, 0), B(-1, 1), and C (3,3).Plugging these values into the formula, we get: Area = (1/2) * |(0(1 -3) + (-1)(3 -0) + 3(0 -1))| Area = (1/2) * |(-0 + 3 -3)| Area = (1/2) * |0| Area = 0 The area of the triangle is 0. This means that the three points are collinear, and there is no triangle formed by these points.</p>
<p>Our Reference To find the area of the triangle, we can use the formula for the area of a triangle with coordinates A(x1, y1), B(x2, y2), and C(x3, y3): Area = (1/2) * |(x1(y2 -y3) + x2(y3 -y1) + x3(y1 -y2))| In this case, the coordinates are A(0, 0), B(-1, 1), and C (3,3).Plugging these values into the formula, we get: Area = (1/2) * |(0(1 -3) + (-1)(3 -0) + 3(0 -1))| Area = (1/2) * |(-0 + -3 -3)| Area = (1/2) * |-6| Area = 3 The area of the triangle is 3. sired output format.The prompt may include a reference answer, a sample response to the benchmark question being graded.Our analysis focuses on the impact of these reference answers on LLM Judge performance.Zheng et al. (2023) noted that LLM Judges struggle to evaluate math, reasoning, and coding questions.The authors demonstrated that providing LLM Judges with prompts that include synthetic references (i.e., unfiltered LLM-generated sample responses) improves the judges' overall agreement with human annotators compared to using no reference at all.However, Inflection AI (2024) noted that many of the reference answers used in MT-Bench were actually incorrect.This leads to the obvious question: if a judge model cannot generate the correct response and it is not provided with a correct reference, how can it accurately assess the correctness of other responses?As an analogy, if a student cannot answer an exam question, how can they be trusted to grade the exam without an answer key?</p>
<p>To investigate this question, we collect a new dataset aimed at evaluating automated grading of response correctness.We start with existing benchmark data from MT-Bench and manually correct each of the incorrect references to create a gold reference set.We additionally create the Business and Finance Fundamentals Benchmark (BFF-Bench), an entirely new conversational benchmark.BFF-Bench contains 160 math and reasoning questions within business and finance -domains that are particularly sensitive to correctness (Krumdick et al., 2024).For each of these questions, we also include gold references written by human experts.</p>
<p>We demonstrate that the ability of an LLM Judge to answer a particular question as a candidate significantly impacts the degree to which the LLM Judge is able to produce judgments of correctness that align with our human annotators.This issue can be masked by aggregate statistics, especially when the judge performs well on the underlying benchmark itself.</p>
<p>Figure 1 shows this behavior in two leading models.The measured agreement with human annotators is heavily dependent on whether the judge model can correctly answer the question itself, unless it is provided a gold human-written reference.Providing an LLM Judge with reference answers generated by the same LLM increases the overall agreement only when the LLM is already able to generate the correct answer.</p>
<p>This dependence poses a serious risk with using LLM-as-a-Judge: LLM Judges provide a poor signal of quality on the most difficult questions within a benchmark.This is a significant limitation to applying LLM-as-a-Judge to more difficult domains.It also limits the ability of a weaker LLM Judge to grade a stronger candidate model.The weaker judge will give an increasingly poor signal as the gap in performance between the two models increases.</p>
<p>Our main contributions are as follows:</p>
<ol>
<li>
<p>We create a new business-and financethemed conversational benchmark with 80 two turn conversations (160 total questions) and gold reference answers written by human annotators.</p>
</li>
<li>
<p>We release a dataset of human judgments of correctness for 1,200 LLM responses to evaluate methods for both pairwise and single grading.</p>
</li>
<li>
<p>We demonstrate that LLM Judges struggle to accurately grade models' responses to a question when the LLM Judge itself cannot correctly answer the same question.This suggests that the LLM-as-a-Judge framework is limited in its ability to evaluate difficult domains and models that are more capable than the LLM Judge.</p>
</li>
<li>
<p>We show that simply providing humanwritten reference answers improves judge models' overall agreement with human annotators and reduces self-preference bias.</p>
</li>
</ol>
<p>Related Work</p>
<p>Reference-free Evaluations in NLP.Referencefree evaluations attempt to estimate the quality of generated text without any kind of gold reference.This means that using LLM-as-a-Judge without human annotated data is also a form of reference-free evaluation.There is a long history of development, analysis and critique of reference-free evaluations for various text generation tasks in NLP (Ito et al., 2025).For example, Sheng et al. (2024) demonstrated that on some tasks reference-free metrics can have higher agreement with human annotators than reference-based metrics in certain situations.Deutsch et al. (2022) studied the limitations of reference-free evaluations in machine translation and summarization by directly comparing against reference-based baselines.They demonstrated that reference-free evaluations have inherent biases that limit their usefulness, strongly favoring outputs from the underlying generative model used in the evaluation.</p>
<p>LLM-as-a-Judge and Correctness.Thakur et al. (2025) studied whether LLMs can judge correctness on 400 short-answer questions from Triv-iaQA (Joshi et al., 2017).Similarly, Badshah and Sajjad (2024) studied agreement with human measures of correctness on 300 short-answer questions from TriviaQA, HotPotQA (Yang et al., 2018) and TruthfulQA (Lin et al., 2022).Both studies found that some judge models were able to get very high overall agreement with human annotators.Zeng et al. (2024) created a similar judgment benchmark dataset but focused on evaluating instruction-following rather than correctness.They too found that the most capable judges were able to get a very high agreement with their human annotators.They evaluated the impact of using selfgenerated references and found it overall beneficial for evaluating instruction-following.Tan et al. (2025) constructed a judgment benchmark specifically for measuring correctness.They focus only on the pairwise case, and generate all of their data synthetically using questions whose answers can be verified via a regular expression.In general, they found that models performed very poorly on their benchmark, even when provided with a self-generated reference.</p>
<p>Limitations of LLM-as-a-Judge.It has been well demonstrated that LLM Judges have biases that can impact their judging abilities.The length, order, and tone of a response have all been shown to strongly influence LLM Judges' evaluation of the quality (Zheng et al., 2023;Koo et al., 2023;Wei et al., 2024).</p>
<p>Multiple benchmarks have been proposed that highlight different shortcomings of LLM-as-a-Judge.CoBBLEr (Koo et al., 2023) creates a benchmark that evaluates judges across six unique biases.CALM (Ye et al., 2025) creates a framework for evaluating judge biases and quantify 12 biases including societal biases.Feuer et al. (2024) demonstrates failure modes of LLM judges by trying to isolate the different factors that the LLM Judge might be implicitly weighing.They conclude that LLM Judges are overly influenced by the writing style of models' responses, and do not adequately capture other factors such as safety and correctness.</p>
<p>Various approaches have been proposed in order to minimize these biases and improve agreement with human annotators, including randomizing the ordering during pairwise grading (Zheng et al., 2023), explicit length de-biasing (Dubois et al., 2024), using an ensemble of judge models as a jury (Verga et al., 2024), creating rubrics (Liu et al., 2023), performing prompt optimization (Zhou et al., 2024), and using a pairwise-preference search method (Liu et al., 2024).Zhang et al. (2024) proposed improving performance by dynamically generating a reference conditioned on the response to be graded.</p>
<p>Datasets</p>
<p>We aim to use as little synthetic data as possible in constructing our core benchmark.We rely on human annotation to label response correctness, generate new and challenging questions, and to write gold reference solutions.</p>
<p>(C)MT-Bench</p>
<p>We selected the 20 conversations from MT-Bench within the math and reasoning categories.We selected this subset since they are evaluated with references by default within the MT-Bench pipeline and have answers that are either correct or incorrect.This is in contrast to the more open-ended categories with a wide range of acceptable answers (e.g."Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.").</p>
<p>We manually analyzed the provided GPT-4generated reference answers for these conversations.Of the 40 questions, we found that 15 (35%) were either entirely incorrect or inconsistent.</p>
<p>We created correct references for each of these questions by editing the existing references by hand.We included not only the correct answer but a coherent chain-of-thought context with all of the necessary sub-steps.An example of one of these incorrect and corrected references can be found in Figure 2. We refer to this corrected subset of the MT-Bench Dataset as (C)MT-Bench.</p>
<p>BFF-Bench</p>
<p>We collected 80 two-turn conversational questions with golden reference answers, all written entirely by hand.Questions are designed to be difficult, unambiguous, and a blend of business and financial domain knowledge and quantitative reasoning.A sample question can be seen in Figure 3.</p>
<p>The questions and reference answers were collected by working with domain experts in business and finance.Each question went through an iterative quality control process to ensure that the questions and responses were adequate for grading.Each response was verified at least twice to ensure correctness.Additionally, we adjusted the formatting of the reference answers to ensure high readability and removed any company-or personspecific references.</p>
<p>Each question was designed to have an unambiguous answer to make judging whether or not a model was correct well-defined.Many of the questions require some amount of mathematical manipulation, ranging from simple addition to evaluating complex financial formulas.However, some require background domain knowledge alone.Because questions were designed to be difficult and to require both mathematical and financial knowledge, each question took approximately an hour to create.</p>
<p>Correctness Dataset</p>
<p>We collected responses on (C)MT-Bench and BFF-Bench from a diverse set of six models across various model families and sizes: Gemma 2 2b (Team et al., 2024), Qwen 2.5 7b (Yang et al., 2024), Phi-4 (Abdin et al., 2024), Yi 1.5 34B (AI et al., 2025), Llama 3.3 70B (Grattafiori et al., 2024), and GPT-4o (OpenAI et al., 2024).In total, this process created 1,200 question-answer pairs.</p>
<p>We collected correctness labels for these pairs from an in-house team of analysts.Each analyst had either an undergraduate or masters degree in a financial discipline.For each question, we provided the human-written reference solution.The annotators were free to use any external resources, except for LLM-based chatbots.Each questionresponse pair was annotated by three annotators.The annotators were tasked with evaluating the correctness of the entire model response.They were instructed to grade each response holistically: if any part was wrong, the entire response was to be marked as wrong.The full annotator instructions can be found in Appendix E. The breakdown of the total number of questions, responses and annotations can be found in Table 1  We instructed annotators to select between "Incorrect", "Correct" and "Not Sure".54% of the questions received unanimously consistent annotations (e.g., all three annotations were "Incorrect" or all were "Correct").87% of the questions had at most 1 human annotation that differed from the other 2 human annotations (e.g., 2 annotations Question (Turn One): What is the IFRS 16 treatment of short term or low value leased assets?What are the restrictions?Gold Response: IFRS 16 Leases permit a simplified treatment for assets with a lease period of 12 months or less, or . . .Question (Turn Two): On April 1st 2023, Abigail acquired telephones for her salesforce under a two-year lease agreement.The terms of the lease require an initial payment of $3,000, followed by a payment of $9,000 on March 31st 2024 and an additional $9,000 on March 31st 2025.Show the impact of this lease arrangement on Abigail's financial statements for the year ended 31 December 2023 under IFRS 16?</p>
<p>Gold Response: Annual lease rental expense = Total rentals payable/Total lease period = $3,000 + $9,000 + . . .The expense in this period of $7,875 is not the same as the payment of $3,000 so we need to accrue an additional expense of $4,875.Figure 3: Truncated example of a question from BFF-Bench.For each question, we include a human-written gold answer that contains the final answer and a complete chain-of-thought reasoning.The untruncated version of this example is available in Figure 6 in the appendix.might be "Incorrect" and 1 as "Not Sure").Only 13% of the questions had either no consensus at all, or a consensus of "Not Sure.".These 13% of questions were deemed ambiguous and were removed from our analysis.Table 3 shows the human ratings of correctness for our six models on the two tasks.</p>
<p>We created two judgment datasets from these labels: a single grading set and a pairwise grading set.For the single grading task, the judge model is given a question and response and must predict the human-consensus label of "Correct" or "Incorrect."For the pairwise task, the judge model is given a question with two separate responses and must select the one that is more correct.To construct the pairwise dataset, we iterate over every question and create pairs of correct and incorrect responses, ensuring that every pair contains exactly one correct and one incorrect answer.</p>
<p>The final dataset composition used for our analysis is shown in Table 2 4 Experimental Setup</p>
<p>Judge Models</p>
<p>We evaluate the same set of six models as both candidate models answering questions and judge models grading responses.The only exception was the Gemma 2 2B model, which was only used as a candidate. 2The judge model is allowed to use Chain-Of-Thought (Wei et al., 2022) to first generate a set of reasoning steps before generating a final answer.To reduce the overall noise in this analysis, we compute our judgments using self-consistency (Wang et al., 2023), taking the majority vote over five separate inferences generated with a temperature of 0.7.</p>
<p>Prompts</p>
<p>We evaluate two different judgment formats: "single grading" and "pairwise grading".For the single grading case, the model is given a prompt that instructs it to grade only the correctness of the model response.We include the full interaction history for the multi-turn questions within the judgment prompt.</p>
<p>The pairwise evaluation is similar except the model is asked to determine which of the two provided models' responses is "more correct."Following (Zheng et al., 2023), we run each pair twice, reversing the ordering of the references so as to account for any positional bias the judge model may have (e.g., always favoring the first model's response).If the judge model's selection differs between the two runs, we count it as a tie.</p>
<p>We use a modified version of the MT-Bench prompt for both the single and the pairwise case and with and without references.All eight prompts used can be found in Appendix A.</p>
<p>Grading References</p>
<p>Our main analysis concerns the impact of the provided reference on the judge's performance.Specif-Table 4: Comparing agreement with the human annotators across questions in which the judge model was labeled to answer correctly or incorrectly as a candidate.Cells in red represent cases in which the model is worse when it was wrong as a candidate (α &lt; 0.05).Numbers in bold indicate the maximum values on that subset with statistical significance (α &lt; 0.05).Providing the gold human written references provides mild overall gains on questions which the model was able to answer, but significant gains on the questions it could not.</p>
<p>ically, we look at three main types of provided references:</p>
<p>• None: no reference is provided within the LLM Judge's prompt</p>
<p>• Self: the LLM Judge provides its own reference answers</p>
<p>• Human: a human provides manually created reference answers</p>
<p>In the "None" case, the model is given a prompt that does not include a reference, and it is asked to directly evaluate the provided response.For all other cases, we use the same prompt while only varying the provided reference.</p>
<p>We additionally create two types of baseline references."Wrong" refers to human-edited versions of the "Human" references that try to maintain the overall style but are incorrect.Each has some amount of inaccurate information or incorrect reasoning that changes the final answer to the question.Any correct response would then be incorrect with respect to these references.However, it would not impact the grading of responses that were already incorrect unless they happened to make the same error.</p>
<p>"Random" are randomly selected references from other questions.These references are noninformative without being actively misleading.</p>
<p>Cohen's Kappa</p>
<p>To evaluate the agreement between our LLM Judges and human annotations, we use the Cohen's Kappa (Cohen, 1960) metric.This is an agreement measure that accounts for the chance of random agreement.It is defined as follows,
κ = 1 − 1 − p o 1 − p e ,(1)
where p o is the observed probability of agreement and p e is the expected agreement, if there was no correlation.This metric will always be between -1 and 1, with -1 corresponding to no agreement, 0 corresponding to random agreement and 1 corresponding to total agreement.We calculate 95% confidence intervals (CI) on this estimate via bootstrapping.Details on the CI calculations can be found in Appendix B.</p>
<p>Results</p>
<p>Our main results are summarized in Table 4.Our judges exhibit relatively high agreements with our human annotators in both evaluation setups (i.e., single and pairwise).Overall, providing a human reference improves the agreement with our human annotators for all evaluated judges across both the single and pairwise cases.Notably, providing a human reference to a relatively small model, such as Qwen 2.5 7B, can yield better judgments than using a larger judge model, such as GPT-4o, on the same set of responses.This improvement is likely due to the fact that putting the correct answer in context reduces the overall complexity of the judging task.Rather than having to determine the correct response and then compare this with the given candidate response, it can focus solely on the comparison between the reference and the given answer.</p>
<p>The results between our single and pairwise grading setups do not directly reflect whether one setup is better or not.The pairwise dataset was created to always have one correct and one incorrect response from two different candidate models.The difference in response quality between two candidate models can be quite large, making the judge model's grading relatively straightforward in this setup.Thus, as expected, we observed higher agreement with the human annotators in the pairwise grading setup compared to the single grading setup.</p>
<p>For each question in the evaluation, we also have a label indicating whether the judge model was able to answer that question correctly as a candidate model.When we condition the judge's agreement on this label, a clear trend emerges.On questions the judge had answered correctly, providing a human reference generally does not significantly improve performance in either the single or pairwise task.</p>
<p>However, when the judge model was unable to correctly answer a given question, there are very large differences in the amount of agreement when using different reference types.This is especially the case for higher-performing judges.For example, when GPT-4o uses its own responses to questions as a reference (i.e., "Self" response), the Cohen's Kappa decreases from 0.78 to 0.14 in the pairwise case.</p>
<p>Error Analysis</p>
<p>Different types of references-wrong, humangenerated, or self-generated-each lead to different profiles of judge behavior.One common judgment bias is self-preference, where a model tends to overrate its own responses.To quantify this bias we computed the false positive rate (FPR) for each judge when evaluating its own responses versus all other models.In this context, the FPR represents the rate at which the model erroneously labels incorrect responses as correct.We also evaluated the false negative rate (FNR), which is the rate at which the model erroneously labels correct responses as incorrect.A model with a strong selfpreference bias would exhibit a high FPR and a low FNR when grading its own references.Figure 4 displays the FPR and FNR aggregated over each judge with the "Wrong", "Random", "Self", "None", and "Human" reference types (Metrics per judge can be found in Figure 5).For every reference type, we see that on average models have a higher FPR when grading their own responses.The gap is particularly large when the model is provided with its own generated reference or no reference at all.Thus, providing a human reference reduces both the overall rate of error and the relative difference between a model's judgment of its own responses and those of others.</p>
<p>When the model is given a "Wrong" reference, the FPR is low and the FNR is very high, which suggests that the incorrect information leads judges to erroneously classify correct responses as incorrect.However, this effect is not observed with the "Random" reference.This evidence supports the idea that judges are sensitive to the correctness of the reference, and providing a slightly incorrect reference is worse than providing an unrelated reference or no reference at all.</p>
<p>Even when provided with a "Wrong" reference, Table 5: Cohen's Kappa comparison between different references, evaluated over the subset of total questions that GPT-4o gets correct.There is no longer a statistically significant difference in performance in between the human-written gold references and the verified GPT-4o (✓) references, suggesting that verifying the correctness of the LLM generated references can be enough.</p>
<p>the LLM Judges have a non-zero FPR.This means that they are still grading responses as correct.We analyzed these judgments to understand why this was happening.Although most were simply errors, we observed cases of a judge model identifying the reference as being incorrect and judging the response correctly.An example of this "overruling" behavior is provided in Appendix D.</p>
<p>Reference correctness matters more than being human-written.To better understand what makes the human-written references lead to higher alignment with our human annotators, we compared the performance across: human-written gold references; a set of GPT-4o references judged by humans to be correct; a set of "Wrong" references; and, a set of "Random" references.Table 6 presents the results for these different reference types.The disparity we observed in the previous section between Human and the synthetic references diminishes.At the same time, there is a significant gap between the "Human" references and the "Wrong" references.This suggests that the most important part of the human-written references is their correctness, rather than any other superficial attributes.</p>
<p>LLM-as-a-Judge outperforms simple referencebased baselines.These results indicate that providing a model with a high quality reference is the key to judgment performance.This potentially calls into question the added value of the LLM-asa-Judge framework itself.We evaluated a series of reference-based baselines on our pairwise grading task.These used both traditional metrics and embedding models to select the response that has the highest similarity with the reference.More information on how these baselines were computed can</p>
<p>Cohen's Kappa Random 0.00 ±0.08 Traditional Metrics ROUGE (Lin, 2004) 0.26 ±0.08 METEOR (Banerjee and Lavie, 2005) 0.19 ±0.08 BLEU (Papineni et al., 2002) 0.02 ±0.08Text Embedding OpenAI (Large) (OpenAI, 2024) 0.38 ±0.08 Mistral Embed Linq (Choi et al., 2024) 0.35 ±0.08 Stella 1.5B (Zhang et al., 2025) 0.24 ±0.08 LLM-as-a-Judge Llama 3.3 70b 0.87 ±0.04 Yi 1.5 34B 0.50 ±0.07</p>
<p>Table 6: Reference-based baselines for the pairwise grading setup.We include the performance of the best and worst LLM Judges provided with a human reference.</p>
<p>be found in Appendix C. Overall, we find that these baselines still lag behind even the worst LLM-as-Judge method on this task.</p>
<p>Conclusion</p>
<p>LLM-as-a-Judge performance is sensitive to the provided reference.We observe large variations in performance when changing only the provided reference in the judgment prompt.Providing accurate references increases alignment with the human annotators, while providing wrong references can actively hurt.This increase is most distinct for questions the judge model struggles to answer correctly.</p>
<p>Improving agreement on such questions is key to advancing our evaluations of frontier models.Therefore, we strongly recommend verifying your references.The results in Table 6 imply that a judge model is liable to work well as long as the model can either answer the question itself or is provided a correct reference.This means that verifying responses generated from a model rather than manually writing and checking answers via annotators is possible.Although this still requires some human annotation, our results seem to demonstrate that this is unavoidable.However, it is important to note that although using verified references may improve the overall agreement metrics, it may come at the cost of increased bias.Specifically, using a model's own references exacerbates the self-preference bias compared to providing no reference at all.</p>
<p>Limitations</p>
<p>Our work assumes that collecting accurate, highquality human annotations is feasible, which is not always the case.Our analysis consisted a large part of data from a two domains, business and finance.It is possible that other domains may have different trends.Human annotations are expensive and time consuming to collect, which limited our analysis and sample sizes.</p>
<p>We do not believe this work poses any significant risks.We hope that increasing the quality of evaluation metrics can help reduce the risk of developing and deploying LLMs.Building frameworks for stronger evaluations can aid in developing potentially harmful technologies, but they are also crucial to creating safer, more useful systems.tive as possible.After providing your explanation, output your final verdict by strictly following this format: "[[A]]" if assistant A is more correct, and " [[B]]" if assistant B is more correct.Prompt Template: Please act as an impartial judge and evaluate the correctness of the response provided by an AI assistant to the user question.You will only be given the assistant's answer.You should only focus on the assitant's answer to the second question.Identify any mistakes.Be as objective as possible.After providing your analysis, you must rule whether the response is correct or incorrect.Output your final verdict by strictly following this format: " [[Correct]]" if the answer is correct and " [[Incorrect]]" is the answer is incorrect.Prompt Template:</p>
<p>B Computing Confidence Interval</p>
<p>We used a blockwise boostrap in order to compute our confidence intervals.A model judging the same question N does not constitute N independent samples.To account for this, we bootstrap by sampling at the unique question level rather than at the level of the individual response.In practice we find that this does not lead to a significant difference than bootstrapping over each response individually.</p>
<p>Although the intervals that we compute are nonsymmetric, we convert them into symmetric intervals for a more compact display in tables.To do so, we select which ever bound leads to the widest possible interval.</p>
<p>C Reference-Based Similarity Classifiers</p>
<p>We implement a series of simple reference based classifiers.For each of these classifiers, we compute a similarity score between the human-written reference and each of the two possible responses.We then select the response with the highest similarity with the reference as our selected "Correct" label.</p>
<p>We used the evaluate library (0.4.3) (Hugging-Face, 2025) to compute ROUGE, BLEU and ME-TEOR scores.We used the default settings for each metric.For the embedding metrics, we compute the cosine similarity between the embeddings for the human-written reference and the two responses.</p>
<p>D Judge Refusal</p>
<p>The following example is GPT-4o grading a response from Phi-4 using a wrong reference which inverts the definition of annuities and IRAs.The text generated by GPT-4o notes the inconsistencies between the reference and the response, but overrules the reference and still marks the response as correct.We observed this behavior multiple times, only with GPT-4o.</p>
<p>Judgment:</p>
<p>The assistant's answer provides a breakdown of the differences between annuities and IRAs, similar to the reference answer.Let's compare the key elements:</p>
<ol>
<li>Annuity: -The assistant correctly identifies annuities as financial products designed to provide a steady income stream, and they can be immediate or deferred.It mentions different types of annuities (fixed, variable, indexed) and notes the tax-deferred nature of annuities and the penalties for early withdrawal.</li>
</ol>
<p>-The reference answer describes annuities as investment accounts for holding financial assets, emphasizing accumulating contributions and interest until retirement age, and mentions custodial fees and mutual funds' expense ratios.-There is a discrepancy in the definition: the reference answer incorrectly describes annuities as accounts for holding investments, which aligns more with the description of IRAs.</p>
<ol>
<li>
<p>IRA: -The assistant correctly describes IRAs as tax-advantaged savings accounts with different types (Traditional and Roth) and their unique tax implications.It covers tax treatment, access to funds, and ownership.-The reference answer incorrectly labels IRAs as insurance products, which is inaccurate.It also incorrectly suggests IRAs do not have contribution limits, which is a mistake.</p>
</li>
<li>
<p>Key Differences: -The assistant highlights investment control, flexibility, tax treatment, and income stream as key differences, which are generally accurate and align with the correct understanding of annuities and IRAs.</p>
</li>
</ol>
<p>Overall, the reference answer contains significant inaccuracies regarding the fundamental nature of annuities and IRAs.In contrast, the assistant's answer provides a more accurate description and comparison of these financial products.</p>
<p>[</p>
<p>[Correct]]</p>
<p>E Annotator Guidelines</p>
<p>The model will attempt to answer the provided question (first text box above the other two boxes).A model will generally attempt to fool the judge with confident wrong answers in cases where it does not know the correct answer.Annotators will utilize the Reference (bottom left text box), which contains the answer to the posed question (top text box).Model Answers should include the relevant facts/formulas needed to answer the question, with a step-by-step explanation of any mathematical manipulations.</p>
<p>It is important to remember that everything should be graded only with respect to the reference answer.If you find any errors within the reference, please mark the error in the notes!However, you should always enter the grade as if the reference is correct.</p>
<p>A general set of guidelines is:</p>
<p>1.If the model's final answer or recommendation disagrees with the reference, it should be marked "Incorrect".</p>
<ol>
<li>
<p>If the model agrees with the reference but its intermediate steps disagree, it should also be marked "Incorrect".</p>
</li>
<li>
<p>If the model agrees with the reference, all of the intermediate steps agree, and it provides the additional information that is not obviously wrong based on the reference, it should be marked "Correct"</p>
</li>
<li>
<p>If the model agrees with the reference, all of the intermediate steps agree, but the answer is slightly off due to rounding or performing approximate calculations, it should be marked as "Correct"</p>
</li>
<li>
<p>If the model agrees with the reference, all of the intermediate steps agree, but it is lacking some amount of information given within the reference, you should make a judgment call as to whether the missing information is crucial to the answer.If you are unsure, lean towards "Correct" or mark it "Unsure"</p>
</li>
</ol>
<p>Question (Turn One): What is the IFRS 16 treatment of short term or low value leased assets?What are the restrictions?</p>
<p>Gold Response: IFRS 16 Leases permit a simplified treatment for assets with a lease period of 12 months or less, or of low value.Although the standard does not give a numerical definition of 'low value' it does give examples of the types of assets that may be included, and this includes telephones.The simplified treatment allows the lease payments to be charged as an expense over the lease period, applying the accruals concept.IFRS 16 Leases does not specify a particular monetary amount below which an asset would be considered 'low value', although the basis for conclusion indicates a value of $5,000 as a guide.The standard also gives the following examples of low-value assets:</p>
<p>-tablets -small personal computers -telephones -small items of furniture.</p>
<p>The assessment of whether an asset qualifies as having a 'low value' must be made based on its value when new.Therefore, a car would not qualify as a low-value asset, even if it was very old at the commencement of the lease.</p>
<p>Question (Turn Two): On April 1st 2023, Abigail acquired telephones for her salesforce under a two-year lease agreement.The terms of the lease require an initial payment of $3,000, followed by a payment of $9,000 on March 31st 2024 and an additional $9,000 on March 31st 2025.Show the impact of this lease arrangement on Abigail's financial statements for the year ended 31 December 2023 under IFRS 16?</p>
<p>Gold Response:Annual lease rental expense = Total rentals payable/Total lease period = $3,000 + $9,000 + $9,000 / 2 years = $10,500 per annum Expense to 31 December 2023 = $10,500 × 9/12 = $7,875</p>
<p>The expense in this period of $7,875 is not the same as the payment of $3,000 so we need to accrue an additional expense of $4,875.</p>
<p>Figure 6: The untruncated example from BFF-Bench.</p>
<p>Figure 2 :
2
Figure 2: Example of an incorrect reference included in MT-Bench along with our corrected reference.Error and correction are highlighted in bold.</p>
<p>Figure 4 :
4
Figure 4: Error analysis for the judge models conditioned on the grading references.Self refers to the cases in which the judge model was judging responses from itself.Others refers to when it was judging responses from other models.</p>
<p>Assistant A's Conversation with User|&gt;</p>
<p>Figure 5 :
5
Figure 5: The full graph displaying the error rates for different LLM Judges and different reference types.The same trend displayed in Figure 4 holds for each model.Rand.refers to the Random reference type.</p>
<p>Table 1 :
1
Source Dataset Composition</p>
<p>.
DatasetSingle Pairwise(C)MT-Bench228162BFF-Bench808442Total1036604</p>
<p>Table 2 :
2
Final Benchmark Composition.Numbers represent the total number of labeled question-response pairs included from each dataset.</p>
<p>Table 3 :
3
. Human annotator correctness rating of each model as a candidate.
Model(C)MT-Bench (%) BFF-Bench (%)GPT-4o75.068.12Llama 3.3 70b85.046.25Phi 477.551.88Qwen 2.5 7b67.546.25Yi 1.5 34b65.036.25Gemma 2 2b50.035.0</p>
<p>±0.06 0.74 ±0.06 0.57 ±0.08 GPT-4o (✓) 0.61 ±0.07 0.66 ±0.07 0.52 ±0.09Random 0.50 ±0.08 0.47 ±0.08 0.09 ±0.1 None 0.46 ±0.08 0.56 ±0.08 0.22 ±0.09Wrong 0.21 ±0.06 0.22 ±0.06 0.24 ±0.09
ReferenceOverallRightWrongHuman0.69
We found the Gemma 2 2B model to inconsistently follow the required judgment output format.
A Judgment PromptsA.1 Single, Reference, Turn One System Prompt: You are a helpful assistant.Prompt Template:[Instruction] Please act as an impartial judge and evaluate the correctness of the response provided by an AI assistant to the user question.You will be given a reference correct answer and the assistant's answer.The reference answer is always correct.Begin you evaluation by comparing the assistant's answer to the correct answer.Identify any mistakes.Be as objective as possible.After providing your explanation, you must rule whether the response is correct or incorrect.Output your final verdict by strictly following this format: "[[Correct]]" if the answer is correct and "[[Incorrect]]" is the answer is incorrect.[Question] {question}
Pairwise Grading Overall Judge Answered Correctly Judge Answered Incorrectly None Self Human None Self Human None Self Human GPT. Qwen, 4o 0.47 ±0.07 0.64 ±0.06 0.85 ±0.04 0.51 ±0.07 0.78 ±0.06 0.86 ±0.05 0.28 ±0.15 0.14 ±0.17 0.80 ±0.10</p>
<p>. Qwen, 7B 0.17 ±0.06 0.24 ±0.07 0.56 ±0.07 0.12 ±0.10 0.45 ±0.12 0.53 ±0.12 0.16 ±0.06 0.09 ±0.08 0.52 ±0.08</p>
<p>References Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, James R Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, C T Caio, Anh Mendes, Eric Nguyen, Price, arXiv:2412.08905Olli Saarikivi, and 8 others. 2024. Phi-4 technical report. Gustavo de RosaPreprint</p>
<p>Shiming Yang, and 14 others. 2025. Yi: Open foundation models by 01. : Ai, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, arXiv:2403.04652Preprint</p>
<p>Referenceguided verdict: Llms-as-judges in automatic evaluation of free-form text. Sher Badshah, Hassan Sajjad, arXiv:2408.092352024Preprint</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, Michigan2005Association for Computational Linguistics</p>
<p>Chanyeol Choi, Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, Minkyung Cho, Jy Yong Sohn, arXiv:2412.03223Linq-embed-mistral technical report. 2024Preprint</p>
<p>A coefficient of agreement for nominal scales. Educational and Psychological Measurement. Jacob Cohen, 196020</p>
<p>On the limitations of reference-free evaluations of generated text. Daniel Deutsch, Rotem Dror, Dan Roth, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Length-controlled alpacaeval: A simple way to debias automatic evaluators. Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, arXiv:2404.044752024Preprint</p>
<p>Style outweighs substance: Failure modes of llm judges in alignment benchmarking. Benjamin Feuer, Micah Goldblum, Teresa Datta, Sanjana Nambiar, Raz Besaleli, Samuel Dooley, Max Cembalest, John P Dickerson, 2024</p>
<p>Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, arXiv:2407.217832024PreprintThe llama 3 herd of models</p>
<p>Evaluate: A library for evaluating machine learning models and datasets. Huggingface, 2025</p>
<p>Inflection-2.5: meet the world's best personal ai. Inflection AI. 2024</p>
<p>Reference-free evaluation metrics for text generation: A survey. Takumi Ito, Kees Van Deemter, Jun Suzuki, arXiv:2501.120112025Preprint</p>
<p>TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer, 10.18653/v1/P17-1147Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae , Myung Kim, Dongyeop Kang, arXiv:2309.17012Benchmarking cognitive biases in large language models as evaluators. 2023arXiv preprint</p>
<p>BizBench: A quantitative reasoning benchmark for business and finance. Michael Krumdick, Rik Koncel-Kedziorski, Dac Viet, Varshini Lai, Charles Reddy, Chris Lovering, Tanner, 10.18653/v1/2024.acl-long.452Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>From generation to judgment: Opportunities and challenges of llm-as-a-judge. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, Huan Liu, ArXiv, abs/2411.165942024a</p>
<p>From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gonzalez, Ion Stoica, ArXiv, abs/2406.119392024b</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>TruthfulQA: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, 10.18653/v1/2022.acl-long.229Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<dl>
<dt>Aligning with human judgement: The role of pairwise preference in large language model evaluators. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier, First Conference on Language Modeling. 2024</dt>
<dd>
<p>Openai, Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, Alex Aleksander M Ądry, Alex Baker-Whitcomb, Alex Beutel, Borzunov, arXiv:2410.21276Gpt-4o system card. Preprint. Alex Carney, Alex Chow, Alex Kirillov2024401</p>
</dd>
</dl>
<p>New embedding models and api updates. 2024OpenAI</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Is reference necessary in the evaluation of NLG systems? when and where?. Shuqian Sheng, Yi Xu, Luoyi Fu, Jiaxin Ding, Lei Zhou, Xinbing Wang, Chenghu Zhou, 10.18653/v1/2024.naacl-long.474Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics20241</p>
<p>Judgebench: A benchmark for evaluating LLM-based judges. Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Yuan Tang, Alejandro Cuadron, Chenguang Wang, Raluca Popa, Ion Stoica, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, arXiv:2408.00118Preprintand 179 others. 2024. Gemma 2: Improving open language models at a practical size</p>
<p>Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes, arXiv:2406.126242025Preprint</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis, arXiv:2404.187962024Preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates. Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, Mei Han, ArXiv, abs/2408.130062024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2022</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, arXiv:2412.15115Junyang Lin, Kai Dang, and 22 others. 2024. Qwen2.5 technical report. arXiv preprint</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, Conference on Empirical Methods in Natural Language Processing (EMNLP). 2018</p>
<p>Justice or prejudice? quantifying biases in LLM-as-a-judge. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh V Chawla, Xiangliang Zhang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Evaluating large language models at evaluating instruction following. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen, International Conference on Learning Representations (ICLR). 2024</p>
<p>Jasper and stella: distillation of sota embedding models. Dun Zhang, Jiacheng Li, Ziyang Zeng, Fulong Wang, arXiv:2412.190482025Preprint</p>
<p>Reviseval: Improving llmas-a-judge via response-adapted references. Qiyuan Zhang, Yufei Wang, Tiezheng Yu, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, Chen Ma, ArXiv, abs/2410.051932024</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>Fairer preferences elicit improved human-aligned large language model judgments. Han Zhou, Xingchen Wan, Yinhong Liu, Nigel Collier, Ivan Vulić, Anna Korhonen, arXiv:2406.113702024Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
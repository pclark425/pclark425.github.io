<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3866 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3866</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3866</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-d91bd7bdea31775302a8a0b997b6d67bf20ac297</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d91bd7bdea31775302a8a0b997b6d67bf20ac297" target="_blank">Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?</a></p>
                <p><strong>Paper Venue:</strong> Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</p>
                <p><strong>Paper TL;DR:</strong> The research finds that ChatGPT has the potential to generate effective Boolean queries and is a tool of potential value for researchers conducting systematic reviews, particularly for rapid reviews where time is a constraint and where one can trade off higher precision for lower recall.</p>
                <p><strong>Paper Abstract:</strong> Systematic reviews are comprehensive literature reviews for a highly focused research question. These reviews are considered the highest form of evidence in medicine. Complex Boolean queries are developed as part of the systematic review creation process to retrieve literature, as they permit reproducibility and understandability. However, it is difficult and time-consuming to develop high-quality Boolean queries, often requiring the expertise of expert searchers like librarians. Recent advances in transformer-based generative models have shown their ability to effectively follow user instructions and generate answers based on these instructions. In this paper, we investigate ChatGPT as a means for automatically formulating and refining complex Boolean queries for systematic review literature search. Overall, our research finds that ChatGPT has the potential to generate effective Boolean queries. The ability of ChatGPT to follow complex instructions and generate highly precise queries makes it a tool of potential value for researchers conducting systematic reviews, particularly for rapid reviews where time is a constraint and where one can trade off higher precision for lower recall. We also identify several caveats in using ChatGPT for this task, highlighting that this technology needs further validation before it is suitable for widespread uptake.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3866.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3866.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A black-box transformer-based generative language model used in this study to automatically formulate and refine PubMed Boolean queries for systematic review literature search via carefully engineered prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ChatGPT-based Boolean query generator</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Uses the ChatGPT service as a black-box LLM to generate or refine Boolean search queries for PubMed. Interactions are driven by engineered natural-language prompts (single-shot, few-shot examples, and multi-step guided prompts) to produce executable PubMed Boolean queries, optionally including MeSH terms and PICO structure.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>ChatGPT model internals and training corpus are undisclosed (paper treats it as black-box); in the experiments the inputs given to ChatGPT were: systematic review titles, optional seed study text, and optional example queries (from CLEF TAR/Seed collection). Generated queries were executed against PubMed (via Entrez API) and evaluated against the CLEF TAR and Seed Collection evaluation judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language prompts containing the systematic review title; variants include: simple instruction prompts, detailed prompts invoking an 'information specialist' persona, prompts containing an example review and example query (HQE or related), prompts requesting explicit PICO extraction, or multi-step guided prompts that supply seed-study text and ask for term extraction and categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt engineering (single-prompt and multi-step guided prompting). For guided prompts, a stepwise pipeline is used: (1) extract ~50 relevant terms from seed text, (2) classify terms into categories (health condition, treatment, study design, N/A), (3) combine items by OR per category and AND across categories to form a Boolean query, (4) ask the model to refine the query and add MeSH terms. No retrieval-augmented generation or external document retrieval into the LLM during generation is reported beyond providing seed-study text in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Executable PubMed Boolean queries (strings) including field tags (e.g., [Title/Abstract], [MeSH], OR/AND clauses), intended for direct submission to PubMed's Entrez API.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Generated queries were executed on PubMed via Entrez API to retrieve PMIDs. Retrieval results were compared to gold-standard abstract-level relevance assessments from two test collections (CLEF TAR 2017/2018 and the Seed Collection). Metrics reported: precision, recall, F1, F3; statistical significance tested with paired t-tests and Bonferroni correction. Variability across multiple runs (non-determinism) was measured by re-running prompts 10 times.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChatGPT-generated queries achieved substantially higher precision than automated baseline query formulation methods, but at the cost of much lower recall. Including example queries in prompts (especially high-quality examples) improved overall effectiveness; guided multi-step prompting following the objective method improved both precision and recall versus single prompts. Query refinement via ChatGPT improved precision and F-measures relative to seed queries (with limited recall loss in best settings). Generated queries showed high run-to-run variability. Many generated MeSH terms were incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Model treated as black-box: authors could not verify training data overlap (possible leakage). High non-deterministic variability in generated queries across runs. Generated MeSH terms frequently not in MeSH vocabulary (55–66% incorrect in experiments). Generated queries tended to trade recall for precision, making them less appropriate for high-recall systematic reviews. Some generated queries had many OR clauses yielding huge retrievals and poor measured effectiveness; relevance judgments biased towards original queries may hide true performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared to automatic formulation baselines (Original, Conceptual, Objective methods), ChatGPT produced higher precision but lower recall; for CLEF collection ChatGPT queries had higher F-measure than baselines, but performed worse on the Seed Collection. Guided-prompt ChatGPT queries outperformed single-prompt ChatGPT runs. Refinement of objective-method queries by ChatGPT yielded large precision gains with modest recall drops, outperforming unrefined seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3866.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3866.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-Prompt Formulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-Prompt Query Formulation (q1–q5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of prompt-engineering strategies used in this paper where a single prompt (varying in detail and inclusion of examples) is submitted to ChatGPT to formulate a systematic-review Boolean query from the review title.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Single-prompt ChatGPT formulation (q1–q5)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Five single-prompt designs: simple (brief instruction), detailed (persona + background), and with example (providing an example review title and its query); one variant (q5) instructs the model to extract PICO elements before building the query. Prompts replace placeholders with the target review title (and example title/query where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Prompt input: the systematic review title for each topic; optionally a provided example review title and example Boolean query. Evaluation used CLEF TAR and Seed Collection topics; generated queries executed on PubMed.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language prompt containing the review title and instructions; some variants include an example query (HQE or related example identified via a BERT-based relatedness selection).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Single-shot prompt engineering (few-shot approach when including an example). No retrieval augmentation beyond embedding an example or title in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>A single PubMed-formatted Boolean query string per prompt submission.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Execution on PubMed (Entrez API) and evaluation against collection judgments producing precision, recall, F1, F3; statistical tests versus baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Single-prompt strategies produced queries with higher precision than automatic baselines but lower recall overall. Including example queries notably improved F1/F3 and recall in CLEF; semantically related example queries raised precision further; PICO-extraction prompts increased precision but reduced recall. Outputs were variable across repeated runs (recall variance larger than precision).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Non-deterministic outputs; choice and type of example strongly affect results; poorer MeSH handling; single-prompt outputs tended to have lower recall, limiting use for high-recall systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Single-prompt ChatGPT queries outperformed some automated methods on precision and F-measures in CLEF but underperformed on the Seed Collection; providing examples (HQE or related) often improved performance relative to simple prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3866.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3866.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-Prompt Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-Prompt Query Refinement (q6–q7)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt designs that provide ChatGPT with an existing Boolean 'seed' query and instruct it to correct/refine the query to retrieve fewer irrelevant documents and more relevant documents for a given review title.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ChatGPT-based single-prompt refinement</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Two refinement prompts: a simple refine instruction (q6) and a version with an example showing an initial and refined query (q7). The seed query can be the original human query or algorithmically generated (objective/conceptual).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Inputs to ChatGPT: the review title and the seed Boolean query (and optionally an example seed→refined pair). Evaluation was performed by executing the refined queries on PubMed and comparing to CLEF TAR relevance judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Structured prompt consisting of review_title and initial_query (strings); optionally example_review_initial_query and example_review_refined_query.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt engineering for refinement: LLM asked to modify query structure/clauses and suggest MeSH terms; no external retrieval augmentation beyond seed query text.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>A refined PubMed-executable Boolean query string.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Execution on PubMed and set-based metrics (precision, recall, F1, F3) versus seed queries and baselines (statistical testing reported). Variability assessed over 10 repeated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Refinement with ChatGPT increased precision and F-measures across seed types, though recall generally decreased. Greatest improvements seen when refining objective-method auto-generated seed queries: large precision gains with only ~11% drop in recall (best-case). Variability across runs was lower than for from-scratch formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Refinements sometimes introduced incorrect MeSH terms; outputs remain non-deterministic. Refinement performance depends on seed query quality—ChatGPT is best used to sharpen high-recall seeds rather than to create high-recall queries from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Refined queries (particularly q7-objective) outperformed their seed objective queries in precision and F-measures while maintaining relatively high recall compared to other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3866.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3866.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Guided Prompt Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Objective-method-guided Multi-step Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-step prompting pipeline that mimics the objective automatic query formulation method: term extraction from a seed study, term categorization (A/B/C/N/A), clause assembly (OR within categories, AND across), and final refinement (including MeSH suggestions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Guided multi-step ChatGPT prompting (objective-method emulation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Divides query formulation into four explicit prompts/steps: (1) ask ChatGPT to list ~50 relevant terms/phrases from seed-study text; (2) ask it to classify each term into categories (health condition, treatment, study-design, N/A); (3) instruct ChatGPT to assemble a Boolean query by OR-ing terms within categories and AND-ing across categories into a PubMed-executable query; (4) request refinement and addition of MeSH terms. This explicitly embeds the objective method workflow into LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Requires at least one seed study text per topic (the Seed Collection provided such seed studies). The pipeline feeds seed-study text and the review title into successive prompts. Generated queries executed on PubMed; evaluation used Seed Collection judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Seed-study text and explicit stepwise instructions provided in natural language prompts; final query aims at the review title/topic.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Structured chain-of-prompting (manual chain-of-thought style prompting across multiple interactions) to break the complex task into sub-tasks (term mining, categorization, assembly, refinement). Not a formal chain-of-thought internal to the model but an externally enforced multi-step workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>PubMed-executable Boolean queries assembled from categorized term lists and refined with suggested MeSH terms.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Execution via PubMed Entrez and comparison against Seed Collection relevance judgments; measured precision, recall, F1, F3 and run-to-run variability (10 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Guided prompts produced substantially better queries than single-prompt 'q4' on the Seed Collection: both precision and recall improved (e.g., precision increased ~249.6%, recall increased ~301.6% relative to q4 in reported table). However, outcomes were highly dependent on choice of seed study and showed high variability across seed choices and repeated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Effectiveness sensitive to seed-study selection; high run-to-run variability even with same seed; MeSH term errors persisted. Non-determinism and instability across seeds remain concerns for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Outperformed the single-prompt strategies (q4) on the Seed Collection, and in many cases improved both precision and recall compared to automated baselines, highlighting the benefit of mimicking the objective formulation pipeline via multi-step prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3866.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3866.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific generative pre-trained transformer for biomedical text generation and mining, cited in the paper as an example of LLMs applied in the biomedical domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>BioGPT (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Referenced in related work as a biomedical-domain generative transformer for text generation and mining; the paper cites it to situate ChatGPT among domain-specific biomedical LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not described within this paper; the citation implies biomedical corpora usage but no experimental details are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not described in this paper (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not described in this paper; only cited as an example of a biomedical LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No results reported in this paper; BioGPT is cited as related work only.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3866.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3866.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Query Expansion (Claveau 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Text Generation for Query Expansion in Information Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited study that applies neural text generation for query expansion in IR; mentioned as one of the few prior works using generative models for query expansion, but not for Boolean queries for systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural Text Generation for Query Expansion in Information Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Neural text-generation query-expansion (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Referenced as prior work that used neural text generation for query expansion; the current paper notes this as one of the very few examples where generative models were used in IR contexts related to query expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not described in this paper; only the citation is provided without experimental details.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not described here; the cited work likely uses neural text generation for producing expansion terms, but details are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No results described here; cited as related evidence that generative models have been applied to query expansion tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining <em>(Rating: 2)</em></li>
                <li>Neural Text Generation for Query Expansion in Information Retrieval <em>(Rating: 2)</em></li>
                <li>Language Models Are Few-Shot Learners <em>(Rating: 1)</em></li>
                <li>Can Large Language Models Reason about Medical Questions? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3866",
    "paper_id": "paper-d91bd7bdea31775302a8a0b997b6d67bf20ac297",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI)",
            "brief_description": "A black-box transformer-based generative language model used in this study to automatically formulate and refine PubMed Boolean queries for systematic review literature search via carefully engineered prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_or_method_name": "ChatGPT-based Boolean query generator",
            "system_or_method_description": "Uses the ChatGPT service as a black-box LLM to generate or refine Boolean search queries for PubMed. Interactions are driven by engineered natural-language prompts (single-shot, few-shot examples, and multi-step guided prompts) to produce executable PubMed Boolean queries, optionally including MeSH terms and PICO structure.",
            "input_corpus_description": "ChatGPT model internals and training corpus are undisclosed (paper treats it as black-box); in the experiments the inputs given to ChatGPT were: systematic review titles, optional seed study text, and optional example queries (from CLEF TAR/Seed collection). Generated queries were executed against PubMed (via Entrez API) and evaluated against the CLEF TAR and Seed Collection evaluation judgments.",
            "topic_or_query_specification": "Natural-language prompts containing the systematic review title; variants include: simple instruction prompts, detailed prompts invoking an 'information specialist' persona, prompts containing an example review and example query (HQE or related), prompts requesting explicit PICO extraction, or multi-step guided prompts that supply seed-study text and ask for term extraction and categorization.",
            "distillation_method": "Prompt engineering (single-prompt and multi-step guided prompting). For guided prompts, a stepwise pipeline is used: (1) extract ~50 relevant terms from seed text, (2) classify terms into categories (health condition, treatment, study design, N/A), (3) combine items by OR per category and AND across categories to form a Boolean query, (4) ask the model to refine the query and add MeSH terms. No retrieval-augmented generation or external document retrieval into the LLM during generation is reported beyond providing seed-study text in prompts.",
            "output_type_and_format": "Executable PubMed Boolean queries (strings) including field tags (e.g., [Title/Abstract], [MeSH], OR/AND clauses), intended for direct submission to PubMed's Entrez API.",
            "evaluation_or_validation_method": "Generated queries were executed on PubMed via Entrez API to retrieve PMIDs. Retrieval results were compared to gold-standard abstract-level relevance assessments from two test collections (CLEF TAR 2017/2018 and the Seed Collection). Metrics reported: precision, recall, F1, F3; statistical significance tested with paired t-tests and Bonferroni correction. Variability across multiple runs (non-determinism) was measured by re-running prompts 10 times.",
            "results_summary": "ChatGPT-generated queries achieved substantially higher precision than automated baseline query formulation methods, but at the cost of much lower recall. Including example queries in prompts (especially high-quality examples) improved overall effectiveness; guided multi-step prompting following the objective method improved both precision and recall versus single prompts. Query refinement via ChatGPT improved precision and F-measures relative to seed queries (with limited recall loss in best settings). Generated queries showed high run-to-run variability. Many generated MeSH terms were incorrect.",
            "limitations_or_challenges": "Model treated as black-box: authors could not verify training data overlap (possible leakage). High non-deterministic variability in generated queries across runs. Generated MeSH terms frequently not in MeSH vocabulary (55–66% incorrect in experiments). Generated queries tended to trade recall for precision, making them less appropriate for high-recall systematic reviews. Some generated queries had many OR clauses yielding huge retrievals and poor measured effectiveness; relevance judgments biased towards original queries may hide true performance.",
            "comparison_to_baselines_or_humans": "Compared to automatic formulation baselines (Original, Conceptual, Objective methods), ChatGPT produced higher precision but lower recall; for CLEF collection ChatGPT queries had higher F-measure than baselines, but performed worse on the Seed Collection. Guided-prompt ChatGPT queries outperformed single-prompt ChatGPT runs. Refinement of objective-method queries by ChatGPT yielded large precision gains with modest recall drops, outperforming unrefined seeds.",
            "uuid": "e3866.0",
            "source_info": {
                "paper_title": "Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Single-Prompt Formulation",
            "name_full": "Single-Prompt Query Formulation (q1–q5)",
            "brief_description": "A family of prompt-engineering strategies used in this paper where a single prompt (varying in detail and inclusion of examples) is submitted to ChatGPT to formulate a systematic-review Boolean query from the review title.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Single-prompt ChatGPT formulation (q1–q5)",
            "system_or_method_description": "Five single-prompt designs: simple (brief instruction), detailed (persona + background), and with example (providing an example review title and its query); one variant (q5) instructs the model to extract PICO elements before building the query. Prompts replace placeholders with the target review title (and example title/query where applicable).",
            "input_corpus_description": "Prompt input: the systematic review title for each topic; optionally a provided example review title and example Boolean query. Evaluation used CLEF TAR and Seed Collection topics; generated queries executed on PubMed.",
            "topic_or_query_specification": "Natural-language prompt containing the review title and instructions; some variants include an example query (HQE or related example identified via a BERT-based relatedness selection).",
            "distillation_method": "Single-shot prompt engineering (few-shot approach when including an example). No retrieval augmentation beyond embedding an example or title in the prompt.",
            "output_type_and_format": "A single PubMed-formatted Boolean query string per prompt submission.",
            "evaluation_or_validation_method": "Execution on PubMed (Entrez API) and evaluation against collection judgments producing precision, recall, F1, F3; statistical tests versus baselines.",
            "results_summary": "Single-prompt strategies produced queries with higher precision than automatic baselines but lower recall overall. Including example queries notably improved F1/F3 and recall in CLEF; semantically related example queries raised precision further; PICO-extraction prompts increased precision but reduced recall. Outputs were variable across repeated runs (recall variance larger than precision).",
            "limitations_or_challenges": "Non-deterministic outputs; choice and type of example strongly affect results; poorer MeSH handling; single-prompt outputs tended to have lower recall, limiting use for high-recall systematic reviews.",
            "comparison_to_baselines_or_humans": "Single-prompt ChatGPT queries outperformed some automated methods on precision and F-measures in CLEF but underperformed on the Seed Collection; providing examples (HQE or related) often improved performance relative to simple prompts.",
            "uuid": "e3866.1",
            "source_info": {
                "paper_title": "Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Single-Prompt Refinement",
            "name_full": "Single-Prompt Query Refinement (q6–q7)",
            "brief_description": "Prompt designs that provide ChatGPT with an existing Boolean 'seed' query and instruct it to correct/refine the query to retrieve fewer irrelevant documents and more relevant documents for a given review title.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "ChatGPT-based single-prompt refinement",
            "system_or_method_description": "Two refinement prompts: a simple refine instruction (q6) and a version with an example showing an initial and refined query (q7). The seed query can be the original human query or algorithmically generated (objective/conceptual).",
            "input_corpus_description": "Inputs to ChatGPT: the review title and the seed Boolean query (and optionally an example seed→refined pair). Evaluation was performed by executing the refined queries on PubMed and comparing to CLEF TAR relevance judgments.",
            "topic_or_query_specification": "Structured prompt consisting of review_title and initial_query (strings); optionally example_review_initial_query and example_review_refined_query.",
            "distillation_method": "Prompt engineering for refinement: LLM asked to modify query structure/clauses and suggest MeSH terms; no external retrieval augmentation beyond seed query text.",
            "output_type_and_format": "A refined PubMed-executable Boolean query string.",
            "evaluation_or_validation_method": "Execution on PubMed and set-based metrics (precision, recall, F1, F3) versus seed queries and baselines (statistical testing reported). Variability assessed over 10 repeated runs.",
            "results_summary": "Refinement with ChatGPT increased precision and F-measures across seed types, though recall generally decreased. Greatest improvements seen when refining objective-method auto-generated seed queries: large precision gains with only ~11% drop in recall (best-case). Variability across runs was lower than for from-scratch formulation.",
            "limitations_or_challenges": "Refinements sometimes introduced incorrect MeSH terms; outputs remain non-deterministic. Refinement performance depends on seed query quality—ChatGPT is best used to sharpen high-recall seeds rather than to create high-recall queries from scratch.",
            "comparison_to_baselines_or_humans": "Refined queries (particularly q7-objective) outperformed their seed objective queries in precision and F-measures while maintaining relatively high recall compared to other methods.",
            "uuid": "e3866.2",
            "source_info": {
                "paper_title": "Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Guided Prompt Pipeline",
            "name_full": "Objective-method-guided Multi-step Prompting",
            "brief_description": "A multi-step prompting pipeline that mimics the objective automatic query formulation method: term extraction from a seed study, term categorization (A/B/C/N/A), clause assembly (OR within categories, AND across), and final refinement (including MeSH suggestions).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Guided multi-step ChatGPT prompting (objective-method emulation)",
            "system_or_method_description": "Divides query formulation into four explicit prompts/steps: (1) ask ChatGPT to list ~50 relevant terms/phrases from seed-study text; (2) ask it to classify each term into categories (health condition, treatment, study-design, N/A); (3) instruct ChatGPT to assemble a Boolean query by OR-ing terms within categories and AND-ing across categories into a PubMed-executable query; (4) request refinement and addition of MeSH terms. This explicitly embeds the objective method workflow into LLM prompts.",
            "input_corpus_description": "Requires at least one seed study text per topic (the Seed Collection provided such seed studies). The pipeline feeds seed-study text and the review title into successive prompts. Generated queries executed on PubMed; evaluation used Seed Collection judgments.",
            "topic_or_query_specification": "Seed-study text and explicit stepwise instructions provided in natural language prompts; final query aims at the review title/topic.",
            "distillation_method": "Structured chain-of-prompting (manual chain-of-thought style prompting across multiple interactions) to break the complex task into sub-tasks (term mining, categorization, assembly, refinement). Not a formal chain-of-thought internal to the model but an externally enforced multi-step workflow.",
            "output_type_and_format": "PubMed-executable Boolean queries assembled from categorized term lists and refined with suggested MeSH terms.",
            "evaluation_or_validation_method": "Execution via PubMed Entrez and comparison against Seed Collection relevance judgments; measured precision, recall, F1, F3 and run-to-run variability (10 runs).",
            "results_summary": "Guided prompts produced substantially better queries than single-prompt 'q4' on the Seed Collection: both precision and recall improved (e.g., precision increased ~249.6%, recall increased ~301.6% relative to q4 in reported table). However, outcomes were highly dependent on choice of seed study and showed high variability across seed choices and repeated runs.",
            "limitations_or_challenges": "Effectiveness sensitive to seed-study selection; high run-to-run variability even with same seed; MeSH term errors persisted. Non-determinism and instability across seeds remain concerns for reproducibility.",
            "comparison_to_baselines_or_humans": "Outperformed the single-prompt strategies (q4) on the Seed Collection, and in many cases improved both precision and recall compared to automated baselines, highlighting the benefit of mimicking the objective formulation pipeline via multi-step prompting.",
            "uuid": "e3866.3",
            "source_info": {
                "paper_title": "Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "BioGPT",
            "name_full": "BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining",
            "brief_description": "A domain-specific generative pre-trained transformer for biomedical text generation and mining, cited in the paper as an example of LLMs applied in the biomedical domain.",
            "citation_title": "BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining",
            "mention_or_use": "mention",
            "system_or_method_name": "BioGPT (referenced)",
            "system_or_method_description": "Referenced in related work as a biomedical-domain generative transformer for text generation and mining; the paper cites it to situate ChatGPT among domain-specific biomedical LLMs.",
            "input_corpus_description": "Not described within this paper; the citation implies biomedical corpora usage but no experimental details are provided here.",
            "topic_or_query_specification": "Not described in this paper (reference only).",
            "distillation_method": "Not described in this paper; only cited as an example of a biomedical LLM.",
            "output_type_and_format": "Not described in this paper.",
            "evaluation_or_validation_method": "Not described in this paper.",
            "results_summary": "No results reported in this paper; BioGPT is cited as related work only.",
            "limitations_or_challenges": "Not discussed in this paper.",
            "comparison_to_baselines_or_humans": "Not discussed in this paper.",
            "uuid": "e3866.4",
            "source_info": {
                "paper_title": "Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Neural Query Expansion (Claveau 2021)",
            "name_full": "Neural Text Generation for Query Expansion in Information Retrieval",
            "brief_description": "A cited study that applies neural text generation for query expansion in IR; mentioned as one of the few prior works using generative models for query expansion, but not for Boolean queries for systematic reviews.",
            "citation_title": "Neural Text Generation for Query Expansion in Information Retrieval",
            "mention_or_use": "mention",
            "system_or_method_name": "Neural text-generation query-expansion (referenced)",
            "system_or_method_description": "Referenced as prior work that used neural text generation for query expansion; the current paper notes this as one of the very few examples where generative models were used in IR contexts related to query expansion.",
            "input_corpus_description": "Not described in this paper; only the citation is provided without experimental details.",
            "topic_or_query_specification": "Not described in this paper.",
            "distillation_method": "Not described here; the cited work likely uses neural text generation for producing expansion terms, but details are not provided in this paper.",
            "output_type_and_format": "Not described in this paper.",
            "evaluation_or_validation_method": "Not described in this paper.",
            "results_summary": "No results described here; cited as related evidence that generative models have been applied to query expansion tasks.",
            "limitations_or_challenges": "Not discussed in this paper.",
            "comparison_to_baselines_or_humans": "Not discussed in this paper.",
            "uuid": "e3866.5",
            "source_info": {
                "paper_title": "Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining",
            "rating": 2
        },
        {
            "paper_title": "Neural Text Generation for Query Expansion in Information Retrieval",
            "rating": 2
        },
        {
            "paper_title": "Language Models Are Few-Shot Learners",
            "rating": 1
        },
        {
            "paper_title": "Can Large Language Models Reason about Medical Questions?",
            "rating": 1
        }
    ],
    "cost": 0.01635575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?</h1>
<p>SHUAI WANG, The University of Queensland, Australia<br>HARRISEN SCELLS, Leipzig University, Germany<br>GUIDO ZUCCON, The University of Queensland, Australia<br>BEVAN KOOPMAN, CSIRO, Australia</p>
<p>Systematic reviews are comprehensive reviews of the literature for a highly focused research question. These reviews are often treated as the highest form of evidence in evidence-based medicine, and are the key strategy to answer research questions in the medical field. To create a high-quality systematic review, complex Boolean queries are often constructed to retrieve studies for the review topic. However, it often takes a long time for systematic review researchers to construct a high quality systematic review Boolean query, and often the resulting queries are far from effective. Poor queries may lead to biased or invalid reviews, because they missed to retrieve key evidence, or to extensive increase in review costs, because they retrieved too many irrelevant studies. Recent advances in Transformer-based generative models have shown great potential to effectively follow instructions from users and generate answers based on the instructions being made. In this paper, we investigate the effectiveness of the latest of such models, ChatGPT, in generating effective Boolean queries for systematic review literature search. Through a number of extensive experiments on standard test collections for the task, we find that ChatGPT is capable of generating queries that lead to high search precision, although trading-off this for recall. Overall, our study demonstrates the potential of ChatGPT in generating effective Boolean queries for systematic review literature search. The ability of ChatGPT to follow complex instructions and generate queries with high precision makes it a valuable tool for researchers conducting systematic reviews, particularly for rapid reviews where time is a constraint and often trading-off higher precision for lower recall is acceptable.</p>
<h2>ACM Reference Format:</h2>
<p>Shuai Wang, Harrisen Scells, Guido Zuccon, and Bevan Koopman. 2023. Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?. 1, 1 (February 2023), 19 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn</p>
<h2>1 INTRODUCTION</h2>
<p>To construct a high-quality systematic review, all evidence related to a research topic is examined by the researchers of the review; those relevant to the research topic will be selected and further evaluated and synthesised. To gather all evidence related to the review, Boolean queries are authored to search medical databases. Boolean queries provide reproducibility, explainability, and the benefit of filtering out articles not relevant to the research topic [39], therefore reducing the workload of unnecessary document assessments. However, constructing a high-quality Boolean query is challenging, even for experienced searchers, and the resulting queries are often sub-optimal [59].</p>
<p>Authors' addresses: Shuai Wang, The University of Queensland, Brisbane, Australia, shuai.wang2@uq.edu.au; Harrisen Scells, Leipzig University, Leipzig, Germany, harry.scells@uni-leipzig.de; Guido Zuccon, The University of Queensland, Brisbane, Australia, g.zuccon@uq. edu.au; Bevan Koopman, CSIRO, Brisbane, Australia, bevan.koopman@csiro.au.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To help researchers construct better systematic review Boolean queries, automatic Boolean query formulation and refinement methods have been developed [6, 49, 59, 62-66, 77, 78, 80, 80]. With formulation, we refer to the task of creating a Boolean query from scratch, at times relying on one or more "seed" documents; i.e., example studies related to the systematic review. With refinement, we refer to the task of improving an existing Boolean query; e.g., by adding or removing Boolean clauses and operators to yield a higher precision (less irrelevant documents retrieved) while maintaining recall. Although results have been promising and methods have been integrated into tools for assisting in the query creation process [34, 58, 60, 78], automated techniques are yet far from being able to yield high-quality Boolean queries.</p>
<p>Recent advances in text generation models [21, 35] have led to great successes in task-based question-answering and provide high-quality responses based on user intentions [35]. One of such generative models, ChatGPT ${ }^{1}$, is considered the most expressive text-generation model at the time of writing, and achieved state-of-theart effectiveness across natural language processing tasks. This paper investigates how to use ChatGPT to construct high-quality systematic review Boolean queries effectively. The following research questions guide our investigation into using ChatGPT:</p>
<p>RQ1: How does ChatGPT compare with current state-of-the-art methods for formulating and refining systematic review Boolean queries?
RQ2: To what extent do the prompts used to generate systematic review Boolean queries impact the effectiveness of the Boolean queries produced by ChatGPT?
RQ3: What is the effect of guiding the query formulation process with ChatGPT through multiple prompts that mimic the process of the current state-of-the-art automated Boolean query generation methods?
RQ4: What are the caveats and potential challenges of using ChatGPT to create systematic review Boolean queries?</p>
<p>To our knowledge, this is the first such attempt to comprehensively evaluate the effectiveness of using ChatGPT for creating Boolean queries. This includes carefully engineering prompts for ChatGPT and thoroughly evaluating using standardised test collections for systematic reviews.</p>
<p>We find that ChatGPT compares favourably with current state-of-the-art query generation methods. Improvements in precision sometimes come at the expense of a recall, although we show that this may be mitigated with better MeSH term handling and the practice of snowballing (locating more studies via citation networks). We show generating a good prompt is paramount. Providing a sample Boolean query as part of the prompt is beneficial. Multiple prompting interactions with ChatGPT are better than a single interaction. While results are promising, they are also volatile in that effectiveness varies across runs and prompts; MeSH suggestion, in particular, is poor with ChatGPT.</p>
<p>The findings of this research can help systematic review researchers use ChatGPT for systematic review Boolean query construction and refinement, and understand its limitations and caveats.</p>
<h1>2 RELATED WORK</h1>
<h3>2.1 Systematic Review Automation</h3>
<p>Constructing systematic reviews requires manual effort by trained professionals, across several distinct phases, with different types of manual effort required at each phase.</p>
<p>Phase 1 is to define a specific research question; software can help in defining a good querstion [1].
Phase 2 is to author Boolean queries based on the research question. The query defines which (and how many) results are returned, so their quality greatly impacts all later phases. This is why we focus specifically on query</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>generation. Automated methods have been developed for query formulation [13, 34, 55], query refinement [34, 60] and query exploitation $[7,17]$.</p>
<p>Phase 3 is to screen all the titles and abstracts retrieved by the query, manually assessing them for relevance. The primary automation approach to help here is screen prioritisation [3-5, 10, 16, 18, 19, 26, 31, 33, 42, 43, 46, 47, $61,68,69,81,82]$, which involves ranking the studies according to their likely relevance to the research question. Once ranked, automated approaches can define a 'stopping criteria', after which screening does not continue because all relevant documents are likely to be found. Active learning approaches [15, 44] are often employed during screening to help rank include studies to be screened.</p>
<p>Phase 4 involves extracting specific details from studies relevant to the review; data extraction methods can help here $[27,71]$.</p>
<p>Phase 5 is the synthesis of all the evidence into a single coherent review document. Synthesis automation [12, $50,54,72,73]$ have been developed here; they include text thematic analysis [75] and even text generation [51].</p>
<h1>2.2 Automatic Query Formulation and Refinement</h1>
<p>Query formulation is the process of deriving a Boolean query, based on the research question, according to a specific set of guidelines. The guideline presents two accepted procedures that exist for developing a query for systematic reviews. These procedures describe the steps one should take when developing a query. The first procedure is called the conceptual method [11]. This procedure requires first identifying several high-level concepts from pilot searches or potentially relevant studies known a priori. These high-level concepts are then used to discover synonyms and related keywords. The query is then iteratively refined, a process driven by the expertise of the information specialist. The second procedure is called the objective method [24, 67]. The first step of this procedure is to create a small set of potentially relevant studies that will seed the rest of the procedure, e.g., through pilot searchers, as in the conceptual method. Multiple statistical procedures follow, which extract terms from these studies and weakly validate the query. The identified terms must still be added to the query manually, which, like the conceptual method, is driven by the expertise of the information specialist. The exact details of these methods are unimportant for understanding their use in this paper; they should be understood simply as a series of steps one can follow to arrive at a query.</p>
<p>Naturally, these methods require a considerable amount of time (as the information specialist must perform multiple pilot searches and spend time validating the search) [31, 52] and are prone to human error [56, 57]. To this end, Scells et al. [63] investigated automating these two query formulation procedures. The main finding of this line of research was that computationalising these procedures could not match the effectiveness of humans; however, further manual refinement of the automatically generated queries dramatically improved retrieval effectiveness.</p>
<p>Automatic Query Refinement was developed based on the observed benefit from manual query refinement. Such methods take an initial human authors query and apply a series of transformations (adding terms or clauses) to make the query more effective [34, 60]. In combination with query visualisation [60] tools, these query refinement tools were able to improve the initial query.</p>
<p>Learnings from the query formulation approach drive two clear directions for using ChatGPT to automate query formulation: the first is to allow ChatGPT to generate queries however it sees fit, and the second is to guide ChatGPT by prompting it to follow the instructions of the conceptual or objective procedures. We refer to the first method as unguided and the second method as guided. Learnings from the query refinement work made us hypothesise that providing an existing query to ChatGPT and asking for a refinement could be beneficial.</p>
<h1>2.3 Prompt Engineering for Transformer-based Generative Language Models</h1>
<p>Prompt engineering is the process of guiding a generative language model to perform a particular task. In some respect, prompt engineering can be seen as a way to program a generative language model through natural language [53]. A popular way of guiding model output through prompt engineering is for text-to-image generative models [37, 48]. This 'zero-shot' or 'few-shot' approach to tasks with generative language models has also achieved state-of-the-art results on several natural language tasks [8]. More recently, prompt engineering has been applied to natural language tasks for medicine, such as question answering [36, 38]. The use of generative language models for query formulation or refinement is relatively under-explored. We are only aware of a single other paper that has published results about using a generative language model for query expansion [14], and no work has examined this method for creating Boolean queries for systematic literature review search.</p>
<h2>3 ENGINEERING PROMPTS FOR SYSTEMATIC REVIEWS</h2>
<p>In this paper we investigate the use of ChatGPT to create queries for systematic review literature search. The basic mechanism employed by ChatGPT is that to take an input sequence of text (called prompt), process it through the model, and output the next token in the sequence. This process is repeated several times to generate a complete response to the prompt. ChatGPT relies upon the Transformer architecture [76] trained on a massive amount of text data, allowing it to learn patterns in the way that language is used. During the generation process, the model uses these learned patterns to generate text that is similar to the text observed at training.</p>
<p>A key component in the use of and interaction with ChatGPT is the design of the prompt used to instruct the model to generate an answer. To instruct ChatGPT to create Boolean queries for systematic review literature search we iteratively devise a number of prompts of increased complexity, including prompts that rely on example Boolean queries. We also experiment with prompts that guide the generation of the query through multiple interactions (guided prompts), and by leveraging insights from the existing objective method of query formulation [25, 63, 65]. Prompts are devised for two tasks related to Boolean query generation: the formulation task and the refinement task.</p>
<h3>3.1 Single Prompts for Query Formulation</h3>
<p>Single query formulation prompt refers to a prompt that instructs to formulate systematic review Boolean queries using the title of the review. We design five such prompts for the Boolean query formulation task; these prompts are reported in Table 1. They can be classified into three categories: simple, detailed and with examples. As shown in Table 1, simple prompt refers to a prompt that only uses one sentence to briefly state the task for ChatGPT. This is seen as the most common usage for ChatGPT when users are not expert at constructing high-quality prompts. On the other hand, detailed prompt means that a background story is included, which justifies clearly what is required for ChatGPT to successfully complete the task. The with example prompt also includes an expected query formulation example, so that ChatGPT knows what is expected for it to generate a high-quality answer.</p>
<p>While the difference between the two detailed prompts 'q2' and 'q3' is the way we describe the background story; for with example prompts 'q4' and 'q5', 'q5' also describes one Boolean query formulation strategy to identify PICO elements. PICO refers to: Patient/ Problem, Intervention, Comparison and Outcome, and it is often used before systematic review search to clearly identify the research question and generate high-quality systematic review Boolean queries [20]. The intuition for including PICO element extraction in the prompt is that we want to help ChatGPT to better understand how the example query is formulated logically.</p>
<p>For each systematic review topic, we replace " ${$ review_title $}$ " with the title of the review. For prompts with an example, we further replace " ${$ example_review_title $}$ " with the title of the example review topic and " ${$ example_review_query $}$ " with the Boolean query used by that example review. For instance, for the example title "Thromboelastography (TEG) and rotational thromboelastometry (ROTEM) for trauma-induced coagulopathy in adult trauma patients with bleeding", we insert the Boolean query:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prompt ID</th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Simple</td>
<td style="text-align: center;">q1</td>
<td style="text-align: center;">For a systematic review titled "[review_title]", can you generate a systematic review Boolean query to find all included studies on PubMed for the review topic?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">q2</td>
<td style="text-align: center;">You are an information specialist who develops Boolean queries for systematic reviews. You have extensive experience developing highly effective queries for searching the medical literature. Your specialty is developing queries that retrieve as few irrelevant documents as possible and retrieve all relevant documents for your information need. Now you have your information need to conduct research on {review_title}. Please construct a highly effective systematic review Boolean query that can best serve your information need.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">q3</td>
<td style="text-align: center;">Imagine you are an expert systematic review information specialist; now you are given a systematic review research topic, with the topic title " ${$ review_title}". Your task is to generate a highly effective systematic review Boolean query to search on PubMed (refer to the professionally made ones); the query needs to be as inclusive as possible so that it can retrieve all the relevant studies that can be included in the research topic; on the other hand, the query needs to retrieve fewer irrelevant studies so that researchers can spend less time judging the retrieved documents.</td>
</tr>
<tr>
<td style="text-align: center;">With Example</td>
<td style="text-align: center;">q4</td>
<td style="text-align: center;">You are an information specialist who develops Boolean queries for systematic reviews. You have extensive experience developing highly effective queries for searching the medical literature. Your specialty is developing queries that retrieve as few irrelevant documents as possible and retrieve all relevant documents for your information need. You are able to take an information need such as: "{example_review_title}" and generate valid pubmed queries such as: " ${$ example_review_query}". Now you have your information need to conduct research on " ${$ review_title}", please generate a highly effective systematic review Boolean query for the information need.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">q5</td>
<td style="text-align: center;">You are an information specialist who develops Boolean queries for systematic reviews. You have extensive experience developing highly effective queries for searching the medical literature. Your specialty is developing queries that retrieve as few irrelevant documents as possible and retrieve all relevant documents for your information need. A professional information specialist will extract PICO elements from information needs in a common practice in constructing a systematic review Boolean query. PICO means Patient/ Problem, Intervention, Comparison and Outcome. PICO is a format for developing a good clinical research question prior to starting one's research. It is a mnemonic used to describe the four elements of a sound clinical foreground question. You are able to take an information need such as: "{example_review_title}" and you generate valid pubmed queries such as: " ${$ example_review_query}". Now you have your information need to conduct research on " ${$ review_title}". First, extract PICO elements from the information needs and construct a highly effective systematic review Boolean query that can best serve your information need.</td>
</tr>
</tbody>
</table>
<p>Table 1. Prompts for single prompt query formulation
(Thrombelastography[mesh:noexp] OR (thromboelasto<em>[All Fields] OR thrombelasto</em>[All Fields] OR ROTEM[All Fields] OR "tem international"[All Fields] OR (thromb<em>[All Fields] AND elastom</em>[All Fields]) OR (rotational[All Fields] AND thrombelast[All Fields])) OR (Thrombelastogra<em>[All Fields] OR Thromboelastogra</em>[All Fields] OR TEG[All Fields] OR haemoscope[All Fields] OR haemonetics[All Fields] OR (thromb<em>[All Fields] AND elastogra</em>[All Fields])))</p>
<h1>3.2 Single Prompts for Query Refinement</h1>
<p>Single query refinement prompts refer to prompts that provide ChatGPT with a title of a systematic review and a corresponding Boolean query for that review, and instruct the model to produce a modification of that query that leads to higher search effectiveness. These prompts could be used by reviewers to improve a query they have</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prompt ID</th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Simple</td>
<td style="text-align: center;">q6</td>
<td style="text-align: center;">For a systematic review seed Boolean query: "[initial_query]", This query retrieves too many irrelevant documents and too few relevant documents about the information need: "[review_title]", Please correct this query so that it can retrieve fewer irrelevant documents and more relevant documents.</td>
</tr>
<tr>
<td style="text-align: center;">With Example</td>
<td style="text-align: center;">q7</td>
<td style="text-align: center;">For a systematic review seed Boolean query: "[example_review_initial_query]" ,This query retrieves too many irrelevant documents and too few relevant documents about the information need: "[example_review_title]", therefore it should be corrected to: "[example_review_refined_query]". Now your task is to correct a systematic review Boolean query: "[initial_query]" for information need "[review_title]", so it can retrieve fewer irrelevant documents and more relevant documents.</td>
</tr>
</tbody>
</table>
<p>Table 2. Prompts for single prompt query refinement
manually devised, or could be integrated into a more complex pipeline of automation. We show our designed query refinement prompts in Table 2.</p>
<p>We categorise prompts for query refinement into simple and with example. Similar to single prompt query formulation, simple prompt 'q6' indicates that the prompt is constructed only by briefly describing the task. For with example prompt 'q7', we include an example that tells ChatGPT what it means by successfully refining the query. When asking ChatGPT to refine queries, we replace " ${$ initial_query $]$ " with the initial Boolean query we want ChatGPT to refine. For prompts that require an example, we further replace {example_review_title} with the title of the example review topic; " ${$ example_review_initial_query $}$ " with the initial query of the example topic, and " ${$ example_review_refined_query $}$ " with the final query refined in the example topic.</p>
<p>We investigate the effectiveness of two types of examples: (1) one high-quality systematic review example (HQE) and (2) an example that is similar or related to the querying topic (RE). To identify a related example, we use a monoBERT architecture to find the closest example from the test collection [45]. Specifically, we concatenate the review title of the querying topic with the title of one potential example review and we pass it through PubMedBERT, a domain-specific BERT model pre-trained on PubMed abstracts [23]. We get a final classification score which refers to the relatedness of the potential example topic to the querying topic; we select the top-ranked example to include in the prompt during prompt creation.</p>
<h1>3.3 Guided Prompts for Query Formulation</h1>
<p>We design a multi-step prompt that follows the same logic from one of the current state-of-the-art automated query formulation methods, namely the objective method [63]. We show our designed guided prompt with an example in Table 3. Specifically, we follow a four step pipeline to generate the Boolean query: in the first step, Boolean query terms are identified using one seed study, then these terms are classified into four categories: terms relating to health conditions (A), terms relating to a treatment (B), terms relating to types of study design (C), or others (N/A) in the second step. For the third step, terms in the same category are combined by 'OR', and final queries are combined by 'AND'. Finally, ChatGPT is asked to refine the query by adding more terms such as MeSH Terms. By doing a guided prompt, we can better control the terminology of Boolean query formulation from ChatGPT, also dividing the complex task into multiple, more manageable subtasks that are effective.</p>
<h2>4 EXPERIMENTAL SETTINGS</h2>
<p>Our experiments are conducted using two collections: CLEF technological assisted reviews (TAR) datasets [28-30] and systematic review collection with seed studies (Seed Collection) [79]. CLEF TAR dataset is published each year from 2017 to 2019 as a validation dataset for more effective systematic review Boolean query formulation and screening. We use CLEF TAR 2017 and 2018 as they both consist of only Diagnostic Test Accuracy systematic reviews. As CLEF TAR 2017 reviews is a subset of the CLEF TAR 2018 dataset, we obtained 72 review topics</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Step</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Example Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Step1</td>
<td style="text-align: center;">Follow my instructions precisely to develop a highly effective Boolean query for a medical systematic review literature search. Do not explain or elaborate. Only respond with exactly what I request. First, Given the following statement and text from a relevant study, please identify 50 terms or phrases that are relevant. The terms you identify should be used to retrieve more relevant studies, so be careful that the terms you choose are not too broad. You are not allowed to have duplicates in your list. <br> statement: "Prevalence of Differentiated Thyroid Cancer in Autopsy Studies Over Six Decades: A Meta-Analysis" <br> Text: Ten occult carcinomas of the thyroid gland were found in 274 unselected autopsies at the Pathology Service, Hospital José Joaquín Aguirre, between December 1980 and March 1983. This is the lowest incidence among the most recent published series. The smallest papillary microcarcinomas presented most frequently as nonencapsulated nonsclerosing tumors without inflammatory infiltrate or fibrosis, suggesting that they may represent the early stages of development. Our data show a relatively high and similar frequency of papillary microcarcinomas in surgical and autopsy series, but do not demonstrate risk factors for clinical evolution.</td>
<td style="text-align: center;">1. Differentiated thyroid cancer <br> 2. Prevalence <br> 3. Autopsy studies <br> 4. Meta-analysis <br> 5. Occult carcinomas <br> 6. Thyroid gland <br> 7. Incidence <br> 8. Environmental factors <br> 9. Etiology <br> 10. Low risk <br> ... <br> 50. Parameters.</td>
</tr>
<tr>
<td style="text-align: center;">Step 2</td>
<td style="text-align: center;">For each item in the list you created in step 1, classify it into as of three categories: terms relating to health conditions (A), terms relating to a treatment (B), terms relating to types of study design (C). When an item does not fit one of these categories, mark it as (N/A). Each item needs to be categorised into (A), (B), (C), or (N/A).</td>
<td style="text-align: center;">1. (A) Differentiated thyroid cancer <br> 2. (N/A) Prevalence <br> 3. (C) Autopsy studies <br> 4. (C) Meta-analysis <br> 5. (A) Occult carcinomas <br> 6. (A) Thyroid gland <br> 7. (N/A) Incidence <br> 8. (N/A) Environmental factors <br> 9. (N/A) Etiology <br> 10. (N/A) Low risk <br> ... <br> 50. (N/A) Parameters.</td>
</tr>
<tr>
<td style="text-align: center;">Step 3</td>
<td style="text-align: center;">Using the categorised list you created in step 2, create a Boolean query that can be submitted to PubMed which groups together items from each category. For example: ((itemA1[Title/Abstract] OR itemA2[Title/Abstract] or itemA2[Title/Abstract]) AND (itemB1[Title/Abstract] OR itemB2[Title/Abstract] OR itemB3[Title/Abstract]) AND (itemC1[Title/Abstract] OR itemC2[Title/Abstract] OR itemC3[Title/Abstract]))</td>
<td style="text-align: center;">((((differentiated[Title/Abstract] OR thyroid[Title/Abstract] OR carcinoma[Title/Abstract] OR papillary[Title/Abstract] OR microcarcinoma[Title/Abstract]) AND (cancer[Title/Abstract] OR incidence[Title/Abstract] OR etiology[Title/Abstract] OR risk[Title/Abstract] OR gender[Title/Abstract] OR hormonal[Title/Abstract] OR nodular[Title/Abstract] OR goiter[Title/Abstract] OR Hashimoto's[Title/Abstract] OR malignancy[Title/Abstract] OR concomitant[Title/Abstract] OR tumor[Title/Abstract] OR infiltrate[Title/Abstract] OR fibrosis[Title/Abstract] OR development[Title/Abstract] OR frequency[Title/Abstract])) AND (autopsy[Title/Abstract] OR surgical[Title/Abstract] OR material[Title/Abstract] OR series[Title/Abstract] OR specimens[Title/Abstract] OR cases[Title/Abstract])).</td>
</tr>
<tr>
<td style="text-align: center;">Step 4</td>
<td style="text-align: center;">Use your expert knowledge to refine the query, making it retrieve as many relevant documents as possible while minimising the total number of documents retrieved. Also add relevant MeSH terms into the query where necessary, e.g., MeSHTerm[MeSH]. Retain the general structure of the query, however, with each main clause of the query corresponding to a PICO element. The final query still needs to be executable on PubMed, so it should be a valid query.</td>
<td style="text-align: center;">((((differentiated thyroid cancer[MeSH] OR "differentiated thyroid"[All Fields] OR "thyroid carcinoma"[All Fields] OR "papillary microcarcinoma"[All Fields]) AND (prevalence[All Fields] OR incidence[MeSH] OR "etiology of"[All Fields] OR "risk factors"[All Fields] OR gender[All Fields] OR hormonal[All Fields] OR "nodular goiter"[All Fields] OR "Hashimoto's thyroiditis"[MeSH] OR malignancy[MeSH] OR "concomitant lesion"[All Fields] OR tumor[All Fields] OR infiltrate[All Fields] OR fibrosis[All Fields] OR "early stages of development"[All Fields] OR frequency[All Fields])) AND (autopsy[MeSH] OR surgical[All Fields] OR material[All Fields] OR series[All Fields] OR specimens[All Fields] OR cases[All Fields]))</td>
</tr>
</tbody>
</table>
<p>Table 3. Example designed guided prompt for query formulation.
after removing duplicated reviews. Seed collection contains 40 systematic review topics with real seed studies that guide systematic review researchers during Boolean query formulation. Both collections contain systematic review titles, Boolean queries and relevant assessments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prompts</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">F3</th>
<th style="text-align: center;">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\stackrel{\rightharpoonup}{\infty}$</td>
<td style="text-align: center;">Baselines</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">$0.0207^{*}$</td>
<td style="text-align: center;">$0.0290^{*}$</td>
<td style="text-align: center;">$0.0481^{*}$</td>
<td style="text-align: center;">$0.8317^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Conceptual</td>
<td style="text-align: center;">$0.0015^{*}$</td>
<td style="text-align: center;">$0.0027^{*}$</td>
<td style="text-align: center;">$0.0101^{*}$</td>
<td style="text-align: center;">$0.6997^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Objective</td>
<td style="text-align: center;">$0.0002^{*}$</td>
<td style="text-align: center;">$0.0005^{*}$</td>
<td style="text-align: center;">$0.0023^{*}$</td>
<td style="text-align: center;">$0.9128^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Simple</td>
<td style="text-align: center;">q1</td>
<td style="text-align: center;">0.0543</td>
<td style="text-align: center;">0.0500</td>
<td style="text-align: center;">0.0590</td>
<td style="text-align: center;">$0.1293^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Detailed</td>
<td style="text-align: center;">q2</td>
<td style="text-align: center;">0.1166</td>
<td style="text-align: center;">0.0654</td>
<td style="text-align: center;">0.0696</td>
<td style="text-align: center;">$0.1310^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">q3</td>
<td style="text-align: center;">0.0844</td>
<td style="text-align: center;">0.0443</td>
<td style="text-align: center;">$0.0497^{*}$</td>
<td style="text-align: center;">$0.1175^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">With Example</td>
<td style="text-align: center;">q4</td>
<td style="text-align: center;">0.0752</td>
<td style="text-align: center;">0.0642</td>
<td style="text-align: center;">0.0847</td>
<td style="text-align: center;">0.5035</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">q5</td>
<td style="text-align: center;">0.0958</td>
<td style="text-align: center;">0.0717</td>
<td style="text-align: center;">0.0844</td>
<td style="text-align: center;">$0.3335^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Baselines</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">0.0367</td>
<td style="text-align: center;">$0.0651^{*}$</td>
<td style="text-align: center;">$0.1099^{*}$</td>
<td style="text-align: center;">$0.7366^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Conceptual</td>
<td style="text-align: center;">0.0018</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.4138</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Objective</td>
<td style="text-align: center;">0.0057</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.5192</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Simple</td>
<td style="text-align: center;">q1</td>
<td style="text-align: center;">0.0501</td>
<td style="text-align: center;">0.0274</td>
<td style="text-align: center;">0.0298</td>
<td style="text-align: center;">0.0528</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Detailed</td>
<td style="text-align: center;">q2</td>
<td style="text-align: center;">0.0983</td>
<td style="text-align: center;">0.0310</td>
<td style="text-align: center;">0.0278</td>
<td style="text-align: center;">$0.0394^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">q3</td>
<td style="text-align: center;">0.0730</td>
<td style="text-align: center;">0.0329</td>
<td style="text-align: center;">0.0329</td>
<td style="text-align: center;">0.0519</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">With Example</td>
<td style="text-align: center;">q4</td>
<td style="text-align: center;">0.0283</td>
<td style="text-align: center;">0.0274</td>
<td style="text-align: center;">0.0374</td>
<td style="text-align: center;">0.1290</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">q5</td>
<td style="text-align: center;">0.0188</td>
<td style="text-align: center;">0.0193</td>
<td style="text-align: center;">0.0271</td>
<td style="text-align: center;">0.0785</td>
</tr>
</tbody>
</table>
<p>Table 4. Single Prompt query formulation results. CLEF indicates CLEF TAR collection and $S C$ indicates seed collection. Statistical significant differences (Student's two-tailed, paired t-test with Bonferonni correction, $\mathrm{p}&lt;0.05$ ) between q4 and all other methods are indicated by *.</p>
<p>To generate prompts, we replace fields in our designed prompts, shown in Table 1, 2 and 3 with specific review topic information in the collection. For prompts with examples, we selected topic CD010438 from CLEF TAR collection as the HQE topic for generating queries for both collections, as the query obtained high effectiveness and has a simple structure for ChatGPT to follow.</p>
<p>We do not run query refinement for Seed collection as we could not get queries generated by objective and conceptual method reported in the original collection [79]. The guided prompt method relies on the objective method, which requires at least one seed study to start with query formulation. Thus we did not run guided prompting on CLEF TAR as it does not contain seed studies.</p>
<p>After obtaining generated Boolean queries from ChatGPT, any queries that produced incorrectly formatted Boolean queries were removed and generated again for the review topic.</p>
<p>To evaluate the generated Boolean queries, we executed them using PubMed's Entrez API to obtain retrieved PubMed IDs [9]. We then used set-based measures such as precision, f-measure, and recall to assess the retrieved PubMed IDs using the abstract-level relevant assessment in the collection.</p>
<h1>5 MAIN RESULTS</h1>
<h3>5.1 Single Prompt Query Formulation</h3>
<p>We show results of single prompt query formulation in Table 4. The results indicate that queries generated from ChatGPT generally obtain a higher precision compared to the current-state-of-the-art automatic query formulation methods, with a trade off of lower recall. For F-measure (which captures both precision and recall), ChatGPT generated queries are more effective than both state-of-the-art and original authored queries on the CLEF collection. However, they are less effective on the Seed Collection. Search in systematic reviews generally</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">F3</th>
<th style="text-align: center;">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.0751</td>
<td style="text-align: center;">0.0642</td>
<td style="text-align: center;">0.0872</td>
<td style="text-align: center;">0.5035</td>
</tr>
<tr>
<td style="text-align: center;">$\square$ q4-RE</td>
<td style="text-align: center;">$0.1105(+47.1 \%)$</td>
<td style="text-align: center;">$0.0909(+41.6 \%)$</td>
<td style="text-align: center;">$0.1144(+31.2 \%)$</td>
<td style="text-align: center;">$0.4183(-38.1 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">q4-HQE</td>
<td style="text-align: center;">0.0283</td>
<td style="text-align: center;">0.0274</td>
<td style="text-align: center;">0.0374</td>
<td style="text-align: center;">0.129</td>
</tr>
<tr>
<td style="text-align: center;">q4-RE</td>
<td style="text-align: center;">$0.0351(+24.0 \%)$</td>
<td style="text-align: center;">$0.0140(-48.9 \%)$</td>
<td style="text-align: center;">$0.0139(-62.8 \%)$</td>
<td style="text-align: center;">$0.0161^{*}(-87.6 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 5. Comparison of result for single prompt query generation prompt 'q4' when using different type of examples. CLEF indicates CLEF TAR collection and $S C$ indicates seed collection; For each collection, two type of example is used, $q 4-H Q E$ refers to using one high quality example, while $q 4-R E$ refers to using related query as example. Statistical significant differences $(p&lt;0.05)$ between the two types of examples are indicated by *.
requires high-recall to ensure that all evidence can be found relating to the research topic. All the ChatGPT generated queries obtain a lower recall than the baseline methods, suggesting that ChatGPT generated queries may be not suitable for high-recall retrieval, but rather best suited when time is limited; e.g., for rapid reviews [40].</p>
<p>Using different Simple and Detailed prompts (q1-3) only had a minor impact on effectiveness. For CLEF, q2 statistically significantly better with respect to precision; otherwise the prompt type did not have a strong effect. However, we found that prompts that include a high quality systematic review topic as example are able to significantly outperform those without, shown as a consistently higher F_1, F_3 and recall. When comparing the effectiveness of two prompts with examples, we found that asking ChatGPT to generate PICO elements before generating its final Boolean query resulted in Boolean queries with a lower recall but higher precision. Overall, our findings indicate that including a high quality systematic review query example in the prompt is crucial, while the level of detail in the task description may not have a significant impact.</p>
<p>Next, to assess the impact of query construction example to the effectiveness of generated Boolean queries using ChatGPT, we further test the effectiveness when different types of examples is used, as described in section 3.1.</p>
<p>Table 5 compares the effectiveness of queries generated using the most relevant topic in the prompt to queries generated using a high-quality example. Using a relevant topic as example can result in queries with higher precision, but lower recall.</p>
<p>ChatGPT will generate different responses for the same prompt. To study variability of effectiveness, we select the more effective prompt, q4, and we re-run the prompt 10 times. Variability is shown in Figure 1. Recall varied more than precision and F-measures: variance of recall is $12 \%$ of its mean value; precision was $7.1 \%$, F-1 $6.6 \%$ and F-3 $7.2 \%$. Our result indicates that the generated queries from the same prompt would mainly differ in the ability to obtain more relevant documents (recall) from the included studies.</p>
<h1>5.2 Single Prompt Query Refinement</h1>
<p>The results of single prompt query refinement, as shown in Table 6, indicate that ChatGPT is capable of improving the effectiveness of systematic review Boolean queries. Specifically, the use of ChatGPT for query refinement leads to an increase in precision and F-measure, while obtaining a lower recall. Therefore, it is crucial to first create a seed query with a high recall, and then use ChatGPT to refine the query in order to achieve highly effective Boolean queries.</p>
<p>Furthermore, our results indicate that the greatest improvement in effectiveness is achieved when using seed queries that are automatically formulated by the objective method. This will also result in query with the highest recall among all methods. After refinement using ChatGPT, there is only a $11 \%$ drop in recall, with considerable gains in precision, F-1, and F-3.</p>
<p>Variability is once again studies by running the best method, refined-objective run, 10 times. We show the variability of the query refinement in Figure 2. There is less variance in query refinement than query formation</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Topic-by-topic variability boxplot for effectiveness of 10 iterative runs in single prompt query formulation. CLEF indicates CLEF TAR collection and $S C$ indicates seed collection.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompts</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">F1</th>
<th style="text-align: left;">F3</th>
<th style="text-align: left;">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original</td>
<td style="text-align: left;">0.0207</td>
<td style="text-align: left;">0.0290</td>
<td style="text-align: left;">0.0481</td>
<td style="text-align: left;">0.8317</td>
</tr>
<tr>
<td style="text-align: left;">q6-Original</td>
<td style="text-align: left;">$0.0795^{*}$</td>
<td style="text-align: left;">$0.0597^{*}$</td>
<td style="text-align: left;">$0.0802^{*}$</td>
<td style="text-align: left;">$0.5060^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">Conceptual</td>
<td style="text-align: left;">0.0014</td>
<td style="text-align: left;">0.0027</td>
<td style="text-align: left;">0.0100</td>
<td style="text-align: left;">0.6996</td>
</tr>
<tr>
<td style="text-align: left;">q7-conceptual</td>
<td style="text-align: left;">0.0022</td>
<td style="text-align: left;">0.0039</td>
<td style="text-align: left;">0.0069</td>
<td style="text-align: left;">$0.2699^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">Objective</td>
<td style="text-align: left;">0.0002</td>
<td style="text-align: left;">0.0005</td>
<td style="text-align: left;">0.0023</td>
<td style="text-align: left;">$\mathbf{0 . 9 1 2 8}$</td>
</tr>
<tr>
<td style="text-align: left;">q7-Objective</td>
<td style="text-align: left;">$0.0460^{*}$</td>
<td style="text-align: left;">$0.0471^{*}$</td>
<td style="text-align: left;">$0.0652^{*}$</td>
<td style="text-align: left;">$0.8115^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">q4</td>
<td style="text-align: left;">0.0751</td>
<td style="text-align: left;">0.0642</td>
<td style="text-align: left;">0.0872</td>
<td style="text-align: left;">0.5035</td>
</tr>
<tr>
<td style="text-align: left;">q7-q4</td>
<td style="text-align: left;">$\mathbf{0 . 1 1 6 2}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 7 7 2}$</td>
<td style="text-align: left;">$\mathbf{0 . 0 9 2 1}$</td>
<td style="text-align: left;">$0.3179^{*}$</td>
</tr>
</tbody>
</table>
<p>Table 6. Result table for Single Prompt query refinement on CLEF TAR collection. For a refinement method 'q6-Original', 'q6' indicates the prompt used to generate the refined query; 'Original' indicate the seed queries used for ChatGPT to refine. For each query refinement method, statistical significant differences ( $p&lt;0.05$ ) between refined prompt and seed queries is indicated by *.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Topic-by-topic variability boxplot for effectiveness of 10 iterative runs in single prompt query refinement.
(Figure 1). This is understandable given the query structure is already provided by the seed query, whereas query formulation must be done from scratch from the title of the review.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Topic-by-topic variability boxplot for effectiveness of using different seed studies for guided prompt query formulation.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Topic-by-topic variability boxplot for effectiveness of 10 iterative runs using the same seed study in guided prompt query formulation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompts</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">F1</th>
<th style="text-align: left;">F3</th>
<th style="text-align: left;">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">q4</td>
<td style="text-align: left;">0.0284</td>
<td style="text-align: left;">0.0274</td>
<td style="text-align: left;">0.0374</td>
<td style="text-align: left;">0.1290</td>
</tr>
<tr>
<td style="text-align: left;">Guided</td>
<td style="text-align: left;">$0.0993(+249.6 \%)$</td>
<td style="text-align: left;">$0.0492(+79.6 \%)$</td>
<td style="text-align: left;">$0.0565(+51.1 \%)$</td>
<td style="text-align: left;">$0.5171(+301.6 \%)^{\circ}$</td>
</tr>
</tbody>
</table>
<p>Table 7. Result table for Guided Prompt query formulation on Seed Collection, compared with single prompt query generation 'q4'; Statistical significant differences ( $p=0.05$ ) between guided prompt and single prompt is indicated by ${ }^{\circ}$.</p>
<h1>5.3 Guided Prompt Query Formulation</h1>
<p>The result of guided prompt query formulation from Table 7 shows that if using well-chosen seed study, queries generated from guided prompt are more effective than single prompt queries.</p>
<p>However, like previous experiments in single prompt query generation and refinement, the effectiveness varies considerably across runs; and the effectiveness also depends on the seed study being used. We show the variability of query effectiveness in Figure 3 when different seed studies are used to generate queries, and in Figure 4, we show the variability of effectiveness when the same seed study is used, picked from the best seed study effectiveness from the first run.</p>
<p>From the variability graph, we see that query generation using guided prompt is not stable across different seed studies. Furthermore, even when same seed study is used to generate multiple runs, there is a high degree of variability. The range of precision and recall for some topics can span from 0 to 1 , especially when the average effectiveness is high.</p>
<h2>6 QUERY FAILURE ANALYSIS</h2>
<p>We attempt to uncover the characteristics of queries with poor effectiveness, with a guise of identifying ways to improve query generation. To do this, we follow the steps below to select a set of successful and failing queries: (1) Select the best method for query formulation (q4) and query refinement (q7-objective).</p>
<p>(2) From the 10 iterative runs from each task, we extract an 'oracle' result, using recall as an indicator of effectiveness. (Tie broken using precision.)
(3) Next we compare the oracle effectiveness with that of the original query. Successful topics are defined as those where the oracle has higher precision and recall than the original query; failing topics are those where the oracle has lower precision and recall than the original query. The intuition of selecting successful topics and failing topics above is based on the fact that higher recall often means more documents are retrieved from the query, and results in lower precision.
Only the CLEF dataset was used because the Seed Collection contains topics that are not necessarily systematic reviews; e.g., scoping review, rapid review, etc.</p>
<p>Using this method above, we identify seven successful topics ${ }^{2}$ and six failed topics ${ }^{3}$ for query formulation. For query refinement, we identify three succesfull topics ${ }^{4}$, and eleven topics for query refinement ${ }^{5}$.</p>
<p>By comparing the failed queries with other queries, we summarize three key findings:
(1) Poorly performing ChatGPT generated queries tended to retrieve a large number of results: for comparison of successful queries and failing queries for query formulation, the median ratio for successful queries is 0.30 , while for failing queries, this ratio is 10.28 . Same for query refinement, the median ratio for or successful queries and failing queries are 1.39 and 2.74 , respectively. This means that failing queries may be categorised by having a large number of OR clauses; thus it may be possible to detect such queries and introduce mitigation strategies.
(2) We also notice that some of the ChatGPT queries contain many incorrect MeSH Terms. For the best performing prompt for query formulation, on average three MeSH Terms were generated, and $55 \%$ of the MeSH Terms generated were not in the MeSH vocabulary. On the other hand, for query refinement prompt, only 1.5 MeSH Terms were generated on average per query, with $66 \%$ of the MeSH Terms not in the MeSH vocabulary. The is no strong correlation between the ratio of incorrectly generated MeSH Terms and query effectiveness. However, we also acknowledge that this does not mean certain incorrect MeSH Terms do not have a major negative impact on effectiveness. Further experiment is needed (e.g., MeSH Term correction [77, 80]) to check how these MeSH Terms impact query effectiveness and how would effectiveness changes if MeSH Terns are corrected.
(3) Relevance judgements were done using the original query so may be biased towards this. In all the failing queries, the number of unjudged document in the retrieved documents set accounts for more than $94.8 \%$ of the retrieved documents, much higher than that from the original queries $63.7 \%$; On the other hand, the portion of unjudged documents from generated and original queries for other topics was similar, at $56.2 \%$ and $55.4 \%$, respectively. It may be that the failing queries simply retrieved many more unjudged documents because they had poor performance. But there is also a possibility that they actually managed to find clusters of documents relevant to topics but never assessed by reviewers. While this is conjecture, it does spark some ideas for solutions to improve such queries. If there are, in fact, clusters of relevant documents amongst a large number of non-relevant ones, then automated classification techniques may help to uncover these [2, 32, 41, 70, 74]. In particular, active learning [15, 44] may be used to selectively assess documents from different clusters to hone in on relevant documents.</p>
<h1>7 SUMMARY OF FINDINGS</h1>
<p>In light of the empirical results reported in Section 5, we provide a summary of the key findings of our study in terms of the research questions we set to investigate.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>RQ1: How does ChatGPT compare with current state-of-the-art methods for formulating and refining systematic review Boolean queries?</p>
<p>In terms of automatic query formulation, our results indicate that the use of ChatGPT compares favourably with the current state-of-the-art automated Boolean query generation methods in terms of precision, at the expenses of a lower recall.</p>
<p>In terms of automatic query refinement, our results indicate that ChatGPT is effective at refining existing Boolean queries that have been generated by a previous step of automated Boolean query formulation. In particular, the ChatGPT-based refinement of queries generated using the objective method for Boolean query formulation leads to the most considerable improvements in effectiveness.</p>
<p>In both tasks, improvements are observed in terms of increased precision, i.e. a reduction of irrelevant documents being retrieved by the query. These improvements come at the expense of losses in recall (though for the best query refinement settings these losses are only marginal). High recall is often a key requirement for traditional systematic reviews and thus ChatGPT may not be well suited in these settings. However, we note that typically, in manually created Boolean queries, MeSH terms are used to improve recall, but the queries obtained through ChatGPT contain a large number of incorrect MeSH terms (see Section 6). The addition of a post-processing step that resolves incorrect MeSH terms produced by ChatGPT may alleviate the losses in recall we witnessed in the results. For this, it may be possible to adapt existing methods for MeSH term suggestion [77, 80], which also could be used to further refine the queries produced by ChatGPT by adding more effective MeSH terms the model may have failed to identify. We further note that within the systematic review process, the step of snowballing is designed to further increase recall. Snowballing refers to the recursive analysis of references cited in retrieved documents [22] - in all effects, this process adds to the set to be reviewed studies not retrieved by the Boolean query. Thus, it may be that losses in recall observed when using ChatGPT to create queries may be recovered through snowballing on the identified relevant studies.</p>
<p>RQ2: To what extent do the prompts used to generate systematic review Boolean queries impact the effectiveness of the Boolean queries produced by ChatGPT?</p>
<p>Our results suggest that the type of prompt used has considerable effects on the effectiveness of the queries produced by ChatGPT. We further review the impact of guided prompts in the next research question, and here we only focus on single prompts.</p>
<p>In this case, we observe that integrating example Boolean queries into the prompts tends to provide better queries than simple prompts (Table 4). This is particularly the case for the CLEF datasets - while the same does not hold on the seed collection, we note this collection is limited in the number of topics available, so results may be less generalisible. Notably, improvements obtained when including query examples are large for recall (this is also the case on the seed collection), thus partially mitigating the low recall observed throughout the queries produced with ChatGPT. We also observe that the nature of the example query influences results. Specifically, example queries that are semantically close to the systematic review topic for which the new query is being created lead to higher precision. On the other hand, using high quality but potentially unrelated queries in the prompts, instead, leads to higher recall.</p>
<p>We also have observed that the inclusion in the prompt of instructions to generate PICO elements does lead to considerable losses in recall, and provides improvements in precision only on the CLEF collection.</p>
<p>RQ3: What is the effect of guiding the query formulation process with ChatGPT through multiple prompts that mimic the process of the current state-of-the-art automated Boolean query generation method?</p>
<p>We have leveraged the process set out by the existing objective query generation method [63, 65] to design a sequence of prompts that iteratively guide ChatGPT in the process of generating a Boolean query. Our results show that guided prompts lead to higher effectiveness than single prompt strategies: improvements are observed for both precision and recall. There are caveats though that one need to be wary about - we discuss these next.</p>
<p>RQ4: What are the caveats and potential challenges of using ChatGPT to create systematic review Boolean queries?</p>
<p>Our results highlight two main caveats practitioners should be wary about if relying on ChatGPT to create Boolean queries for systematic review literature search: (1) incorrect MeSH terms, (2) high variability in query effectiveness across multiple requests.</p>
<p>We already touched upon the first caveat when analysing RQ1. 55\% and over of the MeSH terms generated by ChatGPT are actually not in the MeSH vocabulary, and thus are incorrect. The effect of these incorrect MeSH terms is still unclear, as it is unclear whether existing methods for automatic MeSH term suggestion could be improved to correct the MeSH terms generated by ChatGPT [77, 80].</p>
<p>The second caveat refers to the results showed in Figures 1- 4, which highlight that if multiple generations are performed with ChatGPT for the same prompt, different queries are generated - and their effectiveness can largely differ. The fact that the output of ChatGPT is non-deterministic given the same prompt ${ }^{6}$ suggests that it may use top-k sampling, or a similar technique, for text generation. Top-k sampling involves generating the $k$ most likely next tokens for a given input and then randomly selecting one of those tokens as the next step in the generation process. Nevertheless, aside from the specific technique used for text generation, the non-deterministic nature of ChatGPT and the fact that this has a sensible impact of the effectiveness of the Boolean queries that are generated, pose a threat for the uptake of the methods we investigated in this paper. In fact, users would typically have no or limited insight in the effectiveness of the query that ChatGPT generates and thus may be unable to identify high-yield queries among those generated for the same prompt.</p>
<h1>8 LIMITATIONS IN OUR USE OF CHATGPT</h1>
<p>In our experiment, we use ChatGPT as a black-box application and test its effectiveness in generating systematic review Boolean queries. One significant limitation of our paper is that details about how ChatGPT works internally are undisclosed, including model architecture and what's included in training data. Therefore, we do acknowledge that ChatGPT may have already seen the original queries for the systematic review topics in our test collection during model training and this might impact the effectiveness of the queries ChatGPT generated. In addition, we are unclear whether interaction history (beyond immediate context of conversation, which the service provider, OpenAI, states being approximately 3000 words or 4000 tokens) may have effect on the generation. Although when executing an experiment for a new query topic we instantiate a new conversation within ChatGPT, we do not know what information the service provider retains for a user and whether they use previous conversations of the user to personalise generation.</p>
<p>Thirdly, our experiments were executed over a two-weeks period ion early January 2023. We are unsure whether modifications to the model have been made during this period. OpenAI disclosed the model we used is the January 9 version and no experiment was run once OpenAI updated their model to the Jan 30 version. Yet, we could not control for minor engineering or model updates OpenAI may have made to the model but not disclosed.</p>
<h2>9 CONCLUSION</h2>
<p>This paper used ChatGPT to formulate and refine Boolean queries for systematic reviews. We designed an extensive set of prompts to investigate these tasks on over 100 systematic review topics. Queries generated by ChatGPT obtained higher precision but lowered recall compared to the queries generated by the current state-of-the-art automatic query formulation methods. We also showed that ChatGPT could generate Boolean queries with even higher effectiveness with a guided prompt. One caveat of our results is that ChatGPT generates different queries even if the same prompt is used, which vary in effectiveness. This issue would need to be</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>resolved before using ChatGPT to generate Boolean queries for systematic review literature search in practice, where reproducibility is paramount.</p>
<p>In summary, the paper makes the following contributions:</p>
<ul>
<li>To the best of our knowledge, this paper is the first of its kind to investigate the effectiveness of Boolean queries designed for systematic review literature search generated by ChatGPT.</li>
<li>Several strategies for generating prompts for ChatGP.</li>
<li>An extensive evaluation of a large number of systematic review topics and prompts provides insight into how different prompts may impact the effectiveness of the systematic review Boolean queries generated through ChatGPT.</li>
<li>A query failure analysis provides possible directions for future research on generating better queries with ChatGPT or post-process results from these queries.
Overall, it is still unclear whether all Boolean queries could be developed by transformer-based generative models like ChatGPT. This paper shows the potential for such models, tempered by many issues around variability, robustness and reproducibility. Whatever the outcome, we firmly believe this is an area that is a promising and exciting ground for future research.</li>
</ul>
<h1>REFERENCES</h1>
<p>[1] 2019. Review Manager Web (RevMan Web).
[2] JJ García Adeva, JM Pikatza Atxa, M Ubeda Carrillo, and E Ansuategi Zengotitabengoa. 2014. Automatic Text Classification to Support Systematic Reviews in Medicine. Expert Systems with Applications 41, 4 (2014), 1498-1508.
[3] Amal Alharbi, William Briggs, and Mark Stevenson. 2018. Retrieving and Ranking Studies for Systematic Reviews: University of Sheffield's Approach to CLEF eHealth 2018 Task 2. In CEUR Workshop Proceedings: Working Notes of CLEF 2018: Conference and Labs of the Evaluation Forum, Vol. 2125. CEUR Workshop Proceedings.
[4] Amal Alharbi and Mark Stevenson. 2017. Ranking Abstracts to Identify Relevant Evidence for Systematic Reviews: The University of Sheffield's Approach to CLEF eHealth 2017 Task 2.. In CEUR Workshop Proceedings: Working Notes of CLEF 2017: Conference and Labs of the Evaluation Forum.
[5] Antonios Anagnostou, Athanasios Lagopoulos, Grigorios Tsoumakas, and Ioannis P Vlahavas. 2017. Combining Inter-Review Learning-to-Rank and Intra-Review Incremental Training for Title and Abstract Screening in Systematic Reviews.. In CEUR Workshop Proceedings: Working Notes of CLEF 2017: Conference and Labs of the Evaluation Forum.
[6] Maisie Badami, Boualem Benatallah, and Marcos Baez. 2022. Systematic Literature Review Search Query Refinement Pipeline: Incremental Enrichment and Adaptation. In International Conference on Advanced Information Systems Engineering. Springer, 129-146.
[7] Florian Boudin, Jian-Yun Nie, and Martin Dawes. 2010. Clinical Information Retrieval Using Document and PICO Structure. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. $822-830$.
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, and Amanda Askell. 2020. Language Models Are Few-Shot Learners. Advances in neural information processing systems 33 (2020), 1877-1901.
[9] Kathi Canese and Sarah Weis. 2013. PubMed: the bibliographic database. The NCBI handbook 2, 1 (2013).
[10] Jiayi Chen, Su Chen, Yang Song, Hongyu Liu, Yueyao Wang, Qinmin Hu, Liang He, and Yan Yang. 2017. ECNU at 2017 eHealth Task 2: Technologically Assisted Reviews in Empirical Medicine.. In CEUR Workshop Proceedings: Working Notes of CLEF 2017: Conference and Labs of the Evaluation Forum.
[11] Justin Clark. 2013. Systematic Reviewing. In Methods of Clinical Epidemiology, Gail M. Williams Suhail A. R. Doi (Ed.). Springer.
[12] Justin Clark, Paul Glasziou, Chris Del Mar, Alexandra Bannach-Brown, Paulina Stehlik, and Anna Mae Scott. 2020. A full systematic review was completed in 2 weeks using automation tools: a case study. Journal of clinical epidemiology 121 (2020), 81-90.
[13] Justin Michael Clark, Sharon Sanders, Matthew Carter, David Honeyman, Gina Cleo, Yvonne Auld, Debbie Booth, Patrick Condron, Christine Dalais, Sarah Bateup, et al. 2020. Improving the Translation of Search Strategies Using the Polyglot Search Translator: A Randomized Controlled Trial. Journal of the Medical Library Association 108, 2 (2020), 195.
[14] Vincent Claveau. 2021. Neural Text Generation for Query Expansion in Information Retrieval. In IEEE/WIC/ACM International Conference on Web Intelligence. ACM, ESSENDON VIC Australia, 202-209. https://doi.org/10.1145/3486622.3493957
[15] A.M. Cohen, W.R. Hersh, K. Peterson, and P.Y. Yen. 2006. Reducing workload in systematic review preparation using automated citation classification. JAMIA 13, 2 (2006), 206-219.
[16] Aaron M Cohen and Neil R Smalheiser. 2018. UIC/OHSU CLEF 2018 Task 2 Diagnostic Test Accuracy Ranking Using Publication Type Cluster Similarity Measures. In CEUR Workshop Proceedings: Working Notes of CLEF 2018: Conference and Labs of the Evaluation Forum, Vol. 2125.
[17] Dina Demner-Fushman and Jimmy Lin. 2007. Answering Clinical Questions with Knowledge-Based and Statistical Techniques. Computational Linguistics 33, 1 (2007), 63-103.
[18] Giorgio Maria Di Nunzio, Federica Beghini, Federica Vezzani, and Geneviève Henrot. 2017. An Interactive Two-Dimensional Approach to Query Aspects Rewriting in Systematic Reviews. IMS Unipd at CLEF eHealth Task 2.. In CEUR Workshop Proceedings: Working Notes of CLEF 2017: Conference and Labs of the Evaluation Forum.
[19] Giorgio Maria Di Nunzio, Giacomo Ciuffreda, and Federica Vezzani. 2018. Interactive Sampling for Systematic Reviews. IMS Unipd at CLEF 2018 eHealth Task 2.. In CEUR Workshop Proceedings: Working Notes of CLEF 2018: Conference and Labs of the Evaluation Forum.
[20] Mette Brandt Eriksen and Tove Faber Frandsen. 2018. The impact of patient, intervention, comparison, outcome (PICO) as a search strategy tool on literature search quality: a systematic review. Journal of the Medical Library Association: JMLA 106, 4 (2018), 420.
[21] Roberto Gozalo-Brizuela and Eduardo C Garrido-Merchan. 2023. ChatGPT is not all you need. A State of the Art Review of large Generative AI models. arXiv preprint arXiv:2301.04655 (2023).
[22] Trisha Greenhalgh and Richard Peacock. 2005. Effectiveness and efficiency of search methods in systematic reviews of complex evidence: audit of primary sources. Bmj 331, 7524 (2005), 1064-1065.
[23] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2020. Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. arXiv:arXiv:2007.15779</p>
<p>[24] Elke Hausner, Charlotte Guddat, Tatjana Hermanns, Ulrike Lampert, and Siw Waffenschmidt. 2015. Development of search strategies for systematic reviews: validation showed the noninferiority of the objective approach. Journal of clinical epidemiology 68, 2 (2015), $191-199$.
[25] Elke Hausner, Siw Waffenschmidt, Thomas Kaiser, and Michael Simon. 2012. Routine development of objectively derived search strategies. Systematic reviews 1, 1 (2012), 19.
[26] Noah Hollmann and Carsten Eickhoff. 2017. Ranking and Feedback-Based Stopping for Recall-Centric Document Retrieval.. In CEUR Workshop Proceedings: Working Notes of CLEF 2017: Conference and Labs of the Evaluation Forum.
[27] William Hsu, William Speier, and Ricky K Taira. 2012. Automated Extraction of Reported Statistical Analyses: Towards a Logical Representation of Clinical Trial Literature. In AMIA Annual Symposium Proceedings, Vol. 2012. 350.
[28] E. Kanoulas, D. Li, L. Azzopardi, and R. Spijker. 2017. CLEF 2017 Technologically Assisted Reviews in Empirical Medicine Overview. In CLEF'17.
[29] Evangelos Kanoulas, Dan Li, Leif Azzopardi, and Rene Spijker. 2019. CLEF 2019 technology assisted reviews in empirical medicine overview. In CEUR workshop proceedings, Vol. 2380.
[30] Evangelos Kanoulas, Rene Spijker, Dan Li, and Leif Azzopardi. 2018. CLEF 2018 Technology Assisted Reviews in Empirical Medicine Overview. In CLEF 2018 Evaluation Labs and Workshop: Online Working Notes, CEUR-WS.
[31] Sarvnaz Karimi, Justin Zobel, Stefan Pohl, and Falk Scholer. 2009. The Challenge of High Recall in Biomedical Systematic Search. In Proceedings of the 3rd International Workshop on Data and Text Mining in Bioinformatics. 89-92.
[32] S.N. Kim, D. Martinez, L. Cavedon, and L. Yencken. 2011. Automatic Classification of Sentences to Support Evidence Based Medicine. BMC bioinformatics 12, 2 (2011).
[33] Grace Eunkyung Lee. 2017. A Study of Convolutional Neural Networks for Clinical Document Classification in Systematic Reviews: Sysreview at CLEF eHealth 2017. In CEUR Workshop Proceedings: Working Notes of CLEF 2017: Conference and Labs of the Evaluation Forum.
[34] Hang Li, Harrisen Scells, and Guido Zuccon. 2020. Systematic review automation tools for end-to-end query formulation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2141-2144.
[35] Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2022. A survey of pretrained language models based text generation. arXiv preprint arXiv:2201.05273 (2022).
[36] Valentin Liévin, Christoffer Egeberg Hother, and Ole Winther. 2023. Can Large Language Models Reason about Medical Questions? arXiv:2207.08143 [cs]
[37] Vivian Liu and Lydia B Chilton. 2022. Design Guidelines for Prompt Engineering Text-to-Image Generative Models. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (CHI '22). Association for Computing Machinery, New York, NY, USA, 1-23. https://doi.org/10.1145/3491102.3501825
[38] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining. Briefings in Bioinformatics 23, 6 (Nov. 2022), bbac409. https://doi.org/10.1093/ bib/bbac409
[39] Andrew MacFarlane, Tony Russell-Rose, and Farhad Shokraneh. 2022. Search strategy formulation for systematic reviews: Issues, challenges and opportunities. Intelligent Systems with Applications (2022), 200091.
[40] Iain J Marshall, Rachel Marshall, Byron C Wallace, Jon Brassey, and James Thomas. 2019. Rapid Reviews May Produce Different Results to Systematic Reviews: A Meta-Epidemiological Study. Journal of clinical epidemiology 109 (2019), 30-41.
[41] Iain J Marshall, Anna Noel-Storr, Joël Kuiper, James Thomas, and Byron C Wallace. 2018. Machine Learning for Identifying Randomized Controlled Trials: An Evaluation and Practitioner's Guide. Research synthesis methods 9, 4 (2018), 602-614.
[42] D. Martinez, S. Karimi, L. Cavedon, and T. Baldwin. 2008. Facilitating Biomedical Systematic Reviews Using Ranked Text Retrieval and Classification. In Proceedings of the 13th Australasian Document Computing Symposium.
[43] Adamantios Minas, Athanasios Lagopoulos, and Grigorios Tsoumakas. 2018. Aristotle University's Approach to the Technologically Assisted Reviews in Empirical Medicine Task of the 2018 CLEF eHealth Lab.. In CEUR Workshop Proceedings: Working Notes of CLEF 2018: Conference and Labs of the Evaluation Forum.
[44] M. Miwa, J. Thomas, A. O'Mara-Eves, and S. Ananiadou. 2014. Reducing systematic review workload through certainty-based screening. JBF 51 (2014), 242-253.
[45] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT. arXiv preprint arXiv:1910.14424 (2019).
[46] Christopher Norman, Mariska Leeflang, and Aurélie Névéol. 2018. LIMSI@ CLEF eHealth 2018 Task 2: Technology Assisted Reviews by Stacking Active and Static Learning.. In CEUR Workshop Proceedings: Working Notes of CLEF 2018: Conference and Labs of the Evaluation Forum.
[47] Christopher Norman12, Mariska Leeflang, and Aurélie Névéol. 2017. Limsi@ Clef Ehealth 2017 Task 2: Logistic Regression for Automatic Article Ranking. In CEUR Workshop Proceedings: Working Notes of CLEF 2019: Conference and Labs of the Evaluation Forum.
[48] Jonas Oppenlaender. 2022. A Taxonomy of Prompt Modifiers for Text-To-Image Generation. arXiv:2204.13988 [cs]</p>
<p>[49] Mohammadreza Pourreza and Faezeh Ensan. 2022. Towards semantic-driven boolean query formalization for biomedical systematic literature reviews. International Journal of Medical Informatics (2022), 104928.
[50] Animesh Prasad, Manpreet Kaur, and Min-Yen Kan. 2018. Neural ParsCit: A Deep Learning Based Reference String Parser. Journal on Digital Libraries 19 (2018), 323-337.
[51] John Rathbone. 2017. Automating systematic reviews. Ph. D. Dissertation. Bond University.
[52] Scott Reeves, Ivan Koppel, Hugh Barr, Della Freeth, and Marilyn Hammick. 2002. Twelve Tips for Undertaking a Systematic Review. Medical teacher 24, 4 (2002).
[53] Laria Reynolds and Kyle McDonell. 2021. Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. ACM, Yokohama Japan, 1-7. https://doi.org/10.1145/ 3411763.3451760
[54] Raul Rodriguez-Esteban and Ivan Iossifov. 2009. Figure Mining for Biomedical Research. Bioinformatics 25, 16 (2009), 2082-2084.
[55] Tony Russell-Rose and Philip Gooch. 2018. 2dSearch: A Visual Approach to Search Strategy Formulation. In Proceedings of the 1st Biennial Conference on Design of Experimental Search and Information Retrieval Systems.
[56] José Antonio Salvador-Oliván, Gonzalo Marco-Cuenca, and Rosario Arquero-Avilés. 2019. Errors in Search Strategies Used in Systematic Reviews and Their Effects on Information Retrieval. Journal of the Medical Library Association : JMLA 107, 2 (April 2019), 210-221. https://doi.org/10.5195/jmla. 2019.567
[57] Margaret Sampson and Jessie McGowan. 2006. Errors in Search Strategies Were Identified by Type and Frequency. Journal of Clinical Epidemiology 59, 10 (2006), 1057-e1.
[58] Harrisen Scells, Connor Forbes, Justin Clark, Bevan Koopman, and Guido Zuccon. 2022. The Impact of Query Refinement on Systematic Review Literature Search: A Query Log Analysis. In Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval. 34-42.
[59] Harrisen Scells and Guido Zuccon. 2018. Generating Better Queries for Systematic Reviews. In Proceedings of the 41st annual International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval (Ann Arbor, MI, USA) (SIGIR '18). 10 pages.
[60] Harrisen Scells and Guido Zuccon. 2018. searchrefiner: A Query Visualisation and Understanding Tool for Systematic Reviews. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management. ACM, 1939-1942.
[61] Harrisen Scells, Guido Zuccon, Anthony Deacon, and Bevan Koopman. 2017. QUT Ielab at CLEF eHealth 2017 Technology Assisted Reviews Track: Initial Experiments with Learning to Rank. In CEUR Workshop Proceedings: Working Notes of CLEF 2017: Conference and Labs of the Evaluation Forum.
[62] Harrisen Scells, Guido Zuccon, and Bevan Koopman. 2019. Automatic Boolean Query Refinement for Systematic Review Literature Search. In The World Wide Web Conference. 1646-1656.
[63] Harrisen Scells, Guido Zuccon, and Bevan Koopman. 2020. A Comparison of Automatic Boolean Query Formulation for Systematic Reviews. Information Retrieval Journal (2020), 1-26.
[64] Harrisen Scells, Guido Zuccon, Bevan Koopman, and Justin Clark. 2020. Automatic Boolean Query Formulation for Systematic Review Literature Search. In Proceedings of The Web Conference 2020. 1071-1081.
[65] Harrisen Scells, Guido Zuccon, Bevan Koopman, and Justin Clark. 2020. A Computational Approach for Objectively Derived Systematic Review Search Strategies. In European Conference on Information Retrieval. Springer, 385-398.
[66] Harrisen Scells, Guido Zuccon, Mohamed A Sharaf, and Bevan Koopman. 2020. Sampling Query Variations for Learning to Rank to Improve Automatic Boolean Query Generation in Systematic Reviews. In Proceedings of The Web Conference 2020. 3041-3048.
[67] Michael Simon, Elke Hausner, Susan F Klaus, and Nancy E Dunton. 2010. Identifying Nurse Staffing Research in Medline: Development and Testing of Empirically Derived Search Strategies with the PubMed Interface. BMC medical research methodology 10, 1 (2010), 76.
[68] Gaurav Singh, Iain Marshall, James Thomas, and Byron Wallace. 2017. Identifying Diagnostic Test Accuracy Publications Using a Deep Model. In CEUR Workshop Proceedings: Working Notes of CLEF 2017: Conference and Labs of the Evaluation Forum, Vol. 1866.
[69] Jaspreet Singh and Lini Thomas. 2017. IIIT-H at CLEF eHealth 2017 Task 2: Technologically Assisted Reviews in Empirical Medicine.. In CEUR Workshop Proceedings: Working Notes of CLEF 2017: Conference and Labs of the Evaluation Forum.
[70] Claire Stansfield, James Thomas, and Josephine Kavanagh. 2013. 'Clustering' Documents Automatically to Support Scoping Reviews of Research: A Case Study. Research synthesis methods 4, 3 (2013), 230-241.
[71] Rodney Summerscales, Shlomo Argamon, Jordan Hupert, and Alan Schwartz. 2009. Identifying Treatments, Groups, and Outcomes in Medical Abstracts. In Proceedings of the 6th Midwest Computational Linguistics Colloquium.
[72] Rodney L Summerscales, Shlomo Argamon, Shangda Bai, Jordan Hupert, and Alan Schwartz. 2011. Automatic Summarization of Results from Clinical Trials. In Proceedings of the 2011 IEEE International Conference on Bioinformatics and Biomedicine. 372-377.
[73] James Thomas and Angela Harden. 2008. Methods for the thematic synthesis of qualitative research in systematic reviews. BMC medical research methodology 8,1 (2008), 45.
[74] Prem Timsina, Jun Liu, and Omar El-Gayar. 2016. Advanced Analytics for the Automation of Medical Systematic Reviews. Information Systems Frontiers 18, 2 (2016), 237-252.</p>
<p>[75] Mercedes Torres Torres and Clive E Adams. 2017. RevManHAL: towards automatic text generation in systematic reviews. Systematic Reviews 6, 1 (2017).
[76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[77] Shuai Wang, Hang Li, Harrisen Scells, Daniel Locke, and Guido Zuccon. 2021. Mesh term suggestion for systematic review literature search. In Proceedings of the 25th Australasian Document Computing Symposium. 1-8.
[78] Shuai Wang, Hang Li, and Guido Zuccon. 2022. MeSH Suggester: A Library and System for MeSH Term Suggestion for Systematic Review Boolean Query Construction. arXiv preprint arXiv:2212.09018 (2022).
[79] Shuai Wang, Harrisen Scells, Justin Clark, Bevan Koopman, and Guido Zuccon. 2022. From little things big things grow: A collection with seed studies for medical systematic review literature search. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 3176-3186.
[80] Shuai Wang, Harrisen Scells, Bevan Koopman, and Guido Zuccon. 2022. Automated MeSH Term Suggestion for Effective Query Formulation in Systematic Reviews Literature Search. Intelligent Systems with Applications (2022), 200141.
[81] Huaying Wu, Tingting Wang, Jiayi Chen, Su Chen, Qinmin Hu, and Liang He. 2018. Ecnu at 2018 Ehealth Task 2: Technologically Assisted Reviews in Empirical Medicine. Methods-a Companion to Methods in Enzymology 4, 5 (2018), 7.
[82] Zhe Yu and Tim Menzies. 2017. Data Balancing for Technologically Assisted Reviews: Undersampling or Reweighting.. In CEUR Workshop Proceedings: Working Notes of CLEF 2017: Conference and Labs of the Evaluation Forum.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We validated this by issuing our prompts, as well as through other interactions, across separate "chats" to avoid ChatGPT to use the previous input and output as part of the input for the current interaction.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
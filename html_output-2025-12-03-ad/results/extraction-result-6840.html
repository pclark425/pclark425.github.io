<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6840 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6840</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6840</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-788da23e46632cca4696a4a8286e2ea9b33a1b46</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/788da23e46632cca4696a4a8286e2ea9b33a1b46" target="_blank">GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> GPT-MolBERTa is presented, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties and performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks.</p>
                <p><strong>Paper Abstract:</strong> With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPT-MolBERTa performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks. Additionally, further analysis of the attention mechanisms show that GPT-MolBERTa is able to pick up important information from the input textual data, displaying the interpretability of the model.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6840.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6840.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI ChatGPT (GPT-3.5) used for textual description generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only large language model (ChatGPT, GPT-3.5 family) used in this work as an external API to convert SMILES into rich, human-readable textual descriptions of molecules via prompt engineering; these descriptions are used as the pretraining corpus for downstream molecular property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (OpenAI GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, API prompt-based generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in paper (external OpenAI model); used as a black-box generator to convert SMILES into detailed textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-engineered, few-shot/zero-shot text generation via ChatGPT API: authors send SMILES with priming prompt "You are able to generate important and verifiable features about molecular SMILES" and then "Generate a description about the following SMILES molecule ..." to obtain per-molecule textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Input: SMILES (canonicalization applied downstream); Output: detailed textual descriptions (natural language) that include functional groups, atomic composition, inferred properties, and sometimes the SMILES embedded in text.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Creation of a textual corpus to pretrain a transformer encoder for molecular property prediction (drug discovery / molecular property modeling), not for de novo molecule synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Post-filtering: responses with fewer than 100 tokens removed; non-canonical SMILES removed from dataset (overall ~0.14% removed). No explicit chemical validity/synthetic-accessibility filters applied to ChatGPT outputs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>ChatGPT used as an external text generator (API). Generated corpus then processed with tokenizers and used to pretrain RoBERTa/BERT via HuggingFace/PyTorch. No docking, QM, retrosynthesis, or other chemistry simulators integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Inputs to ChatGPT were SMILES drawn from 14 MoleculeNet datasets; approximately 326,000 textual descriptions were generated corresponding to those molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>No direct evaluation metrics reported for ChatGPT-generated text quality beyond token-length filtering; downstream evaluation used property-prediction metrics (ROC-AUC for classification, RMSE for regression).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Not applicable for ChatGPT text generation itself (no numeric quality metrics reported); downstream models trained on the ChatGPT-generated corpus achieved the reported property-prediction numbers (see GPT-MolBERTa entry).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper does not report systematic evaluation of ChatGPT textual accuracy or hallucination rates; only trivial filtering (minimum token length) was applied. The authors note that SMILES and textual descriptions may lack stereochemical/3D detail and that generated textual data quality could affect downstream performance; no chemical-validity checking of generated text was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6840.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6840.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-MolBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-MolBERTa (GPT Molecular Features; RoBERTa encoder pretrained on ChatGPT-generated molecular descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised chemical language model in which ~326k ChatGPT-generated textual descriptions of molecules (from MoleculeNet SMILES) are used to pretrain a RoBERTa (and BERT for comparison) encoder via masked language modeling; the pretrained encoder is then fine-tuned with a classification/regression head for molecular property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-MolBERTa (RoBERTa encoder pretrained on ChatGPT-generated descriptions; BERT variant also evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Encoder-only transformer (RoBERTa/BERT) pretrained with masked language modeling on textual corpus, then fine-tuned for downstream prediction tasks (classification/regression).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretraining corpus: ~326,000 natural-language textual descriptions generated by ChatGPT from SMILES in 14 MoleculeNet datasets; authors trained a tokenizer (RoBERTa Byte-Level BPE and BERT WordPiece) specifically on this molecular-text corpus and pretrained via MLM.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>No de novo molecule generation reported; generation here refers to (1) ChatGPT generating textual descriptions from given SMILES via prompt engineering, and (2) masked-language-model self-supervised pretraining (15% token masking with dynamic masking for RoBERTa) to learn representations.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Primary representation for model input: rich natural-language textual descriptions (that include references to SMILES, functional groups, atom types, etc.). The original SMILES strings were used to prompt ChatGPT and also appear within descriptions; model does not use graph or explicit 3D coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecular property prediction benchmarks (drug-discovery relevant tasks) — classification datasets: BBBP, Tox21, ClinTox, HIV, BACE, SIDER; regression datasets: ESOL, FreeSolv, Lipophilicity.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Data curation constraints: removed ChatGPT responses with fewer than 100 tokens; removed non-canonical SMILES; scaffold split (Murcko scaffold) used for train/val/test (80/10/10) to create challenging splits. Early stopping during finetuning. No explicit generative constraints (SAS, toxicity) because model is not used to generate new molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Pipeline uses ChatGPT for corpus generation (external API), HuggingFace transformers and tokenizers, and PyTorch for model training; no chemistry simulation tools (docking, QM), retrosynthesis planners, or property predictors were integrated as part of generation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Pretraining corpus derived from SMILES drawn from 14 MoleculeNet datasets (total ~326k descriptions). Fine-tuning/evaluation used MoleculeNet benchmark datasets: ESOL (1128), FreeSolv (642), Lipophilicity (4200), HIV (41127), BACE (1513), BBBP (2039), Tox21 (7831), SIDER (1427), ClinTox (1478).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream evaluation: ROC-AUC for classification tasks; RMSE for regression tasks. Additional evaluation procedures: averages and standard deviations over 3 runs; scaffold splitting; early stopping.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Classification (ROC-AUC, GPT-MolBERTa RoBERTa): BBBP 74.1 ± 0.15, Tox21 65.9 ± 0.06, ClinTox 49.7 ± 0.12, HIV 75.5 ± 1.29, BACE 73.4 ± 0.47, SIDER 58.5 ± 0.35. Regression (RMSE, GPT-MolBERTa RoBERTa): FreeSolv 0.896 ± 0.02, ESOL 0.477 ± 0.01, Lipophilicity 0.758 ± 0.01. The model outperforms several baselines on regression (notably FreeSolv and ESOL) and shows moderate performance on classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reported limitations include: (1) pretraining dataset (~326k) is small relative to other chemical-language models pretrained on millions of molecules, likely limiting peak performance; (2) textual descriptions and SMILES may not encode stereochemistry or full 3D/geomtric information, potentially limiting property prediction accuracy; (3) classification performance is only moderate compared to some GNNs; (4) no wet-lab synthesis or experimental validation of generated compounds (model not used to design/synthesize new molecules); (5) authors did not systematically assess accuracy/faithfulness/hallucination in ChatGPT-generated descriptions, which could introduce noise; (6) no integration with property-prediction or synthesis-constraining tools was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GuacaMol: Benchmarking Models for de Novo Molecular Design <em>(Rating: 2)</em></li>
                <li>Automatic chemical design using a data-driven continuous representation of molecules <em>(Rating: 2)</em></li>
                <li>Chemformer: a pre-trained transformer for computational chemistry <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6840",
    "paper_id": "paper-788da23e46632cca4696a4a8286e2ea9b33a1b46",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "ChatGPT (GPT-3.5)",
            "name_full": "OpenAI ChatGPT (GPT-3.5) used for textual description generation",
            "brief_description": "A decoder-only large language model (ChatGPT, GPT-3.5 family) used in this work as an external API to convert SMILES into rich, human-readable textual descriptions of molecules via prompt engineering; these descriptions are used as the pretraining corpus for downstream molecular property prediction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (OpenAI GPT-3.5)",
            "model_type": "decoder-only LLM, API prompt-based generation",
            "model_size": null,
            "training_data_description": "Not specified in paper (external OpenAI model); used as a black-box generator to convert SMILES into detailed textual descriptions.",
            "generation_method": "Prompt-engineered, few-shot/zero-shot text generation via ChatGPT API: authors send SMILES with priming prompt \"You are able to generate important and verifiable features about molecular SMILES\" and then \"Generate a description about the following SMILES molecule ...\" to obtain per-molecule textual descriptions.",
            "chemical_representation": "Input: SMILES (canonicalization applied downstream); Output: detailed textual descriptions (natural language) that include functional groups, atomic composition, inferred properties, and sometimes the SMILES embedded in text.",
            "target_application": "Creation of a textual corpus to pretrain a transformer encoder for molecular property prediction (drug discovery / molecular property modeling), not for de novo molecule synthesis.",
            "constraints_used": "Post-filtering: responses with fewer than 100 tokens removed; non-canonical SMILES removed from dataset (overall ~0.14% removed). No explicit chemical validity/synthetic-accessibility filters applied to ChatGPT outputs reported.",
            "integration_with_external_tools": "ChatGPT used as an external text generator (API). Generated corpus then processed with tokenizers and used to pretrain RoBERTa/BERT via HuggingFace/PyTorch. No docking, QM, retrosynthesis, or other chemistry simulators integrated.",
            "dataset_used": "Inputs to ChatGPT were SMILES drawn from 14 MoleculeNet datasets; approximately 326,000 textual descriptions were generated corresponding to those molecules.",
            "evaluation_metrics": "No direct evaluation metrics reported for ChatGPT-generated text quality beyond token-length filtering; downstream evaluation used property-prediction metrics (ROC-AUC for classification, RMSE for regression).",
            "reported_results": "Not applicable for ChatGPT text generation itself (no numeric quality metrics reported); downstream models trained on the ChatGPT-generated corpus achieved the reported property-prediction numbers (see GPT-MolBERTa entry).",
            "experimental_validation": false,
            "challenges_or_limitations": "Paper does not report systematic evaluation of ChatGPT textual accuracy or hallucination rates; only trivial filtering (minimum token length) was applied. The authors note that SMILES and textual descriptions may lack stereochemical/3D detail and that generated textual data quality could affect downstream performance; no chemical-validity checking of generated text was reported.",
            "uuid": "e6840.0",
            "source_info": {
                "paper_title": "GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-MolBERTa",
            "name_full": "GPT-MolBERTa (GPT Molecular Features; RoBERTa encoder pretrained on ChatGPT-generated molecular descriptions)",
            "brief_description": "A self-supervised chemical language model in which ~326k ChatGPT-generated textual descriptions of molecules (from MoleculeNet SMILES) are used to pretrain a RoBERTa (and BERT for comparison) encoder via masked language modeling; the pretrained encoder is then fine-tuned with a classification/regression head for molecular property prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-MolBERTa (RoBERTa encoder pretrained on ChatGPT-generated descriptions; BERT variant also evaluated)",
            "model_type": "Encoder-only transformer (RoBERTa/BERT) pretrained with masked language modeling on textual corpus, then fine-tuned for downstream prediction tasks (classification/regression).",
            "model_size": null,
            "training_data_description": "Pretraining corpus: ~326,000 natural-language textual descriptions generated by ChatGPT from SMILES in 14 MoleculeNet datasets; authors trained a tokenizer (RoBERTa Byte-Level BPE and BERT WordPiece) specifically on this molecular-text corpus and pretrained via MLM.",
            "generation_method": "No de novo molecule generation reported; generation here refers to (1) ChatGPT generating textual descriptions from given SMILES via prompt engineering, and (2) masked-language-model self-supervised pretraining (15% token masking with dynamic masking for RoBERTa) to learn representations.",
            "chemical_representation": "Primary representation for model input: rich natural-language textual descriptions (that include references to SMILES, functional groups, atom types, etc.). The original SMILES strings were used to prompt ChatGPT and also appear within descriptions; model does not use graph or explicit 3D coordinates.",
            "target_application": "Molecular property prediction benchmarks (drug-discovery relevant tasks) — classification datasets: BBBP, Tox21, ClinTox, HIV, BACE, SIDER; regression datasets: ESOL, FreeSolv, Lipophilicity.",
            "constraints_used": "Data curation constraints: removed ChatGPT responses with fewer than 100 tokens; removed non-canonical SMILES; scaffold split (Murcko scaffold) used for train/val/test (80/10/10) to create challenging splits. Early stopping during finetuning. No explicit generative constraints (SAS, toxicity) because model is not used to generate new molecules.",
            "integration_with_external_tools": "Pipeline uses ChatGPT for corpus generation (external API), HuggingFace transformers and tokenizers, and PyTorch for model training; no chemistry simulation tools (docking, QM), retrosynthesis planners, or property predictors were integrated as part of generation.",
            "dataset_used": "Pretraining corpus derived from SMILES drawn from 14 MoleculeNet datasets (total ~326k descriptions). Fine-tuning/evaluation used MoleculeNet benchmark datasets: ESOL (1128), FreeSolv (642), Lipophilicity (4200), HIV (41127), BACE (1513), BBBP (2039), Tox21 (7831), SIDER (1427), ClinTox (1478).",
            "evaluation_metrics": "Downstream evaluation: ROC-AUC for classification tasks; RMSE for regression tasks. Additional evaluation procedures: averages and standard deviations over 3 runs; scaffold splitting; early stopping.",
            "reported_results": "Classification (ROC-AUC, GPT-MolBERTa RoBERTa): BBBP 74.1 ± 0.15, Tox21 65.9 ± 0.06, ClinTox 49.7 ± 0.12, HIV 75.5 ± 1.29, BACE 73.4 ± 0.47, SIDER 58.5 ± 0.35. Regression (RMSE, GPT-MolBERTa RoBERTa): FreeSolv 0.896 ± 0.02, ESOL 0.477 ± 0.01, Lipophilicity 0.758 ± 0.01. The model outperforms several baselines on regression (notably FreeSolv and ESOL) and shows moderate performance on classification tasks.",
            "experimental_validation": false,
            "challenges_or_limitations": "Reported limitations include: (1) pretraining dataset (~326k) is small relative to other chemical-language models pretrained on millions of molecules, likely limiting peak performance; (2) textual descriptions and SMILES may not encode stereochemistry or full 3D/geomtric information, potentially limiting property prediction accuracy; (3) classification performance is only moderate compared to some GNNs; (4) no wet-lab synthesis or experimental validation of generated compounds (model not used to design/synthesize new molecules); (5) authors did not systematically assess accuracy/faithfulness/hallucination in ChatGPT-generated descriptions, which could introduce noise; (6) no integration with property-prediction or synthesis-constraining tools was reported.",
            "uuid": "e6840.1",
            "source_info": {
                "paper_title": "GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GuacaMol: Benchmarking Models for de Novo Molecular Design",
            "rating": 2,
            "sanitized_title": "guacamol_benchmarking_models_for_de_novo_molecular_design"
        },
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "rating": 2,
            "sanitized_title": "automatic_chemical_design_using_a_datadriven_continuous_representation_of_molecules"
        },
        {
            "paper_title": "Chemformer: a pre-trained transformer for computational chemistry",
            "rating": 1,
            "sanitized_title": "chemformer_a_pretrained_transformer_for_computational_chemistry"
        }
    ],
    "cost": 0.01064725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GPT-MolBERTa: GPT Molecular Features</h1>
<h2>Language Model for molecular property</h2>
<h2>prediction</h2>
<p>Suryanarayanan Balaji, ${ }^{\dagger}$ Rishikesh Magar, ${ }^{\ddagger}$ Yayati Jadhav, ${ }^{\ddagger}$ and Amir Barati<br>Farimani ${ }^{*}, \ddagger, \dagger$<br>$\dagger$ Department of Chemical Engineering, Carnegie Mellon University, Pittsburgh PA, USA 15213<br>$\ddagger$ Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh PA, USA 15213<br>E-mail: barati@cmu.edu</p>
<h4>Abstract</h4>
<p>With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a selfsupervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPTMolBERTa performs well on various molecule property benchmarks, and approaching</p>
<p>state of the art performance in regression tasks. Additionally, further analysis of the attention mechanisms show that GPT-MolBERTa is able to pick up important information from the input textual data, displaying the interpretability of the model.</p>
<h2>Introduction</h2>
<p>Molecular property prediction is vital for drug discovery, guiding compound selection, evaluation, and generation, [1, 2, 3] however, experiments to determine molecular properties can be very expensive and time consuming. Computational techniques such as machine learning (ML) can be an excellent approach to predict the properties since they are fast and can directly map the molecules to their properties. To develop accurate molecular property prediction models, an important factor to consider is the molecular representation, which involves encoding chemical compounds for computational analysis. [4] Different methods for representing molecules include SMILES, [5] graph-based representations, [6] and molecular fingerprints. [7, 8] Typically, Graph Neural Networks (GNNs) [9, 10, 11, 12, 13, 14, 15, 16] show superior performance, capturing detailed geometric and atomic neighborhood information. However, their interpretability can be limited. In contrast, SMILES, a string-based representation, stands out for its simplicity and adaptability. [17, 18, 19, 20, 21, 22, 23] The inherent string based nature of SMILES makes them well-suited for transformer-like architectures, and this is further enhanced by the availability of extensive databases for training [24, 25]</p>
<p>Transformers, initially developed for natural language processing tasks, [26, 27, 28, 29, 30, 31, 32] are now increasingly being exploited in molecular ML. Transformer models that utilize SMILES as inputs have emerged for molecular property prediction and generation. [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47] Their sequential data processing capability, combined with the inherent attention mechanism, provides some level of interpretability. While SMILES play a significant role in the areas of molecular property prediction, modeling, and design, [9, 48, 49, 50, 51, 52] they have inherent limitations. SMILES are non-canonical in nature, where a single SMILES string could represent multiple molecules. [17] Additionally, SMILES fail to encode the topographical information of the molecule such</p>
<p>as 3D structure and stereochemistry, limiting the performance of many machine learning models. ${ }^{53}$ Considering these challenges, it prompts the question: Can we develop a representation that maintains the simplicity of SMILES, yet embeds explicit details about a molecule's structural attributes and potentially incorporates geometric insights? Developing such a representation could enhance the performance of transformer models in molecular property prediction.</p>
<p>Models like MatBERT ${ }^{54}$ and MatSciBERT, ${ }^{55}$ pretrained on material science tasks, show promise in property predictions. These results imply that domain-specific pretraining might be more beneficial than just using larger transformer models. However, using molecular domain papers for pretraining might not provide specific information. An alternative is generating unique text descriptions for SMILES molecules using large language models, with models like ChatGPT ${ }^{56,57}$ showing potential in this area.</p>
<p>Textual descriptions give a broader look at molecules, covering everything from basic atomic details to complex geometric information and interactions. SMILES notation is good at providing an overall view of the molecule, however, textual description provides more details and is more comprehensive. For example, for water molecule, hundred pages of information is available (for example, this text is taken from water molecule from Wikipedia page: "Water is an inorganic compound with the chemical formula H2O. It is a transparent, tasteless, odorless, and nearly colorless chemical substance, and it is the main constituent of Earth's hydrosphere and the fluids of all known living organisms, the bond angle between ..."). The depth provided by text might help improve how we model and predict molecular properties, blending the best of both SMILES and geometry based graphs representations.</p>
<p>In this paper, we introduce GPT-MolBERTa (GPT Molecule-RoBERTa), a chemical language model that leverages molecular text descriptions as inputs for downstream molecular property prediction tasks. The text descriptions were generated through the use of generative large language models, in this case OpenAI's ${ }^{58}$ ChatGPT. Figure 1 provides an overview of the methodology used in this paper. The initial step involves sending SMILES strings</p>
<p>into ChatGPT, where they are used to generate rich textual descriptions through the use of an optimized prompt. These descriptions include details about functional groups, shape, and chemical properties and are consolidated into a text corpus. This corpus is subsequently used as input for a RoBERTa model, upon which a classification/regression head was added for molecular property prediction. The model was pretrained on the text descriptions of approximately 326,000 molecules sourced from MoleculeNet ${ }^{59}$ and evaluated on MoleculeNet's benchmark datasets.</p>
<p>Notably, GPT-MolBERTa demonstrates strong overall performance, approaching state-of-the-art levels in regression tasks. The promising aspect is that the model's pretraining was conducted with only around 300,000 molecules, a significantly smaller dataset compared to other models that use millions of molecules. This finding suggests that pretraining with a comparable number of text descriptions holds the potential to improve molecular property prediction, offering exciting possibilities for future research and applications.</p>
<h1>Methods</h1>
<p>GPT-MolBERTa consists of two sections, namely the data generation and the model pretraining and finetuning (Figure 1). Text descriptions of each molecule were first generated through the use of ChatGPT. These were used to train a self-supervised transformer based encoder model, which was used to extract high dimensional vector representation of the molecular descriptions. This pretrained model was then finetuned on the downstream molecular property prediction benchmarks from MoleculeNet, through the addition of a regression or classification head onto the architecture. Both pretrained LLM models of BERT and RoBERTa were explored to compare their efficiency in learning. The complete model implementation was executed using HuggingFace ${ }^{60}$ within the PyTorch framework.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of GPT-MolBERTa. SMILES strings are sent to ChatGPT, which generates rich textual descriptions consisting of information about functional groups, molecular weight, density, and other properties. These descriptions are then used to pretrain a RoBERTa model. The model is then fine-tuned on MoleculeNet datasets, with the addition of a classification/regression head to the first token embeddings.</p>
<h1>Dataset Generation and Curation</h1>
<p>We generate the textual descriptions of SMILES molecules through the use of ChatGPT$3.5 .{ }^{58}$ We conduct prompt engineering to obtain the specific prompt which will generate meaningful data. Information about the molecular structures, atomic weights, functional groups were all obtained from the input. Based on our best practices and prompt engineering, we found out that priming the ChatGPT with the prompt " You are able to generate important and verifiable features about molecular SMILES" and then prompting " Generate a description about the following SMILES molecule ..." results in the best description of molecules. (Figure 1) These descriptions were subsequently added to the original dataset, with insignificant responses (comprising fewer than 100 tokens) being excluded. Approximately 326000 text description of molecules were obtained from 14 datasets present in MoleculeNet. ${ }^{59}$</p>
<h2>Tokenization</h2>
<p>Tokenization is a fundamental step in text processing ${ }^{61}$ which involves breaking input text into indivisible units. RoBERTa employs Byte Level Byte-Pair Encoding, ensuring no unknown tokens. ${ }^{62}$ We followed RoBERTa protocols when tokenizing inputs for our model. After tokenization, the tokens are further processed into input embeddings. Positional encoding is added, embedding token positions in the sequence for tasks like prediction and generation.</p>
<h2>Transformer Model</h2>
<p>The GPT MolBerta's transformer encoder consists of multiple stacked layers each consisting of multi-head self attention layers followed by feed-forward networks. The input data is tokenized and positionally encoded before transformed into embeddings. These embeddings will be sent to the self-attention layers which will extract context information of the input</p>
<p>data followed by a feed-forward layer to obtain the final representations.
The embeddings are subsequently passed on to the attention layers. Both BERT ${ }^{31}$ and RoBERTa ${ }^{32}$ models use the multi-head scaled dot product attention mechanism, allowing for parallel processing of input tokens. In the self-attention mechanism, each token in the sequence is projected into its corresponding query, key and value vector ( $\mathrm{Q}, \mathrm{K}$ and V ) through the use of learnable weight matrices $\left(W_{Q}, W_{K}\right.$ and $\left.W_{V}\right)$. Given that $d_{k}$ is the dimension of Q and K , the scaled dot-product attention A for a single head is calculated by the following equation. ${ }^{26}$</p>
<p>$$
\text { Attention }(\mathrm{Q}, \mathrm{~K}, \mathrm{~V})=\text { Softmax }\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$</p>
<p>Multi-head attention conducts these calculations in parallel across different heads, allowing for the model to jointly attend to information from different representation subspaces at different positions. ${ }^{26}$ These outputs will be concatenated and projected into the output embedding with the same size as the input embedding. The operation is shown below.</p>
<p>$$
\begin{aligned}
&amp; \text { MultiHead }(\mathrm{Q}, \mathrm{~K}, \mathrm{~V})=\text { Concat }\left(\text { Head }<em h="h">{1}, \ldots, \text { Head }</em> \
&amp; \quad \text { where } \text { Head }}\right) W^{O<em i="i">{i}=\text { Attention }\left(Q W</em>\right)
\end{aligned}
$$}^{Q}, K W_{i}^{K}, V W_{i}^{V</p>
<p>The outputs would then be sent to a feed-forward network to transform the attention-derived features. Layer normalization and residual connections are employed throughout the encoder layer to enhance training stability and convergence speed. Multiple encoder layers are used to improve the quality of the embeddings obtained.</p>
<h1>Pretraining</h1>
<p>Pretraining a large language model involves training the model on an extensive corpus of data before finetuning it on a specific downstream task. In this study, we pretrained both the BERT and RoBERTa tokenizers using a molecular text corpus to extract their specific vocabularies. While both BERT and RoBERTa have already been pretrained on the</p>
<p>BooksCorpus ${ }^{63}$ and English Wikipedia, we opted to train our own tokenizer as it would be tuned specifically to the vocabulary in our dataset, allowing for better identification of tokens. In the case of the RoBERTa tokenizer, the special tokens were post processed to ensure parity with the existing RoBERTa vocabulary.</p>
<p>Following tokenization, the models were pretrained in a self-supervised ${ }^{64-66}$ manner using masked language modeling, ${ }^{31}$ where $15 \%$ of the input tokens were masked and the model would predict the masked tokens using bidirectional context. RoBERTa goes one step further through introducing dynamic token masking, where different tokens in the sequence are masked per epoch. Masked language modeling allows the model to learn meaningful representation of the input data, without the need for labels.</p>
<h1>Finetuning</h1>
<p>GPT-MolBERTa was finetuned on property prediction tasks from several benchmark datasets present in MoleculeNet, the details of which are given in Table 1. The final molecular property prediction will be conducted through adding a classification or regression head to the embeddings from the first token. Early stopping was also implemented if the validation loss does not show improvement over a specified number of epochs, minimizing the risk of overfitting.</p>
<p>For a given dataset, we first removed all non-canonical SMILES strings. This filtering along with the earlier removal of insignificant responses accounts for $0.14 \%$ of the dataset. We use the binary cross-entropy and root mean squared error (RMSE) as our loss functions and the Adam optimizer for our training. All benchmarks were scaffold split in the ratio of 80/10/10 to match the standards used in MoleculeNet. Scaffold splitting splits molecules according to their Murcko Scaffolds, making the train and test datasets as dissimilar to each other, resulting in a more challenging task. For datasets involving multiple labels, we adopted a consistent methodology: training and validation were conducted for each label using the same model. The subsequent averages were then computed and reported for both training</p>
<p>and testing phases. This process was repeated three times per dataset to determine average and standard deviation performance on the test set. Hyper-parameters are shown in S4 of Supplementary Information.</p>
<p>Table 1: Datasets from the MoleculeNet used for finetuning tasks. We finetune our model on three regression datasets and 6 classification datasets.</p>
<table>
<thead>
<tr>
<th>Task (Metric)</th>
<th>Dataset</th>
<th># Molecules</th>
</tr>
</thead>
<tbody>
<tr>
<td>Regression(RMSE)</td>
<td>ESOL</td>
<td>1128</td>
</tr>
<tr>
<td></td>
<td>FreeSolv</td>
<td>642</td>
</tr>
<tr>
<td></td>
<td>Lipophilicity</td>
<td>4200</td>
</tr>
<tr>
<td>Classification (ROC-AUC)</td>
<td>HIV</td>
<td>41127</td>
</tr>
<tr>
<td></td>
<td>BACE</td>
<td>1513</td>
</tr>
<tr>
<td></td>
<td>BBBP</td>
<td>2039</td>
</tr>
<tr>
<td></td>
<td>Tox21</td>
<td>7831</td>
</tr>
<tr>
<td></td>
<td>SIDER</td>
<td>1427</td>
</tr>
<tr>
<td></td>
<td>ClinTox</td>
<td>1478</td>
</tr>
</tbody>
</table>
<h1>Results and Discussion</h1>
<p>To evaluate GPT-MolBERTa's effectiveness, we conducted a comprehensive benchmark on various classification and regression tasks from the MoleculeNet datasets. The results are summarized in Table 2, comparing the model's test area under the curve (AUC) to baseline models. The averages and standard deviations from three runs were reported, with two models used for comparison.</p>
<h2>MoleculeNet Benchmark</h2>
<p>The MoleculeNet dataset tasks are divided into regression and classification categories. For classification, we evaluate six datasets: BBBP, Tox21, ClinTox, HIV, BACE, and SIDER, each highlighting different molecular properties. Additionally, we also consider three regression datasets: ESOL, FreeSOLV, and Lipophilicity.</p>
<p>In terms of classification tasks, our model's performance aligns with other baseline models utilizing string-based representations(Table 2). Additionally, our results are consistent with some graph neural networks, such as GIN ${ }^{67}$ and GCN. ${ }^{68}$ Overall, our model demonstrates moderate success across the benchmark classification datasets when compared to both GNN and string-based model baselines. It's noteworthy that GPT-MolBERTa was pretrained on a dataset of 326,000 points, smaller than other baselines that were pretrained on datasets an order of magnitude larger. We believe that pretraining on a more extensive corpus might enhance our framework's downstream performance.</p>
<p>We assessed our model's performance on the MoleculeNet regression tasks, with results presented in Table 3. This table lists the root mean squared error (RMSE) for each regression dataset. While GPT-MolBERTa posts solid results in classification, it truly stands out in regression tasks. Specifically, it outperforms other GNN models and baseline models that use string-based representations, especially in the FreeSolv and ESOL datasets. For the Lipophilicity dataset, GPT-MolBERTa's performance aligns closely with other baselines. Notably, our model registers a performance gain of $5.88 \%$ over the top-performing baseline, MolBERT, for the FreeSolv dataset, and an $11.32 \%$ improvement for the ESOL dataset.</p>
<p>Table 2: Classification Benchmarks on MoleculeNet. We benchmark the model against standard GNN baseline as well as transformer baselines. The evaluation metric used for classification tasks is ROC-AUC. The best performing result among the string representation based approaches has been shown in boldface and the best performing GNN result has been italicized.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">BBBP</th>
<th style="text-align: center;">Tox 21</th>
<th style="text-align: center;">ClinTox</th>
<th style="text-align: center;">HIV</th>
<th style="text-align: center;">BACE</th>
<th style="text-align: center;">SIDER</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GCN $^{68}$</td>
<td style="text-align: center;">$71.9 \pm 0.9$</td>
<td style="text-align: center;">$70.9 \pm 2.6$</td>
<td style="text-align: center;">$62.5 \pm 2.8$</td>
<td style="text-align: center;">$74.0 \pm 3.0$</td>
<td style="text-align: center;">$71.6 \pm 2.0$</td>
<td style="text-align: center;">$53.6 \pm 3.2$</td>
</tr>
<tr>
<td style="text-align: center;">GIN $^{67}$</td>
<td style="text-align: center;">$65.8 \pm 4.5$</td>
<td style="text-align: center;">$74.0 \pm 0.8$</td>
<td style="text-align: center;">$58.0 \pm 4.4$</td>
<td style="text-align: center;">$75.3 \pm 1.9$</td>
<td style="text-align: center;">$70.1 \pm 5.4$</td>
<td style="text-align: center;">$57.3 \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;">SchNet ${ }^{13}$</td>
<td style="text-align: center;">$84.8 \pm 2.2$</td>
<td style="text-align: center;">$77.2 \pm 2.3$</td>
<td style="text-align: center;">$71.5 \pm 3.7$</td>
<td style="text-align: center;">$70.2 \pm 3.4$</td>
<td style="text-align: center;">$76.6 \pm 1.1$</td>
<td style="text-align: center;">$53.9 \pm 3.7$</td>
</tr>
<tr>
<td style="text-align: center;">MGCN ${ }^{14}$</td>
<td style="text-align: center;">$85.0 \pm 6.4$</td>
<td style="text-align: center;">$70.7 \pm 1.6$</td>
<td style="text-align: center;">$63.4 \pm 4.2$</td>
<td style="text-align: center;">$73.8 \pm 1.6$</td>
<td style="text-align: center;">$73.4 \pm 3.0$</td>
<td style="text-align: center;">$55.2 \pm 1.8$</td>
</tr>
<tr>
<td style="text-align: center;">D-MPNN ${ }^{11}$</td>
<td style="text-align: center;">$71.2 \pm 3.8$</td>
<td style="text-align: center;">$68.9 \pm 1.3$</td>
<td style="text-align: center;">$90.5 \pm 5.3$</td>
<td style="text-align: center;">$75.0 \pm 2.1$</td>
<td style="text-align: center;">$85.3 \pm 5.3$</td>
<td style="text-align: center;">$63.2 \pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;">Hu et al. ${ }^{69}$</td>
<td style="text-align: center;">$70.8 \pm 1.5$</td>
<td style="text-align: center;">$78.7 \pm 0.4$</td>
<td style="text-align: center;">$78.9 \pm 2.4$</td>
<td style="text-align: center;">$80.2 \pm 0.9$</td>
<td style="text-align: center;">$85.9 \pm 0.8$</td>
<td style="text-align: center;">$65.2 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;">MolCLR-GCN ${ }^{53}$</td>
<td style="text-align: center;">$73.8 \pm 0.2$</td>
<td style="text-align: center;">$74.7 \pm 0.8$</td>
<td style="text-align: center;">$86.7 \pm 1.0$</td>
<td style="text-align: center;">$77.8 \pm 0.5$</td>
<td style="text-align: center;">$78.8 \pm 0.5$</td>
<td style="text-align: center;">$66.9 \pm 1.2$</td>
</tr>
<tr>
<td style="text-align: center;">MolCLR-GIN ${ }^{53}$</td>
<td style="text-align: center;">$73.6 \pm 0.5$</td>
<td style="text-align: center;">$79.8 \pm 0.7$</td>
<td style="text-align: center;">$93.2 \pm 1.7$</td>
<td style="text-align: center;">$80.6 \pm 1.1$</td>
<td style="text-align: center;">$89.0 \pm 0.3$</td>
<td style="text-align: center;">$68.0 \pm 1.1$</td>
</tr>
<tr>
<td style="text-align: center;">MolBERT ${ }^{33}$</td>
<td style="text-align: center;">$76.2 \pm 0.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$78.3 \pm 0.0$</td>
<td style="text-align: center;">$86.6 \pm 0.0$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ChemBERTa-2 ${ }^{39}$</td>
<td style="text-align: center;">$72.8 \pm 0.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$62.2 \pm 0.0$</td>
<td style="text-align: center;">$79.9 \pm 0.0$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CLM $^{43}$</td>
<td style="text-align: center;">$91.5 \pm 0.0$</td>
<td style="text-align: center;">$79.5 \pm 0.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$81.3 \pm 0.0$</td>
<td style="text-align: center;">$86.1 \pm 0.0$</td>
<td style="text-align: center;">$61.9 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">SELFormer ${ }^{41}$</td>
<td style="text-align: center;">$90.2 \pm 0.0$</td>
<td style="text-align: center;">$65.3 \pm 0.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$68.1 \pm 0.0$</td>
<td style="text-align: center;">$83.2 \pm 0.0$</td>
<td style="text-align: center;">$74.5 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-MolBERTa</td>
<td style="text-align: center;">$74.1 \pm 0.15$</td>
<td style="text-align: center;">$65.9 \pm 0.06$</td>
<td style="text-align: center;">$49.7 \pm 0.12$</td>
<td style="text-align: center;">$75.5 \pm 1.29$</td>
<td style="text-align: center;">$73.4 \pm 0.47$</td>
<td style="text-align: center;">$58.5 \pm 0.35$</td>
</tr>
</tbody>
</table>
<p>Table 3: Regression Benchmarks on MoleculeNet. We benchmark the model against standard GNN baseline as well as transformer baselines. The evaluation metric used for regression tasks is RMSE. The best performing result among the string representation based approaches has been shown in boldface and the best performing GNN result has been italicized.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">FreeSolv</th>
<th style="text-align: left;">ESOL</th>
<th style="text-align: left;">Lipophilicity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GCN $^{68}$</td>
<td style="text-align: left;">$2.87 \pm 0.14$</td>
<td style="text-align: left;">$1.43 \pm 0.05$</td>
<td style="text-align: left;">$0.85 \pm 0.08$</td>
</tr>
<tr>
<td style="text-align: left;">GIN $^{67}$</td>
<td style="text-align: left;">$2.76 \pm 0.18$</td>
<td style="text-align: left;">$1.45 \pm 0.02$</td>
<td style="text-align: left;">$0.85 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: left;">SchNet ${ }^{13}$</td>
<td style="text-align: left;">$3.22 \pm 0.76$</td>
<td style="text-align: left;">$1.05 \pm 0.06$</td>
<td style="text-align: left;">$0.91 \pm 0.10$</td>
</tr>
<tr>
<td style="text-align: left;">MGCN ${ }^{14}$</td>
<td style="text-align: left;">$3.35 \pm 0.01$</td>
<td style="text-align: left;">$1.27 \pm 0.15$</td>
<td style="text-align: left;">$1.11 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: left;">D-MPNN ${ }^{11}$</td>
<td style="text-align: left;">$2.18 \pm 0.91$</td>
<td style="text-align: left;">$0.98 \pm 0.26$</td>
<td style="text-align: left;">$0.65 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: left;">Hu et al. ${ }^{69}$</td>
<td style="text-align: left;">$2.83 \pm 0.12$</td>
<td style="text-align: left;">$1.22 \pm 0.02$</td>
<td style="text-align: left;">$0.74 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">MolCLR-GCN ${ }^{53}$</td>
<td style="text-align: left;">$2.39 \pm 0.14$</td>
<td style="text-align: left;">$1.16 \pm 0.00$</td>
<td style="text-align: left;">$0.78 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: left;">MolCLR-GIN ${ }^{53}$</td>
<td style="text-align: left;">$2.20 \pm 0.20$</td>
<td style="text-align: left;">$1.11 \pm 0.01$</td>
<td style="text-align: left;">$0.65 \pm 0.08$</td>
</tr>
<tr>
<td style="text-align: left;">MolBERT ${ }^{33}$</td>
<td style="text-align: left;">$0.948 \pm 0.33$</td>
<td style="text-align: left;">$0.531 \pm 0.04$</td>
<td style="text-align: left;">$\mathbf{0 . 5 6 1} \pm \mathbf{0 . 0 3}$</td>
</tr>
<tr>
<td style="text-align: left;">ChemBERTa-2 ${ }^{39}$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$0.798 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">ChemFormer ${ }^{35}$</td>
<td style="text-align: left;">$1.23 \pm 0.00$</td>
<td style="text-align: left;">$0.633 \pm 0.00$</td>
<td style="text-align: left;">$0.598 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">SELFormer ${ }^{41}$</td>
<td style="text-align: left;">$2.797 \pm 0.00$</td>
<td style="text-align: left;">$0.682 \pm 0.00$</td>
<td style="text-align: left;">$0.735 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-MolBERTa</td>
<td style="text-align: left;">$\mathbf{0 . 8 9 6} \pm \mathbf{0 . 0 2}$</td>
<td style="text-align: left;">$\mathbf{0 . 4 7 7} \pm \mathbf{0 . 0 1}$</td>
<td style="text-align: left;">$0.758 \pm 0.01$</td>
</tr>
</tbody>
</table>
<h1>Effect of the Transformer Encoder</h1>
<p>After observing GPT-MolBERTa's performance, we aim to assess the generalizability of our framework. For comparison, we trained a BERT encoder with the same dataset. We tokenized the input using BERT's Word Piece ${ }^{70}$ Tokenizer and kept the model architecture identical to that of RoBERTa.</p>
<p>With BERT, we observe a similar trend in performance across the MoleculeNet benchmarks. It shows especially strong performance in regression tasks, while aligning with other baseline models utilizing string-based representations for the classification tasks. The model comparisons are shown in Table 4 below.</p>
<p>Table 4: Performance comparison between different transformer encoders. The table presents a performance comparison between different transformer encoders, specifically BERT and RoBERTa, in capturing essential molecular representations. The \% Change column represents the relative improvement of RoBERTa over BERT. Positive values indicate improved performance for classification tasks, while negative values signify better performance in regression tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">BERT</th>
<th style="text-align: left;">RoBERTa</th>
<th style="text-align: left;">Change (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BBBP</td>
<td style="text-align: left;">$71.3 \pm 1.79$</td>
<td style="text-align: left;">$74.1 \pm 0.15$</td>
<td style="text-align: left;">3.83</td>
</tr>
<tr>
<td style="text-align: left;">BACE</td>
<td style="text-align: left;">$74.4 \pm 1.53$</td>
<td style="text-align: left;">$73.4 \pm 0.47$</td>
<td style="text-align: left;">-1.34</td>
</tr>
<tr>
<td style="text-align: left;">ClinTox</td>
<td style="text-align: left;">$49.6 \pm 0.15$</td>
<td style="text-align: left;">$49.7 \pm 0.12$</td>
<td style="text-align: left;">0.07</td>
</tr>
<tr>
<td style="text-align: left;">SIDER</td>
<td style="text-align: left;">$56.7 \pm 0.70$</td>
<td style="text-align: left;">$58.5 \pm 0.35$</td>
<td style="text-align: left;">3.23</td>
</tr>
<tr>
<td style="text-align: left;">Tox21</td>
<td style="text-align: left;">$63.4 \pm 0.85$</td>
<td style="text-align: left;">$65.9 \pm 0.06$</td>
<td style="text-align: left;">3.94</td>
</tr>
<tr>
<td style="text-align: left;">HIV</td>
<td style="text-align: left;">$70.6 \pm 1.38$</td>
<td style="text-align: left;">$75.5 \pm 1.29$</td>
<td style="text-align: left;">7.04</td>
</tr>
<tr>
<td style="text-align: left;">FreeSolv</td>
<td style="text-align: left;">$1.006 \pm 0.051$</td>
<td style="text-align: left;">$0.896 \pm 0.023$</td>
<td style="text-align: left;">-10.90</td>
</tr>
<tr>
<td style="text-align: left;">ESOL</td>
<td style="text-align: left;">$0.531 \pm 0.040$</td>
<td style="text-align: left;">$0.477 \pm 0.007$</td>
<td style="text-align: left;">-10.23</td>
</tr>
<tr>
<td style="text-align: left;">Lipophilicity</td>
<td style="text-align: left;">$0.810 \pm 0.013$</td>
<td style="text-align: left;">$0.758 \pm 0.008$</td>
<td style="text-align: left;">-6.42</td>
</tr>
</tbody>
</table>
<p>From Table 4, it is observed that RoBERTa consistently outperforms BERT in both classification and regression tasks, demonstrating up to $7.04 \%$ and $10.23 \%$ in HIV and ESOL datasets respectively. This suggests that the learned representations exhibit strong general-</p>
<p>izability, as the difference in model performance is about $10 \%$, hinting at the potential for even better performance with more advanced models.</p>
<h1>Effect of Pretraining</h1>
<p>To evaluate the benefits of pretraining, we compared the performance of two models: one that was trained from scratch and another that was pretrained using Masked Language Modeling. As depicted in Figure 2, there's a clear advantage to pretraining - it leads to a noticeable improvement in property prediction accuracy. This suggests that GPT-MolBERTa can effectively utilize unlabeled data to craft representations that are both meaningful and applicable to molecular property prediction tasks. An added benefit is that pretrained models already have a basic grasp of chemistry. Researchers can further fine-tune these models for specific tasks, combining general and specialized knowledge.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Effect of Pretraining on GPT-MolBERTa with (a) Classification tasks and (b) Regression tasks. The comparison between the pretrained model and the model trained from scratch is demonstrated for each dataset.</p>
<h1>Understanding the Representations</h1>
<p>Figure 3 displays the attention visualization of the RoBERTa model. ${ }^{71}$ It reveals that the model particularly focuses on certain parts of the description, such as the SMILES string, as well as specific information like atom type and properties. Taking the molecule 'NC12C3CC1(C3)OC2=N' as an example, the attention mechanism underscores terms like "Nitrogen", "stereochemistry", "Aromatic Ring", "rings", "fused", and "heterocyclic". While the SMILES representation can encapsulate some of this data, textual descriptions add an interpretable dimension by assigning word attributes to elements, like specifying "benzene rings". This added interpretability is a significant advantage of our model. This added interpretability is a significant advantage of our model. This increased clarity is a notable benefit of our model. By using specific terms like "benzene rings," the model provides a clearer picture of the molecule's structure and properties. This method offers a balance between a detailed representation and easily understandable information, making it useful for to interpret important characteristic of molecules leveraged by the model for final property prediction.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">The</th>
<th style="text-align: center;">given</th>
<th style="text-align: center;">SMILES</th>
<th style="text-align: center;">molecule</th>
<th style="text-align: center;">NC12C3CC1(C3)OCZ-N,</th>
<th style="text-align: center;">represents</th>
<th style="text-align: center;">a</th>
<th style="text-align: center;">complex</th>
<th style="text-align: center;">organic</th>
<th style="text-align: center;">compound</th>
<th style="text-align: center;">Here</th>
<th style="text-align: center;">are</th>
<th style="text-align: center;">some</th>
<th style="text-align: center;">important</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">features</td>
<td style="text-align: center;">about</td>
<td style="text-align: center;">this</td>
<td style="text-align: center;">molecule</td>
<td style="text-align: center;">v/c/c1</td>
<td style="text-align: center;">Molecular</td>
<td style="text-align: center;">Formula</td>
<td style="text-align: center;">The</td>
<td style="text-align: center;">molecular</td>
<td style="text-align: center;">formula</td>
<td style="text-align: center;">of</td>
<td style="text-align: center;">this</td>
<td style="text-align: center;">compound</td>
<td style="text-align: center;">is C10H12N2O</td>
</tr>
<tr>
<td style="text-align: center;">V/c12</td>
<td style="text-align: center;">Aromatic</td>
<td style="text-align: center;">Ring</td>
<td style="text-align: center;">The</td>
<td style="text-align: center;">molecule</td>
<td style="text-align: center;">contains</td>
<td style="text-align: center;">a</td>
<td style="text-align: center;">fused</td>
<td style="text-align: center;">aromatic</td>
<td style="text-align: center;">ring</td>
<td style="text-align: center;">system,</td>
<td style="text-align: center;">represented</td>
<td style="text-align: center;">by</td>
<td style="text-align: center;">the</td>
</tr>
<tr>
<td style="text-align: center;">number</td>
<td style="text-align: center;">1,</td>
<td style="text-align: center;">2,</td>
<td style="text-align: center;">and</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">in</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">string</td>
<td style="text-align: center;">This</td>
<td style="text-align: center;">ring</td>
<td style="text-align: center;">system</td>
<td style="text-align: center;">consists</td>
<td style="text-align: center;">of</td>
</tr>
<tr>
<td style="text-align: center;">two</td>
<td style="text-align: center;">benzene</td>
<td style="text-align: center;">rings</td>
<td style="text-align: center;">fused</td>
<td style="text-align: center;">together</td>
<td style="text-align: center;">v/c12</td>
<td style="text-align: center;">Nitrogen</td>
<td style="text-align: center;">Atom</td>
<td style="text-align: center;">The</td>
<td style="text-align: center;">presence</td>
<td style="text-align: center;">of</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">nitrogen</td>
<td style="text-align: center;">atom</td>
</tr>
<tr>
<td style="text-align: center;">(N)</td>
<td style="text-align: center;">in the</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">string</td>
<td style="text-align: center;">indicates</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">substitution</td>
<td style="text-align: center;">of</td>
<td style="text-align: center;">one</td>
<td style="text-align: center;">of the</td>
<td style="text-align: center;">carbon</td>
<td style="text-align: center;">atoms</td>
<td style="text-align: center;">in</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">the</td>
<td style="text-align: center;">aromatic</td>
<td style="text-align: center;">ring</td>
<td style="text-align: center;">with</td>
<td style="text-align: center;">a</td>
<td style="text-align: center;">nitrogen</td>
<td style="text-align: center;">atom</td>
<td style="text-align: center;">v/cv4</td>
<td style="text-align: center;">Oxygen</td>
<td style="text-align: center;">Atom</td>
<td style="text-align: center;">The</td>
<td style="text-align: center;">molecule</td>
<td style="text-align: center;">also</td>
<td style="text-align: center;">contains</td>
</tr>
<tr>
<td style="text-align: center;">an</td>
<td style="text-align: center;">oxygen</td>
<td style="text-align: center;">atom</td>
<td style="text-align: center;">(O)</td>
<td style="text-align: center;">attached</td>
<td style="text-align: center;">to</td>
<td style="text-align: center;">one</td>
<td style="text-align: center;">of the</td>
<td style="text-align: center;">carbon</td>
<td style="text-align: center;">atoms</td>
<td style="text-align: center;">in</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">aromatic</td>
<td style="text-align: center;">ring</td>
</tr>
<tr>
<td style="text-align: center;">.</td>
<td style="text-align: center;">This</td>
<td style="text-align: center;">oxygen</td>
<td style="text-align: center;">atom</td>
<td style="text-align: center;">is</td>
<td style="text-align: center;">connected</td>
<td style="text-align: center;">to</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">carbon</td>
<td style="text-align: center;">atom</td>
<td style="text-align: center;">represented</td>
<td style="text-align: center;">by</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">number</td>
</tr>
<tr>
<td style="text-align: center;">in</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">string</td>
<td style="text-align: center;">v/cv5</td>
<td style="text-align: center;">Heterocyclic</td>
<td style="text-align: center;">Structure</td>
<td style="text-align: center;">The</td>
<td style="text-align: center;">combination</td>
<td style="text-align: center;">of the</td>
<td style="text-align: center;">nitrogen</td>
<td style="text-align: center;">atom</td>
<td style="text-align: center;">and</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">the</td>
<td style="text-align: center;">oxygen</td>
<td style="text-align: center;">atom</td>
<td style="text-align: center;">in</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">molecule</td>
<td style="text-align: center;">creates</td>
<td style="text-align: center;">a</td>
<td style="text-align: center;">heterocyclic</td>
<td style="text-align: center;">structure</td>
<td style="text-align: center;">.</td>
<td style="text-align: center;">Heterocyclic</td>
<td style="text-align: center;">compounds</td>
<td style="text-align: center;">are</td>
</tr>
<tr>
<td style="text-align: center;">compounds</td>
<td style="text-align: center;">that</td>
<td style="text-align: center;">contain</td>
<td style="text-align: center;">at</td>
<td style="text-align: center;">least</td>
<td style="text-align: center;">one</td>
<td style="text-align: center;">atom</td>
<td style="text-align: center;">other than</td>
<td style="text-align: center;">carbon</td>
<td style="text-align: center;">in the</td>
<td style="text-align: center;">ring</td>
<td style="text-align: center;">v/cv6</td>
<td style="text-align: center;">.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Stereochemistry</td>
<td style="text-align: center;">The</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">string</td>
<td style="text-align: center;">does</td>
<td style="text-align: center;">not</td>
<td style="text-align: center;">provide</td>
<td style="text-align: center;">information</td>
<td style="text-align: center;">about</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">stereochemistry</td>
<td style="text-align: center;">of</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">molecule</td>
</tr>
<tr>
<td style="text-align: center;">Therefore,</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">exact</td>
<td style="text-align: center;">spatial</td>
<td style="text-align: center;">arrangement</td>
<td style="text-align: center;">of</td>
<td style="text-align: center;">atoms</td>
<td style="text-align: center;">and</td>
<td style="text-align: center;">groups</td>
<td style="text-align: center;">in the</td>
<td style="text-align: center;">compound</td>
<td style="text-align: center;">cannot</td>
<td style="text-align: center;">be</td>
<td style="text-align: center;">determined</td>
</tr>
<tr>
<td style="text-align: center;">solely</td>
<td style="text-align: center;">from the</td>
<td style="text-align: center;">given</td>
<td style="text-align: center;">SMILES</td>
<td style="text-align: center;">representation</td>
<td style="text-align: center;">v/cv9</td>
<td style="text-align: center;">is</td>
<td style="text-align: center;">important</td>
<td style="text-align: center;">to</td>
<td style="text-align: center;">note</td>
<td style="text-align: center;">that</td>
<td style="text-align: center;">while</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">SMILES</td>
</tr>
<tr>
<td style="text-align: center;">string</td>
<td style="text-align: center;">provides</td>
<td style="text-align: center;">a</td>
<td style="text-align: center;">concise</td>
<td style="text-align: center;">representation</td>
<td style="text-align: center;">of</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">molecule's</td>
<td style="text-align: center;">structure,</td>
<td style="text-align: center;">it</td>
<td style="text-align: center;">does</td>
<td style="text-align: center;">not</td>
<td style="text-align: center;">provide</td>
<td style="text-align: center;">information</td>
</tr>
<tr>
<td style="text-align: center;">the</td>
<td style="text-align: center;">specific</td>
<td style="text-align: center;">functional</td>
<td style="text-align: center;">groups</td>
<td style="text-align: center;">or</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">overall</td>
<td style="text-align: center;">chemical</td>
<td style="text-align: center;">properties</td>
<td style="text-align: center;">of the</td>
<td style="text-align: center;">compound</td>
<td style="text-align: center;">.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 3: A sample attention map from the model. Given a sample description, it highlights the sections of the descriptions according to its attention scores, showing how the model focuses on specific aspects of the descriptions.</p>
<p>To further delve into the representations learned by GPT-MolBERTa, we employed dimension reduction through t-SNE embedding. We applied the t-SNE algorithm to map the test sets of both the ESOL and FreeSolv datasets, as they showcased the best performance among the models. The resulting visualizations are presented in Figure 4. Upon closer inspection of these visualizations, an interesting pattern emerges. GPT-MolBERTa demonstrates its ability to effectively cluster labels, where labels exhibiting more negative values are clustered towards the bottom-right, and the more positive values are clustered towards the top-right, observed for both the ESOL and FreeSolv datasets. This observation underscores GPTMolBERTa's capacity to extract meaningful and informative features from the input data, highlighting its practicality and potential for molecular discovery and property prediction.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: t-SNE Embeddings of the First Token of GPT-MolBERTa for a) ESOL and b) FreeSolv datasets: Each point in this plot represents log solvation energy for ESOL and free hydration energy for FreeSolv.</p>
<h2>Conclusion</h2>
<p>In this work, we introduce GPT-MolBERTa framework, that harnesses text descriptions of molecules to train large-scale language models for molecular property prediction. We successfully demonstrate the viability of our strategy by benchmarking the framework on MoleculeNet. GPT-MolBERTa's ability to represent molecules showed consistent performance across a wide variety of chemicals, suggesting it can generalize well even with limited data. We believe that the performance of the model access to more extensive data, similar to the approach used by more established models like ChemBERTa.</p>
<p>A distinct feature of our model is an added layer of interpretability by leveraging the attention mechanism. The attention visualizations highlight which parts of the molecular description the model views as most important, offering clearer insights into its decision-making process. Furthermore, we can also look into advanced techniques, such as contrastive learning, as avenues to improve the model's performance. With further refinement, we believe this model can play a pivotal role in applications like drug discovery.</p>
<p>Acknowledgement</p>
<p>The authors would like to express gratitude to Janghoon Ock for many insightful discussions regarding Transformer-based models.</p>
<p>Data Availability</p>
<p>The Python code and datasets used in this study can be accessed on GitHub using the following link: https://github.com/Suryanarayanan-Balaji/GPT-MolBERTa</p>
<p>References</p>
<p>(1) Shen, J.; Nicolaou, C. A. Molecular property prediction: recent trends in the era of artificial intelligence. ScienceDirect 2019, 32-33, 29–36.</p>
<p>(2) Bartók, A. P.; Kondor, R.; Csányi, G. On representing chemical environments. Phys. Rev. B 2013, 87, 184115.</p>
<p>(3) Huang, B.; von Lilienfeld, O. A. Communication: Understanding molecular representations in machine learning: The role of uniqueness and target similarity. The Journal of Chemical Physics 2016, 145, 161102.</p>
<p>(4) Laurianne David, R. M., Amol Thakkar; Engkvist, O. Molecular representations in AIdriven drug discovery: a review and practical guide. Journal of Cheminformatics 2020, 12.</p>
<p>(5) Weininger, D. SMILES, a Chemical Language and Information System. 1. Introduction to Methodology and Encoding Rules. J. Chem. Inf. Comp. Sci. 1988, 28, 31–36.</p>
<p>(6) Duvenaud, D. K.; Maclaurin, D.; Iparraguirre, J.; Bombarell, R.; Hirzel, T.; Aspuru-</p>
<p>Guzik, A.; Adams, R. P. Convolutional Networks on Graphs for Learning Molecular Fingerprints. Advances in Neural Information Processing Systems. 2015.
(7) Rogers, D.; Hahn, M. Extended-connectivity fingerprints. Journal of chemical information and modeling 2010, 50, 742-754.
(8) Capecchi, A.; Probst, D.; Reymond, J.-L. One molecular fingerprint to rule them all: drugs, biomolecules, and the metabolome. Journal of cheminformatics 2020, 12, 1-15.
(9) Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; Dahl, G. E. Neural Message Passing for Quantum Chemistry. Proceedings of the 34th International Conference on Machine Learning. 2017; pp 1263-1272.
(10) Jiang, D.; Wu, Z.; Hsieh, C.; Chen, G.; Liao, B.; Wang, Z.; Shen, C.; Cao, D.; Wu, J.; Hou, T. Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models. Journal of Cheminformatics 2021, 13, 12.
(11) Yang, K.; Swanson, K.; Jin, W.; Coley, C.; Eiden, P.; Gao, H.; Guzman-Perez, A.; Hopper, T.; Kelley, B.; Mathea, M.; Palmer, A.; Settels, V.; Jaakkola, T.; Jensen, K.; Barzilay, R. Analyzing Learned Molecular Representations for Property Prediction. Journal of Chemical Information and Modeling 2019, 59, 3370-3388.
(12) Johannes Gasteiger, J. G. . S. G. DIRECTIONAL MESSAGE PASSING FOR MOLECULAR GRAPHS. International Conference on Learning Representations 2020,
(13) Schutt, K. T.; Sauceda, H. E.; Kindermans, P.-J.; Tkatchenko, A.; Muller, K.-R. SchNet - A deep learning architecture for molecules and materials. The Journal of Chemical Physics 2018, 148 .
(14) Lu, C.; Liu, Q.; Wang, C.; Huang, Z.; Lin, P.; He, L. Molecular property prediction: A</p>
<p>multilevel quantum interactions modeling perspective. Proceedings of the AAAI conference on artificial intelligence. 2019; pp 1052-1060.
(15) Karamad, M.; Magar, R.; Shi, Y.; Siahrostami, S.; Gates, I. D.; Farimani, A. B. Orbital graph convolutional neural network for material property prediction. Physical Review Materials 2020, 4, 093801.
(16) Ock, J.; Tian, T.; Kitchin, J.; Ulissi, Z. Beyond independent error assumptions in large GNN atomistic models. The Journal of Chemical Physics 2023, 158, 214702.
(17) Wigh, D. S.; Goodman, J. M.; Lapkin, A. A. A review of molecular representation in the age of machine learning. WIREs Computational Molecular Science 2022, 12.
(18) Cheng, A. H.; Cai, A.; Miret, S.; Malkomes, G.; Phielipp, M.; Aspuru-Guzik, A. Group SELFIES: a robust fragment-based molecular string representation. Digital Discovery 2023,
(19) Zhang, D.; Xia, S.; Zhang, Y. Accurate prediction of aqueous free solvation energies using 3d atomic feature-based graph neural network with transfer learning. Journal of Chemical Information and Modeling 2022, 62, 1840-1848.
(20) Sachdev, K.; Gupta, M. K. A comprehensive review of feature based methods for drug target interaction prediction. Journal of Biomedical Informatics 2019, 93, 103159.
(21) Guo, Z.; Guo, K.; Nan, B.; Tian, Y.; Iyer, R. G.; Ma, Y.; Wiest, O.; Zhang, X.; Wang, W.; Zhang, C., et al. Graph-based molecular representation learning. arXiv preprint arXiv:2207.04869 2022,
(22) Rogers, D.; Hahn, M. Extended-Connectivity Fingerprints. J. Chem. Inf. Model. 2010, $50,742-754$.
(23) Gómez-Bombarelli, R.; Wei, J. N.; Duvenaud, D.; Hernández-Lobato, J. M.; SánchezLengeling, B.; Sheberla, D.; Aguilera-Iparraguirre, J.; Hirzel, T. D.; Adams, R. P.;</p>
<p>Aspuru-Guzik, A. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science 2018, 4, 268-276.
(24) Kim, S.; Thiessen, P. A.; Bolton, E. E.; Chen, J.; Fu, G.; Gindulyte, A.; Han, L.; He, J.; He, S.; Shoemaker, B. A.; Wang, J.; Yu, B.; Zhang, J.; Bryant, S. H. PubChem Substance and Compound databases. Nucleic Acids Research 2015, 44, D1202-D1213.
(25) Gaulton, A. et al. The ChEMBL database in 2017. Nucleic Acids Research 2017, 45.
(26) Vaswani, A.; Parmar, N. S. N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; Polosukhin, I. Attention is All you Need. Advances in Neural Information Processing Systems. 2017.
(27) Tianyang Lin, X. L., Yuxin Wang; Qiu, X. A survey of transformers. AI Open 2022, $3,111-132$.
(28) Katikapalli Subramanyam Kalyan, A. R.; Sangeetha, S. AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing. AI Open 2021,
(29) Su, J.; Lu, Y.; Pan, S.; Murtadha, A.; Wen, B.; Liu, Y. ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING. 2022,
(30) Katharopoulos, A.; Vyas, A.; Pappas, N.; Fleuret, F. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. Proceedings of the 37th International Conference on Machine Learning. 2020; pp 5156-5165.
(31) Jacob Devlin, K. L., Ming-Wei Chang; Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. The Journal of Chemical Physics 2019, 148 .
(32) Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; Stoyanov, V. RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019,</p>
<p>(33) Fabian, B.; Edlich, T.; Gaspar, H.; Segler, M.; Meyers, J.; Fiscato, M.; Ahmed, M. Molecular representation learning with language models and domain-relevant auxiliary tasks. 2020,
(34) Nathan Brown, M. H. S., Marco Fiscato; Vaucher, A. C. GuacaMol: Benchmarking Models for de Novo Molecular Design. Journal of Chemical Information and Modeling 2019, 59, 1096-1108.
(35) Irwin, R.; Dimitriadis, S.; He, J.; Bjerrum, E. J. Chemformer: a pre-trained transformer for computational chemistry. Machine Learning: Science and Technology 2022, 3.
(36) Sterling, T.; Irwin, J. J. ZINC 15 - Ligand Discovery for Everyone. Journal of Chemican Information and Modeling 2015, 55, 2324-2337.
(37) Bjerrum, E. J.; Sattarov, B. Improving Chemical Autoencoder Latent Space and Molecular De novo Generation Diversity with Heteroencoders. 2018,
(38) Chithrananda, S.; Grand, G.; Ramsundar, B. ChemBERTa: Large-Scale SelfSupervised Pretraining for Molecular Property Prediction. 2020,
(39) Ahmad, W.; Simon, E.; Chithrananda, S.; Grand, G.; Ramsundar, B. ChemBERTa-2: Towards Chemical Foundation Models. 2022,
(40) Ross, J.; Belgodere, B.; Chenthamarakshan, V.; Padhi, I.; Mroueh, Y.; Das, P. LargeScale Chemical Language Representations Capture Molecular Structure and Properties. Nature Machine Intelligence 2022, 4, 1256-1264.
(41) Yüksel, A.; Ulusoy, E.; Ünlü, A.; Doğan, T. SELFormer: molecular representation learning via SELFIES language models. Machine Learning: Science and Technology 2023, 4 .
(42) Krenn, M.; Häse, F.; Nigam, A.; Friederich, P.; Aspuru-Guzik, A. Self-Referencing</p>            </div>
        </div>

    </div>
</body>
</html>
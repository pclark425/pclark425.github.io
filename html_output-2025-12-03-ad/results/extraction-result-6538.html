<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6538 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6538</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6538</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-271957303</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.13457v3.pdf" target="_blank">Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Self-consistency (SC), a widely used decoding strategy for chain-of-thought reasoning, shows significant gains across various multi-step reasoning tasks but comes with a high cost due to multiple sampling with the preset size. Its variants, Adaptive self-consistency (ASC) and Early-stopping self-consistency (ESC), dynamically adjust the number of samples based on the posterior distribution of a set of pre-samples, reducing the cost of SC with minimal impact on performance. Both methods, however, do not exploit the prior information about question difficulty. It often results in unnecessary repeated sampling for easy questions that could be accurately answered with just one attempt, wasting resources. To tackle this problem, we propose Difficulty-Adaptive Self-Consistency (DSC), which leverages the difficulty information of batch queries from both prior and posterior perspectives to adaptively allocate inference resources, further reducing the overall cost of SC. To demonstrate the effectiveness of DSC, we conduct extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning on six benchmarks. The empirical results show that DSC consistently surpasses the strong baseline ASC and ESC in terms of costs by a significant margin, while attaining comparable performances.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6538.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6538.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Difficulty-Adaptive Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-step, difficulty-aware variant of self-consistency that (1) ranks problem difficulty with an LLM, (2) partitions problems into easy/hard to skip redundant sampling on easy items, and (3) pre-allocates sample sizes for hard items to reduce resampling input costs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Difficulty-Adaptive Self-Consistency (DSC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble / adaptive self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (adapts sampling per-item; treats many items as producing similar outputs and avoids redundant diverse sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, GSM8K, CommonsenseQA, StrategyQA, Last Letter Concatenation, Coin Flip</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>arithmetic (grade-school and contest math), commonsense question answering, and symbolic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>average cost reduction vs SC (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>65.29</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>SC, ESC, ASC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>24.81</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>DSC is designed to exploit the (often high) similarity of outputs across multiple CoT samples on easy problems to avoid redundant sampling; it uses prior difficulty ranking to save input/output tokens. The paper reports DSC attains comparable accuracy while substantially lowering inference cost vs standard SC and also reducing costs vs ESC/ASC by leveraging difficulty priors and sample pre-allocation. Ablations show each sub-step (Difficulty Ranking, Problem Partition, Sample Size Pre-Allocation) contributes to cost savings.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6538.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6538.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding strategy that samples multiple chain-of-thought (CoT) reasoning paths and aggregates answers (typically via majority voting) to improve multi-step reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse in intention (multiple samples) but empirically often produces similar/homogeneous answers (high overlap with single-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>95.81</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT (single-sample chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>The paper notes a high overlap between SC and single-shot CoT on GSM8K (only ~3.8% of questions benefit from SC with GPT-4), meaning repeated sampling often yields the same answer and wastes cost; SC provides accuracy gains but with large cost proportional to number of samples.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6538.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6538.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Self-Consistency (ASC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive stopping self-consistency method that samples one-by-one and stops when a confidence-based stopping criterion (Dirichlet criterion) deems the current answer set sufficiently certain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Adaptive Self-Consistency (ASC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble / adaptive stopping</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (samples sequentially until agreement/confidence achieved)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>95.79</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ASC reduces average sample sizes by stopping early when a confidence threshold is reached, but the paper highlights residual waste because ASC still requires pre-sampling for all problems and can incur substantial extra input costs from re-sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6538.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6538.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ESC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Early-stopping Self-Consistency (ESC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A windowed early-stopping variant that divides the preset sample budget into small sequential windows and stops when all answers in a window are identical.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Escape sky-high cost: Early-stopping selfconsistency for multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Early-stopping Self-Consistency (ESC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble / windowed early stopping</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (samples in windows to detect consensus)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>95.8</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ESC can substantially reduce cost compared to SC if window constraints are chosen well, but like ASC it still incurs extra input cost from repeated few-shot inputs across multiple samples; DSC further reduces cost by leveraging difficulty priors.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6538.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6538.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits multi-step reasoning by providing exemplars with intermediate reasoning steps; typically used as the generation backbone for SC and its variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / in-context reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (one sampled chain-of-thought), but can be diversified by sampling multiple CoT outputs</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>proportion identical between CoT and SC (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>96.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>On GSM8K with GPT-4 the authors report only ~3.8% of questions benefit from SC over single-shot CoT, implying CoT and SC answers are identical for ~96.2% of items â€” an observation motivating DSC's difficulty-aware resource allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6538.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6538.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DifficultyRanking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Difficulty Ranking (component of DSC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based procedure to rank dataset items by difficulty via repeated random batching and intra-batch ranking, producing prior difficulty scores used to allocate inference resources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>prior difficulty ranking via LLM</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>prior estimation / meta-prediction</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single-style ranking output (produces ordering used to treat items differently)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>mathematical problem difficulty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman correlation with human difficulty (avg over MATH subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.624</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>The paper reports good correlation between LLM-derived difficulty ranks and human labels (GPT-4 avg Spearman ~0.624; GPT-3.5 moderate). The authors note ranking costs are low because outputs are concise, and the prior difficulty signal yields net cost savings despite its own small cost.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6538.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6538.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-DSC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DSC evaluated on Mistral-7B-Instruct-v0.3 (small open-source model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experiment showing DSC's potential on smaller models: DSC applied to Mistral-7B on GSM8K yields comparable accuracy with lower cost than SC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Difficulty-Adaptive Self-Consistency (DSC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble / adaptive self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>73.18</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>On GSM8K with Mistral-7B the paper reports SC accuracy ~73.19 (cost 0.3881) and DSC accuracy ~73.18 with lower cost (0.2799), suggesting DSC generalizes to smaller models for cost reductions while maintaining accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6538.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6538.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSC_GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Difficulty-Adaptive Self-Consistency evaluated with GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DSC applied to GPT-3.5-Turbo reduces cost substantially compared to SC while preserving comparable accuracy across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Difficulty-Adaptive Self-Consistency (DSC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble / adaptive self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, GSM8K, CommonsenseQA, StrategyQA, Last Letter, Coin Flip</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>arithmetic, commonsense, symbolic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>average cost reduction vs SC (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>56.04</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>SC, ESC, ASC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>21.86</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper reports DSC reduces average cost on GPT-3.5-Turbo by ~56.04% vs SC and by ~21.86% vs ESC while maintaining comparable accuracy; ablations indicate Sample Size Pre-Allocation reduces input tokens dramatically.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Adaptive consistency for efficient reasoning and coding with LLMs <em>(Rating: 2)</em></li>
                <li>Escape sky-high cost: Early-stopping selfconsistency for multi-step reasoning. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Large language models can self-correct with key condition verification. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6538",
    "paper_id": "paper-271957303",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "DSC",
            "name_full": "Difficulty-Adaptive Self-Consistency",
            "brief_description": "A three-step, difficulty-aware variant of self-consistency that (1) ranks problem difficulty with an LLM, (2) partitions problems into easy/hard to skip redundant sampling on easy items, and (3) pre-allocates sample sizes for hard items to reduce resampling input costs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "unknown",
            "reasoning_method_name": "Difficulty-Adaptive Self-Consistency (DSC)",
            "reasoning_method_type": "ensemble / adaptive self-consistency",
            "reasoning_style_diversity": "mixed (adapts sampling per-item; treats many items as producing similar outputs and avoids redundant diverse sampling)",
            "benchmark_name": "MATH, GSM8K, CommonsenseQA, StrategyQA, Last Letter Concatenation, Coin Flip",
            "task_description": "arithmetic (grade-school and contest math), commonsense question answering, and symbolic reasoning",
            "performance_metric": "average cost reduction vs SC (%)",
            "performance_value": 65.29,
            "comparison_target_method": "SC, ESC, ASC",
            "performance_difference": 24.81,
            "statistical_significance": false,
            "analysis_notes": "DSC is designed to exploit the (often high) similarity of outputs across multiple CoT samples on easy problems to avoid redundant sampling; it uses prior difficulty ranking to save input/output tokens. The paper reports DSC attains comparable accuracy while substantially lowering inference cost vs standard SC and also reducing costs vs ESC/ASC by leveraging difficulty priors and sample pre-allocation. Ablations show each sub-step (Difficulty Ranking, Problem Partition, Sample Size Pre-Allocation) contributes to cost savings.",
            "ablation_study_present": true,
            "uuid": "e6538.0",
            "source_info": {
                "paper_title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "SC",
            "name_full": "Self-Consistency",
            "brief_description": "Decoding strategy that samples multiple chain-of-thought (CoT) reasoning paths and aggregates answers (typically via majority voting) to improve multi-step reasoning accuracy.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "unknown",
            "reasoning_method_name": "Self-Consistency (SC)",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse in intention (multiple samples) but empirically often produces similar/homogeneous answers (high overlap with single-shot CoT)",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school arithmetic word problems",
            "performance_metric": "accuracy (%)",
            "performance_value": 95.81,
            "comparison_target_method": "CoT (single-sample chain-of-thought)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "The paper notes a high overlap between SC and single-shot CoT on GSM8K (only ~3.8% of questions benefit from SC with GPT-4), meaning repeated sampling often yields the same answer and wastes cost; SC provides accuracy gains but with large cost proportional to number of samples.",
            "ablation_study_present": false,
            "uuid": "e6538.1",
            "source_info": {
                "paper_title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "ASC",
            "name_full": "Adaptive Self-Consistency (ASC)",
            "brief_description": "An adaptive stopping self-consistency method that samples one-by-one and stops when a confidence-based stopping criterion (Dirichlet criterion) deems the current answer set sufficiently certain.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "unknown",
            "reasoning_method_name": "Adaptive Self-Consistency (ASC)",
            "reasoning_method_type": "ensemble / adaptive stopping",
            "reasoning_style_diversity": "mixed (samples sequentially until agreement/confidence achieved)",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school arithmetic word problems",
            "performance_metric": "accuracy (%)",
            "performance_value": 95.79,
            "comparison_target_method": "SC",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "ASC reduces average sample sizes by stopping early when a confidence threshold is reached, but the paper highlights residual waste because ASC still requires pre-sampling for all problems and can incur substantial extra input costs from re-sampling.",
            "ablation_study_present": false,
            "uuid": "e6538.2",
            "source_info": {
                "paper_title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "ESC",
            "name_full": "Early-stopping Self-Consistency (ESC)",
            "brief_description": "A windowed early-stopping variant that divides the preset sample budget into small sequential windows and stops when all answers in a window are identical.",
            "citation_title": "Escape sky-high cost: Early-stopping selfconsistency for multi-step reasoning.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "unknown",
            "reasoning_method_name": "Early-stopping Self-Consistency (ESC)",
            "reasoning_method_type": "ensemble / windowed early stopping",
            "reasoning_style_diversity": "mixed (samples in windows to detect consensus)",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school arithmetic word problems",
            "performance_metric": "accuracy (%)",
            "performance_value": 95.8,
            "comparison_target_method": "SC",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "ESC can substantially reduce cost compared to SC if window constraints are chosen well, but like ASC it still incurs extra input cost from repeated few-shot inputs across multiple samples; DSC further reduces cost by leveraging difficulty priors.",
            "ablation_study_present": false,
            "uuid": "e6538.3",
            "source_info": {
                "paper_title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits multi-step reasoning by providing exemplars with intermediate reasoning steps; typically used as the generation backbone for SC and its variants.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "unknown",
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential / in-context reasoning",
            "reasoning_style_diversity": "single style (one sampled chain-of-thought), but can be diversified by sampling multiple CoT outputs",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school arithmetic word problems",
            "performance_metric": "proportion identical between CoT and SC (%)",
            "performance_value": 96.2,
            "comparison_target_method": "SC",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "On GSM8K with GPT-4 the authors report only ~3.8% of questions benefit from SC over single-shot CoT, implying CoT and SC answers are identical for ~96.2% of items â€” an observation motivating DSC's difficulty-aware resource allocation.",
            "ablation_study_present": false,
            "uuid": "e6538.4",
            "source_info": {
                "paper_title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "DifficultyRanking",
            "name_full": "Difficulty Ranking (component of DSC)",
            "brief_description": "An LLM-based procedure to rank dataset items by difficulty via repeated random batching and intra-batch ranking, producing prior difficulty scores used to allocate inference resources.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "unknown",
            "reasoning_method_name": "prior difficulty ranking via LLM",
            "reasoning_method_type": "prior estimation / meta-prediction",
            "reasoning_style_diversity": "single-style ranking output (produces ordering used to treat items differently)",
            "benchmark_name": "MATH",
            "task_description": "mathematical problem difficulty estimation",
            "performance_metric": "Spearman correlation with human difficulty (avg over MATH subsets)",
            "performance_value": 0.624,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "The paper reports good correlation between LLM-derived difficulty ranks and human labels (GPT-4 avg Spearman ~0.624; GPT-3.5 moderate). The authors note ranking costs are low because outputs are concise, and the prior difficulty signal yields net cost savings despite its own small cost.",
            "ablation_study_present": true,
            "uuid": "e6538.5",
            "source_info": {
                "paper_title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Mistral-7B-DSC",
            "name_full": "DSC evaluated on Mistral-7B-Instruct-v0.3 (small open-source model)",
            "brief_description": "An experiment showing DSC's potential on smaller models: DSC applied to Mistral-7B on GSM8K yields comparable accuracy with lower cost than SC.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct-v0.3",
            "model_size": "7B",
            "reasoning_method_name": "Difficulty-Adaptive Self-Consistency (DSC)",
            "reasoning_method_type": "ensemble / adaptive self-consistency",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school arithmetic word problems",
            "performance_metric": "accuracy (%)",
            "performance_value": 73.18,
            "comparison_target_method": "SC",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "On GSM8K with Mistral-7B the paper reports SC accuracy ~73.19 (cost 0.3881) and DSC accuracy ~73.18 with lower cost (0.2799), suggesting DSC generalizes to smaller models for cost reductions while maintaining accuracy.",
            "ablation_study_present": false,
            "uuid": "e6538.6",
            "source_info": {
                "paper_title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "DSC_GPT-3.5",
            "name_full": "Difficulty-Adaptive Self-Consistency evaluated with GPT-3.5-Turbo",
            "brief_description": "DSC applied to GPT-3.5-Turbo reduces cost substantially compared to SC while preserving comparable accuracy across benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_size": "unknown",
            "reasoning_method_name": "Difficulty-Adaptive Self-Consistency (DSC)",
            "reasoning_method_type": "ensemble / adaptive self-consistency",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "MATH, GSM8K, CommonsenseQA, StrategyQA, Last Letter, Coin Flip",
            "task_description": "arithmetic, commonsense, symbolic reasoning",
            "performance_metric": "average cost reduction vs SC (%)",
            "performance_value": 56.04,
            "comparison_target_method": "SC, ESC, ASC",
            "performance_difference": 21.86,
            "statistical_significance": false,
            "analysis_notes": "Paper reports DSC reduces average cost on GPT-3.5-Turbo by ~56.04% vs SC and by ~21.86% vs ESC while maintaining comparable accuracy; ablations indicate Sample Size Pre-Allocation reduces input tokens dramatically.",
            "ablation_study_present": true,
            "uuid": "e6538.7",
            "source_info": {
                "paper_title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Adaptive consistency for efficient reasoning and coding with LLMs",
            "rating": 2,
            "sanitized_title": "adaptive_consistency_for_efficient_reasoning_and_coding_with_llms"
        },
        {
            "paper_title": "Escape sky-high cost: Early-stopping selfconsistency for multi-step reasoning.",
            "rating": 2,
            "sanitized_title": "escape_skyhigh_cost_earlystopping_selfconsistency_for_multistep_reasoning"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models can self-correct with key condition verification.",
            "rating": 1,
            "sanitized_title": "large_language_models_can_selfcorrect_with_key_condition_verification"
        }
    ],
    "cost": 0.0169365,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning
12 Feb 2025</p>
<p>Xinglin Wang wangxinglin@bit.edu.cn 
School of Computer Science
Beijing Institute of Technology</p>
<p>Shaoxiong Feng shaoxiongfeng2023@gmail.com 
Xiaohongshu Inc</p>
<p>Yiwei Li liyiwei@bit.edu.cn 
School of Computer Science
Beijing Institute of Technology</p>
<p>Peiwen Yuan peiwenyuan@bit.edu.cn 
School of Computer Science
Beijing Institute of Technology</p>
<p>Yueqi Zhang zhangyq@bit.edu.cn 
School of Computer Science
Beijing Institute of Technology</p>
<p>Chuyi Tan tanchuyi@bit.edu.cn 
School of Computer Science
Beijing Institute of Technology</p>
<p>Boyuan Pan panboyuan@xiaohongshu.com 
Xiaohongshu Inc</p>
<p>Yao Hu 
Xiaohongshu Inc</p>
<p>Kan Li likan@bit.edu.cn 
School of Computer Science
Beijing Institute of Technology</p>
<p>Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning
12 Feb 2025940A86301B8F8346540C495127959B35arXiv:2408.13457v3[cs.CL]
Self-consistency (SC), a widely used decoding strategy for chain-of-thought reasoning, shows significant gains across various multi-step reasoning tasks but comes with a high cost due to multiple sampling with the preset size.Its variants, Adaptive self-consistency (ASC) and Early-stopping self-consistency (ESC), dynamically adjust the number of samples based on the posterior distribution of a set of pre-samples, reducing the cost of SC with minimal impact on performance.Both methods, however, do not exploit the prior information about question difficulty.It often results in unnecessary repeated sampling for easy questions that could be accurately answered with just one attempt, wasting resources.To tackle this problem, we propose Difficulty-Adaptive Self-Consistency (DSC), which leverages the difficulty information of batch queries from both prior and posterior perspectives to adaptively allocate inference resources, further reducing the overall cost of SC.To demonstrate the effectiveness of DSC, we conduct extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning on six benchmarks.The empirical results show that DSC consistently surpasses the strong baseline ASC and ESC in terms of costs by a significant margin, while attaining comparable performances. 1</p>
<p>Introduction</p>
<p>Large language models (LLMs) have exhibited strong reasoning capabilities (Bubeck et al., 2023), especially with chain-of-thought (CoT) prompting (Wei et al., 2022b).Based on this, Wang et al. (2022) introduced a simple decoding strategy called self-consistency (SC) to further improve reasoning performance, leveraging the fact that challenging â€  Corresponding author.</p>
<p>1 Our code and data have been released on https:// github.com/WangXinglin/DSC. Table 1: Average tokens and cost ($) statistics of input and output for GPT-3.5-Turbo and GPT-4 on GSM8K.The cost is calculated according to https: //openai.com/pricing.Given that the input for reasoning tasks usually involves several demonstrations (leading to lengthy contexts), the cost of input cannot be overlooked.</p>
<p>reasoning tasks typically require more reasoning paths to arrive at the correct answer.In contrast to the standard chain-of-thought prompting which only generates the greedy one, this method samples multiple reasoning paths according to a preset sample size, and then derives the final answer through majority-voting-based scheme.Despite generally leading to improvements, SC introduces a significant overhead proportional to the number of sampled outputs.As LLMs continue to grow in size and complexity, the sampling time and computational costs associated with majority voting become increasingly challenging.Recently, some works seek to reduce the cost of SC by dynamically adjusting the number of samples based on the posterior distribution of pre-samples.ASC (Aggarwal et al., 2023) samples one by one, and stops sampling when the existing sample answers have established a clear majority as judged by a lightweight stopping criterion.Alternatively, ESC (Li et al., 2023) divides the large preset sample size into several sequential small windows, and stop sampling when answers within a window are all the same.However, both approaches still suffer from the following two shortcomings: (1) Both require a certain amount of pre-sampling for all problems, which still results in redundant waste.As shown in Figure 1, there is a high overlap of inference results between SC and CoT, which suggests that only a small portion of questions (3.8% for GPT-4) benefit from SC. Generating multiple samples for the remaining questions compared to CoT (only sampling once) results in a significant waste of costs.Although ASC and ESC somewhat reduce the ineffective cost of SC by decreasing average sample size, there remains redundant sampling for simple problems 2 .(2) Multiple re-samples bring additional significant input costs.Both ASC and ESC focus solely on reducing the number of outputs, without considering the extra input costs brought by multiple re-samples (Table 1).Consequently, for problems requiring multiple sampling times, ASC and ESC will introduce substantial extra input costs, sometimes outweighing the savings achieved through output sampling reduction (Table 3).</p>
<p>To alleviate these issues, we take inspiration from the strategies humans employ when solving reasoning problems within a limited total time.As illustrated in Figure 2, humans pre-assess the diffi-2 At least 3 samples per problem for ASC and 4 for ESC.</p>
<p>culty of the problems before reasoning, and adaptively allocating problem-solving time based on the accessed difficulty.In light of this, we propose Difficulty-Adaptive Self-Consistency (DSC), a novel method which leverages the difficulty information from both prior and posterior perspectives to further reduce the inference computational cost of SC.As illustrated in Figure 3, DSC consists of three steps.Firstly, we propose Difficulty Ranking algorithm which utilizes the LLM itself to rank the difficulty of the given problem set.While using LLM for ranking will incur additional costs, later experiments will demonstrate that the prior difficulty information gained from this process can lead to a greater reduction in overall inference costs.Based on this, we propose a Problem Partition strategy that divides the problem set into two parts, easy and hard, using the information regarding difficulty rankings.For problems belonging to easy part, a single CoT sampling is performed, which saves cost without sacrificing performance.Lastly, we propose Sample Size Pre-Allocation algorithm to predict the sample size needed for problems belonging to hard part, which reduce the number of re-samples and save the cost of inputs.</p>
<p>We evaluate DSC on a wide range of arithmetic, commonsense and symbolic reasoning tasks over GPT-3.5-Turbo and GPT-4 models.The empirical results demonstrate that DSC outperforms the existing strong baselines ASC and ESC by a significant margin on six popular benchmarks, while attaining comparable performances.Further experiments confirm the effectiveness of the proposed three-step algorithms: Difficulty Ranking, Problem Partition, and Sample Size Pre-Allocation.Step 1 Difficulty Ranking
Q ! Q "
Step 2 Problem Partition</p>
<p>Step 3 Sample Size Pre-Allocation</p>
<p>Pre-sample once for partition</p>
<p>Execution from hard to easy
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ All are Stopping pre-sample Saving Sample once â€¦ D Q " D Q # D Q #$%&amp;! D Q #$% D Q ! D</p>
<p>Demonstrations</p>
<p>All answers are the same Not all answers are the same
Q "$! D Q #$%&amp;'&amp;! D Q #$%&amp;! D</p>
<p>Execution from easy to hard</p>
<p>Total sample size</p>
<p>Pre-allocate sample size Saving re-sample times
Q #$%&amp;' D 4 â€¦ â€¦ â€¦0 â€¦ 0 1 5 Q " D 32 40 7 DSC re-sample times 0 â€¦ 2 1 4 2 ESC re-sample times 0 â€¦ 2 2 9 9 Q x N â€¦ k m Batch size = B Q x B Q x B â€¦ R times Task Definition Questions â€¦ Q Q x B x B</p>
<p>Easy Hard</p>
<p>Questions divided to easy part Questions divided to hard part</p>
<p>Figure 3: Overall workflow of proposed Difficulty-Adaptive Self-Consistency.DSC first ranks problem difficulty using LLM itself (step 1), then partitions problems into easy and hard to save sampling cost for easy ones (step 2), and finally pre-allocates sample sizes to reduce resampling costs for hard problems (step 3).</p>
<p>In summary, this work includes the following key contributions:</p>
<p>â€¢ We analyzed two common issues present in existing SC variants, ESC and ASC, designed for cost-efficient decoding.</p>
<p>â€¢ We proposed Difficulty-Adaptive Self-Consistency, a novel method consisting of three steps to fully utilize the difficulty information of given problems so as to reduce the inference computational cost.</p>
<p>â€¢ We conducted extensive experiments on six popular benchmarks and validated the effectiveness and generalizability of our proposed method.</p>
<p>â€¢ To the best of our knowledge, we are the first to propose dynamically allocating sampling resources based on query difficulty.</p>
<p>Methodology</p>
<p>The core idea behind DSC is to fully utilize the difficulty information of given problems, allocating computational resources based on their respective levels of difficulty.The overall workflow of DSC is presented in Figure 3, including three steps: Difficulty Ranking, Problem Partition and Sample Size Pre-Allocation.</p>
<p>Difficulty Ranking</p>
<p>As problems usually do not carry difficulty label themselves, we seek to utilize the powerful comparative and ranking capabilities of LLM to obtain the difficulty rank of the given problem set.Considering the limited context window size of LLM and its lost in the middle (Liu et al., 2024b) issue in understanding long contexts, we randomly divide the problem set of N into batches of size B (B &lt;&lt; N ) and let LLM rank the difficulty of the Algorithm 1 Difficulty Ranking.</p>
<p>Require: Questions Q 1:N , LLM M, Ranking prompt P , random split rounds R, Batch size B Ensure: Questions sorted by difficulty Q1:N
1: D all â† {i : [ ] for i âˆˆ [1, N ]} 2: for r â† 1, R do 3: Randomly divide Q 1:N into batches b 1:L r , L = âŒˆ N B âŒ‰ 4:
for i â† 1, N do 5:</p>
<p>Dcurrent â† âˆ… 6:</p>
<p>for j â† 1, L do: 7:</p>
<p>Dcurrent â† Dcurrent.Append(M(P, b j r )) 8:</p>
<p>end for 9: D all â† D all .Merge(Dcurrent) 10:</p>
<p>end for 11: end for 12: Q1:N â†Sort(Average(D all ))</p>
<p>problems in each batch.Since a single random split only allows us to obtain the difficulty ranking of each problem within the corresponding batch, we perform R times random batch splitting, and sort the average difficulty rankings of each problem in different batches to obtain the difficulty ranking of entire problem set.Algorithm 1 illustrates the specific implementation of difficulty ranking.Specifically, D all is a dictionary that stores the difficulty levels of all N questions.Its keys represent the indices of the questions, and its values are lists used to store the difficulties of the questions across different batches.Merge indicates appending the difficulty of question i in the current batch r to D all [i] (List).Average refers to calculating the average value of D all [i], so as to obtain difficulty value of question i across the entire problem set.Given the additional costs of using LLM for ranking, we carefully design the ranking prompts 3 to minimize these costs, and later experiments will prove that the prior difficulty information gained can significantly reduce overall inference costs.</p>
<p>Problem Partition</p>
<p>After obtaining the problem set with difficulty ranking, we aim to find which part of the problems only require LLM to perform CoT sampling once.A simple and intuitive idea is that when LLM is very confident about a number of continuous problems sorted by difficulty (the results of all pre-samples are the same), it can be inferred that the problems easier than these problems only need one CoT sampling.Guided by this, we design the Problem Partition algorithm, illustrated in Algorithm 2. Specifically, we pre-sample the problem set sorted by difficulty from hard to easy, and store the entropy 3 See Appendix A.1 for corresponding prompts.</p>
<p>Algorithm 2 Problem Partition.</p>
<p>Require: Questions from hard to easy Q 1:N , LLM M, Demonstrations D, Pre-sample size p, Judge window k Ensure: Anchor point A, Easy part questions QEasy,
Hard part questions Q Hard 1: E all â† [ ]; a â† 1 2: for i â† 1, N do 3: Scurrent â† M(Q i , D, p) 4: E all â† E all .Append(Entropy(Scurrent)) 5: if i &gt; k and Sum(E all [i âˆ’ k : i âˆ’ 1]) = 0 then 6: A â† i 7: Break 8: end if 9: end for 10: Q Hard â† Q 1:A ; QEasy â† Q A+1:N
of pre-sampling results of each problem (S current ) in a list.We stop pre-sampling when the latest k items in the list are all zero.The remaining problems without pre-sampling are then divided into the easy part, and a single CoT sampling is performed for each.</p>
<p>Sample Size Pre-Allocation</p>
<p>After performing CoT sampling on the easy part problems, we seek to predict and allocate the sample size for the problems belonging to hard part, so as to mitigate the substantial cost brought by multiple re-samples.Considering that the required sample size for each problem should be similar to those needed for problems of comparable difficulty, we predict the sample size of the current problem based on the total sample size of the nearest m easier4 problems.Algorithm 3 shows the workflow of Sample Size Pre-Allocation.Specifically, we sample questions belonging to the hard part from easy to difficult.For the current question, we predict its pre-allocation sample size P A based on the average total sample size of its previous m questions.Then, we judge the distribution of the current samples based on the stopping criteria C5 .When the criteria is not met, we re-sample based on the expansion window e, until the sampling distribution meets the criteria for stopping sampling or the number of samples reaches the max sample size L.After sampling for the current question ends, we add its samples and total sample size to S all and N all lists respectively, in order to pre-allocate the sample size for the next question.</p>
<p>Algorithm 3 Sample Size Pre-Allocation.</p>
<p>Require: Hard part questions from easy to hard Q 1:A , LLM M, Demonstrations D, Stopping Criteria C, Max sample size L, Extend window size e, Prediction window size m Ensure: Sampling of Q 1:A based on pre-allocation S all 1:
S all â† {i : [ ] for i âˆˆ [1, A]}; N all â† [ ] 2: for i â† 1, A do 3: Scurrent â† âˆ…; P A â† 0; Ncurrent â† 0 4: if i &gt; m then 5: P A â† Average(N all [i âˆ’ m : i âˆ’ 1]) 6: Scurrent â† Scurrent.Append(M(Q i , D, P A)) 7:
Ncurrent â† Ncurrent + P A 8:</p>
<p>end if 9:</p>
<p>while Ncurrent &lt; L and NOT C(Scurrent) do 10:
Scurrent â† Scurrent.Append(M(Q i , D, e)) 11: Ncurrent â† Ncurrent + e 12:
end while 13:</p>
<p>S all â† S all .Merge(Scurrent) 14:</p>
<p>N all â† N all .Append(Ncurrent) 15: end for 3 Experiments</p>
<p>Experimental Setup</p>
<p>Benchmarks</p>
<p>We evaluate the proposed DSC on six benchmark datasets from three categories of reasoning tasks: For arithmetic reasoning, we consider MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021).MultiArith (Roy and Roth, 2015), SVAMP (Patel et al., 2021), AddSub (Hosseini et al., 2014) and ASDiv (Miao et al., 2020) are not chosen in this paper because they are relatively simple.For commonsense reasoning, CommonsenseQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021) are used.For symbolic reasoning, we use Last Letter Concatenation and Coin Flip from Wei et al. (2022a).The data version is from Kojima et al. (2022).</p>
<p>Baselines</p>
<p>We compare DSC to the following self-consistency methods: (1) SC (Wang et al., 2023) is the standard self-consistency which samples multiple reasoning paths and then derives the final answer through majority-voting; (2) ASC (Aggarwal et al., 2023) samples one by one, and stops sampling when the existing samples meet a designed stopping criteria, which measures the LLM's confidence in its current samples; (3) ESC (Li et al., 2023) proposes using small window detection to save cost, which divides the large preset sample size into several sequential small windows, and stops sampling when answers within a window are all the same.Specifically, ASC and ESC are two strong baselines for cost-efficient self-consistency methods, we reproduce both methods according to their original implementation.</p>
<p>Implementation details</p>
<p>We perform experiments using two powerful LLMs: GPT-4 (OpenAI, 2023) and GPT-3.5-Turbo6 .We calculate the cost based on the price of the API we use.All experiments are conducted in the few-shot setting without training or fine-tuning the language models.To ensure a fair comparison, we use the same prompts as Wei et al. (2022a).Following Li et al. (2023), The sampling temperature T for MATH is 0.5 while for other datasets is 0.7.For difficulty ranking, we use CoT sampling.We set the default parameters as follows: batch size B as 8 and random split rounds R as 5 for Difficulty Ranking; pre-sample size p as 4 and judge window size k as 32 for Problem Partition; extend window size e as 4 and prediction window size m as 16 for Sample Size Pre-Allocation; max sample size L as 40 for all baselines.All experiments are repeated 100 times and the average performance is reported.Unless otherwise specified, the reported cost of DSC includes the cost brought by all three substeps.</p>
<p>Main Results</p>
<p>DSC significantly reduces costs while barely affecting performance.Table 2 summarizes the cost and accuracy of SC, ASC, ESC, and proposed DSC for each dataset.We show that DSC consistently outperforms all baselines on cost by a significant margin across all datasets, while barely affecting performance.Specifically, DSC reduces the cost on GPT-4 by an average of 65.29% and on GPT-3.5-Turbo by 56.04% compared to SC.In comparison to the strongest baseline method ESC, DSC reduces the cost on GPT-4 by an average of 24.81% and on GPT-3.5-Turbo by 21.86%, which demonstrates the effectiveness of DSC.Furthermore, the above results show that the prior difficulty information gained from difficulty ranking can help greatly lower overall inference costs, even though it first introduces some additional costs.</p>
<p>DSC is scalable across various max sampling size.We conduct experiments with various max sampling size to validate the scalability of ESC.</p>
<p>Analysis of Three Sub-steps</p>
<p>To further validate the effectiveness of proposed three sub-steps, including Difficulty Ranking, Problem Partition, and Sample Size Pre-Allocation, we conduct experimental analyses on each of them respectively.</p>
<p>Difficulty Ranking</p>
<p>Difficulty Ranking exhibits a good correlation with humans.As shown in Table 5, we calculate Spearman, Pearson, and Kendall correlation coefficients on seven subsets of MATH benchmark.Overall, GPT-4 demonstrates a high consistency with human labels, while the weaker GPT-3.5-Turboshows a moderate consistency.Specifically, both GPT-4 and GPT-3.5-Turborank weakly on Geometry difficulty, which may be due to the fact that text LLMs cannot intuitively assess the difficulty of geometry problems through visual information  like humans can.</p>
<p>Difficulty Ranking produces an acceptable cost.</p>
<p>As we instruct LLM to produce extremely concise outputs for Difficulty Ranking, the cost of output tokens brought by Difficulty ranking is very low8 .Table 7 gives comparison of input tokens brought by Difficulty Ranking and few-shot reasoning across six datasets.We show that the average input cost for each problem's difficulty ranking is less than the average single input cost for its few-shot reasoning.Considering that ESC and ASC require multiple re-samples for reasoning (multiple fewshot inputs), this additional input cost of Difficulty Ranking is totally acceptable.</p>
<p>Problem Partition</p>
<p>Problem Partition proves to be effective across all datasets.As shown in Table 6, we conduct an ablation study on Problem Partition (step 2).We show that the removal of Problem Partition results in an increase in both input and output tokens of DSC across all six datasets, which validates the effectiveness and generalizability of Problem Partition.Furthermore, we find that the removal of Problem Partition has a small impact on output tokens when it comes to MATH dataset, which could be due to the fact that for datasets with overall higher difficulty, the easy part problem (only require one CoT sample) for LLM is fewer.</p>
<p>Judge window size is essential for the accuracy of DSC.Regarding the intuitive idea proposed in Section 2.2, a straightforward question is how large the judge window size k needs to be to ensure the accuracy on simpler problems with single CoT sampling.To answer this, we conduct an experiment of DSC under different judge window size k with GPT-4 on GSM8K.As shown in Figure 4, the accuracy increases with the rise of k until it reaches 32.Meanwhile, the cost of DSC also increases with the enlargement of k, as the easy part divided by the problem partition decreases with its increase.Therefore, it is a good choice to set a smaller judge window size while ensuring accuracy. 9</p>
<p>Sample Size Pre-Allocation</p>
<p>Sample Size Pre-Allocation significantly alleviates the substantial input token costs brought by multiple re-samples.We conduct an ablation study of proposed Sample Size Pre-Allocation (step 3) to validate its effectiveness.As shown in Table 6, with the removal of Sample Size Pre-Allocation, the count of input tokens on DSC rose on all datasets on GPT-3.5-Turbo and GPT-4 (with the exception of GPT-4 on Coinflip), validating the effectiveness and generalizability of Sample Size Pre-Allocation.Moreover, we notice that the input tokens of ASC and ESC far exceed that of SC, which once again confirms that multiple re-samples will bring a large amount of additional overhead.Meanwhile, DSC significantly reduced the number of input tokens through Pre-Allocation, keeping it comparable to SC.In addition, we find that Sample Size Pre-Allocation leads to a slight increase in output tokens (due to the predicted sample size of some questions exceeding the actual requirement).</p>
<p>Overall, we believe it is a good choice to trade a small number of output tokens for a significant saving in input tokens.</p>
<p>Prediction window size is the key to trade output tokens for more savings in input tokens.Given that Sample Size Pre-Allocation can trade output tokens for savings in input tokens, a natural question is when the max total savings can be achieved.As shown in Figure 5, we count the tokens of input and output and the corresponding cost of DSC 9 We select 32 as the default value for k. under different prediction window size m.The experimental results show that the cost of DSC decreases rapidly first and then slighly increases with the increase of m, and the input tokens also follow a similar trend.As m rises, simpler questions predict larger sample sizes for the current query.However, when m becomes sufficiently large, the predicted sample size falls below the actual value, leading to more re-sampling.In contrast, output tokens gradually decrease because, as m increases, the predicted sample size continues to decline, reducing redundancy in output token sampling.In summary, it is suggested to select m within a relatively moderate range (e.g. from 8 to 32).</p>
<p>Related Work</p>
<p>Chain-of-thought Reasoning Chain-of-thought prompting has been proven to be an effective method of solving complex reasoning problems (Wei et al., 2022a).By following the pattern of gradually solving sub-problems, few-shot CoT (Fu et al., 2023) are capable of stimulating LLM reasoning abilities.On this basis, Least-to-most prompting (Zhou et al., 2023) suggest explicitly splitting the problem and solving them step by step.Zheng et al. (2023) reach the final answer by iteratively generating answers and using the previously generated answers as context hints.</p>
<p>Self-Consistency Self-consistency (Wang et al., 2023) refers to a simple decoding strategy for further improving reasoning performance, leveraging the fact that complex reasoning tasks typically allow for more than one correct reasoning path.Li et al. (2024) assign appropriate weights for answer aggregation to achieve adaptive self-consistency.Jain et al. (2023) and Wang et al. (2024) extend it for open-ended generation tasks like code generation and text summarization.However, they require multiple sampling with the pre-set size, which will incur much more computation cost.To achieve cost-efficient self-consistency, Aggarwal et al. (2023) introduce an adaptive stopping criterion based on the amount of agreement between the samples so far.Li et al. (2023) divide the large preset sample size into several sequential small windows, and stop sampling when answers within a window are all the same.</p>
<p>Difficulty-adaptive Test-Time Scaling Testtime scaling (Wu et al., 2024a;Snell et al., 2024;Brown et al., 2024) has been widely explored as an effective approach to improving model performance.However, conventional test-time scaling applies a uniform amount of computation to all queries, regardless of their difficulty, resulting in unnecessary computational overhead.</p>
<p>Conclusion</p>
<p>In this work, we propose a novel cost-efficient decoding method called Difficulty-Adaptive Self-Consistency (DSC), which fully leverages the difficulty information of given problem set to allocate computational resources based on their respective levels of difficulty, so as to alleviate issues of redundant sampling on simple questions and multiple re-sampling limitations that current cost-efficient self-consistency methods face.Extensive experiments show that DSC consistently surpasses two strong cost-effcient self-consistency baselines ASC and ESC by a significant margin on six popular benchmarks, while attaining comparable accuracy.Additional experiments confirm the effectiveness of the proposed three-step algorithms.</p>
<p>Limitations</p>
<p>Despite the remarkable efficiency gain on variety of reasoning tasks, the current implementation of DSC still suffers from the following two limitations:</p>
<p>â€¢ As the performance of difficulty ranking declines when handling mixed-type problems (Table 10), DSC may encounter challenges when applied in scenarios requiring real-time reasoning for mixed-type problems.Exploring further classification of problems by type or optimizing the current Difficulty Ranking algorithm could enhance its applicability.</p>
<p>â€¢ Given that DSC demands an awareness of the test set to rank samples based on their difficulty, its use could be restricted in scenarios where a single user is only permitted one input at a time.Nevertheless, the application scenarios of DSC include but are not limited to:</p>
<p>(1) The server end (such as OpenAI company, etc) simultaneously receives a large number of query requests from users.(2) A user possesses a batch of data that requires a one-time inference.</p>
<p>Ethics Statement</p>
<p>All of the datasets used in this study were publicly available, and no annotators were employed for our data collection.We confirm that the datasets we used did not contain any harmful content and was consistent with their intended use (research).We have cited the datasets and relevant works used in this study.</p>
<p>A Appendix</p>
<p>A.1 Prompt for Difficulty Ranking</p>
<p>Your task is to rank the given questions from easy to hard based on their difficulty level.Questions to be evaluated: Q1:{Question 1} Q2:{Question 2} ...</p>
<p>Qn: {Question n}</p>
<p>The output format should be a commaseparated list containing the Qnumber of corresponding question.Do not give any explanation.</p>
<p>Difficulty Ranking result (from easy to hard):</p>
<p>A.2 Comparison of Inference Time Between Different Methods</p>
<p>Considering the inference time is important for real world scenarios, we calculate the inference time of different methods on the MATH test set (5000 questions) with GPT-4.As shown in Table 8, for DSC, the ranking time corresponds to the time produced by step 1, while sampling time corresponds to the time generated by step 2 and step 3.The results show that, compared to SC, both ASC and ESC require significantly more time due to the need for a large amount of resampling.DSC, on the other hand, effectively mitigates this issue by pre-allocating the sample size for questions.</p>
<p>A.3 Comparison with ESC under Different Window Size</p>
<p>To further demonstrate the effectiveness of DSC compared to ESC, we make a comparison between the performance of DSC and ESC under different window sizes on the MATH dataset using GPT-4, while maintaining the rest of the settings completely consistent with the main experiment.As shown in Table 9, when the window size is greater than or equal to 5, ESC can maintain its Accuracy and its cost increases as the window size enlarges.However, when the window size is less than 5, the Accuracy of ESC drops significantly due to excessively relaxed constraints.Overall, the performance of DSC consistently surpasses that of ESC by a large margin.</p>
<p>A.5 Hyperparameter Experiment of Difficulty Ranking</p>
<p>As shown in Figure 6, we conduct hyperparameter experiments on the proposed Difficulty Ranking algorithm.The experimental results indicate that a large batch size (â‰¥ 16) leads to a decrease in LLM ranking performance, which is consistent with our expectations.For both GPT-3.5-Turbo and GPT-4, difficulty ranking approximately converges in the 5th round.Therefore, we choose 5 and 8 as the default values for iteration and batch size for Difficulty Ranking, respectively.</p>
<p>A.6 Necessity of Few-Shot Setting in DSC Previous studies (Wei et al., 2022a;Wang et al., 2023) have experimentally demonstrated that compared to zero-shot (without demonstrations), few-  shot (with demonstrations) significantly improves the performance of LLMs on reasoning tasks.Therefore, we adopted the same few-shot setting as SC, ASC, and ESC.To further verify this, we test the Accuracy of SC on GPT-3.5-Turbounder both zero-shot and few-shot (8-shot as default) settings with different sample sizes N on MATH500.</p>
<p>The results (Table 11) show that removing demonstrations leads to a significant performance drop, validating the necessity of multiple demonstrations.As a matter of fact, during our research, we tried to dynamically allocate the number of demonstrations based on question difficulty to reduce input costs.However, we find that reducing the number of demonstrations significantly de-creased performance, regardless of the difficulty of the questions.</p>
<p>Moreover, recent study (Agarwal et al., 2024) has demonstrated that a greater number of demonstrations generally leads to more performance improvements.For most tasks, the optimal number of demonstrations is often greater than 512, far exceeding the default setting of 8 in DSC, which results in much higher input costs.This further highlights the value of DSC.</p>
<p>A.7 Comparison of DSC with Self-Correction and Verifier-Based Methods</p>
<p>We consider that self-correct (Wu et al., 2024b), verifier-based methods (Wu et al., 2024a), and Selfconsistency (Wang et al., 2023) can all be seen as approaches that aim to improve model performance by increasing computational effort, albeit through different implementation paths.In comparison, DSC takes a different direction by focusing on optimizing efficiency-reducing computational costs as much as possible while maintaining model performance.This distinction positions DSC as complementary to self-correct, verifier-based methods, and Self-consistency, with a unique focus on efficiency.Furthermore, while DSC is implemented based on Self-consistency, its concept of dynamically allocating computational resources according to question difficulty could also be adapted to self-correct and verifier-based methods to enhance efficiency.For example, in the case of self-correct, this might involve assessing whether a query requires selfcorrection based on its difficulty and estimating the necessary degree of correction (e.g., the number of iterations).Similarly, for verifier-based methods, which already involve multiple sampling followed by verification, integrating DSC could potentially lead to more efficient inference processes.</p>
<p>A.8 Background of ESC and ASC ESC ESC proposes extension window sampling, where the extension window is a fixed preset value w, Each time, w entries are sampled through the LLM and added to the sampling set.If the answers of the current w samples are the same or the preset maximum sampling value is reached, the sampling is stopped.The core idea of ESC is that if the model's one-time w sample answers are completely the same, it can be considered that it has a high confidence in this answer, and sampling can be stopped.</p>
<p>ASC ASC (Aggarwal et al., 2023) proposes the Dirichlet Stopping Criteria, where sampling is conducted one by one.After each sampling, the Dirichlet Stopping Criteria is used to determine whether all current samples meet a specific distribution.If they do, the sampling is stopped.If not, the one-byone sampling continues until the preset maximum sampling value is reached.The Dirichlet Stopping Criteria is shown in Equation 1, where v represents the current set of samples, m is the number of v, C thresh is the preset threshold (set to 0.95 for ASC in default), and p 1 is the probability of the most frequently occurring answer in the set v. The core idea of ASC is to measure the two answers with the highest probability in the current set of samples.If the difference between the probability of the answer with the highest probability and the second highest probability exceeds a threshold (C thresh ), it can be considered that the model is very confident in the answer with the highest probability, and sampling can be stopped.For further details, please refer to the original paper.
P p 1 &gt; m max i=2 p i | v &gt; C thresh
(1)</p>
<p>A.9 Application Scenarios of DSC</p>
<p>As discussed in the Limitations section, DSC could be constrained in scenarios where a single user is restricted to one input at a time.However, it is widely applicable to common scenarios including: (1) servers (e.g., OpenAI or similar providers) managing a large number of simultaneous query requests from different users (e.g., ChatGPT receives requests from multiple users to solve multiple math problems simultaneously), and (2) cases where a user perform inference on a batch of data (E.g., Submitting multiple requests at once to efficiently receive feedback).For these application scenarios, Recently, an increasing number of studies have focused on performing inference on large volumes of samples with LLMs (Cheng et al., 2023;Lin et al., 2024;Son et al., 2024;Liu et al., 2024a;Cong et al., 2025), which is computationally and financially costly in industrial and real-world applications.Therefore, the application scenarios of DSC have become a key focus in current research, attracting extensive attention from the academic and industrial communities, and we believe DSC's potential applications and value will be more and more substantial in the future.</p>
<p>Figure 1 :
1
Figure1: The proportion of identical inference results between CoT and SC on GSM8K with GPT-3.5-Turbo and GPT-4.We set sample size of SC as 40.</p>
<p>Figure 2 :
2
Figure 2: Leveraging their prior knowledge, humans assess the difficulty level of a problem before solving it, and allocate appropriate time for its resolution based on the difficulty.</p>
<p>Figure 4 :
4
Figure 4: Cost ($) and accuracy (%) of DSC under different judge window size k with GPT-4 on GSM8K.</p>
<p>Figure 5 :
5
Figure 5: Tokens of input and output and the corresponding cost ($) of DSC under different prediction window size m with GPT-4 on MATH.</p>
<p>Figure 6 :
6
Figure 6: Performance of Difficulty Ranking under different batch sizes and iterations on MATH with GPT-3.5-Turbo and GPT-4.</p>
<p>Table 2 :
2
Table 3 shows the performance across different maximum sampling sizes.First we can see the Accuracy (%) and cost ($) across six reasoning benchmarks.The best performance of cost is highlighted in bold.47.97 0.0131 â†“ 48.69 0.0163 â†“ 49.10 0.0193 â†“ 49.39 0.0220 â†“ 49.62 ESC 0.0075 â†“ 47.98 0.0108 â†‘ 48.70 0.0140 â†‘ 49.12 0.0172 â†‘ 49.41 0.0203 â†‘ 49.65 DSC 0.0069 â†‘ 47.98 0.0097 â†‘ 48.71 0.0123 â†‘ 49.13 0.0148 â†‘ 49.42 0.0171 â†‘ 49.66
ModelMethodMATHGSM8KCSQASQALetterCoinflipCostAccCostAccCostAccCostAccCostAccCostAccSC0.4220 58.52 0.3509 95.81 0.1280 85.82 0.1493 81.89 0.1691 94.51 0.1528 100GPT-4ASC ESC0.5726 58.48 0.1712 95.79 0.1639 85.82 0.1323 81.89 0.0602 94.50 0.0657 100 0.4062 58.49 0.1014 95.80 0.0767 85.81 0.0647 81.89 0.0375 94.51 0.0338 100DSC0.3142 58.51 0.0699 95.79 0.0598 85.81 0.0516 81.88 0.0259 94.50 0.0264 100SC0.0181 49.43 0.0094 83.22 0.0030 76.80 0.0043 71.47 0.0043 80.44 0.0037 76.60GPT-3.5-TurboASC ESC0.0193 49.39 0.0064 83.18 0.0034 76.78 0.0032 71.47 0.0018 80.43 0.0034 76.59 0.0172 49.41 0.0048 83.19 0.0017 76.79 0.0019 71.47 0.0014 80.43 0.0020 76.59DSC0.0148 49.42 0.0036 83.19 0.0012 76.79 0.0015 71.46 0.0011 80.42 0.0016 76.60ModelMethod1624324048CostAccCostAccCostAccCostAccCostAccSC0.1791 -57.34 0.2602 -57.95 0.3411 -58.27 0.4220 -58.52 0.5030 -58.67GPT-4ASC ESC0.3038 â†“ 57.33 0.4038 â†“ 57.93 0.4923 â†“ 58.23 0.5726 â†“ 58.48 0.6470 â†“ 58.62 0.1870 â†“ 57.34 0.2631 â†“ 57.94 0.3361 â†‘ 58.25 0.4062 â†‘ 58.49 0.4723 â†‘ 58.64DSC0.1659 â†‘ 57.34 0.2192 â†‘ 57.94 0.2680 â†‘ 58.25 0.3142 â†‘ 58.51 0.3585 â†‘ 58.66SC0.0074 -47.98 0.0110 -48.71 0.0145 -49.13 0.0181 -49.43 0.0216 -49.67GPT-3.5-TurboASC0.0095 â†“</p>
<p>Table 3 :
3
Reasoning accuracy (%) and corresponding cost ($) with various max sampling size on MATH.We mark the performance of cost poorer than SC with â†“ and that better than SC with â†‘.The best performance of cost is highlighted in bold.
Method Input tokens Output tokensCostAccSC84660450.3881 73.19ASC1598232650.6754 73.14ESC445844150.3986 73.18DSC161738570.2799 73.18Table 4: Accuracy (%) and cost ($) on GSM8K withsmall open-source language model Mistral-7B-Instruct-v0.3.performance of SC continuously improves as maxsampling size L increases, which is consistent withthe results in (Wang et al., 2023). On this basis,ESC can significantly save costs while maintainingperformance for different L. On the contrary, dueto the substantial additional input cost brought bymultiple re-samples, ASC and ESC result in highercost than SC when the cost savings of output tokensare limited.DSC Maintains Generalizability on Small Open-source Language Model. To further explore theeffectiveness of DSC on small open-source models,
we conduct experiments on the open-source model Mistral-7B-Instruct-v0.3 (Jiang et al., 2023) using GSM8K dataset.As shown in Table 4, we convert the token cost into price according to that of GPT-4 for a simpler comparison and keep the other settings completely consistent with the main experiment 7 .The experimental results indicate that DSC has the potential to work effectively on smaller models.</p>
<p>Table 5 :
5
Performance of Difficulty Ranking on MATH dataset.
ModelMetricGeometryCounting &amp; Prealgebra ProbabilityIntermediate Number Precalculus Algebra Avg Algebra TheorySpearman0.5400.5780.5650.6540.6540.6840.691 0.624GPT-4Pearson0.5350.5850.5670.6600.6540.6910.695 0.627Kendall0.4170.4480.4400.5190.5200.5470.547 0.491Spearman0.4150.3070.3620.4840.4450.4970.573 0.440GPT-3.5-TurboPearson0.4150.3140.3700.4950.4520.5140.578 0.448Kendall0.3180.2330.2790.3750.3440.3830.446 0.340ModelMethodMATHGSM8KCSQASQALetterCoinflipInput Output Input Output Input Output Input Output Input Output Input OutputSC5766746 846 5426 726 1770 611 2184 302 2667 428 2332ASC11100 3994 4266 719 4805 327 3726 351 1397 308 1722 234GPT-4ESC3574 4983 1423 982 1644 455 1228 465454401540294DSC8344606 97549597827182231031411342858w/o step 2 8474701 1035 755 1081 370873381353313431235w/o step 3 2896 4142 1033 479 1068 25390329032810842858SC576 11851 846 6005 726 1749 611 2644 302 2735 428 2316ASC13421 8380 8075 1577 5633 365 4698 535 2150 487 4932 624GPT-3.5-TurboESC4223 10045 2889 2273 1946 506 1599 731742673 1613 819DSC7939456 1410 1809 1140 376985591470535704736w/o step 2 7999610 1426 1879 1172 401997605482584707761w/o step 3 3485 8634 2144 1679 1467 362 1238 560568513 1290 648</p>
<p>Table 6 :
6
Ablation study of Problem Partition (step 2) and Sample Size Pre-Allocation (step 3).We count input and output tokens across six reasoning benchmarks.To simplify the comparison, the tokens produced by Difficulty Ranking are not counted here.
TaskMATH GSM8K CSQA SQA Letter CoinflipRanking403364261 130 161308Reasoning 576846726 611 302428Table 7: Comparison of average input tokens for eachquestion brought by Difficulty Ranking and few-shotreasoning across six reasoning datasets.</p>
<p>Table 10 :
10
Performance comparison of Difficulty Ranking under mixed and unmixed problem types on MATH.
ModelMetricUnmixed MixedSpearman0.64510.5148GPT-4Pearson0.64870.5197Kendall0.50600.3931Spearman0.48280.4070GPT-3.5-TurboPearson0.48870.4145Kendall0.36970.3073SettingN15102040Zero-shot 27.01 31.44 33.84 35.21 35.93Few-shot33.08 38.54 41.39 43.01 43.94Table 11: Accuracy Comparison of SC under Zero-Shotand Few-Shot Settings with Different Sample Sizes Non MATH500 Using GPT-3.5-Turbo
As the pre-allocated sample size could be larger than the required sample size (causing higher output costs), we use the sample size of easier rather than harder neighboring problems for prediction.
Following ASC, we use Dirichlet Stopping Criteria as default. See Appendix A.8 for more details.
We use the "2023-05-15" version of API for both.
Please note that for DSC, the cost of all three sub-steps are included.
With an average consumption of 15 tokens for each question.
AcknowledgementsThis work is supported by the Beijing Natural Science Foundation, China (Nos.4222037, L181010).
Many-shot in-context learning. Rishabh Agarwal, Avi Singh, M Lei, Bernd Zhang, Luis Bohnet, Rosias, C Y Stephanie, Biao Chan, Aleksandra Zhang, Hugo Faust, Larochelle, ICML 2024 Workshop on In-Context Learning. 2024</p>
<p>Let's sample step by step: Adaptiveconsistency for efficient reasoning and coding with llms. Pranjal Aggarwal, Aman Madaan, Yiming Yang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Large language monkeys: Scaling inference compute with repeated sampling. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Christopher Quoc V Le, Azalia RÃ©, Mirhoseini, arXiv:2407.217872024arXiv preprint</p>
<p>SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Justin Chih-Yao Chen, Archiki Prasad, arXiv:2409.12147Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. 2024. Magicore: Multi-agent, iterative, coarseto-fine refinement for reasoning. arXiv preprint</p>
<p>Batch prompting: Efficient inference with large language model apis. Zhoujun Cheng, Jungo Kasai, Tao Yu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track. the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track2023</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>Baton: Enhancing batch-wise inference efficiency for large language models via dynamic re-batching. Peizhuang Cong, Chen Qizhi, Haochen Zhao, Tong Yang, THE WEB CONFERENCE 2025. 2025</p>
<p>Learning how hard to think: Input-adaptive allocation of lm computation. Mehul Damani, Idan Shenfeld, Andi Peng, Andreea Bobu, Jacob Andreas, arXiv:2410.047072024arXiv preprint</p>
<p>Specializing smaller language models towards multi-step reasoning. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot, 10.48550/arXiv.2301.12726CoRR, abs/2301.127262023</p>
<p>Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 10.1162/tacl_a_00370Trans. Assoc. Comput. Linguistics. 92021</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 20212021. December 2021</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, 10.3115/v1/d14-1058Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarACL2014. 2014. October 25-29, 2014A meeting of SIGDAT, a Special Interest Group of the ACL</p>
<p>Self-consistency for open-ended generations. Siddhartha Jain, Xiaofei Ma, 10.48550/arXiv.2307.06857CoRR, abs/2307.06857Anoop Deoras, and Bing Xiang. 2023</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, NeurIPS. 2022</p>
<p>Turning dust into gold: Distilling complex reasoning capabilities from llms by leveraging negative data. Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, Kan Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Escape sky-high cost: Early-stopping selfconsistency for multi-step reasoning. Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, Kan Li, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Batchprompt: Accomplish more with less. Jianzhe Lin, Maurice Diesendruck, Liang Du, Robin Abraham, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Cliqueparcel: An approach for batching llm prompts that jointly optimizes efficiency and faithfulness. Jiayi Liu, Tinghan Yang, Jennifer Neville, arXiv:2402.148332024aarXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 10.1162/tacl_a_00638Transactions of the Association for Computational Linguistics. 112024b</p>
<p>Adaptive inference-time compute: Llms can predict if they can do better, even mid-generation. Rohin Manvi, Anikait Singh, Stefano Ermon, arXiv:2410.027252024arXiv preprint</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, 10.18653/v1/2020.acl-main.92Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020OnlineAssociation for Computational Linguistics2020. July 5-10, 2020</p>
<p>10.48550/arXiv.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021Association for Computational Linguistics2021. June 6-11, 2021</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, 10.18653/v1/d15-1202Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalThe Association for Computational Linguistics2015. 2015. September 17-21, 2015</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Ku, arXiv:2408.03314mar. 2024arXiv preprint</p>
<p>Multi-task inference: Can large language models follow multiple instructions at once?. Guijin Son, Sangwon Baek, Sangdae Nam, Ilgyun Jeong, Seungone Kim, 10.18653/v1/2024.acl-long.304Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/n19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019. June 2-7, 20191</p>
<p>Integrate the essence and eliminate the dross: Finegrained self-consistency for free-form language generation. Xinglin Wang, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Boyuan Pan, Heda Wang, Yao Hu, Kan Li, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022aIn NeurIPS</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 2022b35</p>
<p>An empirical analysis of compute-optimal inference for problemsolving with language models. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang, 2024a</p>
<p>Large language models can self-correct with key condition verification. Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, Meng Jiang, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024b</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, 10.48550/arXiv.2304.09797CoRR, abs/2304.097972023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael SchÃ¤rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, Ed H Chi, ICLR 2023The Eleventh International Conference on Learning Representations. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>            </div>
        </div>

    </div>
</body>
</html>
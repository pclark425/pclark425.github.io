<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8238 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8238</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8238</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-277954804</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.14128v1.pdf" target="_blank">TALES : Text Adventure Learning Environment Suite</a></p>
                <p><strong>Paper Abstract:</strong> Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open-and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at microsoft.github.io/tale-suite .</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8238.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8238.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimonSays+Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simon Says With Memory (TEXTWORLDEXPRESS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new TEXTWORLDEXPRESS game mode introduced in this paper that supplies the agent with a list of actions at game start; the agent must remember and reproduce the sequence exactly across long horizons, failing and restarting on any mistake.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Zero-shot / Reasoning baseline agents (as evaluated on Simon Says With Memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline agents used by the paper: (1) a zero-shot agent that issues single short action phrases using a minimal system prompt; (2) a reasoning-agent variant which uses a reasoning model backbone (e.g., Claude-3.7-Sonnet) and optionally produces reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>various (Claude-3.7-Sonnet, Claude-3.5-Sonnet, Gemini-2.5-pro-preview, o1, gpt-4o, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>State-of-the-art proprietary and open LLMs of differing sizes and families used in zero-shot and reasoning-agent experiments (no model architectures or sizes are introduced by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Simon Says With Memory (TEXTWORLDEXPRESS)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>An instruction-following memory task: the environment gives an explicit list of actions to follow; the agent must reproduce them in order, receiving a point per correct action and restarting if out-of-order. Designed to test short-term episodic recall / instruction-following across long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>task-provided short-term episodic list (game-state memory requirement)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>No explicit agent-side memory architecture is required by the benchmark; the 'memory' is the list provided by the environment which the agent must recall. The paper does not introduce or require a separate memory module for agents in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Agents must include the environment-provided list in their input context (observation) and then produce actions that reproduce the list; beyond that, no additional retrieval or external memory mechanism is mandated by the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Top LLMs achieved near-maximum normalized score on the hardest mode of Simon Says With Memory (the paper reports that, except for a single notable exception, selected LLMs achieved near-maximum score; specific per-model numeric scores are not reported in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>The paper uses Simon Says With Memory as a filter: models included in deeper transcript analysis are those that achieved >=90% on the 100-step version, but the paper does not present systematic ablations comparing explicit memory modules versus none on this game.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Not specified; the benchmark itself tests recall of an explicit provided sequence rather than agent-side memory architectures. The paper's operational conclusion is that passing this task (i.e., being able to follow a provided long sequence) is a prerequisite for meaningful evaluation on the full TALES suite.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>None reported specific to memory architectures—failures would be due to inability to keep the provided list in the model's effective context or to follow long sequences, but no direct analysis of different memory strategies is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>The paper recommends using Simon Says With Memory as a unit test to ensure an agent has minimal long-horizon instruction-following / recall capability before evaluating on the larger TALES suite.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TALES : Text Adventure Learning Environment Suite', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8238.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8238.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work cited by the paper showing that repeated reflection plus an episodic memory buffer can improve LLM agent performance in text environments; presented in related work as an example of memory-enhanced LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion (as described in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An autonomous LLM-driven agent that augments online interaction with iterative self-reflection and an episodic memory buffer to store and reuse past observations/lessons.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>text-adventure / interactive tasks (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Reflexion was applied to interactive tasks/text environments to demonstrate the utility of iterative self-reflection and an episodic memory buffer; cited here as evidence that memory can improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic memory buffer (external, persistent across steps for storing past observations and reflections)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Dynamic episodic buffer storing past action–observation pairs and reflection outputs; used to inform future prompts/actions (exact internal details are in the Reflexion paper, not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Memory entries and reflection summaries are fed back into subsequent LLM prompts to influence action generation (i.e., retrieval-augmented prompt augmentation via an episodic store).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as prior art demonstrating improvement from adding episodic memory; this paper does not run Reflexion or perform a direct ablation study itself.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>The referenced work suggests episodic buffers combined with iterative reflection improves performance, but the TALES paper does not evaluate which external memory strategy is best.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in TALES beyond the citation; the TALES authors note that memory methods from prior work (like Reflexion) exist but do not reproduce or directly compare them in this benchmark study.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>TALES highlights that episodic-memory approaches (e.g., Reflexion) have been promising in prior literature, and suggests that more systematic evaluation of memory strategies in benchmarks like TALES is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TALES : Text Adventure Learning Environment Suite', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8238.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8238.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1 (two-step thinking-trace then action pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model pipeline included in the experiments that first generates explicit thinking traces and then issues a second query to generate the next action using those traces, effectively using a transient scratchpad/working-memory step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent variant used in the experiments that produces explicit thinking traces (internal chain-of-thought style outputs) and then issues a second LLM query using those traces to generate the final action.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>DeepSeek-R1 (architecture not fully specified in the paper's text excerpt)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A pipeline-style agent (two-stage): first stage generates reasoning / thinking traces; second stage consumes those traces to produce actions. Detailed model internals are not provided in the TALES paper excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TALES suite (various games incl. TEXTWORLD, ALFWORLD, SCIENCEWORLD, JERICHO)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A diverse suite of text-adventure games used to evaluate agent reasoning across grounded, inductive, deductive, and spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>scratchpad / ephemeral working memory (generated thinking traces)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Two-stage pipeline: (1) generate internal reasoning/thinking traces; (2) feed those traces into a follow-up prompt to produce the actionable command. The traces act as an ephemeral internal memory that is passed between LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Explicitly feed generated thinking traces into a second LLM call (i.e., the traces are concatenated into the prompt for action generation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>The paper notes the DeepSeek-R1 workflow in a footnote (generating traces then using a second query) but does not present a controlled ablation comparing with/without the trace step in the TALES experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Not identified; TALES does not conclude whether the two-stage thinking-trace pipeline is superior to other memory strategies within its experimental setup.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No explicit ablation provided here; the broader paper highlights that even when reasoning traces exist they can (if discarded or poorly integrated) contradict later steps or be inconsistently applied, limiting benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>TALES suggests that generating reasoning traces alone is insufficient unless those traces are meaningfully integrated into subsequent decisions; the paper notes they discarded reasoning traces for most agents' forward context, which reduced potential benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TALES : Text Adventure Learning Environment Suite', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8238.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8238.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.7-Sonnet (reasoning agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.7-Sonnet (reasoning-model backbone used for agentic play)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The top-performing reasoning model in TALES experiments; it can produce stepwise reasoning traces but the paper reports that those traces were often discarded and not used as persistent memory during gameplay.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reasoning agent (Claude-3.7-Sonnet backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A reasoning-agent configuration powered by Claude-3.7-Sonnet which at each step produces reasoning traces about the next best action; traces were generated but (in TALES' initial experiments) were thrown away rather than persistently added to the agent's context.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Claude-3.7-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A high-performing reasoning-focused LLM (proprietary model family). Specific model size/architecture not provided in the paper excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TALES suite (including ALFWORLD, SCIENCEWORLD, TEXTWORLDEXPRESS, JERICHO, Simon Says)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A diverse set of text-adventure benchmarks stressing long-horizon, grounded, deductive, inductive, and spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>The agent can (and did) generate per-step reasoning (thinking) traces, but the TALES evaluation deliberately discarded these traces at each step (i.e., they were not concatenated into future prompts), so there is no persistent memory architecture used in the experiments as-run.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>None in the released experiments; reasoning traces were not integrated into subsequent prompts (they were thrown away), producing occasional contradictions across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>TALES does not present an ablation comparing Claude-3.7-Sonnet with integrated persistent memory vs the discarded-traces setup; it notes this as a limitation and an area where memory could plausibly help.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Not determined by TALES; the authors observe that although Claude-3.7-Sonnet sometimes displays all reasoning types, failures in grounded and inductive reasoning arise partly from not integrating prior reasoning/failure information into future steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The authors explicitly report that discarding reasoning traces leads to contradictions between steps and failures to properly incorporate past failure signals; this particularly degrades performance on long-horizon games (e.g., SCIENCEWORLD, JERICHO).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>TALES recommends persistent, well-integrated memory strategies (e.g., retaining and retrieving past reasoning/failure observations) to mitigate grounded and inductive reasoning failures, but does not experimentally validate a single best approach in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TALES : Text Adventure Learning Environment Suite', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8238.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8238.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot single-query agent baseline (minimal prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The simplest agent used in TALES: a single-shot policy that maps the current textual observation (and environment feedback) to a single short action phrase, with no explicit memory beyond the LLM's prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Zero-shot agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A minimal agent that receives the current observation + feedback as input and outputs a single short action phrase (temperature often set to 0); used as a baseline to evaluate inherent LLM capabilities without additional scaffolding or memory augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>various (same pool of LLMs evaluated in TALES: Claude, Gemini, GPT variants, Llama family, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Diverse set of LLMs used in a zero-shot fashion; the TALES paper evaluates 34 models in total across frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TALES suite (all included games)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Unified benchmark of text-adventure games intended to probe multiple reasoning skills under minimal scaffolding (no added domain-specific knowledge or memory modules provided by the authors).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>The paper contrasts zero-shot baseline performance with the reasoning-agent variant (which can produce reasoning traces), but does not present a controlled study specifically comparing zero-shot with explicit memory-augmented versions within TALES.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Zero-shot agents routinely fail on long-horizon games and those requiring inductive or grounded reasoning because they lack mechanisms to persistently incorporate prior failures, observations, or plans beyond the immediate prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>TALES suggests that zero-shot LLMs are insufficient for hard human-authored games and that some form of memory or iterative reflection is likely necessary for robust long-horizon performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TALES : Text Adventure Learning Environment Suite', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks <em>(Rating: 1)</em></li>
                <li>Playing text-adventure games with repeated reflection and an episodic memory buffer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8238",
    "paper_id": "paper-277954804",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "SimonSays+Memory",
            "name_full": "Simon Says With Memory (TEXTWORLDEXPRESS)",
            "brief_description": "A new TEXTWORLDEXPRESS game mode introduced in this paper that supplies the agent with a list of actions at game start; the agent must remember and reproduce the sequence exactly across long horizons, failing and restarting on any mistake.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Zero-shot / Reasoning baseline agents (as evaluated on Simon Says With Memory)",
            "agent_description": "Baseline agents used by the paper: (1) a zero-shot agent that issues single short action phrases using a minimal system prompt; (2) a reasoning-agent variant which uses a reasoning model backbone (e.g., Claude-3.7-Sonnet) and optionally produces reasoning traces.",
            "llm_model_name": "various (Claude-3.7-Sonnet, Claude-3.5-Sonnet, Gemini-2.5-pro-preview, o1, gpt-4o, etc.)",
            "llm_model_description": "State-of-the-art proprietary and open LLMs of differing sizes and families used in zero-shot and reasoning-agent experiments (no model architectures or sizes are introduced by this paper).",
            "benchmark_name": "Simon Says With Memory (TEXTWORLDEXPRESS)",
            "benchmark_description": "An instruction-following memory task: the environment gives an explicit list of actions to follow; the agent must reproduce them in order, receiving a point per correct action and restarting if out-of-order. Designed to test short-term episodic recall / instruction-following across long sequences.",
            "memory_used": true,
            "memory_type": "task-provided short-term episodic list (game-state memory requirement)",
            "memory_architecture": "No explicit agent-side memory architecture is required by the benchmark; the 'memory' is the list provided by the environment which the agent must recall. The paper does not introduce or require a separate memory module for agents in these experiments.",
            "memory_integration_strategy": "Agents must include the environment-provided list in their input context (observation) and then produce actions that reproduce the list; beyond that, no additional retrieval or external memory mechanism is mandated by the benchmark.",
            "performance_with_memory": "Top LLMs achieved near-maximum normalized score on the hardest mode of Simon Says With Memory (the paper reports that, except for a single notable exception, selected LLMs achieved near-maximum score; specific per-model numeric scores are not reported in the main text).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "The paper uses Simon Says With Memory as a filter: models included in deeper transcript analysis are those that achieved &gt;=90% on the 100-step version, but the paper does not present systematic ablations comparing explicit memory modules versus none on this game.",
            "best_memory_strategy": "Not specified; the benchmark itself tests recall of an explicit provided sequence rather than agent-side memory architectures. The paper's operational conclusion is that passing this task (i.e., being able to follow a provided long sequence) is a prerequisite for meaningful evaluation on the full TALES suite.",
            "limitations_or_failure_cases": "None reported specific to memory architectures—failures would be due to inability to keep the provided list in the model's effective context or to follow long sequences, but no direct analysis of different memory strategies is provided.",
            "recommendations_or_conclusions": "The paper recommends using Simon Says With Memory as a unit test to ensure an agent has minimal long-horizon instruction-following / recall capability before evaluating on the larger TALES suite.",
            "uuid": "e8238.0",
            "source_info": {
                "paper_title": "TALES : Text Adventure Learning Environment Suite",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "brief_description": "Prior work cited by the paper showing that repeated reflection plus an episodic memory buffer can improve LLM agent performance in text environments; presented in related work as an example of memory-enhanced LLM agents.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "agent_name": "Reflexion (as described in related work)",
            "agent_description": "An autonomous LLM-driven agent that augments online interaction with iterative self-reflection and an episodic memory buffer to store and reuse past observations/lessons.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "text-adventure / interactive tasks (prior work)",
            "benchmark_description": "Reflexion was applied to interactive tasks/text environments to demonstrate the utility of iterative self-reflection and an episodic memory buffer; cited here as evidence that memory can improve performance.",
            "memory_used": true,
            "memory_type": "episodic memory buffer (external, persistent across steps for storing past observations and reflections)",
            "memory_architecture": "Dynamic episodic buffer storing past action–observation pairs and reflection outputs; used to inform future prompts/actions (exact internal details are in the Reflexion paper, not reproduced here).",
            "memory_integration_strategy": "Memory entries and reflection summaries are fed back into subsequent LLM prompts to influence action generation (i.e., retrieval-augmented prompt augmentation via an episodic store).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as prior art demonstrating improvement from adding episodic memory; this paper does not run Reflexion or perform a direct ablation study itself.",
            "best_memory_strategy": "The referenced work suggests episodic buffers combined with iterative reflection improves performance, but the TALES paper does not evaluate which external memory strategy is best.",
            "limitations_or_failure_cases": "Not discussed in TALES beyond the citation; the TALES authors note that memory methods from prior work (like Reflexion) exist but do not reproduce or directly compare them in this benchmark study.",
            "recommendations_or_conclusions": "TALES highlights that episodic-memory approaches (e.g., Reflexion) have been promising in prior literature, and suggests that more systematic evaluation of memory strategies in benchmarks like TALES is needed.",
            "uuid": "e8238.1",
            "source_info": {
                "paper_title": "TALES : Text Adventure Learning Environment Suite",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1 (two-step thinking-trace then action pipeline)",
            "brief_description": "A model pipeline included in the experiments that first generates explicit thinking traces and then issues a second query to generate the next action using those traces, effectively using a transient scratchpad/working-memory step.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "DeepSeek-R1",
            "agent_description": "An agent variant used in the experiments that produces explicit thinking traces (internal chain-of-thought style outputs) and then issues a second LLM query using those traces to generate the final action.",
            "llm_model_name": "DeepSeek-R1 (architecture not fully specified in the paper's text excerpt)",
            "llm_model_description": "A pipeline-style agent (two-stage): first stage generates reasoning / thinking traces; second stage consumes those traces to produce actions. Detailed model internals are not provided in the TALES paper excerpt.",
            "benchmark_name": "TALES suite (various games incl. TEXTWORLD, ALFWORLD, SCIENCEWORLD, JERICHO)",
            "benchmark_description": "A diverse suite of text-adventure games used to evaluate agent reasoning across grounded, inductive, deductive, and spatial tasks.",
            "memory_used": true,
            "memory_type": "scratchpad / ephemeral working memory (generated thinking traces)",
            "memory_architecture": "Two-stage pipeline: (1) generate internal reasoning/thinking traces; (2) feed those traces into a follow-up prompt to produce the actionable command. The traces act as an ephemeral internal memory that is passed between LLM calls.",
            "memory_integration_strategy": "Explicitly feed generated thinking traces into a second LLM call (i.e., the traces are concatenated into the prompt for action generation).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "The paper notes the DeepSeek-R1 workflow in a footnote (generating traces then using a second query) but does not present a controlled ablation comparing with/without the trace step in the TALES experiments.",
            "best_memory_strategy": "Not identified; TALES does not conclude whether the two-stage thinking-trace pipeline is superior to other memory strategies within its experimental setup.",
            "limitations_or_failure_cases": "No explicit ablation provided here; the broader paper highlights that even when reasoning traces exist they can (if discarded or poorly integrated) contradict later steps or be inconsistently applied, limiting benefit.",
            "recommendations_or_conclusions": "TALES suggests that generating reasoning traces alone is insufficient unless those traces are meaningfully integrated into subsequent decisions; the paper notes they discarded reasoning traces for most agents' forward context, which reduced potential benefit.",
            "uuid": "e8238.2",
            "source_info": {
                "paper_title": "TALES : Text Adventure Learning Environment Suite",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Claude-3.7-Sonnet (reasoning agent)",
            "name_full": "Claude-3.7-Sonnet (reasoning-model backbone used for agentic play)",
            "brief_description": "The top-performing reasoning model in TALES experiments; it can produce stepwise reasoning traces but the paper reports that those traces were often discarded and not used as persistent memory during gameplay.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Reasoning agent (Claude-3.7-Sonnet backbone)",
            "agent_description": "A reasoning-agent configuration powered by Claude-3.7-Sonnet which at each step produces reasoning traces about the next best action; traces were generated but (in TALES' initial experiments) were thrown away rather than persistently added to the agent's context.",
            "llm_model_name": "Claude-3.7-Sonnet",
            "llm_model_description": "A high-performing reasoning-focused LLM (proprietary model family). Specific model size/architecture not provided in the paper excerpt.",
            "benchmark_name": "TALES suite (including ALFWORLD, SCIENCEWORLD, TEXTWORLDEXPRESS, JERICHO, Simon Says)",
            "benchmark_description": "A diverse set of text-adventure benchmarks stressing long-horizon, grounded, deductive, inductive, and spatial reasoning.",
            "memory_used": false,
            "memory_type": null,
            "memory_architecture": "The agent can (and did) generate per-step reasoning (thinking) traces, but the TALES evaluation deliberately discarded these traces at each step (i.e., they were not concatenated into future prompts), so there is no persistent memory architecture used in the experiments as-run.",
            "memory_integration_strategy": "None in the released experiments; reasoning traces were not integrated into subsequent prompts (they were thrown away), producing occasional contradictions across steps.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "TALES does not present an ablation comparing Claude-3.7-Sonnet with integrated persistent memory vs the discarded-traces setup; it notes this as a limitation and an area where memory could plausibly help.",
            "best_memory_strategy": "Not determined by TALES; the authors observe that although Claude-3.7-Sonnet sometimes displays all reasoning types, failures in grounded and inductive reasoning arise partly from not integrating prior reasoning/failure information into future steps.",
            "limitations_or_failure_cases": "The authors explicitly report that discarding reasoning traces leads to contradictions between steps and failures to properly incorporate past failure signals; this particularly degrades performance on long-horizon games (e.g., SCIENCEWORLD, JERICHO).",
            "recommendations_or_conclusions": "TALES recommends persistent, well-integrated memory strategies (e.g., retaining and retrieving past reasoning/failure observations) to mitigate grounded and inductive reasoning failures, but does not experimentally validate a single best approach in this paper.",
            "uuid": "e8238.3",
            "source_info": {
                "paper_title": "TALES : Text Adventure Learning Environment Suite",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Zero-shot baseline",
            "name_full": "Zero-shot single-query agent baseline (minimal prompt)",
            "brief_description": "The simplest agent used in TALES: a single-shot policy that maps the current textual observation (and environment feedback) to a single short action phrase, with no explicit memory beyond the LLM's prompt context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Zero-shot agent",
            "agent_description": "A minimal agent that receives the current observation + feedback as input and outputs a single short action phrase (temperature often set to 0); used as a baseline to evaluate inherent LLM capabilities without additional scaffolding or memory augmentation.",
            "llm_model_name": "various (same pool of LLMs evaluated in TALES: Claude, Gemini, GPT variants, Llama family, etc.)",
            "llm_model_description": "Diverse set of LLMs used in a zero-shot fashion; the TALES paper evaluates 34 models in total across frameworks.",
            "benchmark_name": "TALES suite (all included games)",
            "benchmark_description": "Unified benchmark of text-adventure games intended to probe multiple reasoning skills under minimal scaffolding (no added domain-specific knowledge or memory modules provided by the authors).",
            "memory_used": false,
            "memory_type": null,
            "memory_architecture": null,
            "memory_integration_strategy": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "The paper contrasts zero-shot baseline performance with the reasoning-agent variant (which can produce reasoning traces), but does not present a controlled study specifically comparing zero-shot with explicit memory-augmented versions within TALES.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Zero-shot agents routinely fail on long-horizon games and those requiring inductive or grounded reasoning because they lack mechanisms to persistently incorporate prior failures, observations, or plans beyond the immediate prompt context.",
            "recommendations_or_conclusions": "TALES suggests that zero-shot LLMs are insufficient for hard human-authored games and that some form of memory or iterative reflection is likely necessary for robust long-horizon performance.",
            "uuid": "e8238.4",
            "source_info": {
                "paper_title": "TALES : Text Adventure Learning Environment Suite",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
            "rating": 1,
            "sanitized_title": "swiftsage_a_generative_agent_with_fast_and_slow_thinking_for_complex_interactive_tasks"
        },
        {
            "paper_title": "Playing text-adventure games with repeated reflection and an episodic memory buffer",
            "rating": 1,
            "sanitized_title": "playing_textadventure_games_with_repeated_reflection_and_an_episodic_memory_buffer"
        }
    ],
    "cost": 0.018127499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TALES: Text Adventure Learning Environment Suite
22 Apr 2025</p>
<p>Christopher Zhang Cui 
University of California
San Diego</p>
<p>Xingdi Yuan 
Microsoft Research Montréal</p>
<p>Ziang Xiao 
Johns Hopkins University</p>
<p>Prithviraj Ammanabrolu 
University of California
San Diego</p>
<p>Marc-Alexandre Côté 
Microsoft Research Montréal</p>
<p>TALES: Text Adventure Learning Environment Suite
22 Apr 20257BAD003421936B44910A9AE9AAEAAF9DarXiv:2504.14128v2[cs.AI]
Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world.As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action.We introduce TALES, a diverse collection of synthetic and human-written textadventure games designed to challenge and evaluate diverse reasoning capabilities.We present results over a range of LLMs, open-and closed-weights, performing a qualitative analysis on the top performing models.Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment.Code and visualization of the experiments can be found at microsoft.github.io/tales.Preprint.Under review.</p>
<p>I should find some lead and a heat source.I might be able to find heat sources in the kitchen.</p>
<p>You are in the Bathroom.You see a door.Your task is to heat lead until it changes form into a liquid ...</p>
<blockquote>
<p>Go to Kitchen</p>
</blockquote>
<p>You can't do that right now.</p>
<p>Maybe my command failed because I need to open the door before I can exit.</p>
<blockquote>
<p>Open Door</p>
</blockquote>
<p>You opened the door ...</p>
<p>…</p>
<p>You see a pot.You see a bowl with some lead.</p>
<p>There is lead in my current observation, and my original task requires lead, so I should pick it up.</p>
<blockquote>
<p>Get Lead</p>
</blockquote>
<p>You failed to pick up the lead.Now I have the lead.I can return to the kitchen through the hallway.Figure 1: Example of a gameplay trajectory presenting the conversation between the game engine and an agent.We additionally fabricate the agent's reasoning to demonstrate the reasoning types this work concerns, detailed in Section 2. Here, the agent made a mistake in its inductive reasoning, which further caused the generation of a sub-optimal action.</p>
<blockquote>
<p>Go to Kitchen</p>
</blockquote>
<p>Introduction</p>
<p>Reasoning is crucial in sequential decision-making tasks where optimal actions depend on previous choices whose effects may only emerge later.In complex tasks, the agent often needs to leverage a variety of reasoning skills to make the best decision.It becomes more challenging with grounded environments, where the causal constraints between actions are fixed and cannot be violated.Therefore, the ability of a Large Language Model (LLM) to perform this structured thinking and follow these constraints across long contexts is critical for real-world application Trivedi et al. [2024].</p>
<p>We identify four core reasoning skills vital to a LLM-driven agent's ability to interface with applications in real-world settings where there is limited human intervention: Spatial reasoning, to efficiently navigate and understand the spatial relationship between objects Byrne and Johnson-Laird [1989]; Deductive reasoning, to act upon general principles Johnson-Laird [1999]; Inductive reasoning, to draw conclusions from interaction and observation Heit [2000]; and Grounded reasoning, to identify relevant information and perform admissible actions in a given context Endsley et al. [2000].These core reasoning skills inform our selection criteria of text-adventure games for our benchmark as an agent must display proficiency in all skills at a non-trivial level to progress.</p>
<p>Text-adventure games are examples of interactive, grounded environments with complex tasks that have long presented a grand challenge for agents due to their length, long-horizon causal dependencies, and puzzles that require a composition of multiple reasoning skills for progression Hausknecht et al. [2020], Osborne et al. [2022].Figure 1 illustrates an agent navigating through a text-adventure game.It showcases the composite reasoning steps involved with for optimal decision-making at each step, while highlighting how a single failure in any reasoning skill can dramatically reduce overall performance.</p>
<p>To evaluate an LLM-driven agent's comprehensive reasoning capabilities, we introduce TALES, the first benchmark that unifies JERICHO, ALFWORLD, SCIENCEWORLD, TEXTWORLD, and TEXTWORLDEXPRESS in their canonical forms.Unlike other benchmarks that focus on a specific text-adventure game framework, introduce an excessive amount of expert knowledge to explicitly provide the agent the otherwise implicit constraints, or reduce the game's scope to obtain results Paglieri et al. [2024], Chang et al. [2024], we apply minimal scaffolding.This creates a challenging and comprehensive evaluation suite for better understanding the agent's baseline composite reasoning skills without expert knowledge.We introduce TALES by following the ECBD framework Liu et al. [2024] that outlines the key benchmark design decisions.</p>
<p>As an initial litmus test of an agent's compositional reasoning skills, we introduce the game Simon Says to assess whether the agent has the baseline capabilities required to challenge TALES.In this classic children's game, players must follow instructions only when prefaced with "Simon says"making it fundamentally an instruction-following task.The simplest formulation of our text-adventure implementation gives the player a direct walkthrough of required actions, similar to the iconic copy task Graves et al. [2014] where models must reproduce given sequences.Despite this programmatic simplicity, we find that even advanced models struggle with this straightforward instruction-following challenge.We discover that success in this elementary task strongly predicts (Pearson r = 0.83) a model's ability to make meaningful progress in the more complex environments of TALES.</p>
<p>We show the performance of 34 models, open-and closed-weights, in a zero-shot setting on a suite of 122 games.From the nine models that were able to achieve a high score on the most difficult mode of Simon says, we inspect the game transcripts of the agents powered by them on a subset of games that are popular and relevant.We identify common good behaviors and failure modes from the most successful agents, comparing them against four reasoning models in terms of score and token usage with a preliminary investigation into the game transcripts of the top-performing reasoning model, Claude-3.7-Sonnet.Our initial results indicate that the Claude models demonstrate the best overall performance.However, all models struggle to reason across extremely long-horizon contexts where important information is sparsely scattered throughout.This limitation significantly hinders their ability to progress through the JERICHO framework Hausknecht et al. [2020], a collection games meant to be played by humans, slowly and iteratively over extended periods of time.We show that despite the impressive capabilities and likely data contamination of many state-of-the-art LLMs, no agent is capable of completing the gauntlet of games in TALES in a zero-shot setting with minimal inductive bias.</p>
<p>In summary, our contributions are as follows:</p>
<p>• We introduce TALES, a unified framework for evaluating agents in text-adventure game environments.• We outline the reasoning skills required for an agent to be able to successfully complete any text-adventure game in the benchmark.• We introduce the new Simon Says game mode where the agent must echo a command sequence.• We investigate the game transcripts of ALFWORLD, SCIENCEWORLD and the iconic ZORK1 to find where even agents driven by the top models fail to progress in games meant to be enjoyed and solved by human ingenuity.• We provide results from a zero-shot trial for 34 of the top LLMs as of the time of writing.</p>
<p>Reasoning</p>
<p>For complex tasks, achieving the desired result will typically demand a composite of reasoning skills.For example, an LLM agent for computer use needs to execute a series of actions or make a sequence of API calls.Those actions need to be grounded in the specific environment, but the environment affordance may not be clear.The agent needs to act by general principles, interact with the environment, and adjust based on specific observations.It often needs to navigate and spatially understand the interface layout to decide on the next steps.As the task becomes more challenging, the agents need to make a long sequence of decisions within a high-dimensional action space.The task can only be successful if the agent can leverage a composite of different reasoning skills.Lacking a specific reasoning skill would lead to task failure as the error will cascade and be difficult to recover.</p>
<p>From these insights, we identified four reasoning skills that are crucial for an LLM agent to succeed in complex, environment-grounded tasks.Those four reasoning skills comprise the capability module Liu et al. [2024] measured by TALES.</p>
<p>Spatial reasoning.The ability to navigate the environment effectively and understand the spatial relationship among game objects, including path finding, backtracking, and locating items Momennejad et al. [2023].</p>
<p>Deductive reasoning</p>
<p>The ability to derive valid actions through the logical application of general principles within a specific environmental context Johnson-Laird [1999].Deductive reasoning is particularly critical when environmental interactions are limited or when action has substantial costs and irreversible consequences.In such scenarios, the agent must leverage pre-existing knowledge to understand the affordance and constraints of the context and make correct actions towards the goal.</p>
<p>Inductive reasoning</p>
<p>The ability to draw conclusions through interactions and observations.This is a critical skill for agents that interface with complex, interactive systems.Given the diversity of tasks, the environment's affordance may be unknown or contradict with general principles (e.g., a software interface element behaving inconsistently across operating systems).The inductive reasoning skill allows the agent to derive environment-specific rules through exploration and act accordingly.This skill encompasses both adjustments to API calls to adhere to strict function signatures as well as learning from system feedback Zhong et al. [2024].</p>
<p>Grounded reasoning</p>
<p>The ability to make decisions based on relevant information and current context.This reasoning skill is analogous to situational awareness in humans.Although an LLM may be pre-trained on a vast amount of world knowledge, it has to attend to task-specific information when making its decisions.In addition, as agents often have access to the full interaction history at every step, the ability to correctly identify what information is relevant to the current state and reason over said information becomes more and more important as the length of the history grows.</p>
<p>The ability to leverage all of these skills is critical to the success of agents as the complexity of the task increases.Within longer contexts, these reasoning skills often become compositional with a failure in one skill leading to failures in the others later on.We believe text-adventure games are an ideal environment to simultaneously evaluate an agent on all four core reasoning skills at the same time.Figure 1 illustrates a simple task in a text-adventure game where the agent is required use composite reasoning over a number of steps.Here, we see why it is important that the agent be able to perform multiple reasoning skills consistently and compositionally: the agent not realizing it failed to pick up lead in one step leads to further failures later on when attempting to use the object the agent had previously failed to pick up.</p>
<p>TALES</p>
<p>All frameworks included in TALES are text-adventure game environments where the player is provided a textual observation, and sometimes an explicit goal, and are able to interact with the environment through short action phrases.If these action phrases are invalid, the parser will typically return some error message indicative of whether the action has been understood by the environment but is unable to be done, or if the parser just does not understand the action.Some environments use a nearest-neighbor parser which can understand similar action phrases to mean the same thing, e.g., take lamp, get lamp and pick up lamp.Most environments provide the player a score as a metric of how far they have progressed through the game, though some environments only provide a single score at the end of the game if the goal state is reached.Unless otherwise stated, all environments have failure conditions or actions that will cause the game to reset.We provide a short description of each environment and any notable characteristics about the environment or rewards in the following section.We organize the following sections in the rough order of their difficulty, with the recommendation that users should avoid testing on the full TALES without an agent that is able to succeed in the environments up to ALFWORLD.A list of all available games can be found in Appendix E.  1: Attributes of each framework with respect to one another and the top average agent score for each framework across subsets of LLMs.Claude-3.5-Sonnet† outperforms all other zero-shot models.Of the reasoning models, the best performing is a mix of Claude-3.7-Sonnet‡ , gemini-2.5pro-preview@<em> , and o1 # perform the best of the reasoning models.Full tables of scores across all frameworks and games are available in the appendix.</em>Gemini-2.5-pro-preview'sscores are currently only over one run.We will update these scores when all seed runs are complete.</p>
<p>Simon Says: you shall not pass unless you can solve this task</p>
<p>For all frameworks included in TALES, there is a requirement for the agent to be at least minimally proficient in all reasoning skills to make any non-trivial progress.With the release of TALES, we also introduce a new TEXTWORLDEXPRESS game in the form of "Simon Says".Simon Says is unique compared to other games in TALES as it requires only minimal proficiency in grounded reasoning to complete.The basic Simon Says simply provides the agent an action to repeat while Simon Says With Memory provides a list of actions to follow at the start of the game.Both versions award a point for every correct action.The game restarts if any action is performed out of order or is wrong.While outwardly trivial, we believe Simon Says With Memory serves as a good unit test for if an agent possesses sufficiently advanced reasoning to make meaningful progress through TALES.</p>
<p>A prerequisite to success in TALES is the ability to at least follow instructions over a long horizon task.Simon Says is the simplest form of this, posed in a question of whether or not the agent is able to solve the game when provided an expert demonstration.In TALES, we integrate the "CookingWorld" games that were used as part of the NeurIPS 2018 Competition1 .The task involves following a recipe that requires finding ingredients and processing them according to said recipe.We selected one game per difficulty ranging from level 1 (with one location and a recipe of 1 ingredient) to level 10 (having 12 locations and a recipe with 3 ingredients).For all difficulties, the player receive 1 point after completing sub-goals related to the task in the game.Difficulty level 1 can be solved in 7 moves with a max score of 3, while level 10 requires 44 moves with a max score of 11.</p>
<p>TEXTWORLDEXPRESS</p>
<p>TEXTWORLDEXPRESS Jansen and Côté [2022] is a highly optimized re-implementation of many TEXTWORLD game scenarios that runs approximately three orders of magnitudes faster compared to the TEXTWORLD counterparts.While token throughput is the major speed bottleneck in many LLMbased applications, we opt to use TEXTWORLDEXPRESS over TEXTWORLD for the performance improvement where applicable.</p>
<p>While significantly faster, an arguable drawback of using TEXTWORLDEXPRESS over TEXTWORLD is also in its stricter parser.TEXTWORLDEXPRESS simplifies its parser for speed and thus does not allow for nearest-neighbor action phrases.</p>
<p>ALFWORLD</p>
<p>ALFWORLD Shridhar et al. [2021] is a multi-modal framework combining complementary visual and textual observations, where agents are asked to navigate and perform tasks in a household setting.</p>
<p>All tasks provide only a terminal reward of 1 upon task completion.For TALES, we only use its textual modality as it has become the standard in the LLM literature when evaluated on ALFWORLD Yao et al. [2023], Shinn et al. [2023].</p>
<p>The ALFWORLD environments are unique in their lack of informative feedback.Where other environments have a predefined error message relating to the type of error, whether it is due to the parser not recognizing the command or the action not being possible, ALFWORLD has only one error message in the form of Nothing happens.In the original ALFWORLD framework, the visual component compensates for the insufficient text feedback.However, this lack of detailed information significantly increases the difficulty for agents that rely solely on text-based interactions.This difficulty is compounded upon by the limitation that an agent in ALFWORLD can only hold one object at a time.</p>
<p>SCIENCEWORLD</p>
<p>SCIENCEWORLD Wang et al. [2022] is a framework focused on the completion of elementary-level science curriculum tasks.Notably for many of its tasks, SCIENCEWORLD emulates an open-world setting where the player can complete the task in different ways that do not follow one expected trajectory.When it comes to heating objects, this part of the task can be completed by either the oven in the kitchen or the blast furnace in the workshop.Similarly, SCIENCEWORLD also allows the player the freedom to reset the game on command.This is especially important as a number of SCIENCEWORLD games have dead states where it is no longer possible to complete the assigned task in that play-through.</p>
<p>JERICHO</p>
<p>JERICHO Hausknecht et al. [2020] is a suite of 54 2 , human-written, interactive fiction games.We consider JERICHO to be the most difficult framework due to the length and complexity of many of the games.Some can be completed within 17 steps while some others require over 500 steps.These games also cover an extremely wide range of genres and styles and lack the consistency of many other text-game environment suites designed for evaluating agents.For example, 9:05 follows the morning of an ordinary office worker where ANCHORHEAD is a Lovecraftian Horror Story.</p>
<p>Evaluation</p>
<p>TALES allows for evaluation by adapting models with custom prompts and agentic strategies.For our initial release, we adapt examinee models by considering the following two baseline agents:</p>
<p>Zero-shot: a basic agent that uses the prompt shown below; Reasoning agent: an agent that uses a reasoning model as the backbone with the same minimal prompt.We do not provide other instructions to the LLMs on how to play the game to assess LLMs' capabilities when not directed by a human expert with domain knowledge.This differs from other benchmarks such as Chang et al. [2024], Paglieri et al. [2024], Lu et al. [2025] which introduce significant inductive bias by providing the agent important information about the environments it would need to otherwise discover on its own.When calling the LLMs, the observation and feedback are provided as the user inputs while the LLM actions are recorded as the assistant outputs.</p>
<p>For our results in the initial release of TALES, we cap the number of steps the agents can take in any environment to 100 due to compute and monetary limitations.Even though only 57% of the total score is achievable within 100 steps in JERICHO, no agent approaches this score (as we will discuss in Section 4.1).We believe 100 steps to be adequate to demonstrate the current scope of model reasoning capabilities.This number can easily be adjusted in TALES in the future.TALES captures the model's capability evidence by the score from each game environment, ranging from 0-100.Although each game environment has its own customized scoring rules, those rules mark significant milestones in solving the game.We include a breakdown of the percentage of the max score from following the game walkthrough to a certain number of steps in JERICHO in Appendix A.</p>
<p>For each game, we repeat the experiment 5 times due to the stochastic nature of LLMs, but find minimal changes in performance.When available, we set the temperature equal to 0 and run multiple seeds to account for randomness within the environment.Per Section 3.1, we only include models that could achieve at least 90% of the total score in the 100 step version of Simon Says with Memory.</p>
<p>Qualitative analysis and results</p>
<p>In this section, we provide the results from a qualitative analysis of the game transcripts of the zero-shot agents for the corresponding environments.We motivate the selected environments.We describe in detail the specific reasoning skills that we believe the best agents displayed (or fail to display) that contributed to the agent's success or failure.We include examples of specific failure cases that we believe are particularly insightful to the short-comings of the top LLM agents.</p>
<p>Following the zero-shot models, we also provide a short analysis of the strengths and weaknesses of the Claude-3.7-Sonnetreasoning model.For our preliminary work, we only analyze the game transcript and reasoning traces of Claude-3.7-Sonnetas it is the highest scoring reasoning model.</p>
<p>We find a composite of inductive reasoning and grounded reasoning typically acts as the barrier to entry for many text-adventure games as the agents powered by the weaker models are unable to have their action phrases understood fail to account for the reasons why the action failed, and are thus unable to progress through the games at all.Failures in grounded reasoning are more ambiguous to classify versus the other types of reasoning as it is a reasoning mode that is typically done compounded with or alongside another reasoning skill.For example, a failure to navigate from one location to another could be a spatial reasoning failure in planning out an incorrect route, or it could be a grounding reason failure in attempting to traverse from one location to another, not-connected location.A failure to understand that an object was not picked up correctly could be an inductive reasoning failure in not understanding a certain dynamic in the environment from past actions or a grounded reasoning failure in not understanding the failure message from the feedback.</p>
<p>For each of the following game and environments, we describe which skills are primarily required for progression as well as summarize when and where the state of the art fails.</p>
<p>ALFWORLD</p>
<p>On top of grounded reasoning, ALFWORLD requires deductive and inductive reasoning with a small amount of spatial reasoning.</p>
<p>LLMs often fail to discover implicit game dynamics through inductive reasoning.We find even the top agents are unable to infer environment rules that are not explicitly provided.When the agent is holding an object and tries to pick up another object, rather than an informative error message that explains that the player can only hold one object at a time, the ALFWORLD environments simply reply with nothing happened.ALFWORLD returns this feedback if the action fails for any reason, including the case when the action fails to be parsed, and when the action is not possible in the current state.The most common failure mode resulting from this is when an agent picks up an object tangentially related to the task but fails to realize it must put the original object down before picking up another one, often the actual object needed for the task.</p>
<p>Distractor objects significantly hinder LLMs with weaker deductive reasoning.The most successful agents are able to deductively reason about which receptacle to visit first and what objects would actually be needed for the game objective, while weaker agents waste too many steps interacting with distractors and fail to complete the game.In any ALFWORLD game, there are a large enough number of distractor receptacles and objects that if the agent attempts to interact with every distractor, it will quickly hit the maximum number of steps.Because of this, the ability to reason about the most relevant game elements to perform a focused exploration of the environment is crucial for success.</p>
<p>Spatial reasoning is critical even for fully connected game graphs.We find the majority of agents are able to perform this most basic level of spatial reasoning, however a lack thereof results in a severe decrease in performance for some sufficiently advanced agents.ALFWORLD uses receptacles instead of rooms to prevent the agent from accessing all objects from the beginning of the game.</p>
<p>The agent must move to and sometimes open or interact with the receptacle to find the object or complete the task.However, there is no requirement for path-finding or navigating a sequence of receptacles to arrive at a destination.While some agents fail to realize they need to move to the receptacle before being able to interact with it, most agents are able to implicitly reason through this limitation upon invoking the help command.Spatial reasoning is less important in ALFWORLD due to how 'locations' are accessed.</p>
<p>SCIENCEWORLD</p>
<p>Compared to the other synthetic environments, SCIENCEWORLD has more strict reasoning requirements due to the complexity of the environment.</p>
<p>Increased environmental complexity severely degrades agent reasoning capabilities beyond what performance metrics alone indicate.We find the increase in environment complexity from ALFWORLD to SCIENCEWORLD to result in a significant drop in reasoning ability across all models.This is not obvious from the scores alone.SCIENCEWORLD is far more expansive than ALFWORLD with a far larger number of distractors, more complex game mechanics, and sparsely connected locations.For example, SCIENCEWORLD provides 26 possible commands templates when the help command is called where ALFWORLD only has 13.ALFWORLD has an average gold trajectory length of 5.83 where SCIENCEWORLD has an average gold trajectory length of 41.67.These factors lead to a higher requirement for reasoning skills in addition to the larger context length.</p>
<p>LLM agents struggle with processing feedback in longer contexts.The most common inductive reasoning failure mode we found in SCIENCEWORLD were agents not accounting for the feedback from their action.We see this across all agents in SCIENCEWORLD as a result of the previously discussed degradation in reasoning across the board.For example, an agent failing to pick up an object due to a syntactically incorrect action phrase but then leaving the location without any additional attempts.In weaker models, this decrease in inductive reasoning caused the agent to get caught in a loop of repetitive actions, this loop in some cases even leading to the agent repeatedly restarted the game.</p>
<p>ZORK1</p>
<p>ZORK1 still proves an insurmountable challenge even for modern state-of-the-art LLMs.Due to numerous references to zork appearing in Claude-3.7-Sonnet'sthinking traces, we very strongly suspect that Claude-3.5-Sonnetand other LLMs were trained on play-through transcripts of ZORK1.Yet despite the data leakage, no agent is capable of replicating the human-style exploration and longhorizon reasoning that leads to success in most human-written text adventure games.In comparison to Simon Says with Memory, where the walkthrough is directly provided, this allows us to investigate the capabilities of an agent when the walkthrough is instead implicitly provided through parametric knowledge.</p>
<p>Current State-of-the-Art fails long before 100 steps.While ZORK1 requires well over 100 steps to complete, we cap the environment's number of steps in the environment to 100 due to the prohibitive costs of running all models to a termination signal from the environment.However, even with the data leakage, the top agents fail to achieve over 50% of the available score (.291).The web of explorable locations, even more sparsely connected than SCIENCEWORLD with a far larger number of locations, lends itself well to evaluating an agent's spatial reasoning capabilities.In addition to the large number of objects and puzzles, the length of the game also allows us to measure the ability of the LLM to perform grounded reasoning by seeing the degree to which performance degrades with game length, with relevant information getting lost in the ever increasing context.</p>
<p>Parametric knowledge does not make up for a lack of reasoning.Despite having very likely have been trained on playthroughs of the game, even the top agents fail to leverage their parametric knowledge to be able to successfully progress through the game in later steps.Many weaker agents also suffered from different failures in grounded reasoning, specially in later steps.Grounded reasoning failures include hallucinating actions that otherwise make no sense in the context of the game and attempting to interact with objects seen but not in the current location.Similar to SCIENCEWORLD, several agents suffered a severe failure in inductive reasoning where they repeatedly attempted actions that failed, or repeatedly cycled between a set of locations.Surprisingly, despite the suspected data leakage, many stronger agents fell into this pattern of behavior as well.Some agents fail to understanding that a light source is needed for the famous grue puzzle and try for a significant amount of time to open a door to a location when an alternate route has already been found, but are able to understand signals that an object is not meant to be used at the moment.</p>
<p>Reasoning models</p>
<p>What type of reasoning is used is just as important as when reasoning is used.For our preliminary work with reasoning models, we analyze the game transcripts and thinking traces of Claude-3.7-Sonnet.At each step, the agent reasons about the next best action for it to take.For our initial work, we do not use the extended thinking, instead throwing away the thinking traces at each step.As a result, we find that reasoning traces can sometimes contradict the reasoning traces of subsequent steps.More critically, while the reasoning traces sometimes do guide the agent better, we find a significant limitation to be the failure of the reasoning model to perform the correct type of reasoning in certain circumstances.While Claude-3.7-Sonnetdisplays grounded, deductive, inductive, and spatial reasoning at various points throughout the games, its primary failure modes come as a result of a failure in grounded and inductive reasoning.These manifest primarily as the inability to abstract failure modes from repeated attempts as well as basing current action and reasoning in outdated or incorrect game state facts, especially in later steps.We see this in the decrease in performance for SCIENCEWORLD with the longer task horizon, but slight increase in performance for ZORK1 and ALFWORLD.For ZORK1, references to the series appear multiple times, the reasoning traces likely allowing the agent to better attend to the game play-throughs in its parametric knowledge.There is a similar slight increase in performance for ALFWORLD due to its shorter length allowing for a decreased load on grounded and inductive reasoning.Conversely, the significantly longer games of SCIENCEWORLD result in the reasoning decreasing in performance due the agent failing to respond to critical feedback and failing to iterate over previous attempts.</p>
<p>Related Work: Text Game Agents</p>
<p>A large body of work exists in teaching agents to navigate and successfully complete text world games.We specifically divide this section into RL-Based agents, where the text-world is defined as a Partially Observable Markov Decision Process (POMDP) Kaelbling et al. [1998] and LLM-based agents where the observation and other information is fed to the LLM as an input with the output taken as an action.</p>
<p>RL-based</p>
<p>Prior work has explored text world games as benchmark for non-LLM-based agents Narasimhan et al. [2015], Hausknecht et al. [2020].Due to the intractable action space of language, prior RL approaches leveraged action templates to reduce the space of all possible commands down to a subset that could be learned by an RL agent  Ammanabrolu and Riedl [2018] uses the knowledge graph to further filter the actions to only the ones relevant to the current state.Ammanabrolu and Hausknecht [2020] further improves this with a graph attention layer.Peng et al. [2023] leverages a knowledge graph to allow a text-adventure role-playing agent to intrinsically reward itself for following a persona.Cui et al. [2023] extend Peng et al. [2023] to allow for a single agent to learn multiple personas with learnable soft prompts while Cui et al.</p>
<p>[2024] uses these soft prompts to allow the same agent to rapidly learn a new persona.Yao et al.</p>
<p>[2020] combines a language model trained on human play-throughs with an RL agent to generate and then rank the generated actions.Murugesan et al. [2021] generates the graph from commonsense knowledge using an external model.Basavatia et al. [2024] uses a LLM to automatically generate text game environments for an RL to evaluate the agent's generalization to other environments.Golchha et al. [2024] uses a LLM to provide decision-level guidance to an RL agent.</p>
<p>LLM-based</p>
<p>Work around LLMs and text games span both LLM agents acting as players for text games and as the environment, however we focus primarily on the former in this work.Early results demonstrated that even state-of-the-art pre-trained LLMs face difficulty when playing text-adventure games meant for human players Tsai et al. [2023].Shinn et al. [2023] demonstrate how a LLM agent's performance can be improved through the use of repeated reflection and an episodic memory buffer.Lin et al. [2023] leverages a LLM to perform sub-goal planning and grounding for a dual slow-thinking, fast-thinking agent.Prasad et al. [2024] plans and decomposes sub-task into simpler forms when an LLM agent is unable to progress to adapt to task complexity and model capability.Wang et al. [2024] has the LLM generate multiple actions and then uses a model-likelihood based scoring to select the best action.Zhao et al. [2024] introduces LearnAct, which creates and improves actions in the form of python functions.Zhu et al. [2024] uses an action knowledge base and a self-learning strategy to constrain the action path during planning.Song et al. [2024] uses DPORafailov et al. [2023] to train an agent with contrastive optimal and non-optimal exploration trajectories.Yang et al. [2024] uses action reasoning explanations to generate novel trajectories that are then used in contrastive self-learning.Qiao et al. [2025] leverages a world-model based approach, using expert and sampled trajectories to do both global and local planning.</p>
<p>Conclusion</p>
<p>In this work, we introduced TALES, a unified benchmark for LLM agents in text-adventure game environments.We identified a set of reasoning skills essential to agents operating through APIs to interface with outside environments.</p>
<p>We used Simon Says to evaluate whether an agent is capable of the most basic composite reasoning needed to succeed in TALES.The game transcripts from leading LLMs reveal that, despite their impressive language capabilities, these models still struggle with core reasoning challenges inherent to text-adventure games.The difficulty stems not only from long-horizon dependencies and implicit environmental cues but also from the need for sequential, exploratory, and commonsense reasoning-skills that remain a bottleneck for even state-of-the-art LLMs.</p>
<p>Overall, while progress has been made on synthetic text-adventure games, LLM-driven agents are still far from being able to complete games meant to be played for simple, human enjoyment.</p>
<p>A Jericho Walkthrough Scores</p>
<p>Table 2 shows the percent of achievable score when using the walkthrough for all JERICHO for 50, 100, 200, 300, 400, 500 and 1000 steps.</p>
<p>B All Agent Average Scores per Framework</p>
<p>In Table 3, we include the average scores per framework and average-per-game score of all zero-shot and reasoning LLM agents.</p>
<p>C All Agent Average Final Tokens Used per Framework</p>
<p>In Table 4, we include the average final tokens used per game for each framework of all zero-shot and reasoning LLM agents.</p>
<p>D Agent Score Standard Deviations</p>
<p>In Table 5, we include the average standard deviation across seeds per framework of all zero-shot and reasoning LLM agents.</p>
<p>E All Games</p>
<p>In Table 6 we list all tasks and games in their respective frameworks.</p>
<p>F All Scores per Game: TEXTWORLD</p>
<p>G All Scores per Game: TEXTWORLDEXPRESS</p>
<p>Table 8 shows the per-game scores of all models in TEXTWORLDEXPRESS.* Indicates LLM has only been run on one seed.We will update the paper once all run seeds have been completed.</p>
<p>H All Scores per Game: ALFWORLD</p>
<p>Table 9 shows the per-game scores of all models in ALFWORLD.* Indicates LLM has only been run on one seed.We will update the paper once all run seeds have been completed.</p>
<p>I All Scores per Game: SCIENCEWORLD</p>
<p>Table 10 shows the per-task scores of all models in SCIENCEWORLD.* Indicates LLM has only been run on one seed.We will update the paper once all run seeds have been completed.</p>
<p>J All Scores per Game: SCIENCEWORLD</p>
<p>Tables 11 and 12 shows the per-game scores of all models in JERICHO.* Indicates LLM has only been run on one seed.We will update the paper once all run seeds have been completed.3. * Indicates LLM has only been run on one seed.We will update the paper once all run seeds have been completed.</p>
<p>Figure 2 :
2
Figure 2: Max normalized score per step for the hardest mode of Simon Says, ALFWORLD, SCIENCEWORLD, and ZORK1 for top LLMs.Error bars represent the standard deviation of scores across 5 different seeds.We see that outside of one notable exception (o1), all selected LLMs achieve near the maximum score for the most difficult version of the Simon Says game.However, despite increasing performance in synthetic, training environments, LLMs still struggle immensely with the human-written ZORK1.</p>
<p>Table 5 :
5
Standard deviation statistics for different LLMs Ordering is based on the agent's cumulative average score shown in Table</p>
<p>Sa ys W ith M em or y1 0 Si m on Sa ys W ith M em or y1 00 Si m on Sa ys W ith M em or y1 00 Ve rb os e Si m on Sa ys W ith M em or y1 0V er bo se Si m on Sa ys W ith M em or y5 0 Si m on Sa ys W ith M em or y5 0V er bo se So rti ng Te xt W or ld Co m m on se ns tO bj In L ig ht Se en L oo kA tO bj In L ig ht U ns ee n Pi ck A nd Pl ac eS im pl eS ee n Pi ck A nd Pl ac eS im pl eU ns ee n Pi ck C le an T he nP la ce In R ec ep Se en Pi ck C le an T he nP la ce In R ec ep U ns ee n Pi ck C oo lT he nP la ce In R ec ep Se en Pi ck C oo lT he nP la ce In R ec ep U ns ee n Pi ck H ea tT he nP la ce In R ec ep Se en Pi ck H ea tT he nP la ce In R ec ep U ns ee n Pi ck Tw oO bj A nd Pl ac eS ee n Pi ck Tw oO bj A nd Pl ac eU ns ee ane Det erm ine An gle Inc line dPl ane Fric tion Nam edS urfa ces Inc line dPl ane Fric tion Un nam edS urfa ces Lif esp anL ong estL ived Lif esp anL ong estL ived The nSh orte stL ived Lif esp anS hor test Liv ed Me asu reM elti ngP oin tKn own Sub stan ce Me asu reM elti ngP oin tUn kno wn Sub stan ce Me lt Me nde lian Gen etic sKn own Pla nt Me nde lian Gen etic sUn kno wn Pla nt Pow erC om pon ent Pow erC om pon ent Ren ewa ble VsN onr ene wab leE ner gy Tes tCo ndu ctiv ity Tes tCo ndu ctiv ityO fUn kno wn Sub stan</p>
<p>3
System PromptYou are playing a text-based game and your goal is to finish it with the highest score. Upon readingthe text observation, provide a <em>single</em> short phrase to interact with the game, e.g. 'get lamp'(without the backticks). When stuck, try using the 'help' command to see what commands areavailable.</p>
<p>Table 2 :
2
Max score percentage reached by following the provided walkthrough for each JERICHO game.
Game50 Steps 100 Steps 200 Steps 300 Steps 400 Steps 500 Steps 1000 StepsJerichoEnv905100.000100.000100.000100.000100.000100.000100.000JerichoEnvAcorncourt100.000100.000100.000100.000100.000100.000100.000JerichoEnvAdvent26.30042.60063.100100.000100.000100.000100.000JerichoEnvAdventureland21.00042.000100.000100.000100.000100.000100.000JerichoEnvAfflicted46.700100.000100.000100.000100.000100.000100.000JerichoEnvAnchor5.00011.00029.00041.00052.00064.00099.000JerichoEnvAwaken60.000100.000100.000100.000100.000100.000100.000JerichoEnvBalances58.80058.80098.00098.00098.00098.00098.000JerichoEnvBallyhoo15.00030.00050.00075.00095.000100.000100.000JerichoEnvCurses3.8005.60012.70028.20038.20047.50081.800JerichoEnvCutthroat12.00028.00036.00044.000100.000100.000100.000JerichoEnvDeephome20.70028.00060.00076.000100.000100.000100.000JerichoEnvDetective100.000100.000100.000100.000100.000100.000100.000JerichoEnvDragon24.000100.000100.000100.000100.000100.000100.000JerichoEnvEnchanter11.30031.20070.000100.000100.000100.000100.000JerichoEnvEnter35.000100.000100.000100.000100.000100.000100.000JerichoEnvGold12.00030.00051.00075.000100.000100.000100.000JerichoEnvHhgg8.30021.20040.00050.000100.000100.000100.000JerichoEnvHuntdark0.000100.000100.000100.000100.000100.000100.000JerichoEnvInfidel12.50020.00070.000100.000100.000100.000100.000JerichoEnvInhumane33.30077.800100.000100.000100.000100.000100.000JerichoEnvJewel15.60026.70077.800100.000100.000100.000100.000JerichoEnvKarn5.90023.50038.20067.600100.000100.000100.000JerichoEnvLibrary100.000100.000100.000100.000100.000100.000100.000JerichoEnvLoose100.000100.000100.000100.000100.000100.000100.000JerichoEnvLostpig28.60042.90085.70085.70085.70085.70085.700JerichoEnvLudicorp13.30025.30058.70092.700100.000100.000100.000JerichoEnvLurking10.00025.00055.000100.000100.000100.000100.000JerichoEnvMoonlit0.000100.000100.000100.000100.000100.000100.000JerichoEnvMurdac6.80018.00018.00048.00099.60099.60099.600JerichoEnvNight60.000100.000100.000100.000100.000100.000100.000JerichoEnvOmniquest40.000100.000100.000100.000100.000100.000100.000JerichoEnvPartyfoul0.000100.000100.000100.000100.000100.000100.000JerichoEnvPentari100.000100.000100.000100.000100.000100.000100.000JerichoEnvPlanetfall7.50026.30035.00060.000100.000100.000100.000JerichoEnvPlundered16.00044.000100.000100.000100.000100.000100.000JerichoEnvReverb60.000100.000100.000100.000100.000100.000100.000JerichoEnvSeastalker28.00044.00090.000100.000100.000100.000100.000JerichoEnvSherlock23.00037.00055.00084.000100.000100.000100.000JerichoEnvSnacktime100.000100.000100.000100.000100.000100.000100.000JerichoEnvSorcerer23.70037.50053.700100.000100.000100.000100.000JerichoEnvSpellbrkr13.30026.70042.50065.00091.700100.000100.000JerichoEnvSpirit2.4003.2009.60014.40018.80027.20071.200JerichoEnvTemple28.60057.100100.000100.000100.000100.000100.000JerichoEnvTrinity15.00022.00032.00047.00058.00078.000100.000JerichoEnvTryst2052.90014.30024.30041.40058.60074.300100.000JerichoEnvWeapon0.000100.000100.000100.000100.000100.000100.000JerichoEnvWishbringer24.00050.000100.000100.000100.000100.000100.000JerichoEnvYomomma25.70097.10097.10097.10097.10097.10097.100JerichoEnvZenon40.000100.000100.000100.000100.000100.000100.000JerichoEnvZork118.00029.10041.70077.400100.000100.000100.000JerichoEnvZork26.20022.50047.500100.000100.000100.000100.000JerichoEnvZork328.60042.900100.000100.000100.000100.000100.000JerichoEnvZtuu47.000100.000100.000100.000100.000100.000100.000</p>
<p>Table 7
7ModelTEXTWORLD TEXTWORLDEXPRESS ALFWORLD SCIENCEWORLD JERICHO Average Scoreclaude-3.7-sonnet97.391.383.376.512.552.5claude-3.5-sonnet-latest95.581.675.082.39.650.4gemini-2.5-pro-preview<em>100.091.875.064.213.449.3o197.870.228.380.110.344.2gpt-4o83.680.656.761.45.640.6claude-3.5-haiku94.979.826.767.35.039.6Llama-3.1-405B-Instruct90.979.231.751.86.136.4gemini-2.0-flash</em>70.674.520.057.65.134.0Llama-3.3-70B-Instruct69.677.215.055.14.532.8Llama-3.1-70B-Instruct65.681.98.351.95.332.0Qwen2.5-72B-Instruct76.583.836.735.02.930.7Mistral-Large-Instruct-240782.468.36.746.15.830.3o3-mini83.261.111.748.44.529.9gpt-4o-mini56.573.60.027.21.821.8Llama-4-Scout-17B-16E-Instruct41.168.40.027.01.819.8Llama-4-Maverick-17B-128E-Instruct43.556.18.311.52.015.5Mistral-Small-Instruct-240956.127.30.024.41.414.8Llama-3.1-8B-Instruct29.750.30.015.72.313.9DeepSeek-R137.138.60.015.81.012.4Qwen2.5-7B-Instruct27.745.60.012.60.711.7Llama-3.2-3B-Instruct21.442.00.010.01.510.4phi-420.843.80.08.91.610.3Mistral-Small-24B-Instruct-250115.823.00.015.81.48.8DeepSeek-R1-Distill-Llama-70B8.739.80.07.71.38.4Ministral-8B-Instruct-241010.922.80.02.30.44.6Mistral-Small-3.1-24B-Instruct-25032.510.30.010.50.84.5Mixtral-8x22B-Instruct-v0.117.18.40.04.00.43.7Llama-3.2-1B-Instruct0.019.00.02.40.63.3Phi-3-mini-128k-instruct2.79.40.02.40.32.2Phi-3.5-MoE-instruct0.07.00.02.30.41.7Phi-4-mini-instruct0.05.50.02.30.51.5Mixtral-8x7B-Instruct-v0.10.01.60.04.00.31.3Phi-3.5-mini-instruct0.02.00.02.40.51.0Phi-3-medium-128k-instruct0.00.00.02.30.30.7
shows the per-game scores of all models in TEXTWORLD.* Indicates LLM has only been run on one seed.We will update the paper once all run seeds have been completed.</p>
<p>Table 3 :
3
Average scores per framework and average-per-game score.* Indicates LLM has only been run on one seed.We will update the paper once all run seeds have been completed.</p>
<p>Table 4 :
4
Avg final tokens used per LLM per game for each framework.Ordering is based on the agent's cumulative average score shown in Table3.<em> Indicates LLM has only been run on one seed.We will update the paper once all run seeds have been completed.
ModelTEXTWORLD TEXTWORLDEXPRESS ALFWORLD SCIENCEWORLDJERICHOclaude-3.7-sonnet6913888.06307282.57451626.712875273.331168417.0claude-3.5-sonnet-latest6076610.06881268.87876540.010674924.729195027.8gemini-2.5-pro-preview</em>2849760.07967853.86119425.010805876.721030931.5o14776556.011347413.412746025.07430012.021195865.2gpt-4o10686300.04953622.57794605.010712192.020971276.3claude-3.5-haiku11983992.08513610.026764358.320575098.726945885.2Llama-3.1-405B-Instruct6647618.05262465.010628998.313765721.322607862.6gemini-2.0-flash*12861246.77382296.213804826.714997250.016164304.4Llama-3.3-70B-Instruct16637378.07016502.512734873.312886091.320536281.1Llama-3.1-70B-Instruct13325344.05188593.810692573.314461564.721091478.9Qwen2.5-72B-Instruct11265802.05209607.59721118.316805726.719762833.0Mistral-Large-Instruct-240710778852.011022856.211839545.016323204.024325603.0o3-mini11452187.217122330.210356825.010936913.816179790.8gpt-4o-mini15984086.05757615.014523630.017287566.018262067.4Llama-4-Scout-17B-16E-Instruct28970984.012017387.517263393.322246407.322994717.0Llama-4-Maverick-17B-128E-Instruct28754724.021313950.035418391.739487552.037290262.6Mistral-Small-Instruct-240916333494.030451088.810754933.315073068.720826178.5Llama-3.1-8B-Instruct22223970.035883742.59658246.715229302.716550578.1DeepSeek-R139365446.039832268.849632843.343199789.343939930.4Qwen2.5-7B-Instruct14312710.021492626.29133443.316302117.317110767.4Llama-3.2-3B-Instruct23095046.07987831.28462040.019539724.715254468.1phi-418903160.010036393.812606808.315339520.717871344.4Mistral-Small-24B-Instruct-250139909376.050048483.847912501.741828493.347564970.4DeepSeek-R1-Distill-Llama-70B45369568.063738406.271940425.048281926.040740180.7Ministral-8B-Instruct-241022015794.033744751.211271048.310891640.011810491.1Mistral-Small-3.1-24B-Instruct-250344876402.050798643.847750583.339705458.751473350.0Mixtral-8x22B-Instruct-v0.115878216.013758351.39283275.013482764.015651581.1Llama-3.2-1B-Instruct56769178.027921476.245785731.713828560.020164850.7Phi-3-mini-128k-instruct24521498.042999343.825785216.725398945.323788146.7Phi-3.5-MoE-instruct27484888.029519093.824000750.025205561.327168003.0Phi-4-mini-instruct23194728.019929912.519540741.719088743.321250892.6Mixtral-8x7B-Instruct-v0.161279188.055528130.052043461.756099464.756496763.3Phi-3.5-mini-instruct42612546.047621835.041045965.032758494.045743439.6Phi-3-medium-128k-instruct62023544.058592546.258172158.351378752.059533548.1</p>
<p>Table 6 :
6
Games Organized by Framework Jericho</p>
<p>Table 7 :
7
Model Performance on TEXTWORLD Tasks.</p>
<p>https://competitions.codalab.org/competitions/21557
We exclude HOLLYWOOD.Z3 because of segfault errors and THREATRE.Z5 due to game engine errors.
For DeepSeek-R1, we first generate the thinking traces then use a second query to generate an action.</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew J Hausknecht, ArXiv, abs/2001.088372020210911499</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark O Riedl, ArXiv, abs/1812.01628201854458698</p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. Prithviraj Ammanabrolu, Ethan Tien, Matthew J Hausknecht, Mark O Riedl, ArXiv, abs/2006.074092020</p>
<p>Starling: Self-supervised training of text-based reinforcement learning agent with large language models. Shreyas Basavatia, Keerthiram Murugesan, Shivam Ratnakar, ArXiv, abs/2406.058722024</p>
<p>. M J Ruth, Philip N Johnson-Laird Byrne, Spatial reasoning. Journal of memory and language. 2851989</p>
<p>Agentboard: An analytical evaluation board of multi-turn llm agents. Ma Chang, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He, Advances in Neural Information Processing Systems. Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler, Curran Associates, Inc2024. 201837Paper-Datasets_and_Benchmarks_Track.pdf. Marc-Alexandre CôtéTextworld: A learning environment for text-based games. CoRR, abs/1806.11532</p>
<p>Thespian: Multi-character text role-playing game agents. Christopher Cui, Xiangyu Peng, Mark Riedl, 2023</p>
<p>Xiangyu Christopher Z Cui, Mark O Peng, Riedl, arXiv:2405.06059A mixture-of-experts approach to few-shot task transfer in open-ended text worlds. 2024arXiv preprint</p>
<p>Theoretical underpinnings of situation awareness: A critical review. Situation awareness analysis and measurement. Daniel J Mica R Endsley, Garland, 20001</p>
<p>Language guided exploration for rl agents in text environments. Hitesh Golchha, Sahil Yerawar, Dhruvesh Patel, Soham Dan, Keerthiram Murugesan, 2024</p>
<p>Neural turing machines. Alex Graves, Greg Wayne, Ivo Danihelka, 2014</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Nail: A general interactive fiction agent. Matthew J Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, J Williams, ArXiv, abs/1902.04259201960441391</p>
<p>Properties of inductive reasoning. Evan Heit, Psychonomic bulletin &amp; review. 72000</p>
<p>Peter A Jansen, Marc-Alexandre Côté, Textworldexpress: Simulating text games at one million steps per second. arXiv. 2022</p>
<p>. Johnson-Laird Philip, Deductive reasoning. Annual review of psychology. 5011999</p>
<p>Planning and acting in partially observable stochastic domains. Leslie Pack, Kaelbling Michael L Littman, Anthony R Cassandra, Artificial intelligence. 1011-21998</p>
<p>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. Yicheng Bill Yuchen Lin, Karina Fu, Prithviraj Yang, Faeze Ammanabrolu, Shiyu Brahman, Chandra Huang, Yejin Bhagavatula, Xiang Choi, Ren, ArXiv, abs/2305.173902023258960143</p>
<p>Ecbd: Evidence-centered benchmark design for nlp. Yu Liu, Su Blodgett, Jackie Chi, Kit Cheung, Vera Liao, Alexandra Olteanu, Ziang Xiao, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Intelligent go-explore: Standing on the shoulders of giant foundation models. Cong Lu, Shengran Hu, Jeff Clune, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Evaluating cognitive maps and planning in large language models with cogeval. Ida Momennejad, Hosein Hasanbeig, Felipe Vieira, Hiteshi Sharma, Robert Osazuwa Ness, Nebojsa Jojic, Hamid Palangi, Jonathan Larson, 2023</p>
<p>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, Annual Meeting of the Association for Computational Linguistics. 2021</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>A survey of text games for reinforcement learning informed by natural language. Philip Osborne, Heido Nõmm, André Freitas, 10.1162/tacl_a_00495Transactions of the Association for Computational Linguistics. 102022</p>
<p>Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kuciński, Lerrel Pinto, Rob Fergus, Jakob Nicolaus Foerster, Jack Parker-Holder, Tim Rocktäschel, Balrog: Benchmarking agentic llm and vlm reasoning on games. 2024</p>
<p>Story shaping: Teaching agents human-like behavior with stories. Xiangyu Peng, Christopher Cui, Wei Zhou, Renee Jia, Mark Riedl, Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment. the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment202319</p>
<p>Adapt: As-needed decomposition and planning with language models. Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, Tushar Khot, 2024</p>
<p>Agent planning with world knowledge model. Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, Advances in Neural Information Processing Systems. 202537</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362023</p>
<p>A minimal approach for natural language action space in text-based games. Kelvin Dongwon, Meng Ryu, Shirui Fang, Gholamreza Pan, Ehsan Haffari, Shareghi, ArXiv, abs/2305.040822023</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, ArXiv, abs/2303.113662023257636839</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Trial and error: Exploration-based trajectory optimization of llm agents. Yifan Song, Xiang Da Yin, Jie Yue, Sujian Huang, Bill Li, Lin Yuchen, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Appworld: A controllable world of apps and people for benchmarking interactive coding agents. H Trivedi, Tushar Khot, Mareike Hartmann, Ruskin , Raj Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, Niranjan Balasubramanian, ArXiv, abs/2407.189012024271516633</p>
<p>Can large language models play text games well? current state-of-the-art and open questions. Chen Feng Tsai, Xiaochen Zhou, Sierra S Liu, Jing Li, Mo Yu, Hongyuan Mei, ArXiv, abs/2304.028682023</p>
<p>Soft self-consistency improves language model agents. Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsShort Papers22024</p>
<p>Scienceworld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, 2022</p>
<p>React meets actre: When language agents enjoy training data autonomy. Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu, ArXiv, abs/2403.145892024</p>
<p>Keep calm and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew J Hausknecht, Karthik Narasimhan, ArXiv, abs/2010.029032020</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>Counting to explore and generalize in text-based games. Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Rémi Tachet Des Combes, Matthew J Hausknecht, Adam Trischler, ArXiv, abs/1806.11525201849547885</p>
<p>Empowering large language model agents through action learning. Haiteng Zhao, Chang Ma, Guoyin Wang, Jing Su, Lingpeng Kong, Jingjing Xu, Zhi-Hong Deng, Hongxia Yang, 2024CoRR</p>
<p>Policy improvement using language feedback models. Victor Zhong, Dipendra Misra, Xingdi Yuan, Marc-Alexandre Côté, 2024</p>
<p>Knowagent: Knowledge-augmented planning for llm-based agents. Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen, 2024CoRR</p>
<p>. PickAndPlaceSimpleUnseen. 10PickHeatThenPlaceInRecepUnseen</p>
<p>. PickCleanThenPlaceInRecepSeen. 11PickTwoObjAndPlaceSeen</p>
<p>. PickCleanThenPlaceInRecepUnseen 12. PickTwoObjAndPlaceUnseen ScienceWorld 1. Boil. 16InclinedPlaneFrictionNamedSurfaces</p>
<p>ChemistryMixPaintSecondaryColor 19. LifespanLongestLivedThenShortestLived. </p>
<p>. FindAnimal. 21MeasureMeltingPointKnownSubstance</p>
<p>InclinedPlaneDetermineAngle 30. UseThermometer TextWorld 1. CookingLevel1. 66</p>
<p>CookingLevel5 10. CookingLevel10 TWX 1. 910</p>
<p>. PeckingOrder. 1350V</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1834 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1834</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1834</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-7d784c87c5e46fedbe62006d7340baa7c7bec123</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7d784c87c5e46fedbe62006d7340baa7c7bec123" target="_blank">Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A comprehensive study of the performance of various LLMs acting as judges, focusing on a clean scenario in which inter-human agreement is high, and identifies vulnerabilities in judge models, such as their sensitivity to prompt complexity and length, and a tendency toward leniency.</p>
                <p><strong>Paper Abstract:</strong> Offering a promising solution to the scalability challenges associated with human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs). However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold. In this paper, we present a comprehensive study of the performance of various LLMs acting as judges, focusing on a clean scenario in which inter-human agreement is high. Investigating thirteen judge models of different model sizes and families, judging answers of nine different'examtaker models'- both base and instruction-tuned - we find that only the best (and largest) models achieve reasonable alignment with humans. However, they are still quite far behind inter-human agreement and their assigned scores may still differ with up to 5 points from human-assigned scores. In terms of their ranking of the nine exam-taker models, instead, also smaller models and even the lexical metric contains may provide a reasonable signal. Through error analysis and other studies, we identify vulnerabilities in judge models, such as their sensitivity to prompt complexity and length, and a tendency toward leniency. The fact that even the best judges differ from humans in this comparatively simple setup suggest that caution may be wise when using judges in more complex setups. Lastly, our research rediscovers the importance of using alignment metrics beyond simple percent alignment, showing that judges with high percent agreement can still assign vastly different scores.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1834.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1834.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models used as automated judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned and base LLMs (e.g., GPT-4 Turbo, Llama-3/3.1 70B, Mistral 7B, JudgeLM) were used to classify short QA responses as 'correct' or 'incorrect' against reference answers, and their judgments were compared quantitatively to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Llama-2 (7B,13B,70B), Llama-3 (8B,70B), Llama-3.1 (8B,70B), Mistral 7B, GPT-4 Turbo, Gemma 2B, JudgeLM 7B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Single binary judgment per response (instructed to output one word: 'correct' or 'incorrect'), using prompts derived from human guidelines and varying prompt lengths/specificity in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>short-answer question responses (free-form QA outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general-knowledge TriviaQA (short factual answers)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness relative to one or more short reference answers (semantic equivalence to reference)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Three human annotators labelled answers; majority-vote used to form a ground-truth 'Human Judgment'. After establishing high inter-human agreement, one human per sample was used for the main experiments to reduce cost.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3 (for inter-human agreement experiments on 1200 samples); thereafter single human per sample for main 400-sample comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>CS graduate students trained with provided annotation guidelines</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Percent agreement and Scott's Pi (Scott's π); Spearman rank correlation used for ranking comparisons; precision/recall and true/false positive/negative counts also reported</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Inter-human: Scott's π = 0.962 ± 0.0107 (reported as 96.2 ± 1.07 scaled by 100); percent agreement = 98.52% ± 0.42%. Judges: Scott's π varies by model (examples from paper: Llama3-70B = 0.88, Llama3.1-70B = 0.88, Mistral-7B = 0.67, JudgeLM-7B = 0.66, Gemma-2B = 0.26). Percent agreement frequently >80–90% for many judges but Scott's π distinguishes them more.</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Objective, low-ambiguity task (TriviaQA short answers) with clear reference answers; large-capacity judge models (GPT-4 Turbo, Llama-3/3.1 70B) showed high Scott's π; for top judges, even minimal instructions suffice and longer/more specific prompts yield small improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Smaller judge models (Gemma 2B, some Llama-2 variants) and simple lexical baselines have low Scott's π; lexical metrics (EM) produce many false negatives for verbose or paraphrased correct answers; judges are sensitive to prompt complexity for some models and exhibit leniency bias (tendency to mark ambiguous cases as 'correct').</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>The study focused on a low-complexity setting (short factual answers) where human agreement is high; authors note that agreement and judge reliability likely degrade for more complex or subjective artifacts (they caution against extrapolating to complex evaluation tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clarity/specificity of judge instructions affects models differently: top-performing large judges show low variance and improve slightly with more detailed guidelines; many smaller judges lose alignment with additional instruction detail (likely due to instruction-processing difficulties).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>400 judged examples per exam-taker model (400 total unique questions for the main experiments); inter-human alignment computed on 1200 samples.</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Scott's π = 0.962 ± 0.0107 (reported as 96.2 ± 1.07), percent agreement = 98.52% ± 0.42%</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Best judge models (GPT-4 Turbo, Llama-3/3.1 70B) approach but do not reach inter-human agreement; even top judges can differ up to ~5 points in assigned aggregate scores versus humans and can differ more in specific cases. Percent agreement alone is misleading (high percent agreement can coexist with substantial score deviations), while Scott's π better discriminates judge quality.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>Judge models were instruction-tuned variants and JudgeLM (a model fine-tuned to judge) was included; however, the paper did not fine-tune most judge models on the human labels from this study—judges were prompted using human-derived guidelines rather than calibrated on the study's human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Only the largest/top models (GPT-4 Turbo, Llama-3/3.1 70B) show reasonable alignment with humans, yet still fall short of inter-human agreement; 2) Percent agreement overstates alignment—Scott's π is a more informative metric and reveals larger gaps; 3) Even well-aligned judges can differ by up to ~5 points from human-assigned scores and judges with similar rank-correlations can have systematic biases; 4) Simpler/cheaper methods (contains match, Mistral 7B) can nevertheless produce rankings similar to top judges despite lower alignment; 5) Judge vulnerabilities include prompt sensitivity, leniency bias, and failure on dummy/unrelated answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Study limited to a simple, low-ambiguity QA task—results may not generalize to complex or subjective evaluation scenarios (e.g., open-ended generation, preference judgments). Sample size constrained (400 per model) though statistically justified; judges studied are autoregressive instruction-tuned models and not exhaustive of all possible judge designs; some judge models may be harder to 'steer' and smaller models may be more sensitive to prompt or reference order.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1834.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1834.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExactMatch/Contains</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lexical baselines: Exact Match (EM) and Contains substring match</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two lexical automated evaluation baselines—Exact Match (EM) and contains (substring) match—were used as proxies for correctness and compared against human judgments and LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>automated lexical metrics (Exact Match, contains substring)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>short-answer question responses (free-form QA outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general-knowledge TriviaQA (short factual answers)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Exact string match to any reference answer (EM), or reference answer appearing as substring in model response (contains), both case-insensitive</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human majority-vote ground-truth from three annotators (1200 samples used for inter-human agreement; 400-sample main set used for comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3 (for the human-ground-truth majority vote used to evaluate EM/contains)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>CS graduate student annotators using detailed guidelines</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Percent agreement and Scott's Pi used to compare lexical baselines with human judgments; Spearman rank correlations used for ranking comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>EM: low and high variance; reported mean Scott's π ~0.47 with large std (Table 4 shows EM Scott's π = 0.47, std 0.29). Contains: Scott's π ≈ 0.64 (Table 4) and high Spearman rank correlation with human ranking (ρ = 0.99 reported for 'contains' in ranking experiments). Percent agreement for lexical baselines often high but deceptive.</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>When model responses match references verbatim or include the reference string exactly (low paraphrasing/verbosity), EM and contains give correct signals and can rank models well.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Verbose or paraphrased but semantically-correct responses (common with instruction-tuned chat models) lead to many false negatives for EM and some for contains; EM especially exhibits poor alignment and high variance; lexical metrics fail on semantic paraphrase and under-specified/over-specified answers.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Lexical metrics degrade substantially when artifacts are verbose or paraphrased (as with chat models), leading to lower agreement with human judgments; they performed notably worse than humans in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Lexical metrics assume strict string-level criteria; where human criteria allow semantic equivalence or extra-but-correct information, lexical metrics undercount correct answers.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>400 main-sample comparisons; inter-human alignment measured on 1200 samples (lexical baselines computed on same sets)</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Scott's π = 0.962 ± 0.0107 and percent agreement = 98.52% ± 0.42% (used as the human truth baseline against which EM/contains were compared)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Contains produced reasonable ranking correlations with humans (ρ ≈ 0.99) and sometimes similar rankings to GPT-4 Turbo despite lower Scott's π; EM performed poorly (low Scott's π and high variance) and had many false negatives, indicating lexical baselines can be useful for ranking but are unreliable for absolute scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No calibration of lexical metrics; they are applied out-of-the-box (case-insensitive EM and substring contains) as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Lexical metrics (especially contains) can provide a useful, low-cost signal for model ranking despite low formal alignment; EM is brittle and produces many false negatives for paraphrased/verbose correct answers; lexical baselines should be used cautiously and supplemented with semantic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Lexical methods are blind to semantic equivalence and verbosity; they give misleadingly low scores for paraphrased correct answers and high variance (EM). They are inadequate where human judgment permits paraphrase, extra-but-correct information, or different answer ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1834.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1834.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgreementMetrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Percent agreement vs Scott's Pi (Scott's π)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper contrasts percent agreement with Scott's π for measuring alignment between proxy (automated) and human judgments, showing Scott's π better discriminates judge quality by correcting for chance and distributional differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>statistical agreement metrics applied to compare automated methods (LLM judges, lexical baselines) and human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>short-answer question responses (evaluation judgments)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general-knowledge QA judgments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary correctness labels ('correct'/'incorrect') compared across annotators (humans and proxies)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Three human annotators with majority vote forming ground truth; other annotators/proxy evaluators compared to that ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3 (for inter-human agreement calculation)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>CS graduate student annotators</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Percent agreement and Scott's Pi (Scott's π). Spearman's rho used for ranking comparisons; precision/recall also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Inter-human percent agreement = 98.52% ± 0.42%; inter-human Scott's π = 0.962 ± 0.0107. Judges: Scott's π ranged widely (examples: Llama3-70B = 0.88, Gemma-2B = 0.26). The paper reports that percent agreement often remains high (>80–90%) even when Scott's π is low, and high percent agreement can coincide with large score deviations (10–20 points).</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Percent agreement can appear high when class prevalence is skewed; Scott's π better reflects meaningful agreement when label distributions differ. High Scott's π observed for large-capacity judges on an objective task with low ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Percent agreement masks disagreement when chance agreement baseline is high; for judges with biased label distributions (leniency or strictness), percent agreement overestimates alignment whereas Scott's π reveals gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Noted indirectly: for low-ambiguity tasks Scott's π is still necessary to distinguish judge quality; for more complex tasks, choice of agreement metric becomes even more important but the paper did not empirically evaluate complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clear binary criteria and balanced label distributions help both metrics, but Scott's π is specifically recommended because it accounts for chance agreement and rater distribution differences.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Inter-human agreement: 1200 samples; main judge comparisons: 400 samples per exam-taker model (robustness checks with down-sampled sets reported).</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Scott's π = 0.962 ± 0.0107; percent agreement = 98.52% ± 0.42%</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Scott's π provided more discriminative power than percent agreement when comparing proxy evaluators to humans; models with similar percent agreement could differ substantially in Scott's π and in the magnitude of score deviations from human-assigned scores.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Percent agreement is insufficient as a sole metric for judge alignment; Scott's π corrects for chance and rater distribution and better separates judge models whose percent agreement values are similar. The paper recommends reporting both percent agreement and Scott's π and complementing these with qualitative analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Scott's π (and other chance-corrected metrics) has assumptions (e.g., rater distribution equivalence) and the study still focuses on binary correctness labels; complex multi-criteria or ordinal judgments may require different agreement measures not explored in depth here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluation metrics in the era of gpt-4: reliably evaluating large language models on sequence to sequence tasks <em>(Rating: 2)</em></li>
                <li>Evaluating large language models at evaluating instruction following <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1834",
    "paper_id": "paper-7d784c87c5e46fedbe62006d7340baa7c7bec123",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-judge",
            "name_full": "Large Language Models used as automated judges",
            "brief_description": "Instruction-tuned and base LLMs (e.g., GPT-4 Turbo, Llama-3/3.1 70B, Mistral 7B, JudgeLM) were used to classify short QA responses as 'correct' or 'incorrect' against reference answers, and their judgments were compared quantitatively to human judgments.",
            "citation_title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge",
            "llm_judge_model": "Llama-2 (7B,13B,70B), Llama-3 (8B,70B), Llama-3.1 (8B,70B), Mistral 7B, GPT-4 Turbo, Gemma 2B, JudgeLM 7B",
            "llm_judge_prompt_approach": "Single binary judgment per response (instructed to output one word: 'correct' or 'incorrect'), using prompts derived from human guidelines and varying prompt lengths/specificity in ablations",
            "artifact_type": "short-answer question responses (free-form QA outputs)",
            "artifact_domain": "general-knowledge TriviaQA (short factual answers)",
            "evaluation_criteria": "Correctness relative to one or more short reference answers (semantic equivalence to reference)",
            "human_evaluation_setup": "Three human annotators labelled answers; majority-vote used to form a ground-truth 'Human Judgment'. After establishing high inter-human agreement, one human per sample was used for the main experiments to reduce cost.",
            "human_expert_count": "3 (for inter-human agreement experiments on 1200 samples); thereafter single human per sample for main 400-sample comparisons",
            "human_expert_expertise": "CS graduate students trained with provided annotation guidelines",
            "agreement_metric": "Percent agreement and Scott's Pi (Scott's π); Spearman rank correlation used for ranking comparisons; precision/recall and true/false positive/negative counts also reported",
            "agreement_score": "Inter-human: Scott's π = 0.962 ± 0.0107 (reported as 96.2 ± 1.07 scaled by 100); percent agreement = 98.52% ± 0.42%. Judges: Scott's π varies by model (examples from paper: Llama3-70B = 0.88, Llama3.1-70B = 0.88, Mistral-7B = 0.67, JudgeLM-7B = 0.66, Gemma-2B = 0.26). Percent agreement frequently &gt;80–90% for many judges but Scott's π distinguishes them more.",
            "high_agreement_conditions": "Objective, low-ambiguity task (TriviaQA short answers) with clear reference answers; large-capacity judge models (GPT-4 Turbo, Llama-3/3.1 70B) showed high Scott's π; for top judges, even minimal instructions suffice and longer/more specific prompts yield small improvements.",
            "low_agreement_conditions": "Smaller judge models (Gemma 2B, some Llama-2 variants) and simple lexical baselines have low Scott's π; lexical metrics (EM) produce many false negatives for verbose or paraphrased correct answers; judges are sensitive to prompt complexity for some models and exhibit leniency bias (tendency to mark ambiguous cases as 'correct').",
            "artifact_complexity_effect": "The study focused on a low-complexity setting (short factual answers) where human agreement is high; authors note that agreement and judge reliability likely degrade for more complex or subjective artifacts (they caution against extrapolating to complex evaluation tasks).",
            "criteria_clarity_effect": "Clarity/specificity of judge instructions affects models differently: top-performing large judges show low variance and improve slightly with more detailed guidelines; many smaller judges lose alignment with additional instruction detail (likely due to instruction-processing difficulties).",
            "sample_size": "400 judged examples per exam-taker model (400 total unique questions for the main experiments); inter-human alignment computed on 1200 samples.",
            "inter_human_agreement": "Scott's π = 0.962 ± 0.0107 (reported as 96.2 ± 1.07), percent agreement = 98.52% ± 0.42%",
            "proxy_vs_human_comparison": "Best judge models (GPT-4 Turbo, Llama-3/3.1 70B) approach but do not reach inter-human agreement; even top judges can differ up to ~5 points in assigned aggregate scores versus humans and can differ more in specific cases. Percent agreement alone is misleading (high percent agreement can coexist with substantial score deviations), while Scott's π better discriminates judge quality.",
            "calibration_or_training": "Judge models were instruction-tuned variants and JudgeLM (a model fine-tuned to judge) was included; however, the paper did not fine-tune most judge models on the human labels from this study—judges were prompted using human-derived guidelines rather than calibrated on the study's human labels.",
            "key_findings": "1) Only the largest/top models (GPT-4 Turbo, Llama-3/3.1 70B) show reasonable alignment with humans, yet still fall short of inter-human agreement; 2) Percent agreement overstates alignment—Scott's π is a more informative metric and reveals larger gaps; 3) Even well-aligned judges can differ by up to ~5 points from human-assigned scores and judges with similar rank-correlations can have systematic biases; 4) Simpler/cheaper methods (contains match, Mistral 7B) can nevertheless produce rankings similar to top judges despite lower alignment; 5) Judge vulnerabilities include prompt sensitivity, leniency bias, and failure on dummy/unrelated answers.",
            "limitations_noted": "Study limited to a simple, low-ambiguity QA task—results may not generalize to complex or subjective evaluation scenarios (e.g., open-ended generation, preference judgments). Sample size constrained (400 per model) though statistically justified; judges studied are autoregressive instruction-tuned models and not exhaustive of all possible judge designs; some judge models may be harder to 'steer' and smaller models may be more sensitive to prompt or reference order.",
            "uuid": "e1834.0",
            "source_info": {
                "paper_title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ExactMatch/Contains",
            "name_full": "Lexical baselines: Exact Match (EM) and Contains substring match",
            "brief_description": "Two lexical automated evaluation baselines—Exact Match (EM) and contains (substring) match—were used as proxies for correctness and compared against human judgments and LLM judges.",
            "citation_title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges",
            "mention_or_use": "use",
            "proxy_evaluation_method": "automated lexical metrics (Exact Match, contains substring)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "short-answer question responses (free-form QA outputs)",
            "artifact_domain": "general-knowledge TriviaQA (short factual answers)",
            "evaluation_criteria": "Exact string match to any reference answer (EM), or reference answer appearing as substring in model response (contains), both case-insensitive",
            "human_evaluation_setup": "Human majority-vote ground-truth from three annotators (1200 samples used for inter-human agreement; 400-sample main set used for comparisons)",
            "human_expert_count": "3 (for the human-ground-truth majority vote used to evaluate EM/contains)",
            "human_expert_expertise": "CS graduate student annotators using detailed guidelines",
            "agreement_metric": "Percent agreement and Scott's Pi used to compare lexical baselines with human judgments; Spearman rank correlations used for ranking comparisons",
            "agreement_score": "EM: low and high variance; reported mean Scott's π ~0.47 with large std (Table 4 shows EM Scott's π = 0.47, std 0.29). Contains: Scott's π ≈ 0.64 (Table 4) and high Spearman rank correlation with human ranking (ρ = 0.99 reported for 'contains' in ranking experiments). Percent agreement for lexical baselines often high but deceptive.",
            "high_agreement_conditions": "When model responses match references verbatim or include the reference string exactly (low paraphrasing/verbosity), EM and contains give correct signals and can rank models well.",
            "low_agreement_conditions": "Verbose or paraphrased but semantically-correct responses (common with instruction-tuned chat models) lead to many false negatives for EM and some for contains; EM especially exhibits poor alignment and high variance; lexical metrics fail on semantic paraphrase and under-specified/over-specified answers.",
            "artifact_complexity_effect": "Lexical metrics degrade substantially when artifacts are verbose or paraphrased (as with chat models), leading to lower agreement with human judgments; they performed notably worse than humans in this setting.",
            "criteria_clarity_effect": "Lexical metrics assume strict string-level criteria; where human criteria allow semantic equivalence or extra-but-correct information, lexical metrics undercount correct answers.",
            "sample_size": "400 main-sample comparisons; inter-human alignment measured on 1200 samples (lexical baselines computed on same sets)",
            "inter_human_agreement": "Scott's π = 0.962 ± 0.0107 and percent agreement = 98.52% ± 0.42% (used as the human truth baseline against which EM/contains were compared)",
            "proxy_vs_human_comparison": "Contains produced reasonable ranking correlations with humans (ρ ≈ 0.99) and sometimes similar rankings to GPT-4 Turbo despite lower Scott's π; EM performed poorly (low Scott's π and high variance) and had many false negatives, indicating lexical baselines can be useful for ranking but are unreliable for absolute scoring.",
            "calibration_or_training": "No calibration of lexical metrics; they are applied out-of-the-box (case-insensitive EM and substring contains) as baselines.",
            "key_findings": "Lexical metrics (especially contains) can provide a useful, low-cost signal for model ranking despite low formal alignment; EM is brittle and produces many false negatives for paraphrased/verbose correct answers; lexical baselines should be used cautiously and supplemented with semantic evaluation.",
            "limitations_noted": "Lexical methods are blind to semantic equivalence and verbosity; they give misleadingly low scores for paraphrased correct answers and high variance (EM). They are inadequate where human judgment permits paraphrase, extra-but-correct information, or different answer ordering.",
            "uuid": "e1834.1",
            "source_info": {
                "paper_title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "AgreementMetrics",
            "name_full": "Percent agreement vs Scott's Pi (Scott's π)",
            "brief_description": "The paper contrasts percent agreement with Scott's π for measuring alignment between proxy (automated) and human judgments, showing Scott's π better discriminates judge quality by correcting for chance and distributional differences.",
            "citation_title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges",
            "mention_or_use": "use",
            "proxy_evaluation_method": "statistical agreement metrics applied to compare automated methods (LLM judges, lexical baselines) and human judgments",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "short-answer question responses (evaluation judgments)",
            "artifact_domain": "general-knowledge QA judgments",
            "evaluation_criteria": "Binary correctness labels ('correct'/'incorrect') compared across annotators (humans and proxies)",
            "human_evaluation_setup": "Three human annotators with majority vote forming ground truth; other annotators/proxy evaluators compared to that ground truth",
            "human_expert_count": "3 (for inter-human agreement calculation)",
            "human_expert_expertise": "CS graduate student annotators",
            "agreement_metric": "Percent agreement and Scott's Pi (Scott's π). Spearman's rho used for ranking comparisons; precision/recall also reported.",
            "agreement_score": "Inter-human percent agreement = 98.52% ± 0.42%; inter-human Scott's π = 0.962 ± 0.0107. Judges: Scott's π ranged widely (examples: Llama3-70B = 0.88, Gemma-2B = 0.26). The paper reports that percent agreement often remains high (&gt;80–90%) even when Scott's π is low, and high percent agreement can coincide with large score deviations (10–20 points).",
            "high_agreement_conditions": "Percent agreement can appear high when class prevalence is skewed; Scott's π better reflects meaningful agreement when label distributions differ. High Scott's π observed for large-capacity judges on an objective task with low ambiguity.",
            "low_agreement_conditions": "Percent agreement masks disagreement when chance agreement baseline is high; for judges with biased label distributions (leniency or strictness), percent agreement overestimates alignment whereas Scott's π reveals gaps.",
            "artifact_complexity_effect": "Noted indirectly: for low-ambiguity tasks Scott's π is still necessary to distinguish judge quality; for more complex tasks, choice of agreement metric becomes even more important but the paper did not empirically evaluate complex tasks.",
            "criteria_clarity_effect": "Clear binary criteria and balanced label distributions help both metrics, but Scott's π is specifically recommended because it accounts for chance agreement and rater distribution differences.",
            "sample_size": "Inter-human agreement: 1200 samples; main judge comparisons: 400 samples per exam-taker model (robustness checks with down-sampled sets reported).",
            "inter_human_agreement": "Scott's π = 0.962 ± 0.0107; percent agreement = 98.52% ± 0.42%",
            "proxy_vs_human_comparison": "Scott's π provided more discriminative power than percent agreement when comparing proxy evaluators to humans; models with similar percent agreement could differ substantially in Scott's π and in the magnitude of score deviations from human-assigned scores.",
            "calibration_or_training": null,
            "key_findings": "Percent agreement is insufficient as a sole metric for judge alignment; Scott's π corrects for chance and rater distribution and better separates judge models whose percent agreement values are similar. The paper recommends reporting both percent agreement and Scott's π and complementing these with qualitative analyses.",
            "limitations_noted": "Scott's π (and other chance-corrected metrics) has assumptions (e.g., rater distribution equivalence) and the study still focuses on binary correctness labels; complex multi-criteria or ordinal judgments may require different agreement measures not explored in depth here.",
            "uuid": "e1834.2",
            "source_info": {
                "paper_title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluation metrics in the era of gpt-4: reliably evaluating large language models on sequence to sequence tasks",
            "rating": 2
        },
        {
            "paper_title": "Evaluating large language models at evaluating instruction following",
            "rating": 2
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2
        },
        {
            "paper_title": "LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models",
            "rating": 2
        }
    ],
    "cost": 0.017075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges</h1>
<p>Aman Singh Thakur ${ }^{\star}$ and Kartik Choudhary<em> and Venkat Srinik Ramayapally</em><br>University of Massachusetts Amherst<br>{amansinghtha, kartikchoudh, vramayapally}@umass.edu<br>Sankaran Vaidyanathan<br>University of Massachusetts Amherst<br>sankaranv@cs.umass.edu</p>
<h2>Dieuwke Hupkes Meta dieuwkehupkes@meta.com</h2>
<h4>Abstract</h4>
<p>The LLM-as-a-judge paradigm offers a potential solution to scalability issues in human evaluation of large language models (LLMs), but there are still many open questions about its strengths, weaknesses, and potential biases. This study investigates thirteen models, ranging in size and family, as 'judge models' evaluating answers from nine base and instructiontuned 'exam-taker models'. We find that only the best (and largest) models show reasonable alignment with humans, though they still differ with up to 5 points from human-assigned scores. Our research highlights the need for alignment metrics beyond percent agreement, as judges with high agreement can still assign vastly different scores. We also find that smaller models and the lexical metric contains can provide a reasonable signal in ranking the exam-taker models. Further error analysis reveals vulnerabilities in judge models, such as sensitivity to prompt complexity and a bias toward leniency. Our findings show that even the best judge models differ from humans in this fairly sterile setup, indicating that caution is warranted when applying judge models in more complex scenarios.</p>
<h2>1 Introduction</h2>
<p>Over the last few years, large language models (LLMs) have demonstrated remarkable capabilities across various domains (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; AI@Meta, 2024, i.a.). As more and more new LLMs with different architectures and training methods continue to be released and their capabilities expand, accurately evaluating their performance and limitations becomes increasingly challenging (Zheng et al., 2024; Ohmer et al., 2024; Benchekroun et al., 2023; Madaan et al., 2024; Li et al., 2023a).</p>
<p>LLM evaluation methods generally fall into one of two broad categories. Benchmarks such as</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>MMLU (Hendrycks et al., 2021), TruthfulQA (Lin et al., 2021), and GSM8K (Cobbe et al., 2021) assess specific capabilities, while leaderboards such as Chatbot Arena (Chiang et al., 2024) and Open LLM Leaderboard (Beeching et al., 2023) rank models based on human or automated pairwise comparisons. Both approaches face challenges in evaluating free-form text responses, as assessment can be as difficult as generation itself (see e.g. Chang et al., 2023; Bavaresco et al., 2024).</h2>
<p>One approach to evaluating LLMs is using MCQ benchmarks like MMLU, which compare answer log-probabilities instead of assessing generated responses directly. However, this approach limits the range of measurable abilities and differs from how LLMs are used in practice. Lexical methods, such as exact match (EM) or n-gram overlap, are practical and cost-effective but prone to false negatives and often miss subtle semantic differences. These challenges are amplified for instruction-tuned chat models, which tend to produce more verbose responses (Saito et al., 2023; Renze and Guven, 2024).</p>
<p>For these reasons, human evaluation remains the gold standard for evaluating LLM responses.</p>
<p>Human evaluation is, however, expensive and often impractical, leading to the growing use of LLMs as judge models (Lin et al., 2021; Islam et al., 2023; Chiang and Lee, 2023; Liusie et al., 2024). While promising alignment with humans has been noted (Sottana et al., 2023; Zheng et al., 2024), questions about this approach remain. This work examines LLMs as judges, contrasting them with humans and automated methods. Unlike prior studies, we focus on scenarios with high human alignment to separate task ambiguity from judge model limitations. Using TriviaQA (Joshi et al., 2017), we evaluate how judge models of varying architectures and sizes assess exam-taker models.</p>
<p>In this work, we study the properties of LLMs as judges, comparing them with humans and auto-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Average scores assigned by judge models and alignment with human judges. (a) Scores assigned to all exam-taker models by the various judge models. (b) Average percent agreement (blue line) and Scott's $\pi$ scores (red bars) of judge models with human judges (black line). Error bars annotate standard deviation across exam-taker models. Llama3 70B, Llama3.1 70B and GPT-4 Turbo have Scott's $\pi$ coefficient that are indicative of excellent alignment, but are still well below the human alignment score.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Exam-taker models (base \&amp; instruction-tuned)</th>
<th style="text-align: center;">Llama-2 (7B, 13B, 70B), Mistral 7B, GPT-4 Turbo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Judge models (instruction-tuned)</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Llama-2 (7B, 13B, 70B), Llama-3 (8B, 70B), } \ &amp; \text { Llama-3.1 (8B, 70B), Gemma 2B, Mistral 7B, JudgeLM } \ &amp; \text { 7B, GPT-4 Turbo } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Judge models (lexical)</td>
<td style="text-align: center;">Exact Match (EM), Contains</td>
</tr>
</tbody>
</table>
<p>Table 1: Exam-taker models and judge models We consider a wide variety of exam-taker models and judge models; to get an in-depth overview of their abilities, we consider exam-taker models of various sizes \&amp; types.
mated evaluation methods. Contrary to prior work, we focus on a clean scenario in which human alignment is very high, allowing us to distinguish ambiguity and subjectivity in the task itself from potential issues with the judge models. Using the knowledge benchmark TriviaQA (Joshi et al., 2017) as our playground, we investigate how thirteen different judge models with varying architectures and sizes judge nine different exam-taker models. Our main findings are:</p>
<ul>
<li>Even in clean setups, only the best models have high alignment scores. Among the thirteen judge models, only GPT-4 Turbo, Llama-3.1;70B, and Llama-3;70B achieved strong alignment with humans. However, even these fall short of the human alignment coefficient (Figure 1).</li>
<li>Scott's $\pi$ distinguishes judges better than percent alignment. In terms of percent alignment,
judges are rarely discriminable, while Scott's $\pi$ provides a more informative signal. In some cases, high percent agreement can still give scores that differ 10-20 points from the human-assigned scores (Figure 2).</li>
<li>Also Scott's $\pi$ is not all telling While GPT-4 Turbo and Llama-3 achieve excellent alignment scores, they can differ by up to 5 points from human scores. Moreover, in discriminating between exam-taker models, their performance is comparable to cheaper alternatives like Mistral 7B and contains, which have lower alignment scores but more consistent biases (Figure 3).
Through detailed analysis (§5), we gain insights into judge performance. Improved alignment appear to be driven from higher recall rates and fewer false negatives. However, judge models struggle with under-specified answers and exhibit leniency, reducing evaluation consistency. They are also sen-</li>
</ul>
<p>sitive to prompt length and quality. Surprisingly, even when asked to evaluate a verbatim match with a reference, judge models sometimes fail.</p>
<p>Overall, our work highlights the strengths of the LLM-as-a-judge paradigm, while cautioning against overreliance on alignment metrics, even when they are high. Through error analysis, we identify common failure cases, contributing to a deeper understanding of this emerging evaluation paradigm. With this work, our objective is to improve understanding of the emerging mainstream paradigm for evaluating LLM.</p>
<h2>2 Related work</h2>
<p>Various recent studies have used or considered using LLMs as judges for tasks such as evaluating story generation (Chiang and Lee, 2023), retrieval-augmented generation (Es et al., 2023), visual QA (Mañas et al., 2024), code comprehension (Zhiqiang et al., 2023), multilingual evaluation (Hada et al., 2023) and more general open-ended tasks (Zheng et al., 2024). Zhang et al. (2024) and Sottana et al. (2023) propose ways to standardise LLM evaluations and the role that judge models might play in such solutions. Several studies have demonstrated that state-of-the-art LLMs such as GPT-4 Turbo exhibit high alignment with human judgments (Sottana et al., 2023; Zheng et al., 2024), though others also illustrate that the paradigm is not yet without faults. Zeng et al. (2023) propose a benchmark for evaluating the performance of LLMs as judges, and other approaches have been proposed to improve LLM judges such that they are aligned well with humans (Shankar et al., 2024; Zhu et al., 2023).</p>
<p>Despite promising results in various settings, judge models still suffer from known issues of current LLMs such as hallucinations and factual errors (Ye et al., 2023; Turpin et al., 2023) and difficulty in following complex instructions (Li et al., 2023b; He et al., 2024). Furthermore, various studies have reported challenges such as position bias (Pezeshkpour and Hruschka, 2023; Zheng et al., 2023; Wang et al., 2023), verbosity bias (Saito et al., 2023) in their preferences, confusing evaluation criteria (Hu et al., 2024), or focusing more on the style and grammar compared to factuality (Wu and Aji, 2023). Recently, Liusie et al. (2024) have shown that LLMs perform better in comparative assessment compared to absolute scoring, which can be used for reliably measuring the relative per-
formance of models (Liu et al., 2024) and creating classifiers for pairwise grading (Huang et al., 2024).</p>
<p>We build on previous work to investigate the strengths and weaknesses of LLMs as judges. Unlike previous studies, we focus on comparing LLM outputs with reference answers rather than pairwise comparisons on open-ended tasks. With high human alignment in this setting, we gain a clearer view of LLM performance. Furthermore, we extend previous research by considering more LLMs, both as judges and as evaluated models.</p>
<h2>3 Methodology</h2>
<p>To evaluate the strengths and weaknesses of the LLM-as-a-judge paradigm, we focus on a comparatively controlled setup, in which judge models assess answers of exam-taker models on the knowledge benchmark TriviaQA (Joshi et al., 2017). With this methodological design, it is possible to focus on the abilities of the judges in isolation, without having to address human disagreement and error at the same time. In this section, we elaborate the main aspects of our methodology.</p>
<p>Evaluation data As our testbed, we use the TriviaQA dataset (Joshi et al., 2017), consisting of 95 K question-answer pairs sourced from 14 trivia and quiz league websites. Each question in the train and validation set is annotated with a list of short answers containing a minimal set of facts and evidence documents collected from Wikipedia and the Web. For our experiments, we use the validation set of the unfiltered partition of the benchmark, using the short answers as reference answers. We use the training set for few-shot examples.</p>
<p>Since experiments require manual annotation of the exam-taker model responses, we use a random sample of 400 questions from the dataset. In Appendix I, we show with a bootstrapping test that this sample size has low variance for our main result. Through experiments described in $\S 3$, we establish that humans have high agreement on judgements of answers given to the questions in the benchmark.</p>
<p>Exam-taker models To understand the strengths and weaknesses of different judges, we consider answers of pre-trained (base) and instruction-tuned (chat) 'exam-taker models' across a wide variety of model sizes. In particular, we consider Llama-2 (Touvron et al., 2023) in 7B, 13B, and 70B parameter sizes for both base and chat versions, Mistral</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Difference with human evaluation scores versus alignment metric. The delta evaluation score is the difference between the judge and the human score; y -axes are in log scale. Percent alignment (left) shows a very skewed distribution, making it difficult to distinguish models. Scott's $\pi$ (left) provides a clearer difference between models, and is more indicative of deviation of the gold score.</p>
<p>7B (Jiang et al., 2023) base and chat versions, and GPT-4 Turbo ${ }^{1}$ (Achiam et al., 2023) as the examtaker models. The prompts for the exam-taker models contain five few-shot examples of (question, answer) pairs from the TriviaQA training set. The prompts for the instruction-tuned models additionally include a command signaling the model to answer the given question in a succinct manner similar to the provided examples. The prompts are provided in Appendix D.</p>
<p>Judge models To get a comprehensive view of the strengths and weaknesses of judge models across different model sizes and architectures, we use instruction-tuned versions of Llama-2 (Touvron et al., 2023) in 7B, 13B, and 70B sizes, Llama-3 (AI@Meta, 2024) in 8B and 70B sizes, Llama-3.1 (Dubey et al., 2024) in 8B and 70B sizes, Mistral 7B (Jiang et al., 2023), GPT-4 Turbo (Achiam et al., 2023), Gemma 2B (Gemma Team et al., 2024), and JudgeLM 7B (Zhu et al., 2023) as judges. To maintain parity with human and judge evaluation, judge prompts were built from human guidelines in Appendix G. The judges are instructed to respond with only a single word, "correct" or "incorrect". An overview of all exam-taker models and judge models is shown in Table 1. For ease of reading, the judge models are depicted in a different font than the exam-taker models.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Baselines As baselines, we use two commonly used lexical evaluation techniques - exact match (EM) and contains match (contains). For EM, a response is considered correct if the response exactly matches one of the reference answers for the given question. For contains, an answer is considered correct if at least one of the reference answers is a sub-string of the response string. Both EM and contains match are computed in a case-insensitive manner.</p>
<p>Alignment We use two metrics to quantify alignment between judges: percent agreement and Scott's Pi coefficient (Scott, 1955). ${ }^{2}$ Percent agreement expresses a simple percentage of the samples on which two annotators agree. Scott's Pi, denoted as Scott's $\pi$, is an alignment metric that corrects for chance agreement between two annotators and is considered to provide a more robust measure of alignment. Details about the computation of both metrics are given in Appendix F.</p>
<p>Human judgements As a ground-truth assessment, we obtain human annotations for each examtaker model answer. The inter-human alignment is calculated between three human judges using the answers to 1200 randomly sampled questions answers; the human guidelines can be found in Appendix G. We then determine collective "Human</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Judge rankings and true/false positives and negatives. (a) Assigned exam-taker model rankings assigned by highly human aligned judges. Contains stays closely to human-assigned rankings, as well as GPT-4 Turbo and Mistral 7B. (b) False positives and negatives across different judge models, in descending order of human alignment. Both false negatives and false positives increase as human alignment decreases, but well-aligned models tend to produce more false positives than false negatives.</p>
<p>Judgment" through a majority vote.
The average alignment between human evaluators and the majority vote yielded a Scott's $\pi$ of $96.2 \pm 1.07,{ }^{5}$ while the average percentage agreement was $98.52 \% \pm 0.42 \%$, exceeding the alignment previously reported in comparable studies (Zeng et al., 2024).</p>
<p>The details of this experiment are mentioned in Appendix A. Given this near-perfect alignment score, we consider only one human evaluator per sample for the rest of our experiments, to reduce the overall cost of human annotations. The set of questions for which we obtain human annotations is identical for each exam-taker model.</p>
<h2>4 Results</h2>
<p>In this section we discuss our main results, primarily focusing on the relationship between evaluations by various judge models and human evaluations (§ 4.1), and how that impacts their usability (§ 4.2). To do so, we evaluate their alignment with human judgment and assess how differently they rank the nine exam-taker models compared to humans. In Section 5, we further analyse their precision and recall to further investigate the types of errors that can be made by various judge models. Details about compute requirements and others costs for experiments are given in Appendix H.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.1 Alignment between judge models and humans</h3>
<p>We start by computing Scott's $\pi$ scores and percent agreement between the evaluations of each judge model and the human annotators. We show the result in Figure 1. We observe that percent alignment is high for virtually all models, with the exception of Gemma 2B and EM. Scott's $\pi$, on the other hand, has low values for most models, though its value is in the high 80s for Llama-3 70B, Llama-3.1 70B and GPT-4 Turbo. Nevertheless, there still is a significant disparity between human judgment and judge models: the best scoring judge, Llama-3 70B, is 8 points behind human judgment. Notably, EM has the most variance in alignment, while Gemma 2B has the lowest alignment amongst all judges.</p>
<p>In most cases, we observe that Scott's $\pi$ and percent agreement are following the same trend, with the exception of the values for Gemma 2B and EM. Gemma 2B shows higher percent agreement compared to EM, yet it yields the lowest Scott's $\pi$ score within the ensemble. For the percent agreement of judge models, we note a 26-point difference between human judgment and EM, while Scott's $\pi$ exhibits a more substantial 64-point gap. This is also visible in the general decline of alignment scores: while Llama-3 8B has a Scott's $\pi$ score of only 59, its percent agreement is still well above $80 \%$. Overall, Scott's $\pi$ appears to be better able of discriminating various judge models, showing more divergence across the tested judges.</p>
<p>To understand how indicative the two alignment metrics are of the expected accuracy of the overall judgement of the models, we plot, for each judge model and exam-taker model, the difference between the score assigned by the judge and the score assigned by a human. In the figure, we can see that for Scott's $\pi$ values higher than 80 , the evaluation scores are comparatively close to the human evaluation scores, with a difference of up to 5 points in their assigned scores (complete results table provided in Appendix J). For percent alignment, on the other hand, even judges that have more than $90 \%$ may still differ more than 10 points in their assigned score. Interestingly, the deviation from human-judgements for a single judge model can be quite different depending on the exam-taker model. In Figure 1a, Gemma 2B, for instance, sometimes assigns higher scores than humans, and sometimes much lower. In the next section, we further explore this particular pattern.</p>
<h3>4.2 Exploring consistent patterns in judge models</h3>
<p>In the previous section, we saw that none of the judge models were as aligned with humans as humans were with each other. As shown in Figure 2, even the best-aligned judge models can differ by up to 5 points from human-assigned scores. While this limits their ability to perfectly estimate exam-taker model capabilities, judge models can still provide valuable insights to differentiate between examtaker models. For example, judges with consistent biases may not assign identical scores but could rank models similarly, akin to a very strict teacher.</p>
<p>To assess this, we compare the rankings given by each judge model to the nine exam-taker models, computing Spearman's rank correlation coefficients $\rho$ (Spearman, 1904) with the human ranking. The rankings are shown in Figure 3a, with $\rho$ and $\sigma$ values in Appendix L. Most judge models have rank correlations above 0.7 , indicating they struggle to distinguish poorer models but do well with better ones. Notably, models like contains and Mistral 7B, which have divergent scores from humans, show high rank correlation ( $\rho$ of 0.99 and 0.98 , respectively), performing similarly to GPT-4 Turbo and outperforming the better Llama models - though with lower significance values indicating that identifying which models are better should not be equated to assigning them the correct score.</p>
<h2>5 Analysis</h2>
<p>To better understand the judge models, we conduct multiple case studies aimed at identifying common errors and vulnerabilities in the judges we investigate. Specifically, we study their precision and recall and error types (§5.1), their sensitivity to the instruction prompt prompt (§5.2), how they respond to controlled resposes of specific types (§5.3), and the extent to which they have a leniency bias (§5.4).</p>
<h3>5.1 Better aligned models: Precision and recall gains with error spotlights</h3>
<p>We first examine the precision and recall of the judge models. As shown in Figure 4a, both metrics increase moderately with alignment. Figure 3b reveals a similar trend, with a clearer distribution of false positives and negatives. True positives remain consistent across varying judge quality, whereas true negatives exhibit a slight decline as judge quality decreases. Notably, a reduction in judge quality leads to an increase in false positives.</p>
<p>Next, we analyze the errors made by judge models by manually annotating 900 outputs from Llama-7B Base, focusing on top performers GPT-4 Turbo and Llama-3;70B. We categorize error types and determine how often they are correctly judged as incorrect. The results in Table 2 show that both GPT-4 Turbo and Llama-3;70B excel at identifying answers referring to incorrect entities or containing too many entities. Underspecified and incorrect answers are more challenging, with GPT-4 Turbo performing better on answers with fewer entities than Llama-3;70B.</p>
<h3>5.2 Judge model sensitivity to prompt length and specificity</h3>
<p>Next, we investigate how prompt length and specificity affect judge models' inferences to determine whether their performance is influenced by specificity of the prompt. We use four prompt versions with varying length and specificity.</p>
<p>The first two prompts (Without; guidelines;V1/V2, 45 and 58 tokens) ask for an evaluation without further details. The longer prompts (Guidelines; without; examples and Guidelines;with;examples, 245 and 301 tokens) provide more elaborate guidance and examples. All prompts are listed in Appendix M.</p>
<p>Figure 4b shows that GPT-4 Turbo, Llama-3;70B, and Llama-3.1;70B exhibit</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Precision, recall and prompt sensitivity. (a) Recall and precision improve with increasing human alignment (R² = 0.31 and R² = 0.21, respectively). (b) Scott's π scores for judges across different instructions.</p>
<table>
<thead>
<tr>
<th>Error code</th>
<th>Explanation</th>
<th>Example</th>
<th>Proportion</th>
<th>GPT-4 recall</th>
<th>Llama-3 70B recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>Incorrect entity</td>
<td>Response refers to a wrong entity</td>
<td>Henry VII, James I, Edward VI, Mary I and Elizabeth I</td>
<td>86.9%</td>
<td>98.3%</td>
<td>96.6%</td>
</tr>
<tr>
<td>Under-specified</td>
<td>Response contains only part of the answer</td>
<td>Henry VII, Henry VIII, Edward, Mary, and Elizabeth</td>
<td>37.3%</td>
<td>33.9%</td>
<td>23.3%</td>
</tr>
<tr>
<td>Too few entities</td>
<td>Response contains too few entities</td>
<td>Henry VII, Edward VI, Mary I and James I</td>
<td>2.47%</td>
<td>80.0%</td>
<td>60.0%</td>
</tr>
<tr>
<td>Too many entities</td>
<td>Response contains too many entities</td>
<td>Henry VII, Henry VIII, Edward VI, Mary I, James I, and Elizabeth I</td>
<td>2.7%</td>
<td>90.1%</td>
<td>90.1%</td>
</tr>
<tr>
<td>Other</td>
<td>Response is incorrect but cannot be put into any of the above buckets</td>
<td>I'm sorry but I do not know the answer to that question</td>
<td>1.23%</td>
<td>20.0%</td>
<td>40.0%</td>
</tr>
</tbody>
</table>
<p>Table 2: Error analysis for GPT-4 and Llama-3 70B judges. The example question is "Excluding Lady Jane Grey, who were the five monarchs of the House of Tudor?", the correct answer "Henry VII, Henry VIII, Edward VI, Mary I and Elizabeth I" (in any order).</p>
<p>low variance in human agreement as prompt length and specificity increases. Top performers show high alignment with humans even with minimal instructions, while they slightly improve with more detailed prompts. In contrast, other models lose alignment with increased instructions, likely due to difficulty processing complex instructions.</p>
<p>In a follow-up experiment, we investigate the impact of reference order (see Appendix N). Figure 14 and Figure 15 shows that larger models maintain consistent judgments regardless of reference order, while smaller models, except Mistral; 7B, are more sensitive to it.</p>
<h3>5.3 Evaluating controlled responses</h3>
<p>We conduct simple tests on the judge models by having them evaluate dummy benchmark responses. In the first test, the answer is a verbatim reference from the dataset (always correct). In the next three tests, the answers are incorrect. For the second and third tests, the dummy exam-taker model responds with "Yes", and "Sure" respectively. In the fourth test, the evaluated answer is a repetition of the question.</p>
<p>In Figure 5, we observe that while some judge models correctly identify and mark answers as correct (first test) or incorrect (next three tests), others, like Llama-2; 70B, incorrect evaluate many dummy answers, despite showing high human alignment on benchmark evaluations (see Figure 1b). We hypothesize that when the answers are plausible but incorrect, judges can correctly identify them as wrong by comparing them with the reference. However, when the answer is unrelated (e.g., "Yes", and "Sure"), judge models may mistakenly mark them as correct, though further research is needed to clarify this behavior.</p>
<h3>5.4 Leniency bias in judge models</h3>
<p>Lastly, to get a general sense of the inherent biases or misalignment in the evaluation criteria that might be present in the judge models, we estimate if they have a positive or negative bias in their judgment. To do so, we assume that a judge as-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Judge responses to dummy answers. We investigate how judge models respond to dummy answers. judge models remain robust when exam-taker models produce responses identical to the prompt ('repeater'), but are less robust when the responses are "Yes" and "Sure". Even when the answer matches one of the reference answers verbatim ('Gold answer'), judges do not always arrive at the correct judgement.
signs the correct judgment (i.e. same evaluation as the ground truth) with a probability of $P_{c}$ and assigns the rest of the samples to be "correct" with a probability $P_{+}$, which we call their leniency bias. We estimate the values of $P_{c}$ and $P_{+}$from the benchmark results ${ }^{4}$ and show them in Figure 16a. We observe that $P_{+}$for most models is significantly higher than 0.5 (Figure 16b), indicating a tendency of the judge models to evaluate responses as "correct" when their evaluation criteria are not completely aligned with the provided instructions.</p>
<h2>6 Conclusion</h2>
<p>In this work, we conduct an extensive study of LLMs as judges, comparing them to human judges and automated evaluation methods. By focusing on a clean evaluation scenario with high inter-human agreement, we identify potential issues with the LLM-as-a-judge paradigm, separate from task ambiguity.</p>
<p>We find that smaller, cost-efficient models, like Mistral;7B, are less effective than larger models such as GPT-4 Turbo, Llama-3.1;70B, and Llama-3;70B, which are better aligned but still fall short of human alignment. Even with high alignment, their scores can differ by up to 5 points from human scores, highlighting the need for caution when using judges in more complex scenarios. We also note that the commonly used metric of percent aligned fails to differentiate between judges effectively. We suggest future work adopt the more robust Scott's $\pi$ metric for better distinction.</p>
<p>Next, we note that high alignment scores are not</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>always necessary to discriminate between models. While GPT-4 Turbo and Llama-3 have excellent alignment scores, simpler and more cost-efficient models, like contains, perform similarly in ranking exam-taker models, despite lower alignment scores and score deviations. For studies focused on ranking models rather than estimating exact scores, these approaches can be as suitable as more expensive ones.</p>
<p>Lastly, we run experiments to assess judge models' sensitivity to prompts, precision, recall, error types, leniency, and vulnerability to dummy answers. We find that smaller models are more likely to judge positively when in doubt, that loweralignment models lack precision, and that better models are more robust across different prompts but harder to "steer." Some judge models are easily fooled by dummy answers like "Yes" and "Sure" and are better at detecting completely incorrect answers than partially incorrect ones.</p>
<p>Overall, this work contributes to LLM evaluation by assessing judges in a clearly defined framework. Our results highlight the potential of LLMs as judges but caution against blindly trusting their judgments, even when aligned with humans. We recommend computing both percent agreement and Scott's $\pi$, paired with qualitative analysis, to avoid bias. We discuss limitations in Appendix A and plan to expand our work to more complex scenarios in the future.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical re-</p>
<p>port. arXiv preprint arXiv:2303.08774.
AI@Meta. 2024. Llama 3 model card.
Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, André F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, and Alberto Testoni. 2024. Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks. Preprint, arXiv:2406.18403.</p>
<p>Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023. Open llm leaderboard. https://huggingface.co/ spaces/HuggingFaceH4/open_llm_leaderboard.</p>
<p>Youssef Benchekroun, Megi Dervishi, Mark Ibrahim, Jean-Baptiste Gaya, Xavier Martinet, Grégoire Mialon, Thomas Scialom, Emmanuel Dupoux, Dieuwke Hupkes, and Pascal Vincent. 2023. Worldsense: A synthetic benchmark for grounded reasoning in large language models. arXiv preprint arXiv:2311.15930.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Preprint, arXiv:2005.14165.</p>
<p>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937.</p>
<p>Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chatbot arena: An open platform for evaluating LLMs by human preference. Preprint, arXiv:2403.04132.</p>
<p>Davide Chicco, Matthijs J. Warrens, and Giuseppe Jurman. 2021. The matthews correlation coefficient (mcc) is more informative than cohen's kappa and brier score in binary classification assessment. ieee access, 9:78368-78381.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.
J. Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Cauchetsux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy,</p>
<p>Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng</p>
<p>Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubeit Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Özlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vitor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783.</p>
<p>Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023. RAGAS: Automated evaluation of retrieval augmented generation. Preprint, arXiv:2309.15217.</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex CastroRos, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer,</p>
<p>Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open models based on gemini research and technology. Preprint, arXiv:2403.08295.</p>
<p>Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. 2023. Are large language model-based evaluators the solution to scaling up multilingual evaluation? arXiv preprint arXiv:2309.07462.</p>
<p>Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. 2024. Can large language models understand real-world complex instructions? Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):18188-18196.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Preprint, arXiv:2009.03300.</p>
<p>Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, and Xiaojun Wan. 2024. Are LLM-based evaluators confusing nlg quality criteria? arXiv preprint arXiv:2402.12055.</p>
<p>Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. 2024. An empirical study of LLM-as-a-Judge for LLM evaluation: Fine-tuned judge models are task-specific classifiers. Preprint, arXiv:2403.02839.</p>
<p>Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. 2023. FinanceBench: A new benchmark for financial question answering. arXiv preprint arXiv:2311.11944.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.</p>
<p>Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2023a. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470.</p>
<p>Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, and Hongxia Jin. 2023b. Instruction-following evaluation through verbalizer manipulation. arXiv preprint arXiv:2307.10558.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. TruthfulQA: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.</p>
<p>Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, and Nigel Collier. 2024. Aligning with human judgement: The role of pairwise preference in large language model evaluators. arXiv preprint arXiv:2403.16950.</p>
<p>Adian Liusie, Potsawee Manakul, and Mark Gales. 2024. LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 139-151, St. Julian's, Malta. Association for Computational Linguistics.</p>
<p>Lovish Madaan, Aaditya K. Singh, Rylan Schaeffer, Andrew Poulton, Sanmi Koyejo, Pontus Stenetorp, Sharan Narang, and Dieuwke Hupkes. 2024. Quantifying variance in evaluation benchmarks. arXiv preprint arXiv:2406.10229.</p>
<p>Oscar Mañas, Benno Krojer, and Aishwarya Agrawal. 2024. Improving automatic vqa evaluation using large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(5):4171-4179.</p>
<p>Xenia Ohmer, Elia Bruni, and Dieuwke Hupkes. 2024. From form (s) to meaning: Probing the semantic depths of language models using multisense consistency. arXiv preprint arXiv:2404.12145.</p>
<p>Pouya Pezeshkpour and Estevam Hruschka. 2023. Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483.</p>
<p>Robert Gilmore Pontius and Marco Millones. 2011. Death to kappa: birth of quantity disagreement and allocation disagreement for accuracy assessment. Int. J. Remote Sens., 32(15):4407-4429.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Matthew Renze and Erhan Guven. 2024. The benefits of a concise chain of thought on problemsolving in large language models. arXiv preprint arXiv:2401.05618.</p>
<p>Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto. 2023. Verbosity bias in preference labeling by large language models. Preprint, arXiv:2310.10076.
W.A. Scott. 1955. Reliability of content analysis: The case of nominal scale coding. The Public Opinion Quarterly, 17:133-139.</p>
<p>Shreya Shankar, JD Zamfirescu-Pereira, Björn Hartmann, Aditya G Parameswaran, and Ian Arawjo. 2024. Who validates the validators? aligning llmassisted evaluation of llm outputs with human preferences. arXiv preprint arXiv:2404.12272.</p>
<p>Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan. 2023. Evaluation metrics in the era of gpt-4: reliably evaluating large language models on sequence to sequence tasks. arXiv preprint arXiv:2310.13800.
C. Spearman. 1904. The proof and measurement of association between two things. The American Journal of Psychology, 15(1):72-101.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. 2023. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. In Advances in Neural Information Processing Systems, volume 36, pages 74952-74965. Curran Associates, Inc.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.</p>
<p>Minghao Wu and Alham Fikri Aji. 2023. Style over substance: Evaluation biases for large language models. Preprint, arXiv:2307.03025.</p>
<p>Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2023. Cognitive mirage: A review of hallucinations in large language models. arXiv preprint arXiv:2309.06794.</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2023. Evaluating large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641.</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2024. Evaluating large language models at evaluating instruction following. Preprint, arXiv:2310.07641.</p>
<p>Yue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. Llmeval: A preliminary study on how to evaluate large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):19615-19622.</p>
<p>Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2023. On large language models' selection bias in multi-choice questions. arXiv preprint arXiv:2309.03882.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Advances in Neural Information Processing Systems, 36.</p>
<p>Yuan Zhiqiang, Liu Junwei, Zi Qiancheng, Liu Mingwei, Peng Xin, Lou Yiling, et al. 2023. Evaluating instruction-tuned large language models on code comprehension and generation. arXiv e-prints arXiv:2308.01240.</p>
<p>Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges. Preprint, arXiv:2310.17631.</p>
<h2>A Limitations</h2>
<p>In our work, we have evaluated how 11 different LLMs fare as judges in a scenario in which judgements should be relatively straight-forward, and human alignment is high. As any study, our work has several limitations as well as directions that we did not explore but would have been interesting too. In this section, we discuss both.</p>
<p>Simplicity of the task As mentioned in the introduction of our work, the scenario in which judges are used are typically much more complicated than the scenario that we focussed on. Specifically, judges are most often deployed in preference rankings (where two model responses are compared) or to judge complex answers that are difficult to automatically parse. In such tasks, human agreement is often low, making it challenging to judge the judges themselves. In our work, we have deliberately chosen for a simple task, in which human alignment is high. The main premise is, that if a judge does not perform well in this simple setup, caution is suggested also in more complex setups - if someone cannot do multiplication, why would they be able to solve ordinary differential equations. Given the poor understanding of which abilities of LLMs generalise in what dimensions, however, more studies are needed to understand how our results generalise to various other scenarios.</p>
<p>Human alignment In an earlier version of this paper, due to the high cost of human annotations, we opted to select a single model for human annotation as we iteratively modified the exam taker prompt, few-shot examples, and guidelines. We selected the Llama2 7B for this purpose with a random sample of 600 questions. As this is only a single model, it is possible that our human alignment scores are biased because of that. After, we have therefore extended our results with another 600 human-annotated examples from Llama3.1 70B.</p>
<p>For Llama2 7B The average alignment among human evaluators had a Scott's $\pi$ of $96.36 \pm$ 1.46 , and the average percent agreement was $98.33 \% \pm 0.76 \%$. For Llama3.1 70B, we noted that the average alignment among human evaluators had Scott's $\pi$ of $95.78 \pm 0.30, \%$ and the average percent agreement was $98.72 \% \pm 0.10 \%$. Given the similarity of these two numbers, we believe that these 1200 samples provide an adequate estimate. In the paper, we take the average.</p>
<p>Size of the judged samples As each of the nine exam-taker models requires human annotations for each sample, we restricted our analysis to 400 samples in total. This sample size also allowed us to conduct manual annotations and error analysis within 75 human hours/200 GPU hours (see Appendix H) and give reliable confidence intervals while also providing the flexibility to compare a range of models. We were not able to increase the size due to the cost, but a statistical analysis (details provided in Appendix I) illustrated that the variance because of this sample size was very low.</p>
<p>Selection of judges With our selection of judges, we have stuck to autoregressive judges that can be used off-the-shelve, as well as one LLM specifically trained to judge. They are - at the moment of writing - the ones that are most commonly used as LLM-judges, and we have tried to be comprehensive across size and family. Nevertheless, we acknowledge that there are other judges that we could have considered as well. As including more judges in - compared to including more exam-taker models- relatively straightforward because it requires only computational power, no manual annotation, we hope that others may evaluate their newly proposed judges using our setup as well.</p>
<p>Future work All in all, these differences underline how finicky using LLMs as judges can be, and with that confirm the overall conclusions of our study that much more work is needed to better understand the strengths and limitations of judge models across a wide range of scenarios and model accuracies. We consider assessing the strengths across multiple different samples and tasks, which would require many more human annotations, outside the scope of this paper and leave such experimentation for future work.</p>
<h2>B A brief explanation of the theoretical issues with Cohen's kappa</h2>
<p>Cohen's Kappa Coefficient (Cohen, 1960) is a statistic to measure inter-rater agreement for categorical responses. Cohen's Kappa coefficient measures this agreement by computing the observed (percent) agreement between raters $\left(p_{o}\right)$ and comparing it with the hypothetical probability of chance agreement $\left(p_{e}\right)$, which is taken as a baseline, as follows:</p>
<p>$$
\kappa \equiv \frac{p_{o}-p_{e}}{1-p_{e}}
$$</p>
<p>In this equation, the chance agremeent $p_{o}$ constitutes the hypothetical probability that observed agreement occurred by chance, given the observed distributions of the considered raters, under the assumption that the probabilities the raters assign to the observed labels are independent. Specifically, it is defined as:</p>
<p>$$
\begin{aligned}
p_{e} &amp; =\sum_{k} \widehat{p_{k 12}}={ }^{i n d} \sum_{k} \widehat{p_{k 1}} \widehat{p_{k 2}} \
&amp; =\sum_{k} \frac{n_{k 1}}{N} \cdot \frac{n_{k 2}}{N}=\frac{1}{N^{2}} \sum_{k} n_{k 1} n_{k 2}
\end{aligned}
$$</p>
<p>where $\widehat{p_{k 12}}$ is the estimated probability that rater 1 and rater 2 will classify the same item as $k$, rewritten to $\widehat{p_{k 1}} \widehat{p_{k 2}}$ under the assumption that $p_{k 1}$ and $p_{k 2}$ are independent. The crux of the issue with this method of computation, is that $\widehat{p_{k 1}}$ and $\widehat{p_{k 2}}$ are estimated independently from the data. As such, the chance agreement adjusts for the observed average differences between raters, which is in fact part of what we intend to measure.</p>
<p>To address this issue, Scott's Pi (Scott, 1955) instead defines the chance baseline under the assumption that the raters have the same distribution, which is estimated considering the joint distribution of rater 1 and rater 2, rather than considering them separately. It defines $p_{e}$ as:</p>
<p>$$
p_{e}=\sum_{k} \widehat{p_{k}^{2}}=\sum_{k} \sum_{k}\left(\frac{n_{k 1}+n_{k 2}}{2 N}\right)^{2}
$$</p>
<p>As such, contrary to Cohen's Kappa, it captures differences surpassing the chance agreement if rater 1 and rater 2 were in fact equivalent. In other words, we compare against a baseline in which raters would be equivalent, and we measure how much they deviate from that.</p>
<p>Note that if the empirical distributions of rater 1 and rater 2 are the same, so will the values of Scott's Pi and Cohen's Kappa be. This also implies that for larger observed (percent) alignment values, the values for Cohen's Kappa and Scott's Pi will be closer.</p>
<h2>C Model and dataset details</h2>
<p>In Appendix C, we show the different models and datasets used in our experiments, along with version and license details.</p>
<h2>D Model evaluation prompt templates</h2>
<p>In Figure 6 and Figure 7, we show the prompt templates used for the base and chat exam-taker models during the question answering process.</p>
<h2>E Judge LLM Prompt templates</h2>
<p>In Figure 8, we show the prompt template used to guide the judge models during the evaluation process of a 400 -question sample from the TriviaQA unfiltered dataset.</p>
<h2>F Metrics for judge models</h2>
<p>If one of the annotators is taken to be the reference, then the annotations of the other annotator can be categorized as true positives, false positives, true negatives, and false negatives, with the total number of each of them in a benchmark being represented by $T_{P}, F_{P}, T_{N}$, and $F_{N}$ respectively.</p>
<p>Percent agreement is simply the ratio of the numbers of times two annotators agree with each other relative to the total number of annotations. This ratio can have values between 0 and 1 . For the binary case, the alignment ratio $\rho$ is given as</p>
<p>$$
\rho=\frac{T_{P}+T_{N}}{T_{P}+F_{P}+T_{N}+F_{N}}
$$</p>
<p>Scott's Pi, (Scott, 1955), measures the alignment of two annotators while also taking into account the possibility of agreement by pure chance. This coefficient usually has values above 0 in most realworld situations. The value of Scott's Pi is given below where $p_{o}$ is the relative observed agreement, and $p_{e}$ is the hypothetical probability of chance agreement.</p>
<h1>Prompt template for Base exam-taker models</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Can</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">actress</span><span class="w"> </span><span class="n">who</span><span class="w"> </span><span class="n">links</span><span class="w"> </span><span class="s1">&#39;The Darling Buds of May&#39;</span><span class="w"> </span><span class="n">and</span>
<span class="s1">&#39;Rosemary and Thyme&#39;</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Pam</span><span class="w"> </span><span class="n">Ferris</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">neologism</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="k">new</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Word</span><span class="o">/</span><span class="n">expression</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Who</span><span class="o">,</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="mi">2010</span><span class="o">,</span><span class="w"> </span><span class="n">became</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">person</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">outside</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">British</span>
<span class="n">Isles</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">win</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">World</span><span class="w"> </span><span class="n">Snooker</span><span class="w"> </span><span class="n">Championship</span><span class="w"> </span><span class="n">title</span><span class="w"> </span><span class="n">since</span><span class="w"> </span><span class="n">Cliff</span><span class="w"> </span><span class="n">Thorburn</span>
<span class="k">in</span><span class="w"> </span><span class="mi">1980</span><span class="o">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">non</span><span class="w"> </span><span class="n">British</span><span class="w"> </span><span class="n">player</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">win</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">title</span><span class="w"> </span><span class="n">since</span><span class="w"> </span><span class="n">Ken</span>
<span class="n">Doherty</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="mi">1997</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Neil</span><span class="w"> </span><span class="n">Robertson</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Which</span><span class="w"> </span><span class="n">German</span><span class="w"> </span><span class="n">Nazi</span><span class="w"> </span><span class="n">leader</span><span class="w"> </span><span class="n">flew</span><span class="w"> </span><span class="n">solo</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">Ausberg</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="mi">1941</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">landed</span>
<span class="n">by</span><span class="w"> </span><span class="n">parachute</span><span class="w"> </span><span class="n">near</span><span class="w"> </span><span class="n">Glasgow</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="kd">private</span><span class="w"> </span><span class="n">peace</span><span class="w"> </span><span class="n">mission</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Hess</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Where</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">Narita</span><span class="w"> </span><span class="n">airport</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Tokyo</span><span class="o">,</span><span class="w"> </span><span class="n">Japan</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Which</span><span class="w"> </span><span class="n">cartoon</span><span class="w"> </span><span class="n">title</span><span class="w"> </span><span class="n">character</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">friend</span><span class="w"> </span><span class="n">called</span><span class="w"> </span><span class="n">Captain</span><span class="w"> </span><span class="n">Haddock</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span>
</code></pre></div>

<p>Figure 6: Prompt template for base exam-taker models</p>
<h2>Prompt template for Chat exam-taker models</h2>
<p>You are a part of a question answering benchmark. Look at the following examples on how to answer the questions.</p>
<div class="codehilite"><pre><span></span><code>&quot;&quot;
Q: Can you name the actress who links &#39;The Darling Buds of May&#39; and
&#39;Rosemary and Thyme&#39;?
A: Pam Ferris
Q: A neologism is a new?
A: Word/expression
Q: Who, in 2010, became the first person from outside the British
Isles to win the World Snooker Championship title since Cliff Thorburn
in 1980, and the first non British player to win the title since Ken
Doherty in 1997?
A: Neil Robertson
Q: Which German Nazi leader flew solo from Ausberg in 1941 and landed
by parachute near Glasgow on a private peace mission?
A: Hess
Q: Where would you find Narita airport?
A: Tokyo, Japan
&quot;&quot;

Your task is to answer the following question. Remember to be concise
and only give the answer in a few words.
Q:Which cartoon title character has a friend called Captain Haddock?
A:
</code></pre></div>

<p>Figure 7: Prompt template for Chat exam-taker models</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Asset</th>
<th style="text-align: left;">Version</th>
<th style="text-align: left;">License</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TriviaQA</td>
<td style="text-align: left;">mandarjoshi/trivia_qa</td>
<td style="text-align: left;">apache-2.0</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2 7B Base</td>
<td style="text-align: left;">meta-llama/Llama-2-7b-hf</td>
<td style="text-align: left;">llama2</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2 7B Chat</td>
<td style="text-align: left;">meta-llama/Llama-2-7b-chat-hf</td>
<td style="text-align: left;">llama2</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2 13B Base</td>
<td style="text-align: left;">meta-llama/Llama-2-13b-hf</td>
<td style="text-align: left;">llama2</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2 13B Chat</td>
<td style="text-align: left;">meta-llama/Llama-2-13b-chat-hf</td>
<td style="text-align: left;">llama2</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2 70B Base</td>
<td style="text-align: left;">meta-llama/Llama-2-70b-hf</td>
<td style="text-align: left;">llama2</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2 70B Chat</td>
<td style="text-align: left;">meta-llama/Llama-2-70b-chat-hf</td>
<td style="text-align: left;">llama2</td>
</tr>
<tr>
<td style="text-align: left;">Mistral 7B Base</td>
<td style="text-align: left;">mistralai/Mistral-7B-v0.1</td>
<td style="text-align: left;">apache-2.0</td>
</tr>
<tr>
<td style="text-align: left;">Mistral 7B Chat</td>
<td style="text-align: left;">mistralai/Mistral-7B-Instruct-v0.2</td>
<td style="text-align: left;">apache-2.0</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3 8B Chat</td>
<td style="text-align: left;">meta-llama/Meta-Llama-3-8B-Instruct</td>
<td style="text-align: left;">llama3</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3 70B Chat</td>
<td style="text-align: left;">meta-llama/Meta-Llama-3-70B-Instruct</td>
<td style="text-align: left;">llama3</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3.1 8B Chat</td>
<td style="text-align: left;">meta-llama/Meta-Llama-3.1-8B-Instruct</td>
<td style="text-align: left;">llama3.1</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3.1 70B Chat</td>
<td style="text-align: left;">meta-llama/Meta-Llama-3.1-70B-Instruct</td>
<td style="text-align: left;">llama3.1</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM</td>
<td style="text-align: left;">BAAI/JudgeLM-7B-v1.0</td>
<td style="text-align: left;">Non-commercial license</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 Turbo</td>
<td style="text-align: left;">gpt-4-turbo-2024-04-09</td>
<td style="text-align: left;">N/A</td>
</tr>
</tbody>
</table>
<p>Table 3: Version and license details for the different models and datasets used in experiments.</p>
<h1>Prompt template for judge models</h1>
<p>Your task is to look at the following question, and based on the references provided, determine if the model's response is correct or incorrect. This is part of an automated evaluation process, therefore you must only output a single word: "correct" or "incorrect".</p>
<p>Question:
Which Australian did Roger Federer defeat to win his first Wimbledon Men's Singles title in 2003?</p>
<p>References:
MARK PHILIPPOUSSIS
MARK PHILIPPOUSSIS
Model Response:
Mark Philippoussis
Evaluation (correct/incorrect):</p>
<p>Figure 8: Prompt templates for the judge models</p>
<p>$$
\pi=\frac{p_{o}-p_{e}}{1-p_{e}}=\frac{\left(\frac{T P+T N}{T P+T N+F P+F N}\right)-\left(\frac{(T P+F P)(T P+F N)+(T N+F N)(T N+F P)}{(T P+T N+F P+F N)^{2}}\right)}{1-\left(\frac{(T P+F P)(T P+F N)+(T N+F N)(T N+F P)}{(T P+T N+F P+F N)^{2}}\right)}
$$</p>
<h1>G Human Annotation Guidelines</h1>
<h2>Human annotation guidelines</h2>
<p>You will be given a question, a set of reference answers and the answer given by an LLM. Your task is to judge if the answer given by the LLM is correct, as if you were the LLMs teacher grading their exam. An answer should be counted as correct if it is semantically equivalent to (one of the) reference answers. In doing so, please follow the following guidelines:</p>
<ul>
<li>Underspecified answers (e.g. "December" instead of "December 20") should be marked incorrect.</li>
<li>Answers that have more information than requested (e.g. "December 20, in Paris" instead of "December 20") should be marked correct, provided the extra information is not incorrect or contrasting the rest of the answer.</li>
<li>Answers with unnecessary verbosity but correct answers should be marked correct (E.g. "Thanks for asking this question! The correct answer is: ...").</li>
</ul>
<p>If you have trouble judging whether the answer is correct, for instance because you feel you are lacking knowledge required to judge so, please indicate so by marking the answer "maybe correct" or "maybe incorrect", so that we can further review it.</p>
<p>Preliminary research involved iterative refinement of human annotation guidelines to ensure consistency and reproducibility across annotators with general English semantic knowledge. CS graduate students served as annotators for this experiment. We provide the guidelines used for human evaluation below.</p>
<h2>H Experiment costs</h2>
<p>The costs for the different experiments described in this work belong in three categories - GPU-hours for running open-source models on one or more Nvidia A100 GPUs, OpenAI credits for making API calls to OpenAI models, ${ }^{5}$ and human hours for manual annotations of benchmark responses. The estimated costs for the final reported experiments are given in Appendix K. In addition to this, previous unreported experiments and trials had an approximate cost of 120 GPU-hours, 100 USD in OpenAI credits, and 50 human hours, bringing the total experimental cost for this work to approximately 200 GPU-hours, USD 125 OpenAI credits, and 75 human annotation hours.</p>
<h2>I Statistical reliability of Evaluation sample</h2>
<p>Due to computational constraints discussed in Appendix A and Appendix H, we limit our evaluation set to randomly sampled 400 questions from TriviaQA (Joshi et al., 2017). In this section, we further take 5 samples of 300 randomly selected questions from the evaluation set and calculate the mean and standard deviation of Scott's Pi. From Appendix I, it can be observed that even on down-sampled sets, the Scott's $\pi$ values are similar to Figure 1b. Standard deviation of all the judge models from the mean Scott's $\pi$ is also minimal, barring EM lexical match.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Judge Model</th>
<th style="text-align: center;">Mean Scott's $\pi$</th>
<th style="text-align: center;">Std Dev</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama3-70B</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.0046</td>
</tr>
<tr>
<td style="text-align: left;">Llama3.1-70B</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.0039</td>
</tr>
<tr>
<td style="text-align: left;">Llama3.1-8B</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.0050</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-13B</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.0043</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.0114</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.0108</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-7B</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.0026</td>
</tr>
<tr>
<td style="text-align: left;">Contains</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.0087</td>
</tr>
<tr>
<td style="text-align: left;">Llama3-8B</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.0126</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-7B</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.0112</td>
</tr>
<tr>
<td style="text-align: left;">EM</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.29</td>
</tr>
<tr>
<td style="text-align: left;">Gemma-2B</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.007</td>
</tr>
</tbody>
</table>
<p>Table 4: Weak Scott's $\pi$ variation for the 5 downsampled sets indicating robustness for the evaluation sample</p>
<h2>J Judge Scores</h2>
<p>We show the scores assigned by each judge model to each exam-taker model, visualised in Figure 1a in Appendix K.</p>
<h2>K Exam-taker model base vs chat analysis</h2>
<p>Given the human judgments we have available, we take the opportunity to investigate the performance differences between base and their corresponding chat models. In Appendix K, we show the scores assigned by various judge models to four base-chat pairs. According to the default metric EM, the base models outperform the chat models by a large margin. Interestingly, while this difference gets smaller when the answers are judged by humans (second column) or GPT-4 Turbo, there is still a substantial difference for all four pairs, suggesting that the difference is not merely an effect of the increased verbosity of the chat models. Further evidence for that hypothesis is provided by Figure 9b, in which we can see that while $14 \%$ of the errors are shared between the base-chat pairs, almost another $14 \%$ of the examples get judged correctly by the base models but not by the chat models, while the opposite happens in only $2.5 \%$ of the cases.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Experiment</th>
<th style="text-align: center;">GPU-hours</th>
<th style="text-align: center;">OpenAI credits</th>
<th style="text-align: center;">Human hours</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Main benchmarks</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Main evaluations</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">Human alignment</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: left;">Error analysis</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">Controlled responses</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Leniency bias</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Guideline bias</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Reference bias</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">$\mathbf{7 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{2 4}$</td>
<td style="text-align: center;">$\mathbf{2 6}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Estimated costs for the final reported experiments. GPU-hours are in equivalent Nvidia A100 hours, OpenAI credits are in USD, and human hours are time spent in manual annotation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Judge Models</th>
<th style="text-align: center;">Exam taker models</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chat</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama 3.1 8B</td>
<td style="text-align: center;">65.25</td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;">83.50</td>
<td style="text-align: center;">60.25</td>
<td style="text-align: center;">70.50</td>
<td style="text-align: center;">75.50</td>
<td style="text-align: center;">73.75</td>
<td style="text-align: center;">59.00</td>
<td style="text-align: center;">89.00</td>
</tr>
<tr>
<td style="text-align: center;">Llama 3.1 70B</td>
<td style="text-align: center;">62.00</td>
<td style="text-align: center;">74.25</td>
<td style="text-align: center;">85.00</td>
<td style="text-align: center;">55.50</td>
<td style="text-align: center;">64.75</td>
<td style="text-align: center;">74.00</td>
<td style="text-align: center;">72.25</td>
<td style="text-align: center;">60.50</td>
<td style="text-align: center;">92.25</td>
</tr>
<tr>
<td style="text-align: center;">Llama 3 8B</td>
<td style="text-align: center;">76.00</td>
<td style="text-align: center;">83.25</td>
<td style="text-align: center;">91.50</td>
<td style="text-align: center;">73.25</td>
<td style="text-align: center;">82.75</td>
<td style="text-align: center;">85.25</td>
<td style="text-align: center;">81.75</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">97.25</td>
</tr>
<tr>
<td style="text-align: center;">Llama 3 70B</td>
<td style="text-align: center;">64.25</td>
<td style="text-align: center;">75.50</td>
<td style="text-align: center;">86.50</td>
<td style="text-align: center;">57.00</td>
<td style="text-align: center;">64.00</td>
<td style="text-align: center;">75.75</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">62.50</td>
<td style="text-align: center;">92.75</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2 7B</td>
<td style="text-align: center;">80.50</td>
<td style="text-align: center;">85.25</td>
<td style="text-align: center;">92.00</td>
<td style="text-align: center;">80.50</td>
<td style="text-align: center;">70.75</td>
<td style="text-align: center;">90.75</td>
<td style="text-align: center;">84.00</td>
<td style="text-align: center;">83.25</td>
<td style="text-align: center;">97.75</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2 13B</td>
<td style="text-align: center;">68.25</td>
<td style="text-align: center;">75.50</td>
<td style="text-align: center;">86.50</td>
<td style="text-align: center;">63.25</td>
<td style="text-align: center;">62.75</td>
<td style="text-align: center;">77.50</td>
<td style="text-align: center;">74.50</td>
<td style="text-align: center;">67.50</td>
<td style="text-align: center;">93.5</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2 70B</td>
<td style="text-align: center;">71.25</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">90.25</td>
<td style="text-align: center;">67.50</td>
<td style="text-align: center;">74.75</td>
<td style="text-align: center;">81.25</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">96.75</td>
</tr>
<tr>
<td style="text-align: center;">Mistral 7B</td>
<td style="text-align: center;">72.50</td>
<td style="text-align: center;">80.75</td>
<td style="text-align: center;">90.50</td>
<td style="text-align: center;">69.00</td>
<td style="text-align: center;">74.75</td>
<td style="text-align: center;">82.50</td>
<td style="text-align: center;">80.25</td>
<td style="text-align: center;">72.00</td>
<td style="text-align: center;">96.25</td>
</tr>
<tr>
<td style="text-align: center;">Gemma 2B</td>
<td style="text-align: center;">79.75</td>
<td style="text-align: center;">87.00</td>
<td style="text-align: center;">91.25</td>
<td style="text-align: center;">58.50</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">68.50</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">55.75</td>
<td style="text-align: center;">80.50</td>
</tr>
<tr>
<td style="text-align: center;">JudgeLM</td>
<td style="text-align: center;">69.50</td>
<td style="text-align: center;">77.75</td>
<td style="text-align: center;">86.25</td>
<td style="text-align: center;">63.75</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">82.75</td>
<td style="text-align: center;">77.25</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">94.50</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">60.50</td>
<td style="text-align: center;">71.50</td>
<td style="text-align: center;">82.50</td>
<td style="text-align: center;">54.50</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">69.75</td>
<td style="text-align: center;">56.50</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: center;">Exact Match</td>
<td style="text-align: center;">46.75</td>
<td style="text-align: center;">56.00</td>
<td style="text-align: center;">63.75</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">36.25</td>
<td style="text-align: center;">59.50</td>
<td style="text-align: center;">20.25</td>
<td style="text-align: center;">58.25</td>
</tr>
<tr>
<td style="text-align: center;">Contains Match</td>
<td style="text-align: center;">50.75</td>
<td style="text-align: center;">60.00</td>
<td style="text-align: center;">68.00</td>
<td style="text-align: center;">39.00</td>
<td style="text-align: center;">46.25</td>
<td style="text-align: center;">59.50</td>
<td style="text-align: center;">57.25</td>
<td style="text-align: center;">44.00</td>
<td style="text-align: center;">70.00</td>
</tr>
<tr>
<td style="text-align: center;">Human Eval</td>
<td style="text-align: center;">62.50</td>
<td style="text-align: center;">72.75</td>
<td style="text-align: center;">83.75</td>
<td style="text-align: center;">56.00</td>
<td style="text-align: center;">56.50</td>
<td style="text-align: center;">72.25</td>
<td style="text-align: center;">71.75</td>
<td style="text-align: center;">60.75</td>
<td style="text-align: center;">91.50</td>
</tr>
</tbody>
</table>
<p>Table 6: Judge model score card for every exam-taker model.</p>
<p>We consider two alternative hypotheses:
i) The chat models have a worse understanding of the particular prompt format, which is tuned more to fit base models; or
ii) The chat models have 'unlearned' some knowledge during their alignment training.</p>
<p>To disentangle these two factors, we manually analyse 400 questions for Llama-2 70B and Llama-2 70B-chat, using our earlier error codes. The results, shown in Figure 9a, sugest that, at least to some extent, the difference between base and chat models is in fact due to 'unlearning' of knowledge: while the number of errors is more or less equal among most categories, there is a stark difference in the incorrect entity category. Substantially more often than the base models, the chat models do answer the question with a semantically plausible but incorrect entity. In Appendix MAppendix M, we provide examples of such cases. The results do not show any evidence to support the first hypothesis: the number of errors where the answer cannot be parsed or is just entirely incorrect does not differ between base and chat models.</p>
<h2>L Exam-taker model ranking correlation</h2>
<p>In Appendix L, We use the Spearman Rank correlation coefficient (Spearman, 1904) to assess the rankings of the exam-taker models. To validate these rankings, we randomly select 6 out of 9 examtaker models across 5 samples, subsequently calculating the mean $(\rho)$ and standard deviation $(\sigma)$ of the rankings. The results reveal that the contains model exhibits the highest stability and $\rho$ among the rankings, while the majority of judge models achieve a coefficient exceeding 0.7 , indicating a strong alignment. Notably, smaller models such
as Mistral 7B perform on par with GPT-4 Turbo, highlighting the robustness of smaller models in maintaining rankings.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Judges</th>
<th style="text-align: center;">$\rho$</th>
<th style="text-align: center;">$\sigma$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Contains</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-13B</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.18</td>
</tr>
<tr>
<td style="text-align: left;">JudgeLM-7B</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-7B</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: left;">Llama3.1-70B</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.07</td>
</tr>
<tr>
<td style="text-align: left;">Llama3-70B</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: left;">Llama3.1-8B</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.10</td>
</tr>
<tr>
<td style="text-align: left;">Llama3-8B</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.07</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.13</td>
</tr>
<tr>
<td style="text-align: left;">Gemma-2B</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: left;">EM</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.13</td>
</tr>
</tbody>
</table>
<p>Table 8: Spearman Rank Correlation Coefficient $\rho$.</p>
<h2>M Too much info confuses judges</h2>
<p>In Figure 10-13, we report the guidelines we used for the experiments in § 5.2. The simplest prompt used is Without Guidelines v1 (see Figure 10) where we define a sequential and structured process for the judge model. In Without Guidelines v2 (see Figure 11), we add an additional focus on the overall task and outcome as well. For Guidelines without examples (see Figure 12), we provide the judge models with detailed instructions about the task at hand, along with explicit guidelines on how to evaluate the answers. Additionally, for Guidelines with examples(see Figure 13), we also provide examples to the judge models for further reference.</p>
<p>Table 7: Scores of base and chat models by various judges</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Base-Chat pair</th>
<th style="text-align: center;">Judge models</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Contains</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 <br> Turbo</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-3 <br> 70B</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Chat</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Chat</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Chat</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Chat</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">Chat</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2 7B</td>
<td style="text-align: center;">46.75</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">50.75</td>
<td style="text-align: center;">39.00</td>
<td style="text-align: center;">62.25</td>
<td style="text-align: center;">56.00</td>
<td style="text-align: center;">60.50</td>
<td style="text-align: center;">54.50</td>
<td style="text-align: center;">64.25</td>
<td style="text-align: center;">57.00</td>
</tr>
<tr>
<td style="text-align: center;">Mistral 7B</td>
<td style="text-align: center;">59.50</td>
<td style="text-align: center;">20.25</td>
<td style="text-align: center;">57.25</td>
<td style="text-align: center;">44.00</td>
<td style="text-align: center;">71.75</td>
<td style="text-align: center;">60.75</td>
<td style="text-align: center;">69.75</td>
<td style="text-align: center;">56.50</td>
<td style="text-align: center;">73.50</td>
<td style="text-align: center;">62.50</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2 13B</td>
<td style="text-align: center;">56.00</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">60.00</td>
<td style="text-align: center;">46.25</td>
<td style="text-align: center;">72.75</td>
<td style="text-align: center;">56.50</td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;">59.00</td>
<td style="text-align: center;">76.50</td>
<td style="text-align: center;">64.00</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2 70B</td>
<td style="text-align: center;">63.75</td>
<td style="text-align: center;">36.25</td>
<td style="text-align: center;">68.00</td>
<td style="text-align: center;">59.50</td>
<td style="text-align: center;">83.75</td>
<td style="text-align: center;">72.25</td>
<td style="text-align: center;">82.50</td>
<td style="text-align: center;">73.00</td>
<td style="text-align: center;">86.50</td>
<td style="text-align: center;">75.75</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: a) Distribution of incorrect question counts by error codes for Llama2 70B Base vs Chat exam-taker models evaluated on 400 questions. b) Pie chart showing the percentage of questions categorized by the judgment from Base and Chat models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question: <br> Which British artist's works include 'The First Real Target'?</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">References</td>
<td style="text-align: left;">Peter Blake, Peter Balke, Sir Peter Blake</td>
</tr>
<tr>
<td style="text-align: left;">LLama-2 70B <br> Base</td>
<td style="text-align: left;">Peter Blake</td>
</tr>
<tr>
<td style="text-align: left;">LLama-2 70B <br> Chat</td>
<td style="text-align: left;">Patrick Caulfield</td>
</tr>
<tr>
<td style="text-align: left;">Mistral 7B <br> Base</td>
<td style="text-align: left;">David Hockney</td>
</tr>
<tr>
<td style="text-align: left;">Mistral 7B <br> Chat</td>
<td style="text-align: left;">Damien Hirst</td>
</tr>
</tbody>
</table>
<p>Table 9: Knowledge unlearning example 1.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question: <br> Who was the first cricketer to score 10,000 test runs?</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">References</td>
<td style="text-align: left;">Sunil Gavaskar, Sunil Manohar Gavaskar, SM Gavaskar, <br> Sunny gavaskar, Gavaskar</td>
</tr>
<tr>
<td style="text-align: left;">LLama-2 70B <br> Base</td>
<td style="text-align: left;">Sunil Gavaskar</td>
</tr>
<tr>
<td style="text-align: left;">LLama-2 70B <br> Chat</td>
<td style="text-align: left;">Sachin Tendulkar</td>
</tr>
<tr>
<td style="text-align: left;">Mistral 7B <br> Base</td>
<td style="text-align: left;">Sachin Tendulkar</td>
</tr>
<tr>
<td style="text-align: left;">Mistral 7B <br> Chat</td>
<td style="text-align: left;">Sachin Tendulkar was the first cricketer to score <br> 10,000 runs in Test matches.</td>
</tr>
</tbody>
</table>
<p>Table 10: Knowledge unlearning example 2</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Pricing details for OpenAI models are available at https: //openai.com/api/pricing/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ In an earlier version of this paper, we used Cohen's kappa (Cohen, 1960) to measure alignment. It has since come to our attention that - despite it's widespread use - this metric has some well-documented theoretical issues (e.g. Pontius and Millones, 2011; Chicco et al., 2021). For the interested reader, we elaborate on these issues in Appendix B.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
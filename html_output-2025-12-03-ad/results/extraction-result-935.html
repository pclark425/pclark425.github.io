<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-935 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-935</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-935</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-1819a53eddb4a5334937561bb57542d7f11c8308</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1819a53eddb4a5334937561bb57542d7f11c8308" target="_blank">What Are Tools Anyway? A Survey from the Language Model Perspective</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A unified definition of tools as external programs used by LMs is provided, a systematic review of LM tooling scenarios and approaches are performed, and some challenges and potential future research in the field are highlighted.</p>
                <p><strong>Paper Abstract:</strong> Language models (LMs) are powerful yet mostly for text generation tasks. Tools have substantially enhanced their performance for tasks that require complex skills. However, many works adopt the term"tool"in different ways, raising the question: What is a tool anyway? Subsequently, where and how do tools help LMs? In this survey, we provide a unified definition of tools as external programs used by LMs, and perform a systematic review of LM tooling scenarios and approaches. Grounded on this review, we empirically study the efficiency of various tooling methods by measuring their required compute and performance gains on various benchmarks, and highlight some challenges and potential future research in the field.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e935.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e935.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LMs with the ability to generate API/function calls by training on model-synthesized examples and using in-context examples at inference; evaluated across cloze, math, QA, multilingual, and temporal tasks showing variable gains depending on task type.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-J (ToolFormer experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer (GPT-J) used as base LM; augmented via a pipeline that synthesizes tool-use examples, trains the model to emit tool-call tokens, and uses in-context examples at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>QA (general question answering, as reported in ToolFormer experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Cloze / Math / Temporal tasks where tool calls (e.g., calculator, get_time, search) are used</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / computation / perception</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>+14.7 pp (cloze), +30.4 pp (math), +5.8 pp (QA), -0.2 pp (multilingual), +13.0 pp (temporal) (Δ performance vs no-tool baseline, as reported in the survey's Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>tool-call interface via special tokens/function-call expressions; switches between text-generation and tool-execution modes</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>training-time learning from model-synthesized examples + inference-time in-context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training-time intervention + prompting</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Synthesize examples where the LM emits tool-call tokens, fine-tune the LM to generate tool-calls and corresponding text, and at inference provide in-context examples/instructions so the model produces function-call expressions that are executed and their outputs injected back into generation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Task-dependent improvements: e.g., math +30.4 percentage points; cloze +14.7 pp; QA +5.8 pp (all Δ vs no-tool baseline as reported in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Survey explains that probabilistic LMs struggle at symbolic computation and accessing up-to-date or non-parametric information; augmenting with tools (calculator, get_time, search) directly supplies computation or perception abilities that LMs lack, hence large gains on computation/perception-heavy tasks but smaller or negative gains on tasks (e.g., multilingual) that LMs already handle well.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Are Tools Anyway? A Survey from the Language Model Perspective', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e935.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e935.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>API-Bank / Lynx</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>API-Bank / Lynx (trained tool-using model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>API-Bank is a benchmark of aggregated real-world APIs; Lynx is a 7B model (initialized from Alpaca 7B) fine-tuned to use APIs, demonstrating substantial gains on API-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Api-bank: A comprehensive benchmark for tool-augmented llms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Lynx (tool-using model initialized from Alpaca 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B parameter autoregressive LM (Alpaca-based) fine-tuned to call/use APIs in the API-Bank benchmark; integrates API metadata/examples at training to learn API usage.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>API-Bank (aggregated API tool usage benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use (API calling / tool-augmented QA)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>+24.4 pp (Δ performance vs Alpaca baseline on API-Bank as reported in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>tool-use interface with API calling capability; model fine-tuned to generate API call expressions and use returned outputs</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised fine-tuning (3 epochs on API-Bank training set)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training-time fine-tuning to use APIs</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Fine-tune a 7B LM (Alpaca) on API-Bank examples that require generating API calls and using API outputs as part of the final answer; baseline uses the un-fine-tuned Alpaca model.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improved overall correctness by +24.4 percentage points compared to the zero-shot Alpaca-7B baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>API-style tasks require non-parametric access and precise tool invocation, which base LMs lack; supervised fine-tuning on API usage teaches models to plan and emit API calls, closing the gap for API-centric tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Are Tools Anyway? A Survey from the Language Model Perspective', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e935.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e935.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolAlpaca</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolAlpaca: Generalized tool learning for language models with 3000 simulated cases</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset/method and fine-tuned models (ToolAlpaca-7B/13B) that train Vicuna models to use tools/APIs, yielding large performance improvements on aggregated API/tool benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolalpaca: Generalized tool learning for language models with 3000 simulated cases</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ToolAlpaca models (Vicuna-based ToolAlpaca-7B and ToolAlpaca-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-derived LMs (7B and 13B) fine-tuned on ToolAlpaca synthetic tool-usage examples to learn when/how to call APIs and use tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B (reported variants)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolAlpaca aggregated API/tool-use benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use (API calling / task automation)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>+45.2 pp (Δ performance on API-style benchmark vs baseline, Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>tool-call generation capability learned via fine-tuning; uses synthetic examples mapping NL intents to tool calls</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised fine-tuning on model-synthesized examples</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training-time fine-tuning with synthetic tool-usage examples</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Generate large-scale synthetic examples that pair user intents with API/tool call sequences and fine-tune Vicuna models to produce executable tool-calls as part of solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Large aggregate improvement (+45.2 percentage points) on API-style tasks compared to the baseline Vicuna models.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Many API tasks are infeasible for parameter-only LMs—explicitly training on tool-call generation and execution allows the model to leverage non-parametric tools, producing large gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Are Tools Anyway? A Survey from the Language Model Perspective', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e935.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e935.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chameleon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chameleon: Plug-and-play compositional reasoning with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that composes LLM reasoning with tool usage (plug-and-play) evaluated with GPT-4 on ScienceQA and TabMWP, reporting modest improvements when tools augment reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chameleon: Plug-and-play compositional reasoning with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4 (Chameleon evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large dialog-capable LM (GPT-4) used in a plug-and-play framework that composes LLM reasoning with tool invocations to solve compositional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>ScienceQA (multi-step QA)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>TabMWP (table math world problems) and ScienceQA</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / tool-assisted computation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>+2.6 pp (ScienceQA), +1.9 pp (TabMWP) (Δ vs baseline CoT/PoT baselines as reported in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>plug-and-play composition layer that instructs the LM to call tools or compose sub-solvers; integrates tool documentation to enable zero-shot tool usage</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>inference-time prompting with tool documentation (zero-shot/low-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting / compositional tool integration</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Provide tool documentation and compose LLM reasoning with tool calls at inference (plug-and-play) to tackle compositional QA and table math tasks without heavy fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Small absolute gains on evaluated datasets: +2.6 pp on ScienceQA and +1.9 pp on TabMWP (vs CoT/PoT baselines with GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Even strong LMs (GPT-4) obtain only modest gains because many reasoning tasks require precise programmatic computation or structured queries; tool integration helps but improvements depend on task alignment and how much the LM already models the required skill.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Are Tools Anyway? A Survey from the Language Model Perspective', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e935.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e935.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LATM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models as tool makers (LATM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool-making pipeline that uses LMs to build task-specific tools and then uses them to solve BigBench tasks; reported large gains on aggregate BigBench selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models as tool makers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LATM pipeline (uses large LMs to create and use tools)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Procedure where an LM produces code/tools for tasks (tool creation), verifies them, and then uses the created tools to solve examples; evaluated on BigBench tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>BigBench selected tasks (various reasoning/QA tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool-making followed by tool use on BigBench</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool making and subsequent tool use (multi-step/programmatic)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>+29.1 pp (Δ average accuracy across selected BigBench tasks vs CoT baseline, Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>tool creation module (LM-generated tools), verification/validation pipeline, then use of created tools in solving tasks</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>pipeline involving training/validation/inference stages for tool creation and use (LM-driven generation + verification)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>tool-making (procedural/tool-creation intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use LMs to write small tool programs (e.g., Python functions) for individual tasks, verify them, and then run solutions that call those tools instead of relying purely on the LM's parametric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported +29.1 percentage point improvement on selected BigBench tasks compared to a chain-of-thought baseline, indicating large gains from making and using specialized tools.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>LLMs may lack the deterministic procedural computation or domain-specific routines needed for some tasks; automatically creating and invoking precise tools supplies deterministic computation and domain logic the LM lacks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Are Tools Anyway? A Survey from the Language Model Perspective', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e935.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e935.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CREATOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CREATOR: Disentangling abstract and concrete reasonings of large language models through tool creation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A test-time tool creation pipeline where the LM builds, verifies, and uses tools per example (including rectification steps), evaluated on math and table tasks with small or mixed gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Creator: Disentangling abstract and concrete reasonings of large language models through tool creation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>CREATOR pipeline (LM-driven per-example tool creation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LM-based test-time pipeline that generates tools/programs for individual examples, verifies outputs, and iteratively rectifies solutions; evaluated on MATH and TabMWP.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MATH, TabMWP (tool-creation and use per example)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool creation / programmatic multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>+4.5 pp (MATH), +0.0 pp (table/TabMWP) (Δ vs PoT baseline, Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>per-example tool creation and verification loop; program synthesis + execution + rectification</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>primarily inference-time tool creation/verification pipeline (no heavy fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>test-time tool creation and verification</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>At test time, generate candidate tool/program for an example, execute and verify outputs, and iterate to rectify errors; uses program-of-thought style baselines for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Small or task-dependent effects: +4.5 pp on MATH, no improvement on the inspected table benchmark (TabMWP).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Per-example tool creation can be noisy and repetitive; creating robust reusable tools is challenging and may limit benefits versus handcrafted or aggregated tools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Are Tools Anyway? A Survey from the Language Model Perspective', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e935.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e935.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRAFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool-creation method that adds heuristic-based training to craft less repetitive, more reusable tools; evaluated on math and table tasks with moderate gains and high compute costs compared to more efficient methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Craft: Customizing llms by creating and retrieving from specialized toolsets</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>CRAFT pipeline (LM-based tool creation + retrieval + training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline that creates specialized tools via LMs, trains/heuristically optimizes tool creation to reduce repetition, and retrieves tools to solve new examples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MATH, TabMWP (tool creation + retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool making / multi-step reasoning / programmatic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>+13.2 pp (MATH), +17.2 pp (table) (Δ vs baseline as reported in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>tool creation + retrieval subsystem; heuristic-based training to craft reusable tools</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>training-time methods to bias tool creation toward reusability (includes significant token cost in training and inference)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>tool-making with heuristic training (hybrid training+inference)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Create many candidate tools, use heuristic supervision/refactoring to make tools less repetitive and more reusable, then retrieve and apply these tools to solve tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Moderate gains: +13.2 pp on MATH and +17.2 pp on table tasks, at higher training/inference compute cost compared to some alternatives (per Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Higher computational overhead and redundancy in naive tool creation can reduce efficiency; improving tool reusability yields better practical gains for multi-example distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Are Tools Anyway? A Survey from the Language Model Perspective', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e935.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e935.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TroVE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TroVE: Inducing verifiable and efficient toolboxes for solving programmatic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that induces reusable tools on-the-fly relying purely on execution signals (no heavy training), achieving strong accuracy gains on math and table programmatic tasks with low token cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Trove: Inducing verifiable and efficient toolboxes for solving programmatic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>TroVE pipeline (LM + execution-based tool induction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Inference-time method that induces and verifies small reusable tools using execution feedback and multi-candidate generation, optimizing for low token cost and reusability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>MATH, TabMWP (programmatic tasks solved via induced toolboxes)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool making + programmatic multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>+21.0 pp (MATH), +12.0 pp (table) (Δ vs primitive/PoT baselines, Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>execution-driven induction and verification; three-mode generation and multi-candidate sampling to produce reusable tools</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>primarily inference-time induction using execution feedback (no large additional training)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>inference-time tool induction (execution-signal-driven)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use lightweight generation strategies (three-mode generation, multi-candidate sampling) and execute candidates to verify correctness, inducing small reusable functions/toolboxes on the fly and reusing them across examples.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Substantial improvements with low compute: +21.0 pp on MATH and +12.0 pp on table tasks while incurring relatively few extra tokens per example (per Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>When precise programmatic computation is required, leveraging execution feedback to induce deterministic tools reduces reliance on the LM's parametric reasoning, closing a large portion of the performance gap efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Are Tools Anyway? A Survey from the Language Model Perspective', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>Toolalpaca: Generalized tool learning for language models with 3000 simulated cases <em>(Rating: 2)</em></li>
                <li>Api-bank: A comprehensive benchmark for tool-augmented llms <em>(Rating: 2)</em></li>
                <li>Chameleon: Plug-and-play compositional reasoning with large language models <em>(Rating: 2)</em></li>
                <li>Large language models as tool makers <em>(Rating: 2)</em></li>
                <li>Creator: Disentangling abstract and concrete reasonings of large language models through tool creation <em>(Rating: 2)</em></li>
                <li>Craft: Customizing llms by creating and retrieving from specialized toolsets <em>(Rating: 2)</em></li>
                <li>Trove: Inducing verifiable and efficient toolboxes for solving programmatic tasks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-935",
    "paper_id": "paper-1819a53eddb4a5334937561bb57542d7f11c8308",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "ToolFormer",
            "name_full": "Toolformer: Language models can teach themselves to use tools",
            "brief_description": "A method that augments LMs with the ability to generate API/function calls by training on model-synthesized examples and using in-context examples at inference; evaluated across cloze, math, QA, multilingual, and temporal tasks showing variable gains depending on task type.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-J (ToolFormer experiments)",
            "model_description": "Autoregressive transformer (GPT-J) used as base LM; augmented via a pipeline that synthesizes tool-use examples, trains the model to emit tool-call tokens, and uses in-context examples at inference.",
            "model_size": "6.7B",
            "qa_task_name": "QA (general question answering, as reported in ToolFormer experiments)",
            "qa_performance": null,
            "interactive_task_name": "Cloze / Math / Temporal tasks where tool calls (e.g., calculator, get_time, search) are used",
            "interactive_task_type": "tool use / computation / perception",
            "interactive_performance": "+14.7 pp (cloze), +30.4 pp (math), +5.8 pp (QA), -0.2 pp (multilingual), +13.0 pp (temporal) (Δ performance vs no-tool baseline, as reported in the survey's Table 3)",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "tool-call interface via special tokens/function-call expressions; switches between text-generation and tool-execution modes",
            "training_method": "training-time learning from model-synthesized examples + inference-time in-context prompting",
            "intervention_type": "training-time intervention + prompting",
            "intervention_description": "Synthesize examples where the LM emits tool-call tokens, fine-tune the LM to generate tool-calls and corresponding text, and at inference provide in-context examples/instructions so the model produces function-call expressions that are executed and their outputs injected back into generation.",
            "intervention_effect": "Task-dependent improvements: e.g., math +30.4 percentage points; cloze +14.7 pp; QA +5.8 pp (all Δ vs no-tool baseline as reported in Table 3).",
            "hypothesized_cause_of_gap": "Survey explains that probabilistic LMs struggle at symbolic computation and accessing up-to-date or non-parametric information; augmenting with tools (calculator, get_time, search) directly supplies computation or perception abilities that LMs lack, hence large gains on computation/perception-heavy tasks but smaller or negative gains on tasks (e.g., multilingual) that LMs already handle well.",
            "uuid": "e935.0",
            "source_info": {
                "paper_title": "What Are Tools Anyway? A Survey from the Language Model Perspective",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "API-Bank / Lynx",
            "name_full": "API-Bank / Lynx (trained tool-using model)",
            "brief_description": "API-Bank is a benchmark of aggregated real-world APIs; Lynx is a 7B model (initialized from Alpaca 7B) fine-tuned to use APIs, demonstrating substantial gains on API-style tasks.",
            "citation_title": "Api-bank: A comprehensive benchmark for tool-augmented llms",
            "mention_or_use": "use",
            "model_or_agent_name": "Lynx (tool-using model initialized from Alpaca 7B)",
            "model_description": "A 7B parameter autoregressive LM (Alpaca-based) fine-tuned to call/use APIs in the API-Bank benchmark; integrates API metadata/examples at training to learn API usage.",
            "model_size": "7B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "API-Bank (aggregated API tool usage benchmark)",
            "interactive_task_type": "tool use (API calling / tool-augmented QA)",
            "interactive_performance": "+24.4 pp (Δ performance vs Alpaca baseline on API-Bank as reported in Table 3)",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "tool-use interface with API calling capability; model fine-tuned to generate API call expressions and use returned outputs",
            "training_method": "supervised fine-tuning (3 epochs on API-Bank training set)",
            "intervention_type": "training-time fine-tuning to use APIs",
            "intervention_description": "Fine-tune a 7B LM (Alpaca) on API-Bank examples that require generating API calls and using API outputs as part of the final answer; baseline uses the un-fine-tuned Alpaca model.",
            "intervention_effect": "Improved overall correctness by +24.4 percentage points compared to the zero-shot Alpaca-7B baseline.",
            "hypothesized_cause_of_gap": "API-style tasks require non-parametric access and precise tool invocation, which base LMs lack; supervised fine-tuning on API usage teaches models to plan and emit API calls, closing the gap for API-centric tasks.",
            "uuid": "e935.1",
            "source_info": {
                "paper_title": "What Are Tools Anyway? A Survey from the Language Model Perspective",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ToolAlpaca",
            "name_full": "ToolAlpaca: Generalized tool learning for language models with 3000 simulated cases",
            "brief_description": "A dataset/method and fine-tuned models (ToolAlpaca-7B/13B) that train Vicuna models to use tools/APIs, yielding large performance improvements on aggregated API/tool benchmarks.",
            "citation_title": "Toolalpaca: Generalized tool learning for language models with 3000 simulated cases",
            "mention_or_use": "use",
            "model_or_agent_name": "ToolAlpaca models (Vicuna-based ToolAlpaca-7B and ToolAlpaca-13B)",
            "model_description": "Vicuna-derived LMs (7B and 13B) fine-tuned on ToolAlpaca synthetic tool-usage examples to learn when/how to call APIs and use tool outputs.",
            "model_size": "7B / 13B (reported variants)",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ToolAlpaca aggregated API/tool-use benchmark",
            "interactive_task_type": "tool use (API calling / task automation)",
            "interactive_performance": "+45.2 pp (Δ performance on API-style benchmark vs baseline, Table 3)",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "tool-call generation capability learned via fine-tuning; uses synthetic examples mapping NL intents to tool calls",
            "training_method": "supervised fine-tuning on model-synthesized examples",
            "intervention_type": "training-time fine-tuning with synthetic tool-usage examples",
            "intervention_description": "Generate large-scale synthetic examples that pair user intents with API/tool call sequences and fine-tune Vicuna models to produce executable tool-calls as part of solutions.",
            "intervention_effect": "Large aggregate improvement (+45.2 percentage points) on API-style tasks compared to the baseline Vicuna models.",
            "hypothesized_cause_of_gap": "Many API tasks are infeasible for parameter-only LMs—explicitly training on tool-call generation and execution allows the model to leverage non-parametric tools, producing large gains.",
            "uuid": "e935.2",
            "source_info": {
                "paper_title": "What Are Tools Anyway? A Survey from the Language Model Perspective",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Chameleon",
            "name_full": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "brief_description": "A method that composes LLM reasoning with tool usage (plug-and-play) evaluated with GPT-4 on ScienceQA and TabMWP, reporting modest improvements when tools augment reasoning.",
            "citation_title": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4 (Chameleon evaluations)",
            "model_description": "Closed-source large dialog-capable LM (GPT-4) used in a plug-and-play framework that composes LLM reasoning with tool invocations to solve compositional tasks.",
            "model_size": null,
            "qa_task_name": "ScienceQA (multi-step QA)",
            "qa_performance": null,
            "interactive_task_name": "TabMWP (table math world problems) and ScienceQA",
            "interactive_task_type": "multi-step reasoning / tool-assisted computation",
            "interactive_performance": "+2.6 pp (ScienceQA), +1.9 pp (TabMWP) (Δ vs baseline CoT/PoT baselines as reported in Table 3)",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "plug-and-play composition layer that instructs the LM to call tools or compose sub-solvers; integrates tool documentation to enable zero-shot tool usage",
            "training_method": "inference-time prompting with tool documentation (zero-shot/low-shot)",
            "intervention_type": "prompting / compositional tool integration",
            "intervention_description": "Provide tool documentation and compose LLM reasoning with tool calls at inference (plug-and-play) to tackle compositional QA and table math tasks without heavy fine-tuning.",
            "intervention_effect": "Small absolute gains on evaluated datasets: +2.6 pp on ScienceQA and +1.9 pp on TabMWP (vs CoT/PoT baselines with GPT-4).",
            "hypothesized_cause_of_gap": "Even strong LMs (GPT-4) obtain only modest gains because many reasoning tasks require precise programmatic computation or structured queries; tool integration helps but improvements depend on task alignment and how much the LM already models the required skill.",
            "uuid": "e935.3",
            "source_info": {
                "paper_title": "What Are Tools Anyway? A Survey from the Language Model Perspective",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LATM",
            "name_full": "Large language models as tool makers (LATM)",
            "brief_description": "A tool-making pipeline that uses LMs to build task-specific tools and then uses them to solve BigBench tasks; reported large gains on aggregate BigBench selection.",
            "citation_title": "Large language models as tool makers",
            "mention_or_use": "use",
            "model_or_agent_name": "LATM pipeline (uses large LMs to create and use tools)",
            "model_description": "Procedure where an LM produces code/tools for tasks (tool creation), verifies them, and then uses the created tools to solve examples; evaluated on BigBench tasks.",
            "model_size": null,
            "qa_task_name": "BigBench selected tasks (various reasoning/QA tasks)",
            "qa_performance": null,
            "interactive_task_name": "Tool-making followed by tool use on BigBench",
            "interactive_task_type": "tool making and subsequent tool use (multi-step/programmatic)",
            "interactive_performance": "+29.1 pp (Δ average accuracy across selected BigBench tasks vs CoT baseline, Table 3)",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "tool creation module (LM-generated tools), verification/validation pipeline, then use of created tools in solving tasks",
            "training_method": "pipeline involving training/validation/inference stages for tool creation and use (LM-driven generation + verification)",
            "intervention_type": "tool-making (procedural/tool-creation intervention)",
            "intervention_description": "Use LMs to write small tool programs (e.g., Python functions) for individual tasks, verify them, and then run solutions that call those tools instead of relying purely on the LM's parametric reasoning.",
            "intervention_effect": "Reported +29.1 percentage point improvement on selected BigBench tasks compared to a chain-of-thought baseline, indicating large gains from making and using specialized tools.",
            "hypothesized_cause_of_gap": "LLMs may lack the deterministic procedural computation or domain-specific routines needed for some tasks; automatically creating and invoking precise tools supplies deterministic computation and domain logic the LM lacks.",
            "uuid": "e935.4",
            "source_info": {
                "paper_title": "What Are Tools Anyway? A Survey from the Language Model Perspective",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "CREATOR",
            "name_full": "CREATOR: Disentangling abstract and concrete reasonings of large language models through tool creation",
            "brief_description": "A test-time tool creation pipeline where the LM builds, verifies, and uses tools per example (including rectification steps), evaluated on math and table tasks with small or mixed gains.",
            "citation_title": "Creator: Disentangling abstract and concrete reasonings of large language models through tool creation",
            "mention_or_use": "use",
            "model_or_agent_name": "CREATOR pipeline (LM-driven per-example tool creation)",
            "model_description": "LM-based test-time pipeline that generates tools/programs for individual examples, verifies outputs, and iteratively rectifies solutions; evaluated on MATH and TabMWP.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MATH, TabMWP (tool-creation and use per example)",
            "interactive_task_type": "tool creation / programmatic multi-step reasoning",
            "interactive_performance": "+4.5 pp (MATH), +0.0 pp (table/TabMWP) (Δ vs PoT baseline, Table 3)",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "per-example tool creation and verification loop; program synthesis + execution + rectification",
            "training_method": "primarily inference-time tool creation/verification pipeline (no heavy fine-tuning reported)",
            "intervention_type": "test-time tool creation and verification",
            "intervention_description": "At test time, generate candidate tool/program for an example, execute and verify outputs, and iterate to rectify errors; uses program-of-thought style baselines for comparison.",
            "intervention_effect": "Small or task-dependent effects: +4.5 pp on MATH, no improvement on the inspected table benchmark (TabMWP).",
            "hypothesized_cause_of_gap": "Per-example tool creation can be noisy and repetitive; creating robust reusable tools is challenging and may limit benefits versus handcrafted or aggregated tools.",
            "uuid": "e935.5",
            "source_info": {
                "paper_title": "What Are Tools Anyway? A Survey from the Language Model Perspective",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "CRAFT",
            "name_full": "CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets",
            "brief_description": "A tool-creation method that adds heuristic-based training to craft less repetitive, more reusable tools; evaluated on math and table tasks with moderate gains and high compute costs compared to more efficient methods.",
            "citation_title": "Craft: Customizing llms by creating and retrieving from specialized toolsets",
            "mention_or_use": "use",
            "model_or_agent_name": "CRAFT pipeline (LM-based tool creation + retrieval + training)",
            "model_description": "Pipeline that creates specialized tools via LMs, trains/heuristically optimizes tool creation to reduce repetition, and retrieves tools to solve new examples.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MATH, TabMWP (tool creation + retrieval)",
            "interactive_task_type": "tool making / multi-step reasoning / programmatic tasks",
            "interactive_performance": "+13.2 pp (MATH), +17.2 pp (table) (Δ vs baseline as reported in Table 3)",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "tool creation + retrieval subsystem; heuristic-based training to craft reusable tools",
            "training_method": "training-time methods to bias tool creation toward reusability (includes significant token cost in training and inference)",
            "intervention_type": "tool-making with heuristic training (hybrid training+inference)",
            "intervention_description": "Create many candidate tools, use heuristic supervision/refactoring to make tools less repetitive and more reusable, then retrieve and apply these tools to solve tasks.",
            "intervention_effect": "Moderate gains: +13.2 pp on MATH and +17.2 pp on table tasks, at higher training/inference compute cost compared to some alternatives (per Table 3).",
            "hypothesized_cause_of_gap": "Higher computational overhead and redundancy in naive tool creation can reduce efficiency; improving tool reusability yields better practical gains for multi-example distributions.",
            "uuid": "e935.6",
            "source_info": {
                "paper_title": "What Are Tools Anyway? A Survey from the Language Model Perspective",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "TroVE",
            "name_full": "TroVE: Inducing verifiable and efficient toolboxes for solving programmatic tasks",
            "brief_description": "A method that induces reusable tools on-the-fly relying purely on execution signals (no heavy training), achieving strong accuracy gains on math and table programmatic tasks with low token cost.",
            "citation_title": "Trove: Inducing verifiable and efficient toolboxes for solving programmatic tasks",
            "mention_or_use": "use",
            "model_or_agent_name": "TroVE pipeline (LM + execution-based tool induction)",
            "model_description": "Inference-time method that induces and verifies small reusable tools using execution feedback and multi-candidate generation, optimizing for low token cost and reusability.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "MATH, TabMWP (programmatic tasks solved via induced toolboxes)",
            "interactive_task_type": "tool making + programmatic multi-step reasoning",
            "interactive_performance": "+21.0 pp (MATH), +12.0 pp (table) (Δ vs primitive/PoT baselines, Table 3)",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "execution-driven induction and verification; three-mode generation and multi-candidate sampling to produce reusable tools",
            "training_method": "primarily inference-time induction using execution feedback (no large additional training)",
            "intervention_type": "inference-time tool induction (execution-signal-driven)",
            "intervention_description": "Use lightweight generation strategies (three-mode generation, multi-candidate sampling) and execute candidates to verify correctness, inducing small reusable functions/toolboxes on the fly and reusing them across examples.",
            "intervention_effect": "Substantial improvements with low compute: +21.0 pp on MATH and +12.0 pp on table tasks while incurring relatively few extra tokens per example (per Table 3).",
            "hypothesized_cause_of_gap": "When precise programmatic computation is required, leveraging execution feedback to induce deterministic tools reduces reliance on the LM's parametric reasoning, closing a large portion of the performance gap efficiently.",
            "uuid": "e935.7",
            "source_info": {
                "paper_title": "What Are Tools Anyway? A Survey from the Language Model Perspective",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2
        },
        {
            "paper_title": "Toolalpaca: Generalized tool learning for language models with 3000 simulated cases",
            "rating": 2
        },
        {
            "paper_title": "Api-bank: A comprehensive benchmark for tool-augmented llms",
            "rating": 2
        },
        {
            "paper_title": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models as tool makers",
            "rating": 2
        },
        {
            "paper_title": "Creator: Disentangling abstract and concrete reasonings of large language models through tool creation",
            "rating": 2
        },
        {
            "paper_title": "Craft: Customizing llms by creating and retrieving from specialized toolsets",
            "rating": 2
        },
        {
            "paper_title": "Trove: Inducing verifiable and efficient toolboxes for solving programmatic tasks",
            "rating": 2
        }
    ],
    "cost": 0.0206195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>What Are Tools Anyway? <br> A Survey from the Language Model Perspective</h1>
<p>Zora Zhiruo Wang<em> Zhoujun Cheng</em> ${ }^{<em>}$ Hao Zhu</em> Daniel Fried<em> Graham Neubig ${ }^{</em>}$<br><em>Carnegie Mellon University ${ }^{</em>}$ Shanghai Jiao Tong University</p>
<h4>Abstract</h4>
<p>Language models (LMs) are powerful yet mostly for text generation tasks. Tools have substantially enhanced their performance for tasks that require complex skills. However, many works adopt the term "tool" in different ways, raising the question: What is a tool anyway? Subsequently, where and how do tools help LMs? In this survey, we provide a unified definition of tools as external programs used by LMs, and perform a systematic review of LM tooling scenarios and approaches. Grounded on this review, we empirically study the efficiency of various tooling methods by measuring their required compute and performance gains on various benchmarks, and highlight some challenges and potential future research in the field.</p>
<h2>1 Introduction</h2>
<p>Language Models (LMs) have become increasingly effective in solving text-generation tasks, by taking in natural language (NL) instructions from users and outputting NL responses, such as answering the "What is the capital of the US?" with "Washington D.C.". However, LMs often struggle to perform tasks that require complex skills (e.g., math or complex reasoning), and are fundamentally unable to solve other tasks that require access to information not included in their training data (e.g., the current weather or date).</p>
<p>To solve this problem, researchers and practitioners are turning to LMs enhanced with tools, which help facilitate the task-solving process of LMs, or extend LMs with new abilities that the LM does not possess otherwise (Qin et al., 2023; Mialon et al., 2023). For example, a calculator tool may be used to facilitate mathematical calculations, or a get_time() tool could be used to obtain the current time, which is not available purely through the LM's parameters. Inspired by the tools used by humans (Shumaker et al., 2011), some works introduce application-specific software as tools, such as using a search engine to obtain knowl-
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of tools extending and facilitating LM task-solving.
edge (Lazaridou et al., 2022; Komeili et al., 2022), using a translator to process unknown languages (Schick et al., 2023), or using a SQL engine to query databases (Hao et al., 2023; Zhuang et al., 2023). With the development of numerous application programming interfaces (APIs) on the web, many works collect APIs as tools to access world data in real-time (Balog et al., 2016; Xu et al., 2023; Qin et al., 2024) via multiple modalities (Tang et al., 2023), even performing professional activities such as financial analysis (Li et al., 2023) and digital marketing (Huang et al., 2024). Instead of using black-box APIs with unseen implementations, other works use locally-crafted functions to query over structured tables (Wang et al., 2024a; Cao et al., 2023) or images (Surís et al., 2023), where the function tools can be created by human (Gupta \&amp; Kembhavi, 2022) or model experts (Wang et al., 2023a; Cai et al., 2023; Wang et al., 2024b).
However, despite this broad and burgeoning area of tool use in LMs, existing surveys only cover certain tool categories such as software (Mialon et al., 2023) or APIs (Qin et al., 2023).</p>
<p>In this paper, we (1) provide a unified view of tool use across a broad range of scenarios, (2) empirically analyze the cost efficiency of tooling methods, to give practical guidance on when and how one should use tools, and (3) offer concrete suggestions for evaluations.</p>
<p>We start with proposing a unified definition of tools and explain why tools help task-solving (§2). We first introduce the basic tool-use paradigm (§3) and study a variety of tool-using scenarios by enumerating which tools exist and to which tasks they apply (§4). Next, we study advanced approaches for complex tool usage and even make new tools if they are unavailable for the task (§5). We then summarize existing testbeds and evaluation metrics across LM tooling works, and highlight several missing aspects with concrete metric suggestions (§6). Lastly, grounding on our empirical analysis about when tools are effective, we identify the most efficient tooling approaches and the tasks that benefit most from tools (§7).</p>
<h1>2 Background</h1>
<h3>2.1 What are tools?</h3>
<p>Because LMs are products of the digital world, tools employed by LMs are often computer programs that are executable in corresponding environments, e.g., Python programs are executable in Python environments. Referring back to human-used tools, Shumaker et al. (2011) defines animal tool use as "the external employment of an unattached or manipulable attached environmental object to alter more efficiently the form, position, or condition of another object." Similar to this definition of physical tools, LM-used program tools should also be external to the employer (i.e., the LM) and are part of the environment. In the meantime, instead of arbitrary program snippets, a tool is a function (e.g., plus_one), meaning that it can be applied to other objects (e.g., data) and yield an output (e.g. plus_one(1) $\rightarrow 2$ ).
Existing definitions of LM-used tools touch on some of these aspects. Qin et al. (2023) make an intuitive appeal to the similarity to human tool use, but do not define what entails a tool. Mialon et al. (2023) define a tool as "an external module that is typically called using a rule or a special token and whose output is included in the augmented LM's context." We argue for a somewhat more broad definition than this, which encompasses a wide variety of more recent works on tool usage:
Definition 1. An LM-used tool is a function interface to a computer program that runs externally to the $L M$, where the $L M$ generates the function calls and input arguments in order to use the tool. ${ }^{1}$</p>
<h3>2.2 Why are tools helpful?</h3>
<p>Tools can help task-solving in different ways, depending on the functionality of individual tools. We summarize their functions into three major categories: perception, action, and computation. A tool may belong to one or more of these three categories.
Perception Perception tools provide or collect information from the environment. An example is using a get_time() API to obtain the current time, which is not included in the LM's parametric knowledge learned from training.
Action Action tools can exert actions on the environment and change its state. For example, turn_left() can shift the direction of an embodied agent, or executing make_post (website, post) can change the content on a website.
Computation Computation tools do not necessarily perceive or modify the external environment, but use programs to tackle complex computational tasks. For example, a calculator is a computation tool for mathematical calculation. Note that the computation also includes more general acts of computing beyond numerical calculation. Therefore, a translator is also a computation tool that can be used to translate between languages.
Note that many tools can fall into multiple categories. For instance, a search engine is a tool that can perform both computation and perception. As computation, it measures</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>document similarity and selects relevant ones, but it also perceives the environment (i.e., the web) and fetches data (i.e., returned documents) from it. In a similar spirit, SQL queries can be used as computation tools (e.g., SELECT SQRT(16) / 10 kS result), perception tools for viewing data (e.g., SELECT name FROM data), action tools to modify data (e.g., INSERT INTO data VALUES name), or all of the above (e.g., INSERT INTO counts (grp_id, grp_cnt) SELECT grp_id, COUNT(*) FROM data GROUP BY grp_id).</p>
<h1>2.3 Tools and "Agents"</h1>
<p>There has recently been a burgeoning of work on LM-powered agents (Xi et al., 2023; Sumers et al., 2024). Russell \&amp; Norvig (2010) define agents as "anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators." According to this definition, agents are programs that use perception tools to perceive the situated environment, or action tools to interact with the environment. Models that only use computation tools and do not interact with their environments through perception or action tools arguably do not fall under the category of "agents" according to this definition.</p>
<h2>3 The basic tool use paradigm</h2>
<p>First, in this section, we show an illustrative example of a basic tool-use paradigm introduced by Toolformer (Schick et al., 2023), which many toolrelated works adopt (Figure 2). Assuming an LM communicates with users mainly in natural language, upon receiving a user query such as "How is the weather today?", the LM then proceeds to generate either text or tool calls. In the example, starts with generating a few tokens of text "It is ...". When the LM needs to seek external tools to complete the task, e.g., get real-time weather information, it generates tokens of the tool name and corresponding input arguments enclosed with (parentheses) to construct a complete tool calling expression. This completed expression will trigger a shift from text-generation mode to tool-execution mode. The server hosting the tool will execute the expression and return the execution result to the LM.</p>
<p>Taking the example in Figure 2, the LM sends the check_weather() call to the weather server and receives the output "sunny". The returned result replaces the tool call in the LM-generated tokens (e.g., from "It is check_weather ()" to "It is sunny"), which is used for subsequent steps of generation. Accordingly, the LM shifts back to the text generation mode and continues to finish the response by generating new text tokens, e.g., adding 'today.', and finally returning the response to the user.
In order for LMs to use this basic paradigm of using tools, current works mainly leverage inference-time prompting and training-time learning methods.
Inference-time prompting Leveraging the ability of LMs to learn in-context (Brown et al., 2020), many works provide tool information through a prompt and expect LMs to acquire abilities to use these tools from input contexts. This is achieved by providing instructions about the task, example pairs of queries and solutions that use tools (Gupta \&amp; Kembhavi, 2022; Lu et al., 2023a; Paranjape et al., 2023; Shen et al., 2023a; Yang et al., 2023), and/or documentation of the tools' functionality (Hsieh et al., 2023).
Learning by training Beyond learning tools from test-time contexts, LMs can learn from examples that use these tools during training. LMs can simply be trained to generate toolusing solutions, where the examples can be manually annotated by humans (Li et al., 2023), synthesized by larger teacher LMs (Tang et al., 2023; Qin et al., 2024; Huang et al., 2024), or bootstrapped by the test-time LM itself (Schick et al., 2023).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Example Tools</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">园 Knowledge access</td>
<td style="text-align: center;">sql_executor(query: str) -&gt; answer: any <br> search_engine(query: str) -&gt; document: str <br> retriever(query: str) -&gt; document: str</td>
</tr>
<tr>
<td style="text-align: center;">(i) Computation activities</td>
<td style="text-align: center;">calculator(formula: str) -&gt; value: int | float python_interpreter(program: str) -&gt; result: any worksheet.insert_row(row: list, index: int) -&gt; None</td>
</tr>
<tr>
<td style="text-align: center;">(3) Interaction w/ the world</td>
<td style="text-align: center;">get_weather(city_name: str) -&gt; weather: str get_location(ip: str) -&gt; location: str calendar.fetch_events(date: str) -&gt; events: list email.verify(address: str) -&gt; result: bool</td>
</tr>
<tr>
<td style="text-align: center;">目 Non-textual modalities</td>
<td style="text-align: center;">cat_image.delete(image_id: str) -&gt; None <br> spotify.play_music(name: str) -&gt; None <br> visual_qa (query: str, image: Image) -&gt; answer: str</td>
</tr>
<tr>
<td style="text-align: center;">(1) Special-skilled LMs</td>
<td style="text-align: center;">QA(question: str) -&gt; answer: str <br> translation(text: str, language: str) -&gt; text: str</td>
</tr>
</tbody>
</table>
<p>Table 1: Exemplar tools for each category.</p>
<h1>4 Scenarios where tools are useful</h1>
<p>While LMs may easily learn to do many tasks to high accuracy without tools, many other tasks greatly benefit from tool use. In this section, we study a broad range of scenarios where tools have been used to assist agents. We discuss tasks where human-created, applicationspecific tools can improve their performance or other positive aspects (§4.1), as well as scenarios where tools may not be as useful (§4.2).</p>
<h3>4.1 Utilizing existing tools for specific applications</h3>
<p>While it is difficult to exhaustively enumerate every scenario where tools could be useful, we summarize some major categories of tool use in Table 1 and below. Note that a tool may fall into one or more categories.</p>
<p>园 Knowledge access LMs store limited knowledge during training due to both limits in (i) the data that they are trained on and (ii) the ability of LMs to accurately memorize and utilize all of the data that they see at training time. Several varieties of tools can be used to alleviate this issue. SQL and SPARL executors can provide access to data in structured knowledge bases (Thoppilan et al., 2022; Parisi et al., 2022; Hao et al., 2023) or knowledge graphs (Zhuang et al., 2023). An search engine tool over the Internet (Yao et al., 2023; Schick et al., 2023; Paranjape et al., 2023) can enable LMs to access more up-to-date information (Komeili et al., 2022; Lazaridou et al., 2022). More generally, retrieval-augmented generation systems (Asai et al., 2023) can be seen as using a retriever tool (Mialon et al., 2023).
(i) Computation activities Complex computing activities such as math calculations are known to be challenging for neural LMs (Schick et al., 2023). While even a calculator can enhance LMs' numeracy abilities (Parisi et al., 2022; Hao et al., 2023), more generic Python programs are also employed to aid reasoning tasks (Gao et al., 2023b; Chen et al., 2023; Wang et al., 2023b). For more complex professional jobs, business tools are also applied, such as using worksheet to manipulate Google Sheets ( Xu et al., 2023), or even tools for financial, medical, education, or advertising domains (Tang et al., 2023; Huang et al., 2024).
(3) Interaction with the world LMs without tools are fundamentally unable to perceive and act in the world around them, necessitating tool use where such perception and action is necessary. For instance, LMs can access real-time information such as weather (Xu et al., 2023; Tang et al., 2023), or positional knowledge such as location (Qin et al., 2024). On the other hand, LMs can manipulate real-world information such as managing calendars (Schick et al., 2023) and emails (Qin et al., 2024). In addition to web-based activities, LMs can engage in physical activities in embodied environments, such as fishing with rods or mining with axes in the Minecraft world (Wang et al., 2023a); further propagate to the real-world</p>
<p>tasks to perform cooking (Singh et al., 2022; Shridhar et al., 2020), plotting (Liang et al., 2023), and even conducting chemical research (Boiko et al., 2023).</p>
<p>Non-textual modalities While many LMs only consume and generate texts, some works bring in access to visual (Gupta \&amp; Kembhavi, 2022; Surís et al., 2023), audio (Yang et al., 2023; Gao et al., 2023a), or other modalities. For example, LMs can access images with cat_image APIs (Xu et al., 2023; Tang et al., 2023) or songs (Huang et al., 2024) provided by spotify, even answer questions about them (Gupta \&amp; Kembhavi, 2022; Gao et al., 2023a).</p>
<p>Accessing specialized LMs Some works propose to use specialized LMs as tools, essentially using the main LM as a task planner to dispatch requests to other LMs. Schick et al. (2023) propose QA models to fill in factoid details in responses, Thoppilan et al. (2022); Schick et al. (2023); Paranjape et al. (2023) use machine translation models to assist multilingual tasks. Beyond specific tasks, some works adopt multiple neural models from Hugginface or similar platforms (Patil et al., 2023; Shen et al., 2023a), or further fine-tune them on various data (Viswanathan et al., 2023). Compared to the base LM, these tool models mainly vary in their specialized skills, and may or may not have substantial architectural differences from the base LMs.</p>
<h1>4.2 Where are tools not useful?</h1>
<p>Despite the fact that tools can be helpful under many scenarios discussed above, it is also important to note scenarios where tools are arguably not very helpful. Some examples of tasks where tools have not (yet) been used to great effect include machine translation, summarization, and sentiment analysis (among others). These are tasks that are not easy to perform using non-ML methods (c.f. solving math problems or accessing databases, which can be done using a calculator or SQL), and can be performed with high accuracy by a powerful LM alone. One intuitive reason is that the tools currently leveraged for these tasks are neural networks and have limited advantages over the base LM. Imagine if we leverage tools on these tasks, the tools would mostly generally be another neural LM with specialized skills, e.g., an LM specifically trained on many summarization datasets to perform this task. However, this special-skilled neural LM may not have significant architectural differences from the base tool-using LM, or be smaller in size or training tokens hence having inferior language modeling abilities in general. In comparison, the base LM capable of solution planning and tool management, usually are more powerful (e.g., GPT-4) and can achieve reasonable performance on a wide variety of tasks, perhaps even outperforming special-purpose LMs (Robinson et al., 2023).</p>
<h2>5 Advanced tool-use methods</h2>
<p>Given this understanding of the basic tooling paradigm and the scenarios in which tools are useful, we now discuss more advanced approaches for tools. Concretely, we study multi-tool selection and usage ( $\S 5.1$ ), complex tooling under programmatic contexts ( $\S 5.2$ ), and creation of tools when they are not available a-priori (§5.3).</p>
<h3>5.1 Complex tool selection and usage</h3>
<p>Depending on the number of tools available, the system may include an implicit or explicit tool selection module. If tools are already designated for the task (Lazaridou et al., 2022; Thoppilan et al., 2022), then no tool selection is needed. If a small number (e.g., 5-10) of tools are available, metadata and use cases of these tools can be provided as input contexts along with the user query (Schick et al., 2023; Paranjape et al., 2023), and LMs can directly select tools from contexts via a standard generation process. If the toolbox size further grows (e.g., to hundreds), fitting all tools into model inputs is not feasible. Thus an extra retrieval step is often incorporated: a retriever model short-lists the most relevant tools and feeds their metadata to the solution-generation LM. Specifically, Zhou et al. (2023); Qin et al. (2024) train retriever models that map NL intents to tool documentation. Yuan et al. (2023) ask LMs to write hypothetical descriptions and use the SimCSE retriever (Gao et al., 2021) to</p>
<p>find similar tools. More easily, one can directly use off-the-shelf embeddings (Meng et al., 2024; OpenAI) or training-free sparse retrievers (Robertson et al., 2009).
For complex queries that require multiple tools to solve, the common approach so far is to break down the task and tackle each step sequentially (Paranjape et al., 2023) by selecting and using tools with intermediate contexts. However, this sequential multi-turn paradigm may not be reflective of more complex or realistic usage of the involved tools. For example, a user may prefer nested function calls check weather (get_local_time('Pittsburgh')) to allow information hiding or encapsulation (Rogers, 2001), parallel calls to reduce round trips with the API (Eleti et al., 2023), or iterative calls buy_ticket (event) in a loop until it returns True to indicate a successful transaction.</p>
<h1>5.2 Tools in programmatic contexts</h1>
<p>Unlike text-based tasks where tools are auxiliary modules to extend LM abilities, on programmatic tasks, where code LMs can solve the problem by generating programs, tools can be seen as compositions of basic functions. In this part, we discuss tools in programmatic tasks for domain-specific (§5.2.1) and general-purpose problems (§5.2.2).
Focus on varied tools Depending on the tasks of interest, existing works focus on different types of tools under programmatic contexts. With the increasing complexity of these tools and presumably a decreasing familiarity of LMs about them, there are works that adopt (i) built-in functions of a programming language (PL) to augment LMs in symbolic reasoning, (ii) external libraries in pre-designed packages to tackle complex open-domain coding queries (Wang et al., 2023c), and (iii) utility functions unseen at training time to solve specific tasks.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Relative to what is considered as the base LM or base actions, tools can refer to built-in functions, external libraries, or task-specific utility functions (from left to right).</p>
<h3>5.2.1 Domain-specific semantic parsing</h3>
<p>NL-to-code generation systems have been studied for many years on special-domain tasks such as querying databases (Zelle \&amp; Mooney, 1996; Zettlemoyer \&amp; Collins, 2012) or knowledge graphs (Berant et al., 2013). Code produced by these systems is often domain-specific logical forms (DSL) manually designed by experts, such as lambda expressions (Liang, 2013) or SQL queries (Yu et al., 2018), and more recently, the QDMR grammar (Wolfson et al., 2020) as an extension to SQL. In addition to knowledge-oriented tasks, many agentic tasks adopt DSL to operate in corresponding environments, such as click or type in web navigation (Liu et al., 2018; Yao et al., 2022; Zhou et al., 2024), placeItem and killMob in the embodied Minecraft world (Wang et al., 2023a), or set_joint_target for robot dogs (Yu et al., 2023). Because DSLs are often specific enough to the target problems, most works directly use these built-in actions. Yet still, for complex task queries, solution programs written in basic DSL actions alone can be hard to interpret or cumbersome to use, e.g., it is hard to tell that the lambda expression (fold xs $(\lambda(\mathrm{n} x)(+1 \mathrm{n})) 0$ ) is to calculate the length of xs.</p>
<h3>5.2.2 General-purpose code generation</h3>
<p>Recent code generation systems have expanded from using DSL to more general-purpose PLs such as Python or Java (Yin \&amp; Neubig, 2017; Chen et al., 2021). These languages enable more programming flexibility and readily apply to versatile scenarios. As we have</p>
<p>introduced using built-in actions as tools in $\S 4.1$, we discuss more on two other common categories of tools for code LMs, namely external libraries and task-specific utility functions.
External libraries From the usage of PLs, built-in functions are internal to whichever environment, whereas third-party libraries lie externally and need to be imported to tackle specific contexts, such as Figure 3 (middle). Aligning with this conception, Zhang et al. (2023) use Python libraries such as matplotlib to plot figures and pandas to manage data.</p>
<p>Utility functions For more task-specific applications, expert-crafted utility functions, usually unseen at training time, are incorporated as tools. E.g., in Figure 3 (right), the highlighted locate_objects function is designed by human experts (Gupta \&amp; Kembhavi, 2022; Surís et al., 2023) to load neural models and perform post-processing to obtain the detected box region. In a similar spirit, Cheng et al. (2023) use GPT as a tool to query world facts external to the tabular contents, Cao et al. (2023) further design macro operation APIs to support advanced tabular operations. However, because human tool curation requires expertise and effort, some works explore using LMs to automatically create tools instead.</p>
<h1>5.3 Tool creation and reuse</h1>
<p>While one can readily use tools for tasks equipped with pre-designed tools, for tasks that do not have readily-applicable, human-created tools, some works explore using LMs to make tools and use them.</p>
<p>Domain-specific library abstraction Works that use DSLs often compose frequently-used-together actions as shortcut tools. For example, Ellis et al. (2023) learn function abstractions such as length and count_to from lambda primitives (e.g., $0,+$ ) for the list processing task. Wong et al. (2021); Bowers et al. (2023) similarly build functions bottom-up from a large corpus of DSL programs. More recently, Grand et al. (2023) use LLMs to abstract libraries with autodocumentation. Further for agentic tasks, Liu et al. (2018) learn common workflows to guide web navigation, such as composing the basic {click, like} actions to form a higher-level login action click(like('login'))..
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>General-purpose tool making Nonetheless, on general-purpose PLs, running the DSLoriented methods above may expand their search space and limit their scalability. Instead, recent works often leverage LMs' procedural knowledge to alleviate the search issue. To start, Wang et al. (2023a) designs an automatic learning curriculum in Minecraft to make and use Java program tools. LATM (Cai et al., 2023) use LMs to build, verify, and use Python tools on BigBench (bench authors, 2023) tasks, where however, all examples require the same single tool hence have limited difficulty. CREATOR (Qian et al., 2023) extend tool-making to harder tasks such as math and table world problems, and improves task success by creating tools yet repetitively for individual examples, thus CRAFT (Yuan et al., 2023) add heuristic-based training to craft less repetitive tools. Towards more efficient pipelines, ReGAL (Stengel-Eskin et al., 2024) learns from refactoring a smaller number of programs, while TroVE (Wang et al., 2024b) purely relies on inference-time execution signal and induces reusable tools on-the-fly.</p>
<h2>6 How to evaluate tool use?</h2>
<p>In this section, we study existing LM-tooling benchmarks (§6.1) and their evaluation metrics (§6.2), lastly, we discuss the missing yet important evaluation aspects of tools (§6.3).</p>
<h3>6.1 Testbeds for evaluating tools</h3>
<p>LM tool use can be evaluated on (i) repurposed existing datasets that can additionally benefit from tools (§6.1.1), and (ii) newly crafted benchmarks that necessitate tool use (§6.1.2).</p>
<h1>6.1.1 Repurposed Existing Datasets</h1>
<p>Many tasks are solvable by using LMs, yet often with great difficulty or inefficiency. Therefore, some works use tool-augmented LMs as an alternative approach to solve these tasks.
Many of these datasets require reasoning. Starting from when questions are expressed in NL, such as complex reasoning with the Big-bench (bench authors, 2023) dataset, mathematical problems with the MATH (Hendrycks et al., 2021) dataset, and reasoning over world knowledge to answer questions in NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) datasets. Beyond free-form texts, datasets that require reasoning over structured data can also benefit from tools. These tasks include table-based QA with tabular math world problems in TabMWP (Lu et al., 2023b), Wikipedia tables in WTQ (Pasupat \&amp; Liang, 2015), and complex-structured tables in HiTab (Cheng et al., 2022). Beyond the text modality, datasets that require reasoning over other modalities also benefit from modality-extending tools, e.g., answering questions about an image with the GQA (Hudson \&amp; Manning, 2019) dataset, or image pairs with the NLVR2 dataset (Suhr et al., 2019).
Because tool use is proposed as an alternative method to solve these datasets, evaluations of these tool-augmented systems follow the standard evaluation process for individual datasets. Concretely, almost all tasks are measured by answer exact match, either in textual or numerical formats. Note that, to obtain the final answers for lexical matching evaluations, all tool-calling expressions need to be executed, and the execution outputs are incorporated into the final answers produced by the tool-augmented systems, as introduced in $\S 3$.</p>
<h3>6.1.2 Aggregated API Benchmarks</h3>
<p>Existing benchmarks can only benefit from a limited set of tools, yet there are far more tools we can utilize to perform versatile tasks in the real world, particularly the API tools created by human developers spread on the web. Therefore, many recent works aggregate API tools from various web sources and create benchmarks for using these APIs, as shown in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: left;">Tool Source</th>
<th style="text-align: left;">Example Curation</th>
<th style="text-align: left;">Domain (\$4.1)</th>
<th style="text-align: left;">Executable</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ToolBench $_{1}$</td>
<td style="text-align: left;">existing dataset</td>
<td style="text-align: left;">adopted, human annotated</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ToolBench $_{2}$</td>
<td style="text-align: left;">RapidAPI</td>
<td style="text-align: left;">model synthesized</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ToolQA</td>
<td style="text-align: left;">existing dataset</td>
<td style="text-align: left;">model synthesized</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ToolAlpaca</td>
<td style="text-align: left;">PublicAPIs</td>
<td style="text-align: left;">model synthesized</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">API-Bank</td>
<td style="text-align: left;">PublicAPIs</td>
<td style="text-align: left;">human annotated</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MetaTool</td>
<td style="text-align: left;">OpenAI Plugins</td>
<td style="text-align: left;">model synthesized</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Gorilla</td>
<td style="text-align: left;">HF, Torch, TF</td>
<td style="text-align: left;">model synthesized</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">HuggingGPT</td>
<td style="text-align: left;">HF</td>
<td style="text-align: left;">human annotated</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Task Bench</td>
<td style="text-align: left;">HF, PublicAPIs</td>
<td style="text-align: left;">model synthesized</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Benchmarks of providing aggregated APIs to LMs as tools. HF is short for HuggingFace. ' $\boldsymbol{X}^{*}$ ' means that: though tools employed by HuggingGPT are executable, it does not evaluate the execution output due to the cost of hosting and inferencing.</p>
<p>Tool sources Tools are mainly aggregated from existing datasets or public APIs. While Xu et al. (2023); Zhuang et al. (2023) adopt existing datasets and propose alternative methods via tool augmentation, these benchmarks are limited in domains. Several works scrape more APIs from online sources such as Public APIs (Tang et al., 2023), RESTful APIs (Tang et al., 2023), or the OpenAI plugin list (Huang et al., 2024). Beyond human-crafted APIs (Li et al., 2023), neural models from ML platforms can be similarly presented in an API format (Patil et al., 2023; Shen et al., 2023a,b). Nonetheless, as tools are collected from heterogeneous sources, it is challenging to select the best benchmark or unify all these varied benchmarks.
Example curation Examples can be adopted from existing datasets, annotated by humans, or synthesized by LMs. While most examples adopted from existing datasets are human annotated (Xu et al., 2023), only Li et al. (2023) do so for scraped APIs, by surveying 500 people and creating 314 dialogues manually. Most other works prompt GPT models to</p>
<p>synthesize examples (Qin et al., 2024; Tang et al., 2023; Shen et al., 2023b; Zhuang et al., 2023; Huang et al., 2024), however, leading to issues of naturalness and executability.
First, LMs are often asked to create examples, even tool outputs in Tang et al. (2023), given a heuristically selected set of tools. This approach leads to potential issues in two-fold: (i) the selected tools may not be used together in practice, and (ii) the synthesized examples may not reflect the natural use cases of these tools. Second, 5 out of 9 benchmarks in Table 2 do not support tool execution, to alleviate the cost of hosting multiple APIs, especially when they may fail or produce unstable outputs. For example, the weather returned by the check_weather API may change over time. This un-executability causes issues in evaluation. Instead of matching final execution results using lexical- (Li et al., 2023) or neural-based metrics (Tang et al., 2023; Qin et al., 2024), works with unexecutable tools resort to pseudo matching of API calling expressions with lexical (Tang et al., 2023; Shen et al., 2023a; Huang et al., 2024) and syntactical (Patil et al., 2023; Shen et al., 2023b) means.</p>
<h1>6.2 What metrics are measured now?</h1>
<p>Task completion Tools are used to assist task solving. Most works that allow tool execution evaluate the task completion score to quantify the effectiveness of utilizing tools.
Tool selection For datasets with execution issues (Huang et al., 2024; Shen et al., 2023b), another common metric is the accuracy of selecting the correct tools. This helps disentangle incorrect tool selection errors from inaccurate tool usage errors. Despite that tool selection mainly serves as a proxy for evaluating task completion when having unexecutable tools, it can be seen as a measure of LM planning abilities - the process of breaking down a task into multiple steps and selecting tools to complete individual steps.
Tool reusability While tool reusability is often deemed important in took-making literature (Cai et al., 2023; Yuan et al., 2023), only Wang et al. (2024b) evaluates tool reusability by the size of induced toolboxes over a fixed number of examples. As its literal meaning, reusable tools can be (re)used to solve multiple examples hence having more generic functionalities. Adopting a reusable tool is more efficient than using multiple specific tools, and facilitates human verification in both speed and accuracy dimensions (Wang et al., 2024b).</p>
<h3>6.3 What properties are missing?</h3>
<p>Efficiency of tool integration As demonstrated by our empirical study (§7), the benefits brought by the tools come with the cost of additional computation, especially for teaching LMs to use tools via training or prompting. In addition to performance gain, reporting the computation overhead can enable fairer comparisons between different approaches.
Quality of tools While existing works mostly focus on how tools improve task accuracy, the performance of tools themselves is also important. Tool performance can cover multiple aspects such as completing the call quickly, requiring less computation, and not putting users at risk or failing unexpectedly. One way to measure these aspects is to conduct API testing (Yasar, 2022; Ehsan et al., 2022) on their runtime, memory usage, and success rate.
Reliability of unstable tools Particularly for tools that involve neural models or randomized components, their output quality may be unstable and unpredictable. For example, the VQA tool (Gupta \&amp; Kembhavi, 2022) may answer some questions correctly but others incorrectly. It is important to be aware of this uncertainty in contrast to stable, rule-based tools such as a calculator, further alleviate this instability and guarantee more predictable outputs.
Reproducible testing Many tools interact with the real world and may return different results at different times. For example, check_weather may return "sunny" today but "cloudy" tomorrow. This irreproducible behavior poses great challenges to creating static evaluation benchmarks with reference answers. While some works alleviate this by evaluating API calls without executing them, a more rigorous method could be parallel testing (Sharma et al., 2018) - executing the model-generated program and the reference program in parallel, and measuring if their final outputs match.</p>
<p>Safe usage Most systems may only opt to use tools if they are trusted to be secure (Barbir et al., 2007). At the very least, users favor tools that can be easily understood and verified. Further, systems may need to enforce mutual authentication and ensure data integrity (Ehsan et al., 2022). Yet there are more security threats and methods beyond the discussion here. We encourage readers to peruse the referenced works above for thorough studies.</p>
<h1>7 Trade-offs in tool usage</h1>
<p>Leveraging tools often brings better performance, however, should we always use tools? More concretely, is the performance gain from using tools worthy of the computation cost spent for LMs to learn to use tools, or the inference cost at test time? Existing works mainly focus on task accuracy, but a more nuanced picture emerges when we take other factors into account. We empirically study the performance gain and learning cost of various methods on their experimented datasets in Table 3, using which we discover more efficient (i.e., achieve greater gains with less compute) methods and tasks that benefit more from tools.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">$\Delta$ Perf.</th>
<th style="text-align: center;"># Params (B)</th>
<th style="text-align: center;"># Tokens (M)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">train</td>
<td style="text-align: center;">test</td>
</tr>
<tr>
<td style="text-align: center;">tool <br> use</td>
<td style="text-align: center;">ToolFormer</td>
<td style="text-align: center;">cloze</td>
<td style="text-align: center;">$+14.7$</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">642.1</td>
<td style="text-align: center;">269.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">math</td>
<td style="text-align: center;">$+30.4$</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">3864.2</td>
<td style="text-align: center;">421.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">$+5.8$</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">1101.2</td>
<td style="text-align: center;">189.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">multilingual</td>
<td style="text-align: center;">$-0.2$</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">606.0</td>
<td style="text-align: center;">274.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">temporal</td>
<td style="text-align: center;">$+13.0$</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">508.8</td>
<td style="text-align: center;">202.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">API-Bank</td>
<td style="text-align: center;">API</td>
<td style="text-align: center;">$+24.4$</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">190414.6</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ToolAlpaca</td>
<td style="text-align: center;">API</td>
<td style="text-align: center;">$+45.2$</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">241889.3</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chameleon</td>
<td style="text-align: center;">science</td>
<td style="text-align: center;">$+2.6$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">88.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">table</td>
<td style="text-align: center;">$+1.9$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">325.9</td>
</tr>
<tr>
<td style="text-align: center;">tool <br> making</td>
<td style="text-align: center;">LATM</td>
<td style="text-align: center;">BigBench</td>
<td style="text-align: center;">$+29.1$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">4720.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CREATOR</td>
<td style="text-align: center;">math</td>
<td style="text-align: center;">$+4.5$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">5113.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">table</td>
<td style="text-align: center;">$+0.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">6827.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CRAFT</td>
<td style="text-align: center;">math</td>
<td style="text-align: center;">$+13.2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4126.6</td>
<td style="text-align: center;">4098.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">table</td>
<td style="text-align: center;">$+17.2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2750.6</td>
<td style="text-align: center;">5018.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TroVE</td>
<td style="text-align: center;">math</td>
<td style="text-align: center;">$+21.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1825.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">table</td>
<td style="text-align: center;">$+12.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1358.8</td>
</tr>
</tbody>
</table>
<p>Table 3: Computation cost (number of tokens in $M$ and parameters in $B$ ) of tooling methods and their performance gain on experimented datasets. To fairly compare costs on datasets with different sizes, we report the average number of tokens spent on a testing example.</p>
<p>For each work and each dataset they experimented with, ${ }^{2}$ we evaluate the performance gain after LM learned or made tools to solve tasks, compared to the baseline LM with no prior exposure to tool-related information. We also quantify the computation cost of their tooling approaches during the token-consuming training and inference processes. For works using models with known sizes, we report both (i) the number of tokens in input prompts and outputs, and (ii) the parameters in experimented models to achieve corresponding performance improvements. For methods using the size-unknown GPT-4 model, which are also comparable w.r.t. to model size since they use the same GPT-4 model, we only report the number of tokens processed. We elaborate more on computation details in §A.
What tasks benefit the most from tools? In general, tasks that cover multiple domains experience the highest increase, such as the ToolAlpaca benchmark in tool-using and the BigBench dataset in tool-making scenarios. Nonetheless, substantial gains may be expected</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>on API benchmarks (i.e., API-Bank and ToolAlpaca), because all examples are synthesized use cases for designated tools (§4.1), no-tool baselines are deprived of necessary components (i.e., tools) to solve the task, therefore achieving much lower accuracy.
On existing benchmarks, the ToolFormer method is the most efficient on MATH problems, showing the highest 30.4 increase with little computation $(0.17 \mathrm{MB})$. While other tasks improve less, multilingual tasks even degrade by -0.2 points, despite using a similar amount of compute. This variance across tasks aligns with expectations: using a calculator tool greatly improves the arithmetic ability of probabilistic LMs, which are not naturally suitable for symbolic calculations; however, LMs are originally built to solve language tasks such as machine translation (MT), so assigning the MT task to another (usually smaller) LM may not bring substantial improvements.</p>
<p>What methods are efficient in tool-making? While it is hard to conduct fair comparisons for many works experimenting on different datasets, in tool-making scenarios (Figure 6), the three methods (Creator, CRAFT, TroVE) experiment on the same MATH and TabMWP datasets, thus enabling fair comparisons in both cost and performance dimensions. TroVE appears to be the most efficient method in general, costing only $1.2-1.4 \mathrm{~K}$ tokens while improving the performance by $12.0-21.0$ points in accuracy. In contrast, CREATOR and CRAFT are less efficient, costing 3.8-6.0 times of compute, yet achieve only minimal ( $0.0-4.5 \%$ ) or comparable ( $4.1-5.0 \%$ ) accuracy increases.
Training-time vs inference-time cost Training-time and inference-time costs may not be equally important to many practitioners, since inference may be run many times but training often only needs to be done once. ${ }^{3}$ If we only consider inference-time cost in Table 3, the efficiency ranking of tooling methods changes. On one hand, tool-making method rankings roughly remain the same, except that CRAFT requires less compute than CREATOR on both tasks after getting rid of the training cost. On the other hand, however, the ranking among tool-using methods drastically changes: ToolFormer requires more compute than API-Bank and ToolAlpaca when considering only inference-time cost. We conjecture this is mainly due to differences in baseline setups: ToolFormer adds in-context examples than the CoT baseline, API-Bank and ToolAlpaca use the same prompt for baseline and fine-tuned LMs with varied abilities to utilize tools presented in the prompt. In general, if the user has sufficient budgets for training but higher demands on inference-time efficiency, the training approaches proposed by API-Bank and ToolAlpaca could be more suitable.</p>
<h1>8 Final Remarks</h1>
<p>Our survey provides definitions for LM-used tools and systematic summaries of existing approaches. While our empirical analysis guides when (on what tasks) and how (use what methods) should one use tools, we hope readers can more clearly understand the scenarios and techniques of LM tooling, from basic paradigm to advanced settings, and across LMs speaking natural and programming languages. We believe tools can greatly extend and facilitate LM abilities, and hope our work elicits more discussions and research developments in (i) proposing benchmarks with natural use cases and executable tools, (ii) utilizing comprehensive evaluation metrics proposed in $\S 6$, and (iii) exploring more challenging and realistic scenarios for tool-using and tool-making techniques.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Acknowledgments</h1>
<p>We thank Saujas Vaduguru, Sherry Tongshuang Wu, Jiawei Liu, Shihao Liang, Pengfei Liu for the helpful discussions. Zora Zhiruo Wang is supported by the Teng Family Presidential Fellowship. Hao Zhu is supported by NSF EAGER Award #2141751.</p>
<h2>References</h2>
<p>Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts). Association for Computational Linguistics, 2023. URL https:/ / aclanthology.org/2023.acl-tutorials.6.</p>
<p>Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. Deepcoder: Learning to write programs. arXiv preprint arXiv:1611.01989, 2016.</p>
<p>Abbie Barbir, Chris Hobbs, Elisa Bertino, Frederick Hirsch, and Lorenzo Martino. Challenges of testing web services and security in soa implementations. Test and Analysis of Web Services, pp. 395-440, 2007.</p>
<p>BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. URL https://openreview.net/forum?id=uyTL5Bvosj.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1533-1544, 2013.</p>
<p>Daniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nature, 624(7992):570-578, 2023.</p>
<p>Matthew Bowers, Theo X. Olausson, Lionel Wong, Gabriel Grand, Joshua B. Tenenbaum, Kevin Ellis, and Armando Solar-Lezama. Top-down synthesis for library learning. Proc. ACM Program. Lang., 2023. URL https://doi.org/10.1145/3571234.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. URL https:// proceedings.neurips. cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023. URL https://arxiv.org/pdf/ 2305.17126.</p>
<p>Yihan Cao, Shuyi Chen, Ryan Liu, Zhiruo Wang, and Daniel Fried. Api-assisted code generation for question answering on varied table structures. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https: //aclanthology.org/2023.emnlp-main.897.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023. URL https://openreview.net/forum?id= YfZ4ZPt8zd.</p>
<p>Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, JianGuang Lou, and Dongmei Zhang. Hitab: A hierarchical table dataset for question answering and natural language generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), May 2022.</p>
<p>Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. Binding language models in symbolic languages. In The Eleventh International Conference on Learning Representations, 2023. URL https: / openreview.net/forum?id=lH1PV42cbF.</p>
<p>Adeel Ehsan, Mohammed Ahmad M. E. Abuhaliqa, Cagatay Catal, and Deepti Mishra. Restful api testing methodologies: Rationale, challenges, and solution directions. Applied Sciences, 2022. URL https://www.mdpi.com/2076-3417/12/9/4369.</p>
<p>Atty Eleti, Jeff Harris, and Logan Kilpatrick. Function calling and other api updates, 2023. URL https://openai.com/blog/function-calling-and-other-api-updates.</p>
<p>Kevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lore Anaya Pozo, Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. Philosophical Transactions of the Royal Society A, 2023.</p>
<p>Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640, 2023a.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764-10799. PMLR, 2023b.</p>
<p>Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. URL https://aclanthology.org/2021.emnlp-main.552.</p>
<p>Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X Olausson, Muxin Liu, Joshua B Tenenbaum, and Jacob Andreas. Lilo: Learning interpretable libraries by compressing and documenting code. arXiv preprint arXiv:2310.19791, 2023.</p>
<p>Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. arXiv preprint arXiv:2211.11559, 2022. URL https://arxiv.org/ pdf/2211.11559.</p>
<p>Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= BHXsb69bSx.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. URL https://arxiv.org/pdf/2103.03874.</p>
<p>Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Tool documentation enables zero-shot tool-usage with large language models. arXiv preprint arXiv:2308.00675, 2023.</p>
<p>Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, and Lichao Sun. Metatool benchmark for large language models: Deciding whether to use tools and which to use. arXiv preprint arXiv:2310.03128, 2024.</p>
<p>Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6700-6709, 2019.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017. URL https:/ / aclanthology.org/P17-1147.</p>
<p>Mojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022. URL https:// aclanthology.org/2022.acl-long.579.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 2019. URL https:// aclanthology.org/Q19-1026.
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022. URL https://arxiv.org/abs/2203.05115.
Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https:// aclanthology.org/2023.emnlp-main.187.</p>
<p>Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 9493-9500. IEEE, 2023.</p>
<p>Percy Liang. Lambda dependency-based compositional semantics. arXiv preprint arXiv:1309.4408, 2013.
Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=ryTp3f-0-.
Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https://openreview.net/forum?id=HtqnVSCj3q.
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2023b. URL https://arxiv.org/pdf/2209.14610.
Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfr-embedding-mistral:enhance text retrieval with transfer learning. Salesforce AI Research Blog, 2024. URL https://blog.salesforceairesearch.com/sfr-embedded-mistral/.
Grégoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=jh7wH2AzKK.
OpenAI. New embeddings models and api updates. URL https://openai.com/blog/ new-embedding-models-and-api-updates.
Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023. URL https://arxiv.org/abs/2303. 09014 .</p>
<p>Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255, 2022. URL https://arxiv.org/abs/2205.12255.</p>
<p>Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, July 2015. URL https://aclanthology.org/P15-1142.</p>
<p>Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.</p>
<p>Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. Creator: Disentangling abstract and concrete reasonings of large language models through tool creation. arXiv preprint arXiv:2305.14318, 2023. URL https://arxiv.org/pdf/2305.14318.</p>
<p>Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023.</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=dHng2O0Jjr.</p>
<p>Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends ${ }^{\circledR}$ in Information Retrieval, 3(4):333-389, 2009.</p>
<p>Nathaniel Robinson, Perez Ogayo, David R. Mortensen, and Graham Neubig. ChatGPT MT: Competitive for high- (but not low-) resource languages. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz (eds.), Proceedings of the Eighth Conference on Machine Translation, pp. 392-418, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.wmt-1.40. URL https://aclanthology.org/2023.wmt-1.40.</p>
<p>Wm. Paul Rogers. Encapsulation is not information hiding. JavaWorld, 2001. URL https:// www.infoworld.com/article/2075271/encapsulation-is-not-information-hiding.html.</p>
<p>Stuart J Russell and Peter Norvig. Artificial intelligence a modern approach. London, 2010.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. URL https: //arxiv.org/abs/2302.04761.</p>
<p>Abhinav Sharma, M Revathi, et al. Automated api testing. In 2018 3rd International Conference on Inventive Computation Technologies (ICICT), pp. 788-791. IEEE, 2018.</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https://openreview.net/ forum?id=yHdTscY6Ci.</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, and Yueting Zhuang. Taskbench: Benchmarking large language models for task automation. arXiv preprint arXiv:2311.18760, 2023b.</p>
<p>Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740-10749, 2020.</p>
<p>Robert W Shumaker, Kristina R Walkup, and Benjamin B Beck. Animal tool behavior: the use and manufacture of tools by animals. JHU Press, 2011.</p>
<p>Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. In Workshop on Language and Robotics at CoRL 2022, 2022. URL https://openreview.net/forum?id=3K4-U_5cRw.</p>
<p>Elias Stengel-Eskin, Archiki Prasad, and Mohit Bansal. Regal: Refactoring programs to discover generalizable abstractions. arXiv preprint arXiv:2401.16467, 2024.</p>
<p>Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2019. URL https:// aclanthology.org/P19-1644.</p>
<p>Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas Griffiths. Cognitive architectures for language agents. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=1i6ZCvflQJ. Survey Certification.</p>
<p>Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.</p>
<p>Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. URL https://arxiv.org/abs/2201.08239.</p>
<p>Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, and Graham Neubig. Prompt2model: Generating deployable models from natural language instructions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Association for Computational Linguistics, 2023. URL https:// aclanthology.org/2023.emnlp-demo.38.</p>
<p>Ben Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive language model, 2021. URL https://github.com/kingoflolz/mesh-transformer-jax.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023a. URL https://openreview.net/forum?id=P8E4Br72j3.</p>
<p>Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691, 2023b.</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. arXiv preprint arXiv:2402.01030, 2024a.</p>
<p>Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. Execution-based evaluation for open-domain code generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1271-1290. Association for Computational Linguistics, December 2023c. URL https://aclanthology.org/2023.findings-emnlp. 89.</p>
<p>Zhiruo Wang, Daniel Fried, and Graham Neubig. Trove: Inducing verifiable and efficient toolboxes for solving programmatic tasks. arXiv preprint arXiv:2401.12869, 2024b. URL https://arxiv.org/abs/2401.12869.</p>
<p>Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, 2020. URL https://aclanthology.org/2020.lrec-1.494.</p>
<p>Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. Break it down: A question understanding benchmark. Transactions of the Association for Computational Linguistics, 2020. URL https://doi.org/10.1162/tacl_a_00309.</p>
<p>Catherine Wong, Kevin M Ellis, Joshua Tenenbaum, and Jacob Andreas. Leveraging language to learn program abstractions and search heuristics. In Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research, 2021. URL https://proceedings.mlr.press/v139/wong21a.html.</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.</p>
<p>Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation capability of open-source large language models. arXiv preprint arXiv:2305.16504, 2023.</p>
<p>Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.</p>
<p>Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 82ad13ec01f9fe44c01cb91814fd7b8c-Paper-Conference.pdf.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=WE_vluYUL-X.</p>
<p>Kinza Yasar. Software testing, 2022. URL https://www.techtarget.com/whatis/definition/ software-testing.</p>
<p>Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 440450, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1041. URL https://aclanthology.org/P17-1041.</p>
<p>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018.</p>
<p>Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. Language to rewards for robotic skill synthesis. arXiv preprint arXiv:2306.08647, 2023.</p>
<p>Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R. Fung, Hao Peng, and Heng Ji. Craft: Customizing llms by creating and retrieving from specialized toolsets. arXiv preprint arXiv:2309.17428, 2023.</p>
<p>John M Zelle and Raymond J Mooney. Learning to parse database queries using inductive logic programming. In Proceedings of the national conference on artificial intelligence, pp. $1050-1055,1996$.</p>
<p>Luke S Zettlemoyer and Michael Collins. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. arXiv preprint arXiv:1207.1420, 2012.</p>
<p>Kechi Zhang, Huangzhao Zhang, Ge Li, Jia Li, Zhuo Li, and Zhi Jin. Toolcoder: Teach code generation models to use api search tools. arXiv preprint arXiv:2305.04032, 2023. URL https: / / arxiv.org/abs/2305.04032.</p>
<p>Shuyan Zhou, Uri Alon, Frank F. Xu, Zhiruo Wang, Zhengbao Jiang, and Graham Neubig. Docprompting: Generating code by retrieving the docs. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= ZTCxT2t2Ru.</p>
<p>Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= oKn9c6ytLx.</p>
<p>Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset for llm question answering with external tools. arXiv preprint arXiv:2306.13304, 2023.</p>
<h1>A Detailed computation process for tooling trade-offs</h1>
<p>For each method measured in $\S 7$, we describe the detailed processes in estimating their computation cost and performance improvement. For open-source models, we estimate cost $C=6 N D$, where $N$ is the number of tokens and $D$ is the parameter size (Figure 7, left). Because the parameter size $D$ of closed-source GPT is unknown, we only measure the number of extra tokens $N$ per example (Figure 7, right).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: Computation cost of different approaches using open-source (left) and closedsource (right) models, and their performance gain on experimented datasets. We use different colors to represent tasks and different shapes to represent methods.</p>
<h2>A. 1 Methods using known-sized models</h2>
<p>For methods using models whose parameter sizes are known, we estimate the computation cost by the FLOPs during any additional modules such as training and inference with additional context. In general, the computation cost is majorly affected by (1) the number of tokens processed, and (2) the parameter size of models.
API-Bank (Li et al., 2023) This work trains the Lynx model that uses tools to solve problems in the proposed API-Bank dataset. The Lynx model is initialized by Alpaca 7B parameters, and trained on the API-Bank training set with 3 epochs. Therefore, we adopt the Alpaca 7B as the baseline and Lynx as the tool-using model, where the 3-epoch training is the additional computation cost introduced to enable tool use. We calculate the total number of tokens involved in the training process, including the example i/o and additional instructions. Because the baseline and proposed method use the same prompt at inference time, no additional computation is required. Regarding task performance, we adopt the total correctness across all evaluation systems, as reported in Table 3. We report the difference between the fine-tuned Lynx-7B and the zero-shot Alpaca-7B.
ToolAlpaca (Tang et al., 2023) This work proposes the ToolAlpaca dataset and trains Vicuna models to use tools. The baseline models are Vicuna-7B and Vicuna-13B models. The trained tool-using models are called ToolAlpaca-7B and ToolAlpaca-13B models. All ToolAlpaca models are trained on the training split for 3 epochs, so we estimate the cost during this training process for 7 B and 13 B models, respectively. We adopt the 'overall' results reported in Table 3, on examples with both simulated tools and real-world APIs, and report their average results. We measure the performance gain by the difference between the ToolAlpaca-7/13B and Vicuna-7/13B.
Toolformer (Schick et al., 2023) This work integrates five tools - question answering system, calculator, Wikipedia search, machine translation system, and calendar - respectively for five tasks transformed from a subset of CCNet (Wenzek et al., 2020). Starting with GPT-J models (Wang \&amp; Komatsuzaki, 2021) as the no-tool baseline, they train on $25 k$ model-synthesized examples for each tool and obtain the Toolformer models, causing a total of 1 M FLOPs for each task. At inference time, they add special instruction and in-context examples to prompt tool using, resulting in extra compute. Because each task contains multiple datasets, we report the average results to represent the general task performance.</p>
<h1>A. 2 Models with unknown size</h1>
<p>While many of the works use GPT-3.5 or GPT-4 models that do not release their parameter size, we estimate the cost by using the number of tokens processed in extra modules.</p>
<p>Chameleon (Lu et al., 2023a) This work proposes to take a tool-augmented approach to improve on two existing datasets - ScienceQA and TabMWP. Because all experiments use ChatGPT and GPT-4 models, whose parameter sizes are unknown, we only examine results with (the better) GPT-4 model to fairly compare with other methods using GPT4. Specifically for the ScienceQA dataset, we adopt the Chain-of-Thought (CoT) baseline reported in the paper, and report task accuracy as in the All column in Table 3. We calculate the difference in number of tokens between the proposed Chameleon methods against the CoT baseline. For the TabMWP dataset, we adopt the Program-of-Thought (PoT) baseline and similarly calculate the token number difference using the provided results. ${ }^{4}$ We adopt numbers in the All column in Table 4 as the TabMWP accuracy.
LATM (Cai et al., 2023) This work proposes to use LMs to make tools for individual tasks in BigBench. Compared to the chain-of-thought (CoT) baseline, the proposed LATM method integrates training, validation, and inference stages to make tools and solve questions. We estimate the compute cost by the additional number of tokens used for LATM than for CoT. We measure each method by averaging its accuracy across all six selected tasks.
CRAFT (Yuan et al., 2023) This work uses LMs to make tools for math, table, and image reasoning tasks. We calculate the number of tokens used during training and inference, using its released code and data. ${ }^{5}$ CRAFT similarly implements CoT as the baseline, and proposes further training, verification, and finally testing in the CRAFT method. We report its task accuracy on the representative datasets from each task - MATH, TabMWP, and GQA - to enable fairer comparison with other works having overlapping datasets.
CREATOR (Qian et al., 2023) As a prior work for CRAFT, CREATOR similarly tests on MATH and table tasks, but designs its methods differently. In addition to CoT, this work implements a stronger program-oriented baseline called Program-of-Thought (PoT). We also adopt PoT as the main baseline without tool making or using. The CREATOR method operates at test time, with multiple steps through tool making, solution generation, verification, rectification, etc. We calculate the difference in number of tokens between the CREATOR approach and the baseline PoT setting. We adopt the task accuracy reported in Table 2 (MATH) and Table 3 (TabMWP) from the original paper.
TroVE (Wang et al., 2024b) TroVE also induces tools without training supervision. This work adopts the primitive baseline, a presumably stronger version of PoT yet without much textual explanation. The main implementation change in TroVE is the three-mode generation and multi-candidate sampling. We calculate the additional tokens used in TroVE compared to the primitive baseline. The dataset reports task accuracy, solution complexity, and toolbox size, we only adopt the task accuracy to fairly compare with other works.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://github.com/lupantech/chameleon-llm
${ }^{5}$ https://github.com/lifan-yuan/CRAFT&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
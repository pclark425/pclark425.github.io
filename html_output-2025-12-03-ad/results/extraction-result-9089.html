<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9089 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9089</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9089</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-76f023c3a819fc58989a064a1b50825b11fce95d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/76f023c3a819fc58989a064a1b50825b11fce95d" target="_blank">Capturing Failures of Large Language Models via Human Cognitive Biases</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work uses cognitive biases as motivation to generate hypotheses for problems that models may have, and develops experiments that elicit these problems and indicates that experimental methodology from cognitive science can help characterize how machine learning systems behave.</p>
                <p><strong>Paper Abstract:</strong> Large language models generate complex, open-ended outputs: instead of outputting a class label they write summaries, generate dialogue, or produce working code. In order to asses the reliability of these open-ended generation systems, we aim to identify qualitative categories of erroneous behavior, beyond identifying individual errors. To hypothesize and test for such qualitative errors, we draw inspiration from human cognitive biases -- systematic patterns of deviation from rational judgement. Specifically, we use cognitive biases as motivation to (i) generate hypotheses for problems that models may have, and (ii) develop experiments that elicit these problems. Using code generation as a case study, we find that OpenAI's Codex errs predictably based on how the input prompt is framed, adjusts outputs towards anchors, and is biased towards outputs that mimic frequent training examples. We then use our framework to elicit high-impact errors such as incorrectly deleting files. Our results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9089.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9089.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 Anchoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3 (anchoring replication)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replication/adaptation of a classical anchoring estimation task (Jacowitz & Kahneman 1995) applied to GPT-3 to test whether model estimates shift toward provided numeric anchors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI GPT-3 (davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive large language model (GPT-3 family) queried via prompts adapted from Jacowitz & Kahneman (1995); used as a black-box via OpenAI API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Anchoring (adapted from Jacowitz & Kahneman 1995)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Estimation tasks where a lower or upper numeric bound (anchor) is prepended to the question; measures whether subsequent numeric estimates shift toward the anchor (cognitive domain: probabilistic / numeric judgment, bias in estimation).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>For anchor adjustment p=20%: No change 10.7%, Shift toward anchor 28.6%, Shift away 10.7%, Gibberish 50.0%. For p=50%: No change 14.3%, Toward anchor 42.9%, Away 10.7%, Gibberish 32.1%. Authors additionally note that when GPT-3 updated its estimate it sometimes matched the anchor exactly (reported by authors as 67% of updated estimates in their analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3 routinely incorporates anchors into its estimates and often shifts estimates toward the anchor, qualitatively mirroring human anchoring behavior reported in psychological literature; however many outputs were classified as gibberish and no direct human baseline on these adapted prompts is reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>14 (of 15) reported Jacowitz & Kahneman questions were used with true answers computed; anchors set at +/- p% (p in {20%,50%}); prompts explicitly state e.g. 'The length ... is greater than 1000.'; outputs categorized as no change / towards / away / gibberish; model variant used: davinci-001; small-sample replication; templates used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Small sample size, templated prompts, and high fraction of 'gibberish' outputs (~32–50% depending on p). Paper does not provide a directly comparable human baseline on the exact adapted prompts; some reported statistics (e.g., 67% exact match to anchor among updated estimates) are summary-level and not linked to a full distribution in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Capturing Failures of Large Language Models via Human Cognitive Biases', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9089.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9089.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 Framing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3 (framing effect replication)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replication/adaptation of Tversky & Kahneman's framing effect (choice between certain vs probabilistic outcomes) to test whether GPT-3's choices vary by equivalent 'gain' vs 'loss' framings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI GPT-3 (davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive large language model (GPT-3 family) used to answer four-line framing prompts (save vs die framings) adapted from Tversky & Kahneman (1981).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Framing effect (adapted from Tversky & Kahneman 1981)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary choice tasks asking to prefer a certain outcome versus a probabilistic outcome, presented in either a 'save' (gain) framing or a 'die' (loss) framing; assesses susceptibility to framing (cognitive domain: decision framing / risk preference).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Across 704 prompts (various population sizes and save probabilities) GPT-3 chose the risky (probabilistic) option: overall 45.4% under the save framing and 74.1% under the die framing. In the original-problem regime (save probability = 1/3) GPT-3 chose risky option 11.3% under save framing and 60.0% under die framing. Results vary with save probability (e.g., for higher save probabilities GPT-3 often chose the risky option in both framings).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Original Tversky & Kahneman (1981) reported humans chose the risky option 28% of the time under the save framing and 78% under the die framing (these are reported in the paper for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3 qualitatively mirrors the human framing effect (more often chooses risky option under 'die' framing than under 'save' framing, especially near the original experimental regime), though absolute rates differ (GPT-3 higher in some conditions) and model behavior changes across save-probability regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used davinci-002; prompts formatted like the original Tversky & Kahneman four-line scenarios; population sizes varied (60–6000) and save fractions tested had denominators < 7; for each configuration the risky option was varied in position and label to avoid positional confounds; total of 704 prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>GPT-3 sometimes chooses the risky option in both framings for higher save probabilities (authors note humans might do similarly in that regime). The experiment uses template prompts and no human subjects were run on the exact expanded prompt set within this paper (paper compares to the original human results from Tversky & Kahneman).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Capturing Failures of Large Language Models via Human Cognitive Biases', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9089.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9089.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex / CodeGen Framing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Framing-inspired test on code models (irrelevant preceding functions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framing-effect-inspired transformation for code generation: prepending irrelevant preceding functions (IPFs) to HumanEval prompts to test whether code models rely on irrelevant prompt content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (davinci-001) and Salesforce CodeGen (mono 6.2B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Codex: OpenAI code-generation model (API 'davinci-001' variant, greedy decoding); CodeGen: Salesforce CodeGen 6.2B 'mono' model (weights available; inference run locally), both autoregressive models fine-tuned on code/GitHub.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Codex: not specified in paper; CodeGen: 6.2B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Framing-effect-inspired IPF test (HumanEval with irrelevant preceding functions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Transformed HumanEval programming prompts by prepending an irrelevant preceding function (random HumanEval prompt + a short framing line, e.g., 'raise NotImplementedError' or 'print("Hello world!")') to test sensitivity to irrelevant context (cognitive domain analog: susceptibility to irrelevant framing).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Codex baseline functional accuracy on HumanEval: 32.9%. With various framing lines, framed accuracies dropped to between 2.4% and 10.6% depending on framing line (drops of ~22.3–30.5 percentage points). Codex outputted the framing line verbatim in framed prompts ≈81% of the time vs ≈4.5% on untransformed prompts. CodeGen baseline 25.6% → framed accuracies 1.5%–11.0%; framing line output rates ≈70.7% (framed) vs 0% (original).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Both code models' functional accuracy dropped substantially when irrelevant preceding functions were prepended, and they often reproduced the irrelevant framing line verbatim—indicating reliance on semantically irrelevant prompt content rather than the target docstring/spec.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used HumanEval (164 problems); five framing lines tested (raise NotImplementedError, pass, assert False, return False, print(...)); measured functional accuracy (fraction of programs passing all provided tests) and frequency of verbatim reproduction of framing line; greedy decoding used for both models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>This is an adaptation of a cognitive-bias experiment to code generation; there is no human baseline on these transformed code prompts reported in the paper. The result is specific to how HumanEval prompts and the particular framing lines were constructed; model internals and training distributional causes are not directly established.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Capturing Failures of Large Language Models via Human Cognitive Biases', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9089.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9089.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex / CodeGen Anchoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anchoring-inspired prompt contamination for code models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anchoring-like test for code generation: prepending anchor functions (canonical-solution prefixes plus related-but-incorrect anchor lines) to see if models adjust generated code toward the anchor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (davinci-001) and Salesforce CodeGen (mono 6.2B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive code models: Codex via OpenAI API (greedy decoding), CodeGen run locally; prompts combine an anchor function, the original HumanEval prompt, and some canonical solution lines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Codex: not specified in paper; CodeGen: 6.2B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Anchoring-inspired anchor-function test (HumanEval with prepended anchor functions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Prepend to target prompts an anchor function that is similar to a valid solution but contains specific incorrect lines (e.g., 'for var in [vars]: print(var)' or 'tmp = str(var1)+str(var2); return tmp') to probe whether outputs are adjusted toward these anchors (cognitive domain analog: anchoring in adjustment from partial information).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Prepending anchor functions consistently lowered functional accuracy across various numbers of canonical solution lines. Codex generated 'for var in' in ~32%–61% of outputs (when ≥1 canonical line prompted) and 'print(var)' in ~26%–44%; CodeGen showed qualitatively similar patterns. For add-var anchors Codex produced 'return tmp' in ~26%–46% of solutions; CodeGen produced 'return tmp' in ~13%–79% depending on prompt context. Exact verbatim copying of the anchor function was relatively rare (Codex 7%–12%, CodeGen 4%–12%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Models adjust their outputs toward related-but-incorrect anchor code included in the prompt, analogous to human anchoring; the effect is measurable (substantial fractions of generated solutions include anchor lines) and degrades functional accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used HumanEval; varied number of canonical solution lines included in prompt (0–8) to modulate difficulty; measured functional accuracy with and without anchors and the incidence of anchor-line fragments or exact anchor-copy in outputs; performed control experiments altering anchor function names (negligible effect).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human-performer baseline on these exact code tasks is provided. Some anchor influence stems from partial copying but many cases incorporate anchor elements without verbatim copying. Findings depend on prompt engineering choices and the HumanEval set; model internals/training frequency not directly measured.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Capturing Failures of Large Language Models via Human Cognitive Biases', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9089.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9089.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex Availability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Availability-heuristic-inspired test on Codex (order-of-operations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Test if Codex tends to output more-frequent/available solutions from training data by flipping operation order (unary-first vs binary-first) in simple math prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Codex model queried on MathEquations prompts that request specific algebraic compositions (e.g., square_sum vs sum_square).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Availability heuristic (unary-first vs binary-first MathEquations)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Prompts ask for simple arithmetic functions combining unary (square, cube, quadruple, sqrt) and binary (sum, difference, product) operations; tests whether model outputs the more training-frequent unary-first solution even when binary-first semantics requested (cognitive domain analog: availability bias / memory-based frequency effects).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>When flipping the order from unary-first to binary-first, Codex accuracy dropped from 50% to 17%. Among combinations where flipping induced an error, 75% of the incorrect binary-first outputs were instead the unary-first solution (i.e., model produced the more frequent/available pattern).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Codex often outputs the more-available (presumed more frequent in training data) unary-first solution when the prompt requests binary-first order, consistent with an availability-like bias.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Considered 12 combos of binary ops (sum,difference,product) × unary ops (square,cube,quadruple,sqrt); focused on Codex because CodeGen produced many nonsensical outputs on these prompts; control experiments used non-instructional prompt variants and produced qualitatively similar results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>The assumption that unary-first patterns are more frequent in training data is plausible but not directly verified in the paper. CodeGen was excluded due to nonsensical outputs on these prompt styles, so findings are Codex-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Capturing Failures of Large Language Models via Human Cognitive Biases', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9089.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9089.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex Attribute Substitution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attribute substitution-inspired test on Codex (conflicting function names)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Test whether Codex uses simpler heuristics (e.g., function name) rather than the full specification when name conflicts with desired behavior (cognitive domain: attribute substitution / heuristic substitution).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Codex evaluated on MathEquation prompts where the requested function name contradicts the docstring-specified operation (e.g., function named product_plus_2 but docstring asks for sum).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Attribute substitution (conflicting function name vs docstring)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Prompts request a function implementing operation A but request a conflicting function name describing operation B; tests whether model follows simpler cue (name) over full specification (cognitive domain: heuristic substitution / reliance on salient attribute).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>On 90 MathEquation prompts where desired solution and requested function name differ, Codex's accuracy fell from 100% (no conflicting name) to 4.4%–4.6% (depending on where the conflicting name was placed). Between 52% and 80% of prompts resulted in Codex returning the behavior matching the (contradictory) function name rather than the requested operation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Codex frequently relies on the simpler/most salient cue (the function name) rather than the full instruction, consistent with attribute substitution behavior in humans.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Tested three placements for the contradictory name (in docstring, in signature below docstring, or above the docstring) and non-instructional prompt controls; used 90 prompts covering operations (sum,difference,product) × various numeric suffixes; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No direct human baseline for these codified prompts; results are sensitive to prompt format (placement of name) though effect persisted across placements. The stimuli are synthetic MathEquation prompts which may differ from typical real-world code repositories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Capturing Failures of Large Language Models via Human Cognitive Biases', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9089.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9089.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex High-Impact Deletions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>High-impact erroneous deletions elicited via attribute substitution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using attribute-substitution-inspired prompts to elicit dangerous, high-impact behavior from Codex: incorrect file-deletion code that deletes files it should not (e.g., deletes files containing any of a set rather than all).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Codex prompted to 'delete all files in an inputted directory that import all of [package set]'; model outputs examined for harmful simplifications (e.g., checking only first package or any package instead of all).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Attribute substitution → high-impact deletion test</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Prompts present conjunctions of package-import checks (A ∧ B ∧ ...); tests whether model simplifies to checking only a subset (A or A ∨ B) causing erroneous deletion (cognitive domain analog: attribute substitution / simplification under complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>When the number of package imports ≥ 3, Codex erroneously deletes files on at least ~80% of prompts; for number of packages ≤ 2 Codex produced a correct output on ~90% of prompts. As the number of packages increases, Codex more frequently simplifies to checking only the first package (A) rather than the conjunction.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Codex exhibits a propensity to simplify conjunctive checks into easier-to-compute heuristics, producing high-impact erroneous behavior (deleting files) under realistic-seeming prompts; this is consistent with attribute substitution observed in humans but with severe practical consequences.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Constructed prompts instructing deletion of files that import a specified set of packages; authors classified outputs into correct (check all packages), deletes on first package only (A), or deletes on any package (A ∨ B), and measured frequency as a function of number of packages; control experiments used more descriptive docstrings and observed qualitatively similar patterns though with more low-impact errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>These experiments are potentially harmful (authors note misuse risk). The test is an adapted cognitive-bias probe targeted at code generation and has no human baseline in the paper; generalization beyond the tested prompt class and to other models may vary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Capturing Failures of Large Language Models via Human Cognitive Biases', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measures of anchoring in estimation tasks <em>(Rating: 2)</em></li>
                <li>The framing of decisions and the psychology of choice <em>(Rating: 2)</em></li>
                <li>Judgment under uncertainty: Heuristics and biases <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 2)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 1)</em></li>
                <li>What makes good in-context examples for GPT-3 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9089",
    "paper_id": "paper-76f023c3a819fc58989a064a1b50825b11fce95d",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-3 Anchoring",
            "name_full": "OpenAI GPT-3 (anchoring replication)",
            "brief_description": "Replication/adaptation of a classical anchoring estimation task (Jacowitz & Kahneman 1995) applied to GPT-3 to test whether model estimates shift toward provided numeric anchors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI GPT-3 (davinci-001)",
            "model_description": "Autoregressive large language model (GPT-3 family) queried via prompts adapted from Jacowitz & Kahneman (1995); used as a black-box via OpenAI API.",
            "model_size": null,
            "test_battery_name": "Anchoring (adapted from Jacowitz & Kahneman 1995)",
            "test_description": "Estimation tasks where a lower or upper numeric bound (anchor) is prepended to the question; measures whether subsequent numeric estimates shift toward the anchor (cognitive domain: probabilistic / numeric judgment, bias in estimation).",
            "llm_performance": "For anchor adjustment p=20%: No change 10.7%, Shift toward anchor 28.6%, Shift away 10.7%, Gibberish 50.0%. For p=50%: No change 14.3%, Toward anchor 42.9%, Away 10.7%, Gibberish 32.1%. Authors additionally note that when GPT-3 updated its estimate it sometimes matched the anchor exactly (reported by authors as 67% of updated estimates in their analysis).",
            "human_baseline_performance": null,
            "performance_comparison": "GPT-3 routinely incorporates anchors into its estimates and often shifts estimates toward the anchor, qualitatively mirroring human anchoring behavior reported in psychological literature; however many outputs were classified as gibberish and no direct human baseline on these adapted prompts is reported in the paper.",
            "experimental_details": "14 (of 15) reported Jacowitz & Kahneman questions were used with true answers computed; anchors set at +/- p% (p in {20%,50%}); prompts explicitly state e.g. 'The length ... is greater than 1000.'; outputs categorized as no change / towards / away / gibberish; model variant used: davinci-001; small-sample replication; templates used.",
            "limitations_or_caveats": "Small sample size, templated prompts, and high fraction of 'gibberish' outputs (~32–50% depending on p). Paper does not provide a directly comparable human baseline on the exact adapted prompts; some reported statistics (e.g., 67% exact match to anchor among updated estimates) are summary-level and not linked to a full distribution in the paper.",
            "uuid": "e9089.0",
            "source_info": {
                "paper_title": "Capturing Failures of Large Language Models via Human Cognitive Biases",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "GPT-3 Framing",
            "name_full": "OpenAI GPT-3 (framing effect replication)",
            "brief_description": "Replication/adaptation of Tversky & Kahneman's framing effect (choice between certain vs probabilistic outcomes) to test whether GPT-3's choices vary by equivalent 'gain' vs 'loss' framings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI GPT-3 (davinci-002)",
            "model_description": "Autoregressive large language model (GPT-3 family) used to answer four-line framing prompts (save vs die framings) adapted from Tversky & Kahneman (1981).",
            "model_size": null,
            "test_battery_name": "Framing effect (adapted from Tversky & Kahneman 1981)",
            "test_description": "Binary choice tasks asking to prefer a certain outcome versus a probabilistic outcome, presented in either a 'save' (gain) framing or a 'die' (loss) framing; assesses susceptibility to framing (cognitive domain: decision framing / risk preference).",
            "llm_performance": "Across 704 prompts (various population sizes and save probabilities) GPT-3 chose the risky (probabilistic) option: overall 45.4% under the save framing and 74.1% under the die framing. In the original-problem regime (save probability = 1/3) GPT-3 chose risky option 11.3% under save framing and 60.0% under die framing. Results vary with save probability (e.g., for higher save probabilities GPT-3 often chose the risky option in both framings).",
            "human_baseline_performance": "Original Tversky & Kahneman (1981) reported humans chose the risky option 28% of the time under the save framing and 78% under the die framing (these are reported in the paper for comparison).",
            "performance_comparison": "GPT-3 qualitatively mirrors the human framing effect (more often chooses risky option under 'die' framing than under 'save' framing, especially near the original experimental regime), though absolute rates differ (GPT-3 higher in some conditions) and model behavior changes across save-probability regimes.",
            "experimental_details": "Used davinci-002; prompts formatted like the original Tversky & Kahneman four-line scenarios; population sizes varied (60–6000) and save fractions tested had denominators &lt; 7; for each configuration the risky option was varied in position and label to avoid positional confounds; total of 704 prompts.",
            "limitations_or_caveats": "GPT-3 sometimes chooses the risky option in both framings for higher save probabilities (authors note humans might do similarly in that regime). The experiment uses template prompts and no human subjects were run on the exact expanded prompt set within this paper (paper compares to the original human results from Tversky & Kahneman).",
            "uuid": "e9089.1",
            "source_info": {
                "paper_title": "Capturing Failures of Large Language Models via Human Cognitive Biases",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Codex / CodeGen Framing",
            "name_full": "Framing-inspired test on code models (irrelevant preceding functions)",
            "brief_description": "A framing-effect-inspired transformation for code generation: prepending irrelevant preceding functions (IPFs) to HumanEval prompts to test whether code models rely on irrelevant prompt content.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (davinci-001) and Salesforce CodeGen (mono 6.2B)",
            "model_description": "Codex: OpenAI code-generation model (API 'davinci-001' variant, greedy decoding); CodeGen: Salesforce CodeGen 6.2B 'mono' model (weights available; inference run locally), both autoregressive models fine-tuned on code/GitHub.",
            "model_size": "Codex: not specified in paper; CodeGen: 6.2B",
            "test_battery_name": "Framing-effect-inspired IPF test (HumanEval with irrelevant preceding functions)",
            "test_description": "Transformed HumanEval programming prompts by prepending an irrelevant preceding function (random HumanEval prompt + a short framing line, e.g., 'raise NotImplementedError' or 'print(\"Hello world!\")') to test sensitivity to irrelevant context (cognitive domain analog: susceptibility to irrelevant framing).",
            "llm_performance": "Codex baseline functional accuracy on HumanEval: 32.9%. With various framing lines, framed accuracies dropped to between 2.4% and 10.6% depending on framing line (drops of ~22.3–30.5 percentage points). Codex outputted the framing line verbatim in framed prompts ≈81% of the time vs ≈4.5% on untransformed prompts. CodeGen baseline 25.6% → framed accuracies 1.5%–11.0%; framing line output rates ≈70.7% (framed) vs 0% (original).",
            "human_baseline_performance": null,
            "performance_comparison": "Both code models' functional accuracy dropped substantially when irrelevant preceding functions were prepended, and they often reproduced the irrelevant framing line verbatim—indicating reliance on semantically irrelevant prompt content rather than the target docstring/spec.",
            "experimental_details": "Used HumanEval (164 problems); five framing lines tested (raise NotImplementedError, pass, assert False, return False, print(...)); measured functional accuracy (fraction of programs passing all provided tests) and frequency of verbatim reproduction of framing line; greedy decoding used for both models.",
            "limitations_or_caveats": "This is an adaptation of a cognitive-bias experiment to code generation; there is no human baseline on these transformed code prompts reported in the paper. The result is specific to how HumanEval prompts and the particular framing lines were constructed; model internals and training distributional causes are not directly established.",
            "uuid": "e9089.2",
            "source_info": {
                "paper_title": "Capturing Failures of Large Language Models via Human Cognitive Biases",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Codex / CodeGen Anchoring",
            "name_full": "Anchoring-inspired prompt contamination for code models",
            "brief_description": "Anchoring-like test for code generation: prepending anchor functions (canonical-solution prefixes plus related-but-incorrect anchor lines) to see if models adjust generated code toward the anchor.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (davinci-001) and Salesforce CodeGen (mono 6.2B)",
            "model_description": "Autoregressive code models: Codex via OpenAI API (greedy decoding), CodeGen run locally; prompts combine an anchor function, the original HumanEval prompt, and some canonical solution lines.",
            "model_size": "Codex: not specified in paper; CodeGen: 6.2B",
            "test_battery_name": "Anchoring-inspired anchor-function test (HumanEval with prepended anchor functions)",
            "test_description": "Prepend to target prompts an anchor function that is similar to a valid solution but contains specific incorrect lines (e.g., 'for var in [vars]: print(var)' or 'tmp = str(var1)+str(var2); return tmp') to probe whether outputs are adjusted toward these anchors (cognitive domain analog: anchoring in adjustment from partial information).",
            "llm_performance": "Prepending anchor functions consistently lowered functional accuracy across various numbers of canonical solution lines. Codex generated 'for var in' in ~32%–61% of outputs (when ≥1 canonical line prompted) and 'print(var)' in ~26%–44%; CodeGen showed qualitatively similar patterns. For add-var anchors Codex produced 'return tmp' in ~26%–46% of solutions; CodeGen produced 'return tmp' in ~13%–79% depending on prompt context. Exact verbatim copying of the anchor function was relatively rare (Codex 7%–12%, CodeGen 4%–12%).",
            "human_baseline_performance": null,
            "performance_comparison": "Models adjust their outputs toward related-but-incorrect anchor code included in the prompt, analogous to human anchoring; the effect is measurable (substantial fractions of generated solutions include anchor lines) and degrades functional accuracy.",
            "experimental_details": "Used HumanEval; varied number of canonical solution lines included in prompt (0–8) to modulate difficulty; measured functional accuracy with and without anchors and the incidence of anchor-line fragments or exact anchor-copy in outputs; performed control experiments altering anchor function names (negligible effect).",
            "limitations_or_caveats": "No human-performer baseline on these exact code tasks is provided. Some anchor influence stems from partial copying but many cases incorporate anchor elements without verbatim copying. Findings depend on prompt engineering choices and the HumanEval set; model internals/training frequency not directly measured.",
            "uuid": "e9089.3",
            "source_info": {
                "paper_title": "Capturing Failures of Large Language Models via Human Cognitive Biases",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Codex Availability",
            "name_full": "Availability-heuristic-inspired test on Codex (order-of-operations)",
            "brief_description": "Test if Codex tends to output more-frequent/available solutions from training data by flipping operation order (unary-first vs binary-first) in simple math prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (davinci-001)",
            "model_description": "Codex model queried on MathEquations prompts that request specific algebraic compositions (e.g., square_sum vs sum_square).",
            "model_size": null,
            "test_battery_name": "Availability heuristic (unary-first vs binary-first MathEquations)",
            "test_description": "Prompts ask for simple arithmetic functions combining unary (square, cube, quadruple, sqrt) and binary (sum, difference, product) operations; tests whether model outputs the more training-frequent unary-first solution even when binary-first semantics requested (cognitive domain analog: availability bias / memory-based frequency effects).",
            "llm_performance": "When flipping the order from unary-first to binary-first, Codex accuracy dropped from 50% to 17%. Among combinations where flipping induced an error, 75% of the incorrect binary-first outputs were instead the unary-first solution (i.e., model produced the more frequent/available pattern).",
            "human_baseline_performance": null,
            "performance_comparison": "Codex often outputs the more-available (presumed more frequent in training data) unary-first solution when the prompt requests binary-first order, consistent with an availability-like bias.",
            "experimental_details": "Considered 12 combos of binary ops (sum,difference,product) × unary ops (square,cube,quadruple,sqrt); focused on Codex because CodeGen produced many nonsensical outputs on these prompts; control experiments used non-instructional prompt variants and produced qualitatively similar results.",
            "limitations_or_caveats": "The assumption that unary-first patterns are more frequent in training data is plausible but not directly verified in the paper. CodeGen was excluded due to nonsensical outputs on these prompt styles, so findings are Codex-specific.",
            "uuid": "e9089.4",
            "source_info": {
                "paper_title": "Capturing Failures of Large Language Models via Human Cognitive Biases",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Codex Attribute Substitution",
            "name_full": "Attribute substitution-inspired test on Codex (conflicting function names)",
            "brief_description": "Test whether Codex uses simpler heuristics (e.g., function name) rather than the full specification when name conflicts with desired behavior (cognitive domain: attribute substitution / heuristic substitution).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (davinci-001)",
            "model_description": "Codex evaluated on MathEquation prompts where the requested function name contradicts the docstring-specified operation (e.g., function named product_plus_2 but docstring asks for sum).",
            "model_size": null,
            "test_battery_name": "Attribute substitution (conflicting function name vs docstring)",
            "test_description": "Prompts request a function implementing operation A but request a conflicting function name describing operation B; tests whether model follows simpler cue (name) over full specification (cognitive domain: heuristic substitution / reliance on salient attribute).",
            "llm_performance": "On 90 MathEquation prompts where desired solution and requested function name differ, Codex's accuracy fell from 100% (no conflicting name) to 4.4%–4.6% (depending on where the conflicting name was placed). Between 52% and 80% of prompts resulted in Codex returning the behavior matching the (contradictory) function name rather than the requested operation.",
            "human_baseline_performance": null,
            "performance_comparison": "Codex frequently relies on the simpler/most salient cue (the function name) rather than the full instruction, consistent with attribute substitution behavior in humans.",
            "experimental_details": "Tested three placements for the contradictory name (in docstring, in signature below docstring, or above the docstring) and non-instructional prompt controls; used 90 prompts covering operations (sum,difference,product) × various numeric suffixes; greedy decoding.",
            "limitations_or_caveats": "No direct human baseline for these codified prompts; results are sensitive to prompt format (placement of name) though effect persisted across placements. The stimuli are synthetic MathEquation prompts which may differ from typical real-world code repositories.",
            "uuid": "e9089.5",
            "source_info": {
                "paper_title": "Capturing Failures of Large Language Models via Human Cognitive Biases",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Codex High-Impact Deletions",
            "name_full": "High-impact erroneous deletions elicited via attribute substitution",
            "brief_description": "Using attribute-substitution-inspired prompts to elicit dangerous, high-impact behavior from Codex: incorrect file-deletion code that deletes files it should not (e.g., deletes files containing any of a set rather than all).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (davinci-001)",
            "model_description": "Codex prompted to 'delete all files in an inputted directory that import all of [package set]'; model outputs examined for harmful simplifications (e.g., checking only first package or any package instead of all).",
            "model_size": null,
            "test_battery_name": "Attribute substitution → high-impact deletion test",
            "test_description": "Prompts present conjunctions of package-import checks (A ∧ B ∧ ...); tests whether model simplifies to checking only a subset (A or A ∨ B) causing erroneous deletion (cognitive domain analog: attribute substitution / simplification under complexity).",
            "llm_performance": "When the number of package imports ≥ 3, Codex erroneously deletes files on at least ~80% of prompts; for number of packages ≤ 2 Codex produced a correct output on ~90% of prompts. As the number of packages increases, Codex more frequently simplifies to checking only the first package (A) rather than the conjunction.",
            "human_baseline_performance": null,
            "performance_comparison": "Codex exhibits a propensity to simplify conjunctive checks into easier-to-compute heuristics, producing high-impact erroneous behavior (deleting files) under realistic-seeming prompts; this is consistent with attribute substitution observed in humans but with severe practical consequences.",
            "experimental_details": "Constructed prompts instructing deletion of files that import a specified set of packages; authors classified outputs into correct (check all packages), deletes on first package only (A), or deletes on any package (A ∨ B), and measured frequency as a function of number of packages; control experiments used more descriptive docstrings and observed qualitatively similar patterns though with more low-impact errors.",
            "limitations_or_caveats": "These experiments are potentially harmful (authors note misuse risk). The test is an adapted cognitive-bias probe targeted at code generation and has no human baseline in the paper; generalization beyond the tested prompt class and to other models may vary.",
            "uuid": "e9089.6",
            "source_info": {
                "paper_title": "Capturing Failures of Large Language Models via Human Cognitive Biases",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measures of anchoring in estimation tasks",
            "rating": 2
        },
        {
            "paper_title": "The framing of decisions and the psychology of choice",
            "rating": 2
        },
        {
            "paper_title": "Judgment under uncertainty: Heuristics and biases",
            "rating": 2
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 2
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 1
        },
        {
            "paper_title": "What makes good in-context examples for GPT-3",
            "rating": 1
        }
    ],
    "cost": 0.023244749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Capturing Failures of Large Language Models via Human Cognitive Biases</h1>
<p>Erik Jones<br>UC Berkeley<br>erjones@berkeley.edu</p>
<p>Jacob Steinhardt<br>UC Berkeley<br>jsteinhardt@berkeley.edu</p>
<h4>Abstract</h4>
<p>Large language models generate complex, open-ended outputs: instead of outputting a class label they write summaries, generate dialogue, or produce working code. In order to asses the reliability of these open-ended generation systems, we aim to identify qualitative categories of erroneous behavior, beyond identifying individual errors. To hypothesize and test for such qualitative errors, we draw inspiration from human cognitive biases-systematic patterns of deviation from rational judgement. Specifically, we use cognitive biases as motivation to (i) generate hypotheses for problems that models may have, and (ii) develop experiments that elicit these problems. Using code generation as a case study, we find that OpenAI's Codex errs predictably based on how the input prompt is framed, adjusts outputs towards anchors, and is biased towards outputs that mimic frequent training examples. We then use our framework to elicit high-impact errors such as incorrectly deleting files. Our results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Recent large language models have achieved new, exciting capabilities. In contrast to traditional classifiers, these models can generate open-ended text, enabling use cases like summarization [Stiennon et al., 2020], dialog [Thoppilan et al., 2022], and code generation [Chen et al., 2021].
The open-ended power of these systems, however, poses new reliability challenges. We must understand not only when systems err, but also the kinds of errors they make, as some errors are much more costly than others. For example, erroneous code that does not compile is less dangerous than code that deletes all files in the home directory. Studying how frequently an error occurs is difficult, as the same error (e.g. delete all files) can appear in a wide range of syntactically diverse outputs. In order to better reason about how complex systems err, we need methods to test whether systems make the same qualitative error across different prompts, even when the generated outputs differ.
To study these reliability challenges, we primarily focus on code generation models. Such models complete programs from comments, descriptions of code functionality, or initial lines of code. Code generation is particularly amenable to study since it is objective: generated solutions are unambiguously correct or incorrect. Yet it is also open-ended: the set of programs a model could output is arbitrarily large, so the rate at which a specific program is outputted is not very descriptive.
Many of the reliability challenges posed by code generation models, and open-ended systems broadly, also arise when studying qualitative failures in human decision making. These failures, called cognitive biases, are systematic ways in which humans deviate from rational judgment [Tversky and Kahneman, 1974]. For example, Tversky and Kahneman find that humans inadequately adjust</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of our experimental framework. We use a cognitive bias (framing effect) to inspire a potential code generation failure mode (relying on irrelevant information). We then transform inputs in a way that we suspect will elicit the failure mode (prepending sum). We evaluate whether the modifications lower accuracy, and if the output is an instance of the targeted failure mode.</p>
<p>Estimates away from initial values, and disproportionately recall distinctive examples. To uncover cognitive biases, Tversky and Kahneman ask questions that are crafted to systematically reveal some qualitative irrationality. They uncover insights into human behavior from the diverse responses, without complete mechanistic insight into the minds that they aim to analyze.</p>
<p>In this work, we extend Tversky and Kahneman's experimental methodology and results to elicit failure modes of large code and language models, without relying on complete mechanistic insight into their behavior (Figure 1). Given a potential failure mode (e.g., relying on irrelevant information in the input), we construct a transformation over inputs that largely preserves semantics, but that we suspect will elicit the failure (e.g., prepending an irrelevant function). We first test if the model is sensitive to the transformation, by measuring if it decreases accuracy. Then, we check that the model outputs have elements that are indicative of the targeted failure (e.g., copies of the irrelevant function).</p>
<p>We draw on four different cognitive biases to hypothesize potential failures of OpenAI's Codex [Chen et al., 2021] and Salesforce's CodeGen [Nijkamp et al., 2022], then apply our framework to each. Our results indicate that these models often rely on irrelevant information when generating solutions, adjust solutions towards related-but-incorrect solutions, are biased based on training-set frequencies, and reverts to computationally simpler problems when faced with a complex calculation. We also apply our framework to OpenAI's GPT-3 [Brown et al., 2020], and show that it updates its predictions towards anchors, and predictably adjusts its responses based on the question framing.</p>
<p>Finally, we show that our framework can uncover <em>high-impact errors</em>: errors that are harmful and difficult to undo. Specifically, we use our framework to systematically generate prompts where Codex erroneously deletes files. Our results indicate that experimental methodology from cognitive science can help uncover failure modes of complex machine learning systems.</p>
<h2>2 Related Work</h2>
<p>Large language models. Recent work has developed large, capable, autoregressive language models, which predict future tokens from past tokens [Radford et al., 2019; Wang and Komatsuzaki, 2021; Brown et al., 2020; Chen et al., 2021; Rae et al., 2021]. These models can be used for open-ended generation tasks such as summarization [Stiennon et al., 2020; Ziegler et al., 2019; Rothe et al., 2020], dialogue [Ram et al., 2018; Thoppilan et al., 2022], and long-form question answering [Fan et al., 2019], among others. Model-generated code has been used to solve both programming and statistics questions [Chen et al., 2021; Tang et al., 2021].</p>
<p>There is some existing work studying failures of large language models. Benchmarks that measure model performance on multiple choice questions [Wang et al., 2019b, a; Hendrycks et al., 2021b], mathematics [Hendrycks et al., 2021c; Cobbe et al., 2021], long-form question answering [Lin et al., 2021; Gabriel et al., 2021; Shuster et al., 2021; Krishna et al., 2021], and coding problems [Hendrycks et al., 2021a; Chen et al., 2021] reveal inputs that the model errs on, but not the kind of error it makes. Another line of work shows that test-based language models can internalize bias and stereotypes [Sheng et al., 2019; Nadeem et al., 2020; Groenwold et al., 2020; Blodgett et al., 2021; Gehman et al., 2020], and propose applying fairness measurements from cognitive-social sciences to machine learning systems [Jacobs and Wallach, 2021]. Some work adversarially prompts models to leak training data [Carlini et al., 2020], or output specific content [Wallace et al., 2019; Carlini et al.,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Left. Example of a HumanEval problem from Chen et al. [2021]. The problem contains a prompt (blue), a canonical solution to the prompt (green), and a few test-cases (black). The prompt contains two components: a function signature (first line), and a docstring (remaining lines). Right. Illustration of our framing experiment. The transformed prompt (everything above the black line) contains an irrelevant preceding function (IPF) prepended to a prompt from HumanEval (blue). The IPF contains a randomly chosen prompt from HumanEval (purple) and a framing line (red). The output Codex generates (below the black line) matches the framing line. When we omit the random HumanEval prompt and the framing line (leaving only blue), Codex produces the correct output.
2020]. And a final line of work identifies additional potential failures of current and future machine learning systems [Bender et al., 2021, Bommasani et al., 2021, Weidinger et al., 2021].</p>
<p>Cognitive biases. Tversky and Kahneman [1974] define human cognitive biases: systematic patterns of deviation from rational judgment. They observe that humans employ heuristics when computing probabilities or assessing values, and that these heuristics lead to predictable errors. Follow-up work has added to, refined, and validated the set of known cognitive biases [Tversky and Kahneman, 1973, 1981, Strack et al., 1988, Kahneman and Frederick, 2002, Windhager et al., 2010, Meyer, 2014].</p>
<p>Some known failure modes of large language models resemble cognitive biases. Zhao et al. [2021] and Liu et al. [2021] show that the specific random samples used for few-shot learning can change GPT-3's prediction on binary and multiple choice tasks. Similarly, Wallace et al. [2019] show that innocuous prompts can routinely generate toxic model output. Our framework builds on this work by (i) identifying the link to cognitive biases, (ii) focusing on open-ended generation, and (iii) leveraging Tversky and Kahneman's experimental methodology to elicit qualitative failure modes.</p>
<h1>3 Code Generation Experiments</h1>
<h3>3.1 Models</h3>
<p>We study two code models: OpenAI's Codex [Chen et al., 2021], and Salesforce's CodeGen. [Nijkamp et al., 2022]. Both models are autoregressive-given a sequence of previous tokens, they predict the next token. Practitioners query these code models with partial programs, docstrings, or function signatures, and obtain completions as output.
Codex. We study OpenAI's Codex, a large language model trained to generate code from docstrings [Chen et al., 2021]. We use the OpenAI API to query the "davinci-001" version of Codex, and use greedy decoding to generate solutions. Details of this model architecture are not public, but it is likely similar to the largest model from Chen et al. [2021]: a 12B parameter version of GPT-3 [Brown et al., 2020] that is fine-tuned on GitHub instead of the CommonCrawl.</p>
<p>CodeGen. We additionally study the 6.2 billion parameter "mono" version of CodeGen, which is trained on text data and fine-tuned on GitHub. Unlike Codex, the weights of CodeGen are publicly available, ${ }^{2}$ so we run inference locally. We use greedy decoding to generate solutions.</p>
<h3>3.2 Benchmarks</h3>
<p>In order to identify whether code models some failure mode, we need to generate prompts that elicit that failure. To do so, we systematically apply transformations to standard prompts. We use two benchmarks as sources of prompts to transform: HumanEval, and MathEquations.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Framing Line</th>
<th>Model</th>
<th>Functional accuracy</th>
<th></th>
<th>Outputs framing line</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Original</td>
<td>Framed</td>
<td>Original</td>
<td>Framed</td>
</tr>
<tr>
<td>raise NotImplemented</td>
<td>CODEX</td>
<td>32.9</td>
<td>2.4</td>
<td>1.4</td>
<td>91.7</td>
</tr>
<tr>
<td></td>
<td>CODEGEN</td>
<td>25.6</td>
<td>1.5</td>
<td>0.0</td>
<td>79.3</td>
</tr>
<tr>
<td>pass</td>
<td>CODEX</td>
<td>32.9</td>
<td>3.0</td>
<td>9.7</td>
<td>92.7</td>
</tr>
<tr>
<td></td>
<td>CODEGEN</td>
<td>25.6</td>
<td>2.1</td>
<td>0.0</td>
<td>78.7</td>
</tr>
<tr>
<td>assert False</td>
<td>CODEX</td>
<td>32.9</td>
<td>3.3</td>
<td>0.0</td>
<td>92.7</td>
</tr>
<tr>
<td></td>
<td>CODEGEN</td>
<td>25.6</td>
<td>4.2</td>
<td>0.1</td>
<td>72.6</td>
</tr>
<tr>
<td>return False</td>
<td>CODEX</td>
<td>32.9</td>
<td>4.9</td>
<td>11.5</td>
<td>65.6</td>
</tr>
<tr>
<td></td>
<td>CODEGEN</td>
<td>25.6</td>
<td>3.6</td>
<td>0.0</td>
<td>64.6</td>
</tr>
<tr>
<td>print("Hello world!")</td>
<td>CODEX</td>
<td>32.9</td>
<td>10.6</td>
<td>0.0</td>
<td>62.2</td>
</tr>
<tr>
<td></td>
<td>CODEGEN</td>
<td>25.6</td>
<td>11.0</td>
<td>0.0</td>
<td>58.2</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of the framing experiments. We compare functional accuracy and the rate at which framing line is outputted over HumanEval with (framed) and without (original) irrelevant preceding functions. We find that the irrelevant preceding functions lower functional accuracy across all framing lines for Codex and CodeGen. Moreover, we find that the outputted function often appears verbatim in the generated output, suggesting that both models rely on irrelevant information in the prompt.</p>
<p>HumanEval. We use the HumanEval benchmark as a diverse source of "normal" prompts [Chen et al., 2021]. HumanEval contains 164 programming problems, each of which includes a function signature and a docstring. The docstring contains an English description of the desired functionality and a few example input-output pairs. HumanEval also contains a canonical solution for each program, which we use in Section 3.3.2. We give an example problem from HumanEval in Figure 2.</p>
<p>MathEquations. We also curate a set of prompts of basic arithmetic functions. For example, we prompt Codex to "Write a function that sums the squares of its inputs", or "Write a function that sums its inputs called product_plus_five". Further details are given in Sections 3.3.3 and 3.3.4.</p>
<h1>3.3 Empirical results</h1>
<p>In this section, we show how cognitive biases can (i) inspire hypotheses for potential failure modes, and (ii) help us design experiments to test these hypotheses. Our approach has three steps. First, we construct a transformation over prompts that largely preserves semantics, but that we suspect will elicit a specific cognitive-bias-inspired failure mode. Next, we measure if code models are sensitive to the transformation, by measuring the decrease in accuracy. And finally, we check that the generated output has elements that are indicative of the targeted failure mode. Our approach mirrors the high-level methodology from Tversky and Kahneman [1974]; we empirically elicit specific failure modes using targeted prompts, without complete mechanistic insight into the system that we study.
We draw inspiration from four cognitive biases: the framing effect (Tversky and Kahneman [1981]; Section 3.3.1), anchoring (Tversky and Kahneman [1974]; Section 3.3.2), the availability heuristic (Tversky and Kahneman [1973]; Section 3.3.3), and attribute substitution (Kahneman and Frederick [2002]; Section 3.3.4).</p>
<h3>3.3.1 Inspiration: Framing effect</h3>
<p>We first draw inspiration from the framing effect: predictable shifts in human responses when the same problem is framed in different ways [Tversky and Kahneman, 1981]. In their study identifying the effect, Tversky and Kahneman [1981] find that subjects favor certainly saving 200 people over saving 600 with probability $1 / 3$, yet prefer losing 600 with probability $2 / 3$ over certainly losing 400 (even though these are equivalent). At its core, the framing effect shows how humans can rely on semantically irrelevant information when they make decisions.
Using the framing effect as inspiration, we hypothesize that code generation models may generate solutions exclusively from irrelevant information in the prompt. To elicit this failure, we transform HumanEval prompts by prepending irrelevant preceding functions. Specifically, to generate irrelevant</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of our anchoring experiment using a real example (expanded in Figure 7). We construct the anchor function (left) by taking the function signature from the HumanEval prompt (blue), appending $n$ lines of the canonical solution (green), then adding anchoring lines (red). We construct the full prompt (center) by combining the anchor function, the original HumanEval prompt, and the first $n$ lines of the canonical solution. The solution Codex generates (right) combines elements of a canonical solution (checks condition and adds to ret.), with the anchor function (for var loop).
preceding functions, we combine a random prompt from HumanEval with a framing line. We test five framing lines: raise NotImplementedError, pass, assert False, return False, and print("Hello world!"). We first check that prepending these irrelevant preceding functions decreases functional accuracy. ${ }^{3}$ Next, to test if models relied on irrelevant information in the prompt, we measure how much more frequently the framing line appears verbatim in the generated output.
We report the results of our framing experiments in Table 1. We find that adding irrelevant preceding functions consistently lowers functional accuracy, by between 22.3 and 30.5 points for Codex, across the different framing lines we tested. Moreover, both models frequently generate the framing line: $81 \%$ of the time for Codex and $70.7 \%$ of time for CodeGen, compared to only $4.5 \%$ and $0.0 \%$ over untransformed prompts respectively. These results suggest that code generation models can erroneously rely on irrelevant information in the prompt in predictable ways, even in the extreme case when doing so contradicts the type specification in the function signature (return False).</p>
<h1>3.3.2 Inspiration: Anchoring</h1>
<p>We next draw inspiration from anchoring: humans' tendency to insufficiently adjust their estimates away from initial values. For example, Tversky and Kahneman [1974] find that subjects' median estimate for the fraction of African countries in the UN shifts from $25 \%$ to $45 \%$, based on whether they were first asked if the fraction was greater or less than $10 \%$ and $65 \%$, respectively. Anchoring captures how humans adjust to partial information, versus irrelevant information (framing effect).
Using anchoring as inspiration, we hypothesize that code generation models may adjust their output towards related solutions, when these solutions are included in the prompt. To elicit this failure, we prepend anchor functions to prompts: functions that are similar to a valid solution for a HumanEval prompt, but contain some error. We first check that prepending these anchor functions decreases functional accuracy, as in Section 3.3.1. Next, to test if models adjust their output towards related solutions, we check that the generated solution contains elements of the anchor function.
We aim to construct anchor functions that are similar to functions in HumanEval prompts and that compile, but are incorrect. To do so, we take a prefix of the canonical solution, then add additional anchor lines that produce an incorrect output. See Figure 3 for an example. We describe two types of anchor lines, and how we test their influence on the generated solutions, in the following paragraphs.
Print-var anchor lines. We first study print-var anchor lines, which iterate over all variables in the function signature and print their values. For a function with inputs var1 and var2, the associated print-var anchor lines are:</p>
<div class="codehilite"><pre><span></span><code><span class="k">for</span><span class="w"> </span><span class="k">var</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="p">[</span><span class="n">var1</span><span class="p">,</span><span class="w"> </span><span class="n">var2</span><span class="p">]:</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="k">var</span><span class="p">)</span>
</code></pre></div>

<p>To study the influence of the print-var anchor lines on the solution, we measure how often (i) just the first line (for loop), and (ii) just the second line (print statement) appear in the generated solution.
Add-var anchor lines. We also study add-var anchor lines, which return the sum of all variables in the function signature (converted to strings). For a function with inputs var1 and var2, the add-var anchor lines are:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results of the print-var anchoring experiment. Left. We measure the functional accuracy of Codex (top) and CodeGen (bottom) with no anchor function prepended (baseline acc) and with a printvar anchor function prepended (anchor acc), and find that prepending the anchor function consistently lowers accuracy. Right. We measure the influence of the anchor function on the generated solution by plotting the fraction of generated solutions that contain "for var in" from the print-var anchor prompt (for var loop), the fraction of generated solutions that include "print (var)" (prints var), and the fraction of generated solutions that output the anchor function verbatim without additional content (exact copy), as a function of the number of canonical solution lines added to the prompt.
tmp = str(var1) + str(var2)
return tmp
To study the influence of the add-var anchor lines on the solution, we measure how often return tmp appears in the generated solution.</p>
<p>Print-var results. In Figure 4, we show that prepending print-var anchor functions consistently lowers Codex and CodeGens' functional accuracies across different number of prompted canonical solution lines. We vary the number of canonical solution lines to study prompts of different difficulties; as the number of solution lines increases, the number remaining lines models must produce decreases. ${ }^{4}$
We additionally find that elements of anchor function often appear in both models' outputs, suggesting that code generation models adjust their solutions towards related solutions. In Figure 4, we see that Codex generates for var in $32 \%-61 \%$ of solutions when at least one line of the canonical solution is included, and generates print (var) in $26 \%-44 \%$ of solutions. CodeGen's behavior is qualitatively similar. Both models sometimes even incorporate the anchor lines into correct solutions; on Codex, the for var loop is used in a correct solution for $3 \%-11 \%$ of all outputs, while print (var) is used in a correct solution for $1 \%-9 \%$ of outputs.
Control experiments. One concern might be that models just outputs the anchor function verbatim, as in Section 3.3.1, but we find that this does not explain the full results-both models include anchor lines in many solutions that do not copy the anchor function verbatim. We also find that changing the name of the anchor function leads to only negligible changes; see Appendix A. 1 for details.</p>
<p>Add-var results. We next consider results for add-var anchor lines. Full results for the add-var anchor prompts are presented in Appendix A. 1 and are qualitatively similar to the print-var results.
One again, we find that prepending the anchor function consistently lowers functional accuracy. Moreover, the outputted solutions often include an anchor line. For example, Codex and CodeGen generate return tmp in $26 \%-46 \%$ and $13 \%-79 \%$ of solutions respectively, depending on how many canonical solution lines we prompt with. These results are not caused by models outputting the anchoring function verbatim: this only occurs between $7 \%$ and $12 \%$ of the time for Codex, and $4 \%$ and $12 \%$ for CodeGen. Overall, our findings suggest that code generation models can err by adjusting its output towards related solutions, when the solutions are included in the prompt.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<div class="codehilite"><pre><span></span><code>&quot; &quot; &quot;
Write a function that sums its
inputs called product_plus_2
&quot; &quot; &quot;
def product_plus_2(x, y):
    return x * y + 2
</code></pre></div>

<p>Figure 5: Left. Availability heuristic example where Codex mixes up the order of operations. The correct function signature (blue), square_sum matches the prompt. However, the incorrect function call (red) instead squares its inputs before summing them. The prompt is above the horizontal line, while the generated code is below. Right. Attribute substitution example where Codex relies on the function name to generate output. Codex correctly generates the desired function name (blue), but errs by using the function name instead of the prompt to generate the return statement (red).</p>
<h1>3.3.3 Inspiration: Availability heuristic</h1>
<p>We next draw inspiration from the availability heuristic: the tendency of humans to evaluate how frequently an example occurs based on how easy it is to recall. For example, Tversky and Kahneman [1973] find that humans tend to incorrectly report that there are more first words that start with " $r$ " and " $k$ " than have third letter " $r$ " and " $k$ ", because the former quickly come to mind.
Using the availability heuristic as motivation, we hypothesize that code generation models may err by outputting solutions to related prompts that appear more frequently in the training set. To elicit this failure, we start with prompts that apply a unary operation before a binary operation (unary-first), then flip the order (binary-first). Programmers tend to apply unary operations first (e.g. when computing Euclidean distances or variances), so we conjecture that they appear more frequently on GitHub. We first check that flipping the order of operations decreases accuracy. Next, to test if code generation models instead outputs related prompts that occur more frequently in the training set, we measure whether code generation models instead output the unary-first solution.</p>
<p>We consider all 12 combinations of the binary operations sum, difference, and product, with unary operations square, cube, quadruple, and square root. Focusing on Codex, ${ }^{5}$ we find that accuracy drops from $50 \%$ to $17 \%$ when flipping the order from unary-first to binary-first. Among combinations where flipping the order leads to error, we find that $75 \%$ of the binary-first outputs are the unary-first solution. We exhibit one such error in Figure 5: when prompted to square the sum of its inputs, Codex generates the correct function name (square_sum ), but reverses the order of operations. Our results suggest that Codex can err by outputting solutions to related, frequent prompts in the training set.
Control experiments. One worry is that the dip in performance is due the instructional nature of our prompts. We rule this out by evaluating Codex on prompts where the docstring appears beneath the function signature and is a definition rather than command, to more closely mimic some functions on GitHub. We obtain qualitatively similar results on these prompts, see Appendix A. 4 for details.</p>
<h3>3.3.4 Inspiration: Attribute substitution</h3>
<p>Finally, we draw inspiration from attribute substitution: the human tendency to respond to a complicated question using a simpler, related question [Kahneman and Frederick, 2002]. For example, a professor when asked how likely a candidate is to be tenured, may instead respond with how impressive they found their job talk.
Using attribute substitution as inspiration, we hypothesize that Codex may use simple-but-incorrect heuristics to generate solutions. To elicit this failure, we add requests for conflicting function names to MathEquation prompts. For example, in Figure 5 we prompt Codex to write a program that sums its inputs called product_plus_2. We first check that adding conflicting function names decreases Codex's functional accuracy. Next, to test if Codex uses simple-but-incorrect heuristics to generate solutions, we check whether the generate solution matches the function name.
We evaluate Codex using 90 MathEquation prompts where the desired solution and requested function name differ. To construct prompts, we begin with a prompt that Codex originally solves</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Name location</th>
<th>Correct</th>
<th>Matches function name</th>
<th>Other error</th>
</tr>
</thead>
<tbody>
<tr>
<td>No name</td>
<td>100.0</td>
<td>-</td>
<td>0.0</td>
</tr>
<tr>
<td>Docstring</td>
<td>4.4</td>
<td>80.0</td>
<td>15.6</td>
</tr>
<tr>
<td>Function signature</td>
<td>4.4</td>
<td>70.0</td>
<td>25.6</td>
</tr>
<tr>
<td>Name first</td>
<td>4.6</td>
<td>51.7</td>
<td>43.7</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of the attribute substitution experiments. We report accuracy when we do not request a contradictory function name (no name), we request a function name in the docstring (docstring), in the function signature below the docstring (function signature), or above the docstring (name first). Overall, we find that Codex frequently generates solutions based on the function name.
(sum, difference, or product), then append a request for a specific, contradictory function name (see Appendix A. 4 for full implementation details).</p>
<p>We report our experimental results in Table 2. When we request a conflicting function name, Codex's accuracy drops from 100% to only 4.4%-4.6%. This finding holds whether we request the function name in the docstring, write it in the function signature below the docstring, or write the function name over a simple description on the function. Moreover, for between 52% and 80% of prompts, Codex responds with the function specified in the function name. Our results indicate that Codex can err by using simple-but-incorrect heuristics to generate solutions.</p>
<h1>4 GPT-3 Results</h1>
<p>In this section, we extend our study from Codex to GPT-3. To test GPT-3 for failure modes, we try to faithfully reproduce and extend the anchoring experiment of Jacowitz and Kahneman [1995] and framing effect experiment of Tversky and Kahneman [1981].</p>
<p>Anchoring. As in Section 3.3.2 we study Section 3.3.2, we study anchoring: humans' tendency to insufficiently adjust their estimates away from an initial value [Tversky and Kahneman, 1974]. We largely replicate the anchoring study presented in Jacowitz and Kahneman [1995], but test the "davinci-001" version of OpenAI's GPT-3 instead of humans.</p>
<p>In their original experiment, Jacowitz and Kahneman asked students to estimate quantities such as the length of the Mississippi river in miles. They then asked new students to estimate the same quantities, but first gave them a upper or lower bound on the true answer (e.g. the Mississippi river is longer than 700 miles), which they call anchors. They find that students tend to underestimate the true quantity when prompted with the lower anchor, and overestimate it when prompted with the upper anchor.
We adapt the anchoring study from Jacowitz and Kahneman [1995] by finding the true answer for 14 of their 15 original questions ${ }^{6}$, then computing upper and lower anchors by increasing and decreasing the true answer by a fixed percentage $p$. See Appendix B. 1 for a full list of questions and true answers. As an example, if the actual answer is 2000 and $p$ is 50\%, the upper anchor is 3000 and the lower anchor is 1000. We use this bound as an anchor, so that a typical prompt might be:</p>
<p>The length of the Mississippi river (in miles) is greater than 1000. anchor</p>
<p>What is the length of the Mississippi River (in miles)? Answer:
To study anchoring in GPT-3, we measure how prepending the anchor changes GPT-3's estimate. We categorize four potential changes: the estimate does not change, the estimate shifts towards the anchor, the estimate shifts away from the anchor, and the estimate is gibberish. We report the results in Table 3 for $p \in{20 \%, 50 \%}$. We find that GPT-3 routinely updates its estimate when an anchor is prepended, and tends to shift the estimate towards the anchor. We also find that while GPT-3's updated estimate sometimes matches the anchor exactly ( $67 \%$ of the time), it also often lands between the anchor and the original prediction, mirroring the behavior of humans.</p>
<p>Our replication has a few limitations. Like the original study our sample size is small, we construct prompts with templates, and many of the outputs-on average 41\%-are gibberish. Nevertheless, our results suggest that GPT-3 incorporates the anchor during estimation.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Anchor change to model output</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>$p$</td>
<td>No change</td>
<td>Towards anchor</td>
<td>Away from anchor</td>
<td>Gibberish</td>
</tr>
<tr>
<td>20%</td>
<td>10.7</td>
<td>28.6</td>
<td>10.7</td>
<td>50.0</td>
</tr>
<tr>
<td>50%</td>
<td>14.3</td>
<td>42.9</td>
<td>10.7</td>
<td>32.1</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of the adaptation of the anchoring study from Jacowitz and Kahneman [1995] on GPT-3. We consider anchors that are 20% and 50% increases and decreases from the ground truth answer, and measure how often GPT-3's revised prediction does not change, shifts towards / away from the anchor, or is gibberish, aggregated across lower and upper anchors.</p>
<div class="codehilite"><pre><span></span><code><span class="o">***</span>
<span class="n">Delete</span> <span class="nb">all</span> <span class="n">files</span> <span class="ow">in</span> <span class="n">an</span> <span class="n">inputted</span> <span class="n">directory</span> <span class="n">that</span> <span class="kn">import</span><span class="w"> </span><span class="nn">all</span>
<span class="n">of</span> <span class="n">statsmodels</span><span class="p">,</span> <span class="n">plotly</span><span class="p">,</span> <span class="n">seaborn</span><span class="p">,</span> <span class="ow">and</span> <span class="n">scipy</span>
<span class="o">***</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="k">def</span><span class="w"> </span><span class="nf">delete_all</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">root</span><span class="p">,</span> <span class="nb">dir</span><span class="p">,</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">walk</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">f</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s2">&quot;import statsmodels&quot;</span><span class="p">,</span> <span class="n">fp</span><span class="o">.</span><span class="n">read</span><span class="p">()):</span>
                <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">f</span><span class="p">))</span>
</code></pre></div>

<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Left. Example where Codex incorrectly deletes files. We prompt Codex to delete files containing all of statsmodels, plotly, seaborn, and scipy. Codex correctly iterates through all files in the inputted directory (blue), but then incorrectly deletes all files containing statsmodels (red), as attribute substitution suggests. Right. Plot describing the errors Codex makes as a function of the number of packages. We find that Codex often incorrectly deletes files if they contain any of the listed packages, and relies more on just the first package as the number of packages increases.</p>
<p>Framing effect. As in Section 3.3.1, we study the framing effect: predictable shifts in human responses when the same problem is framed in different ways. We largely replicate the framing experiment presented in Tversky and Kahneman [1981]: we compare GPT-3's responses to two equivalent decisions: choosing to either deterministically save (or let die) some fraction of a population, or to probabilistically save (let die) the whole population.</p>
<p>We measure the rate at which GPT-3 chooses the probabilistic option across different population sizes and different fractions / probabilities. See Section B.2 for full results. When using the probability in the original study, GPT-3 qualitatively mirrors humans: it chooses the probabilistic option far more frequently under the "not save" framing than under the "save framing". However, for higher probabilities, GPT-3 consistently chooses the probabilistic option for both framings; we conjecture that humans could exhibit similar behavior in this regime, since the probabilistic option is more certain. Overall, our results suggest that GPT-3 selects different options based on the framing, and could be a test-bed to identify qualitative human behaviors without running full human studies.</p>
<h1>5 High-Impact Errors</h1>
<p>We have shown how our framework helps us elicit failures of large language models. In this section, we use our framework to construct cases where Codex makes high-impact errors: harmful errors that are hard to undo. Specifically, we construct prompts where Codex incorrectly deletes files.</p>
<p>As in Section 3.3.4 we draw inspiration from attribute substitution: the tendency of humans to respond to a complex question with a simpler, related question. Using attribute substitution as motivation, we hypothesize that Codex may simplify complex expressions such as conjunctions. Instead of checking all components of a conjunction at once, it might "give up" and consider subsets of the components individually (e.g. checking for $A$ or $A \vee B$ instead of $A \wedge B$ ). To elicit this failure, we prompt Codex to delete files containing specific sets of package imports; see Figure 6 for an example. We measure how often Codex generates a simpler output that erroneously deletes files, as well as how often it produces the correct output. See Appendix C for additional details.</p>
<p>We test for two types of simpler outputs: code deleting all files containing first package in the set (i.e. $A$ instead of $A \wedge B$ ), and code deleting all files containing any package in the set (i.e. $A \vee B$</p>
<p>instead of $A \wedge B$ ). The latter operation is computationally simpler than checking if a file contains all packages, since Codex can delete a file whenever a single package in the set appears.
In Figure 6, we illustrate the breakdown of the errors Codex makes as a function of the number of package imports in the prompt. We find that Codex erroneously deletes files on at least $80 \%$ of prompts when the number of package imports is at least three, despite producing a correct output on $90 \%$ of prompts when the number of packages is at most two. Moreover, we find that Codex increasingly errs by using only the first package as the problem gets more challenging (i.e. the number of packages increases), as attribute substitution predicts.
Control experiments. To very that our findings generalize to different classes of realistic prompts, we test Codex on prompts containing a descriptive docstring beneath the function signature delete_all_with_libraries(directory). We observe qualitatively similar results, though we find more instances of low-impact errors; see Appendix C for details. Overall, our results demonstrate how our framework can preemptively elicit high-impact errors, like erroneous deletions.</p>
<h1>6 Discussion</h1>
<p>In this work, we identify and test for classes of errors that open-ended generation systems can make, using cognitive biases as motivation. To do so, we generate hypotheses for potential qualitative failure modes, then construct transformations over prompts that elicit these failures. Our experiments uncover deficiencies of Codex, CodeGen, and GPT-3, and elicit high-impact errors that are challenging to undo. While we focus on a few specific failure modes, future work could apply our framework to uncover additional failures. Moreover, our framework queries systems as a black-box, so it could be used to quickly probe for errors in future systems as they are released.
Some of our results highlight how optimizing likelihood could be at odds with human intent. For example, over GitHub, programs may more often match their function signature than docstring (Section 3.3.3), or tend to complete to pass if the preceding function does (Section 3.3.1). Nevertheless, our results elicit qualitative errors regardless of the "correct" behavior (i.e. even when what is incorrect and correct flips), and demonstrate the importance of documenting qualitative failures.
The reliability challenges posed by the open-ended generation systems that we study sometimes also apply to classifiers. Some classification errors can be more costly than others [Oakden-Rayner et al., 2020], classifiers may use irrelevant information to make predictions [Sagawa et al., 2020], and input-level transformations like universal adversarial triggers [Wallace et al., 2019] and distribution shifts [Hendrycks and Dietterich, 2019] induce errors. However, while classification errors may be succinctly summarized with a confusion matrix, generation errors cannot, since each output appears infrequently. To tame the large output space, our transformations must induce categories of errors that we can reliably measure. Despite this additional constraint, we are able to construct model-agnostic transformations: we do not use the training data, model parameters, or even output logits. Our success in this restricted setting demonstrates the comparative brittleness of completion systems.
We present a method to systematically elicit errors from large language models. While we believe our work is important to understand model behavior, bad actors could exploit the errors we reveal (e.g. by deleting files on systems with a Codex back-end). Nevertheless, we introduce new robustness challenges for developers and identify misuses of these models, which we feel supersedes this risk.
As a subroutine in our experimental pipeline, we use cognitive biases as inspiration to identify potential failure modes. This is an example of using a reference system-a system that is analogous to the ML models we study in some meaningful way-to generate insights into ML systems [Steinhardt, 2022]. We use humans as the reference, focusing specifically on their susceptibility to cognitive biases. Other references, such as complex systems or evolution, may uncover new errors and insights. Moreover, ML systems could additionally err in ways that known systems do not, so it will also be useful to have intrinsic methods for characterizing model errors. Overall, our work underscores the need for more extensive testing of generative ML systems before their widespread deployment.</p>
<h2>Acknowledgements</h2>
<p>We thank the anonymous reviewers, Ruiqi Zhong, Jean-Stanislas Denain, Aditi Raghunathan, Jessy Lin, and Lawrence Chan for feedback. This work was supported by NSF Award Grant no. 1804794.</p>
<h1>References</h1>
<p>Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchel. On the dangers of stochastic parrots: Can language models be too big? In ACM Conference on Fairness, Accountability, and Transparency (FAccT), 2021.</p>
<p>Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. Stereotyping norwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In Association for Computational Linguistics (ACL), 2021.</p>
<p>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.</p>
<p>Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. arXiv preprint arXiv:2012.07805, 2020.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Association for Computational Linguistics (ACL), 2019.</p>
<p>Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. GO FIGURE: A meta evaluation of factuality in summarization. In Findings of the Association for Computational Linguistics (Findings of ACL), 2021.</p>
<p>Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462, 2020.</p>
<p>Sophie Groenwold, Lily Ou, Aesha Parekh, Samhita Honnavalli, Sharon Levy, Diba Mirza, and William Yang Wang. Investigating african-american vernacular english in transformer-based text generation. In Empirical Methods in Natural Language Processing (EMNLP), 2020.</p>
<p>Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations (ICLR), 2019 .</p>
<p>Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with APPS. In Advances in Neural Information Processing Systems (NeurIPS), 2021a.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations (ICLR), 2021b.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Advances in Neural Information Processing Systems (NeurIPS), 2021c.</p>
<p>Abigail Z. Jacobs and Hanna Wallach. Measurement and fairness. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), 2021.</p>
<p>Karen E. Jacowitz and Daniel Kahneman. Measures of anchoring in estimation tasks. Personality and Social Psychology Bulletin, 21(11):1161-1166, 1995.</p>
<p>Daniel Kahneman and Shane Frederick. Representativeness revisited: Attribute substitution in intuitive judgment. In Heuristics and Biases: The Psychology of Intuitive Judgement, pages 49-81. 2002 .</p>
<p>Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. Hurdles to progress in long-form question answering. In North American Association for Computational Linguistics (NAACL), 2021.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3. arXiv preprint arXiv:2101.06804, 2021.</p>
<p>David E. Meyer. Semantic priming well established. Science, 345(6196):523-523, 2014.
Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020.</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huam Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. A conversational paradigm for program synthesis. arXiv preprint arXiv:2203.13474, 2022.</p>
<p>Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Ré. Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. In Proceedings of the ACM Conference on Health, Inference, and Learning, pages 151-159, 2020.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, J. Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, G. V. D. Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, I. Higgins, Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, D. Budden, Esme Sutherland, K. Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lorraine Li, A. Kuncoro, Aida Nematzadeh, E. Gribovskaya, Domenic Donato, Angeliki Lazaridou, A. Mensch, J. Lespiau, Maria Tsimpoukelli, N. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, I. Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv, 2021.</p>
<p>Ashwin Ram, Rohit Prasad, Chandra Khatri, Anu Venkatesh, Raefer Gabriel, Qing Liu, Jeff Nunn, Behnam Hedayatnia, Ming Cheng, Ashish Nagar, Eric King, Kate Bland, Amanda Wartick, Yi Pan, Han Song, Sk Jayadevan, Gene Hwang, and Ari Pettigrue. Conversational ai: The science behind the alexa prize. arXiv preprint arXiv:1801.03604, 2018.</p>
<p>Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. Leveraging pre-trained checkpoints for sequence generation tasks. Transactions of the Association for Computational Linguistics (TACL), 8:264-280, 2020.</p>
<p>Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. In International Conference on Learning Representations (ICLR), 2020.</p>
<p>Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a babysitter: On biases in language generation. In Empirical Methods in Natural Language Processing (EMNLP), 2019.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.</p>
<p>Jacob Steinhardt. Anchor weights for ML. Bounded Regret, 2022.
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In Advances in Neural Information Processing Systems (NeurIPS), 2020.</p>
<p>Fritz Strack, Leonard L. Martin, and Nobert Schwarz. Priming and communication: Social determinants of information use in judgments of life satisfaction. European Journal of Social Psychology, 18(5):429-442, 1988.</p>
<p>Leonard Tang, Elizabeth Ke, Nikhil Singh, Nakul Verma, and Iddo Drori. Solving probability and statistics problems by program synthesis. arXiv preprint arXiv:2111.08276, 2021.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.</p>
<p>Amos Tversky and Daniel Kahneman. Availability: A heuristic for judging frequency and probability. Cognitive Psychology, 5(2):207-232, 1973.</p>
<p>Amos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases. Science, 185(4157):1124-1131, 1974.</p>
<p>Amos Tversky and Daniel Kahneman. The framing of decisions and the psychology of choice. Science, 211(4481):453-458, 1981.</p>
<p>Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing NLP. In Empirical Methods in Natural Language Processing (EMNLP), 2019.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems (NeurIPS), 2019a.</p>
<p>Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations (ICLR), 2019b.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive language model, 2021.</p>
<p>Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359, 2021.</p>
<p>Sonja Windhager, Florian Hutzler, Claus-Christian Carbon, Elisabeth Oberzaucher, Katrin Schaefer, Truls Thorstensen, Helmut Leder, and Karl Grammer. Laying eyes on headlights: Eye movements suggest facial features in cars. Collegium Antropologicum, 34(3):1075-1080, 2010.</p>
<p>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning (ICML), 2021.</p>
<p>Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">common</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span><span class="w"> </span><span class="n">l2</span><span class="p">):</span>
<span class="w">    </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">()</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">el</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">l1</span><span class="p">:</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="k">var</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="p">[</span><span class="n">l1</span><span class="p">,</span><span class="w"> </span><span class="n">l2</span><span class="p">]:</span>
<span class="w">            </span><span class="nb">print</span><span class="p">(</span><span class="k">var</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">common</span><span class="p">(</span><span class="n">l1</span><span class="p">:</span><span class="w"> </span><span class="n">list</span><span class="p">,</span><span class="w"> </span><span class="n">l2</span><span class="p">:</span><span class="w"> </span><span class="n">list</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return sorted unique common elements for two lists.</span>
<span class="sd">    &gt;&gt;&gt; common([1, 4, 3, 653, 5], [5, 1, 5, 9, 653, 121])</span>
<span class="sd">    [1, 5, 653]</span>
<span class="sd">    &gt;&gt;&gt; common([5, 3, 2, 8], [3, 2])</span>
<span class="sd">    [2, 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">()</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">el</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">l1</span><span class="p">:</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="k">var</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="p">[</span><span class="n">l1</span><span class="p">,</span><span class="w"> </span><span class="n">l2</span><span class="p">]:</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">el1</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">var</span><span class="p">:</span>
<span class="w">                </span><span class="n">ret</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">el</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">sorted</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nv">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">set</span><span class="ss">()</span>
<span class="k">for</span><span class="w"> </span><span class="nv">el</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">l1</span>:
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nv">e2</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">l2</span>:
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nv">e1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nv">e2</span>:
<span class="w">            </span><span class="nv">ret</span>.<span class="nv">add</span><span class="ss">(</span><span class="nv">el</span><span class="ss">)</span>
<span class="k">return</span><span class="w"> </span><span class="nv">sorted</span><span class="ss">(</span><span class="nv">ret</span><span class="ss">)</span>
</code></pre></div>

<p>Figure 7: Actual example of how an anchor function impacts the generated solution. We construct the anchor function by taking the function signature from the HuamnEval prompt (blue), removing the docstring and variable typing, appending $n$ lines of the canonical solution (green), then adding anchoring lines (red). We prompt Codex with the anchor function, the HumanEval prompt, and the first $n$ lines of the canonical solution (above black line). The full canonical solution is on the right (green text, grey box). We see that the solution Codex generates (below black line) combines elements of the canonical solution (e.g. checks condition and adds to ret.), with the anchor function (e.g. for var loop).</p>
<h1>A Additional Details and Results for Code Generation Experiments</h1>
<p>In this section, we provide additional experimental details and results for the experiments in Section 3. We include additional details for anchoring (Appendix A.1), the availability heuristic (Appendix A.3), and attribute substitution (Appendix A.4).</p>
<h2>A. 1 Anchoring</h2>
<p>In this section, we include additional experimental details and results from the anchoring experiments in Section 3.3.2.</p>
<h2>A.1.1 Additional experimental details</h2>
<p>Filtering prompts for longer canonical solutions. In Section 3.3.2, we discussed how we filter out prompts whose entire solution would appear in the prompt. For example, if the canonical solution is 4 lines but our experiment calls for six, we omit the prompt. This leaves all 164 prompts for 0 canonical solution lines added, 127 for one line added, 117 for two lines added, 107 for three lines added,, 99 for four lines added, 82 for five lines added, 65 for six lines added, 55 for seven lines added, and 47 for eight lines added.</p>
<p>Additional prompt example. In Figure 3, we showed an example prompt and output from our anchoring experiment. We expand on these results in Figure 7 by including the entire prompt, and the canonical solution as reference.</p>
<p>Changing the anchor function name We additionally study anchoring experiments where the name of the anchor function and function to be completed differ. This is different from the experiment in Section 3.3.2 where the names of the anchor function and the function to be completed were the same. Unless otherwise noted, we append 1 to the name of the anchor function and 2 to the name of the function to be completed. We propagate this change to other instances of the function name in the</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Results of the add-vars anchoring experiment. Left. We measure the functional accuracy of Codex (top) and CodeGen (bottom) without an anchor function (baseline acc), the functional accuracy with an add-var anchor function prepended (anchor acc), and find that the anchor function consistently lowers accuracy. Right. We measure the influence of the anchor function on the generated solution by plotting the fraction of generated solutions that contain return tmp from the add-var anchor prompt (returns tmp), and the fraction of generated solutions that output the anchor function verbatim without additional content (exact copy), as a function of the number of canonical solution lines added to the prompt.
function signature of and the docstring. However, all components of the prompts from Section 3.3.2 remain unchanged.</p>
<h1>A. 2 Additional experimental results</h1>
<p>In this section we provide additional experimental results, including the add-var anchor function results, tables containing numbers used to generate plots, and the results of our experiments where the anchor function and function to be completed have different names.
Add-var results We first exhibit the results of the add-var anchor line experiments described in Section 3.3.2. In Figure 8, we plot the functional accuracy of prompts with (baseline) and without (anchor) prepended anchor functions for both Codex and CodeGen, and find that while the baseline functional accuracy increases, the anchor functional accuracy remains roughly constant. Moreover, we see that both models adjust their output to related-but-incorrect solutions; in the same plot, we see that our test for the anchor, the presence of return tmp consistently appears in the generated solutions, while both anchor lines rarely appear together.
Tabular results. We additionally report the full experimental results for print-var anchor functions and add-var anchor functions for both Codex and CodeGen. Table 4, and the full experimental results for add-var anchor function sin Table 5. These include more information than the figures, since we additionally include the fraction of prompts that are functionally correct and pass the anchor tests.
Control experiment: changing the function name. In Figure 9 we plot the results of the print-var anchoring experiment where we append 1 to the function name in the anchor function, and 2 to the function name of the function to be completed (see Table 6 for numerical results). We plot the analogous add-var results in Figure 10 and include full numerical results in Table 7. Both results are nearly identical to the results where the function name is shared presented in Section 3.3.2, and suggest that the shared function name is not responsible for our anchoring results.</p>
<h2>A. 3 Availability Heuristic</h2>
<p>In this section, we augment Section 3.3.3 with additional additional availability heuristic experiments that use non-instructional prompts. These prompts give the correct function name, add in variables $x$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Sol. lines</th>
<th style="text-align: center;">Anc. acc.</th>
<th style="text-align: center;">Prints</th>
<th style="text-align: center;">P. + pass</th>
<th style="text-align: center;">For var</th>
<th style="text-align: center;">F.v. + pass</th>
<th style="text-align: center;">Copy</th>
<th style="text-align: center;">No anc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CODEX</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">47.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">57.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">60.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">64.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">66.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">70.2</td>
</tr>
<tr>
<td style="text-align: center;">CODEGen</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">25.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">36.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">47.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">53.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">53.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">51.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">55.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">46.7</td>
</tr>
</tbody>
</table>
<p>Table 4: Full results for the print-var anchor experiments, used to generate the plot in Figure 4. For different numbers of canonical solution lines (sol. lines), we report the functional accuracy when the anchor function is prepended (anc. acc.), the fraction of generated solutions that include print (var) (prints), the fraction of generated solutions that include print (var) and are functionally correct (p. + pass), the fraction of generated solutions that include for var in (for var), the fraction of generated solutions that include for var in and are functionally correct (f. v. + pass), the fraction of solutions that are exactly the anchor function (copy), and the functional accuracy without the anchor function prepended (no anc.).
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Results of the print-var anchoring experiment on Codex, where we append 1 to the name of the anchor function and 2 to the name of the function to be completed.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Results of the add-var anchoring experiment on Codex, where we append 1 to the name of the anchor function and 2 to the name of the function to be completed.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Sol. lines</th>
<th style="text-align: center;">Anc. acc.</th>
<th style="text-align: center;">Rets. temp</th>
<th style="text-align: center;">Rets. tmp + passes</th>
<th style="text-align: center;">Verbatim</th>
<th style="text-align: center;">No anc. acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CODEX</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">47.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">57.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">60.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">64.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">66.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">70.2</td>
</tr>
<tr>
<td style="text-align: center;">CODEGen</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">24.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">31.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">35.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">45.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">51.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">51.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">49.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">44.7</td>
</tr>
</tbody>
</table>
<p>Table 5: Full results for the add-var anchor experiments, used to generate the plot in Figure 8. For different numbers of canonical solution lines (sol. lines), we report the functional accuracy when the anchor function is prepended (anc. acc.), the fraction of generated solutions that include return tmp (rets. tmp), the fraction of generated solutions that include return tmp and are functionally correct (rets. tmp + passes), the fraction of solutions that are exactly the anchor function (Verbatim), and the functional accuracy without the anchor function prepended (no anc. acc.).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Sol. lines</th>
<th style="text-align: center;">Anc. acc.</th>
<th style="text-align: center;">Prints</th>
<th style="text-align: center;">P. + pass</th>
<th style="text-align: center;">For var</th>
<th style="text-align: center;">F.v. + pass</th>
<th style="text-align: center;">Copy</th>
<th style="text-align: center;">No anc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">47.2</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">57.0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">60.6</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">64.6</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">66.2</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">70.2</td>
</tr>
</tbody>
</table>
<p>Table 6: Full results of the print-var anchoring experiment on Codex where we append 1 to the name of the anchor function and 2 to the name of the function to be completed. These numbers are used to generate the plot in Figure 9.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Sol. lines</th>
<th style="text-align: center;">Anc. acc.</th>
<th style="text-align: center;">Rets. temp</th>
<th style="text-align: center;">Rets. tmp + passes</th>
<th style="text-align: center;">Verbatim</th>
<th style="text-align: center;">No anc. acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">47.2</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">57.0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">60.6</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">64.6</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">66.2</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">70.2</td>
</tr>
</tbody>
</table>
<p>Table 7: Full results of the add-var anchoring experiment where we append 1 to the name of the anchor function and 2 to the name of the function to be completed. These numbers are used to generate the plot in Figure 9</p>
<p>and $y$, and add the description below the function signature, but keep all other experimental details from Section 3.3.3 constant. An example prompt is as follows:</p>
<div class="codehilite"><pre><span></span><code>def square_sum(x, y):
    #function squares the sum of its inputs
</code></pre></div>

<p>Codex achieves higher accuracy with this prompt than the prompt from Section 3.3.3; it achieves an accuracy of $54.1 \%$. However, the unary-first bias remains: $27.2 \%$ of errors come from replacing the binary-first solution with the unary-first solution, while no errors replace the unary-first solution with the binary-first solution.</p>
<h1>A. 4 Attribute substitution</h1>
<p>In this section, we provide more details on how we generate prompts for the attribute substitution experiment in Section 3.3.4.</p>
<p>Prompts in Section 3.3.4. We consider two types of MathEquation prompts for our experiments in Section 3.3.4. First, we consider prompts that include the function name in the docstring:</p>
<div class="codehilite"><pre><span></span><code><span class="ss">&quot;&quot;&quot;</span>
<span class="ss">Write a function that computes the [operation] of its inputs called [name]</span>
<span class="ss">&quot;&quot;&quot;</span>
</code></pre></div>

<p>And second, we consider prompts that already include the function name.</p>
<div class="codehilite"><pre><span></span><code><span class="ss">&quot;&quot;&quot;</span>
<span class="ss">Write a function that computes the [operation] of its inputs</span>
<span class="ss">&quot;&quot;&quot;</span>
<span class="n">def</span><span class="w"> </span><span class="o">[</span><span class="n">name</span><span class="o">]</span>
</code></pre></div>

<p>We consider names of the form [operation]<em>plus</em>[number]. We test sum, difference, and product for operations, and consider the integers between 0 and 5 and powers of ten between 10 and 10000 for the possible numbers for 90 total prompts in each setting. These are the prompts we use to report numbers in Table 2.</p>
<p>Control experiment: non-instructional prompt. We next test non-instructional prompts, where the prompt includes the correct function name, variables $x$ and $y$, and a description below the function signature. Other experimental details from Section 3.3.4 remain constant. An example prompt is as follows:</p>
<div class="codehilite"><pre><span></span><code>def product_plus_2(x, y):
    #returns the sum of its inputs
</code></pre></div>

<h2>B Additional Details and Results for GPT3 Experiments</h2>
<p>In this section, we include additional details and results on experiments described in Section 4. We focus on the anchoring results in Appendix B.1, and the framing effect results in Appendix B. 2</p>
<h2>B. 1 Anchoring</h2>
<p>In this section, we provide more details for the replication study of Jacowitz and Kahneman [1995] described in Section 4. Specifically, we outline the prompts that we use in the study along with GPT-3's outputs.</p>
<p>In Table 8, we show the prompts we use from the Jacowitz and Kahneman [1995] study along with the true answer, then the lower and upper anchors using $p$ of $50 \%$. We additional study $p=20 \%$ to generate the results in Table 3. We find the true answers for meat consumption ${ }^{7}$, distance from San Francisco to New York ${ }^{8}$, the height of the tallest redwood ${ }^{9}$, the number of female professors at</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: center;">Actual</th>
<th style="text-align: center;">Low. anc (50\%)</th>
<th style="text-align: center;">Up. anc (50\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Length of the Mississippi River (in miles)</td>
<td style="text-align: center;">2350</td>
<td style="text-align: center;">1175</td>
<td style="text-align: center;">3525</td>
</tr>
<tr>
<td style="text-align: left;">Height of Mount Everest (in feet)</td>
<td style="text-align: center;">29032</td>
<td style="text-align: center;">14516</td>
<td style="text-align: center;">43548</td>
</tr>
<tr>
<td style="text-align: left;">Amount of meat eaten per year by the <br> average American (in pounds)</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">216</td>
</tr>
<tr>
<td style="text-align: left;">Distance from San Francisco to New York City (in miles)</td>
<td style="text-align: center;">2569</td>
<td style="text-align: center;">1284</td>
<td style="text-align: center;">3854</td>
</tr>
<tr>
<td style="text-align: left;">Height of the tallest redwood (in feet)</td>
<td style="text-align: center;">380</td>
<td style="text-align: center;">190</td>
<td style="text-align: center;">570</td>
</tr>
<tr>
<td style="text-align: left;">Number of United Nation members</td>
<td style="text-align: center;">193</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">290</td>
</tr>
<tr>
<td style="text-align: left;">Number of female professors at the <br> University of California, Berkeley</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">384</td>
</tr>
<tr>
<td style="text-align: left;">Population of Chicago (in millions)</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Year the telephone was invented</td>
<td style="text-align: center;">1876</td>
<td style="text-align: center;">938</td>
<td style="text-align: center;">2814</td>
</tr>
<tr>
<td style="text-align: left;">Average number of babies born per <br> day in the United States</td>
<td style="text-align: center;">10267</td>
<td style="text-align: center;">5134</td>
<td style="text-align: center;">15400</td>
</tr>
<tr>
<td style="text-align: left;">Maximum speed of a house cat (in miles per hour)</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">45</td>
</tr>
<tr>
<td style="text-align: left;">Amount of gas used per month by <br> average American (in gallons)</td>
<td style="text-align: center;">656</td>
<td style="text-align: center;">328</td>
<td style="text-align: center;">984</td>
</tr>
<tr>
<td style="text-align: left;">Number of state colleges and universities in California</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">34</td>
</tr>
<tr>
<td style="text-align: left;">Number of Lincoln's presidency</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">24</td>
</tr>
</tbody>
</table>
<p>Table 8: Prompts we use from the Jacowitz and Kahneman [1995], with the researched true answer, along with the lower and upper anchors with anchor adjustment $50 \%$.</p>
<p>Berkeley ${ }^{10}$, the population of Chicago ${ }^{11}$, the year the telephone was invented ${ }^{12}$, the number of babies born per day in the United States ${ }^{13}$, the maximum speed of a house cat ${ }^{14}$, and the amount of gas used per month by the average American ${ }^{15}$ at the URLs listed in the footnotes. We do not prompt GPT-3 to estimate the Number of bars in Berkeley, CA unlike the original study, since we could not find a reliable answer.</p>
<h1>B. 2 Framing effect</h1>
<p>In this section, we test GPT-3 with an expansion of framing effect study from Tversky and Kahneman [1981]. In their original experiment, Tversky and Kahneman asked people to choose between two treatment options: certainly saving some fraction of the population (e.g. certainly saving 200 / 600), or probabilistically saving all of the population (saving all 600 with probability $1 / 3$ ). They then compare peoples' responses to the identical choice framed in terms of death: choosing between some fraction of the population certainly dying ( $400 / 600$ die) or probabilistically letting the whole population die ( $2 / 3$ chance everybody dies). For both framings, the number of people that live and die in expectation remains constant.
We run this experiment on the "davinci-002" version of GPT-3 using Tversky and Kahneman [1981]'s prompt formatting. Specifically, for the save framing we prompt GPT-3 with the following four-lined prompt:</p>
<p>Imagine 600 people are affected by a deadly disease. Choose Option A or Option B
Option A: Exactly 200 people will be saved.
Option B: $1 / 3$ probability that 600 people will be saved, and $2 / 3$ probability that no people will be saved.
Answer: Option
For the death framing, we use an analogous four-lined prompt.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Fraction of the time the risky option is chosen as a function of the save probability for both save framing and die framing. Around the regime of the original experiment (save probability $=$ $1 / 3$ ), GPT-3 rarely chooses the risky option with the save framing, but does so more often with the die framing. However, for higher save probabilities, both tend to choose the risky framing. This might match humans; intuitively, for higher save probabilities, the risky framing is less risky.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt framing</th>
<th style="text-align: center;">Save probability range</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Less than 0.5</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">Greater than 0.5</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Tversky and Kahneman [1981]</td>
</tr>
<tr>
<td style="text-align: center;">Save framing</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">28</td>
</tr>
<tr>
<td style="text-align: center;">Die framing</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">78</td>
</tr>
</tbody>
</table>
<p>Table 9: Results of the framing effect experiment on GPT-3. For the save and die framinigs, we report the average probability (all), the average over different ranges of probabilities, and the original numbers reported in Tversky and Kahneman [1981].</p>
<div class="codehilite"><pre><span></span><code>Imagine 600 people are affected by a deadly disease. Choose Option A or Option B
Option A: Exactly 400 people will die.
Option B: 1/3 probability that nobody will die, and 2/3 probability that 600 people
    will die.
Answer: Option
</code></pre></div>

<p>In these prompt, the population size, or total number of affected people is 600 , and the save fraction, or fraction of people certainly saved is $1 / 3$. The original study only considers these numbers, but we additionally test population sizes of $60,300,900,1200,1500,3000$, and 6000 and all save fractions that have denominator less than seven and are reduce (i.e. $1 / 2$, not $2 / 4$ ). We test the rate at which GPT-3 selects the risky option or option which probabilistically lets everyone die, for both the save framing and die framing. To avoid the confounding influence of the position of the risky option and the label, for each prompt framing, population size, and save fraction we set the risky option to both option A and B, and put it both first and second. This gives us a total of 704 prompts.
In Figure 11 plot the fraction of the time the risky option is chosen as a function of the save probability for both the save and die framing, and results aggregated over save probability ranges in Table 9. Our results show that averaged over all probabilities, our results are qualitatively similar to the results from Tversky and Kahneman [1981]: in the original paper with the save framing people choose the risky option $28 \%$ of the time compared to $45 \%$ for GPT-3, and with the death framing choose the risky option $78 \%$ of the time compared to $74 \%$ for GPT-3. Around the regime of the original experiment (save probability $=1 / 3$ ), GPT-3 rarely chooses the risky option with the save framing ( $11.3 \%$ ), but does so more often with the die framing ( $60.0 \%$ ). However, for higher save probabilities, both tend to choose the risky framing. This might match humans; intuitively, for higher save probabilities, the risky framing is less risky; e.g. the human has a $80 \%$ chance of saving everyone. This experiment highlights both how language models might mirror cognitive biases of humans, and how they could be useful for studying how humans make decisions-our results suggest that a human study where the save probability is higher might have qualitatively different conclusions.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ https://www.dailycal.org/2020/04/02/female-faculty-faces-challenges-despite-increase-in-uc-berkeley-gen
${ }^{11}$ https://en.wikipedia.org/wiki/Demographics_of_Chicago
${ }^{12}$ https://www.sciencemuseum.org.uk/objects-and-stories/ahoy-alexander-graham-bell-and-first-telephone-call
${ }^{13}$ https://www.babycenter.com/pregnancy/your-body/surprising-facts-about-birth-in-the-united-states_ 1372273 (as of 2019)
${ }^{14}$ https://www.petfinder.com/cats/cat-behavior-and-training/
how-fast-cats-run-how-high-cats-jump
${ }^{15}$ https://www.fool.com/investing/2017/01/14/heres-how-much-gasoline-the-average-american-consu. aspx (As of 2017)&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
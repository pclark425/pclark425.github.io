<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3807 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3807</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3807</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-90.html">extraction-schema-90</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <p><strong>Paper ID:</strong> paper-714671a26060d289a5e12888a92e904a19034982</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/714671a26060d289a5e12888a92e904a19034982" target="_blank">MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems</a></p>
                <p><strong>Paper Venue:</strong> Journal of the mechanics and physics of solids</p>
                <p><strong>Paper TL;DR:</strong> A flexible multi-modal mechanics language model, MeLM, applied to solve various nonlinear forward and inverse problems, that can deal with a set of instructions, numbers and microstructure data, and can solve complex degenerate mechanics design problems and determine novel material architectures across a range of hierarchical levels is reported.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3807.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3807.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MeLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mechanics Language Model (MeLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive, decoder-only transformer trained as a multi-modal 'mechanics' language model to solve forward and inverse mechanics tasks (predict constitutive responses and generate microstructures) using text-encoded numerical and structural tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MeLM (custom autoregressive transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only autoregressive Transformer with rotary position embeddings (RoPE); two model variants reported: ~126.4M parameters (model dim 512) and ~303.1M parameters (model dim 1024); 24 transformer layers, 8 attention heads, GELU activation; byte-level (utf8) tokenizer; trained end-to-end on text prompts pairing tasks and numeric/text outputs. Uses VQ-VAE to encode images into discrete token sequences for microstructures and Gumbel-softmax sampling at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>relationship discovery / predictive mapping and design (forward and inverse mechanics); emergent interaction-potential discovery from training data (implicit/operator discovery), not explicit symbolic equation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Materials science / Applied mechanics (hierarchical architected materials, carbon nanotubes, protein unfolding mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>input_data_type</strong></td>
                            <td>Textual prompts containing numbers and tokens: gene vectors (numeric parameter vectors), VQ-VAE discrete token sequences encoding microstructure images, chirality pairs for CNTs, protein primary sequences, and paired simulation-derived property vectors (stress-strain curves, modulus, unfolding forces).</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Autoregressive transformer is trained on concatenated task+result strings (causal next-token prediction) so the model learns mappings from structural encodings to numeric property sequences and vice versa; VQ-VAE is used to convert high-dimensional images to discrete token sequences; byte-level tokenization allows numbers/equations to be handled as regular text; inference sampling uses Gumbel softmax with temperature control (low T for accurate forward prediction, higher T for design diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Numeric property sequences (stress-strain curves), discrete microstructure token sequences/gene vectors, numeric modulus values, predicted unfolding forces — i.e., learned functional mappings and design candidates rather than explicit closed-form symbolic equations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Hierarchical honeycomb dataset: 1,445 microstructure — stress-strain pairs from prior coarse-grained molecular modeling (Lew et al. [50]); CNT dataset: 818 chirality — modulus pairs from Čanađija [69]; Protein dataset: 16,618 sequence — unfolding force pairs from BSDB / Sikora et al. [70,71].</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Coefficient of determination (R^2) for forward and inverse mapping tasks; cycle-consistency checks (feeding inverse outputs to forward model).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High predictive accuracy on the supplied mechanics tasks: hierarchical honeycomb forward R^2 ≈ 0.95–0.96 and inverse R^2 ≈ 0.91–0.96 depending on encoding; CNT modulus prediction and inverse design reported as 'excellent' (plots in paper) and protein unfolding-force prediction achieved good agreement with simulation-derived values (examples highlighted; no numeric R^2 explicitly reported for CNT/protein in main text but model retains honeycomb performance when expanded). The model discovers implicit interaction patterns via graph-forming attention and can generate diverse, cycle-consistent design candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No experiment in the paper attempts explicit discovery or symbolic extraction of closed-form mathematical laws or equations from large corpora of scholarly texts; outputs are mappings/predictions (numerical sequences and tokenized structures) rather than algebraic expressions. The paper notes potential hallucination risks of general LLMs and the need for careful development, training, validation, and additional pretraining/fine-tuning (e.g., LoRA) to avoid catastrophic forgetting. Dataset sizes are modest for some tasks (e.g., 1.4k honeycomb pairs), and no standardized benchmarks for equation-discovery are employed. The approach does not implement symbolic regression or a symbolic reasoning pipeline to produce human-interpretable equations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Authors argue Transformers generalize better than CNNs/GNNs in discovering operators and constructing graphs because they can discover adjacency graphs via attention; they position MeLM as complementary to conventional multiscale modeling and GNNs. However, no direct empirical comparison to symbolic-regression methods or equation-discovery systems is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_counterexamples</strong></td>
                            <td>No reported case in the experiments where MeLM extracted an incorrect explicit law because the model was not tasked with producing explicit equations; the paper cautions that large general LLMs 'often lack specific domain knowledge and ... can hallucinate predictions', highlighting the need for domain-specific models and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3807.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3807.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based theory ingestion (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using pre-trained large language models to ingest textbooks/scholarly corpora to learn theoretical frameworks and equations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed capability mentioned in the paper: fine-tuning or pretraining LLMs on textbooks and LaTeX-formatted equations to enable learning of theoretical concepts and possibly develop new insights or solutions in mechanics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General pre-trained LLMs (e.g., GPT-4, PaLM, LaMDA, Falcon; proposed to be fine-tuned or adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General large autoregressive/encoder-decoder transformer LLMs trained on broad corpora (examples named: GPT-4, PaLM, LaMDA, Falcon); the paper suggests these could be fine-tuned with LoRA or similar adaptation methods to incorporate domain-specific mechanics knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>proposed: theoretical law learning / distillation of equations and derivations from textual corpora (textbooks, LaTeX), i.e., knowledge extraction and possibly symbolic equation learning</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Theoretical and applied mechanics, broader physical sciences</td>
                        </tr>
                        <tr>
                            <td><strong>input_data_type</strong></td>
                            <td>Scholarly text, textbooks, LaTeX-formatted equations, proofs and derivations, potentially large-scale unlabeled corpora of domain literature</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Proposed strategy: pre-train/fine-tune general LLMs on domain corpora (including LaTeX-formatted equations) and use adaptation strategies such as LoRA to avoid catastrophic forgetting; the model would ingest textual derivations and could be fine-tuned on labeled downstream tasks to learn to produce/summarize equations or theoretical relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Speculative: distilled theoretical concepts, derivations, and potentially equations or symbolic relationships; not demonstrated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Not applied; authors do not report use of standardized equation-discovery benchmarks (e.g., SRBench) or large scholarly corpora for law discovery in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not reported / proposed only; the paper suggests cycle-consistency checks and validation with physics-based modeling or experiments but does not define concrete metrics for law extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No empirical results — idea proposed as an exciting avenue: that ingesting textbooks and equations formatted in LaTeX might allow the model to learn powerful theoretical concepts and potentially support discovery; authors recommend careful development, pretraining and fine-tuning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Speculative; concrete challenges noted include hallucination risk of large LLMs, domain-knowledge gaps in general models, catastrophic forgetting during fine-tuning, need for careful validation against physics-based models or experiments, and lack of explicit symbolic-regression pipelines to extract closed-form equations from model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Authors suggest combining general-purpose LLM reasoning capabilities with domain-specific fine-tuning (advantage over vanilla general models), but no direct empirical comparisons to symbolic regression or dedicated equation-discovery systems are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_counterexamples</strong></td>
                            <td>No demonstrations provided; thus no counterexamples in this paper, only general cautions about hallucination and domain-mismatch in off-the-shelf LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactica: A Large Language Model for Science <em>(Rating: 2)</em></li>
                <li>Generative Pretrained Autoregressive Transformer Graph Neural Network Applied to the Analysis and Discovery of Novel Proteins. <em>(Rating: 2)</em></li>
                <li>Deep Language Models for Interpretative and Predictive Materials Science <em>(Rating: 2)</em></li>
                <li>FieldPerceiver: Domain Agnostic Transformer Model to Predict Multiscale Physical Fields and Nonlinear Material Properties through Neural Ologs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3807",
    "paper_id": "paper-714671a26060d289a5e12888a92e904a19034982",
    "extraction_schema_id": "extraction-schema-90",
    "extracted_data": [
        {
            "name_short": "MeLM",
            "name_full": "Mechanics Language Model (MeLM)",
            "brief_description": "An autoregressive, decoder-only transformer trained as a multi-modal 'mechanics' language model to solve forward and inverse mechanics tasks (predict constitutive responses and generate microstructures) using text-encoded numerical and structural tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MeLM (custom autoregressive transformer)",
            "model_description": "Decoder-only autoregressive Transformer with rotary position embeddings (RoPE); two model variants reported: ~126.4M parameters (model dim 512) and ~303.1M parameters (model dim 1024); 24 transformer layers, 8 attention heads, GELU activation; byte-level (utf8) tokenizer; trained end-to-end on text prompts pairing tasks and numeric/text outputs. Uses VQ-VAE to encode images into discrete token sequences for microstructures and Gumbel-softmax sampling at inference.",
            "task_type": "relationship discovery / predictive mapping and design (forward and inverse mechanics); emergent interaction-potential discovery from training data (implicit/operator discovery), not explicit symbolic equation extraction",
            "domain": "Materials science / Applied mechanics (hierarchical architected materials, carbon nanotubes, protein unfolding mechanics)",
            "input_data_type": "Textual prompts containing numbers and tokens: gene vectors (numeric parameter vectors), VQ-VAE discrete token sequences encoding microstructure images, chirality pairs for CNTs, protein primary sequences, and paired simulation-derived property vectors (stress-strain curves, modulus, unfolding forces).",
            "method_description": "Autoregressive transformer is trained on concatenated task+result strings (causal next-token prediction) so the model learns mappings from structural encodings to numeric property sequences and vice versa; VQ-VAE is used to convert high-dimensional images to discrete token sequences; byte-level tokenization allows numbers/equations to be handled as regular text; inference sampling uses Gumbel softmax with temperature control (low T for accurate forward prediction, higher T for design diversity).",
            "output_type": "Numeric property sequences (stress-strain curves), discrete microstructure token sequences/gene vectors, numeric modulus values, predicted unfolding forces — i.e., learned functional mappings and design candidates rather than explicit closed-form symbolic equations.",
            "benchmark_or_dataset": "Hierarchical honeycomb dataset: 1,445 microstructure — stress-strain pairs from prior coarse-grained molecular modeling (Lew et al. [50]); CNT dataset: 818 chirality — modulus pairs from Čanađija [69]; Protein dataset: 16,618 sequence — unfolding force pairs from BSDB / Sikora et al. [70,71].",
            "evaluation_metrics": "Coefficient of determination (R^2) for forward and inverse mapping tasks; cycle-consistency checks (feeding inverse outputs to forward model).",
            "results_summary": "High predictive accuracy on the supplied mechanics tasks: hierarchical honeycomb forward R^2 ≈ 0.95–0.96 and inverse R^2 ≈ 0.91–0.96 depending on encoding; CNT modulus prediction and inverse design reported as 'excellent' (plots in paper) and protein unfolding-force prediction achieved good agreement with simulation-derived values (examples highlighted; no numeric R^2 explicitly reported for CNT/protein in main text but model retains honeycomb performance when expanded). The model discovers implicit interaction patterns via graph-forming attention and can generate diverse, cycle-consistent design candidates.",
            "limitations_or_challenges": "No experiment in the paper attempts explicit discovery or symbolic extraction of closed-form mathematical laws or equations from large corpora of scholarly texts; outputs are mappings/predictions (numerical sequences and tokenized structures) rather than algebraic expressions. The paper notes potential hallucination risks of general LLMs and the need for careful development, training, validation, and additional pretraining/fine-tuning (e.g., LoRA) to avoid catastrophic forgetting. Dataset sizes are modest for some tasks (e.g., 1.4k honeycomb pairs), and no standardized benchmarks for equation-discovery are employed. The approach does not implement symbolic regression or a symbolic reasoning pipeline to produce human-interpretable equations.",
            "comparison_to_other_methods": "Authors argue Transformers generalize better than CNNs/GNNs in discovering operators and constructing graphs because they can discover adjacency graphs via attention; they position MeLM as complementary to conventional multiscale modeling and GNNs. However, no direct empirical comparison to symbolic-regression methods or equation-discovery systems is presented.",
            "notable_counterexamples": "No reported case in the experiments where MeLM extracted an incorrect explicit law because the model was not tasked with producing explicit equations; the paper cautions that large general LLMs 'often lack specific domain knowledge and ... can hallucinate predictions', highlighting the need for domain-specific models and validation.",
            "uuid": "e3807.0",
            "source_info": {
                "paper_title": "MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "LLM-based theory ingestion (proposed)",
            "name_full": "Using pre-trained large language models to ingest textbooks/scholarly corpora to learn theoretical frameworks and equations",
            "brief_description": "A proposed capability mentioned in the paper: fine-tuning or pretraining LLMs on textbooks and LaTeX-formatted equations to enable learning of theoretical concepts and possibly develop new insights or solutions in mechanics.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "General pre-trained LLMs (e.g., GPT-4, PaLM, LaMDA, Falcon; proposed to be fine-tuned or adapted)",
            "model_description": "General large autoregressive/encoder-decoder transformer LLMs trained on broad corpora (examples named: GPT-4, PaLM, LaMDA, Falcon); the paper suggests these could be fine-tuned with LoRA or similar adaptation methods to incorporate domain-specific mechanics knowledge.",
            "task_type": "proposed: theoretical law learning / distillation of equations and derivations from textual corpora (textbooks, LaTeX), i.e., knowledge extraction and possibly symbolic equation learning",
            "domain": "Theoretical and applied mechanics, broader physical sciences",
            "input_data_type": "Scholarly text, textbooks, LaTeX-formatted equations, proofs and derivations, potentially large-scale unlabeled corpora of domain literature",
            "method_description": "Proposed strategy: pre-train/fine-tune general LLMs on domain corpora (including LaTeX-formatted equations) and use adaptation strategies such as LoRA to avoid catastrophic forgetting; the model would ingest textual derivations and could be fine-tuned on labeled downstream tasks to learn to produce/summarize equations or theoretical relationships.",
            "output_type": "Speculative: distilled theoretical concepts, derivations, and potentially equations or symbolic relationships; not demonstrated in this paper.",
            "benchmark_or_dataset": "Not applied; authors do not report use of standardized equation-discovery benchmarks (e.g., SRBench) or large scholarly corpora for law discovery in this work.",
            "evaluation_metrics": "Not reported / proposed only; the paper suggests cycle-consistency checks and validation with physics-based modeling or experiments but does not define concrete metrics for law extraction.",
            "results_summary": "No empirical results — idea proposed as an exciting avenue: that ingesting textbooks and equations formatted in LaTeX might allow the model to learn powerful theoretical concepts and potentially support discovery; authors recommend careful development, pretraining and fine-tuning strategies.",
            "limitations_or_challenges": "Speculative; concrete challenges noted include hallucination risk of large LLMs, domain-knowledge gaps in general models, catastrophic forgetting during fine-tuning, need for careful validation against physics-based models or experiments, and lack of explicit symbolic-regression pipelines to extract closed-form equations from model outputs.",
            "comparison_to_other_methods": "Authors suggest combining general-purpose LLM reasoning capabilities with domain-specific fine-tuning (advantage over vanilla general models), but no direct empirical comparisons to symbolic regression or dedicated equation-discovery systems are provided.",
            "notable_counterexamples": "No demonstrations provided; thus no counterexamples in this paper, only general cautions about hallucination and domain-mismatch in off-the-shelf LLMs.",
            "uuid": "e3807.1",
            "source_info": {
                "paper_title": "MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactica: A Large Language Model for Science",
            "rating": 2
        },
        {
            "paper_title": "Generative Pretrained Autoregressive Transformer Graph Neural Network Applied to the Analysis and Discovery of Novel Proteins.",
            "rating": 2
        },
        {
            "paper_title": "Deep Language Models for Interpretative and Predictive Materials Science",
            "rating": 2
        },
        {
            "paper_title": "FieldPerceiver: Domain Agnostic Transformer Model to Predict Multiscale Physical Fields and Nonlinear Material Properties through Neural Ologs",
            "rating": 1
        }
    ],
    "cost": 0.011297499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems</h1>
<p>Markus J. Buehler ${ }^{1,2 <em>}$<br>${ }^{1}$ Laboratory for Atomistic and Molecular Mechanics (LAMM), Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA<br>${ }^{2}$ Center for Computational Science and Engineering, Schwarzman College of Computing, Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA<br></em>mbuehler@MIT.EDU</p>
<h4>Abstract</h4>
<p>We report a flexible multi-modal mechanics language model, MeLM, applied to solve various nonlinear forward and inverse problems, that can deal with a set of instructions, numbers and microstructure data. The framework is applied to various examples including bio-inspired hierarchical honeycomb design, carbon nanotube mechanics, and protein unfolding. In spite of the flexible nature of the model-which allows us to easily incorporate diverse materials, scales, and mechanical features-it performs well across disparate forward and inverse tasks. Based on an autoregressive attention-model, MeLM effectively represents a large multi-particle system consisting of hundreds of millions of neurons, where the interaction potentials are discovered through graph-forming self-attention mechanisms that are then used to identify relationships from emergent structures, while taking advantage of synergies discovered in the training data. We show that the model can solve complex degenerate mechanics design problems and determine novel material architectures across a range of hierarchical levels, providing an avenue for materials discovery and analysis. Looking beyond the demonstrations reported in this paper, we discuss other opportunities in applied mechanics and general considerations about the use of large language models in modeling, design, and analysis that can span a broad spectrum of material properties from mechanical, thermal, optical, to electronic.</p>
<p>Keywords: Mechanics; Attention; Transformer; Language model; Forward; Inverse; Design; Modeling; Multiscale; Atomistic; Encoding; Representation; Causal; Emergent; Collective; Graph neural network</p>
<h2>1. Introduction</h2>
<p>The emergence of large language models (LLMs) has challenged conventional strategies of modeling problems across a variety of scientific and engineering disciplines [1-16]. Such models can complement conventional multiscale modeling for the analysis and design of hierarchical materials [17-20], and many other applications in mechanics [21]. Specifically, data-driven modeling using machine learning and related approaches has emerged as a powerful strategy [22-28] that includes both analysis tasks (such as, predicting properties from protein sequences or microstructures) and inverse design tasks (designing novel proteins, biomolecules or composites to meet a set of target properties [29]. Specifically, generative mechanics is an emerging frontier in materials discovery that complements earlier work in proteins [30] and architected materials [31-34]. Attention-based transformer models [15,35-42] have been shown to display emergent behaviors [43] with important questions that should be investigated specific to applications in science and engineering, and as explored here, applied mechanics. The possibility to realize neural implementations of category theory-based material models is achieved here through the use of graph-forming attention neural networks that construct intricate knowledge graphs from data [41,44,45] through Transformer models [42,46,47]. While many existing models trained on large corpi of data like GPT-3/4, PalM/Bard, or ChatGPT offer impressive general capabilities, they often lack specific domain knowledge and the ability to solve a particular problem (and can, in some instances, hallucinate predictions). We can overcome these challenges by creating a new framework specifically developed to solve domain problems, ultimately forming a sort of "MechGPT" for mechanics. This requires careful development, training, and validation.</p>
<p>The rationale for using attention-based transformer models to solve mechanics problems is rooted in the insight that Transformer architectures provide a first-principles based, flexible framework to capture complex relationships in data, using a language approach that breaks down mechanics into elementary building blocks. Other neural architectures (e.g. fully connected deep neural nets, convolutional operators, graph neural networks, recurrent models like RNN or LSTM, etc.), while successfully applied to specific problems, can not easily generalize beyond the core application against which they have been trained for. In convolutional models (e.g. CNN) filters are applied in Euclidian space, focusing on neighboring pixels or voxels. By successively applying CNN layers we can effectively achieve a multi-level coarse-graining strategy akin to physics-based strategies. However, this approach can be limiting especially when dealing with complex data such as genes, language or equations, sequences, or time-series, because long-range relationships cannot be learned on fine-grained details. To generalize this concept, graph neural nets (GNNs) have been widely used for mechanics problems, molecules, proteins, and composites. GNNs relax the constraint operators applied in Euclidian space and expand it to treat apply convolutional operators in the space of adjacencies defined by the graph's edges and associated features (e.g., node features may include the atom type, and edge features the bond length, or bond order). While these models often perform very well, they typically require graphs to be known. Transformer models can be seen as architectural frameworks that generalize even further - to apply operators in graph space, while also discovering the graph itself. In addition, they possess a heightened capacity to discover complex operators beyond conventional convolutional filters, including the possibility for long-range processing. This offers a much more general formulation, following from the concept of neural ologs [41,48], and motivates a theoretical basis for their high expressivity, generalization capacity and emergence potential. In this vein, deep Transformer models effectively represent a large multi-particle system - consisting of hundreds of millions of particles (neurons), where the interaction potential is defined by the nature of the graphs formed, and scaled up through a large set of feed forward layers and head layers during which the interaction potentials discover function from emergent structures [49]. Since Transformers learn to generalize operators across all data seen, and since they are trained to generate sparse graphs via the use of softmax functions (see equations (1-3) in Materials and Methods), they capture and generalize knowledge well and are able to discover, and draw upon, synergistic relationships between isolated patterns or sources of data. This may explain why Transformers excel when they are large, and trained on increasingly complex tasks.</p>
<p>In this paper we focus several mechanics problems to illustrate the method as a platform approach (bio-inspired hierarchical honeycomb design, carbon nanotube mechanics, and protein unfolding mechanics, giving three examples in different material domains). The applications include broadly relevant tasks to predict the constitutive stress-strain response for a given microstructure, and to design microstructure candidates so that it has a particular required stress-strain response. Figure 1 shows an overview of the problems solved in this study. Figure 1a depicts the two main problems modeled, including the forward problem (predict constitutive response from a microstructure) and the inverse problem (predict candidate microstructures to meet a specified stress-strain response). Figure 1b depicts details on the honeycomb design, inspired by natural honeybee hives, and potential additive manufacturing routes (these are not explored in this study). The right panel in Figure 1c depicts the hierarchical design principles underlying these structures, as first reported in [50]. Figure 1c shows the stressstrain response calculated using coarse-grained molecular mechanics modeling (these results, including details, are contained in earlier work [50]; here we use the dataset generated therein to develop, train and test the language model). The constitutive response to compressive loading is considered in this work, albeit there is no fundamental limitation as to what properties can be described. Figure 1d shows a cycle of the forward and inverse problems for an example microstructure, setting the stage for the problem tackled in this paper. Figure 1ef show additional applications studied in this paper, including carbon nanotube nanomechanics and protein unfolding mechanics, illustrating the boundary conditions of the mechanical tests performed (tensile loading of nanotubes to determine its modulus, and unfolding of a protein by pulling apart its ends while recording the maximum resistance force).</p>
<p>The plan of this paper is as follows. First, we introduce the overall approach of using language models to address mechanics problems, including model development and validation. We then report a series of applications studies including the design new hierarchical composite structures with targeted mechanical properties, modeling and design of nanotube mechanics, and protein unfolding. We focus on a careful analysis of the accuracy of the</p>
<p>predictions and the suitability to process applied mechanics problems that have unique or degenerate solutions. We conclude with an outlook to challenges and opportunities.</p>
<h1>2. Results and Discussion</h1>
<p>This work uses multi-headed attention mechanisms that are used widely in many transformer models for sequence and graph data [51]. One of the appeals of this architecture is that it can easily be trained, allows for pre-training and fine-tuning strategies, and can capture extremely complex linguistic, scientific and mathematical relationships [52,53]. Here we show how a LLM framework can be used to accurately solve mechanics tasks.</p>
<p>Figure 2 shows a general flowchart overview of the strategy implemented in this study. Figure 2a provides details of the model is used to solve a variety of problems, such as calculate properties or generate new designs. The input is provided to the model as utf8 encoded string data, which includes tasks, numbers, and other details of the problem solved, then provided to the model, which then samples probability distributions of the output text. The model thereby predicts text in response to the queries, which can be mined to either extract numerical values (e.g. resulting stress-strain response) or a microstructure design. Figure 2b shows two complete sample prompts. The model is easy to train. This is because during training, the entire sequence of 'ground truth' is fed, consisting of the task, and the result (marked in bold). A trained model can easily be extended to learn new tasks (e.g. add protein mechanics to its base knowledge about honeycomb mechanics), or fine-tuned with new or more accurate data.</p>
<p>When the model is used to make predictions, that is, during inference, only the task is provided, and the model predicts the result of the task by completing the sentence (that is, predicting the bold characters). The model can be trained to feature several additional, related or unrelated tasks. Sampling can be repeated multiple times and the resulting predictions (e.g. material designs) can be tested against desired outputs or other parameters to create a set of possible designs that can be screened for performance. If unique solutions exist, repeated sampling results in consistent responses. If multiple solutions exist, repeated sampling results in answers that represent the distribution of solutions (e.g. multiple designs, or a distribution of values).</p>
<h3>2.1 Hierarchical honeycomb mechanics</h3>
<p>We first discuss our experiments conducted with hierarchical honeycomb materials (it is noted, however, that the method can be adopted easily for other microstructure data). Two methods are presented, one specific to the type of hierarchical honeycomb design based on underlying mathematical relations, expressed via a gene vector [50], and a more general discrete, quantized autoencoding strategy that can be applied to any type of high-dimensional data for use in MeLM. The reason for reducing the dimensionality of microstructure data is that it is not efficient to directly encode images, for instance, as text format (since the sequences would be extremely long, and hence render the model highly memory intensive as the matrix multiplications in building the interaction graphs generally scales nonlinearly with the length of the input sequence). Figure 3 shows details of these two strategies.</p>
<p>The first method (Figure 3a), as first reported in [50], uses a gene vector to define a mathematical encoding of the microstructure. In this approach, the hierarchical honeycomb microstructure is defined by a series of numbers, e.g.:
$12,4,7,3,0,0,0,0,0,0$
denoting the gene vector. Then, the property calculation task is defined as seeking a calculation on such a gene vector:</p>
<div class="codehilite"><pre><span></span><code>GeneGetProperties&lt; 12, 4, 7, 3, 0, 0, 0, 0, 0, 0&gt; [-0.988,-0.913,-0.835,-0.760,-
0.695,-0.629,-0.567,-0.520,-0.517,-0.511,-0.503,-0.497,-0.489,-0.481,-0.475,-0.467,
0.461,-0.452,-0.445,-0.435,-0.430,-0.420,-0.410,-0.402,-0.393,-0.384,-0.374,-0.364,
0.352,-0.339,-0.320,-0.300]
</code></pre></div>

<p>Similarly, the inverse task is given by:</p>
<div class="codehilite"><pre><span></span><code><span class="n">GeneGetStructure</span><span class="o">&lt;-</span><span class="m">0.987</span><span class="p">,</span><span class="m">-0.917</span><span class="p">,</span><span class="m">-0.863</span><span class="p">,</span><span class="m">-0.823</span><span class="p">,</span><span class="m">-0.781</span><span class="p">,</span><span class="m">-0.744</span><span class="p">,</span><span class="m">-0.702</span><span class="p">,</span><span class="m">-0.653</span><span class="p">,</span><span class="m">-0.623</span><span class="p">,</span><span class="o">-</span>
<span class="m">0.612</span><span class="p">,</span><span class="m">-0.611</span><span class="p">,</span><span class="m">-0.606</span><span class="p">,</span><span class="m">-0.595</span><span class="p">,</span><span class="m">-0.584</span><span class="p">,</span><span class="m">-0.572</span><span class="p">,</span><span class="m">-0.560</span><span class="p">,</span><span class="m">-0.550</span><span class="p">,</span><span class="m">-0.540</span><span class="p">,</span><span class="m">-0.531</span><span class="p">,</span><span class="m">-0.520</span><span class="p">,</span><span class="m">-0.510</span><span class="p">,</span><span class="o">-</span>
<span class="m">0.500</span><span class="p">,</span><span class="m">-0.487</span><span class="p">,</span><span class="m">-0.469</span><span class="p">,</span><span class="m">-0.446</span><span class="p">,</span><span class="m">-0.416</span><span class="p">,</span><span class="m">-0.386</span><span class="p">,</span><span class="m">-0.348</span><span class="p">,</span><span class="m">-0.302</span><span class="p">,</span><span class="m">-0.260</span><span class="p">,</span><span class="m">-0.236</span><span class="p">,</span><span class="m">-0.208</span><span class="o">&gt;</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="m">8</span><span class="p">,</span>
<span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">]</span>
</code></pre></div>

<p>While the study of hierarchical honeycombs was facilitated via structural specification through gene vectors as developed in [50], this strategy is not generally applicable. For instance, many engineering structural designs cannot be captured by parametric equations (or are unknown a priori), and biological designs (e.g. leaf microstructures) do not follow any known mathematical laws. To address this generalization challenge, Figure 3b outlines a more flexible approach. Following the vector quantized variational autoencoder model (VQ-VAE) reported in [54] we use a codebook language-based representation of microstructures (therein, a microstructure image is encoded via 64 tokens defined by integer numbers between $0 \ldots 127$ in order to realize an encoded language representation (it is noted that the VQ-VAE model itself uses an attention-based architecture mixed with convolutional operators). An example for one of the hierarchical honeycombs would be:</p>
<div class="codehilite"><pre><span></span><code><span class="mf">97</span><span class="p">,</span><span class="mf">121</span><span class="p">,</span><span class="w"> </span><span class="mf">43</span><span class="p">,</span><span class="w"> </span><span class="mf">43</span><span class="p">,</span><span class="w"> </span><span class="mf">20</span><span class="p">,</span><span class="mf">110</span><span class="p">,</span><span class="mf">115</span><span class="p">,</span><span class="w"> </span><span class="mf">2</span><span class="p">,</span><span class="w"> </span><span class="mf">43</span><span class="p">,</span><span class="mf">110</span><span class="p">,</span><span class="w"> </span><span class="mf">22</span><span class="p">,</span><span class="mf">110</span><span class="p">,</span><span class="w"> </span><span class="mf">87</span><span class="p">,</span><span class="w"> </span><span class="mf">43</span><span class="p">,</span><span class="mf">121</span><span class="p">,</span><span class="w"> </span><span class="mf">36</span><span class="p">,</span><span class="w"> </span><span class="mf">2</span><span class="p">,</span><span class="w"> </span><span class="mf">97</span><span class="p">,</span>
<span class="mf">87</span><span class="p">,</span><span class="mf">113</span><span class="p">,</span><span class="mf">110</span><span class="p">,</span><span class="mf">113</span><span class="p">,</span><span class="w"> </span><span class="mf">47</span><span class="p">,</span><span class="mf">121</span><span class="p">,</span><span class="mf">115</span><span class="p">,</span><span class="mf">113</span><span class="p">,</span><span class="mf">110</span><span class="p">,</span><span class="mf">113</span><span class="p">,</span><span class="w"> </span><span class="mf">26</span><span class="p">,</span><span class="mf">121</span><span class="p">,</span><span class="w"> </span><span class="mf">27</span><span class="p">,</span><span class="w"> </span><span class="mf">20</span><span class="p">,</span><span class="w"> </span><span class="mf">87</span><span class="p">,</span><span class="w"> </span><span class="mf">87</span><span class="p">,</span><span class="mf">121</span><span class="p">,</span><span class="mf">121</span><span class="p">,</span><span class="mf">110</span><span class="p">,</span><span class="mf">121</span><span class="p">,</span><span class="mf">121</span><span class="p">,</span>
<span class="mf">43</span><span class="p">,</span><span class="mf">110</span><span class="p">,</span><span class="w"> </span><span class="mf">27</span><span class="p">,</span><span class="w"> </span><span class="mf">87</span><span class="p">,</span><span class="w"> </span><span class="mf">27</span><span class="p">,</span><span class="w"> </span><span class="mf">22</span><span class="p">,</span><span class="mf">110</span><span class="p">,</span><span class="w"> </span><span class="mf">47</span><span class="p">,</span><span class="mf">113</span><span class="p">,</span><span class="w"> </span><span class="mf">43</span><span class="p">,</span><span class="w"> </span><span class="mf">27</span><span class="p">,</span><span class="w"> </span><span class="mf">99</span><span class="p">,</span><span class="mf">121</span><span class="p">,</span><span class="w"> </span><span class="mf">87</span><span class="p">,</span><span class="w"> </span><span class="mf">20</span><span class="p">,</span><span class="mf">121</span><span class="p">,</span><span class="w"> </span><span class="mf">43</span><span class="p">,</span><span class="mf">115</span><span class="p">,</span><span class="w"> </span><span class="mf">97</span><span class="p">,</span>
<span class="mf">36</span><span class="p">,</span><span class="mf">113</span><span class="p">,</span><span class="mf">121</span><span class="p">,</span><span class="w"> </span><span class="mf">20</span><span class="p">,</span><span class="w"> </span><span class="mf">47</span><span class="p">,</span><span class="w"> </span><span class="mf">20</span>
</code></pre></div>

<p>Conceptually this is similar to the gene vector approach but the particular encoding is discovered by the vector quantized variational autoencoder model, and can be identified easily for a diverse range of high-dimensional data. Then, similar as before the tasks are given by, starting with the forward task:</p>
<div class="codehilite"><pre><span></span><code>GetProperties&lt; 97,121, 43, 43, 43,121, 97, 26, 43, 62, 97, 26, 87,121, 87, 43, 47, 47,
87, 20, 26,121, 20,113, 87,110, 20,121, 2, 47, 26, 26, 2,110,121,121, 87, 87, 97,
2, 43, 47, 2, 26, 47,121,110, 20, 47, 47, 87, 22,110,121, 20, 97, 47, 87,113,121, 26,
62, 87, 99&gt; [-0.987,-0.928,-0.865,-0.849,-0.840,-0.819,-0.799,-0.779,-0.747,-0.732,-
0.707,-0.693,-0.665,-0.624,-0.589,-0.550,-0.544,-0.496,-0.466,-0.432,-0.411,-0.384,-
0.348,-0.331,-0.295,-0.252,-0.200,-0.150,-0.087,-0.013,0.042,0.098]
</code></pre></div>

<p>And for the inverse task:</p>
<div class="codehilite"><pre><span></span><code><span class="n">GetStructure</span><span class="o">&lt;-</span><span class="m">0.987</span><span class="p">,</span><span class="m">-0.928</span><span class="p">,</span><span class="m">-0.865</span><span class="p">,</span><span class="m">-0.849</span><span class="p">,</span><span class="m">-0.840</span><span class="p">,</span><span class="m">-0.819</span><span class="p">,</span><span class="m">-0.799</span><span class="p">,</span><span class="m">-0.779</span><span class="p">,</span><span class="m">-0.747</span><span class="p">,</span><span class="m">-0.732</span><span class="p">,</span><span class="o">-</span>
<span class="m">0.707</span><span class="p">,</span><span class="m">-0.693</span><span class="p">,</span><span class="m">-0.665</span><span class="p">,</span><span class="m">-0.624</span><span class="p">,</span><span class="m">-0.589</span><span class="p">,</span><span class="m">-0.550</span><span class="p">,</span><span class="m">-0.544</span><span class="p">,</span><span class="m">-0.496</span><span class="p">,</span><span class="m">-0.466</span><span class="p">,</span><span class="m">-0.432</span><span class="p">,</span><span class="m">-0.411</span><span class="p">,</span><span class="m">-0.384</span><span class="p">,</span><span class="o">-</span>
<span class="m">0.348</span><span class="p">,</span><span class="m">-0.331</span><span class="p">,</span><span class="m">-0.295</span><span class="p">,</span><span class="m">-0.252</span><span class="p">,</span><span class="m">-0.200</span><span class="p">,</span><span class="m">-0.150</span><span class="p">,</span><span class="m">-0.087</span><span class="p">,</span><span class="m">-0.013</span><span class="p">,</span><span class="m">0.042</span><span class="p">,</span><span class="m">0.098</span><span class="o">&gt;</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="m">97</span><span class="p">,</span><span class="m">121</span><span class="p">,</span><span class="w"> </span><span class="m">43</span><span class="p">,</span><span class="w"> </span><span class="m">43</span><span class="p">,</span>
<span class="m">43</span><span class="p">,</span><span class="m">121</span><span class="p">,</span><span class="w"> </span><span class="m">97</span><span class="p">,</span><span class="w"> </span><span class="m">26</span><span class="p">,</span><span class="w"> </span><span class="m">43</span><span class="p">,</span><span class="w"> </span><span class="m">62</span><span class="p">,</span><span class="w"> </span><span class="m">97</span><span class="p">,</span><span class="w"> </span><span class="m">26</span><span class="p">,</span><span class="w"> </span><span class="m">87</span><span class="p">,</span><span class="m">121</span><span class="p">,</span><span class="w"> </span><span class="m">87</span><span class="p">,</span><span class="w"> </span><span class="m">43</span><span class="p">,</span><span class="w"> </span><span class="m">47</span><span class="p">,</span><span class="w"> </span><span class="m">47</span><span class="p">,</span><span class="w"> </span><span class="m">87</span><span class="p">,</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="m">26</span><span class="p">,</span><span class="m">121</span><span class="p">,</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="m">113</span><span class="p">,</span>
<span class="m">87</span><span class="p">,</span><span class="m">110</span><span class="p">,</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="m">121</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">47</span><span class="p">,</span><span class="w"> </span><span class="m">26</span><span class="p">,</span><span class="w"> </span><span class="m">26</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="m">110</span><span class="p">,</span><span class="m">121</span><span class="p">,</span><span class="m">121</span><span class="p">,</span><span class="w"> </span><span class="m">87</span><span class="p">,</span><span class="w"> </span><span class="m">87</span><span class="p">,</span><span class="w"> </span><span class="m">97</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">43</span><span class="p">,</span><span class="w"> </span><span class="m">47</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">26</span><span class="p">,</span>
<span class="m">47</span><span class="p">,</span><span class="m">121</span><span class="p">,</span><span class="m">110</span><span class="p">,</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="m">47</span><span class="p">,</span><span class="w"> </span><span class="m">47</span><span class="p">,</span><span class="w"> </span><span class="m">87</span><span class="p">,</span><span class="w"> </span><span class="m">22</span><span class="p">,</span><span class="m">110</span><span class="p">,</span><span class="m">121</span><span class="p">,</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="m">97</span><span class="p">,</span><span class="w"> </span><span class="m">47</span><span class="p">,</span><span class="w"> </span><span class="m">87</span><span class="p">,</span><span class="m">113</span><span class="p">,</span><span class="m">121</span><span class="p">,</span><span class="w"> </span><span class="m">26</span><span class="p">,</span><span class="w"> </span><span class="m">62</span><span class="p">,</span><span class="w"> </span><span class="m">87</span><span class="p">,</span><span class="w"> </span><span class="m">99</span><span class="p">]</span>
</code></pre></div>

<p>All training data, no matter which description of microstructures (or other data, see sections below), properties, or features is used, is fed into an autoregressive transformer architecture, which processes text using a byte-level tokenizer that can effectively and flexibly deal with a variety of string data in utf8 encoding (Figure 4, see Materials and Methods for details). Both numbers and text characters (various symbols, including math symbols, LaTeX, etc.), allowing also for human interaction with the model as an interactive modality, is possible. No special encoding is necessary for numerical input and output, providing a straightforward way to build datasets for training.</p>
<p>Once the model is trained, we can use it to test its performance on a variety of cases. We start by looking at the overall performance of the forward and inverse tasks, and then dive deeper into specific examples. Figure 5 shows results from forward and inverse tasks, depicting how the predicted stresses compare with the ground truth (GT) stresses. Figure 5a-b show the result for the model using gene vector microstructure encoding (as shown in Figure 3a; Figure 5a, forward task: R2=0.95, Figure 5b, inverse task: R2=0.96), and Figure 5c-d show the result for the vector quantized language encoding (Figure 5c, forward task: R2=0.96, Figure 5d, inverse task: R2=0.91). The two microstructure encoding strategies show similar performance, albeit the gene vector approach yields slightly better performance for both forward and inverse tasks (this can probably be addressed with more training data). The sampling temperature for all predictions is $T_{\text {sample }}=0.01$.</p>
<p>Using the gene vector encoding strategy, Figure 6 shows results from solving the inverse task and comparing the constitutive behavior of the designs with the desired response (as shown in Figure 4c, top panel). The results are obtained for a test set of microstructures not included in the training of the model. For the three examples shown, it can be seen that the model performs well in matching the desired stress-strain response. The right column, labeled GT, shows the microstructure designs in the dataset. As can be seen, the model proposes new, distinct, designs that yield the desired response. Similarly, for the vector quantized language encoding strategy, Figure 7 shows results of solving the inverse task and comparing the constitutive behavior of the designs with the desired response. The model does an excellent job in matching the desired stress-strain response. As before the column labeled GT depicts the microstructure designs in the test dataset. As can be verified in the figures, the model proposes new, distinct, designs that yield the desired response. Figure 8 shows an overview of solutions to the forward problem, for the gene vector encoding strategy (Figure 8a) and the vector quantized language encoding strategy (Figure 8b). For all cases, the model can effectively and accurately predict stress-strain responses for a given microstructure. The sampling temperature in all cases is $T_{\text {sample }}=0.01$. One of the special features of the model is that it because it has learned to solve both the forward and inverse problem, it can be immediately used to check whether or not solutions to the inverse problem are cycle consistent. This is done by feeding the prediction of the inverse model into the forward model and then comparing the desired response and the measured responses. Further validations could be performed, like using experimental essays, or physics-based modeling. These experiments were done, and the results visualized, in the analysis shown in Figure 5b and d, as well as Figures 6-7</p>
<p>One of the exciting opportunities of the model is that it can innately capture the development of degenerate solutions, if they so exist. As shown in Figure 9, by conducting predictions multiple times, the model can predict multiple solutions to the same task. To explore this scenario we show examples of three different microstructure designs that all meet the same design target (using the gene vector encoding strategy). All predicted designs feature the same hierarchical honeycomb patterns, but offer a great level of diverse designs. Further analysis, such as assessing density or material use, or which model fits certain regions in the desired stress-strain response better, can be used to identify the best candidates for a selected microstructure. Increased variation of the design can be obtained by increasing the sampling temperature, in ranges from $T_{\text {sample }}=0.1$ to 1 (however, as the temperature is increased, the constraint of the target solution is more and more relaxed, leading to solutions that deviate more strongly from the design objective as the model becomes more creative). We note that the sampling temperature for the forward problem is always set to $T_{\text {sample }}=0.01$ to ensure the highest-accuracy model predictions for this task. Moreover, for problems where only one solution exists - e.g. a given microstructure's response as defined in the forward task, repeated sampling will not lead to multiple solutions but yields the same response. Increased sampling temperature merely leads to higher deviation from the baseline solution.</p>
<p>Now we move on to experiments conducted with the vector quantized language encoding of microstructures, shown in Figure 10. It can be seen that these de novo designs feature a greater variety of solutions that clearly exceed those of the original training data. This is because its language-based design language captured by the vector quantized autoencoder model is not limited to the mathematical operations behind the gene vector encoding approach. Hence, this model can identify designs that are distinct from the architectural families of geometries included in the training set. While the two left designs closely follow the patterns in the training set, the model can also produce new designs, as shown in the right examples. Generally, increased variation of the design can be obtained by increasing the sampling temperature (as before, the sampling temperature for the forward problem is always set to $T_{\text {sample }}=0.01$ ).</p>
<h1>2.2 Incorporating carbon nanotube and protein mechanics</h1>
<p>Next, we explore whether a trained model (here, the gene encoding model) can be expanded, with additional data, to additionally capture other mechanics applications. We consider two examples, first predicting elastic properties of carbon nanotubes and designing carbon nanotube candidates to meet a certain modulus, and second, to predict the unfolding strength of a protein directly from its sequence without the need to know a 3D structure. The tasks used for the nanotube application are</p>
<p>GetCNTModulus&lt; 41, 5&gt; [0.518]
GetCNTStructure&lt;0.558&gt; [ 44, 0]
As before, the prompts consist of the task and the predicted result (in bold) - the first one, to calculate the CNT modulus from chiralities, and the second one to get CNT chiralities that meet a modulus design demand. Figure 11 shows results of this experiment. As the plots show, the model displays excellent performance in predicting the modulus and generating CNT designs that meet the design objective. Notably, even though we added a distinct set of tasks to the model, it still retains an excellent ability to design hierarchical honeycomb structures. This points to exciting potential to train this model with a larger number of mechanics tasks, scaling it to incorporate larger corpuses of data. However, we believe it is important to carefully examine the successive development of such models as a foundation.</p>
<p>We further explore whether an even more complex task such as end-to-end protein property prediction can be added. Figure 12 shows the results, revealing that the model can accurately predict the unfolding strength of protein sequences. For this case, the prompt is:</p>
<div class="codehilite"><pre><span></span><code>GetProteinForce&lt;MSYYHHHHHHLESTSLYKKAGSENLYFQGNKKDIPWTDLNRASGVGSTGILQAR
IINGVIYVRGNSIPVPNVAPNFIVPVGTFPPAFGTNLPQFDSSGTFYSHGNLSLSLINMSPSGIAVGNPN
NTSMNGKTISFALSAPLL&gt; [0.365]
</code></pre></div>

<p>Two proteins with extreme properties are highlighted Figure 12c. In agreement with understanding of protein mechanics, the strongest protein (depicted as red square) is highly organized and rich in beta-sheet secondary structure (reaching a strength of $\sim 376.68 \mathrm{pN}$ ). Conversely, the weaker protein (shown as a blue circle) reaches a strength of just 74.1 pN ). The relative weakness of this protein can be explained by the structure, showing more disordered regions with only small helical and sheet domains. To show that the model is still capable of solving hierarchical honeycomb design tasks (in alignment with the excellent R2 values seen in Figure 12d-e), we depict a sample design task in Figure 12f. It is anticipated that the protein tasks could be developed towards more accurate predictions, e.g. using a pre-training strategy (unsupervised training of large sets of protein sequences, which is likely advantageous for highly complex problems like protein behavior). We did not yet train an inverse task for protein design, and leave this to future work (as it would require folding the new sequences, followed by molecular simulations, to adequately test the predictions). Generally, we anticipate that such tasks can be accomplished well as evidenced by recent studies. This is left to future work.</p>
<h1>3. Conclusions</h1>
<p>We proposed MelM, a mechanics language model based on autoregressive, decoder-only transformer architectures, which provides a flexible platform for diverse mechanics problems, for both forward and inverse problems (see, Figures 1-4 for problem definition and overall approach). The flexible nature of the modeling strategy, applied here to systems ranging from hierarchical honeycombs to carbon nanotubes to protein mechanics allows us to easily incorporate these models into a wide range of applications and solve multiple and complex tasks, as shown in Table 1. While the studies reported here were focused on only three sample applications, we anticipate that it can be generalized towards other use cases with the potentiality of emergent behavior across tasks, modalities (mixing text, images, numbers, etc.), and domains of knowledge [3,30,32,55]. Especially via the use of the VQ-VAE [54,56] model to encode diverse high-dimensional data in language form, numerous applications can be realized.</p>
<p>The model was trained in stages, showing great potential for adding further data, knowledge or variations of the tasks as data or needs grow. It is also conceivable to train a model on a larger set of unlabeled data (e.g. more microstructures encoded by the VQ-VAE model) and then fine-tune on a smaller, labeled dataset. Such a strategy can, for instance, provide a reservoir of design ideas that can be used to solve inverse problems, where the solutions can exceed those of the training data. Similar emergent behavior has been seen in related applications in chemistry [14] and protein properties [3]. The use of the model in the context of ingesting molecular modeling results and generalizing these towards rapid, effective and generative predictions (as done for all three application examples) is a powerful upscaling strategy that can complement existing multi-scale methods.</p>
<p>It is anticipated that this class of models can find many useful applications in applied mechanics and related fields (e.g. fracture, dislocation mechanics, diffusion, dynamics, etc. [57-64]), especially due to its ease of training and use. One of the compelling features is the lack of a need to encode numerical values differently from text. Here, numbers and their representations (i.e. characters $0 \ldots 9$ and punctuation, like ',' are simply part of the elementary level of building blocks and form part of the input, output, and are naturally part of the interactive dialog with the model). This implies that such a model could easily be trained on mechanics theory, proofs, derivations, and other concepts used widely in theoretical and applied mechanics. By ingesting textbooks (e.g. equations formatted consistently in LaTeX ), this theoretical framework may be able to learn powerful concepts that can be used in the development of new insights or solutions to problems. Within this context, a particularly exciting avenue is to use pre-trained models such as GPT-4, PaLM, LaMDA, or Falcon. The advantage of using models trained on diverse knowledge is that they generate additional convergent insights that can collectively lead to emergent behavior. This potentiality can be tapped into by fine-tuning these to incorporate both linguistic and logical reasoning capabilities (these exist in such general-purpose models) with the ability to solve mechanics problems (the focus on the MeLM model) [5,6,65,66]. The models reported here, and other large language models, can be effectively fine-tuned by using strategies such as Low-Rank Adaptation of Large Language Models (LoRA) adaptor methods [67]. Such strategies are particularly effective to avoid catastrophic forgetting, which can complement fine-tuning tasks that need to rely on earlier training data in downstream deployments.</p>
<h1>4. Materials and Methods</h1>
<h3>4.1 Dataset construction and tokenizer</h3>
<p>We use byte-level tokenization similar to that used in the T5 model [68] to represent UCS Transformation Format 8 (utf8) codes in 384 tokens (each character is represented by one to four bytes; the vocabulary size includes the 256 utf8 byte-level tokens plus a series of special tokens that are not yet used here, but can be used in future adaptations of the model). We thereby encode a variety of tasks including a mix of numbers and text characters; and it also opens the door to future training/fine-tuning of the model to include other datasets. Token sequences are encoded using trainable embedding layers. Various examples of the string construction are given in the main text and in Table 1.</p>
<p>Figure 1c shows the stress-strain response calculated using coarse-grained molecular modeling used to construct the dataset. These results are contained in earlier work (see [50]), resulting in a dataset of 1,445 pairs of microstructure and stress-strain data (for the training data, we use these pairings to generate training text data for both forward and inverse tasks, doubling the size). To train the model to understand the design principles of the language encoding based on the VQ-VAE model, we include a Structure&lt;..&gt; training task, where we include all known design sequences as a reservoir for training. If separate pre-training would be used, this task would be trained first, in isolation, as it does not require any labeled data (this is akin to web scraping before fine-tuning a LLM model on downstream tasks, such as translation or question-answering is done).</p>
<p>The carbon nanotube training data is taken from [69], consisting of 818 pairs of chirality $(n, m)$ and associated mechanical properties (here, we solely focus on the Young's modulus). In the original work, CNT properties were calculated using molecular dynamics modeling. The protein data is taken from the Bio-molecule Stretching Database (BSDB) [70,71], providing pairs of sequences and associated mechanical properties (here we solely focus on the maximum unfolding force). We clean the data by considering only forces below 475 pN , and lengths below 256 amino acids. A total of 16,618 pairs of data is included. In the original work [71], the unfolding forces were calculated using coarse-grained molecular modeling.
$90 \%$ of the data is used for training, and $10 \%$ as test set. Additional validation cases are considered for the hierarchical honeycomb analysis.</p>
<h3>4.2 Attention based transformer model</h3>
<p>Figure 4 shows an overview of the transformer architecture used in this study (following an architecture similar to those used in common GPT models [1-3,43]), which processes input tasks defined by strings of text, and</p>
<p>predicts outputs from it via a series of causal self-attention layers [42]. We use a decoder-only transformer architecture as shown in Figure 4a, featuring rotary position embeddings (RoPE) in order to realize explicit relative position dependency of input tokens as proposed in [72]Rotary position embeddings facilitate broad generalization to varied sequence lengths [73]. The model is developed using causal multi-headed attention to yield an autoregressive model that predicts the next token from the previous ones (Figure 4b).</p>
<p>During training, categorical cross-entropy loss is used to train the model to predict the correct next token for training data, where the training data features complete tasks and the resulting output (see Table 1). The training strategy can be realized simply by shifting the tokens of the output (forming labels) to the left; hence, the model is trained to predict the next tokens from the previous ones. The model is trained simultaneously to solve the forward and inverse problem. As shown in Figure 4c, a trained model can be used to check whether a predicted microstructure yields the desired stress-strain response, to assess cycle consistency.</p>
<p>The key mathematical operation is the masked attention mechanism[11,42]:</p>
<p>$$
\operatorname{Attention}(Q, K, V ; M)=\operatorname{softmax}\left(\frac{Q K^{T}+M}{\sqrt{d_{k}}}\right) V
$$</p>
<p>with a triangular mask $M$ (e.g. here for a sequence length 3):</p>
<p>$$
M=\left(\begin{array}{ccc}
0 &amp; -\infty &amp; -\infty \
0 &amp; 0 &amp; -\infty \
0 &amp; 0 &amp; 0
\end{array}\right)
$$</p>
<p>(this masking strategy ensures that the model only attends to tokens to the left to enforce causality. The causal attention calculation is implemented in multi-headed form by using parallelly stacked attention layers, with a total dimension $d$. The softmax function in (1) turns a vector of $L$ real values into a vector of $L$ real values that sum to 1. Instead of only computing the attention once, in the multi-head strategy (where $h$ denotes the number of attention heads) we divide the input into segments (in the dimension of the hidden dimension, that is, $d_{v, i}=d / h$ ). We calculate scaled dot-product attention over each segment, allowing the model to jointly attend to information from different representation subspaces at different positions:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{MultiHead}(Q, K, V)=\operatorname{Concat}\left(\text { head }<em h="h">{1}, \ldots, \text { head }</em> \
&amp; \text { head }}\right) W^{O<em i="i">{i}=\operatorname{Attention}\left(Q W</em>\right)
\end{aligned}
$$}^{Q}, K W_{i}^{K}, V W_{i}^{V</p>
<p>In self-attention as used in this paper, all $Q, K, V$ come from either input or output embeddings (or other sources) only.</p>
<p>During generation (inference), the task sequence is fed into the model and the output is predicted from it. During sampling iterations, this process is repeated until the full output is produced. All conditioning and distinction of various tasks is provided by the input prompt. Additional sets of tokens are defined to encapsulate various tasks and input/output boundaries &lt;..&gt; encapsulate task, [..] to encapsulate prediction). Causal autoregressive training is performed using cross-entropy loss, where the next token in the input sequence $z$ is the label for the current token (i.e., labels start with the second token of the input, and we remove the last logit since no label exists, see a visual representation in Figure 4b). The training data the task and the corresponding prediction in one sentence, comprising all associated tokens:</p>
<p>$$
z=\left[z_{1}, z_{2}, \ldots, z_{N}\right]
$$</p>
<p>Gumbel softmax sampling [74,75] is used during inference (however, we also explored the use of beam search). This allows one to adapt the creativity of the model. This is achieved by adding a defined level of noise controlled via the sampling temperature $T$ to a fractional set of logit distributions (identified by a sampling threshold) predicted by the transformer model. We then sample the predicted token from this revised distribution. This helps to add expressivity to the generative tasks to achieve more variations in the predictions ( $T$ around or larger than 1). In forward prediction tasks we find that they are best conducted using low sampling temperatures ( $T=0.01$ ).</p>
<p>The model features a dimension of 512 (model used for gene vector microstructure encoding, as well as protein mechanics and carbon nanotube mechanics) and 1024 (model trained for vector quantized encoding). We use a total of 8 attention heads, transformer depth $\mathrm{N}_{\mathrm{T}}=24$, and a feed forward multiplier=4 (2048/4096 channels) (126.4M parameters/303.1M parameters, respectively). We use Gaussian Error Linear Unit (GELU) activation functions [76].</p>
<h1>4.3 Vector quantized variational autoencoder (VQ-VAE) model</h1>
<p>The vector quantized variational autoencoder (VQ-VAE) architecture is used as trained in [54]. We provide some background here about this model for clarity and completeness. The VQ-VAE model learns to encode microstructure images (or other high-dimensional data) into a lower-dimensional, discrete language space. This discrete codebook representation consists of a one-dimensional vector of length $N$ where each entry is one of $n_{c}$ possible "words" in the design language that defines the microstructures. The encoder and decoder blocks each consist of a deep neural network featuring convolutional and self-attention layers (so that the model can effectively learn both coarse-graining features and long-/and short-range relationships, effectively deciphering a design language that captures the space of microstructures). The VQ-VAE model is trained based on unlabeled data of microstructure images. In further developments, this model can be expanded and trainer either on a different set of microstructures or other data, or to feature additional design elements for greater diversity of predictions that can be introduced in the unsupervised training stage of this model. In its current implementation the model can predict a range of hierarchical honeycomb structures; not limited to the specific designs used during training (as can be seen in the results in Figure 10, right two panels), but in the general structural space as the training data).</p>
<h3>4.4 Training process and other hyperparameters</h3>
<p>All code used in this study is developed in PyTorch [77], implemented within the Hugging Face ecosystem. Training is performed using an Adam optimizer [78] with a learning rate of $\mathrm{LR}=0.0001$ and $\varepsilon=1 \mathrm{E}-8$. We use 1,000 warmup training steps (during which the learning rate is ramped from 0 to the desired learning rate, LR), followed by linear decay (training is performed over 20-30 epochs). During training we assess the R2 value of cycle consistent design tasks during training on a test set, and pick the training epoch with the highest R2 for this task (this ensures good forward and inverse performance). The models are developed in Hugging Face for easy accessibility and further fine-tuning.</p>
<h2>Author contributions</h2>
<p>M.J.B. developed the overall concept and the algorithm, designed the ML model, developed the codes, oversaw the work, and drafted the paper.</p>
<p>Code availability: The MeLM model, code, trained weights, and data is available at: https://github.com/lammmit/MeLM.</p>
<p>Acknowledgements: This work was supported by the MIT-IBM Watson AI Lab, the Army Research Office (W911NF1920098 \&amp; W911NF2220213), ONR (N00014-19-1-2375 and N00014-20-1-2189), as well as USDA (2021-69012-35978).</p>
<h2>References</h2>
<p>[1] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I., "Language Models Are Unsupervised Multitask Learners," life-extension.github.io.
[2] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark,</p>
<p>J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D., 2020, "Language Models Are Few-Shot Learners," Adv Neural Inf Process Syst, 2020-December.
[3] Buehler, M. J., 2023, "Generative Pretrained Autoregressive Transformer Graph Neural Network Applied to the Analysis and Discovery of Novel Proteins."
[4] Bates, M., 1995, "Models of Natural Language Understanding," Proc Natl Acad Sci U S A, 92(22), pp. 9977-9982.
[5] Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Huaixiu, H. L., Zheng, S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhao, V., Zhou, Y., Chang, C.C., Krivokon, I., Rusch, W., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern, K., Ringel, M., Tulsee, M., Renelito, D., Santos, D., Duke, T., Soraker, J., Zevenbergen, B., Prabhakaran, V., Diaz, M., Hutchinson, B., Olson, K., Molina, A., Hoffman-John, E., Lee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm, M., Kuzmina, V., Fenton, J., Cohen, A., Bernstein, R., Kurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi, E., and Le Google, Q., 2022, "LaMDA: Language Models for Dialog Applications."
[6] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N., 2022, "PaLM: Scaling Language Modeling with Pathways."
[7] Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R., 2022, "Galactica: A Large Language Model for Science."
[8] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever, "Improving Language Understanding by Generative Pre-Training."
[9] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I., 2021, "Learning Transferable Visual Models From Natural Language Supervision."
[10] Brodnik, N., Carton, S., Muir, C., Ghosh, S., Downey, D., Echlin, M. P., Pollock, T. M., and Daly, S. H., 2023, "Perspective: Large Language Models in Applied Mechanics," J Appl Mech, pp. 1-12.
[11] Hu, Y., and Buehler, M. J., 2023, "Deep Language Models for Interpretative and Predictive Materials Science," APL Machine Learning, 1(1), p. 010901.
[12] Peng, G. C. Y., Alber, $\cdot$ Mark, Adrian, $\cdot$, Tepole, B., Cannon, W. R., De, S., Dura-Bernal, S., Garikipati, K., Karniadakis, G., William, $\cdot$, Lytton, W., Perdikaris, P., Petzold, L., Kuhl, E., Alber, M., Tepole, A. B., and Lytton, W. W., "Multiscale Modeling Meets Machine Learning: What Can We Learn?," Archives of Computational Methods in Engineering, 1, p. 3.
[13] Luu, R. K., and Buehler, M. J., 2023, "Materials Informatics Tools in the Context of Bio-Inspired Material Mechanics," J Appl Mech, 90(9).
[14] Luu, R. K., Wysokowski, M., and Buehler, M. J., 2023, "Generative Discovery of Novel Chemical Designs Using Diffusion Modeling and Transformer Deep Neural Networks with Application to Deep Eutectic Solvents," Appl Phys Lett, 122(23).</p>
<p>[15] Buehler, M. J., 2022, "Modeling Atomistic Dynamic Fracture Mechanisms Using a Progressive Transformer Diffusion Model," J. Appl. Mech., 89(12), p. 121009.
[16] Buehler, M. J., 2023, "Predicting Mechanical Fields near Cracks Using a Progressive Transformer Diffusion Model and Exploration of Generalization Capacity," J. Mater. Res.
[17] Jung, G. S., and Buehler, M. J., 2017, "Multiscale Modeling of Muscular-Skeletal Systems," Annu Rev Biomed Eng.
[18] Barreiro, D. L., Yeo, J., Tarakanova, A., Martin-martinez, F. J., and Buehler, M. J., 2019, "Multiscale Modeling of Silk and Silk-Based Biomaterials - A Review," 1800253, pp. 1-9.
[19] Chen, X., and Drapaca, C., 2022, "On the Dissipation of Conforming and Discontinuous Galerkin Schemes for the Incompressible Navier-Stokes Equations," AIP Adv, 12(7), p. 75004.
[20] Aboelkassem, Y., Powers, J. D., McCabe, K. J., and McCulloch, A. D., 2019, "Multiscale Models of Cardiac Muscle Biophysics and Tissue Remodeling in Hypertrophic Cardiomyopathies," Curr Opin Biomed Eng, 11, pp. 35-44.
[21] Bock, F. E., Aydin, R. C., Cyron, C. J., Huber, N., Kalidindi, S. R., and Klusemann, B., 2019, "A Review of the Application of Machine Learning and Data Mining Approaches in Continuum Materials Mechanics," Front Mater, 6(May).
[22] McCulloch, A. D., 2023, "How Can AI Accelerate Advances in Physiology?," Journal of General Physiology, 155(6).
[23] Wang, S., Sun, S., Li, Z., Zhang, R., and Xu, J., 2017, "Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model," PLoS Comput Biol, 13(1).
[24] Du, Z., Su, H., Wang, W., Ye, L., Wei, H., Peng, Z., Anishchenko, I., Baker, D., and Yang, J., 2021, "The TrRosetta Server for Fast and Accurate Protein Structure Prediction," Nat Protoc, 16(12), pp. 5634-5651.
[25] Suwardi, A., Wang, F. K., Xue, K., Han, M. Y., Teo, P., Wang, P., Wang, S., Liu, Y., Ye, E., Li, Z., and Loh, X. J., 2022, "Machine Learning-Driven Biomaterials Evolution," Advanced Materials, 34(1).
[26] Alber, M., Buganza Tepole, A., Cannon, W. R., De, S., Dura-Bernal, S., Garikipati, K., Karniadakis, G., Lytton, W. W., Perdikaris, P., Petzold, L., and Kuhl, E., 2019, "Integrating Machine Learning and Multiscale Modeling-Perspectives, Challenges, and Opportunities in the Biological, Biomedical, and Behavioral Sciences," npj Digital Medicine 2019 2:1, 2(1), pp. 1-11.
[27] Martínez-Martínez, F., Rupérez-Moreno, M. J., Martínez-Sober, M., Solves-Llorens, J. A., Lorente, D., Serrano-López, A. J., Martínez-Sanchis, S., Monserrat, C., and Martín-Guerrero, J. D., 2017, "A Finite Element-Based Machine Learning Approach for Modeling the Mechanical Behavior of the Breast Tissues under Compression in Real-Time," Comput Biol Med, 90, pp. 116-124.
[28] Hu, Y., and Buehler, M. J., 2022, "End-to-End Protein Normal Mode Frequency Predictions Using Language and Graph Models and Application to Sonification," ACS Nano, 16(12), pp. 20656-20670.
[29] Xue, K., Wang, F. K., Suwardi, A., Han, M. Y., Teo, P., Wang, P., Wang, S., Ye, E., Li, Z., and Loh, X. J., 2021, "Biomaterials by Design: Harnessing Data for Future Development," Mater Today Bio, 12, p. 100165 .
[30] B. Ni, D.L. Kaplan, and M.J. Buehler, 2023, "Generative Design of de Novo Proteins Based on Secondary Structure Constraints Using an Attention-Based Diffusion Model," Chem, p. https://doi.org/10.1016/j.chempr.2023.03.020.
[31] Lew, A. J., and Buehler, M. J., 2023, "Single-Shot Forward and Inverse Hierarchical Architected Materials Design for Nonlinear Mechanical Properties Using an Attention-Diffusion Model," Materials Today.</p>
<p>[32] Hsu, Y.-C., Yang, Z., and Buehler, M. J., 2022, "Generative Design, Manufacturing, and Molecular Modeling of 3D Architected Materials Based on Natural Language Input," APL Mater, 10(4), p. 041107.
[33] Yang, Z., and Buehler, M. J., 2021, "Words to Matter: De Novo Architected Materials Design Using Transformer Neural Networks," Front Mater, 8, p. 740754.
[34] Guo, K., and Buehler, M. J., 2020, "A Semi-Supervised Approach to Architected Materials Design Using Graph Neural Networks," Extreme Mech Lett, 41, p. 101029.
[35] Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. v., and Salakhutdinov, R., 2019, "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context," ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pp. 2978-2988.
[36] Schwaller, P., Laino, T., Gaudin, T., Bolgar, P., Hunter, C. A., Bekas, C., and Lee, A. A., 2019, "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction," ACS Cent Sci, 5(9), pp. 1572-1583.
[37] Kitaev, N., Kaiser, Ł., Levskaya, A., and Research, G., 2020, "Reformer: The Efficient Transformer."
[38] Micheli, V., Alonso, E., and Fleuret, F., 2022, "Transformers Are Sample-Efficient World Models."
[39] Esser, P., Rombach, R., and Ommer, B., 2020, "Taming Transformers for High-Resolution Image Synthesis."
[40] Devlin, J., Chang, M. W., Lee, K., and Toutanova, K., 2019, "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding," NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies Proceedings of the Conference, 1(Mlm), pp. 4171-4186.
[41] Buehler, M. J., 2022, "FieldPerceiver: Domain Agnostic Transformer Model to Predict Multiscale Physical Fields and Nonlinear Material Properties through Neural Ologs," Materials Today, 57, pp. 9-25.
[42] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I., 2017, "Attention Is All You Need," Advances in Neural Information Processing Systems, Neural information processing systems foundation, pp. 5999-6009.
[43] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang, Y., 2023, "Sparks of Artificial General Intelligence: Early Experiments with GPT-4."
[44] Spivak, D. I., Giesa, T., Wood, E., and Buehler, M. J., 2011, "Category Theoretic Analysis of Hierarchical Protein Materials and Social Networks," PLoS One, 6(9).
[45] Giesa, T., Spivak, D. I., and Buehler, M. J., 2012, "Category Theory Based Solution for the Building Block Replacement Problem in Materials Design," Adv Eng Mater, 14(9).
[46] Buehler, M. J., 2022, "Multiscale Modeling at the Interface of Molecular Mechanics and Natural Language through Attention Neural Networks," Acc Chem Res, 55(23), pp. 3387-3403.
[47] Buehler, M. J., 2023, "Unsupervised Cross-Domain Translation via Deep Learning and Adversarial Attention Neural Networks and Application to Music-Inspired Protein Designs," Patterns, 0(0), p. 100692.
[48] Buehler, M. J., "Domain Agnostic Transformer Model to Predict Multiscale Physical Fields and Nonlinear Material Properties through Neural Ologs," Materials Today.
[49] Buehler, M. J., Hartmaier, A., Duchaineau, M. A., Abraham, F. F., and Gao, H., 2005, "Dynamical Complexity of Work-Hardening: A Large-Scale Molecular Dynamics Simulation," Acta Mechanica Sinica/Lixue Xuebao, 21(2), pp. 103-111.</p>
<p>[50] Lew, A. J., Jin, K., and Buehler, M. J., 2023, "Architected Materials for Mechanical Compression: Design via Simulation, Deep Learning, and Experimentation," NPJ Comput Mater, 9.
[51] Veličković, P., Casanova, A., Liò, P., Cucurull, G., Romero, A., and Bengio, Y., 2017, "Graph Attention Networks," 6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings.
[52] "ChatGPT Gets Its 'Wolfram Superpowers'!-Stephen Wolfram Writings" [Online]. Available: https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/. [Accessed: 26-Jun2023].
[53] He-Yueya, J., Poesia, G., Wang, R. E., and Goodman, N. D., 2023, "Solving Math Word Problems by Combining Language Models With Symbolic Solvers."
[54] Buehler, M. J., 2023, "A Computational Building Block Approach towards Multiscale Architected Materials Analysis and Design with Application to Hierarchical Metal Metamaterials," Model Simul Mat Sci Eng, 31(5), p. 054001.
[55] Yang, Z., Hsu, Y.-C., and Buehler, M. J., 2023, "Generative Multiscale Analysis of de Novo ProteomeInspired Molecular Structures and Nanomechanical Optimization Using a VoxelPerceiver Transformer Model," J Mech Phys Solids, 170, p. 105098.
[56] Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., and Wu, Y., 2021, "Vector-Quantized Image Modeling with Improved VQGAN."
[57] Li, X., Wei, Y., Yang, W., and Gao, H., 2009, "Competing Grain-Boundary- and Dislocation-Mediated Mechanisms in Plastic Strain Recovery in Nanocrystalline Aluminum," Proc Natl Acad Sci U S A, 106(38), pp. 16108-16113.
[58] Zhou, H., Zhu, P., Yang, W., and Gao, H., 2022, "A Gradient Eshelby Force on Twinning Partial Dislocations and Associated Detwinning Mechanism in Gradient Nanotwinned Metals," J Mech Phys Solids, 159, p. 104746.
[59] Buehler, M. J., Gao, H., and Huang, Y., 2003, "Atomistic and Continuum Studies of a Suddenly Stopping Supersonic Crack," Comput Mater Sci, 28(3-4 SPEC. ISS.), pp. 385-408.
[60] Abid, N., Pro, J. W., and Barthelat, F., 2019, "Fracture Mechanics of Nacre-like Materials Using DiscreteElement Models: Effects of Microstructure, Interfaces and Randomness," J Mech Phys Solids, 124, pp. $350-365$.
[61] Yang, Z., Yu, C.-H., Guo, K., and Buehler, M. J., 2021, "End-to-End Deep Learning Method to Predict Complete Strain and Stress Tensors for Complex Hierarchical Composite Microstructures," J Mech Phys Solids, 154, p. 104506.
[62] Gao, E., Lin, S.-Z., Qin, Z., Buehler, M. J., Feng, X.-Q., and Xu, Z., 2018, "Mechanical Exfoliation of Two-Dimensional Materials," J Mech Phys Solids, 115.
[63] Akinwande, D., Brennan, C. J., Bunch, J. S., Egberts, P., Felts, J. R., Gao, H., Huang, R., Kim, J. S., Li, T., Li, Y., Liechti, K. M., Lu, N., Park, H. S., Reed, E. J., Wang, P., Yakobson, B. I., Zhang, T., Zhang, Y. W., Zhou, Y., and Zhu, Y., 2017, "A Review on Mechanics and Mechanical Properties of 2D MaterialsGraphene and Beyond," Extreme Mech Lett, 13, pp. 42-77.
[64] Lejeune, E., 2020, "Mechanical MNIST: A Benchmark Dataset for Mechanical Metamodels," Extreme Mech Lett, 36, p. 100659.
[65] "Falcon LLM - Home" [Online]. Available: https://falconllm.tii.ae/. [Accessed: 26-Jun-2023].</p>
<p>[66] Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J., 2023, "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only."
[67] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W., 2021, "LoRA: Low-Rank Adaptation of Large Language Models."
[68] Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and Raffel, C., 2021, "ByT5: Towards a Token-Free Future with Pre-Trained Byte-to-Byte Models," Trans Assoc Comput Linguist, 10, pp. 291-306.
[69] Čanađija, M., 2021, "Deep Learning Framework for Carbon Nanotubes: Mechanical Properties and Modeling Strategies," Carbon N Y, 184, pp. 891-901.
[70] Liu, F. Y. C., Ni, B., and Buehler, M. J., 2022, "PRESTO: Rapid Protein Mechanical Strength Prediction with an End-to-End Deep Learning Model," Extreme Mech Lett, 55, p. 101803.
[71] Sikora, M., Sułkowska, J. I., and Cieplak, M., 2009, "Mechanical Strength of 17134 Model Proteins and Cysteine Slipknots," PLoS Comput Biol, 5(10), p. 1000547.
[72] Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y., 2021, "RoFormer: Enhanced Transformer with Rotary Position Embedding."
[73] Andonian, A., Anthony, Q., Biderman, S., Black, S., Gali, P., Gao, L., Hallahan, E., Levy-Kramer, J., Leahy, C., Nestler, L., Parker, K., Pieler, M., Purohit, S., Songz, T., Phil, W., and Weinbach, S., 2021, "GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch."
[74] Maddison, C. J., Mnih, A., and Teh, Y. W., 2016, "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables," 5th International Conference on Learning Representations, ICLR 2017 Conference Track Proceedings.
[75] Jang, E., Gu, S., and Poole, B., 2016, "Categorical Reparameterization with Gumbel-Softmax," 5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings.
[76] Hendrycks, D., and Gimpel, K., 2016, "Gaussian Error Linear Units (GELUs)."
[77] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S., 2019, "PyTorch: An Imperative Style, High-Performance Deep Learning Library."
[78] Kingma, D. P., and Ba, J., 2014, "Adam: A Method for Stochastic Optimization."
[79] Marcus, G., Davis, E., and Aaronson, S., 2022, "A Very Preliminary Analysis of DALL-E 2."</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the overall problem solved, and details on the mechanics problem studied in this paper, featuring hierarchical honeycombs, carbon nanotubes and proteins. Panel a shows the two major classes of mechanics problems modeled, including the forward task (e.g., predict constitutive response from a microstructure) and the inverse problem (e.g., predict candidate microstructures to meet a specified stress-strain response). Panel b depicts details on the honeycomb design, inspired by natural honeybee hives (image generated using DALL-E [79]), and potential additive manufacturing routes. The right panel depicts the hierarchical design principles underlying these structures (figure adapted from [54]). Panel d shows the stress-strain response calculated using coarse-grained molecular modeling (as done in earlier work [50], including associated experimental validation). The constitutive response to compressive loading is considered in this work, albeit there is no fundamental limitation as to what properties can be modeled. Panel d shows a cycle of the forward and inverse problems for an example microstructure, setting the stage for the problem tackled in this paper. Panels e and f show additional applications studied in this paper, including carbon nanotube nanomechanics (e) and protein unfolding mechanics (f).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of the MeLM model implemented in this study. Panel a: A language model is used to solve a variety of tasks, such as calculate properties or generate new designs. The input is provided to the model as utf8 encoded string data, which includes tasks, numbers, and other details of the problem solved. As ingle language model, implemented as an autoregressive decoder-only transformer model is trained to solve these tasks. The model predicts text output in response to the queries, which can be mined to either extract numerical values (e.g. resulting stress-strain response) or a microstructure design. Panel b shows two sample prompts. During training, the entire sequence is fed (consisting of the task, and the result (marked in bold). When the model is used to make predictions (i.e. during inference), only the task is provided, and the model predicts the result of the task by completing the sentence (predicting the bold characters).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Strategies used to encode microstructural design. The first method, as first reported in [50], uses a gene vector to define a mathematical encoding of the microstructure as shown in panel a. In this approach, the microstructure is defined by a series of numbers, denoted the gene vector. Panel b shows a more flexible approach. Following the vector quantized variational autoencoder model reported in [54] we use a codebook language-based representation of microstructures (therein, a microstructure image is encoded via 64 tokens defined by integer numbers between $0 \ldots 127$; to realize an encoded language representation). While the gene vector approach in a works very well for the designs studied here, the approach using a flexible language encoding is offers a series of advantages and can be applied to any type of data. This method can be flexibly applied to a variety of other high-dimensional data, such as images, voxel data, etc.</p>
<p>a
<img alt="img-3.jpeg" src="img-3.jpeg" />
d
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Overview of the transformer architecture used in this study (following an architecture similar to those used in common GPT models [1-3,43]), which processes input tasks defined by strings of text, and predicts outputs from it via a series of causal self-attention layers [42]. We use a decoder-only transformer architecture as shown in panel a. The model is implemented using causal multi-headed attention to yield an autoregressive model that predicts the next token from the previous ones (panel b). During training, categorical cross-entropy loss is used to train the model to predict the correct next token for training data, where the training data features complete tasks and the resulting output (see Table 1). The training strategy can be realized simply by shifting the tokens of the output (forming labels) to the left; hence, the model is trained to predict the next tokens from the previous ones. The model is trained simultaneously to solve the forward and inverse problem. As shown in panel c, a trained model can be used to check whether a predicted microstructure yields the desired stress-strain response, to assess cycle consistency. Panel d shows an overview of the multi-headed graph forming strategy implemented in the architecture, showing how information is processed across multiple attention heads, resulting in a set of graphs ultimately fed to produce the output probability distribution.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Results from forward and inverse tasks of hierarchical honeycomb mechanics, depicting how the predicted stresses compare with the ground truth (GT) stresses. Panels a-b show the result for the model using gene vector microstructure encoding (as shown in Figure 3a; a, forward task: R2=0.95, b, inverse task: R2=0.96), and Panels c-d show the result for the vector quantized language encoding (c, forward task: R2=0.96, d, inverse task: R2=0.91). The two microstructure encoding strategies show similar performance, albeit the gene vector approach yields slightly better performance for both forward and inverse tasks. The sampling temperature for all predictions is $T_{\text {sample }}=0.01$.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: Solving the inverse task and comparing the constitutive behavior of the designs with the desired response (as shown in Figure 4c, top panel), using the gene vector encoding strategy, for a test set of microstructures not included in the training of the model. For the three examples shown, it can be seen that the model does an excellent job in matching the desired stress-strain response. The right column, labeled GT, shows the microstructure designs in the dataset. As can be seen, the model proposes new, distinct, designs that yield the desired response. The sampling temperature is $T_{\text {sample }}=0.01$.</p>
<p>Input (and calculated stressstrain from Prediction)
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Solving the inverse task and comparing the constitutive behavior of the designs with the desired response (as shown in Figure 4c, top panel), using the vector quantized language encoding strategy, for a test set of microstructures not included in the training of the model. The model does an excellent job in matching the desired stress-strain response. The right column, labeled GT, shows the microstructure designs in the dataset. As can be seen, the model proposes new, distinct, designs that yield the desired response. The sampling temperature is $T_{\text {sample }}=0.01$.</p>            </div>
        </div>

    </div>
</body>
</html>
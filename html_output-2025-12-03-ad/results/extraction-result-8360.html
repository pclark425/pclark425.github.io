<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8360 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8360</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8360</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-a4f8d79f85ddb7e835ef56a6a44c6821ddeea313</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a4f8d79f85ddb7e835ef56a6a44c6821ddeea313" target="_blank">Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Analysis reveals that latent representations of the intermediate answer are constructed more often in queries with higher latent composability, and shows the emergence of latent multi-hop reasoning during pretraining.</p>
                <p><strong>Paper Abstract:</strong> We evaluate how well Large Language Models (LLMs) latently recall and compose facts to answer multi-hop queries like"In the year Scarlett Johansson was born, the Summer Olympics were hosted in the country of". One major challenge in such evaluation is that LLMs may have developed shortcuts by encountering the head entity"Scarlett Johansson"and the answer entity"United States"in the same training sequences or merely guess the answer based on frequency-based priors. To prevent shortcuts, we exclude test queries where the head and answer entities might have co-appeared during training. Through careful selection of relations and facts and systematic removal of cases where models might guess answers or exploit partial matches, we construct an evaluation dataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs demonstrate promising latent multi-hop reasoning abilities without exploiting shortcuts, but only for certain types of queries. For queries requiring latent recall of countries as the intermediate answer, the best models achieve 80% latent composability, but this drops to just 5% for the recall of years. Comparisons with Chain-of-Thought highlight a significant gap between the ability of models to reason latently versus explicitly. Analysis reveals that latent representations of the intermediate answer are constructed more often in queries with higher latent composability, and shows the emergence of latent multi-hop reasoning during pretraining.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A careful examination of large language model performance on grade school arithmetic <em>(Rating: 2)</em></li>
                <li>Faith and fate: Limits of transformers on compositionality <em>(Rating: 2)</em></li>
                <li>When do you need chain-of-thought prompting for ChatGPT? <em>(Rating: 2)</em></li>
                <li>Implicit chain of thought reasoning via knowledge distillation <em>(Rating: 1)</em></li>
                <li>On limitations of the transformer architecture <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8360",
    "paper_id": "paper-a4f8d79f85ddb7e835ef56a6a44c6821ddeea313",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A careful examination of large language model performance on grade school arithmetic",
            "rating": 2
        },
        {
            "paper_title": "Faith and fate: Limits of transformers on compositionality",
            "rating": 2
        },
        {
            "paper_title": "When do you need chain-of-thought prompting for ChatGPT?",
            "rating": 2
        },
        {
            "paper_title": "Implicit chain of thought reasoning via knowledge distillation",
            "rating": 1
        },
        {
            "paper_title": "On limitations of the transformer architecture",
            "rating": 1
        }
    ],
    "cost": 0.0084535,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?</h1>
<p>Sohee Yang ${ }^{1,2}$ Nora Kassner ${ }^{1}$ Elena Gribovskaya ${ }^{1}$ Sebastian Riedel ${ }^{1,2 <em>}$ Mor Geva ${ }^{3,4 </em>}$<br>${ }^{1}$ Google DeepMind ${ }^{2}$ UCL ${ }^{3}$ Google Research ${ }^{4}$ Tel Aviv University<br>{soheeyang, norakassner, egribovskaya,srriedel, pipek}@google.com</p>
<h4>Abstract</h4>
<p>We evaluate how well Large Language Models (LLMs) latently recall and compose facts to answer multi-hop queries like "In the year Scarlett Johansson was born, the Summer Olympics were hosted in the country of". One major challenge in such evaluation is that LLMs may have developed shortcuts by encountering the head entity "Scarlett Johansson" and the answer entity "United States" in the same training sequences or merely guess the answer based on frequency-based priors. To prevent shortcuts, we exclude test queries where the head and answer entities might have co-appeared during training. Through careful selection of relations and facts and systematic removal of cases where models might guess answers or exploit partial matches, we construct an evaluation dataset Socrates (SHOrtCut-FReE LATENT REASONING). We observe that LLMs demonstrate promising latent multi-hop reasoning abilities without exploiting shortcuts, but only for certain types of queries. For queries requiring latent recall of countries as the intermediate answer, the best models achieve $80 \%$ latent composability, but this drops to just $5 \%$ for the recall of years. Comparisons with Chain-of-Thought highlight a significant gap between the ability of models to reason latently versus explicitly. Analysis reveals that latent representations of the intermediate answer are constructed more often in queries with higher latent composability, and shows the emergence of latent multi-hop reasoning during pretraining. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Latent multi-hop reasoning in Large Language Models (LLMs), or latently recalling and composing single-hop facts to answer multi-hop queries</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Evaluation of latent multi-hop reasoning should exclude cases where LLMs can bypass the process of latently composing the single-hop facts by exploiting shortcuts. LLMs can develop shortcuts when they frequently encounter the head entity ("Scarlett Johansson") or the relation pattern in the query ("the country of") with the answer entity ("United States"). We propose desiderata for shortcut-free evaluation of latent multi-hop reasoning ability.
like "In the year Scarlett Johansson was born, the Summer Olympics were hosted in the country of", has been of growing interest in recent years. First, this ability can be a measure towards better localization and controllability of factual knowledge in LLMs, as it can signal learning of a compressed representation of facts and their latent composition (Yang et al., 2024b). This would provide more hope towards locate-then-edit or unlearn paradigm of LLMs (Meng et al., 2022; Hong et al., 2024). For instance, if complex facts are redundantly learned and recalled, edits with only single-hop facts would not propagate to the relevant multi-hop facts (Onoe et al., 2023; Zhong et al., 2023; Cohen et al., 2024; Ju et al., 2024). In addition, the ability to provide accurate answers without explicit Chain-ofThought (CoT) generation (Kojima and Gu, 2022) could reduce inference costs. At the same time, whether LLMs can spontaneously develop latent reasoning abilities during pretraining is of interest from a safety perspective, as latent reasoning is less</p>
<p>visible and hard to monitor given the opaque computations in LLMs (Berglund et al., 2023; Treutlein et al., 2024; Chan et al., 2024). Taken together, these incentives raise the question of How well do today's widely-used LLMs perform latent multi-hop reasoning over factual knowledge?</p>
<p>If today's best models such as GPT-4o (OpenAI, 2024b), Claude 3.5 Sonnet (Anthropic, 2024), or Gemini 1.5 Pro (Gemini Team et al., 2024) struggle to perform latent reasoning, then the current pretraining, instruction-tuning, and scaling paradigm of LLMs may be insufficient for robust development of latent multi-hop reasoning abilities. Thus, we would need to adopt changes in this paradigm or the architecture of models to enhance their ability to latently recall, compose, and manipulate parametric knowledge. However, if there are certain cases where today's LLMs show robust latent reasoning, we could further study these cases to find the underlying causes that make latent reasoning emerge during pretraining.</p>
<p>We evaluate latent multi-hop reasoning abilities by assessing models' performance in answering multi-hop queries. While prior works have suggested that pretrained LLMs develop this ability (Ofir Press et al., 2023; Yang et al., 2024b; Biran et al., 2024; Li et al., 2024), they have not adequately addressed the possibility of models exploiting shortcuts (Elazar et al., 2022; Xu et al., 2022; Tang et al., 2023; Kang and Choi, 2023; Ju et al., 2024); our analysis shows that $85 \%$ of the test queries from a previous dataset are prone to shortcuts.</p>
<p>Shortcuts from frequent co-occurrences of subject-object or relation-object in the training data can allow models to answer the multi-hop queries correctly without going through a true latent reasoning process. For instance, in a query about "Scarlett Johansson" (i.e., the head entity) where the answer entity is "United States", LLMs may simply learn a subject-object entity shortcut if these entities frequently co-occur in training (Elazar et al., 2022; Kang and Choi, 2023; Zhang et al., 2024b; Ju et al., 2024). Similarly, LLMs can develop a relationobject shortcut from the frequency of "United States" appearing as a country and guess the answer based on the frequency-based prior (Elazar et al., 2022; Xu et al., 2022; Tang et al., 2023).</p>
<p>To overcome this challenge, we outline desiderata for shortcut-free evaluation of latent multi-hop reasoning in LLMs, which we address through dataset construction and evaluation procedure. For
the dataset, we only use test queries where the head and answer entities are estimated to never co-occur in pretraining sequences, thus minimizing subjectobject shortcuts. We further minimize shortcuts by carefully selecting relation types and removing queries where the answers are easy to guess from a substring of the head entity. For evaluation, we measure latent composability as the rate of correct multi-hop answers when single-hop facts are known, without explicitly generating the intermediate answer (i.e., the bridge entity). Additionally, we reduce relation-object shortcuts by excluding queries where the model may guess the answer without considering the head entity.</p>
<p>The main challenge in satisfying our desiderata is that most LLMs' pretraining data is inaccessible, making it impossible to directly check entity co-occurrences. To tackle this, we use a proxy corpus of roughly 4.8B unique documents by utilizing six publicly available training corpora, selecting only test queries where the head and answer entities never co-occur. This approximation's effectiveness is validated by showing that strong latent composability for specific query types persists even when extending our entity co-occurrence check to the whole web via Google Search. Our resulting dataset, SOCRATES (SHORTCut-FREE LATENT REASONING), consists of 7,232 pairs of single-hop and multi-hop queries of 17 types of relation compositions with 4 types of bridge entities. Comparative experiments with a dataset constructed from the same data distribution but without careful fact selection, co-occurrence-based filtering, and rigorous evaluation show that latent composability can be overestimated without satisfying the desiderata.</p>
<p>Our results for 41 LLMs from 9 families reveal that there are successful cases of latent multi-hop reasoning, but the performance varies substantially according to the type of bridge entity that connects the facts. Notably, state-of-the-art models demonstrate strong latent composability of over $80 \%$ when the bridge entity is a country. However, the number is only around $6 \%$ for year-based queries, highlighting the importance of considering the distribution of relation composition types when evaluating LLMs' latent reasoning abilities. Models that know more single-hop facts tend to reason better latently, and the ability marginally improves with model scale. On the contrary, CoT composability effectively increases with the number of known facts and model size, with much higher and consistent performance across bridge entity types.</p>
<p>Additional analysis shows that the latent representation of the bridge entity is clearly constructed more often for queries with higher latent composability, and reveals the emergence of latent multi-hop reasoning during pretraining.</p>
<p>In summary, our contributions are as follows:</p>
<ul>
<li>We present SOCRATES and evaluation procedure for latent multi-hop reasoning with minimal risk of shortcut exploitation, which is corroborated to be important through a comparative analysis.</li>
<li>We show that latent composability in LLMs significantly varies according to the bridge entity type.</li>
<li>We show that latent reasoning marginally improves with the number of known single-hop facts and model scale and identify a significant gap between latent and CoT composability.</li>
<li>We present additional analysis results that help better understand LLMs’ mechanisms for latent multi-hop reasoning.</li>
</ul>
<h2>2 Related Work</h2>
<p>Studies have shown that LLMs’ predictions often rely on shortcuts, shallow heuristics, and cooccurrence biases (Chen and Durrett, 2019; Jiang and Bansal, 2019; Geirhos et al., 2020; Elazar et al., 2022; Zhang et al., 2022a; Xu et al., 2022; Liu et al., 2023; Tang et al., 2023; Kang and Choi, 2023; Bachmann and Nagarajan, 2024; Ju et al., 2024). For instance, Elazar et al. (2022) have found that single-hop knowledge predictions can be influenced by subject-object co-occurrences or relationobject co-occurrences. Similarly, Kang and Choi (2023) and Zhang et al. (2024b) show that frequent co-occurrences can lead LLMs to favor highfrequency words over correct responses. Lastly, Ju et al. (2024) demonstrate that head-answer entity co-occurrence frequencies in multi-hop facts are correlated with factual shortcuts, which can cause failures in multi-hop knowledge editing.</p>
<p>Prior works on latent factual multi-hop reasoning have not fully addressed potential shortcuts (Ofir Press et al., 2023; Yang et al., 2024b; Biran et al., 2024; Li et al., 2024). While Ofir Press et al. (2023) attempt to create multi-hop questions unlikely to appear in training, their approach relies on assumptions rather than co-occurrence statistics (leaving shortcuts from subject-object co-occurrences exploitable) and does not address shortcuts from relation-object co-occurrences. Indeed, we show that their dataset is prone to shortcuts, emphasizing the need for an explicit grounding to the co-
occurrence statistics (see Section 6.3). Similarly, Biran et al. (2024) address relation-object shortcuts but overlook subject-object shortcuts.</p>
<p>Construction of a shortcut-free evaluation dataset of latent factual multi-hop reasoning ability of any pretrained $L L M$ presents a unique challenge, as the pretraining data of most of the widely used LLMs is not accessible, making it difficult to verify if certain single-hop facts or their composition appeared in the same training sequence. Our need to consider the knowledge LLMs learn during pretraining makes our work distinct from prior works that aim for shortcut-free evaluation on the tasks where the training dataset is fully accessible (Min et al., 2019; Chen and Durrett, 2019; Ho et al., 2020; Trivedi et al., 2022; Gregucci et al., 2024).</p>
<p>Other studies have attempted to circumvent these issues by fine-tuning LLMs on synthetic or counterfactual tasks to control for single-hop knowledge (Jiang et al., 2022; Kassner et al., 2020; AllenZhu and Li, 2023; Saparov et al., 2023; Hou et al., 2023; Berglund et al., 2023; Petty et al., 2024; Treutlein et al., 2024; Wang et al., 2024). However, these studies do not address our target question of how much latent multi-hop reasoning ability naturally emerges in training. Moreover, finetuning may introduce side effects, such as hallucinations, reduced knowledge learning, and utilization efficiency (Yin et al., 2023; Kang et al., 2024; Ghosal et al., 2024; Gekhman et al., 2024; Gottesman and Geva, 2024). Works on latent compositional reasoning with algorithmic or mathematical tasks (Dziri et al., 2023; Chen et al., 2023; Deng et al., 2024) do not address our target question of latent multi-hop reasoning ability with factual knowledge, and may still suffer from data contamination that inflates performance (Zhang et al., 2024a). Ko et al. (2024) examine performance gaps between different numbers of reasoning hops but focus on more general reasoning capabilities rather than latent reasoning with factual knowledge.</p>
<p>Peng et al. (2024) study the theoretical limitations of compositional abilities. Consistent with our findings, they prove that for high arity relations (like relations with year-type bridge entity in our work), multi-hop reasoning is more difficult, albeit for the specific case of Transformers (Vaswani et al., 2017) with a single layer.</p>
<p>3 Shortcut-Free Evaluation of Latent Multi-Hop Reasoning</p>
<p>Terms and Notations We represent single-hop facts as $r_{1}\left(e_{1}\right)=e_{2}$ and $r_{2}\left(e_{2}\right)=e_{3}$, and multi-hop facts as their composition $r_{2} \circ r_{1}\left(e_{1}\right)=e_{3}$. In the aforementioned example, the entities "Scarlett Johansson", "1984", and "United States" are respectively represented by $e_{1}, e_{2}, e_{3}$, and connected via relations $r_{1}$ (person-birthyear), $r_{2}$ (year-eventcountry), and $r_{2} \circ r_{1}$ (person-birthyear-eventcountry). The set of aliases (names that the entity is also known as), of $e_{1}, e_{2}$, and $e_{3}$ are represented as $E_{1}$, $E_{2}$, and $E_{3}$, respectively. The answer set of the single-hop queries $q\left(r_{1}\left(e_{1}\right)\right), q\left(r_{2}\left(e_{2}\right)\right)$, and multihop query $q\left(r_{2} \circ r_{1}\left(e_{1}\right)\right)$ is $E_{2}, E_{3}$, and $E_{3}$, respectively. For instance, the answer set corresponding to the query "The year Scarlett Johansson was born in is" is $\left{{ }^{\prime} 1948^{\prime \prime}\right}$. We call each tuple $\left(q\left(r_{1}\left(e_{1}\right)\right), q\left(r_{2}\left(e_{2}\right)\right), q\left(r_{2} \circ r_{1}\left(e_{1}\right)\right), E_{1}, E_{2}, E_{3}\right)$ a test case, where $e_{2}$ is a bridge entity that connects the two facts, $e_{1}$ is the head entity, and $e_{3}$ the answer entity. Also, we call "the year Scarlett Johansson was born" the descriptive mention $\mu$ of the bridge entity.</p>
<p>Desideratum 1: Latent multi-hop reasoning We define the latent multi-hop reasoning ability of LLMs as the ability to latently recall and compose learned single-hop facts to answer multihop queries. We evaluate this ability by assessing a model's performance in answering multi-hop queries. For example, if a model learned the correct answer to "The year Scarlett Johansson was born in is" and "In 1984, the Summer Olympics were hosted in the country of", we evaluate whether it recalls and composes these facts latently to correctly answer a multi-hop query like "In the year Scarlett Johansson was born, the Summer Olympics were hosted in the country of". Therefore, to exclusively evaluate latent as opposed to explicit reasoning, we require models to answer the query directly without generating intermediate results (e.g., "1984"), e.g., without using Chain-of-Thought. Therefore, the evaluation should exclude the cases where the model generates the intermediate answers before generating the final answer.</p>
<p>Desideratum 2: Shortcut-free A model has a chance of exploiting shortcuts when it can correctly answer a multi-hop query by observing only part of the query (e.g., without $e_{1}$ or $r_{1}$ in the input).</p>
<p>Shortcut exploitation is problematic for evaluating latent multi-hop reasoning abilities because it allows the model to bypass the need to latently recall and compose single-hop facts. Following Elazar et al. (2022), we consider two types of shortcuts: subject-object shortcuts, where the model predicts objects that frequently co-occur with certain subjects or substrings of subjects, regardless of the relation semantics, and relation-object shortcuts, where the model predicts objects that frequently appear with certain surface form text of a relation, regardless of the subject. Therefore, the evaluation should exclude the queries prone to subject (or substring)-object shortcuts or relation (or paraphrase)-object shortcuts.</p>
<h2>4 Evaluation Dataset</h2>
<p>In this section, we describe our dataset construction process that minimizes the chance of subjectobject shortcut (satisfying Desideratum 2), resulting Socrates (SHOrtCut-FRee LATent reASONING), a dataset for evaluation of shortcut-free evaluation of latent multi-hop reasoning.</p>
<h3>4.1 Dataset Construction</h3>
<p>We generate test cases from a knowledge graph $G$, where facts are represented as subject-relationobject triplets $\langle s, r, o\rangle$. Specifically, we collect pairs of facts $r_{1}\left(e_{1}\right)=e_{2}$ and $r_{2}\left(e_{2}\right)=e_{3}$ and their composition $r_{2} \circ r_{1}\left(e_{1}\right)=e_{3}$ by considering pairs of triplets with a shared bridge entity, i.e., $\left\langle e_{1}, r_{1}, e_{2}\right\rangle,\left\langle e_{2}, r_{2}, e_{3}\right\rangle$. We choose Wikidata (Vrandečić and Krötzsch, 2014) as $G$.</p>
<p>Step 1: Selection of fact pairs We select singlehop facts that are likely well-known, but their composition is unlikely to naturally appear in general text corpora, to minimize the change of the model developing a shortcut between $e_{1}$ and $e_{3}$. We observe that such cases typically occur when the set of possible options for $e_{2}$ is large, there are numerous $e_{1}$ 's that map to the same $e_{2}$, and the set of possible options for $e_{3}$ is not too small (e.g., not person-bloodtype). This should lower the chance of the LLM getting the answer correct by mere guessing (Desideratum 2).</p>
<p>We exclude the following cases: (a) relation compositions where $e_{1}$ and $e_{3}$ are likely to be directly associated, e.g., novel-maincharacter-creator, (b) facts where the head and answer entities can be directly connected via popular single-hop relations other than the tested multi-hop rela-</p>
<table>
<thead>
<tr>
<th>$e_{3}$ type</th>
<th>Relation Composition Type</th>
<th>Count</th>
<th>Example Multi-hop Query</th>
</tr>
</thead>
<tbody>
<tr>
<td>city</td>
<td>person-birthcity-eventyear</td>
<td>33</td>
<td>$e_{1}$ 's birth city hosted the Eurevision Song Contest in the year</td>
</tr>
<tr>
<td>country</td>
<td>university-locationcountry-anthem</td>
<td>101</td>
<td>The country where $e_{1}$ is in has the national anthem named</td>
</tr>
<tr>
<td></td>
<td>university-locationcountry-isocode</td>
<td>30</td>
<td>The ISO 3166-1 numeric code of the country where $e_{1}$ is located is</td>
</tr>
<tr>
<td></td>
<td>university-locationcountry-year</td>
<td>7</td>
<td>The founding year of the location country of $e_{1}$ is</td>
</tr>
<tr>
<td></td>
<td>person-birthcountry-anthem</td>
<td>22</td>
<td>The name of the national anthem of $e_{1}$ 's country of birth is</td>
</tr>
<tr>
<td></td>
<td>person-birthcountry-isocode</td>
<td>6</td>
<td>The ISO 3166-1 numeric code used for the country where $e_{1}$ was born is</td>
</tr>
<tr>
<td>university</td>
<td>person-undergraduniversity-founder</td>
<td>33</td>
<td>The person who founded $e_{1}$ 's undergrad university is named</td>
</tr>
<tr>
<td></td>
<td>person-undergraduniversity-year</td>
<td>25</td>
<td>The year when the university where $e_{1}$ studied as an undergrad was founded is</td>
</tr>
<tr>
<td>year</td>
<td>person-birthyear-winner</td>
<td>4,484</td>
<td>The winner of the Nobel Prize in Literature in $e_{1}$ 's birth year was</td>
</tr>
<tr>
<td></td>
<td>city-eventyear-winner</td>
<td>2</td>
<td>In the year when the G7 Summit were hosted in $e_{3}$ the Nobel Prize in Chemistry was awarded to</td>
</tr>
<tr>
<td></td>
<td>university-inceptionyear-winner</td>
<td>632</td>
<td>In the founding year of $e_{1}$, the Nobel Prize in Literature was awarded to</td>
</tr>
<tr>
<td></td>
<td>university-inceptionyear-hostleader</td>
<td>9</td>
<td>The person who was the host leader of the G7 Summit in the founding year of $e_{1}$ is</td>
</tr>
<tr>
<td></td>
<td>university-inceptionyear-eventcountry</td>
<td>13</td>
<td>In the year $e_{1}$ was founded, the host country of the G7 Summit was</td>
</tr>
<tr>
<td></td>
<td>university-inceptionyear-eventcity</td>
<td>62</td>
<td>In the founding year of $e_{3}$, the host city of the G7 Summit was</td>
</tr>
<tr>
<td></td>
<td>person-birthyear-eventcity</td>
<td>1,389</td>
<td>In the birth year of $e_{1}$, the Winter Olympics were hosted in the city of</td>
</tr>
<tr>
<td></td>
<td>person-birthyear-hostleader</td>
<td>260</td>
<td>The leader of the host of the G7 Summit in $e_{1}$ 's birth year is</td>
</tr>
<tr>
<td></td>
<td>person-birthyear-eventcountry</td>
<td>124</td>
<td>The country where the Eurevision Song Contest took place in the birth year of $e_{1}$ is</td>
</tr>
<tr>
<td>total</td>
<td></td>
<td>7,232</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: Dataset statistics and example queries of Socrates. The head entities are replaced with $e_{1}$ to prevent potential data leakage. A more granular breakdown with the relation composition subtypes is in Appendix Table 4.
tion, e.g., person-birthyear-eventcountry( Scarlett Johansson) = United States = person-birthcountry(Scarlett Johansson), (c) queries with trivially inferrable bridge entities, e.g., university-locationcountry (University of Washington) = United States, which could enable answer prediction via entity substring-based shortcuts, (d) relations where there are likely to be many entities with 1:n relation, such as person-children, and (e) single-hop facts with more than one answer (details in §A.1).</p>
<p>Step 2: Test case construction We convert the selected fact pairs into a set of test cases $\left{\left(q\left(r_{1}\left(e_{1}\right)\right), q\left(r_{2}\left(e_{2}\right)\right), q\left(r_{2} \circ r_{1}\left(e_{1}\right)\right), E_{1}, E_{2}, E_{3}\right)\right}$. To create the two single-hop queries $q\left(r_{1}\left(e_{1}\right)\right)$, $q\left(r_{2}\left(e_{2}\right)\right)$ and their corresponding multi-hop query $q\left(r_{2} \circ r_{1}\left(e_{1}\right)\right)$, we follow the common practice of using diverse handcrafted natural language templates (Petroni et al., 2019; Yang et al., 2024b). For each relation, we use 16 templates ( 4 for each of the two single-hop queries) and randomly sample one template for each query, resulting in approximately 100 K test cases. We construct the queries as incomplete sentences, instead of questions, so that the test query can be naturally completed by any pretrained model to derive the answer without finetuning.</p>
<p>Step 3: Test case filtering using training cooccurrence statistics We filter out cases where any aliases of the head entity $e_{1}$ and answer entity $e_{3}$ co-occur in the same sequence that the evaluated LLM has seen during pretraining, preventing subject-object shortcuts. However, since pretraining sequences and/or corpora of most LLMs are
often inaccessible, we approximate co-occurrences by checking document-level co-occurrences across a proxy corpus of 4.8B unique documents (details in §A.2). While this approximation cannot guarantee complete exclusion of co-occurring entities without access to exact pretraining corpora, we validate our approach using Google Search for webwide co-occurrence verification (§C.3).</p>
<p>The Socrates Dataset Socrates contains 7,232 test cases of 17 types of relation compositions connected by 4 types of bridge entities, as shown in Table 1. Note that the distribution of relation compositions is imbalanced as $e_{1}$ and $e_{3}$ of some relation compositions frequently appear together in the same document and most of test cases are removed by the co-occurrence-based filtering.</p>
<h2>5 Evaluation Procedure</h2>
<p>We introduce an evaluation procedure that satisfies part of Desideratum 2 by minimizing the chance of the model exploiting the relation-object shortcut and Desideratum 1 that the evaluation should exclude cases where the model performs explicit reasoning (§5.1). Then, we define our evaluation metric, latent composability (§5.2).</p>
<h3>5.1 Filtering Guessable and Unusable Queries</h3>
<p>Excluding guessable cases Even when the prediction of the model for a multi-hop query is correct, there is still a chance that the LLM might have guessed the answer correctly by chance, meaning that Desideratum 2 is not satisfied. For instance, when the answer is a popular entity among the potential answer set (e.g., "United States" as a country), the model might exploit a relation-object short-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Latent composability. There are successful cases of latent multi-hop reasoning, although the overall percentage is low.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(c) CoT composability. CoT composability is significantly higher than latent composability.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Effect of model scale on latent composability. Comparative latent composability slightly improves with model scale.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(d) Effect of model scale on CoT composability. Comparative CoT composability increases much more effectively with model scale compared to latent composability.</p>
<p>Figure 2: Latent (upper row) and CoT (lower row) composability on SOCRATES.
cut where the model makes the prediction based on the prior from the textual pattern of the relation (e.g., " $\ldots$ is from the country of").</p>
<p>To filter out the cases where it is indistinguishable whether models are guessing the answer, we adopt the method of Biran et al. (2024) which checks the model's prediction for a set of ablated prompts $Q_{\emptyset}=\left{q\left(r_{2} \circ r_{1}(\emptyset)\right), q\left(r_{2}(\emptyset)\right)\right}$, where the specific information of $e_{1}$ and $r_{1}\left(e_{1}\right)$ is ablated from the multi-hop query $q\left(r_{2} \circ r_{1}\left(e_{1}\right)\right)$, respectively (e.g., {"In the year the person was born, the Summer Olympics were hosted in the country of", "In the year, the Summer Olympics were hosted in the country of"}). When the model answers the multi-hop query correctly, but also answers any of $q_{\emptyset} \in Q_{\emptyset}$ correctly, we exclude the test case from the evaluation. Namely, we detect and remove the cases where models may be exploiting a relationobject shortcut between $r_{2} \circ r_{1} / r_{2}$ and $e_{3}$.</p>
<p>Excluding unusable cases When the model correctly predicts the answer for a test multi-hop query but the LLM has just enumerated multiple potential answer candidates ${ }^{2}$ or the LLM has performed an explicit reasoning (e.g., "1984, United States"), we view these test cases as unusable for the evaluation of latent multi-hop reasoning ability and exclude the test case from the evaluation, following Desideratum 1 (details in §B.1).</p>
<h2>Suppressing CoT for instruction-tuned LLMs</h2>
<p>Since instruction-tuned LLMs tend to perform CoT-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>style reasoning by default ${ }^{3}$, we formulate the task as a fill-in-the-blank task using a CoT-suppressing instruction as described in §C.2. ${ }^{4}$</p>
<h3>5.2 Latent Composability</h3>
<p>We assess latent composability as the ability of the LLM to latently compose the already-learned single-hop facts by calculating the ratio of the cases where the LLM correctly answers the multi-hop query while correctly answering both of the corresponding single-hop queries, excluding the guessable and unusable cases.To check the correctness of model outputs, we use a standard Exact Match with string normalization (details in §B.2).</p>
<p>When comparing latent composability between different models, it is misleading to compare latent composability calculated using different subsets of queries. Therefore, we calculate the ratio using the same test case subset where all of the compared LLMs correctly answer both single-hop queries where the test cases are guessable or unusable for neither of the models. We call this value comparative latent composability.</p>
<h2>6 Experiments</h2>
<p>We use SOCRATES to evaluate the latent multihop reasoning ability of LLMs. Our results show that models can perform latent reasoning without</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(a) Latent composability on country-type (left) and year-type (right) bridge entity subsets. Latent composability varies according to the bridge entity type; it is over $80 \%$ for the best models when the bridge entity is a country, but around $5 \%$ when it is a year.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(b) CoT composability on country-type (left) and year-type (right) bridge entity subsets. CoT composability does not fluctuate as dramatically as latent composability according to the type of bridge entity.</p>
<p>Figure 3: Latent (upper row) and CoT (lower row) composability measured for subsets of the test queries in SOCRATES, grouped according to the type of the bridge entity.
exploiting shortcuts, but their ability depends on the type of bridge entity connecting the two facts. Table 5 in the Appendix shows exemplifying test cases, model predictions, and results.</p>
<h3>6.1 Experimental Setting</h3>
<p>We assess 41 LLMs of different families and sizes. Among the proprietary LLMs, we evaluate Claude 3.5 Sonnet (Anthropic, 2024), GPT-4o (OpenAI, 2024b), GPT-4o mini (OpenAI, 2024a), and Gemini 1.5 Pro and Flash (Gemini Team et al., 2024). Among the open-source LLMs, we evaluate pretrained and/or instruction-tuned models of 2B to 123B parameters from the model families of Mistral (Jiang et al., 2023), Mixtral (Jiang et al., 2024), Qwen 2.5 and 2 (Qwen Team, 2024; Yang et al., 2024a), Yi 1.5 (01.AI et al., 2024), Gemma 1 and 2 (Mesnard et al., 2024; Gemma Team et al., 2024), and OLMo (Groeneveld et al., 2024). Refer to §C. 1 for model and inference details.</p>
<h3>6.2 Evaluation Results on SOCRATES</h3>
<p>There are successful cases of latent multi-hop reasoning, although the overall percentage is low. Figure 2a shows the latent composability of 41 models on SOCRATES. While there is a meaningful number of successful latent multi-hop reasoning cases (434 for Claude 3.5 Sonnet and 438 for GPT40 , the best performing models), the percentage of such cases of the whole dataset is low; latent composability of Claude 3.5 Sonnet and GPT-4o is only $8.4 \%$ and $7.6 \%$, respectively.</p>
<p>Model scaling marginally improves overall performance. Figure 2b shows comparative latent composability among models with different num-
bers of parameters within the same model family ${ }^{5}$. We observe a consistent trend across all model families, where larger models answer more multi-hop queries correctly than smaller models, although the difference is not large in number. The gap in latent composability is $6.7 \%$ (118) and $2.4 \%$ (31) for GPT-4o vs. GPT-4o mini and Gemini 1.5 Pro vs. Gemini 1.5 Flash, respectively.</p>
<p>Latent composability performance varies across bridge entity types. Figure 3a shows the latent composability for two out of the four bridge entity types where the size of the subset of the queries is statistically significant (the results for all four types are in Appendix Figure 7). Notably, the latent composability of Claude 3.5 Sonnet and GPT40 reaches $82.6 \%$ and $84.5 \%$, respectively, when the single-hop facts are connected with countrytype bridge entities, but only $6.7 \%$ and $5.7 \%$ when the facts are connected with year-type bridge entities. The rate of improvement in latent composability with the number of known single-hop facts also varies across different bridge entity types. Our finding implies that it is important to consider the dataset distribution and perform per-relationcomposition analysis when evaluating latent multihop reasoning (explanation in §D.1). Drawing from prior works, we speculate that the high composability of country-related queries might stem from more frequent exposure to learning country-related facts together or in composition during pretraining (explanation in §D.2).</p>
<p>There exist significant disparities between CoT and latent composability. While GPT-4o</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 4: Latent composability measured with shortcut free/prone data and evaluation, averaged across models shown in Appendix Figure 10. Latent composability measured with SOCRATES and the proposed evaluation procedure is about three times lower than the shortcut-prone counterpart.</p>
<p>achieves 92.8% composability with CoT reasoning, it is only 7.6% with latent reasoning (Figure 2c, 2a), with almost no cases where latent reasoning succeeds but CoT fails (Appendix Figure 9). Models that know more single-hop facts and larger models show dramatic improvements in composability for CoT reasoning compared to latent reasoning (Figures 2d, 2b), suggesting that merely increasing parameter count cannot effectively enhance latent multi-hop reasoning. Furthermore, CoT composability remains relatively consistent across bridge entity types (Figure 3b, 3a). Drawing from prior works, we speculate that the explicit generation of the bridge entity is the main factor behind high CoT composability (explanation in §D.3).</p>
<h3>6.3 Additional Analysis</h3>
<p><strong>Shortcut-free evaluation is important.</strong> To check the importance of addressing shortcuts, we perform a comparative experiment with a shortcut-prone dataset and evaluation procedure that does not follow the proposed desiderata. Specifically, we construct a dataset with almost exactly the same distribution of the relation composition types (and thus the bridge entity types) of SOCRATES, but without applying any measure to remove potential shortcuts such as the entity co-occurrence-based filtering or relation-specific heuristics. As the evaluation procedure, we only check whether both of the single-hop facts are known by the model, and do not exclude the <em>guessable</em> and <em>unusable</em> cases.</p>
<p>Latent composability measured with shortcut-free data and evaluation procedure is three times lower than the shortcut-prone counterpart (Figure 4), and the former is consistently lower than the latter across all models (Appendix Figure 10). These results imply that overlooking shortcut exploitation can overestimate the latent composability of the model.</p>
<table>
<thead>
<tr>
<th>Relation</th>
<th>Shortcut-prone</th>
<th>Shortcut-prone</th>
<th>Shortcut-free</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Composition Type</td>
<td>Ratio (%)</td>
<td>Count</td>
<td>Count</td>
<td>Count</td>
</tr>
<tr>
<td>person-country-callingcode</td>
<td>97.44</td>
<td>496</td>
<td>12</td>
<td>468</td>
</tr>
<tr>
<td>person-country-capital</td>
<td>100.00</td>
<td>468</td>
<td>0</td>
<td>468</td>
</tr>
<tr>
<td>person-country-cox5</td>
<td>94.23</td>
<td>441</td>
<td>27</td>
<td>468</td>
</tr>
<tr>
<td>person-country-currency</td>
<td>88.46</td>
<td>414</td>
<td>54</td>
<td>468</td>
</tr>
<tr>
<td>person-country-currencyshort</td>
<td>93.38</td>
<td>437</td>
<td>31</td>
<td>468</td>
</tr>
<tr>
<td>person-country-currencycyquhed</td>
<td>76.28</td>
<td>357</td>
<td>111</td>
<td>468</td>
</tr>
<tr>
<td>person-country-entcommoname</td>
<td>64.32</td>
<td>301</td>
<td>167</td>
<td>468</td>
</tr>
<tr>
<td>person-country-ignocommoname</td>
<td>71.58</td>
<td>335</td>
<td>133</td>
<td>468</td>
</tr>
<tr>
<td>person-country-roundedlat</td>
<td>100.00</td>
<td>468</td>
<td>0</td>
<td>468</td>
</tr>
<tr>
<td>person-country-roundedlng</td>
<td>100.00</td>
<td>468</td>
<td>0</td>
<td>468</td>
</tr>
<tr>
<td>person-country-nuscommoname</td>
<td>75.00</td>
<td>351</td>
<td>117</td>
<td>468</td>
</tr>
<tr>
<td>person-country-ipacommoname</td>
<td>97.22</td>
<td>455</td>
<td>13</td>
<td>468</td>
</tr>
<tr>
<td>person-country-ild</td>
<td>98.72</td>
<td>462</td>
<td>6</td>
<td>468</td>
</tr>
<tr>
<td>person-country-iedcommonname</td>
<td>20.51</td>
<td>96</td>
<td>372</td>
<td>468</td>
</tr>
<tr>
<td>person-year-masterschampion</td>
<td>81.24</td>
<td>589</td>
<td>136</td>
<td>725</td>
</tr>
<tr>
<td>person-year-nebelliti</td>
<td>87.57</td>
<td>620</td>
<td>88</td>
<td>708</td>
</tr>
<tr>
<td>person-year-neprosidest</td>
<td>98.45</td>
<td>697</td>
<td>11</td>
<td>708</td>
</tr>
<tr>
<td>total</td>
<td>85.30</td>
<td>7,415</td>
<td>1,728</td>
<td>8,693</td>
</tr>
</tbody>
</table>
<p>Table 2: Subject-object co-occurrence statistics of Compositional Celebrities (CC) (Ofir Press et al., 2023) dataset in Dolma v1.5 using the WIMBD API.</p>
<p><strong>Explicit grounding to the co-occurrence statistics is important.</strong> To emphasize the need for an explicit grounding to the co-occurrence statistics in the dataset construction process, we check the co-occurrence counts of the subjects and objects in the test queries of the Compositional Celebrities (Ofir Press et al., 2023) dataset in Dolma v1.5 using the WIMBD (What's In My Big Data) (Elazar et al., 2024) API. Table 2 shows that 85.30% of the queries in the dataset are prone to subject-object shortcuts, appearing at least once in Dolma v1.5. More than 95% of the data subsets are prone to subject-object shortcuts for 7 out of 17 categories, where three categories are completely unusable with a ratio of 100%. Note that we have not even used all possible aliases of the subjects and objects, and that we have used only Dolma v1.5 to measure the co-occurrence counts; the percentage can rise even further when all aliases and more pretraining datasets are considered. Considering the relation-object shortcuts will make even more data examples unusable for shortcut-free evaluation of latent multi-hop reasoning.</p>
<p><strong>Latent representation of the bridge entity appears more often for query types with higher latent composability.</strong> We perform an experiment using Patchscopes (Ghandeharioun et al., 2024) following Biran et al. (2024) using Mistral 7B v0.3, which examines whether the model constructs latent representations of the bridge entity (e.g., <em>"1984"</em>) for its descriptive mention (<em>"the year Scarlett Johansson was born"</em>) encountered during multi-hop query processing. Figure 6 shows how often the hidden states at the last token of the descriptive mention taken from different layers (y-</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 5: One successful case of OLMo 7B that demonstrates the emergence of latent multi-hop reasoning ability even when $e_{1}$ and $e_{3}$ have never co-appeared in any sequence throughout pretraining. While being pretrained for 557 K steps, the model starts to correctly predict the answer to the single-hop queries after consistently seeing ( $e_{1}$, $e_{2}$ ) and ( $e_{2}, e_{3}$ ) together across multiple pretraining steps. After the model starts to learn to correctly answer the single-hop queries, the model starts to learn to correctly answer the multi-hop query.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 6: Experimental results with Mistral 7B v0.3 using Patchscopes (Ghandeharioun et al., 2024) to examine whether the model constructs latent representations of the bridge entity (e.g., "1984" for "the year Scarlett Johansson was born") during the multi-hop query processing. Latent representations of bridge entities are constructed more often for queries with country-type bridge entities (that have higher latent composability).
axis) generate the bridge entity when patched into appropriate contexts at different layers (x-axis), for the queries with country/year-type bridge entities where both single hop facts are known by the model. The bridge entity is generated more often (which suggests that the latent bridge entity representations are constructed more often) for the queries with higher latent composability (queries with countrytype bridge entities). Details are in §C.4.</p>
<p>Emergence of latent multi-hop reasoning during pretraining. Our analysis of OLMo 7B's intermediate checkpoints ( 557 checkpoints from 1 K to 557 K pretraining steps) shows that for a subset of prompts, OLMO first learns to predict the singlehop answers, and then begins to correctly answer the respective multi-hop query. Figure 5 illustrates one such case. This set is small: 12 out of 13 cases where the model successfully performs multi-hop reasoning at a point, among 110 cases where the model is correct on both single-hop facts at some point and the model is not likely to be guessing the answer at any point during pretraining. That said, for OLMo, we have access to all the pretraining
sequences in order and can hence guarantee that head entities have not been seen together with the answer entities in any of the pretraining sequences. Together with our other filtering to reduce the probability that answers can be correct by chance and guesswork, this indicates that even a small model like OLMo 7B can perform some level of latent reasoning, albeit only occasionally. More details of the experiment are provided in §C.5.</p>
<h2>7 Conclusion</h2>
<p>We outline desiderata for shortcut-free evaluation of LLMs' ability to latently recall and compose learned single-hop facts to answer multi-hop queries. By filtering entity co-occurrences and systematic removal of potential shortcuts, we construct the SOCRATES dataset, enabling a rigorous assessment of latent multi-hop reasoning. Our analysis reveals that while models can perform latent reasoning effectively in specific scenarios, their ability varies dramatically across different types of queries. This fluctuation, along with the significant gap between latent and explicit CoT reasoning, suggests substantial room for improvement in how LLMs internally compose their knowledge. Our work provides resources and insights for precise evaluation, understanding, and improvement of latent multihop reasoning of LLMs.</p>
<h2>Limitations</h2>
<p>We do not test other forms of compositional reasoning such as comparisons because if the answer is binary, it is hard to rule out the cases of guessing. We do not test more than two hops because latent two-hop composability is already quite low, and adding more complexity to the problem may lower it to zero success cases. We do not consider facts that are subject to frequent change over time to compare models trained at different corpus cutout times. While we cannot know whether the LLMs</p>
<p>we evaluate have not been trained on synthetic data generated from a knowledge graph, the low latent composability and high CoT composability obtained with our dataset suggest that the chance is very low for the evaluated models. While we cannot guarantee that the head and answer entities of every test query of Socrates would have never been learned in a single pretraining sequence for every model that we evaluate, we believe that our dataset construction that utilizes document co-occurrence counts of multiple pretraining corpora provides a tight approximation, which is also supported by the experimental results in $\S$ C.3.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank Sang-Woo Lee, Hoyeon Chang, and Chris Dyer for the valuable feedback and discussions. We are grateful to Yanai Elazar for their support of the WIMBD (Elazar et al., 2024) API throughout the project.</p>
<h2>References</h2>
<p>01.AI, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. 2024. Yi: Open foundation models by 01.AI. arXiv [cs.CL].</p>
<p>Zeyuan Allen-Zhu and Yuanzhi Li. 2023. Physics of language models: Part 3.2, knowledge manipulation. arXiv [cs.CL].</p>
<p>Anthropic. 2024. Introducing Claude 3.5 Sonnet. Anthropic blog.</p>
<p>Gregor Bachmann and Vaishnavh Nagarajan. 2024. The pitfalls of next-token prediction. In ICML.</p>
<p>Shay Banon. 2010. Elasticsearch: A distributed, RESTful search and analytics engine.</p>
<p>Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, and Owain Evans. 2023. Taken out of context: On measuring situational awareness in LLMs. arXiv.</p>
<p>BigScience Workshop et al. 2022. BLOOM: A 176Bparameter open-access multilingual language model. arXiv [cs.CL].</p>
<p>Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. 2024. Hopping too late: Exploring the limitations of large language models on multi-hop queries. In EMNLP.</p>
<p>Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond, Herbie Bradley, Emma Bluemke, Nitarshan Rajkumar, David Krueger, Noam Kolt, Lennart Heim, and Markus Anderljung. 2024. Visibility into AI agents. In FAccT.</p>
<p>Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, and Minjoon Seo. 2024. How do large language models acquire factual knowledge during pretraining? In NeurIPS.</p>
<p>Jifan Chen and Greg Durrett. 2019. Understanding dataset design choices for multi-hop reasoning. In NAACL.</p>
<p>Jiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou. 2023. When do you need chain-of-thought prompting for ChatGPT? arXiv [cs.AI].</p>
<p>Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. 2024. Evaluating the ripple effects of knowledge editing in language models. TACL.</p>
<p>Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. 2024. Implicit chain of thought reasoning via knowledge distillation. In ICLR.</p>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D Hwang, Soumya Sanyal, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. 2023. Faith and fate: Limits of transformers on compositionality. In NeurIPS.</p>
<p>Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A Smith, and Jesse Dodge. 2024. What's in my big data? In ICLR.</p>
<p>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schütze, and Yoav Goldberg. 2022. Measuring causal effects of data statistics on language model's 'factual' predictions. arXiv [cs.CL].</p>
<p>Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. 2020. Shortcut learning in deep neural networks. Nature Machine Intelligence.</p>
<p>Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024. Does fine-tuning LLMs on new knowledge encourage hallucinations? In EMNLP.</p>
<p>Gemini Team et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv [cs.CL].</p>
<p>Gemma Team et al. 2024. Gemma 2: Improving open language models at a practical size. arXiv [cs.CL].</p>
<p>Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual associations in auto-regressive language models. In EMNLP.</p>
<p>Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. 2024. Patchscopes: A unifying framework for inspecting hidden representations of language models. In ICML.</p>
<p>Gaurav Ghosal, Tatsunori Hashimoto, and Aditi Raghunathan. 2024. Understanding finetuning for factual knowledge extraction. In ICML.</p>
<p>Daniela Gottesman and Mor Geva. 2024. Estimating knowledge in large language models without generating a single token. In EMNLP.</p>
<p>Cosimo Gregucci, Bo Xiong, Daniel Hernandez, Lorenzo Loconte, Pasquale Minervini, Steffen Staab, and Antonio Vergari. 2024. Is complex query answering really complex? arXiv [cs.LG].</p>
<p>Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A Smith, and Hannaneh Hajishirzi. 2024. OLMo: Accelerating the science of language models. In $A C L$.</p>
<p>Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In COLING.</p>
<p>Yihuai Hong, Lei Yu, Shauli Ravfogel, Haiqin Yang, and Mor Geva. 2024. Intrinsic evaluation of unlearning using parametric knowledge traces. arXiv [cs.CL].</p>
<p>Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, and Mrinmaya Sachan. 2023. Towards a mechanistic interpretation of multi-step reasoning capabilities of language models. In EMNLP.</p>
<p>Hamish Ivison. 2023. Camels in a changing climate: Enhancing lm adaptation with tulu 2. Ai2 blog.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv [cs.CL].</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. Mixtral of experts. arXiv [cs.LG].</p>
<p>Yichen Jiang and Mohit Bansal. 2019. Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA. In ACL.</p>
<p>Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2022. Understanding and improving zeroshot multi-hop reasoning in generative question answering. In COLING.</p>
<p>Tianjie Ju, Yijin Chen, Xinwei Yuan, Zhuosheng Zhang, Wei Du, Yubin Zheng, and Gongshen Liu. 2024. Investigating multi-hop factual shortcuts in knowledge editing of large language models. In $A C L$.</p>
<p>Cheongwoong Kang and Jaesik Choi. 2023. Impact of co-occurrence on factual knowledge of large language models. In Findings of EMNLP.</p>
<p>Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. 2024. Unfamiliar finetuning examples control how language models hallucinate. arXiv [cs.LG].</p>
<p>Nora Kassner, Benno Krojer, and Hinrich Schütze. 2020. Are pretrained language models symbolic reasoners over knowledge? In CoNLL.</p>
<p>Miyoung Ko, Sue Hyun Park, Joonsuk Park, and Minjoon Seo. 2024. Investigating how large language models leverage internal knowledge to perform complex reasoning. In EMNLP.</p>
<p>Takeshi Kojima and Shixiang Shane Gu. 2022. Large language models are zero-shot reasoners. In NeurIPS.</p>
<p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with PagedAttention. In SOSP.</p>
<p>Zhaoyi Li, Gangwei Jiang, Hong Xie, Linqi Song, Defu Lian, and Ying Wei. 2024. Understanding and patching compositional reasoning in LLMs. In Findings of $A C L$.</p>
<p>Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. 2023. Transformers learn shortcuts to automata. In $I C L R$.</p>
<p>Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. arXiv [cs.CL].</p>
<p>Thomas Mesnard et al. 2024. Gemma: Open models based on gemini research and technology. arXiv [cs.CL].</p>
<p>Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. Compositional questions do not necessitate multi-hop reasoning. In $A C L$.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of EMNLP.</p>
<p>Yasumasa Onoe, Michael J Q Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol Choi. 2023. Can LMs learn new entities from descriptions? challenges in propagating injected knowledge. In $A C L$.</p>
<p>OpenAI. 2022. Introducing chatgpt. OpenAI blog.
OpenAI. 2024a. Gpt-4o mini: advancing cost-efficient intelligence. OpenAI blog.</p>
<p>OpenAI. 2024b. Hello GPT-4o. OpenAI blog.
Binghui Peng, Srini Narayanan, and Christos Papadimitriou. 2024. On limitations of the transformer architecture. In COLM.</p>
<p>Jordan Peterson, Ruairidh Battleday, Thomas Griffiths, and Olga Russakovsky. 2019. Openwebtext corpus. GitHub repository.</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? In EMNLP.</p>
<p>Jackson Petty, Sjoerd van Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette, and Tal Linzen. 2024. The impact of depth on compositional generalization in transformer language models. In NAACL.</p>
<p>Nina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020. E-BERT: Efficient-yet-effective entity embeddings for BERT. In Findings of EMNLP.</p>
<p>Qwen Team. 2024. Qwen2.5: A party of foundation models! Qwen Blog.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. $J M L R$.</p>
<p>Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, and He He. 2023. Testing the general deductive reasoning capacity of large language models using OOD examples. In NeurIPS.</p>
<p>Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. 2024. Dolma: An open corpus of three trillion tokens for language model pretraining research. In $A C L$.</p>
<p>Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît Sagot. 2020. A monolingual approach to contextualized word embeddings for mid-resource languages. In $A C L$.</p>
<p>Ruixiang Tang, Dehan Kong, Longtao Huang, and Hui Xue. 2023. Large language models can be lazy learners: Analyze shortcuts in in-context learning. In Findings of ACL.</p>
<p>Johannes Treutlein, Dami Choi, Jan Betley, Cem Anil, Samuel Marks, Roger Baker Grosse, and Owain Evans. 2024. Connecting the dots: LLMs can infer and verbalize latent structure from disparate training data. In NeurIPS.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop questions via single-hop question composition. TACL.</p>
<p>Ashish Vaswani, Noam Shazeer, Google Brain, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, and Łukasz Kaiser. 2017. Attention is all you need. In NeurIPS.</p>
<p>Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas Sakenis, Jason Huang, Yaron Singer, and Stuart Shieber. 2020. Causal mediation analysis for interpreting neural nlp: The case of gender bias. In NeurIPS.</p>
<p>Denny Vrandečić and Markus Krötzsch. 2014. Wikidata: a free collaborative knowledgebase. Communications of the ACM.</p>
<p>Boshi Wang, Xiang Yue, Yu Su, and Huan Sun. 2024. Grokked transformers are implicit reasoners: A mechanistic journey to the edge of generalization. In NeurIPS.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M Rush. 2020. Transformers: State-of-the-art natural language processing. In EMNLP System Demonstrations.</p>
<p>Nan Xu, Fei Wang, Bangzheng Li, Mingtao Dong, and Muhao Chen. 2022. Does your model classify entities reasonably? diagnosing and mitigating spurious correlations in entity typing. In EMNLP.</p>
<p>An Yang et al. 2024a. Qwen2 technical report. arXiv [cs.CL].</p>
<p>Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. 2024b. Do large language models latently perform multi-hop reasoning? In $A C L$.</p>
<p>Xunjian Yin, Baizhou Huang, and Xiaojun Wan. 2023. ALCUNA: Large language models meet new knowledge. In EMNLP.</p>
<p>Honghua Zhang, Liunian Harold Li, Tao Meng, KaiWei Chang, and Guy Van den Broeck. 2022a. On the paradox of learning to reason from data. In IJCAI.</p>
<p>Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, and Summer Yue. 2024a. A careful examination of large language model performance on grade school arithmetic. arXiv [cs.CL].</p>
<p>Xiao Zhang, Miao Li, and Ji Wu. 2024b. Co-occurrence is not factual association in language models. arXiv [cs.CL].</p>
<p>Yiming Zhang, Shi Feng, and Chenhao Tan. 2022b. Active example selection for in-context learning. In EMNLP.</p>
<p>Yu Zhao, Yuanbin Qu, Konrad Staniszewski, Szymon Tworkowski, Wei Liu, Piotr Miłoś, Yuxiang Wu, and Pasquale Minervini. 2024. Analysing the impact of sequence composition on language model pretraining. In $A C L$.</p>
<p>Zexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. 2023. MQAKE: Assessing knowledge editing in language models via multi-hop questions. In EMNLP.</p>
<h2>A Details of Dataset Construction</h2>
<h2>A. 1 Implementation of steps 1-2</h2>
<p>We choose Wikidata (Vrandečić and Krötzsch, 2014) as the knowledge graph and collect facts in English. To ensure fair evaluation across models with different knowledge cutoff dates, we abstain from using relations subject to change over time (organization-CEO or person-spouse). We select 11 types of $r_{1}\left(e_{1}\right)=e_{2}$ and 10 types of $r_{2}\left(e_{2}\right)=e_{3}$ that are connected to each other with four types of $e_{2}$ - "country", "city", "university", and "year" - resulting in a total of 21 relation composition types, although the number reduces to 17 through filtering during the implementation of step
3. Table 4 shows the 17 relation compositions that consist Socrates. As shown in the table, the relation compositions are divided further into subtypes for events ${ }^{6}$ and award winners ${ }^{7}$.</p>
<p>To use only the entities where $e_{1}$ and $e_{3}$ are not likely to be connected through other popular single-hop relations, we apply a relationspecific heuristic filtering. For instance, for person-birthyear-eventcountry relation (e.g., "The country where the Eurovision Song Contest took place in the birth year of $e_{1}$ is"), we exclude the cases where the country in which the event took place is the same with the person's birth country. A list of such heuristics is provided in Appendix Table 3.</p>
<p>Additionally, we exclude cases where $e_{2}$ can be easily inferred from the surface form of $e_{1}$, such as university-locationcountry(University of Washington) = United States, as these cases are where the multi-hop query is reduced into more like a single-hop query and also the cases where a substring of $e_{1}$ has relatively higher chance to frequently co-appear with $e_{3}$ in the same training sequences. A more detailed explanation is provided in §A.3. Through this process, we collect about 100 K tuples $\left(e_{1}, e_{2}, e_{3}, r_{1}, r_{2}, E_{1}, E_{2}, E_{3}\right)$, where $e_{1}, e_{2}, e_{3}$ are the Wikidata entity titles, and $E_{1}, E_{2}$, $E_{3}$ are their corresponding sets of Wikidata aliases.</p>
<p>We manually construct four natural language templates for each subtype of $r_{2}$ and $r_{1}$ and randomly select one of the templates for each test case. Since multi-hop queries are constructed with the combinations of the templates chosen for $r_{2}$ and $r_{2}$, 16 templates are used for each type of relation composition. We feed $e_{2}$ to a template for $r_{2}$ to create $q\left(r_{2}\left(e_{2}\right)\right)$, and feed the descriptive mention $\mu$ of the bridge entity (e.g., "the year Scarlett Johansson was born") to create $q\left(r_{2} \circ r_{1}\left(e_{1}\right)\right)$, that both take $E_{3}$ as the answer set (e.g., {"United States", "US", $\cdots}$ ). The descriptive mention of the bridge entity is created with a template for $r_{1}$. The same descriptive mention $\mu$ is also used to create $q\left(r_{1}\left(e_{1}\right)\right)$ in the form " $\mu$ is" which takes $E_{2}$ as the answer set (e.g., {"1948"}). We filter out the dataset further during quality assurance (§A.4).</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Relation Composition Type</th>
<th>Heuristic Conditions to Meet</th>
</tr>
</thead>
<tbody>
<tr>
<td>person-birthcity-year</td>
<td>city $\neq$ capital of a country</td>
</tr>
<tr>
<td></td>
<td>person’s birth year $\neq$ event year</td>
</tr>
<tr>
<td>person-undergraduniversity-year</td>
<td>person’s birth year $\neq$ university’s inception year</td>
</tr>
<tr>
<td>person-birthyear-winner</td>
<td>person’s birth/citizenship country $\neq$ winner’s birth/citizenship country</td>
</tr>
<tr>
<td>person-birthyear-event country/city/leader</td>
<td>person’s birth/citizenship country $\neq$ event country</td>
</tr>
<tr>
<td>university-inceptionyear-winner</td>
<td>university’s location country $\neq$ winner birth/citizenship country</td>
</tr>
<tr>
<td></td>
<td>university $\neq$ winner’s university for any degree</td>
</tr>
<tr>
<td>city-eventyear-winner</td>
<td>event country $\neq$ winner’s birth/citizenship country</td>
</tr>
<tr>
<td>university-inceptionyear-event country/city/leader</td>
<td>university’s location country $\neq$ event country</td>
</tr>
</tbody>
</table>
<p>Table 3: Relation-specific heuristic conditions that the collected test cases need to satisfy to prevent using the test queries where spurious correlations of $e_{1}$ and $e_{3}$ exist.</p>
<h3>A. 2 Implementation of step 3 with document co-occurrence counts</h3>
<p>Step 3 can be directly implemented when one has access to the LLM's pretraining sequences. However, we cannot use the approach as-is since LLMs are trained on different pretraining data, and this data is often not publicly available. Moreover, even if the data is available, information on how it has been broken into training sequences is rarely provided. To overcome these challenges, instead of computing the exact co-occurrence counts, we approximate them using two simplifications.</p>
<p>The first simplification is that we check the cooccurrence of the aliases of $e_{1}$ and $e_{3}$ within a document instead of a training sequence. To be specific, we use only test cases where there is no pretraining document in which any of the possible combinations of the aliases of $e_{1}$ and $e_{3}$ appear together. Filtering out test cases with non-zero document co-occurrence count imposes a stricter condition than doing so with sequence co-occurrence count because training sequences are substrings of a document in most LLMs trained with document boundaries, which is the standard approach in pretraining LLMs (Zhao et al., 2024).</p>
<p>The second simplification is that we utilize a proxy corpus to check the document co-occurrence since even the document-level information of the pretraining data of most models is not available.We use six different training corpora: Dolma v1.5, v1.7 (Soldaini et al., 2024), Tulu v2 (Ivison, 2023), OSCAR (Suárez et al., 2020), C4 (Raffel et al., 2020), and OpenWebText (Peterson et al., 2019), each of which contains 4,367M, 2,532M, 326K, 432M, 365M, 8M documents, used to train OLMo, OLMo 0724, OLMo Instruct (Groeneveld et al., 2024), BLOOM (BigScience Workshop et al., 2022), T5 (Raffel et al., 2020), and GPT-2 (Radford et al., 2019), respectively. The number of unique documents from the proxy corpus is roughly 4.8B. ${ }^{8}$ In other words, we only use the test cases where none of any possible combination of the aliases of $e_{1}$ and $e_{3}$ appears together in 4.8B unique documents, which imposes a highly restrictive condition that enables obtaining a tight approximation of the test queries where the head and answer entities do not co-appear in the single pretraining sequence. While we cannot guarantee the exclusion of all entity co-occurring cases without access to exact pretraining corpora, we further validate our approximation using Google Search to check for co-occurrences across the whole web (§C.3).</p>
<p>We use the WIMBD (What's In My Big Data) (Elazar et al., 2024) API to get the document co-occurrence counts of these pretraining corpora which utilize Elasticsearch (Banon, 2010) as the backend with case-insensitive string match and exclude the test cases with non-zero co-occurrence count. After this filtering process, we obtain about 32 K test cases of 17 relation composition types in total. Note that the distribution of the relation compositions is forced to be imbalanced as $e_{1}$ and $e_{3}$ of some relation compositions frequently appear together in the same document and most of the test cases are removed by the co-occurrence-based filtering. We down-sample the test cases with yeartype bridge entities as the queries of these types outweigh other types.</p>
<p>Data statistics Our dataset for shortcut-free evaluation of latent multi-hop reasoning ability contains 7,232 test cases of 17 types of relation compositions connected by 4 types of bridge entities, as shown in Table 1. Note that the distribution of relation compositions is imbalanced as $e_{1}$ and $e_{3}$ of some relation compositions frequently appear together in the same document and most of the</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>test cases are removed by the co-occurrence-based filtering.</p>
<h3>A. 3 Filtering Out the Cases with Easily Inferrable Bridge Entities</h3>
<p>To prevent the multi-hop query from being reduced to be more similar to a single-hop query, we filter out the cases where the bridge entity is easily inferrable from the surface form of $e_{1}$ (Poerner et al., 2020), e.g., university-locationcountry( University of Washington) = United States, since Washington is a geographical location in the United States. Note that they also correspond to the cases where a substring of $e_{1}$ is likely to co-appear with $e_{3}$ in the same training sequence, e.g., university-locationcountry-anthem( University of Washington) = The StarSpangled Banner where "Washington" likely co-occurs with "The Star-Spangled Banner".</p>
<p>There are two such $r_{1}$ in our dataset: university-locationcountry and person-birthcountry. We use the prompt to GPT 3.5 turbo (OpenAI, 2022) and Claude 3 Haiku to guess the bridge entity solely from the name of the head entity for the first single-hop facts of these relation types (instruction is provided in §C.2), and if any of these models correctly predict the answer, we exclude it from the dataset.</p>
<p>As a result, for the cases with country as the bridge entity type, we only use the cases where the country of location of a university or the birth country of a person is hard to guess solely from the name without knowing the correct fact, such as university-locationcountry(The International Graduate School of English) = South Korea and person-birthcountry (Natalie Portman) = Israel.</p>
<h2>A. 4 Dataset Quality Assurance</h2>
<p>We apply several heuristic filterings to enhance the quality of the dataset such as excluding the cases without a natural language Wikidata title, excluding the cases with non-Unicode characters in the entity, excluding $e_{1}$ that contain double quotation marks or slashes, removing country flag emojis from the aliases of countries, and excluding the cases where each of $e_{1}, e_{2}$, and $e_{3}$ is a substring of the others. HTML characters are escaped and normalized. We discover that when all the open-source LLMs we evaluate fail to correctly answer a single-hop query, it is either because there is an error or noise in the answer set (Wikidata aliases) or the single-hop fact is not popular, and thus discard such cases from the dataset. Additionally, we use only the test cases where any alias combination of $e_{1}$ and $e_{2}$, and $e_{2}$ and $e_{3}$ appear together in Dolma v1.5 at least once.</p>
<h2>B Details of Evaluation Procedure</h2>
<h2>B. 1 Excluding unusable Cases</h2>
<p>We observe that there are test cases such that the LLM generation is evaluated as correct, but the test cases are actually unusable for correct evaluation of the latent multi-hop reasoning due to the way the model generates the answer. The first type of such case is when the model completes the query as if constructing the answer choices of a multiplechoice question, such as completing "National anthem of Woodie Flowers's country of birth:" with "1. "O Canada" 2. "The Star-Spangled Banner" 3. "God Save the Queen" ". ${ }^{9}$ When this is the case for the completion of a single-hop or multi-hop query, we mark the case unusable.</p>
<p>The second type of unusable only applies to multi-hop queries. Especially for instruction-tuned models, even though we explicitly instruct the model to directly generate the answer without the bridge entity (§5.1), the models sometimes generate the bridge entity before generating the answer, such as completing "The name of the national anthem of the country where Rishi Bankim Chandra Colleges is based is" with " $\backslash n$ The correct answer is India. $\backslash n$ The national anthem of India is Jana Gana Mana." Such cases should be excluded from the evaluation of the latent multi-hop reasoning ability. Therefore, for the multi-hop queries with the EM score of 1, we additionally check if the LLM completion contains any of $e_{2} \in E_{2}$ before the earliest $e_{3} \in E_{3}$. If it is the case, we mark the case as unusable.</p>
<h2>B. 2 Normalized Exact Match Score</h2>
<p>To check whether an LLM has correctly predicted the answer to the given test query, we use the binary score of normalized exact match score (EM). For each single-hop query, we apply string normalization to the completion of the LLM and each of the answer candidates in the answer set, which is the alias of the entity. The normalization consists of applying lowercase, removing accents, articles, and spaces in abbreviations, and replacing</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Relation Composition Type</th>
<th style="text-align: center;">Relation Composition Subtype</th>
<th style="text-align: center;">Count</th>
<th style="text-align: center;">Example Multi-hop Query</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">person-birthcity-eventyear</td>
<td style="text-align: center;">person-birthcity-g7year</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">The G7 Summit was hosted in $e_{1}$ 's birth city in the year</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthcity-capitalofcultureyear</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">The year the birth city of $e_{1}$, was declared as the European Capital of Culture was</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthcity-olympicswinteryear</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">The city where $e_{1}$ was born hosted the Winter Olympics in the year</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthcity-entertainment</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">The year when the birth city of $e_{1}$ hosted the Eurovision Song Contest was</td>
</tr>
<tr>
<td style="text-align: center;">person-birthcountry-anthen</td>
<td style="text-align: center;">person-birthcountry-anthen</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">The name of the national anthem of the birth country of $e_{1}$ is</td>
</tr>
<tr>
<td style="text-align: center;">person-birthcountry-isocode</td>
<td style="text-align: center;">person-birthcountry-isocode</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">The ISO 3166-1 numeric code of the country where $e_{1}$ was born is</td>
</tr>
<tr>
<td style="text-align: center;">university-locationcountry-anthen</td>
<td style="text-align: center;">university-locationcountry-anthen</td>
<td style="text-align: center;">101</td>
<td style="text-align: center;">The country where $e_{1}$ is based has the national anthem named</td>
</tr>
<tr>
<td style="text-align: center;">university-locationcountry-isocode</td>
<td style="text-align: center;">university-locationcountry-isocode</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">The ISO 3166-1 numeric code used for the country where $e_{1}$ is located is</td>
</tr>
<tr>
<td style="text-align: center;">university-locationcountry-year</td>
<td style="text-align: center;">university-locationcountry-year</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">The country where $e_{1}$ is located was established in the year</td>
</tr>
<tr>
<td style="text-align: center;">person-andegrundaniversity-founder</td>
<td style="text-align: center;">person-andegrundaniversity-founder</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">The person who founded the university where $e_{1}$ studied as an andegrud is named</td>
</tr>
<tr>
<td style="text-align: center;">person-andegrundaniversity-year-</td>
<td style="text-align: center;">person-andegrundaniversity-year</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">The establishment year of the university where $e_{1}$ studied as an andegrud is</td>
</tr>
<tr>
<td style="text-align: center;">city-eventyear-winner</td>
<td style="text-align: center;">city-entertainment-publicthen</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">In the year the Eurovision Song Contest took place in $e_{1}$, the laureate of the Nobel Prize in Chemistry was</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">city-g7year-publicthen</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">In the year when the G7 Summit were hosted in $e_{1}$, the Nobel Prize in Chemistry was awarded to</td>
</tr>
<tr>
<td style="text-align: center;">person-birthyear-evenicity</td>
<td style="text-align: center;">person-birthyear-championshipscity</td>
<td style="text-align: center;">196</td>
<td style="text-align: center;">In $e_{1}$ 's year of birth, the best city of the Champions League final was</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-capitalofculturecity</td>
<td style="text-align: center;">264</td>
<td style="text-align: center;">In the year $e_{1}$ was born, the city that was named the European Capital of Culture was</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-olympicswintercity</td>
<td style="text-align: center;">208</td>
<td style="text-align: center;">In $e_{1}$ 's year of birth, the Winter Olympics were hosted in the city of</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-entertainment</td>
<td style="text-align: center;">391</td>
<td style="text-align: center;">In $e_{1}$ 's birth year, the best city of the Eurovision Song Contest was</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-g7city</td>
<td style="text-align: center;">167</td>
<td style="text-align: center;">In the year $e_{1}$ was born, the best city of the G7 Summit was</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-olympicsummercity</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">The city where the Summer Olympics took place in $e_{1}$ 's year of birth is</td>
</tr>
<tr>
<td style="text-align: center;">person-birthyear-evenicountry</td>
<td style="text-align: center;">person-birthyear-olympicsummercountry</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">In $e_{1}$ 's birth year, the Summer Olympics were hosted in the country of</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-championshipscountry</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">In the birth year of $e_{1}$, the Champions League final was hosted in the country of</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-olympicswintercountry</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">In $e_{1}$ 's birth year, the Winter Olympics were in the country of</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-g7country</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">The country that hosted the G7 Summit in $e_{1}$ 's birth year is</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-entertainmentcity</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">The country where the Eurovision Song Contest took place in the birth year of $e_{1}$ is</td>
</tr>
<tr>
<td style="text-align: center;">person-birthyear-hosilender</td>
<td style="text-align: center;">person-birthyear-hosilender</td>
<td style="text-align: center;">260</td>
<td style="text-align: center;">The person who was the best leader of the G7 Summit in $e_{1}$ 's year of birth is</td>
</tr>
<tr>
<td style="text-align: center;">person-birthyear-winner</td>
<td style="text-align: center;">person-birthyear-nobelpycned</td>
<td style="text-align: center;">655</td>
<td style="text-align: center;">In the birth year of $e_{1}$, the Nobel Prize in Physiology or Medicine was awarded to</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-nobelphysics</td>
<td style="text-align: center;">675</td>
<td style="text-align: center;">The winner of the Nobel Prize in Physics in the year $e_{1}$ was born is</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-publicthen</td>
<td style="text-align: center;">853</td>
<td style="text-align: center;">In the birth year of $e_{1}$, the Nobel Prize in Chemistry was awarded to</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-nobelth</td>
<td style="text-align: center;">777</td>
<td style="text-align: center;">In the birth year of $e_{1}$, the Nobel Prize in Literature was awarded to</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-masteredampion</td>
<td style="text-align: center;">931</td>
<td style="text-align: center;">In the year $e_{1}$ was born, the winner of the Masterc Tournament was</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">person-birthyear-nobelphase</td>
<td style="text-align: center;">933</td>
<td style="text-align: center;">The Nobel Peace Prize in the year $e_{1}$ was born was awarded to</td>
</tr>
<tr>
<td style="text-align: center;">university-inceptonyear-evenicity</td>
<td style="text-align: center;">university-inceptonyear-championshipscity</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">In the year $e_{1}$ was founded, the Champions League final was hosted in the city of</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">university-inceptonyear-entertainmentcity</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">The city that hosted the Eurovision Song Contest in $e_{1}$ 's inception year is</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">university-inceptonyear-olympicswintercity</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">The city that hosted the Winter Olympics in the inception year of $e_{1}$ is</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">university-inceptonyear-g7city</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">In the inception year of $e_{1}$, the best city of the G7 Summit was</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">university-inceptonyear-olympicsummercity</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">In the year $e_{1}$ was founded, the best city of the Summer Olympics was</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">university-inceptonyear-capitalofculturecity</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">The city that became the European Capital of Culture in the founding year of $e_{1}$ was</td>
</tr>
<tr>
<td style="text-align: center;">university-inceptionyear-evenicountry</td>
<td style="text-align: center;">university-inceptonyear-entertainmentcity</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">The country that hosted the Eurovision Song Contest in the inception year of $e_{1}$ is</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">university-inceptonyear-championshipscountry</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">The country where the Champions League final took place in the inception year of $e_{1}$ is</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">university-inceptonyear-g7country</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">In the year $e_{1}$ was founded, the best country of the G7 Summit was</td>
</tr>
<tr>
<td style="text-align: center;">university-inceptonyear-hosilender</td>
<td style="text-align: center;">university-inceptonyear-hosilender</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">In the year $e_{1}$ was founded, the best leader of the G7 Summit was</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">university-inceptonyear-nobelth</td>
<td style="text-align: center;">159</td>
<td style="text-align: center;">In the inception year of $e_{1}$, the laureate of the Nobel Prize in Literature was</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">university-inceptonyear-publicthen</td>
<td style="text-align: center;">156</td>
<td style="text-align: center;">The winner of the Nobel Prize in Chemistry in the year $e_{1}$ was founded is</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">university-inceptonyear-nobelphase</td>
<td style="text-align: center;">109</td>
<td style="text-align: center;">The Nobel Peace Prize in the inception year of $e_{1}$ was awarded to</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">university-inceptonyear-nobelphysend</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">The winner of the Nobel Prize in Physics in the founding year of $e_{1}$ is</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">university-inceptonyear-nobelpycned</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">In the year $e_{1}$ was founded, the Nobel Prize in Physiology or Medicine was awarded to</td>
</tr>
<tr>
<td style="text-align: center;">total</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7,232</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: Dataset statistics and example multi-hop test queries. The head entities are replaced with $e_{1}$ to prevent potential data leakage.
punctuation marks with spaces. The EM is 1 (correct) if any of the answer candidates is included in the generation respecting the word boundaries, and 0 (incorrect) otherwise. For the completion of the multi-hop queries, the calculation of EM goes through one more step of checking if the test case is unusable, as detailed below.</p>
<h2>C Details of Experiments</h2>
<h2>C. 1 Details of Experimental Setting</h2>
<p>Among the open-source LLMs, we evaluate Mistral Large 2407 Instruct (123B), Small 2409 Instruct (22B), and all the pretrained and instructiontuned models of Mistral Nemo 2407 (12B), Mistral 7B v0.3 (Jiang et al., 2023), Mixtral 8x7B v0.1 (Jiang et al., 2024), Qwen 2.5 (7B, 14B, 32B, 72B) (Qwen Team, 2024), Qwen 2 (7B, 72B) (Yang et al., 2024a), Yi 1.5 (6B, 9B, 34B) (01.AI et al., 2024) Gemma (2B, 7B) (Mesnard et al., 2024), Gemma 2 (2B, 9B) (Gemma Team et al., 2024), and OLMo (7B) (Groeneveld et al., 2024).</p>
<p>For all open-source LLMs, we use vLLM (Kwon et al., 2023) or HuggingFace Transformers (Wolf et al., 2020) to run the inference and greedy decoding ${ }^{10}$ All experiments are performed with 1 to 840 GB A100s using half precision. The proprietary LLM APIs are run with the default decoding parameters.</p>
<h2>C. 2 Instruction Details</h2>
<p>For the LLMs that support custom system instruction (Claude, GPT, Mistral, Qwen), we provide the instruction as the system instruction. For other models, we use the instruction at the beginning of the prompt with a separator of " $\backslash n \backslash n$ ".</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 7: Latent composability measured for subsets of the test queries in SOCRATES, grouped according to the type of the bridge entity. Latent composability varies according to the bridge entity type; it is over $80 \%$ for the best models when the bridge entity is a country, but it is around $6 \%$ when it is a year.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 8: CoT composability measured for subsets of the test queries in SOCRATES, grouped according to the type of the bridge entity. CoT composability does not fluctuate as dramatically as latent composability according to the type of the bridge entity, although the result is noisy for the city and university-type bridge entity subsets due to the small denominator.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 9: Ratio of whether each model successfully composes the facts with latent or CoT reasoning, among the cases where the models correctly predict the answer to both single-hop questions, excluding guessable and unusable cases. The results are shown only when the denominator used to calculate the composability is greater or equal to 30. There are almost no cases where latent reasoning succeeds but CoT reasoning fails.</p>
<p>CoT-suppressing instruction To suppress the default CoT behavior of instruction-tuned LLMs, we use the instruction "Fill in the blank. Write down only what goes in the blank. Do not explain your answer. The answer can consist of multiple words." and postpend " $\qquad$ " to all test queries to measure latent composability. This prompt has the most effectively prevented the CoT-style reasoning among several different task formulations that have been manually tested.</p>
<p>CoT-triggering instruction To trigger CoT of instruction-tuned LLMs, we use the following instruction: "Fill in the blank. First, write the step-by-step explanation necessary to get the solution with the prefix "EXPLANATION:". After that, write down the final answer with the prefix "ANSWER:". For the final answer, write down only what goes in the blank. The answer can consist of multiple words." and postpend " $\qquad$ " to the tested multi-hop query.</p>
<p>Internal think-step-by-step instruction We use the following instruction: "Fill in the blank. Write down only what goes in the blank. Think step-bystep, but do it only internally and do not explain it in the answer. The answer can consist of multiple words. $\backslash n \backslash n$ When is $e_{1}$ 's birth year? Use the information. $\backslash n$ " and postpend " $\qquad$ " to the tested multi-hop query.</p>
<p>Guessing the bridge entities For the first singlehop facts with university-locationcountry relations, we use the following instruction: "Guessing from the name, what are the candidates of the country where "The University of Washington" is likely to be located? To be more specific, does "The University of Washington" contain the name of a location? If so, which country is the location in? Moreover, if "The University of Washington" contains a word of a language other than English that is used in specific countries, what are the names of those countries? Make sure to list the names of the countries guessed solely from the name."</p>
<p>For the first single-hop facts with person-birthcountry relations, we use the following instruction: "Guessing from the name, what are the candidates of the country where "Shohei Ohtani" was likely to be born? To be more specific, what are the candidates of the country where someone with the first name "Shohei" was likely to be born? Likely, what are the candidates of the country where someone with the last name "Ohtani" was likely to be born? Make sure to list the names of the countries guessed solely from the person name."</p>
<h2>C. 3 Additional Google Search Filter</h2>
<p>For the subset of the test queries with country-type bridge entities where latent composability is notably high, we experiment with adding a Google Search filter to further exclude test cases where the head and answer entities appear together in any of approximately 400B documents indexed by the Google Search Engine. Note that such filtering is aggressive and greatly reduces the number of test cases usable for measuring latent composability. Since the latent composability of only five models is calculated with a denominator of greater or equal to 30 , we calculate the average relative change of latent composability among these five models. Denoting $c$ as the original latent composability and $c^{\prime}$ as the latent composability on the subset with an additional Google Search filter, we calculate the
average relative drop after applying Google Search as $\mathbb{E}\left[\frac{c-c^{\prime}}{c}\right]$, and the value is minimal as 0.03 .</p>
<h2>C. 4 Patchscopes Experiment</h2>
<p>Using Patchscopes (Ghandeharioun et al., 2024) following the study of Biran et al. (2024), we check how often the latent representation of the bridge entity and the answer entity is constructed at the last token of the descriptive mention of the bridge entity (e.g., "the year Scarlett Johansson was born") and the last token of the multi-hop query.</p>
<p>The experiment is done using the following procedure. First, we take a certain layer's hidden state computed when an LLM processes a multi-hop query (the source prompt) at either the last token position of the descriptive mention of the bridge entity or the multi-hop query. Second, we feed the target prompt "StarCraft: StarCraft is a science fiction real-time strategy game, Leonardo DiCaprio: Leonardo DiCaprio is an American actor, Samsung: Samsung is a South Korean multinational corporation, $x^{" 11}$ into the same LLM, with activation patching (Vig et al., 2020) of replacing a certain layer's hidden state at the token " $x$ " with the hidden state taken from the source prompt. Lastly, we let the model generate the output for the target prompt with the replaced hidden state, and check if the bridge entity or answer entity is included in the model's output. Following Biran et al. (2024), we sample three generations for each patch with a temperature of 1.0 and count it a success if the entity is included in any of the generations.</p>
<p>Since the target prompt follows the format of "entity: entity description" where the entity description always starts by repeating the entity, if a latent representation of the entity is clearly constructed at the multi-hop query, it should be able to generate the entity from itself alone when it is patched to " $x$ " in the target prompt. We perform activation patching from each layer of the computation of the source prompt to each layer of the computation of the target layer and measure the extraction rate of the entity; whether the output with the activation patching contains the entity. Note that the extraction is successful only when the latent representation of the entity emerges sufficiently clearly to be able to decode the entity only from the repre-</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 10: Latent composability measured with shortcut free/prone data and evaluation. The blue bars show the latent composability on SOCRATES evaluated with the proposed evaluation procedure, while the purple bars show the latent composability on shortcut-prone evaluation. The results are shown only when the denominator used to calculate the composability is greater or equal to 30. Latent composability measured with SOCRATES and the proposed evaluation procedure is consistently lower than that measured with shortcut-prone data and evaluation across all models, implying that overlooking shortcut exploitation can lead to an overestimation of the actual latent composability.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 11: Experimental results with Mistral 7B v0.3 that apply Patchscopes (Ghandeharioun et al., 2024) to examine whether the model constructs latent representations of the bridge entity and answer entity at the last token of the descriptive mention of the bridge entity and the last token of the multi-hop query, for the queries with country-type bridge entities (<strong>top</strong>) and year-type bridge entities (<strong>bottom</strong>). Latent representations of bridge entities are constructed more often for queries with country-type bridge entities (that have higher latent composability).</p>
<p>The estimation itself, and thus the extraction rate can be thought as a lower bound of how often the latent representation of the entity is constructed by the model while processing the multi-hop query.</p>
<p>Figure 11 shows how often the hidden states taken from the layers at the end of the descriptive mention or the end of the source prompt (y-axis) generate the bridge or answer entity when patched into the layers of the target prompt (x-axis), for the queries with country and year-type bridge entities where both single hop facts are known by the model. The results for other types of queries are not shown due to an insufficient number of such cases. The bridge entity is generated more often, which suggests that the latent bridge entity representations are constructed more often, for the type of queries with higher latent composability (queries with country-type bridge entities).</p>
<h3>C.5 Emergence of Latent Multi-Hop Reasoning</h3>
<p>OLMo (Groeneveld et al., 2024) provides intermediate training checkpoints (557 checkpoints from 1K to 557K pretraining steps) and the pretrain-</p>
<p>ing sequences that the model learns at each of the 557K steps. This allows us to: (1) definitively verify whether the head entity ( $e_{1}$ ) and answer entity $\left(e_{3}\right)$ of a multi-hop test query appear together in any single sequence during pretraining, without relying on any approximation, and (2) track the emergence of latent multi-hop reasoning ability by monitoring the model's accuracy as training progresses.</p>
<p>We build an ElasticSearch index using all of OLMo 7B's pretraining sequences to check whether $\left(e_{1}, e_{2}\right),\left(e_{2}, e_{3}\right)$, and $\left(e_{1}, e_{3}\right)$ co-appear in any single training sequence that the model learns at each pretraining step. Then, for the test queries where $\left(e_{1}, e_{3}\right)$ never appears across all pretraining steps, we analyze the model's prediction accuracy for both single-hop and multi-hop queries. During the evaluation, we exclude any test case that is guessable or unusable at any of the 557 pretraining steps.</p>
<p>Through this evaluation procedure, we observe 13 (11.8\%) cases where the model successfully performs multi-hop reasoning at some point during pretraining, out of 110 cases where the model is correct on both single-hop facts at some point and the model is not likely to be guessing the answer at any point during pretraining. In 12 of these 13 cases, the model begins to correctly answer the multi-hop query only after learning both constituent singlehop facts. While the number of such success cases is currently limited by the model's capacity and is too small for quantitative analysis, these examples provide direct evidence that LLMs can develop latent multi-hop reasoning capabilities purely through pretraining.</p>
<p>Figure 5 demonstrates one successful case of OLMo 7B that demonstrates the emergence of latent multi-hop reasoning ability even when $e_{1}$ and $e_{3}$ have never co-appeared in any sequence throughout pretraining. It is also noticeable that the model starts to correctly predict the answer to the singlehop queries after consistently seeing $\left(e_{1}, e_{2}\right)$ and $\left(e_{2}, e_{3}\right)$ together across multiple pretraining steps. This aligns with the finding of Chang et al. (2024) that models learn simple facts by accumulating observations of the fact.</p>
<h2>D Discussion</h2>
<h2>D. 1 Importance of Considering Dataset Distribution</h2>
<p>Due to our co-occurrence-based filtering process during dataset construction, year-type bridge entity cases comprise the majority of the dataset, while most test cases with other bridge entity types were filtered out. However, if the dataset had been constructed with mostly country-type bridge entity cases, the measured latent composability would have appeared much stronger. Therefore, when evaluating the latent multi-hop reasoning ability of LLMs, it is crucial to have diverse types of bridge entities, consider the distribution of the types of connected facts, and analyze performance separately for different types.</p>
<h2>D. 2 Why Does Latent Composability Vary Across Bridge Entity Types?</h2>
<p>We observe that latent composability varies significantly across bridge entity types, with notably high performance when facts are connected through country-type bridge entities.</p>
<p>Note that it is unlikely that such a success case of latent multi-hop reasoning is obtained due to a flaw in our dataset construction process. First, our careful dataset construction, which selects only cases where countries cannot be readily inferred (§A.3), accounts for easy guessing of countries from entity names (e.g., university-locationcountry(University of Washington) = United States). Therefore, the high latent composability of the country-type bridge entity cases does not come from the model simplifying the multi-hop query into a single-hoplike problem by easily guessing the first hop. Second, it's unlikely to stem from insufficient filtering of co-occurrences: adding a Google Search filter to exclude cases where entities appear together in search results does not drop latent composability, with an average relative drop of only 0.03 (details in §C.3).</p>
<p>Drawing from findings in finetuning studies, e.g., (Jiang et al., 2022; Wang et al., 2024), one speculative explanation of what has caused LLMs to develop strong composability for test queries with country-type bridge entity is that country-related facts might be more frequently learned in composition during pretraining. While these studies show that exposure to fact compositions during finetuning can improve multi-hop reasoning, we emphasize that extending these findings to pretraining remains an untested hypothesis that warrants future investigation.</p>
<h1>D. 3 Why Is CoT Composability Much Stronger Than Latent Composability?</h1>
<p>We conjecture that the explicit generation of the bridge entity is the main factor behind high CoT composability. Transformer-based LLMs go through a subject enrichment process that helps models recall the attributes of the subject (Geva et al., 2023; Gottesman and Geva, 2024). While LLMs can develop latent representations of bridge entities (e.g., "1984" from "the year Scarlett Johansson was born") in early-middle layers (Yang et al., 2024b; Biran et al., 2024; Li et al., 2024; Wang et al., 2024), these representations may appear too late or not at all (Biran et al., 2024). In contrast, when CoT reasoning generates the correct bridge entity, it ensures a clear and early contextualized representation of the bridge entity to form, facilitating retrieval of the second single-hop fact.</p>
<p>Supporting this hypothesis, merely instructing models to think step-by-step (Kojima and Gu, 2022) but only internally, thus without generating the bridge entity, does not improve performance; Claude 3.5 Sonnet's latent composability remains low ( $6.1 \%$ ) even with an explicit hint to identify and utilize the information of the bridge entity (instruction shown in §C.2). Moreover, $96.0 \%$ of CoT failures of Claude 3.5 Sonnet stem from incorrect bridge entity generation, highlighting its crucial role.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ We slightly modify the original target prompt used in the work of Ghandeharioun et al. (2024) and Biran et al. (2024), to use the description of StarCraft instead of "Syria: Syria is a country in the Middle East", in order to avoid any entity that falls into the types of the bridge entity for our dataset being used as the few-shot example and contaminating our analysis.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>3"Scarlett Johansson was born in 1984, and the Summer Olympics that year were hosted by the United States."
${ }^{4}$ While it is possible to use few-shot learning to restrict the format of the answer (Ofir Press et al., 2023), we use instructions to avoid potential biases in the selection of the few-shot demonstrations (Zhang et al., 2022b).&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9344 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9344</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9344</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-919ae741e8e8796e05839b2b556f4ac636bce463</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/919ae741e8e8796e05839b2b556f4ac636bce463" target="_blank">Uncertainty-aware prediction of chemical reaction yields with graph neural networks</a></p>
                <p><strong>Paper Venue:</strong> Journal of Cheminformatics</p>
                <p><strong>Paper TL;DR:</strong> The predictive distribution of the yield is modeled as a graph neural network that directly processes a set of graphs with permutation invariance that improves the prediction and uncertainty quantification performance in most settings.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we present a data-driven method for the uncertainty-aware prediction of chemical reaction yields. The reactants and products in a chemical reaction are represented as a set of molecular graphs. The predictive distribution of the yield is modeled as a graph neural network that directly processes a set of graphs with permutation invariance. Uncertainty-aware learning and inference are applied to the model to make accurate predictions and to evaluate their uncertainty. We demonstrate the effectiveness of the proposed method on benchmark datasets with various settings. Compared to the existing methods, the proposed method improves the prediction and uncertainty quantification performance in most settings.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9344.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9344.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (reaction-pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional Encoder Representations from Transformers (pre-trained on reaction SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based masked-language encoder architecture (BERT) that the paper cites as having been pre-trained on a reaction SMILES database and then fine-tuned to predict reaction yields from text SMILES representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mapping the space of chemical reactions using attention-based neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (reaction-pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A bidirectional Transformer encoder architecture pre-trained on a reaction SMILES corpus (as cited in the paper). The paper does not report parameter count, exact pretraining data size, or hyperparameters for the pretraining — only that a BERT model pre-trained using a reaction SMILES database was used in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Organic chemistry / chemical reaction yield prediction</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based prediction of chemical reaction yields from reaction SMILES strings (i.e., using SMILES as the textual representation of reactants/products to predict percent yield).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Mean absolute error (MAE), root mean squared error (RMSE), coefficient of determination (R^2); Spearman rank correlation (ρ) used for uncertainty quantification when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>As reported for BERT-based baselines reproduced in this paper (examples for 70/30 train/test splits): Buchwald-Hartwig: MAE = 3.990 ± 0.153 %points, RMSE = 6.014 ± 0.272 %points, R^2 = 0.951 ± 0.005; Suzuki-Miyaura: MAE = 8.128 ± 0.344 %points, RMSE = 12.073 ± 0.463 %points, R^2 = 0.815 ± 0.013. (These are the reproduced baseline numbers in the paper; BERT itself is cited as the underlying encoder.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Training set size (performance degrades with less training data), representation choice (SMILES text vs graph representation), coverage of pretraining/finetuning data with respect to test-set chemistry, and presence/absence of additives or reaction components in out-of-sample splits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>BERT-based models (YieldBERT variants) are used as baselines in this paper; the proposed graph-based GNN model generally outperformed the BERT-based baseline on the benchmark datasets in many in-sample splits, though out-of-sample performance varied.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes limitations tied to small training datasets and out-of-sample generalization (e.g., missing additives in training causing poorer out-of-sample accuracy); reliance on SMILES/text representation may lose structural details compared to graph representations.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors motivate using graph-based representations and uncertainty-aware learning as alternatives to SMILES-based BERT approaches; they identify enriching reaction representations (more atom/bond features and molecular descriptors) and uncertainty quantification as paths to improved accuracy and safer selective prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncertainty-aware prediction of chemical reaction yields with graph neural networks', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9344.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9344.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>YieldBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>YieldBERT (BERT adapted for reaction-yield prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based model adapted to predict chemical reaction yields from reaction SMILES by fine-tuning a pre-trained reaction SMILES BERT encoder to regress percent yield.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prediction of chemical reaction yields using deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>YieldBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An adaptation of a pre-trained BERT encoder fine-tuned to predict reaction yields from reaction SMILES strings. The paper uses reproduced implementations (source code referenced) but does not give internal model size or pretraining corpus details beyond the citation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Organic chemistry / chemical reaction yield prediction</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based regression: predict percent yield of a chemical reaction from a textual reaction SMILES string.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MAE, RMSE, R^2 for regression accuracy; Spearman rank correlation (ρ) between absolute error and uncertainty score for uncertainty calibration (where available).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reproduced baseline results (70/30 train/test): Buchwald-Hartwig: MAE = 3.990 ± 0.153 %p, RMSE = 6.014 ± 0.272 %p, R^2 = 0.951 ± 0.005. Suzuki-Miyaura: MAE = 8.128 ± 0.344 %p, RMSE = 12.073 ± 0.463 %p, R^2 = 0.815 ± 0.013. Other train/test splits shown in Table 2 illustrate degradation with less training data.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Training-data size and split (70/30 down to 2.5/97.5 shown), data augmentation (SMILES randomization helps, see YieldBERT-DA), test distribution shifts (out-of-sample additives missing from training), and quality/coverage of pretraining/fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a baseline and compared against the proposed graph neural network (GNN) with uncertainty-aware outputs; the proposed GNN generally reduced MAE and RMSE by ~5–10% relative to YieldBERT on the benchmark datasets in many in-sample settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Out-of-sample splits and small-training regimes show worse performance; uncertainty quantification without augmentation had weaker Spearman correlation in some settings; reliance on SMILES representation and data coverage can limit generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>The paper highlights that while YieldBERT is effective, graph-based molecular representations and uncertainty-aware training/inference can improve prediction accuracy and provide calibrated uncertainty for selective prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncertainty-aware prediction of chemical reaction yields with graph neural networks', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9344.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9344.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>YieldBERT-DA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>YieldBERT-DA (YieldBERT with data augmentation via SMILES randomization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of YieldBERT that applies SMILES-based data augmentation (randomization) during training and test-time augmentation for uncertainty estimation, used to improve yield-prediction performance and quantify uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data augmentation strategies to improve reaction yield predictions and estimate uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>YieldBERT-DA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A BERT-based yield predictor augmented via SMILES randomization to increase effective training data and employing test-time augmentation to obtain prediction variance as an uncertainty score; exact model size and training corpus are not detailed in this paper beyond the cited references and reproduced code.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Organic chemistry / chemical reaction yield prediction</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based regression and uncertainty estimation: predict percent reaction yield from SMILES while using SMILES randomization as data augmentation and test-time augmentation for uncertainty scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MAE, RMSE, R^2 for regression accuracy; Spearman rank correlation (ρ) between absolute error and uncertainty score to assess uncertainty quantification quality.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reproduced baseline results (70/30 train/test): Buchwald-Hartwig: MAE = 3.090 ± 0.118 %p, RMSE = 4.799 ± 0.261 %p, R^2 = 0.969 ± 0.004, Spearman ρ (uncertainty) = 0.439 ± 0.037. Suzuki-Miyaura: MAE = 6.598 ± 0.270 %p, RMSE = 10.524 ± 0.482 %p, R^2 = 0.859 ± 0.012, Spearman ρ = 0.439 ± 0.018.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Effectiveness of SMILES augmentation (quantity and diversity of augmented SMILES), size of fine-tuning dataset, distribution shift between training and test (e.g., additives absent in training), and the way test-time augmentation is implemented for uncertainty estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared as the best-performing baseline in many experiments; the proposed graph-based GNN with uncertainty-aware outputs typically outperformed YieldBERT-DA in MAE/RMSE (≈5–10% reduction) across the benchmark datasets for many in-sample splits, though out-of-sample performance was mixed (some tests YieldBERT-DA performed worse on average).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although data augmentation improves performance and uncertainty estimates, the method can still struggle under severe out-of-sample shifts (e.g., additives unseen during training). In some out-of-sample splits YieldBERT-DA had lower average performance compared to YieldBERT or the proposed method.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Paper notes that data augmentation (SMILES randomization) is beneficial and that test-time augmentation provides a usable uncertainty score; however, combining richer molecular representations (graphs) and explicit uncertainty-aware loss (aleatoric+epistemic) provides better trade-offs between prediction accuracy and uncertainty calibration according to the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncertainty-aware prediction of chemical reaction yields with graph neural networks', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Prediction of chemical reaction yields using deep learning <em>(Rating: 2)</em></li>
                <li>Data augmentation strategies to improve reaction yield predictions and estimate uncertainty <em>(Rating: 2)</em></li>
                <li>Mapping the space of chemical reactions using attention-based neural networks <em>(Rating: 2)</em></li>
                <li>Predicting reaction performance in C-N cross-coupling using machine learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9344",
    "paper_id": "paper-919ae741e8e8796e05839b2b556f4ac636bce463",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "BERT (reaction-pretrained)",
            "name_full": "Bidirectional Encoder Representations from Transformers (pre-trained on reaction SMILES)",
            "brief_description": "A transformer-based masked-language encoder architecture (BERT) that the paper cites as having been pre-trained on a reaction SMILES database and then fine-tuned to predict reaction yields from text SMILES representations.",
            "citation_title": "Mapping the space of chemical reactions using attention-based neural networks",
            "mention_or_use": "mention",
            "model_name": "BERT (reaction-pretrained)",
            "model_description": "A bidirectional Transformer encoder architecture pre-trained on a reaction SMILES corpus (as cited in the paper). The paper does not report parameter count, exact pretraining data size, or hyperparameters for the pretraining — only that a BERT model pre-trained using a reaction SMILES database was used in prior work.",
            "scientific_subdomain": "Organic chemistry / chemical reaction yield prediction",
            "simulation_task": "Text-based prediction of chemical reaction yields from reaction SMILES strings (i.e., using SMILES as the textual representation of reactants/products to predict percent yield).",
            "evaluation_metric": "Mean absolute error (MAE), root mean squared error (RMSE), coefficient of determination (R^2); Spearman rank correlation (ρ) used for uncertainty quantification when applicable.",
            "simulation_accuracy": "As reported for BERT-based baselines reproduced in this paper (examples for 70/30 train/test splits): Buchwald-Hartwig: MAE = 3.990 ± 0.153 %points, RMSE = 6.014 ± 0.272 %points, R^2 = 0.951 ± 0.005; Suzuki-Miyaura: MAE = 8.128 ± 0.344 %points, RMSE = 12.073 ± 0.463 %points, R^2 = 0.815 ± 0.013. (These are the reproduced baseline numbers in the paper; BERT itself is cited as the underlying encoder.)",
            "factors_affecting_accuracy": "Training set size (performance degrades with less training data), representation choice (SMILES text vs graph representation), coverage of pretraining/finetuning data with respect to test-set chemistry, and presence/absence of additives or reaction components in out-of-sample splits.",
            "comparison_baseline": "BERT-based models (YieldBERT variants) are used as baselines in this paper; the proposed graph-based GNN model generally outperformed the BERT-based baseline on the benchmark datasets in many in-sample splits, though out-of-sample performance varied.",
            "limitations_or_failure_cases": "Paper notes limitations tied to small training datasets and out-of-sample generalization (e.g., missing additives in training causing poorer out-of-sample accuracy); reliance on SMILES/text representation may lose structural details compared to graph representations.",
            "author_recommendations_or_insights": "Authors motivate using graph-based representations and uncertainty-aware learning as alternatives to SMILES-based BERT approaches; they identify enriching reaction representations (more atom/bond features and molecular descriptors) and uncertainty quantification as paths to improved accuracy and safer selective prediction.",
            "uuid": "e9344.0",
            "source_info": {
                "paper_title": "Uncertainty-aware prediction of chemical reaction yields with graph neural networks",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "YieldBERT",
            "name_full": "YieldBERT (BERT adapted for reaction-yield prediction)",
            "brief_description": "A BERT-based model adapted to predict chemical reaction yields from reaction SMILES by fine-tuning a pre-trained reaction SMILES BERT encoder to regress percent yield.",
            "citation_title": "Prediction of chemical reaction yields using deep learning",
            "mention_or_use": "use",
            "model_name": "YieldBERT",
            "model_description": "An adaptation of a pre-trained BERT encoder fine-tuned to predict reaction yields from reaction SMILES strings. The paper uses reproduced implementations (source code referenced) but does not give internal model size or pretraining corpus details beyond the citation.",
            "scientific_subdomain": "Organic chemistry / chemical reaction yield prediction",
            "simulation_task": "Text-based regression: predict percent yield of a chemical reaction from a textual reaction SMILES string.",
            "evaluation_metric": "MAE, RMSE, R^2 for regression accuracy; Spearman rank correlation (ρ) between absolute error and uncertainty score for uncertainty calibration (where available).",
            "simulation_accuracy": "Reproduced baseline results (70/30 train/test): Buchwald-Hartwig: MAE = 3.990 ± 0.153 %p, RMSE = 6.014 ± 0.272 %p, R^2 = 0.951 ± 0.005. Suzuki-Miyaura: MAE = 8.128 ± 0.344 %p, RMSE = 12.073 ± 0.463 %p, R^2 = 0.815 ± 0.013. Other train/test splits shown in Table 2 illustrate degradation with less training data.",
            "factors_affecting_accuracy": "Training-data size and split (70/30 down to 2.5/97.5 shown), data augmentation (SMILES randomization helps, see YieldBERT-DA), test distribution shifts (out-of-sample additives missing from training), and quality/coverage of pretraining/fine-tuning data.",
            "comparison_baseline": "Used as a baseline and compared against the proposed graph neural network (GNN) with uncertainty-aware outputs; the proposed GNN generally reduced MAE and RMSE by ~5–10% relative to YieldBERT on the benchmark datasets in many in-sample settings.",
            "limitations_or_failure_cases": "Out-of-sample splits and small-training regimes show worse performance; uncertainty quantification without augmentation had weaker Spearman correlation in some settings; reliance on SMILES representation and data coverage can limit generalization.",
            "author_recommendations_or_insights": "The paper highlights that while YieldBERT is effective, graph-based molecular representations and uncertainty-aware training/inference can improve prediction accuracy and provide calibrated uncertainty for selective prediction.",
            "uuid": "e9344.1",
            "source_info": {
                "paper_title": "Uncertainty-aware prediction of chemical reaction yields with graph neural networks",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "YieldBERT-DA",
            "name_full": "YieldBERT-DA (YieldBERT with data augmentation via SMILES randomization)",
            "brief_description": "An extension of YieldBERT that applies SMILES-based data augmentation (randomization) during training and test-time augmentation for uncertainty estimation, used to improve yield-prediction performance and quantify uncertainty.",
            "citation_title": "Data augmentation strategies to improve reaction yield predictions and estimate uncertainty",
            "mention_or_use": "use",
            "model_name": "YieldBERT-DA",
            "model_description": "A BERT-based yield predictor augmented via SMILES randomization to increase effective training data and employing test-time augmentation to obtain prediction variance as an uncertainty score; exact model size and training corpus are not detailed in this paper beyond the cited references and reproduced code.",
            "scientific_subdomain": "Organic chemistry / chemical reaction yield prediction",
            "simulation_task": "Text-based regression and uncertainty estimation: predict percent reaction yield from SMILES while using SMILES randomization as data augmentation and test-time augmentation for uncertainty scores.",
            "evaluation_metric": "MAE, RMSE, R^2 for regression accuracy; Spearman rank correlation (ρ) between absolute error and uncertainty score to assess uncertainty quantification quality.",
            "simulation_accuracy": "Reproduced baseline results (70/30 train/test): Buchwald-Hartwig: MAE = 3.090 ± 0.118 %p, RMSE = 4.799 ± 0.261 %p, R^2 = 0.969 ± 0.004, Spearman ρ (uncertainty) = 0.439 ± 0.037. Suzuki-Miyaura: MAE = 6.598 ± 0.270 %p, RMSE = 10.524 ± 0.482 %p, R^2 = 0.859 ± 0.012, Spearman ρ = 0.439 ± 0.018.",
            "factors_affecting_accuracy": "Effectiveness of SMILES augmentation (quantity and diversity of augmented SMILES), size of fine-tuning dataset, distribution shift between training and test (e.g., additives absent in training), and the way test-time augmentation is implemented for uncertainty estimation.",
            "comparison_baseline": "Compared as the best-performing baseline in many experiments; the proposed graph-based GNN with uncertainty-aware outputs typically outperformed YieldBERT-DA in MAE/RMSE (≈5–10% reduction) across the benchmark datasets for many in-sample splits, though out-of-sample performance was mixed (some tests YieldBERT-DA performed worse on average).",
            "limitations_or_failure_cases": "Although data augmentation improves performance and uncertainty estimates, the method can still struggle under severe out-of-sample shifts (e.g., additives unseen during training). In some out-of-sample splits YieldBERT-DA had lower average performance compared to YieldBERT or the proposed method.",
            "author_recommendations_or_insights": "Paper notes that data augmentation (SMILES randomization) is beneficial and that test-time augmentation provides a usable uncertainty score; however, combining richer molecular representations (graphs) and explicit uncertainty-aware loss (aleatoric+epistemic) provides better trade-offs between prediction accuracy and uncertainty calibration according to the authors' experiments.",
            "uuid": "e9344.2",
            "source_info": {
                "paper_title": "Uncertainty-aware prediction of chemical reaction yields with graph neural networks",
                "publication_date_yy_mm": "2022-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Prediction of chemical reaction yields using deep learning",
            "rating": 2
        },
        {
            "paper_title": "Data augmentation strategies to improve reaction yield predictions and estimate uncertainty",
            "rating": 2
        },
        {
            "paper_title": "Mapping the space of chemical reactions using attention-based neural networks",
            "rating": 2
        },
        {
            "paper_title": "Predicting reaction performance in C-N cross-coupling using machine learning",
            "rating": 1
        }
    ],
    "cost": 0.01232975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Uncertainty-aware prediction of chemical reaction yields with graph neural networks</h1>
<p>Youngchun Kwon ${ }^{1,2}$, Dongseon Lee ${ }^{1}$, Youn-Suk Choi ${ }^{1 <em>}$ and Seokho Kang ${ }^{3 </em>}$ (D)</p>
<h4>Abstract</h4>
<p>In this paper, we present a data-driven method for the uncertainty-aware prediction of chemical reaction yields. The reactants and products in a chemical reaction are represented as a set of molecular graphs. The predictive distribution of the yield is modeled as a graph neural network that directly processes a set of graphs with permutation invariance. Uncertainty-aware learning and inference are applied to the model to make accurate predictions and to evaluate their uncertainty. We demonstrate the effectiveness of the proposed method on benchmark datasets with various settings. Compared to the existing methods, the proposed method improves the prediction and uncertainty quantification performance in most settings.</p>
<p>Keywords: Chemical reaction yield prediction, Uncertainty-aware prediction, Graph neural network, Deep learning</p>
<h2>Introduction</h2>
<p>In organic chemistry, the prediction of chemical reaction yields is an important research topic in chemical synthesis planning [1, 2]. This enables the estimation of the overall yield of a complex synthetic pathway and the detection of low-yield reactions that negatively affect the overall yield. It also provides clues for designing new reactions that provide higher yields to save on the time and cost required for experimental syntheses.</p>
<p>Machine learning has achieved remarkable success in the data-driven prediction of chemical reaction yields [1, 3-7]. The main concept is to construct a prediction model that predicts the yield of a chemical reaction by learning from previously accumulated data comprising a number of chemical reactions annotated with their experimentally measured yields. The successful application of a prediction model enables fast and efficient estimation of chemical reaction yields without</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>performing experimental syntheses, which are costly and time-consuming.</p>
<p>Early studies represented each chemical reaction as a fixed-size vector of handcrafted features, such as molecular fingerprints and chemical property descriptors, and constructed an off-the-shelf prediction model on top of the vector representation [3-5, 8]. The limitation of this approach is that the choice of adequate features relies on chemical knowledge and intuition, and some inherent information to the original reaction may be lost in the representation. With advances in deep learning [9], recent studies have applied deep neural networks constructed on a more informative representation of a chemical reaction. Schwaller et al. $[6,10]$ used simplified molecular-input line-entry system (SMILES) to represent a chemical reaction. To predict the reaction yield, they fine-tuned a bidirectional encoder representations from transformers (BERT) model pre-trained using a reaction SMILES database [11] to predict the yield. Saebi et al. [7] represented a chemical reaction as a set of graphs, on which a graph neural network was constructed to predict the yield.</p>
<p>In this paper, we present an alternative method for predicting chemical reaction yields. As a prediction model, we adapt a graph neural network that directly operates</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>on the graph representation of a chemical reaction in a permutation-invariant fashion. We use uncertainty-aware learning and inference in the model to make accurate predictions of yields and determine the confidence of predictions.</p>
<h1><strong>Methods</strong></h1>
<h2><strong>Data representation</strong></h2>
<p>We suppose that a chemical reaction consists of a number of reactants and a single product. This chemical reaction is labeled with its reaction yield. Each instance is represented as (R, P, y), where R = {G^{R,1}, . . . , G^{R,m}} and P = {G^{P}} are the set of m reactants and the resulting product in the reaction, respectively, and y is the reaction yield. The number of reactants m can be different for each reaction.</p>
<p>Each molecule in R and P is defined as an undirected graph G = (V, E), where V and E represent the set of nodes and the set of edges, respectively. The node feature vectors v^{i} ∈ V and edge feature vectors e^{j,k} ∈ E are associated with heavy atoms (e.g., C, N, O, and F) and their bonds (e.g., single, double, triple, and aromatic), respectively. Hydrogen atoms are treated implicitly. The number of heavy atoms and bonds in each molecule is the same as the number of node feature vectors and edge feature vectors in the corresponding graph representation, respectively. Figure 1 illustrates an example of the graph representation of a molecule.</p>
<p>For the j-th atom, v^{j} = (v^{j,1}, . . . , v^{j,p}) is a vector indicating the atom type, formal charge, degree, hybridization, number of hydrogens, valence, chirality, whether it accepts or donates electrons, whether it is aromatic, whether it is in a ring, and associated ring sizes. For the bond between the j-th and k-th atoms, e^{j,k} = (e^{j,k,1}, . . . , e^{j,k,q}) is a vector indicating the bond type, stereochemistry, whether it is in a ring, and whether it is conjugated.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p><strong>Fig. 1</strong> Illustrative example of the graph representation for a molecule</p>
<h2><strong>Prediction model</strong></h2>
<p>To predict the reaction yield y, we introduce a predictive distribution for y conditioned on the set of reactants R and product P, denoted by pθ(y|R, P), which is modeled as a normal distribution as follows:</p>
<p>$$p_θ(y|\mathcal{R}, \mathcal{P}) = \mathcal{N}(y|\mu, \sigma^2), \tag{1}$$</p>
<p>where μ and σ^{2} are the mean and variance of the distribution, respectively. We parameterize the predictive distribution pθ using a neural network f that produces μ and σ^{2} as a function of R and P with a set of parameters θ:</p>
<p>$$(\mu, \sigma^2) = f(\mathcal{R}, \mathcal{P}; \theta). \tag{2}$$</p>
<p>To construct the neural network f, we adapt the architecture presented by Saebi et al. [7] to process two sets of molecular graphs with advanced neural network modules. Figure 2 illustrates the architecture used in this study. The architectural details of each component are presented next.</p>
<p>A message passing neural network (MPNN) [12] is used as the GNN component of f to process each molecular graph G in R and P. The GNN is designed to take G as the input and return the graph representation vector r as the output:</p>
<p>$$\mathbf{r} = \text{GNN}(\mathcal{G}). \tag{3}$$</p>
<p>In the GNN, we apply multiple message passing steps using an edge network as a message function and a gated recurrent unit (GRU) network as an update function to generate node representation vectors. We then apply a set2set model [13] as a readout function for global pooling over the node representation vectors to obtain a graph-level embedding that is invariant to the order of the nodes. The embedding is sparsified by a fully-connected layer to obtain the graph representation vector r. The use of the GNN renders the representation invariant to graph isomorphism.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>We summate the graph representation vectors for $\mathcal{R}=\left{\mathcal{G}^{R, 1}, \ldots, \mathcal{G}^{R, m}\right}$. This makes the representation invariant with respect to the order of the reactants. The summated vector is concatenated with the graph representation vector $\mathcal{P}=\left{\mathcal{G}^{P}\right}$ to generate the reaction representation vector $\mathbf{h}$ :</p>
<p>$$
\mathbf{h}=\left[\sum_{J=1}^{m} \mathbf{r}^{R, J}, \mathbf{r}^{P}\right]
$$</p>
<p>The reaction representation vector $\mathbf{h}$ is further processed by a feed-forward neural network (FNN) with two output units. The first unit returns the predictive mean $\mu$. The second unit returns the log predictive variance $\log \sigma^{2}$.
The main advantages of the prediction model $f$ presented in this study can be summarized as follows. First, the input for the model is the graph representation of a chemical reaction, which can directly encompass various atom and bond features regarding their chemical properties that make the representation more informative. Second, the model can handle chemical reactions of varying sizes with different numbers of reactants as the input. Third, the output of the model is invariant to permutations of reactants in the input reaction and is also invariant to permutations of atoms in each of the reactants/ products. Fourth, the output of the model specifies the corresponding predictive distribution, which allows for uncertainty-aware learning and inference.</p>
<h2>Uncertainty-aware learning</h2>
<p>The learning procedure aims to train the prediction model $f$ such that it can estimate the predictive mean $\mu$ and variance $\sigma^{2}$ of the unknown yield $y$ for a chemical reaction $(\mathcal{R}, \mathcal{P})$. For the model $f$ to learn from data, we construct a training dataset of $N$ chemical reactions and their yields, denoted by $\mathcal{D}=\left{\left(\mathcal{R}<em i="i">{i}, \mathcal{P}</em>\right)\right}}, y_{i<em _theta="\theta">{i=1}^{N}$.
We train the model $f$ based on the maximum likelihood estimation. Based on the normality assumption for the predictive distribution $p</em>$, the log-likelihood is given by:</p>
<p>$$
\begin{aligned}
\log p_{\theta}(y \mid \mathcal{R}, \mathcal{P}) &amp; =-\frac{1}{2} \log \left(2 \pi \sigma^{2}\right)-\frac{1}{2} \frac{(y-\mu)^{2}}{\sigma^{2}} \
&amp; =-\frac{1}{2} \log (2 \pi)-\frac{1}{2}\left[\frac{(y-\mu)^{2}}{\sigma^{2}}+\log \sigma^{2}\right]
\end{aligned}
$$</p>
<p>Given a training dataset $\mathcal{D}$, the model is trained to minimize the objective function $\mathcal{J}$ :</p>
<p>$$
\begin{aligned}
\mathcal{J}(\theta)= &amp; (1-\lambda) \cdot \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\mu_{i}\right)^{2} \
&amp; +\lambda \cdot \frac{1}{N} \sum_{i=1}^{N}\left[\frac{\left(y_{i}-\mu_{i}\right)^{2}}{\sigma_{i}^{2}}+\log \sigma_{i}^{2}\right]
\end{aligned}
$$</p>
<p>which involves two learning objectives with the hyperparameter $\lambda$ that controls the relative strength of each objective. The first term is to minimize the conventional mean squared error over the training dataset $\mathcal{D}$, which corresponds to the maximization of the log-likelihood over $\mathcal{D}$ under the homoscedasticity assumption. The second term is to maximize the log-likelihood over $\mathcal{D}$ under the heteroscedasticity assumption. The first term contributes to stabilizing the training with respect to the predictive mean $\mu$. The second term enables the predictive variance $\sigma^{2}$ to quantify the aleatoric uncertainty caused by the inherent noise in $\mathcal{D}$.</p>
<h2>Uncertainty-aware inference</h2>
<p>Once trained, the prediction model $f$ is used to predict the yields of new chemical reactions. We employ the Monte-Carlo (MC) dropout [14] for the Bayesian approximation of the model $f$. Following the Bayesian approach, the approximate predictive distribution $q$ is given by</p>
<p>$$
q\left(y_{<em>} \mid \mathcal{R}_{</em>}, \mathcal{P}<em>{<em>}\right)=\int p_{\theta}\left(y_{</em>} \mid \mathcal{R}</em>{<em>}, \mathcal{P}_{</em>}\right) d \theta
$$</p>
<p>Given a query reaction $\left(\mathcal{R}<em>{<em>}, \mathcal{P}_{</em>}\right)$, we wish to predict the unknown yield $y</em>{<em>}$ of the reaction as well as to quantify the uncertainty of the prediction. We empirically derive the MC estimates by sampling $T$ predictions $\left{\left(\hat{\mu}_{</em>}^{(t)}, \hat{\sigma}<em t="1">{<em>}^{2(t)}\right)\right}<em>{t=1}^{T}$ based on stochastic forward passes through the model $f$ with dropout applied. Because some hidden units are randomly dropped out at each forward pass, the $T$ predictions vary for the same reaction. The variability in the predictions is primarily caused by the epistemic uncertainty of the model $f$ owing to the insufficiency of the training dataset $\mathcal{D}$.
For prediction, the predictive mean can be estimated by averaging over $\left{\hat{\mu}</em>{</em>}^{(t)}\right}</em>$ :}^{T</p>
<p>$$
\mathrm{E}<em>{q\left(y</em>{<em>} \mid \mathcal{R}_{</em>}, \mathcal{P}<em t="1">{<em>}\right)}\left[y_{</em>}\right] \simeq \frac{1}{T} \sum</em>
$$}^{T} \hat{\mu}_{*}^{(t)</p>
<p>This is used as the prediction of $y_{*}$.
For uncertainty quantification, the predictive variance can be estimated as:</p>
<p>$$ \operatorname{Var}<em>{\theta\left(y</em>{<em>}\right) \mathcal{R}_{</em>},\left.\mathcal{P}<em t="1">{<em>}\right)}\left[y_{</em>}\right] \simeq \frac{1}{T} \sum</em>}^{T} \hat{\sigma<em _="*">{<em>}^{2(t)}+\frac{1}{T} \sum_{t=1}^{T}\left(\tilde{\mu}_{</em>}^{(t)}-\tilde{\mu}</em> $$}\right)^{2</p>
<p>where $\tilde{\mu}_{<em>}=\frac{1}{T} \sum_{t=1}^{T} \hat{\mu}_{</em>}^{(t)}$. This is used as the uncertainty score for the prediction. The predictive variance can be decomposed into two types of uncertainty [15]. The first term corresponds to the aleatoric uncertainty, which accounts for the statistical uncertainty caused by inherent noise in the dataset $\mathcal{D}$. The second term corresponds to the epistemic uncertainty, which accounts for the systemic uncertainty in the model $f$ caused by the insufficiency of $\mathcal{D}$.</p>
<p>The prediction of chemical reaction yields supports the identification of high-yield reactions from a pool of possible candidates in an efficient manner. The prerequisite is that the prediction model must be as accurate as possible. In practice, the prediction model may be imperfect and result in inaccurate predictions. To overcome this issue, we can selectively use the model based on uncertainty quantification. Because a high prediction uncertainty tends to cause erroneous predictions, the rejection of uncertain predictions would be beneficial for the actual use of the prediction model. If the prediction uncertainty is sufficiently low, we can use the model with confidence to identify whether a reaction has a high yield. Otherwise, the model abstains from predicting. Rejected cases can be carefully investigated by chemists in terms of their yields.</p>
<h2>Experimental investigation</h2>
<h2>Datasets</h2>
<p>We investigate the effectiveness of the proposed method using the following two benchmark datasets: BuchwaldHartwig [3] and Suzuki-Miyaura [16]. In these datasets, each reaction was annotated with a measured yield ranging from $0 \%$ to $100 \%$. The summary statistics of the datasets are presented in Table 1.</p>
<p>The Buchwald-Hartwig dataset was released by Ahneman et al. [3]. They conducted high-throughput experiments on the class of Pd-catalyzed Buchwald-Hartwig C-N cross-coupling reactions. They experimented on combinations of 15 aryl halides, 4 ligands, 3 bases, and 23 additives. A total of 3955 reactions were reported with</p>
<p>Table 1 Description of benchmark datasets</p>
<p>|  Dataset | No. reactions | No. reactants
per reaction | No. products
per reaction  |
| --- | --- | --- | --- |
|  Buchwald-Hartwig | 3955 | 6 | 1  |
|  Suzuki-Miyaura | 5760 | $6-14$ | 1  |</p>
<p>their measured yields. The studies [3-6] evaluated the performance of the chemical reaction yield prediction on this dataset.</p>
<p>The Suzuki-Miyaura dataset was released by Perera et al. [16]. They conducted high-throughput experiments on the class of Suzuki-Miyaura cross-coupling reactions. 15 couplings of electrophiles and nucleophiles across combinations of 12 ligands, 8 bases, and 4 solvents were considered, resulting in measured yields for a total of 5760 reactions. The studies $[6,16,17]$ have investigated this dataset.</p>
<p>For experimental investigations, we use 10 random shuffles for each benchmark dataset and 4 out-of-sample splits of the Buchwald-Hartwig dataset $[3,6]$.</p>
<h2>Implementation</h2>
<p>In the experimental investigation, we use the following configurations for the proposed method. For the GNN component of the model, the node representation vectors and graph representation vectors have dimensions of 64 and 1024, respectively. The graph representation vectors were set to have higher dimensionality because they are summated over multiple reactants to obtain the reaction representation vector. The number of message passing steps and set2set processing steps are both set to 3 . Increasing the size of the GNN component may provide better performance, but it also incurs higher computational costs and memory usage. Thus, we set it to moderately large so that it can be trained in a reasonable time. The FNN component of the model has two fully-connected layers with 512 dimensions, followed by an output layer. During training, we standardize the yield $y$ to have a mean of 0 and a variance of 1 over the training dataset $\mathcal{D}$. A dropout rate of 0.1 is applied to the fully-connected layers in the FNN component. The hyperparameter $\lambda$ in the objective function $\mathcal{J}$ is set to 0.1. L2 regularization with a factor of $10^{-5}$ is applied to the parameters $\theta$. To train the model $f$, we update the parameters $\theta$ for 500 epochs using the Adam optimizer with a batch size of 128 . The learning rate is set to $10^{-3}$ for the initial epochs and decayed to $10^{-4}$ and $10^{-5}$ over the last 100 epochs. We did not consider hyperparameter optimization through holdout validation, because it is unsuitable when the training dataset is very small. At inference, we set the number of forward passes $T$ to 30 for MC dropout. We use Equation 8 and Equation 9 for the prediction and uncertainty score, respectively.</p>
<p>The proposed method is implemented using PyTorch in Python. The source code used in this study is available online at http://github.com/seokhokang/ reaction_yield_nn/. The results of the experimental</p>
<p>investigations are reported and discussed in the following section.</p>
<h2>Results and discussion</h2>
<h2>Prediction and uncertainty quantification</h2>
<p>We investigated the effectiveness of the proposed method for predicting the chemical reaction yields on the BuchwaldHartwig and Suzuki-Miyaura datasets. For the proposed method, we derived two ablations by adjusting the hyperparameter $\lambda$ in the objective function $\mathcal{J}$. For the first ablation, the model was trained using only homoscedastic loss by setting $\lambda=0$, which is equivalent to fixing the predictive variance $\sigma$ to 1 . For the second ablation, the model was trained using only heteroscedastic loss by setting $\lambda=1$. For baselines, we considered YieldBERT [6] and YieldBERT-DA [10], which demonstrated superior performance compared to the other methods presented in the literature [3-5]. YieldBERT adapted a pre-trained BERT encoder [11] to predict the chemical reaction yield as a function of the reaction SMILES. YieldBERT-DA is an extension of YieldBERT based on data augmentation, which increases the quantity of the training dataset using SMILES randomization. For YieldBERT-DA, the prediction uncertainty score was computed using the prediction variance obtained from the test-time augmentation, as implemented in [10]. The source codes for YieldBERT and YieldBERT-DA are available online at https://github.com/rxn4chemistry/rxn_yields/, which we used to reproduce the experimental results. Consequently, a total of five methods were compared: YieldBERT, YieldBERT-DA, and the proposed method with $\lambda=0,1$, and 0.1 .</p>
<p>For performance evaluation, we split each dataset into training and test sets. We then trained the prediction</p>
<p>Table 2 Comparison of prediction and uncertainty quantification performance on benchmark datasets</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Training/test split</th>
<th>Measure</th>
<th>YieldBERT</th>
<th>YieldBERT-DA</th>
<th>Proposed</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>$\lambda=0$</td>
<td>$\lambda=1$</td>
<td>$\lambda=0.1$</td>
</tr>
<tr>
<td>Buchwald-Hartwig</td>
<td>70/30</td>
<td>MAE (\%p)</td>
<td>$3.990 \pm 0.153$</td>
<td>$3.090 \pm 0.118$</td>
<td>$3.009 \pm 0.045$</td>
<td>$2.953 \pm 0.058$</td>
<td>$2.920 \pm 0.056$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$6.014 \pm 0.272$</td>
<td>$4.799 \pm 0.261$</td>
<td>$4.509 \pm 0.116$</td>
<td>$4.535 \pm 0.136$</td>
<td>$4.433 \pm 0.085$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.951 \pm 0.005$</td>
<td>$0.969 \pm 0.004$</td>
<td>$0.973 \pm 0.002$</td>
<td>$0.972 \pm 0.002$</td>
<td>$0.974 \pm 0.001$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>-</td>
<td>$0.439 \pm 0.037$</td>
<td>$0.254 \pm 0.027$</td>
<td>$0.445 \pm 0.020$</td>
<td>$0.421 \pm 0.031$</td>
</tr>
<tr>
<td></td>
<td>50/50</td>
<td>MAE (\%p)</td>
<td>$4.792 \pm 0.124$</td>
<td>$3.744 \pm 0.150$</td>
<td>$3.614 \pm 0.095$</td>
<td>$3.482 \pm 0.107$</td>
<td>$3.497 \pm 0.090$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$7.288 \pm 0.198$</td>
<td>$5.877 \pm 0.348$</td>
<td>$5.484 \pm 0.193$</td>
<td>$5.481 \pm 0.355$</td>
<td>$5.387 \pm 0.202$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.928 \pm 0.004$</td>
<td>$0.953 \pm 0.006$</td>
<td>$0.959 \pm 0.003$</td>
<td>$0.959 \pm 0.005$</td>
<td>$0.961 \pm 0.003$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>-</td>
<td>$0.460 \pm 0.021$</td>
<td>$0.227 \pm 0.021$</td>
<td>$0.419 \pm 0.020$</td>
<td>$0.401 \pm 0.014$</td>
</tr>
<tr>
<td></td>
<td>30/70</td>
<td>MAE (\%p)</td>
<td>$6.075 \pm 0.222$</td>
<td>$4.833 \pm 0.167$</td>
<td>$4.677 \pm 0.174$</td>
<td>$4.463 \pm 0.150$</td>
<td>$4.483 \pm 0.165$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$9.338 \pm 0.424$</td>
<td>$7.822 \pm 0.463$</td>
<td>$7.227 \pm 0.407$</td>
<td>$7.053 \pm 0.439$</td>
<td>$6.970 \pm 0.403$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.882 \pm 0.011$</td>
<td>$0.917 \pm 0.010$</td>
<td>$0.929 \pm 0.008$</td>
<td>$0.933 \pm 0.009$</td>
<td>$0.934 \pm 0.008$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>-</td>
<td>$0.464 \pm 0.020$</td>
<td>$0.229 \pm 0.035$</td>
<td>$0.407 \pm 0.022$</td>
<td>$0.385 \pm 0.029$</td>
</tr>
<tr>
<td></td>
<td>20/80</td>
<td>MAE (\%p)</td>
<td>$6.862 \pm 0.212$</td>
<td>$5.781 \pm 0.252$</td>
<td>$5.605 \pm 0.236$</td>
<td>$5.319 \pm 0.179$</td>
<td>$5.311 \pm 0.154$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$10.306 \pm 0.303$</td>
<td>$9.164 \pm 0.668$</td>
<td>$8.567 \pm 0.472$</td>
<td>$8.357 \pm 0.400$</td>
<td>$8.204 \pm 0.372$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.857 \pm 0.008$</td>
<td>$0.886 \pm 0.017$</td>
<td>$0.901 \pm 0.011$</td>
<td>$0.906 \pm 0.009$</td>
<td>$0.909 \pm 0.008$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>-</td>
<td>$0.457 \pm 0.017$</td>
<td>$0.208 \pm 0.044$</td>
<td>$0.373 \pm 0.040$</td>
<td>$0.343 \pm 0.029$</td>
</tr>
<tr>
<td></td>
<td>10/90</td>
<td>MAE (\%p)</td>
<td>$8.607 \pm 0.387$</td>
<td>$7.705 \pm 0.236$</td>
<td>$7.605 \pm 0.420$</td>
<td>$7.244 \pm 0.229$</td>
<td>$7.196 \pm 0.274$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$12.393 \pm 0.499$</td>
<td>$11.633 \pm 0.293$</td>
<td>$11.468 \pm 0.699$</td>
<td>$11.002 \pm 0.436$</td>
<td>$10.875 \pm 0.448$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.793 \pm 0.016$</td>
<td>$0.818 \pm 0.009$</td>
<td>$0.822 \pm 0.022$</td>
<td>$0.837 \pm 0.013$</td>
<td>$0.841 \pm 0.013$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>-</td>
<td>$0.432 \pm 0.024$</td>
<td>$0.148 \pm 0.036$</td>
<td>$0.384 \pm 0.040$</td>
<td>$0.345 \pm 0.031$</td>
</tr>
<tr>
<td></td>
<td>5/95</td>
<td>MAE (\%p)</td>
<td>$12.117 \pm 0.789$</td>
<td>$9.651 \pm 0.338$</td>
<td>$10.056 \pm 0.501$</td>
<td>$10.609 \pm 1.610$</td>
<td>$9.677 \pm 0.408$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$16.740 \pm 0.950$</td>
<td>$14.073 \pm 0.687$</td>
<td>$14.636 \pm 0.672$</td>
<td>$14.693 \pm 1.467$</td>
<td>$14.041 \pm 0.492$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.622 \pm 0.042$</td>
<td>$0.733 \pm 0.027$</td>
<td>$0.711 \pm 0.026$</td>
<td>$0.707 \pm 0.063$</td>
<td>$0.734 \pm 0.019$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>-</td>
<td>$0.411 \pm 0.024$</td>
<td>$0.002 \pm 0.058$</td>
<td>$0.398 \pm 0.141$</td>
<td>$0.399 \pm 0.058$</td>
</tr>
<tr>
<td></td>
<td>2.5/97.5</td>
<td>MAE (\%p)</td>
<td>$15.979 \pm 0.817$</td>
<td>$12.243 \pm 0.631$</td>
<td>$12.409 \pm 0.558$</td>
<td>$13.508 \pm 2.745$</td>
<td>$11.747 \pm 1.005$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$20.463 \pm 0.623$</td>
<td>$17.151 \pm 0.677$</td>
<td>$17.384 \pm 0.775$</td>
<td>$17.992 \pm 2.530$</td>
<td>$16.586 \pm 1.364$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.436 \pm 0.034$</td>
<td>$0.604 \pm 0.031$</td>
<td>$0.593 \pm 0.037$</td>
<td>$0.556 \pm 0.130$</td>
<td>$0.628 \pm 0.062$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>-</td>
<td>$0.381 \pm 0.038$</td>
<td>$0.016 \pm 0.067$</td>
<td>$0.309 \pm 0.176$</td>
<td>$0.300 \pm 0.075$</td>
</tr>
</tbody>
</table>
<p>Table 2 (continued)</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Training/test split</th>
<th>Measure</th>
<th>YieldBERT</th>
<th>YieldBERT-DA</th>
<th>Proposed</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>$\lambda=0$</td>
<td>$\lambda=1$</td>
<td>$\lambda=0.1$</td>
</tr>
<tr>
<td>Suzuki-Miyaura</td>
<td>70/30</td>
<td>MAE (\%p)</td>
<td>$8.128 \pm 0.344$</td>
<td>$6.598 \pm 0.270$</td>
<td>$6.233 \pm 0.207$</td>
<td>$6.118 \pm 0.212$</td>
<td>$6.116 \pm 0.223$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$12.073 \pm 0.463$</td>
<td>$10.524 \pm 0.482$</td>
<td>$9.522 \pm 0.454$</td>
<td>$9.495 \pm 0.430$</td>
<td>$9.467 \pm 0.459$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.815 \pm 0.013$</td>
<td>$0.859 \pm 0.012$</td>
<td>$0.885 \pm 0.010$</td>
<td>$0.885 \pm 0.009$</td>
<td>$0.886 \pm 0.010$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>$-$</td>
<td>$0.439 \pm 0.018$</td>
<td>$0.324 \pm 0.026$</td>
<td>$0.432 \pm 0.024$</td>
<td>$0.425 \pm 0.026$</td>
</tr>
<tr>
<td></td>
<td>50/50</td>
<td>MAE (\%p)</td>
<td>$8.922 \pm 0.235$</td>
<td>$7.539 \pm 0.153$</td>
<td>$6.872 \pm 0.089$</td>
<td>$6.702 \pm 0.082$</td>
<td>$6.725 \pm 0.089$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$13.148 \pm 0.270$</td>
<td>$11.797 \pm 0.250$</td>
<td>$10.272 \pm 0.138$</td>
<td>$10.225 \pm 0.128$</td>
<td>$10.225 \pm 0.135$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.780 \pm 0.009$</td>
<td>$0.823 \pm 0.007$</td>
<td>$0.866 \pm 0.003$</td>
<td>$0.867 \pm 0.003$</td>
<td>$0.867 \pm 0.003$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>$-$</td>
<td>$0.439 \pm 0.019$</td>
<td>$0.322 \pm 0.021$</td>
<td>$0.432 \pm 0.017$</td>
<td>$0.430 \pm 0.012$</td>
</tr>
<tr>
<td></td>
<td>30/70</td>
<td>MAE (\%p)</td>
<td>$10.094 \pm 0.346$</td>
<td>$8.804 \pm 0.249$</td>
<td>$8.021 \pm 0.094$</td>
<td>$7.740 \pm 0.109$</td>
<td>$7.847 \pm 0.094$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$14.614 \pm 0.381$</td>
<td>$13.337 \pm 0.357$</td>
<td>$11.726 \pm 0.152$</td>
<td>$11.526 \pm 0.166$</td>
<td>$11.593 \pm 0.136$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.729 \pm 0.014$</td>
<td>$0.774 \pm 0.012$</td>
<td>$0.825 \pm 0.004$</td>
<td>$0.831 \pm 0.005$</td>
<td>$0.829 \pm 0.004$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>$-$</td>
<td>$0.432 \pm 0.018$</td>
<td>$0.292 \pm 0.012$</td>
<td>$0.428 \pm 0.013$</td>
<td>$0.417 \pm 0.008$</td>
</tr>
<tr>
<td></td>
<td>20/80</td>
<td>MAE (\%p)</td>
<td>$11.229 \pm 0.247$</td>
<td>$10.017 \pm 0.338$</td>
<td>$9.147 \pm 0.185$</td>
<td>$8.726 \pm 0.172$</td>
<td>$8.793 \pm 0.191$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$15.966 \pm 0.381$</td>
<td>$14.851 \pm 0.576$</td>
<td>$13.115 \pm 0.298$</td>
<td>$12.754 \pm 0.316$</td>
<td>$12.734 \pm 0.347$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.676 \pm 0.015$</td>
<td>$0.719 \pm 0.022$</td>
<td>$0.781 \pm 0.010$</td>
<td>$0.793 \pm 0.010$</td>
<td>$0.794 \pm 0.011$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>$-$</td>
<td>$0.432 \pm 0.014$</td>
<td>$0.274 \pm 0.020$</td>
<td>$0.429 \pm 0.017$</td>
<td>$0.408 \pm 0.018$</td>
</tr>
<tr>
<td></td>
<td>10/90</td>
<td>MAE (\%p)</td>
<td>$13.528 \pm 0.395$</td>
<td>$11.954 \pm 0.443$</td>
<td>$11.439 \pm 0.185$</td>
<td>$10.625 \pm 0.249$</td>
<td>$10.739 \pm 0.211$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$18.734 \pm 0.530$</td>
<td>$17.129 \pm 0.683$</td>
<td>$15.967 \pm 0.326$</td>
<td>$15.097 \pm 0.421$</td>
<td>$15.164 \pm 0.344$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.554 \pm 0.025$</td>
<td>$0.627 \pm 0.030$</td>
<td>$0.676 \pm 0.013$</td>
<td>$0.711 \pm 0.016$</td>
<td>$0.708 \pm 0.013$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>$-$</td>
<td>$0.389 \pm 0.022$</td>
<td>$0.221 \pm 0.027$</td>
<td>$0.390 \pm 0.019$</td>
<td>$0.382 \pm 0.019$</td>
</tr>
<tr>
<td></td>
<td>5/95</td>
<td>MAE (\%p)</td>
<td>$15.695 \pm 0.618$</td>
<td>$14.294 \pm 0.507$</td>
<td>$14.214 \pm 0.504$</td>
<td>$13.364 \pm 0.223$</td>
<td>$13.451 \pm 0.353$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$21.181 \pm 0.724$</td>
<td>$20.016 \pm 0.661$</td>
<td>$19.421 \pm 0.588$</td>
<td>$18.463 \pm 0.308$</td>
<td>$18.511 \pm 0.392$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.430 \pm 0.040$</td>
<td>$0.491 \pm 0.034$</td>
<td>$0.521 \pm 0.029$</td>
<td>$0.567 \pm 0.014$</td>
<td>$0.565 \pm 0.018$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>$-$</td>
<td>$0.355 \pm 0.026$</td>
<td>$0.144 \pm 0.052$</td>
<td>$0.389 \pm 0.045$</td>
<td>$0.330 \pm 0.034$</td>
</tr>
<tr>
<td></td>
<td>2.5/97.5</td>
<td>MAE (\%p)</td>
<td>$17.666 \pm 0.496$</td>
<td>$17.587 \pm 0.690$</td>
<td>$18.061 \pm 0.571$</td>
<td>$16.705 \pm 1.090$</td>
<td>$17.189 \pm 0.813$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RMSE (\%p)</td>
<td>$22.967 \pm 0.804$</td>
<td>$23.780 \pm 0.793$</td>
<td>$24.121 \pm 0.655$</td>
<td>$22.156 \pm 1.273$</td>
<td>$22.943 \pm 0.887$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.330 \pm 0.047$</td>
<td>$0.282 \pm 0.047$</td>
<td>$0.261 \pm 0.039$</td>
<td>$0.375 \pm 0.072$</td>
<td>$0.331 \pm 0.051$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Spearman $\rho$</td>
<td>$-$</td>
<td>$0.291 \pm 0.025$</td>
<td>$0.028 \pm 0.054$</td>
<td>$0.280 \pm 0.074$</td>
<td>$0.223 \pm 0.081$</td>
</tr>
</tbody>
</table>
<p>model using the training set and evaluated its performance on the test set. To examine the effects of training set size on performance, the training/test splits were varied as $70 / 30,50 / 50,30 / 70,20 / 80,10 / 90,5 / 95$, and 2.5/97.5. Regarding prediction performance, we used the following three measures calculated on the test set: mean absolute error (MAE), root mean squared error (RMSE), and coefficient of determination $\left(\mathrm{R}^{2}\right)$. Uncertainty quantification performance was evaluated in terms of the Spearman rank correlation coefficient $\rho$ between the absolute prediction error and uncertainty score on the test set $[10,18]$.</p>
<p>Table 2 reports the average and standard deviation of the results over the 10 repetitions. In terms of prediction performance, the proposed method outperformed all the baseline methods. Although YieldBERT-DA was the best baseline method, the MAE and RMSE values of the proposed method reduced by around $5 \sim 10 \%$ compared to those of YieldBERT-DA on both benchmark datasets. The higher prediction performance indicates that the proposed method can provide more accurate predictions of yields for new reactions. Regarding uncertainty quantification performance, the proposed method yielded a Spearman $\rho$ comparable to that of YieldBERT-DA.</p>
<p>For the proposed method, the prediction performance with $\lambda=1$ was slightly better than that with $\lambda=0$. The uncertainty quantification performance with $\lambda=1$ was far better than that with $\lambda=0$, which implies that capturing the aleatoric uncertainty is beneficial. Compared to the ablations, setting $\lambda=0.1$ yielded a better trade-off between prediction performance and uncertainty quantification performance. The results demonstrated that the use of</p>
<p>both homoscedastic and heteroscedastic losses helped to improve performance.</p>
<h2>Out-of-sample prediction</h2>
<p>We also evaluated the performance of the proposed method for out-of-sample prediction. As in [6, 10], we used four out-of-sample training/test splits of the Buch-wald-Hartwig dataset, which we denote by Test 1, Test 2, Test 3, and Test 4. In each split, certain additives are absent from the training set but only appear in the test set. The proposed method was compared with YieldBERT and YieldBERT-DA. The training configurations and evaluation scheme were the same as before. The experiments were repeated five times independently using different random seeds.</p>
<p>Table 3 summarizes the results averaged over the five repetitions. Overall, the proposed method was comparable to the best of the baseline methods for out-of-sample prediction. In terms of prediction performance, the proposed method performed best on Test 2 and Test 4, while was comparable or inferior to the best baseline on Test 1 and Test 3. Among the baselines, YieldBERT-DA yielded a lower performance than YieldBERT on average. For uncertainty quantification performance, the proposed method yielded the highest Spearman $\rho$ for Test 1, Test 3, and Test 4.</p>
<h2>Selective prediction with rejection</h2>
<p>We investigated the effectiveness of the proposed method for selective prediction using 70/30 splits of benchmark datasets. For the proposed method, prediction uncertainty was quantified using the total predictive variance in Eq. 9.</p>
<p>Because it can be decomposed into aleatoric and epistemic uncertainties, we conducted an ablation study to examine the effects of each component. The first ablation quantified the prediction uncertainty using the aleatoric uncertainty term. The second ablation used the epistemic uncertainty term. The proposed method was compared to the best baseline method, YieldBERT-DA, for which the uncertainty quantification was based on the test-time augmentation.</p>
<p>To evaluate the selective prediction performance, we rejected the prediction for a reaction if its uncertainty score was above a certain threshold. The threshold controls the trade-off between prediction accuracy and coverage. As performance measures, we computed the MAE and RMSE on the test set with various prediction coverage rates ranging from $100 \%$ to $30 \%$.</p>
<p>Tables 4 and 5 present the comparison results for the selective prediction performance in terms of the MAE and RMSE with various prediction coverage rates, which are summarized in Fig. 3. The results clearly demonstrated that a high uncertainty score for a reaction causes its predicted yield to be less accurate for all compared methods. Reducing the prediction coverage with more rejections led to a significant improvement in the prediction performance. The proposed method outperformed YieldBERTDA in most cases. The MAE and RMSE decreased by over $10 \%$ and were nearly halved at $90 \%$ and $40 \%$ coverages, respectively, for both datasets.</p>
<p>Regarding the two ablations of the proposed method, the selective prediction performance with the epistemic uncertainty was superior at higher prediction coverages, whereas that with the aleatoric uncertainty was better at lower coverages. Compared to the ablations, using the</p>
<p>Table 3 Comparison of prediction and uncertainty quantification performance on out-of-sample splits of Buchwald-Hartwig dataset</p>
<table>
<thead>
<tr>
<th>Out-of-sample split</th>
<th>Measure</th>
<th>YieldBERT</th>
<th>YieldBERT-DA</th>
<th>Proposed $(\lambda=0.1)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Test 1</td>
<td>MAE $(\% \rho)$</td>
<td>$7.351 \pm 0.099$</td>
<td>$7.015 \pm 0.758$</td>
<td>$8.082 \pm 0.827$</td>
</tr>
<tr>
<td></td>
<td>RMSE $(\% \rho)$</td>
<td>$11.441 \pm 0.342$</td>
<td>$11.761 \pm 1.398$</td>
<td>$13.746 \pm 1.175$</td>
</tr>
<tr>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.824 \pm 0.010$</td>
<td>$0.811 \pm 0.047$</td>
<td>$0.744 \pm 0.042$</td>
</tr>
<tr>
<td></td>
<td>Spearman $\rho$</td>
<td>-</td>
<td>$0.380 \pm 0.065$</td>
<td>$0.454 \pm 0.046$</td>
</tr>
<tr>
<td>Test 2</td>
<td>MAE $(\% \rho)$</td>
<td>$7.266 \pm 0.724$</td>
<td>$6.588 \pm 0.328$</td>
<td>$6.300 \pm 0.647$</td>
</tr>
<tr>
<td></td>
<td>RMSE $(\% \rho)$</td>
<td>$11.144 \pm 1.267$</td>
<td>$9.886 \pm 0.741$</td>
<td>$9.476 \pm 1.027$</td>
</tr>
<tr>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.829 \pm 0.037$</td>
<td>$0.866 \pm 0.020$</td>
<td>$0.876 \pm 0.026$</td>
</tr>
<tr>
<td></td>
<td>Spearman $\rho$</td>
<td>-</td>
<td>$0.494 \pm 0.044$</td>
<td>$0.397 \pm 0.043$</td>
</tr>
<tr>
<td>Test 3</td>
<td>MAE $(\% \rho)$</td>
<td>$9.129 \pm 0.745$</td>
<td>$11.052 \pm 0.950$</td>
<td>$8.986 \pm 0.314$</td>
</tr>
<tr>
<td></td>
<td>RMSE $(\% \rho)$</td>
<td>$14.276 \pm 0.820$</td>
<td>$18.041 \pm 1.395$</td>
<td>$14.939 \pm 0.622$</td>
</tr>
<tr>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.741 \pm 0.030$</td>
<td>$0.585 \pm 0.067$</td>
<td>$0.717 \pm 0.024$</td>
</tr>
<tr>
<td></td>
<td>Spearman $\rho$</td>
<td>-</td>
<td>$0.406 \pm 0.065$</td>
<td>$0.423 \pm 0.031$</td>
</tr>
<tr>
<td>Test 4</td>
<td>MAE $(\% \rho)$</td>
<td>$13.671 \pm 1.067$</td>
<td>$18.422 \pm 0.620$</td>
<td>$13.190 \pm 0.754$</td>
</tr>
<tr>
<td></td>
<td>RMSE $(\% \rho)$</td>
<td>$19.679 \pm 1.397$</td>
<td>$24.279 \pm 0.494$</td>
<td>$18.774 \pm 0.566$</td>
</tr>
<tr>
<td></td>
<td>$\mathrm{R}^{2}$</td>
<td>$0.444 \pm 0.077$</td>
<td>$0.157 \pm 0.034$</td>
<td>$0.496 \pm 0.031$</td>
</tr>
<tr>
<td></td>
<td>Spearman $\rho$</td>
<td>-</td>
<td>$0.366 \pm 0.100$</td>
<td>$0.461 \pm 0.040$</td>
</tr>
</tbody>
</table>
<p>Table 4 Comparison of selective prediction performance in terms of MAE (\%p)</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Coverage</th>
<th>YieldBERT-DA</th>
<th>Proposed $(\lambda=0.1)$</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>Aleatoric</td>
<td>Epistemic</td>
<td>Total Pred. Var.</td>
</tr>
<tr>
<td>Buchwald-Hartwig</td>
<td>100\%</td>
<td>$3.090 \pm 0.118$</td>
<td>$2.920 \pm 0.056$</td>
<td>$2.920 \pm 0.056$</td>
<td>$2.920 \pm 0.056$</td>
</tr>
<tr>
<td></td>
<td>90\%</td>
<td>$2.733 \pm 0.099$</td>
<td>$2.684 \pm 0.050$</td>
<td>$2.669 \pm 0.056$</td>
<td>$2.683 \pm 0.061$</td>
</tr>
<tr>
<td></td>
<td>80\%</td>
<td>$2.534 \pm 0.082$</td>
<td>$2.518 \pm 0.064$</td>
<td>$2.514 \pm 0.063$</td>
<td>$2.505 \pm 0.065$</td>
</tr>
<tr>
<td></td>
<td>70\%</td>
<td>$2.357 \pm 0.092$</td>
<td>$2.302 \pm 0.067$</td>
<td>$2.292 \pm 0.067$</td>
<td>$2.293 \pm 0.064$</td>
</tr>
<tr>
<td></td>
<td>60\%</td>
<td>$2.191 \pm 0.103$</td>
<td>$2.056 \pm 0.099$</td>
<td>$2.070 \pm 0.064$</td>
<td>$2.041 \pm 0.069$</td>
</tr>
<tr>
<td></td>
<td>50\%</td>
<td>$2.020 \pm 0.105$</td>
<td>$1.820 \pm 0.093$</td>
<td>$1.847 \pm 0.075$</td>
<td>$1.803 \pm 0.061$</td>
</tr>
<tr>
<td></td>
<td>40\%</td>
<td>$1.824 \pm 0.106$</td>
<td>$1.593 \pm 0.086$</td>
<td>$1.672 \pm 0.081$</td>
<td>$1.582 \pm 0.077$</td>
</tr>
<tr>
<td></td>
<td>30\%</td>
<td>$1.560 \pm 0.098$</td>
<td>$1.368 \pm 0.112$</td>
<td>$1.509 \pm 0.115$</td>
<td>$1.372 \pm 0.111$</td>
</tr>
<tr>
<td>Suzuki-Miyaura</td>
<td>100\%</td>
<td>$6.598 \pm 0.270$</td>
<td>$6.116 \pm 0.223$</td>
<td>$6.116 \pm 0.223$</td>
<td>$6.116 \pm 0.223$</td>
</tr>
<tr>
<td></td>
<td>90\%</td>
<td>$5.902 \pm 0.247$</td>
<td>$5.589 \pm 0.178$</td>
<td>$5.575 \pm 0.191$</td>
<td>$5.542 \pm 0.178$</td>
</tr>
<tr>
<td></td>
<td>80\%</td>
<td>$5.415 \pm 0.242$</td>
<td>$5.298 \pm 0.174$</td>
<td>$5.269 \pm 0.210$</td>
<td>$5.219 \pm 0.192$</td>
</tr>
<tr>
<td></td>
<td>70\%</td>
<td>$5.052 \pm 0.211$</td>
<td>$5.018 \pm 0.196$</td>
<td>$4.966 \pm 0.183$</td>
<td>$4.939 \pm 0.208$</td>
</tr>
<tr>
<td></td>
<td>60\%</td>
<td>$4.690 \pm 0.181$</td>
<td>$4.641 \pm 0.218$</td>
<td>$4.579 \pm 0.140$</td>
<td>$4.570 \pm 0.188$</td>
</tr>
<tr>
<td></td>
<td>50\%</td>
<td>$4.213 \pm 0.214$</td>
<td>$4.025 \pm 0.252$</td>
<td>$4.064 \pm 0.179$</td>
<td>$3.989 \pm 0.203$</td>
</tr>
<tr>
<td></td>
<td>40\%</td>
<td>$3.921 \pm 0.188$</td>
<td>$3.245 \pm 0.140$</td>
<td>$3.372 \pm 0.111$</td>
<td>$3.195 \pm 0.145$</td>
</tr>
<tr>
<td></td>
<td>30\%</td>
<td>$3.549 \pm 0.120$</td>
<td>$2.510 \pm 0.093$</td>
<td>$2.701 \pm 0.118$</td>
<td>$2.514 \pm 0.115$</td>
</tr>
</tbody>
</table>
<p>total predictive variance combining the aleatoric and epistemic uncertainty improved the performance by taking their individual strengths to detect erroneous predictions.</p>
<h2>Conclusion</h2>
<p>We presented an uncertainty-aware method for predicting chemical reaction yields. We represented a chemical reaction as a set of graphs. We constructed a prediction model whose input was the graphs and output was the predictive mean and variance for the reaction yield. For a query reaction, the predictive mean of the model was used as the predicted yield and the predictive variance was used to quantify the uncertainty of the prediction, which allowed the model to avoid making predictions with high uncertainty. The effectiveness of the proposed method for chemical reaction yield prediction was successfully demonstrated through experimental validation on two benchmark datasets. We also demonstrated that a high</p>
<p>Table 5 Comparison of selective prediction performance in terms of RMSE (\%p)</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Coverage</th>
<th>YieldBERT-DA</th>
<th>Proposed $(\lambda=0.1)$</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>Aleatoric</td>
<td>Epistemic</td>
<td>Total Pred. Var.</td>
</tr>
<tr>
<td>Buchwald-Hartwig</td>
<td>100\%</td>
<td>$4.799 \pm 0.261$</td>
<td>$4.433 \pm 0.085$</td>
<td>$4.433 \pm 0.085$</td>
<td>$4.433 \pm 0.085$</td>
</tr>
<tr>
<td></td>
<td>90\%</td>
<td>$4.129 \pm 0.205$</td>
<td>$4.036 \pm 0.130$</td>
<td>$4.003 \pm 0.160$</td>
<td>$4.037 \pm 0.161$</td>
</tr>
<tr>
<td></td>
<td>80\%</td>
<td>$3.833 \pm 0.206$</td>
<td>$3.796 \pm 0.173$</td>
<td>$3.793 \pm 0.182$</td>
<td>$3.765 \pm 0.185$</td>
</tr>
<tr>
<td></td>
<td>70\%</td>
<td>$3.583 \pm 0.249$</td>
<td>$3.482 \pm 0.176$</td>
<td>$3.424 \pm 0.196$</td>
<td>$3.456 \pm 0.166$</td>
</tr>
<tr>
<td></td>
<td>60\%</td>
<td>$3.382 \pm 0.282$</td>
<td>$3.050 \pm 0.261$</td>
<td>$3.068 \pm 0.211$</td>
<td>$3.001 \pm 0.184$</td>
</tr>
<tr>
<td></td>
<td>50\%</td>
<td>$3.171 \pm 0.317$</td>
<td>$2.653 \pm 0.187$</td>
<td>$2.716 \pm 0.168$</td>
<td>$2.605 \pm 0.115$</td>
</tr>
<tr>
<td></td>
<td>40\%</td>
<td>$2.812 \pm 0.218$</td>
<td>$2.338 \pm 0.178$</td>
<td>$2.503 \pm 0.197$</td>
<td>$2.300 \pm 0.166$</td>
</tr>
<tr>
<td></td>
<td>30\%</td>
<td>$2.518 \pm 0.229$</td>
<td>$2.059 \pm 0.245$</td>
<td>$2.299 \pm 0.270$</td>
<td>$2.044 \pm 0.235$</td>
</tr>
<tr>
<td>Suzuki-Miyaura</td>
<td>100\%</td>
<td>$10.524 \pm 0.482$</td>
<td>$9.467 \pm 0.459$</td>
<td>$9.467 \pm 0.459$</td>
<td>$9.467 \pm 0.459$</td>
</tr>
<tr>
<td></td>
<td>90\%</td>
<td>$9.485 \pm 0.395$</td>
<td>$8.632 \pm 0.334$</td>
<td>$8.592 \pm 0.338$</td>
<td>$8.540 \pm 0.310$</td>
</tr>
<tr>
<td></td>
<td>80\%</td>
<td>$8.911 \pm 0.373$</td>
<td>$8.254 \pm 0.314$</td>
<td>$8.146 \pm 0.403$</td>
<td>$8.098 \pm 0.347$</td>
</tr>
<tr>
<td></td>
<td>70\%</td>
<td>$8.473 \pm 0.323$</td>
<td>$7.848 \pm 0.329$</td>
<td>$7.787 \pm 0.305$</td>
<td>$7.702 \pm 0.397$</td>
</tr>
<tr>
<td></td>
<td>60\%</td>
<td>$8.063 \pm 0.353$</td>
<td>$7.260 \pm 0.400$</td>
<td>$7.218 \pm 0.343$</td>
<td>$7.160 \pm 0.328$</td>
</tr>
<tr>
<td></td>
<td>50\%</td>
<td>$7.439 \pm 0.470$</td>
<td>$6.357 \pm 0.470$</td>
<td>$6.503 \pm 0.456$</td>
<td>$6.293 \pm 0.466$</td>
</tr>
<tr>
<td></td>
<td>40\%</td>
<td>$7.236 \pm 0.521$</td>
<td>$5.126 \pm 0.306$</td>
<td>$5.394 \pm 0.306$</td>
<td>$4.980 \pm 0.250$</td>
</tr>
<tr>
<td></td>
<td>30\%</td>
<td>$6.754 \pm 0.398$</td>
<td>$3.968 \pm 0.152$</td>
<td>$4.337 \pm 0.257$</td>
<td>$3.959 \pm 0.252$</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>predictive variance tends to cause a high prediction error, allowing for selective prediction with rejection.</p>
<p>The accurate prediction of chemical reaction yields with uncertainty quantification can assist in advanced synthesis planning considering imposed constraints in practice, including availability, variability, and budget limits. Future research directions for improving prediction performance will be to enrich the data representation of chemical reactions to make it more informative by incorporating various atom/bond features and molecular descriptors associated with reaction yields.</p>
<h3>Acknowledgements</h3>
<p>The authors thank the anonymous reviewers for their valuable comments.</p>
<h3>Authors' contributions</h3>
<p>YK and SK designed and implemented the methodology. DL performed the analysis. YSC and SK supervised the research. YK and SK wrote the manuscript. All authors read and approved the final manuscript.</p>
<h3>Funding</h3>
<p>This work was supported by Samsung Advanced Institute of Technology and the National Research Foundation of Korea (NRF) Grant funded by the Korea government (MSIT; Ministry of Science and ICT) (No. NRF-2020R1C1C1003232).</p>
<h3>Availability of data and materials</h3>
<p>The source code used in this study is available online at http://github.com/seokhokang/reaction_yield_nn/. The benchmark datasets are publicly accessible from https://github.com/nxn4chemistry/nrn_yields/.</p>
<h2>Declarations</h2>
<h3>Competing interests</h3>
<p>The authors declare that they have no competing interests.</p>
<h3>Author details</h3>
<p>^{1}Samsung Advanced Institute of Technology, Samsung Electronics Co. Ltd., 130 Samsung-ro, Yeongtong-gu, Suwon, Republic of Korea. ^{2}Department of Computer Science and Engineering, Seoul National University, 1 Gwanak-ro, Gwanak-gu, Seoul, Republic of Korea. ^{3}Department of Industrial Engineering, Sungkyunkwan University, 2066 Seobu-ro, Jangan-gu, Suwon, Republic of Korea.</p>
<h1>References</h1>
<ol>
<li>Meuwly M (2021) Machine learning for chemical reactions. Chem Rev</li>
<li>Davies IW (2019) The digitization of organic synthesis. Nature 570(7760):175-181</li>
<li>Ahneman DT, Estrada JG, Lin S, Dreher SD, Doyle AG (2018) Predicting reaction performance in C-N cross-coupling using machine learning. Science 360(6385):186-190</li>
<li>Chuang KV, Keiser MJ (2018) Comment on "Predicting reaction performance in C-N cross-coupling using machine learning". Science. 362(6416)</li>
<li>Sandfort F, Strieth-Kalthoff F, Kühnemund M, Beeckx C, Glorius F (2020) A structure-based platform for predicting chemical reactivity. Chem 6(6):1379-1390</li>
<li>Schwaller P, Vaucher AC, Laino T, Reymond JL (2021) Prediction of chemical reaction yields using deep learning. Machine Learning: Sci Technol. 2(1):015016</li>
<li>Saebi M, Nan B, Herr J, Wahlers J, Wiest O, Chawla N (2021) Graph neural networks for predicting chemical reaction performance. ChemRxiv</li>
<li>Schneider N, Lowe DM, Sayle RA, Landrum GA (2015) Development of a novel fingerprint for chemical reactions and its application to large-scale reaction classification and similarity. J Chem Inf Model 55(1):39-53</li>
<li>LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436-444</li>
<li>Schwaller P, Vaucher AC, Laino T, Reymond JL (2020) Data augmentation strategies to improve reaction yield predictions and estimate uncertainty. In: Proceedings of NeurIPS 2020 Machine Learning for Molecules Workshop</li>
<li>Schwaller P, Probst D, Vaucher AC, Nair VH, Kreutter D, Laino T et al (2021) Mapping the space of chemical reactions using attention-based neural networks. Nat Machine Intell 3(2):144-152</li>
<li>Gilmer J, Schoenholz SS, Riley PF, Vinyals O, Dahl GE (2017) Neural message passing for quantum chemistry. In: Proceedings of International Conference on Machine Learning; p. 1263-1272</li>
<li>Vinyals O, Bengio S, Kudlur M (2015) Order matters: sequence to sequence for sets. In: Proceedings of International Conference on Learning Representations</li>
<li>Gal Y, Ghahramani Z (2016) Dropout as a Bayesian approximation: representing model uncertainty in deep learning. In: Proceedings of International Conference on Machine Learning; p. 1050-1059</li>
<li>Kendall A, Gal Y (2017) What uncertainties do we need in Bayesian deep learning for computer vision? Adv Neural Inf Process Syst 30:5574-5584</li>
<li>Perera D, Tucker JW, Brahmbhatt S, Helal CJ, Chong A, Farrell W et al (2018) A platform for automated nanomole-scale reaction screening and micromolescale synthesis in flow. Science 359(6374):429-434</li>
<li>Granda JM, Donina L, Dragone V, Long DL, Cronin L (2018) Controlling an organic Synthesis robot with machine learning to search for new reactivity. Nature 559(7714):377-381</li>
<li>Hirschfeld L, Swanson K, Yang K, Barzilay R, Coley CW (2020) Uncertainty quantification using neural networks for molecular property prediction. J Chem Inf Model 60(8):3770-3780</li>
</ol>
<h2>Publisher's Note</h2>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
<h2>Ready to submit your research? Choose BMC and benefit from:</h2>
<ul>
<li>fast, convenient online submission</li>
<li>thorough peer review by experienced researchers in your field</li>
<li>rapid publication on acceptance</li>
<li>support for research data, including large and complex data types</li>
<li>gold Open Access which fosters wider collaboration and increased citations</li>
<li>maximum visibility for your research: over 100M website views per year</li>
</ul>
<p>At BMC, research is always in progress.
Learn more biomedcentral.com/submissions</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Correspondence: ysuk.choi@samsung.com; s.kang@skku.edu
${ }^{1}$ Samsung Advanced Institute of Technology, Samsung Electronics Co. Ltd., 130 Samsung-ro, Yeongtong-gu, Suwon, Republic of Korea
${ }^{2}$ Department of Industrial Engineering, Sungkyunkwan University, 2066 Seobu-ro, Jangan-gu, Suwon, Republic of Korea
Full list of author information is available at the end of the article&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>(c) The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
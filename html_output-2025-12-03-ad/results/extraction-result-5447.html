<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5447 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5447</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5447</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-1e6102c981b9464c632ef0b00dbd11dfb0564e4e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1e6102c981b9464c632ef0b00dbd11dfb0564e4e" target="_blank">SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work proposes SelfCheck, a general-purpose zero-shot verification schema for recognizing errors in large language models and uses the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question.</p>
                <p><strong>Paper Abstract:</strong> The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on three datasets (GSM8K, MathQA, and MATH) and find that it successfully recognizes errors and, in turn, increases final answer accuracies.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5447.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5447.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheck (GPT-3.5 → GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheck (regenerate-and-compare checker) using GPT-3.5 as both generator and checker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SelfCheck is a zero-shot, step-by-step verify-then-weight pipeline that decomposes checking into target-extraction, information-collection, step-regeneration and result-comparison; here both generation and checking calls use OpenAI GPT-3.5 (gpt-3.5-0301).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI GPT-3.5 (gpt-3.5-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI API model referred to as GPT-3.5 (gpt-3.5-0301) used as both generator and checker in experiments; exact model size/architecture details are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SelfCheck (regenerate-and-compare)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For each step in a chain-of-thought solution the LLM (checker) extracts the step's target, collects the minimal contextual information, regenerates an independent alternative step from that context, and compares the regenerated step to the original; per-solution step results (-1/0/1) are integrated into a soft confidence weight used for weighted voting across multiple candidate solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MathQA, MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Three math-reasoning datasets: GSM8K (grade-school arithmetic word problems), MathQA (multi-step math with multiple-choice options), and MATH (competition-level math problems).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>GSM8K: 74.3% final answer accuracy (SelfCheck weighted voting); MathQA: 64.6%; MATH (subset): 38.0%. (These are final-answer accuracies when using 2 candidate solutions per question, checker and generator both GPT-3.5, from Table 1.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Majority voting (no SelfCheck weights) with same generator: GSM8K 71.7%; MathQA 59.2%; MATH 35.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: SelfCheck yields consistent improvements over majority voting for GPT-3.5 generator/checker: +2.8±0.9% on GSM8K, +5.4±1.1% on MathQA, +2.2±0.7% on MATH (Table 1). Filtering by SelfCheck confidence also reduces the proportion of incorrect solutions (reported reductions of ≈9%, 22.8%, 16.2% for the three datasets when low-confidence solutions are removed) and raises precision-like measures (e.g. fraction of predicted-correct solutions that are actually correct increases with higher threshold). ROC/verification curves show meaningful true/false positive control in contrast to naive global checking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Step checks are imperfect and can misclassify correct steps; filtering trades recall for precision (correct solutions can be rejected when thresholding). The method was evaluated only on math benchmarks in the paper. Using the same model for generation and checking can leave correlated errors; decorrelation (different checker model) sometimes helps. SelfCheck's improvements depend on having multiple candidate solutions (it functions via weighted voting over candidates).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5447.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5447.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheck (GPT-4 → GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheck using GPT-4 as both generator and checker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same SelfCheck pipeline (regenerate-and-compare) evaluated with GPT-4 (gpt-4-0613) as both the answer generator and the checker on sampled subsets of the datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 API model used as generator and/or checker in experiments; exact parameter counts are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SelfCheck (regenerate-and-compare)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same per-step extraction → information collection → regeneration → comparison pipeline, using GPT-4 for both the generation and the checking calls.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (sample), MathQA (sample), MATH (sample)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Sampled subsets (500 questions each) from the three math datasets used for higher-cost GPT-4 experiments; final-answer correctness measured with 2 candidate solutions per question.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>GSM8K: SelfCheck 86.9% (vs. majority voting 87.1%); MathQA: SelfCheck 80.9% (majority 78.3%); MATH (subset): SelfCheck 51.3% (majority 47.9%). (Table 1; values based on GPT-4 generator and GPT-4 checker on sampled sets.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Majority voting (no SelfCheck weights) with GPT-4 generator: GSM8K 87.1%; MathQA 78.3%; MATH 47.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: SelfCheck improves MathQA (+2.6% ±0.4) and MATH (+3.4% ±0.6) final accuracies when both generator and checker are GPT-4; for GSM8K the difference is negligible/slightly negative (-0.2% ±0.2). The paper argues that for harder tasks (MATH) GPT-4 as checker helps more.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>On some simpler datasets (GSM8K) checking with the same powerful model (GPT-4) produced no improvement or a tiny negative change, suggesting correlated errors between generator and checker; computational cost of using GPT-4 for checking is high (so paper explores cheaper checker alternatives).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5447.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5447.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheck (GPT-4 generator checked by GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheck with GPT-4 as generator and GPT-3.5 as checker (cross-model checking)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cross-model variant where GPT-4 generates candidate solutions and the cheaper GPT-3.5 model performs the SelfCheck stepwise verification, intended to decorrelate generator and checker errors and reduce cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generator: GPT-4 (gpt-4-0613); Checker: GPT-3.5 (gpt-3.5-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generator is GPT-4 (gpt-4-0613) and checker is GPT-3.5 (gpt-3.5-0301); both through OpenAI API, checker is cheaper so used to decorrelate and reduce cost.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SelfCheck (cross-model regenerate-and-compare)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Identical SelfCheck pipeline executed with generator outputs from GPT-4 and independent checking/regeneration/comparison calls to GPT-3.5 (checker does not see the original step when regenerating). Confidence weights from GPT-3.5 used for weighted voting among GPT-4-generated candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (sample), MathQA (sample), MATH (sample)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same sampled subsets used in GPT-4 experiments; 2 candidate solutions per question in reported numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>GSM8K: SelfCheck 88.1% (vs MV 87.1%); MathQA: 81.2% (vs MV 78.3%); MATH: 48.9% (vs MV 47.9%). (Table 1; cross-model checking gave improved final accuracy on GSM8K and MathQA compared to same-model checking.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Majority voting with GPT-4 generator: GSM8K 87.1%; MathQA 78.3%; MATH 47.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: Cross-model checking (GPT-3.5 verifying GPT-4 outputs) yielded modest additional gains over same-model GPT-4 checking on some tasks (e.g., +1.0% GSM8K, +3.0% MathQA in reported samples), supporting the claim that decorrelating checker and generator errors helps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cross-model gains vary by dataset: for the hardest dataset (MATH) GPT-4-as-checker performed better than GPT-3.5, suggesting cheaper checkers are not uniformly sufficient. The paper notes the benefit likely comes from error decorrelation but does not fully characterize when cheaper checkers suffice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5447.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5447.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Global checking (direct LLM verification)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Global direct-checking: prompt the LLM to check the whole reasoning chain at once</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline variant where the LLM is directly prompted to check the entire solution (global check) rather than doing stepwise regenerate-and-compare; found to nearly always declare solutions correct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (used in ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 used with a prompt asking it to check a full solution step-by-step and conclude 'Correct', 'Wrong' or 'Not Sure'.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Global direct checking</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Single prompt asking the LLM to 'Carefully check whether the solution is correct step by step' and output a final verdict; no regeneration/comparison decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MathQA (subset, ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Subset of MathQA (100 random questions) used for ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Verification accuracy (average of TPR and TNR) = 55.0% on MathQA ablation (Table 2); final-answer accuracy similar to majority voting (i.e., provided little useful discrimination).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Majority voting baseline final-answer accuracy (used for comparison, see main experiments) -- global checker produced little to no useful separation so final-answer performance was close to majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>No meaningful improvement: global check nearly always labels chains as correct (>90% in prior work and in this paper), so it fails to provide useful confidence for selection/filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Global direct checking is strongly biased toward declaring solutions correct (very high false positive rate) and provides no meaningful control of TP/FP; verification accuracy near random on MathQA in ablation, supporting the paper's claim that direct global checking is ineffective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5447.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5447.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-stage step checking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-stage step checking (directly ask LLM to check one step given context)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simpler ablation that directly prompts the LLM to check the next step given the question and previous steps, without decomposition into target extraction/information collection/regeneration/comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 used to attempt single-stage per-step checking by directly asking 'Check the correctness of the next step' with context.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-stage check</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Directly prompt the LLM to consider the information it relies on and to judge the next step as 'Correct', 'Wrong', or 'Not Sure' in one shot (no regeneration-comparison pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MathQA (subset, ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>100-question subset of MathQA used for ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Verification accuracy = 57.2% (Table 2). Final-answer generation accuracy with this variant was significantly worse than full SelfCheck in ablations (Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Compared to majority voting, single-stage checking yields little improvement and is inferior to the multi-stage SelfCheck scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Slight improvement over naive global check but substantially worse than SelfCheck, showing decomposition into sub-tasks is important.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still too difficult for the LLM: single-stage step checking performs worse than the multi-stage SelfCheck, consistent with the argument that checking is easier when decomposed and when regeneration provides a decorrelated reference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5447.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5447.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Error-check (0-shot / 1-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct error-checking prompt (zero-shot and one-shot variants) that asks the LLM to identify grounding/reasoning/calculation errors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A direct error-checking ablation that replaces SelfCheck's regeneration+compare with a single error-check prompt; tested in zero-shot and with one exemplar (one-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 used with direct error-checking prompts: a zero-shot prompt asking to check for grounding/reasoning/calculation errors and a 1-shot variant using an exemplar from Ling et al. (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Direct error-check (0-shot and 1-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Single-prompt error checking that asks the LLM to classify a step as 'Correct', 'Wrong', or 'Not Sure' and to check for grounding/reasoning/calculation errors; 1-shot version includes an exemplar guiding how to check.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MathQA (subset, ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>100-question subset of MathQA used for ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Verification accuracy: 0-shot = 63.1%; 1-shot = 64.2% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Compared to majority voting final-answer accuracy, these direct error-check methods provide modest verification skill but underperform the full SelfCheck (verification accuracy 66.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Direct error-checking improves over naive global check and single-stage checking, but still performs worse than the regenerate-and-compare pipeline, supporting the paper's claim that LLMs are better at generation (regeneration) than at direct checking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even with a detailed exemplar (1-shot), direct error-checking remains inferior to step regeneration + comparison; direct checkers also risk correlation with generator errors and overconfident 'correct' labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5447.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5447.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Verification (SV) backward-prediction approach (Weng et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior approach that verifies a full chain by using conclusions from chain-of-thought to predict masked conditions in the question (backwards prediction); typically applied as few-shot and supports only single-step checking assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are reasoners with self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in related work; SV is typically implemented by prompting an LLM in few-shot mode to perform a backward prediction verification; paper does not itself run SV experiments but compares to its reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (backward prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use chain-of-thought conclusion to predict masked parts of the question and check consistency; designed primarily for simpler tasks and single-step recoverability.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mentioned in comparison (reported on problems like GSM8K in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used in prior literature for certain math QA tasks (e.g., GSM8K) as a few-shot verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper cites SV as a prior method that requires few-shot exemplars and is limited (only supports single-step checking and assumptions that question info is fully recoverable), and notes SelfCheck outperforms SV in reported settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SV only supports single-step checking and assumes every piece of question information is recoverable from a correct solution, limiting applicability to simpler tasks (e.g., GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5447.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5447.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deductive Verification (DV)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deductive Verification of chain-of-thought reasoning (Ling et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior method that verifies independent sub-tasks (Natural Programs) using a verifier that expects solutions in a specific 'Natural Programs' format; typically requires specialized generator and in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deductive verification of chain-of-thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as related work; DV involves specialized formats and a verifier tailored to Natural Programs, not a general-purpose zero-shot checker.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Deductive Verification (format-constrained verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Verify sub-tasks of a reasoning chain using a verifier that operates on a constrained, program-like natural representation; requires compatibility between generator and verifier formats.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mentioned (used on compatible tasks in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applied in prior work to problems suitable for Natural Programs; not directly re-implemented in this paper but compared in reported relative performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper notes DV as a baseline that can verify sub-tasks but is restricted by format and specialized generator requirements; SelfCheck reportedly attains larger relative gains in comparable reported settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>DV is limited to specialized solution formats (Natural Programs) and thus not a general-purpose zero-shot verifier; requires customized generator-verifier pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are reasoners with self-verification <em>(Rating: 2)</em></li>
                <li>Deductive verification of chain-of-thought reasoning <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>Let's verify step by step. <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5447",
    "paper_id": "paper-1e6102c981b9464c632ef0b00dbd11dfb0564e4e",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "SelfCheck (GPT-3.5 → GPT-3.5)",
            "name_full": "SelfCheck (regenerate-and-compare checker) using GPT-3.5 as both generator and checker",
            "brief_description": "SelfCheck is a zero-shot, step-by-step verify-then-weight pipeline that decomposes checking into target-extraction, information-collection, step-regeneration and result-comparison; here both generation and checking calls use OpenAI GPT-3.5 (gpt-3.5-0301).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI GPT-3.5 (gpt-3.5-0301)",
            "model_description": "OpenAI API model referred to as GPT-3.5 (gpt-3.5-0301) used as both generator and checker in experiments; exact model size/architecture details are not provided in the paper.",
            "reflection_method_name": "SelfCheck (regenerate-and-compare)",
            "reflection_method_description": "For each step in a chain-of-thought solution the LLM (checker) extracts the step's target, collects the minimal contextual information, regenerates an independent alternative step from that context, and compares the regenerated step to the original; per-solution step results (-1/0/1) are integrated into a soft confidence weight used for weighted voting across multiple candidate solutions.",
            "num_iterations": null,
            "task_name": "GSM8K, MathQA, MATH",
            "task_description": "Three math-reasoning datasets: GSM8K (grade-school arithmetic word problems), MathQA (multi-step math with multiple-choice options), and MATH (competition-level math problems).",
            "performance_with_reflection": "GSM8K: 74.3% final answer accuracy (SelfCheck weighted voting); MathQA: 64.6%; MATH (subset): 38.0%. (These are final-answer accuracies when using 2 candidate solutions per question, checker and generator both GPT-3.5, from Table 1.)",
            "performance_without_reflection": "Majority voting (no SelfCheck weights) with same generator: GSM8K 71.7%; MathQA 59.2%; MATH 35.8%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: SelfCheck yields consistent improvements over majority voting for GPT-3.5 generator/checker: +2.8±0.9% on GSM8K, +5.4±1.1% on MathQA, +2.2±0.7% on MATH (Table 1). Filtering by SelfCheck confidence also reduces the proportion of incorrect solutions (reported reductions of ≈9%, 22.8%, 16.2% for the three datasets when low-confidence solutions are removed) and raises precision-like measures (e.g. fraction of predicted-correct solutions that are actually correct increases with higher threshold). ROC/verification curves show meaningful true/false positive control in contrast to naive global checking.",
            "limitations_or_failure_cases": "Step checks are imperfect and can misclassify correct steps; filtering trades recall for precision (correct solutions can be rejected when thresholding). The method was evaluated only on math benchmarks in the paper. Using the same model for generation and checking can leave correlated errors; decorrelation (different checker model) sometimes helps. SelfCheck's improvements depend on having multiple candidate solutions (it functions via weighted voting over candidates).",
            "uuid": "e5447.0",
            "source_info": {
                "paper_title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "SelfCheck (GPT-4 → GPT-4)",
            "name_full": "SelfCheck using GPT-4 as both generator and checker",
            "brief_description": "Same SelfCheck pipeline (regenerate-and-compare) evaluated with GPT-4 (gpt-4-0613) as both the answer generator and the checker on sampled subsets of the datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI GPT-4 (gpt-4-0613)",
            "model_description": "OpenAI GPT-4 API model used as generator and/or checker in experiments; exact parameter counts are not provided in the paper.",
            "reflection_method_name": "SelfCheck (regenerate-and-compare)",
            "reflection_method_description": "Same per-step extraction → information collection → regeneration → comparison pipeline, using GPT-4 for both the generation and the checking calls.",
            "num_iterations": null,
            "task_name": "GSM8K (sample), MathQA (sample), MATH (sample)",
            "task_description": "Sampled subsets (500 questions each) from the three math datasets used for higher-cost GPT-4 experiments; final-answer correctness measured with 2 candidate solutions per question.",
            "performance_with_reflection": "GSM8K: SelfCheck 86.9% (vs. majority voting 87.1%); MathQA: SelfCheck 80.9% (majority 78.3%); MATH (subset): SelfCheck 51.3% (majority 47.9%). (Table 1; values based on GPT-4 generator and GPT-4 checker on sampled sets.)",
            "performance_without_reflection": "Majority voting (no SelfCheck weights) with GPT-4 generator: GSM8K 87.1%; MathQA 78.3%; MATH 47.9%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: SelfCheck improves MathQA (+2.6% ±0.4) and MATH (+3.4% ±0.6) final accuracies when both generator and checker are GPT-4; for GSM8K the difference is negligible/slightly negative (-0.2% ±0.2). The paper argues that for harder tasks (MATH) GPT-4 as checker helps more.",
            "limitations_or_failure_cases": "On some simpler datasets (GSM8K) checking with the same powerful model (GPT-4) produced no improvement or a tiny negative change, suggesting correlated errors between generator and checker; computational cost of using GPT-4 for checking is high (so paper explores cheaper checker alternatives).",
            "uuid": "e5447.1",
            "source_info": {
                "paper_title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "SelfCheck (GPT-4 generator checked by GPT-3.5)",
            "name_full": "SelfCheck with GPT-4 as generator and GPT-3.5 as checker (cross-model checking)",
            "brief_description": "Cross-model variant where GPT-4 generates candidate solutions and the cheaper GPT-3.5 model performs the SelfCheck stepwise verification, intended to decorrelate generator and checker errors and reduce cost.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Generator: GPT-4 (gpt-4-0613); Checker: GPT-3.5 (gpt-3.5-0301)",
            "model_description": "Generator is GPT-4 (gpt-4-0613) and checker is GPT-3.5 (gpt-3.5-0301); both through OpenAI API, checker is cheaper so used to decorrelate and reduce cost.",
            "reflection_method_name": "SelfCheck (cross-model regenerate-and-compare)",
            "reflection_method_description": "Identical SelfCheck pipeline executed with generator outputs from GPT-4 and independent checking/regeneration/comparison calls to GPT-3.5 (checker does not see the original step when regenerating). Confidence weights from GPT-3.5 used for weighted voting among GPT-4-generated candidates.",
            "num_iterations": null,
            "task_name": "GSM8K (sample), MathQA (sample), MATH (sample)",
            "task_description": "Same sampled subsets used in GPT-4 experiments; 2 candidate solutions per question in reported numbers.",
            "performance_with_reflection": "GSM8K: SelfCheck 88.1% (vs MV 87.1%); MathQA: 81.2% (vs MV 78.3%); MATH: 48.9% (vs MV 47.9%). (Table 1; cross-model checking gave improved final accuracy on GSM8K and MathQA compared to same-model checking.)",
            "performance_without_reflection": "Majority voting with GPT-4 generator: GSM8K 87.1%; MathQA 78.3%; MATH 47.9%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: Cross-model checking (GPT-3.5 verifying GPT-4 outputs) yielded modest additional gains over same-model GPT-4 checking on some tasks (e.g., +1.0% GSM8K, +3.0% MathQA in reported samples), supporting the claim that decorrelating checker and generator errors helps.",
            "limitations_or_failure_cases": "Cross-model gains vary by dataset: for the hardest dataset (MATH) GPT-4-as-checker performed better than GPT-3.5, suggesting cheaper checkers are not uniformly sufficient. The paper notes the benefit likely comes from error decorrelation but does not fully characterize when cheaper checkers suffice.",
            "uuid": "e5447.2",
            "source_info": {
                "paper_title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Global checking (direct LLM verification)",
            "name_full": "Global direct-checking: prompt the LLM to check the whole reasoning chain at once",
            "brief_description": "A baseline variant where the LLM is directly prompted to check the entire solution (global check) rather than doing stepwise regenerate-and-compare; found to nearly always declare solutions correct.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (used in ablation)",
            "model_description": "GPT-3.5 used with a prompt asking it to check a full solution step-by-step and conclude 'Correct', 'Wrong' or 'Not Sure'.",
            "reflection_method_name": "Global direct checking",
            "reflection_method_description": "Single prompt asking the LLM to 'Carefully check whether the solution is correct step by step' and output a final verdict; no regeneration/comparison decomposition.",
            "num_iterations": null,
            "task_name": "MathQA (subset, ablation)",
            "task_description": "Subset of MathQA (100 random questions) used for ablation studies.",
            "performance_with_reflection": "Verification accuracy (average of TPR and TNR) = 55.0% on MathQA ablation (Table 2); final-answer accuracy similar to majority voting (i.e., provided little useful discrimination).",
            "performance_without_reflection": "Majority voting baseline final-answer accuracy (used for comparison, see main experiments) -- global checker produced little to no useful separation so final-answer performance was close to majority voting.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "No meaningful improvement: global check nearly always labels chains as correct (&gt;90% in prior work and in this paper), so it fails to provide useful confidence for selection/filtering.",
            "limitations_or_failure_cases": "Global direct checking is strongly biased toward declaring solutions correct (very high false positive rate) and provides no meaningful control of TP/FP; verification accuracy near random on MathQA in ablation, supporting the paper's claim that direct global checking is ineffective.",
            "uuid": "e5447.3",
            "source_info": {
                "paper_title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Single-stage step checking",
            "name_full": "Single-stage step checking (directly ask LLM to check one step given context)",
            "brief_description": "A simpler ablation that directly prompts the LLM to check the next step given the question and previous steps, without decomposition into target extraction/information collection/regeneration/comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (ablation)",
            "model_description": "GPT-3.5 used to attempt single-stage per-step checking by directly asking 'Check the correctness of the next step' with context.",
            "reflection_method_name": "Single-stage check",
            "reflection_method_description": "Directly prompt the LLM to consider the information it relies on and to judge the next step as 'Correct', 'Wrong', or 'Not Sure' in one shot (no regeneration-comparison pipeline).",
            "num_iterations": null,
            "task_name": "MathQA (subset, ablation)",
            "task_description": "100-question subset of MathQA used for ablation.",
            "performance_with_reflection": "Verification accuracy = 57.2% (Table 2). Final-answer generation accuracy with this variant was significantly worse than full SelfCheck in ablations (Figure 6).",
            "performance_without_reflection": "Compared to majority voting, single-stage checking yields little improvement and is inferior to the multi-stage SelfCheck scheme.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Slight improvement over naive global check but substantially worse than SelfCheck, showing decomposition into sub-tasks is important.",
            "limitations_or_failure_cases": "Still too difficult for the LLM: single-stage step checking performs worse than the multi-stage SelfCheck, consistent with the argument that checking is easier when decomposed and when regeneration provides a decorrelated reference.",
            "uuid": "e5447.4",
            "source_info": {
                "paper_title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Error-check (0-shot / 1-shot)",
            "name_full": "Direct error-checking prompt (zero-shot and one-shot variants) that asks the LLM to identify grounding/reasoning/calculation errors",
            "brief_description": "A direct error-checking ablation that replaces SelfCheck's regeneration+compare with a single error-check prompt; tested in zero-shot and with one exemplar (one-shot).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (ablation)",
            "model_description": "GPT-3.5 used with direct error-checking prompts: a zero-shot prompt asking to check for grounding/reasoning/calculation errors and a 1-shot variant using an exemplar from Ling et al. (2023).",
            "reflection_method_name": "Direct error-check (0-shot and 1-shot)",
            "reflection_method_description": "Single-prompt error checking that asks the LLM to classify a step as 'Correct', 'Wrong', or 'Not Sure' and to check for grounding/reasoning/calculation errors; 1-shot version includes an exemplar guiding how to check.",
            "num_iterations": null,
            "task_name": "MathQA (subset, ablation)",
            "task_description": "100-question subset of MathQA used for ablation.",
            "performance_with_reflection": "Verification accuracy: 0-shot = 63.1%; 1-shot = 64.2% (Table 2).",
            "performance_without_reflection": "Compared to majority voting final-answer accuracy, these direct error-check methods provide modest verification skill but underperform the full SelfCheck (verification accuracy 66.7%).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Direct error-checking improves over naive global check and single-stage checking, but still performs worse than the regenerate-and-compare pipeline, supporting the paper's claim that LLMs are better at generation (regeneration) than at direct checking.",
            "limitations_or_failure_cases": "Even with a detailed exemplar (1-shot), direct error-checking remains inferior to step regeneration + comparison; direct checkers also risk correlation with generator errors and overconfident 'correct' labels.",
            "uuid": "e5447.5",
            "source_info": {
                "paper_title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Self-Verification (SV)",
            "name_full": "Self-Verification (SV) backward-prediction approach (Weng et al., 2022)",
            "brief_description": "Prior approach that verifies a full chain by using conclusions from chain-of-thought to predict masked conditions in the question (backwards prediction); typically applied as few-shot and supports only single-step checking assumptions.",
            "citation_title": "Large language models are reasoners with self-verification",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Described in related work; SV is typically implemented by prompting an LLM in few-shot mode to perform a backward prediction verification; paper does not itself run SV experiments but compares to its reported results.",
            "reflection_method_name": "Self-Verification (backward prediction)",
            "reflection_method_description": "Use chain-of-thought conclusion to predict masked parts of the question and check consistency; designed primarily for simpler tasks and single-step recoverability.",
            "num_iterations": null,
            "task_name": "Mentioned in comparison (reported on problems like GSM8K in prior work)",
            "task_description": "Used in prior literature for certain math QA tasks (e.g., GSM8K) as a few-shot verifier.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Paper cites SV as a prior method that requires few-shot exemplars and is limited (only supports single-step checking and assumptions that question info is fully recoverable), and notes SelfCheck outperforms SV in reported settings.",
            "limitations_or_failure_cases": "SV only supports single-step checking and assumes every piece of question information is recoverable from a correct solution, limiting applicability to simpler tasks (e.g., GSM8K).",
            "uuid": "e5447.6",
            "source_info": {
                "paper_title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Deductive Verification (DV)",
            "name_full": "Deductive Verification of chain-of-thought reasoning (Ling et al., 2023)",
            "brief_description": "Prior method that verifies independent sub-tasks (Natural Programs) using a verifier that expects solutions in a specific 'Natural Programs' format; typically requires specialized generator and in-context examples.",
            "citation_title": "Deductive verification of chain-of-thought reasoning",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Described as related work; DV involves specialized formats and a verifier tailored to Natural Programs, not a general-purpose zero-shot checker.",
            "reflection_method_name": "Deductive Verification (format-constrained verifier)",
            "reflection_method_description": "Verify sub-tasks of a reasoning chain using a verifier that operates on a constrained, program-like natural representation; requires compatibility between generator and verifier formats.",
            "num_iterations": null,
            "task_name": "Mentioned (used on compatible tasks in prior work)",
            "task_description": "Applied in prior work to problems suitable for Natural Programs; not directly re-implemented in this paper but compared in reported relative performance.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Paper notes DV as a baseline that can verify sub-tasks but is restricted by format and specialized generator requirements; SelfCheck reportedly attains larger relative gains in comparable reported settings.",
            "limitations_or_failure_cases": "DV is limited to specialized solution formats (Natural Programs) and thus not a general-purpose zero-shot verifier; requires customized generator-verifier pipeline.",
            "uuid": "e5447.7",
            "source_info": {
                "paper_title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are reasoners with self-verification",
            "rating": 2
        },
        {
            "paper_title": "Deductive verification of chain-of-thought reasoning",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1
        },
        {
            "paper_title": "Let's verify step by step.",
            "rating": 1
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        }
    ],
    "cost": 0.01861725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SelfCheck: using LLMs to zero-shot check THEIR OWN STEP-BY-STEP REASONING</h1>
<p>Ning Miao ${ }^{1 *}$ Yee Whye Teh ${ }^{1}$ Tom Rainforth ${ }^{1}$</p>
<h4>Abstract</h4>
<p>The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-bystep reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on three datasets-GSM8K, MathQA, and MATH—and find that it successfully recognizes errors and, in turn, increases final answer accuracies.</p>
<h2>1 INTRODUCTION</h2>
<p>Recent years have witnessed dramatic changes in the areas of NLP and AI brought on by significant advances in LLMs. From GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), Llama (Touvron et al., 2023) and Falcon (Almazrouei et al., 2023) to GPT-4 (OpenAI, 2023) and PaLM-2 (Google, 2023), the increasing model sizes and exploding amount of training data have empowered LLMs to achieve human-level performance on a large range of tasks, including summarization, translation, and question answering. The invention of Chain-of-Thought prompting (CoT, Wei et al. (2022)) has further enhanced LLMs' ability to solve complex problems by generating step-by-step solutions.</p>
<p>However, the performance of even the largest LLMs is still unsatisfactory on more difficult reasoning problems. For example, GPT-4 with CoT prompting only correctly answers $42.5 \%$ of problems in the MATH dataset (Bubeck et al., 2023; Hendrycks et al., 2021), which is far below human level. Such problems require careful and extensive multi-step reasoning to solve, and LLMs are consequently prone to make mistakes: even though their error rate on individual steps may be low, the probability of generating at least one erroneous step can still be quite high, undermining the final answer.</p>
<p>Recent works have tried to overcome this limitation by checking for errors in these step-by-step solutions (Cobbe et al., 2021; Li et al., 2022; Ling et al., 2023). Such checks can then be used to provide confidence scores in answers and select between different possible alternatives. This checking has typically been performed either by using an external verification model (Cobbe et al., 2021; Lyu et al., 2023; Peng et al., 2023), or through few-shot in-context learning (Brown et al., 2020) of an LLM (Weng et al., 2022; Ling et al., 2023).</p>
<p>Unfortunately, existing methods generally require extra training data and/or domain-specific exemplars, which often makes them inconvenient to use in practice and restricts them to specific domains or data formats. The aim of our work is thus to instead provide a general-purpose, zero-shot, approach to checking that relies only on the original LLM, without the need for additional external resources.</p>
<p>To this end, we introduce SelfCheck, a zero-shot step-by-step checker for self-identifying errors in LLM reasoning chains. SelfCheck uses the LLM to individually check the conditional correctness of each step in the chain based on the preceding steps, in a manner similar to a human going back to check their working. The results of these individual checks are then integrated to form an overall correctness estimation for the whole reasoning chain.</p>
<p>Key to SelfCheck's success is a novel mechanism for performing the checking of individual steps. As we will show, the naive approach of directly asking the LLM to check a step is typically ineffective. Instead, we introduce a multi-stage approach that breaks the problem down into a series of simpler</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>tasks, leverages the generative strengths of the LLM, and decorrelates errors between the original generation and checking. Specifically, using separate calls to the LLM we first extract the target and relevant context for the step, then regenerate an independent alternative step from these, and finally compare the two. The original step is then deemed to pass the check if it matches the regeneration.</p>
<p>Besides providing an estimation of correctness for each solution, SelfCheck can also boost final answer accuracies for the original questions by weighted voting. Namely, given multiple solutions to a question, it uses confidence scores as weights to vote among the answers, which provides a soft way to focus on more accurate solutions.</p>
<p>We evaluate SelfCheck on three math tasks, namely GSM8K (Cobbe et al., 2021), MathQA (Amini et al., 2019), and MATH (Hendrycks et al., 2021). For all datasets, we find that using SelfCheck achieves a significant increase in final answer accuracies compared with simple majority voting and other baselines. We also see that SelfCheck provides an accurate confidence estimation for LLM's solutions, which decreases the proportion of incorrect solutions by $9 \%, 22.8 \%$, and $16.2 \%$ on the three datasets respectively when filtering out solutions with low confidence scores. We further perform a number of ablations to justify some of our key design choices in the SelfCheck approach.</p>
<p>To summarize, we introduce SelfCheck as a novel and effective zero-shot schema for self-checking step-by-step reasoning in LLMs. Unlike previous methods, SelfCheck does not need any finetuning or example crafting, so can be directly applied to reasoning tasks in different domains. Our experiments confirm that it can, in turn, be used to improve final predictive performance of LLMs. Our code is available at https://github.com/NingMiao/SelfCheck.</p>
<h1>2 Related Work</h1>
<p>How to automatically check the correctness of a sequence of reasoning steps is a long-standing question. We now discuss how previous methods have tried to tackle this in an LLM context. We note that none of these works are able to work in the zero-shot setting covered by SelfCheck, requiring either problem-specific examples, an external model, and/or finetuning.</p>
<p>Few-shot verification Though our focus will be on zero-shot checking, for some problems one may have hand-crafted exemplars available that are specifically designed to that particular questionanswering task. Previous methods have been designed to perform checking of LLMs' generated solutions in this few-shot checking scenario.</p>
<p>For example, the Self-Verification (SV) approach of Weng et al. (2022) verifies the whole solution by backward prediction. That is, it uses the conclusion from CoT reasoning to predict a masked condition in the question. However, it only supports single-step checking and is based on the assumption that every piece of information in the question can be recovered using a correct solution of it, which is often not the case. Consequently, it is only applicable to simpler tasks, such as GSM8K.</p>
<p>The Deductive Verification (DV) approach of Ling et al. (2023) instead looks to verify independent sub-tasks, as per SelfCheck. However, its verifier only supports checking reasoning chains in a special format called Natural Programs. As a result, it can only work with a specific specialised generator, without serving as a general verifier for multi-step reasoning.</p>
<p>Verification with external resources In some cases, there might be external resources available to verify the logical correctness or faithfulness of LLM outputs. Lyu et al. (2023) translate a question into a symbolic reasoning chain using an LLM and solve the problem by a symbolic logic solver. Peng et al. (2023) introduced an external database to check for incorrect knowledge in LLM outputs. These methods are limited by the availability of external resources and are typically restricted to checking for certain types of errors.</p>
<p>Training/finetuning a verifier A few other methods train or finetune a separate verifier model to check reasoning chains. Cobbe et al. (2021) finetuned a GPT-3 model on GSM8K to predict the correctness of a solution as a whole. Li et al. (2022) trained a binary deberta-v3-large (He et al., 2020) classifier on each domain to predict step correctness. More recently, Lightman et al. (2023) built a large dataset, which contains step-wise correctness labels from human labelers, and finetuned a GPT-4 model on it. Unlike SelfCheck, all of these methods require extra data and external computational resources, restricting their applicability and ease of use.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example of using SelfCheck, focusing on the checking of a particular step (Step 5). To check the correctness of the step, SelfCheck goes through 4 stages. First, in the target extraction stage, it figures out that the main purpose of Step 5 is to complete the square. In the information collection stage, it then establishes that Step 5 only directly relies on Step 4. Next, the step regeneration stage instructs the LLM to complete the square independently, only using Step 4 as context. The regeneration result shows that the center and radius of the circle are $(3,0)$ and 3 , which is different from what is implied by the original Step 5. Consequently, the result comparison stage concludes that Step 5 is likely to be wrong. After checking all the steps, SelfCheck integrates the results to form an overall confidence score, $w$. See Appendix A for a complete version of the example.</p>
<h1>3 SelfCheck: Using LLMs to Check Their Own Reasoning</h1>
<p>Rather than relying on external resources or problem-specific data like the aforementioned approaches, it would be highly beneficial if we could develop self-contained checking schemes that require only the original LLM itself. In other words, we would like to use the LLM to identify errors in its own step-by-step reasoning, analogously to how a human might go back to check their working.</p>
<p>Unfortunately, directly asking the LLM to check its own reasoning is largely ineffective: it almost invariably declares that the original answer is correct, with Ling et al. (2023) finding answers checked in this way are deemed correct more than $90 \%$ of the time regardless of whether they actually are. As we will show in Section 5, individually prompting the LLM to check each step in the CoT reasoning fares slightly better, but is still only able to offer marginal gains compared to not checking at all.</p>
<p>A more nuanced method to perform this checking is thus required. To this end, we introduce SelfCheck, a general-purpose, zero-shot, checking schema for self-identifying errors in LLM CoT reasoning. Given a question, $q$, and its step-by-step solution, $s$, produced by some generator (which will generally be an LLM with appropriate CoT prompting), SelfCheck considers each step of $s$ in turn and tries to establish its individual correctness based on the preceding steps. This checking is done by leveraging an LLM (which can either be the same LLM used to generate $s$ or a separate one), but rather than directly asking the LLM to perform the check, we instead introduce a novel step checking method (see Section 3.1) that exploits their generative modeling strengths. The results of the checks on individual steps are then combined into a single confidence score, $w \in[0,1]$, for the whole solution. These confidence scores, in turn, allow us to improve predictive performance, by using them to perform weighted voting on multiple solutions to the same question.</p>
<h1>3.1 STEP CHECKING</h1>
<p>To check individual steps of the reasoning process, the first thing we should note is that the correctness of each step is highly dependent on its context, namely the question and previous steps in the solution. For example, we usually need to refer to previous steps for the definition of variables and the meaning of specific numbers. If each step is conditionally correct based on the provided context and the last step provides an answer in the required format, then the overall reasoning will itself be correct. The target of the step checking is thus simply to check the conditional correctness of each step based on the provided context. That is, we only care about catching errors at the current step, and can assume all information from its context to be correct.</p>
<p>A simple idea to try and achieve this would be to feed the current step as well as all its context to an LLM and directly ask it to 'check the correctness of the step'. However, in practice, we find that this task is too difficult for the LLM to do effectively, even with careful prompting that exemplifies how to do the checking in detail (see Section 5). This difficulty comes first from the fact that there are multiple aspects to the checking problem that the checker must deal with simultaneously: it needs to understand the key content in the step and then collect all related information from the context, before actually checking for its correctness. Second, 'checking' is a less common task in the training corpus of most LLMs, such that it is a problem that does not necessarily play to their strengths. Finally, there are likely to be strong correlations between the errors such a checker will make with the errors made in the original generation, undermining its usefulness.</p>
<p>To address these difficulties, SelfCheck instead decomposes the checking task for each step into four stages: target extraction, information collection, step regeneration, and result comparison. The LLM is used to execute each stage successively, with the outcome of the result comparison providing the correctness prediction.</p>
<p>The idea behind this decomposition is to make the LLM focus on an easier task at each stage and ensure the individual tasks carried out are more closely aligned to the LLM's strengths. Moreover, by focusing on regenerating and then comparing, we hope to reduce the correlations between the errors of the checking and the original generation.</p>
<p>At a high level, the stages work by first prompting the LLM to figure out the target of the current step and what information it uses to achieve the target; we find that the LLM is usually able to perform these tasks extremely accurately. Then we ask the LLM to re-achieve the target using only the collected information, providing an alternative to the original step that maintains the same purpose in the overall reasoning process. Here the clear description of the target and the simplified context we provide make the regeneration stage less challenging. As a result, we hope its output will be more reliable and thus serve as a useful reference. Even if this is not the case, it will still hopefully provide a viable alternative, with a distinct generation, that can be used for comparison. The last stage then uses the LLM to compare the original step with the regenerated output. If their main conclusions match/mismatch, this provides evidence that the original step was correct/incorrect.</p>
<p>A worked example of this step-checking process is provided in Figure 1. In the following, we describe each of the subtasks in detail and provide our specific instructions to the LLM. We note here that the different LLM queries are made independently, rather than keeping the queries and answers from previous stages in context. Thus, for example, when the LLM is called to carry out the step regeneration, it does not have access to the original generation. The same prompts are used across LLMs and datasets, thereby providing a general-purpose approach.</p>
<p>Target extraction To check a step (for example, Step 5 in Figure 1), we first need to figure out what the step is trying to achieve. Without a specific target, the regeneration stage would proceed in a random direction, making it impossible to serve as a reference to the original step. We thus use the LLM itself to extract the target of a step using the question and all previous steps (Steps 0-4 in Figure 1) with the following prompt (we omit some line breaks due to space limitations):</p>
<p>The following is a part of the solution to the problem [Question]: [Step 0,..., Step i]. What specific action does the step [Step i] take? Please give a brief answer using a single sentence and do not copy the steps.</p>
<p>During execution, we copy the question and steps into [Question] and [Step 0, ..., Step i] to form the actual input to the LLM. The reason for requesting a brief answer is to try and keep the amount of information retained to the minimum needed, thereby avoiding unnecessary influence on the regeneration and hopefully reducing correlations in errors in turn.</p>
<p>Information collection To reduce the difficulty of the regeneration stage and avoid unrelated information from affecting the result, we filter out information that is not directly related to the current step. Specifically, we ask the LLM to select useful items from the question and all previous items with the following prompt, where [Information j] is simply the j-th sentence in the question:</p>
<p>This is a math question: [Question]. The following is information extracted from the question:
Information 0: [Information 0] Information 1: [Information 1] ...
The following are the first a few steps in a solution to the problem:
Step 0: [Step 0] Step 1: [Step 1] ... Step i-1: [Step i-1]
Which previous steps or information does the next step [Step i] directly follow from?
After retrieving the free-text response from the LLM, we extract step or information ids by regular expression. For example in Figure 1, the current step requires Step 4 and no information from the question as context. The selected steps and information are then fed into the regeneration stage.</p>
<p>Step regeneration Given the target and necessary information of the step, we can now ask the LLM to achieve the target independently with only the collected information, without seeing the original step. Because the step is usually a small jump from previous conclusions, and the information collection stage has already filtered out irrelevant information, we can usually trust regeneration results. The prompt for this stage is:</p>
<p>We are in the process of solving a math problem. We have some information from the problem:
Information 0: [Information $I_{0}$ ] Information 1: [Information $I_{1}$ ] ...
The following are some previous steps: Step 0: [Step $S_{0}$ ] Step 1: [Step $S_{1}$ ] ...
The target for the next step is: [Target]
Please try to achieve the target with the information from the problem or previous steps.
Here [Target] is the output from the target extraction stage. [Information $I_{i}$ ] and [Step $S_{i}$ ] correspond to the specific items selected by the information collection stage. In Figure 1, only Step 4 and no information from the question is directly related to the current step, so SelfCheck simply copies the content of Step 4 into [Step $S_{0}$ ] and removes the block containing [Information $I_{i}$ ].</p>
<p>Result comparison The last step is to compare results from the regeneration stage and the original step with the following prompt:</p>
<p>The following are 2 solutions to a math problem. Solution 1: [Regeneration output] Solution 2: [Step i]
Compare the key points from both solutions step by step and then check whether Solution 1 'supports', 'contradicts' or 'is not directly related to' the conclusion in Solution 2. Pay special attention to the difference in numbers.
If the regeneration output 'supports'/'contradicts' the original step, we can conclude that the original step is likely correct/incorrect respectively. Sometimes, the correctness of the original step cannot be directly inferred from the regeneration output. For example, when the target is to simplify an equation, then there may be multiple valid solutions. In such cases, we are not sure about the correctness of the original step, which makes 'is not directly related to' the third possible outcome of the check.</p>
<h1>3.2 RESULTS INTEGRATION</h1>
<p>After running step-checking and getting a checking result for each step, we need an integration function $\phi$ to give a confidence score, $w \in[0,1]$, for the overall correctness of the solution. The input of $\phi$ should be a vector in the form of $\left[r_{0}, r_{1}, \ldots, r_{n}\right]$, where each item $r_{i}$ represents the step checking result for Step $i$. We will use $r_{i}=-1,0$, and 1 to represent the step-checking results 'contradict', 'is not directly related to' and 'support' respectively. We find that the following simple integration function works well in practice</p>
<p>$$
w=\phi\left(\left[r_{0}, r_{1}, \ldots, r_{n}\right]\right)=2 * \text { Sigmoid }\left(-\lambda_{-1} \sum_{i=0}^{n} \mathbb{1}<em i="i">{r</em>}=-1}-\lambda_{0} \sum_{i=0}^{n} \mathbb{1<em i="i">{r</em>\right)
$$}=0</p>
<p>where $\lambda_{-1}$ and $\lambda_{0}$ are two non-negative hyperparameters with $\lambda_{-1}&gt;\lambda_{0}$; we fix $\lambda_{-1}=1$ and $\lambda_{0}=0.3$ in our experiments. The rationale of this setup is that the more failed checks we see, the more likely the overall reasoning process, and thus final solution, are wrong. Note here that, because the checks are themselves imperfect, we do not necessarily want to immediately reject the whole solution from a single step-check failure, especially for $r_{i}=0$ cases. This is why we take a 'soft' approach to the verification with a confidence score. The number of successful checks, i.e. $\sum_{i=0}^{n} \mathbb{1}<em i="i">{r</em>$, is deliberately not included in our integration function as an increased number of}=1</p>
<p>successful checks does not actually increase our confidence in the overall solution: shorter reasoning chains are generally preferable to longer ones for a given question and LLM.</p>
<p>Once calculated, the resulting confidence score can be directly used as a weight for voting between different possible solutions. We can thus use SelfCheck to increase the accuracy of an LLM's answers by generating multiple possible solutions, calculating confidence scores for each, and then choosing our final answer through weighted voting.</p>
<h1>4 EXPERIMENTS</h1>
<p>We now run experiments on three math-reasoning datasets to evaluate SelfCheck's effectiveness in checking multi-step reasoning and improving final answer accuracies. Note here that our focus on math-reasoning problems is due to ease of performance evaluation and dataset availability; SelfCheck is directly applicable to other question-answering problems with nominal changes to our prompts.</p>
<p>Datasets GSM8K (Cobbe et al., 2021), MathQA (Amini et al., 2019), and MATH (Hendrycks et al., 2021) consist of math problems on primary school, middle school, and competition levels, containing 1319, 2985, and 5000 test samples, respectively. For GSM8K and MathQA, we evaluate SelfCheck on the whole test sets. Due to limited resources, we use a subset of MATH test set taken from Ling et al. (2023). ${ }^{1}$ Besides the levels of difficulty, the three datasets differ from each other in the following aspects. Firstly, MathQA provides 5 options to choose from for each problem, while GSM8K and MATH have no options. Secondly, GSM8K only has arithmetic problems, while MathQA and MATH contain more diverse problems in geometry, physics, probability, and algebra.</p>
<p>LLMs We use GPT-3.5 (gpt-3.5-0301) and GPT-4 (gpt-4-0613) as our LLMs, focusing in particular on the former due to budget restrictions. Note that the same prompts are used for all datasets with both LLMs during evaluation; no dataset-specific customization or tuning has been performed. When devising the prompts, a small number of training samples from MathQA dataset were utilized.</p>
<p>Baselines We use majority voting (also known as Self-Consistency Decoding (Wang et al., 2022) in the context of CoT reasoning) as our main baseline following Ling et al. (2023) and Lightman et al. (2023). Despite its simplicity, this is still quite a strong baseline in the current literature. In particular, most existing few-shot methods report similar results compared with it (Weng et al., 2022; Ling et al., 2023). We also compare with previously quoted results from Self Verification (SV, Ling et al. (2023)) and Deductive Verification (DV, Weng et al. (2022)) when possible. We note though that these approaches are not directly comparable to SelfCheck in general, as they require additional exemplars which will often not be available in practice. Despite this, we will find that SelfCheck outperforms them when comparisons are possible.
We omit results from Faithful-CoT (Lyu et al., 2023), because it has already been shown to decrease the accuracies on GSM8K and MATH by $11.8 \%$ and $4.2 \%$, respectively compared to majority voting (Ling et al., 2023). It is also impossible for us to compare with training/finetuning based methods such as Lightman et al. (2023), because we have neither access to their finetuned models nor computation resources to repeat their training/finetuning. The significant extra data and resources they require also means their contributions are somewhat tangential to SelfCheck regardless.</p>
<h3>4.1 FINAL ANSWER CORRECTNESS</h3>
<p>Figure 2 shows the performance gains using the confidence scores from SelfCheck to do weighted voting compared with baseline methods. The upper plots show that accuracies of both SelfCheck and majority voting have the same increasing tendency as the number of generated solutions per question increases, which is a result of the variance reduction provided by averaging over more solutions. The bottom plots show the difference in accuracy between the two including the standard error in the estimate. We can see that by allocating higher weights to correct solutions, SelfCheck achieves significantly higher accuracies than majority voting for all solution numbers per question. We also find the improvements of SelfCheck (compared with majority voting) to be higher than Deductive Verification and Self-Verification in their reported settings, despite the use of in-context learning</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The upper plots show the accuracies of SelfCheck and majority voting for different numbers of generated solutions per question with GPT-3.5. The lower plots show the accuracy gaps between each method and majority voting, where DV and SV stand for Deductive Verification (Weng et al., 2022) and Self-Verification (Ling et al., 2023), respectively. It is difficult to compare with DV and SV with respect to absolute accuracies because they are using different generator models. However, we can see that SelfCheck achieves higher relative performance gains than both in their reported settings.</p>
<p>Table 1: SelfCheck significantly increases final answer accuracies with both GPT-3.5 and GPT4, even we only have 2 candidate solutions for each question. $\Delta$ Acc is the performance gain of SelfCheck compared with majority voting (MV), with the $\pm$ indicating the standard error. $\boldsymbol{X} \boldsymbol{X}, \boldsymbol{X} \boldsymbol{}$ and $\checkmark \checkmark$ represent the proportions of questions with 0,1 or 2 correct solutions. We see that the gains from SelfCheck are typically larger in cases where it is common for only one of the solutions to be correct, as these are the cases using weighted voting can influence the final answer.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Generator</th>
<th>Checker</th>
<th>$\boldsymbol{X} \boldsymbol{X}(\%)$</th>
<th>$\boldsymbol{X} \boldsymbol{}(\%)$</th>
<th>$\checkmark \checkmark(\%)$</th>
<th>Acc(MV, \%)</th>
<th>Acc(SelfCheck, \%)</th>
<th>$\Delta$ Acc (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GSM8K</td>
<td>GPT-3.5</td>
<td>GPT-3.5</td>
<td>16.8</td>
<td>23.0</td>
<td>60.2</td>
<td>71.7</td>
<td>74.3</td>
<td>$2.8 \pm 0.9$</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>GPT-4</td>
<td>8.8</td>
<td>8.2</td>
<td>83.0</td>
<td>87.1</td>
<td>86.9</td>
<td>$-0.2 \pm 0.2$</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>GPT-3.5</td>
<td>8.8</td>
<td>8.2</td>
<td>83.0</td>
<td>87.1</td>
<td>$\mathbf{8 8 . 1}$</td>
<td>$1.0 \pm 0.3$</td>
</tr>
<tr>
<td>MathQA</td>
<td>GPT-3.5</td>
<td>GPT-3.5</td>
<td>27.6</td>
<td>26.4</td>
<td>46.0</td>
<td>59.2</td>
<td>64.6</td>
<td>$5.4 \pm 1.1$</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>GPT-4</td>
<td>16.2</td>
<td>11.0</td>
<td>72.8</td>
<td>78.3</td>
<td>80.9</td>
<td>$2.6 \pm 0.4$</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>GPT-3.5</td>
<td>16.2</td>
<td>11.0</td>
<td>72.8</td>
<td>78.3</td>
<td>$\mathbf{8 1 . 2}$</td>
<td>$3.0 \pm 0.4$</td>
</tr>
<tr>
<td>MATH $^{+}$</td>
<td>GPT-3.5</td>
<td>GPT-3.5</td>
<td>52.6</td>
<td>23.2</td>
<td>24.2</td>
<td>35.8</td>
<td>38.0</td>
<td>$2.2 \pm 0.7$</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>GPT-4</td>
<td>42.0</td>
<td>20.2</td>
<td>37.8</td>
<td>47.9</td>
<td>$\mathbf{5 1 . 3}$</td>
<td>$3.4 \pm 0.6$</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>GPT-3.5</td>
<td>42.0</td>
<td>20.2</td>
<td>37.8</td>
<td>47.9</td>
<td>48.9</td>
<td>$1.0 \pm 0.8$</td>
</tr>
</tbody>
</table>
<p>from additional examples. We will perform additional ablations on how performance changes when ensembling over a larger number of solutions in Section 5.1.</p>
<p>To investigate the effect of using more powerful LLMs, and of using a different LLM for the generation and checking, we further conducted experiments with GPT-4 and a mix of GPT-4 and GPT-3.5. Because of the high cost of calling the GPT-4 API, we randomly sample 500 questions from each dataset to form the test sets and generate 2 (instead of 10) answers to each question. In Table 1, we see that SelfCheck significantly outperforms majority voting with both GPT-3.5 and GPT-4. We also notice that using GPT-3.5 to check GPT-4 generated answers yields surprisingly good results, actually outperforming checking with GPT-4 on the simpler GSM8K and MathQA tasks. This is likely because using different LLMs helps to further decorrelate the errors of the generator and the checker, and shows that using a cheaper LLM can still often be sufficient for the checking. For the more difficult problems in MATH, using GPT-4 as checker always produces better results, but even here the checking from GPT-3.5 is beneficial compared to doing no checking at all.</p>
<h1>4.2 VERIFICATION PERFORMANCE</h1>
<p>Besides serving as a confidence score calculator to improve the performance of voting, SelfCheck can also predict the correctness of a single solution. To do so, we simply set a threshold $t$ to the confidence score, where solutions with confidence scores $w \geq t$ are classified as correct.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: When raising the classification thresholds $t$, the proportions of real correct solutions in predicted correct solutions (Real + in Pred + ) increase for GSM8K ( $67.5 \% \rightarrow 76.5 \%$ ), MathQA $(59.4 \% \rightarrow 82.2 \%)$ and MATH $(34.6 \% \rightarrow 50.8 \%)$.</p>
<p>Figure 4 shows the ROC curves for each dataset. As a comparison, directly prompting GPT-3.5 to verify whole reasoning chains leads to no meaningful control on the false and true positive rates (FP and TP): they are always both $100 \%$ on MATH and $98 \%$ on GSM8K, as observed by Ling et al. (2023). In other words, the checker always predicts the answer as correct, providing no useful information.
As well as verification accuracies, we may also care about the solution quality after filtering out solutions with low confidence scores $w$. Figure 3 shows that by increasing the threshold $t$, SelfCheck can filter out more incorrect solutions, such that a higher proportion of the solutions that pass the check are indeed correct (Real + in Pred + ). Though this is at the cost of misclassifying more of the real correct solutions as incorrect, this can be a useful feature in cases where the risk of choosing an incorrect solution is higher than rejecting a correct one.</p>
<h1>5 ANALYSIS</h1>
<p>We now perform some ablations to justify some of the key design choices made by SelfCheck and provide insights on its behavior. Limited by budget and time, all experiments in this section are performed on a subset of the MathQA test set with 100 randomly selected questions.</p>
<h3>5.1 MORE SOLUTIONS PER QUESTION?</h3>
<p>Serving as a method to reduce variance, majority voting increased final answer accuracies on different datasets when we increased from 2 to 10 solutions in Figure 2. In cases where we only care about final predictive performance, one might thus question whether it is better to simply use our computational resources to keep increasing the size of this ensemble, rather than relying on a checking scheme.
However, as shown in Figure 5, this effect saturates
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: SelfCheck achieves significantly higher final answer accuracies than majority voting for large ensembles of solutions.
for larger solution ensembles, with the accuracy of majority voting never going above that achieved when $n=9$, thereby never reaching the performance we already achieved by SelfCheck for the smaller ensemble. Moreover, the performance of SelfCheck continues to increase as the ensemble grows. By lowering the weights (confidence) of incorrect solutions, SelfCheck increases the chance of selecting the correct answers, even when their generation probabilities in the generator LLM are low. Therefore, with SelfCheck, LLMs can effectively rectify their own biased beliefs by themselves.</p>
<h1>5.2 ALBATION STUDIES</h1>
<p>In order to pick apart the effect of several critical design choices for SelfCheck, we compare SelfCheck with some of its variants with respect to final answer and verification accuracies on MathQA.</p>
<p>Global v.s. step-by-step checking The first question is can we simply ask an LLM to check the whole solution without taking steps into consideration. To answer it, we prompt the LLM to perform global checking with the following instruction:
The following is a question and a solution to it from a student. Carefully check whether the solution is correct step by step. End your response with your conclusion that starts with "Correct", "Wrong" or "Not Sure". Question: [Question] Solution: [Step 0, Step 1,..., Step n]
Similar to the findings of Ling et al. (2023), we find that the global checker outputs "correct" most of the time and rarely recognizes an error. Consequently, its final answer accuracies are very close to majority voting (in Figure 6) and its verification accuracy ( $55.0 \%$ ) is only marginally above random guess ( $50.0 \%$ ). This lack of ability to deal with the difficulty of global checking is what makes step checking necessary.
Single-stage v.s. multiple-stage step checking Next, we ask whether we really need to decompose the step checking into several stages? To answer this, we design the following prompt to use the LLM directly.
The following is a question and the first a few steps in its solution.
Question: [Question] Solution: [Step 0, Step 1,..., Step i-1]
Check the correctness of the next step: [Step i]
Please consider the information it relies on and check step by step. Please end your response with your conclusion that starts with "Correct", "Wrong" or "Not Sure".
Figure 6 and Table 2 show that although this is better than global checking, it is still significantly worse than SelfCheck with its multi-stage checking. This indicates that checking a step in a single stage is still too challenging for the LLM, so it is necessary to further decompose step checking into a pipeline of easier sub-tasks.
Error check v.s. regenerate and compare We now justify the choice to perform step regeneration and comparison instead of direct error checking for each step. To do so, we replace our regeneration stage and comparison stage with a single error-checking stage. We first compare with a zero-shot version of the variant with the following prompt:
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Generation accuracies for variants of SelfCheck on MathQA with GPT-3.5.</p>
<p>Table 2: Verification accuracies for variants of SelfCheck on MathQA with GPT3.5. The reported verification accuracy is the average of true positive and true negative rates.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Accuracy (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SelfCheck</td>
<td style="text-align: center;">$\mathbf{6 6 . 7 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Global Check</td>
<td style="text-align: center;">$55.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Single stage Check</td>
<td style="text-align: center;">$57.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Error Check (0-shot)</td>
<td style="text-align: center;">$63.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Error Check (1-shot)</td>
<td style="text-align: center;">$64.2 \%$</td>
</tr>
</tbody>
</table>
<p>Given the following information:
Information 0: [Information $I_{0}$ ] Information 1: [Information $I_{1}$ ] ...
Step 0: [Step $S_{0}$ ] Step 1: [Step $S_{1}$ ]
Check the correctness of the next step [Step i]
Please check for grounding errors, reasoning errors and calculation errors step by step. Please end your response with your conclusion that starts with "Correct", "Wrong" or "Not Sure".
We then add an examplar from Ling et al. (2023) (see Appendix B) to make a more powerful one-shot error checker. However, results in Figure 6 and Table 2 show that even with a very detailed and instructive example, direct error checking still performs worse than our regenerate and compare approach, which supports our previous argument that LLMs are better at generation than checking.</p>
<h2>6 CONCLUSIONS</h2>
<p>In this paper, we have introduced SelfCheck, a general-purpose, zero-shot, step-by-step checking scheme for LLMs. Unlike previous approaches, SelfCheck does not require any additional data or external resources: it uses the LLM to identify errors in its own reasoning, leveraging a novel regenerate-and-compare approach. By using the results of this checking to perform weighted voting over different solutions, we find that SelfCheck is able to, in turn, increase final predictive accuracy.</p>
<h1>REFERENCES</h1>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023.</p>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2357-2367, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1245. URL https://aclanthology.org/N19-1245.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Google. Palm 2 technical report. arXiv preprint arXiv:2303.08774, 2023.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations, 2020.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. Deductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872, 2023.</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379, 2023.</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.</p>
<p>Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2022.</p>
<h1>APPENDIX A A COMPLETE EXAMPLE OF SELFCHECK</h1>
<p>To help readers get a better idea of how SelfCheck works, we show a complete version of the example in Figure 1, including the exact inputs and outputs of different stages. Please note that we have compiled all latex codes for readability, though the inputs and outputs to LLMs are in their original form. The question is</p>
<p>Let $S$ be the set of complex numbers $z$ such that the real part of $\frac{1}{z}$ is equal to $\frac{1}{6}$. This set forms a curve. Find the area of the region inside the curve.</p>
<p>We feed the following instruction to the LLM that serves as the generator:
Solve the following problem step by step. Please start each step with "Step :" and split sentences in each step with " $u / n$ ". Please finish you response with "So the answer is ...". Problem: Let $S$ be the set of complex numbers $z$ such that the real part of $\frac{1}{z}$ is equal to $\frac{1}{6}$. This set forms a curve. Find the area of the region inside the curve.</p>
<p>After getting the following solution, we start checking Step 5 as an example:
Solution:
(Step 0) Let $z=x+y i$ be a complex number, where $x$ and $y$ are real numbers.
(Step 1) We are given that the real part of $\frac{1}{z}$ is equal to $\frac{1}{6}$.
(Step 2) Using the fact that the real part of a complex number $a+b i$ is $a$, we have
$\operatorname{Re}\left(\frac{1}{z}\right)=\operatorname{Re}\left(\frac{1}{x+y i}\right)=\frac{a}{x^{2}+y^{2}}$.
(Step 3) Setting $\operatorname{Re}\left(\frac{1}{z}\right)=\frac{1}{6}$, we get $\frac{a}{x^{2}+y^{2}}=\frac{1}{6}$.
(Step 4) Cross-multiplying and rearranging, we get the equation $6 x=x^{2}+y^{2}$.
(Step 5) Completing the square, we obtain $\left(x-\frac{3}{2}\right)^{2}+y^{2}=\frac{9}{4}$.
(Step 6) This is the equation of a circle with center $\left(\frac{3}{2}, 0\right)$ and radius $\frac{3}{2}$.
(Step 7) The area of the region inside the circle is $\frac{9 \pi}{4}$. So the answer is $\frac{9 \pi}{4}$.
Variable definition extraction This optional stage extracts the definitions of variables that appear in the reasoning chain. SelfCheck feeds the definitions to the regeneration stage to avoid LLM not understanding the meaning of a variable. The input to the LLM is:</p>
<p>The following is the solution to a math question: Question: Let $S$ be the set of complex numbers $z$ such that the real part of $\frac{1}{z}$ is equal to $\frac{1}{6}$. This set forms a curve. Find the area of the region inside the curve.</p>
<p>Solution:
(Step 0) Let $z=x+y i$ be a complex number, where $x$ and $y$ are real numbers.
(Step 7) The area of the region inside the circle is $\frac{9 \pi}{4}$. So the answer is $\frac{9 \pi}{4}$.
Are there variables in the solution? If so, please list the definition of variable in the form of "1. variable is defined as...".</p>
<p>The underlined parts are the type of question, the question itself and the steps in its solution, respectively.</p>
<p>The output from the LLM is:
Yes. There are variables in the solution.</p>
<ol>
<li>$z$ is defined as a complex number of the form $x+y i$, where $x$ and $y$ are real numbers.</li>
</ol>
<p>The bold part is then saved to form a part of the input in the regeneration stage.
Target extraction To get a brief and clear target of the current step, the input to the LLM is:
The following is a part of the solution to the problem: Let $S$ be the set of complex numbers $z$ such that the real part of $\frac{1}{z}$ is equal to $\frac{1}{6}$. This set forms a curve. Find the area of the</p>
<p>region inside the curve.
(Step 0) Let $z=x+y i$ be a complex number, where $x$ and $y$ are real numbers.
(Step 5) Completing the square, we obtain $\left(x-\frac{3}{2}\right)^{2}+y^{2}=\frac{9}{4}$.
What specific action does the step "Completing the square, we obtain $\left(x-\frac{3}{2}\right)^{2}+y^{2}=\frac{9}{4}$." take? Please give a brief answer using a single sentence and do not copy the steps.</p>
<p>The underlined parts are the question and reasoning steps before the current one, including the current one.</p>
<p>The output of the LLM is:
The step completes the square to rewrite the equation in standard form of a circle.
The whole sentence is saved and forms the most important part of the input in the regeneration stage.
Information Collection To get sentences in the question and previous steps in the solution that are directly related to the current step, the input to the LLM is:</p>
<p>This is a math question: Question: Let $S$ be the set of complex numbers $z$ such that the real part of $\frac{1}{z}$ is equal to $\frac{1}{6}$. This set forms a curve. Find the area of the region inside the curve.</p>
<p>The following is information extracted from the question:
Information 0: Let $S$ be the set of complex numbers $z$ such that the real part of $\frac{1}{z}$ is equal to $\frac{1}{6}$.
Information 1: This set forms a curve.
Information 2: Find the area of the region inside the curve.</p>
<p>The following is the first a few steps in a solution to the problem:
Step 0: Let $z=x+y i$ be a complex number, where $x$ and $y$ are real numbers.
Step 4: Cross-multiplying and rearranging, we get the equation $6 x=x^{2}+y^{2}$.
Which previous steps or information does the next step "Completing the square, we obtain $\left(x-\frac{3}{2}\right)^{2}+y^{2}=\frac{9}{4}$." directly follow from?</p>
<p>The underlined information (contents after Information X:) is simply a list of sentences in the question. The underlined steps are reasoning steps before (not including) the current one in the solution.</p>
<p>The output from the LLM is:
The next step "Completing the square, we obtain $\left(x-\frac{3}{2}\right)^{2}+y^{2}=\frac{9}{4}$." directly follows from Step 4.</p>
<p>Then we use regular expression to extract the information and step id from LLM's output. Some steps may only rely on information from the question while others may only use results from previous steps.</p>
<p>Step regeneration In this stage, we instruct the LLM to achieve the extracted target with only collected information with the following prompt:</p>
<p>We are in a process of solving a math problem.
Variables are defined as: 1. $z$ is defined as a complex number of the form $x+y i$, where $x$ and $y$ are real numbers.</p>
<p>The following are some previous steps: Step 0: Cross-multiplying and rearranging, we get the equation $6 x=x^{2}+y^{2}$.</p>
<p>The target for next step is: The step completes the square to rewrite the equation in standard form of a circle.</p>
<p>Please try to achieve the target with the information from previous steps.
The underlined parts are the type of question, variable definition (if any), collected information from the question (if any), collected previous steps (if any) and the extracted target. Please notice that the current step does not directly rely on any information from the question, we removed the corresponding block from the input.</p>
<p>The output from the LLM is:
To rewrite the equation in standard form of a circle, we need to complete the square for both $x$ and $y$.
Starting from the given equation: $6 x=x^{2}+y^{2}$.
We add and subtract $(6 / 2)^{2}=9$ on the right-hand side:
$6 x+9-9=x^{2}+y^{2}+9-9$.
Simplifying: $(x-3)^{2}+y^{2}=9$. This is now in the standard form of a circle with center $(3,0)$ and radius 3 .</p>
<p>Then we can directly compare the regenerated step with the original one in the next stage.
Result comparison The input to the LLM is:
The following are 2 solutions to a math problem:
Solution 1: To rewrite the equation in standard form of a circle, we need to complete the square for both $x$ and $y$.
Starting from the given equation: $6 x=x^{2}+y^{2}$.
We add and subtract $(6 / 2)^{2}=9$ on the right-hand side: $6 x+9-9=x^{2}+y^{2}+9-9$.
Simplifying: $(x-3)^{2}+y^{2}=9$. This is now in the standard form of a circle with center $(3,0)$ and radius 3 .</p>
<p>Solution 2: Completing the square, we obtain $\left(x-\frac{3}{2}\right)^{2}+y^{2}=\frac{9}{4}$.
Compare the key points from both solutions step by step and then check whether Solution 1 "supports", "contradicts" or "is not directly related to" the conclusion in Solution 2. Pay special attention to difference in numbers.</p>
<p>The underlined parts are simply the problem type, the regenerated step and the original step.
The output is:
In both solutions, the goal is to rewrite the given equation in standard form of a circle. However, in Solution 1, the completion of the square results in the center of the circle being $(3,0)$ and a radius of 3 . In Solution 2, the completion of the square results in the center of the circle being $(3 / 2,0)$ and a radius of $3 / 2$.</p>
<p>Therefore, Solution 1 contradicts the conclusion in Solution 2 as the centers and radii of the circles are different.</p>
<p>If the word 'support' or 'contradict' appears in the last line of the output, SelfCheck outputs a step score 1 or -1 . Otherwise, SelfCheck outputs 0 meaning that two steps are not related.</p>
<h1>APPENDIX B EXAMPLAR FOR ONE-SHOT ERROR CHECKING</h1>
<p>The following example from Ling et al. (2023) is used to perform one-shot error checking.
Here is some information:
"1. Alice gets 2 apples.
2. Alice gets twice as many oranges as bananas.
3. Original price per apple: $\$ 2$</p>
<p>Discount: $50 \%$
Price per apple after discount: $\$ 2 <em>(1-0.50)=\$ 2 * 0.50=\$ 1 "$
Based on the given information, here is a reasoning process:
"Calculate Alice's cost of the 5 apples after a $50 \%$ discount.
Price per apple after discount: $\$ 1$
Apples be eaten: 3
Oranges be eaten: 6
Total apples: 5
Total oranges: $x$
Total bananas: $2 x$
Alice's total number of fruits: $5+x+2 x=5+4 x$
Alice's total cost of 5 apples: $\$ 1 </em>(5-3)=\$ 2 "$
Double-check the reasoning process, let's analyze its correctness, and end with "yes" or "no".
Answer:
Let's think step by step.
Grounding check
Purpose: Find references for numbers in the reasoning process sequentially
Rule: Any ungrounded number makes the check fails
"x" is a variable, we do not care about variables.
"y" is a variable, we do not care about variables.
" 5 " is a quantity from the given information that represents the number of apples.
" $50 \%$ " is a quantity from the given information that represents the discount.
"\$1" is a quantity from the given information that represents the price per apple after the discount.
" 3 " is a quantity not from the given information, it is ungrounded.
" 6 " is a quantity not from the given information, it is ungrounded.
"\$2" is a quantity from the given information that represents the original price per apple.
Result: Based on the check, there are ungrounded numbers, so the grounding check fails.
Reasoning check:
Purpose: Check whether the reasoning logic correctly answers the question
Rule: Any logic error makes the check fails
To calculate the total cost of apples after a $50 \%$ discount, we should multiply the number of apples by the discounted price. But to answer the question, we do not need to calculate the total number of fruit Alice gets.
Result: Based on the check, there are some logic mistakes, so the reasoning check fails.
Calculation check:
Purpose: Check whether the calculation process is consistent
Rule: Any inconsistent calculation makes the check fails
calculation1:
equation: $\$ 1 *(5-3)$, answer: $\$ 2$
$(5-3)=2$
$\$ 1 * 2=\$ 2$ is consistent with the answer, so the calculation is correct.
calculation2:
equation: $5+x+2 x$, answer: $5+4 x$
$\mathrm{x}+2 \mathrm{x}=3 \mathrm{x}$
$5+3 x$ is inconsistent with the answer, so the calculation is incorrect.
Result: Based on the check, the calculation process is inconsistent, so the calculation check fails.</p>
<p>Check results: Ground check fails, Reasoning check fails, Calculation check fails.
Rule: Any failed check makes the reasoning incorrect.
So the answer is "no".</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/lzloceani/verify_cot/tree/main/results/chatgpt3.5/ natural_program/MATH_np.json&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6672 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6672</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6672</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-4eb8ac1f091befb9eebb60d5b80fdac5667fd758</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4eb8ac1f091befb9eebb60d5b80fdac5667fd758" target="_blank">Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper explores different strategies for modelling numerals with language models, such as memorisation and digit-by-digit composition, and proposes a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary.</p>
                <p><strong>Paper Abstract:</strong> Numeracy is the ability to understand and work with numbers. It is a necessary skill for composing and understanding documents in clinical, scientific, and other technical domains. In this paper, we explore different strategies for modelling numerals with language models, such as memorisation and digit-by-digit composition, and propose a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary. Our evaluation on clinical and scientific datasets shows that using hierarchical models to distinguish numerals from words improves a perplexity metric on the subset of numerals by 2 and 4 orders of magnitude, respectively, over non-hierarchical models. A combination of strategies can further improve perplexity. Our continuous probability density function model reduces mean absolute percentage errors by 18% and 54% in comparison to the second best strategy for each dataset, respectively.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6672.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6672.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>softmax</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flat softmax language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard token-level LM that treats numerals as ordinary vocabulary items using a single softmax over a fixed vocabulary (with UNK tokens for OOVs). Used as a baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>softmax (token-level LSTM LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>recurrent neural network (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (token-level LSTM with D=50 hidden units)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on two domain-specific corpora constructed for this work: (1) clinical records referencing tables from London Chest Hospital; (2) scientific paragraphs from arXiv referencing tables. Vocabularies: 1,000 (clinical) and 5,000 (scientific) most frequent tokens; numerals normalized (no thousands separators, no leading zeros).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Numeral prediction on clinical and scientific corpora (this paper's datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>next-token prediction (language modeling) of numerals; also numeric-value evaluation on the number line</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>contextual natural-language next-token prediction; candidate ranking over in-vocab numbers + percentile points; regression evaluation mapping predicted numeral to numeric value</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>varied magnitudes including integers and continuous attributes; numbers in datasets range up to ~1e26 (scientific) and ~1e7 (clinical); decimals up to 3-4 places</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>none (standard LM training and conditional next-token prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Perplexity (PP), Adjusted Perplexity (APP) for numerals; regression metrics RMSE, MAE, MdAE, MAPE, MdAPE on numeric value prediction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Clinical numerals: PP=12.04, APP=58,443.72; overall token PP=4.28, APP=8.91. On numeric-value regression (clinical): RMSE=997.84, MAE=80.29, MdAE=12.70, MAPE=621.78%, MdAPE=22.41%. (See paper Table 2 & 3.)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Embeddings place numerals close to UNK and dissimilar to words when trained in a single shared space (cosine-similarity heatmaps show numerals clustered together), indicating the model tends to memorise numeral tokens rather than exploit numeric structure.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High sensitivity to out-of-vocabulary numerals (huge APP values), poor handling of open-vocabulary numerals (maps unseen numerals to UNK), lacks smoothness over numeric magnitude; regression errors large for many numbers (high MAPE).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>No model-size scaling study reported; using a flat softmax with fixed vocabulary performs poorly on APP when OOV rates are high.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6672.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6672.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>softmax+rn</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Softmax with digit-RNN token output embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Flat softmax LM that augments output scoring with character/digit-level embeddings for numerals (concatenated/compositional digit representations) while still normalising over the fixed vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>softmax+rn (token LSTM with digit/char-based output embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>recurrent neural network (LSTM) + character-level RNN for numeral embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (token-level LSTM D=50; character-RNN components used for numeral embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Same clinical and scientific corpora described above; digit character set includes digits 0-9, decimal point and EOS token.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Numeral prediction on clinical and scientific corpora (this paper's datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>next-token numeral prediction (LM)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>contextual natural-language next-token prediction with digit-aware scoring</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>same varied magnitudes as other models</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>none (trained LM with combined token and character scoring at output)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PP, APP; RMSE, MAE, MdAE, MAPE, MdAPE on number-line evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Clinical numerals: PP=11.57, APP=56,164.81; overall PP=4.21, APP=8.77. Clinical regression: RMSE=991.38, MAE=74.44, MdAE=13.00, MAPE=503.57%, MdAPE=23.91%. (See Tables 2 & 3.)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Character-aware output scoring slightly improves token perplexity vs flat softmax; digit embeddings show locality (each digit embedding similar to neighboring digits), but the model still operates in a shared token space mixing numerals and words.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still suffers from very large APP due to high OOV numerals; memorisation remains dominant since normalisation is over fixed vocab; regression errors remain large for many magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed; modest improvements over plain softmax reported, but OOV problem persists.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6672.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6672.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>h-softmax</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical softmax (words vs numerals)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hierarchical two-branch LM that first predicts token class (word vs numeral) and then predicts token within the class using separately-normalised distributions, decoupling numeral modelling from words.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>h-softmax (hierarchical softmax with separate branches for words and numerals)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>recurrent neural network (LSTM) with hierarchical output</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (D=50 hidden units; separate output embeddings for word and numeral branches)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Same clinical and scientific corpora; vocabularies as above; hierarchical branch for numerals allows separate handling of numeric tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Numeral prediction on clinical and scientific corpora (this paper's datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>next-token prediction with class-conditioned numeric generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>contextual next-token LM with class (word/numeral) prediction then within-class softmax</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>varied (integers, years, dosage values, continuous attributes)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>none (hierarchical LM training)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PP, APP; regression metrics (RMSE, MAE, MdAE, MAPE, MdAPE)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Clinical numerals: PP=11.78, APP=495.95 (much smaller APP than flat softmax). Overall clinical APP improved (overall token APP=6.05 vs 8.91 for softmax). Clinical regression: RMSE=1095.01, MAE=167.19, MdAE=14.00, MAPE=746.50%, MdAPE=25.00%. (See Tables 2 & 3.)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Separating numerals into their own embedding/output space yields embeddings that reflect numeric magnitude structure (similarities concentrated along diagonal and fanning out with magnitude), and avoids compressing all unseen numerals into UNK-like representations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Although APP is drastically improved relative to flat softmax, regression metrics indicate hierarchical softmax alone does not necessarily reduce numeric-value errors; some numerals with special symbolic meanings (years, percentiles) receive high probability spikes.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>No model scaling analysis; hierarchical separation itself yields large improvements in adjusted perplexity when OOV rates are high.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6672.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6672.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>h-softmax+rn</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical softmax with digit-aware numeral branch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hierarchical softmax model where the numeral branch additionally uses digit/character-level embeddings for numerals (combines hierarchical decoupling with digit composition).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>h-softmax+rn (hierarchical LSTM with digit-RNN for numerals)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>recurrent neural network (LSTM) + character-level RNN in numeral branch</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (D=50 hidden units; additional character RNN parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Same clinical and scientific corpora; numeral characters modelled explicitly in numeral branch.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Numeral prediction on clinical and scientific corpora (this paper's datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>next-token numeral prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>contextual LM with class prediction and digit-aware numeral decoding</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>varied magnitudes and decimal precisions</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>none</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PP, APP; RMSE, MAE, MdAE, MAPE, MdAPE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Clinical numerals: PP=11.65, APP=490.14. Clinical regression: RMSE=1001.04, MAE=83.19, MdAE=12.30, MAPE=491.85%, MdAPE=23.44%. (See Tables 2 & 3.)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Combining hierarchical class prediction with digit-aware representations yields further APP improvements vs flat models; numeral branch can exploit digit-level regularities while remaining separate from word embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still limited on numeric-value regression; APP improvements do not always translate to lower MAPE or MAE; performance varies by context (e.g., years/dosages favored).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not explored.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6672.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6672.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>d-RNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Digit-RNN (digit-by-digit numeral generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-vocabulary model that generates numerals one digit at a time by feeding the token-level hidden state into a character-level (digit-level) RNN, normalising over the small digit vocabulary and eliminating UNK for numerals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>d-RNN (token-level LSTM conditioning a character/digit-level RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>hierarchical RNNs: token-level LSTM + character-level RNN (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (token LSTM D=50; additional digit-RNN parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Same clinical and scientific corpora. Digit vocabulary includes 0-9, decimal point, EOS; open-vocabulary numeral generation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Numeral prediction on clinical and scientific corpora (this paper's datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>open-vocabulary numeral generation; next-token digit-sequence generation; numeric-value ranking and regression</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>contextual LM producing numerals as digit sequences; candidate ranking over union of in-vocab numbers and percentile points for numeric evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>handles varying decimal precision (3-4 decimals used in evaluation to cover 90% of training numerals); numbers across many scales</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>none (trained end-to-end on LM objective)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PP/APP (equivalent for open-vocab), and regression metrics RMSE, MAE, MdAE, MAPE, MdAPE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Clinical numerals: PP=263.22 (APP=263.22). Clinical regression: RMSE=1009.34, MAE=70.21, MdAE=9.00, MAPE=513.81%, MdAPE=17.90%. On scientific data MdAPE often best among models (e.g., scientific MdAPE=52.45%). (See Tables 2 & 3.)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Digit embeddings learned by the digit-RNN show locality: each digit embedding is most similar to its previous and next digit; output probability distributions for numerals have a saw-tooth pattern by decimal precision; first-digit distribution follows dataset and approximates Benford's law.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Produces saw-tooth probabilities and can struggle with large-magnitude rare values; PP for numerals higher (worse) than hierarchical softmax in token PP but APP is fair since open-vocab models have no UNK; tends to be effective at reducing median absolute percentage errors but less effective at reducing large-percentage errors compared to MoG.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not studied; d-RNN is effective at reducing median errors (MdAPE) in some datasets but not necessarily mean/large-error statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6672.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6672.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture-of-Gaussians numeral model (continuous PDF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Novel open-vocabulary approach that models numerals via a continuous probability density over real numbers using a mixture of Gaussians conditioned on the token-level hidden state, combined with a discrete approximation by integrating over a precision-dependent bin (precision r).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoG (mixture-of-Gaussians continuous numeral model + precision discretisation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>recurrent neural network (LSTM) conditioning a mixture density model over reals; pdf discretised to pmf per precision level</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (token LSTM D=50; mixture components K=256 fixed from offline EM fits)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Same clinical and scientific corpora. Mixture components (means and variances) fitted on all numbers from training data via EM and then fixed; mixture weights produced from token hidden state via softmax.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Numeral prediction on clinical and scientific corpora (this paper's datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>open-vocabulary numeral generation via continuous density; numeric-value prediction and ranking</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>contextual LM that predicts a mixture-weighted gaussian pdf over numeric values and separately models decimal precision; discrete pmf obtained by integrating pdf across rounding bin determined by precision r</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>handles continuous-valued attributes and wide numeric ranges; particularly suitable for continuous attributes not seen in vocabulary</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>none (learned mixture weights conditioned on context; means/variances pre-fit on training numbers)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PP/APP (equivalent for open-vocab), RMSE, MAE, MdAE, MAPE, MdAPE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Clinical numerals: PP=226.46 (APP=226.46). Clinical regression: RMSE=998.78, MAE=57.11, MdAE=6.92, MAPE=348.10%, MdAPE=13.64%. MoG achieved the best MAPE in both datasets and reduced MAPE by ~18% (clinical) and ~54% (scientific) relative to the second-best model (paper report).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>MoG produces smooth predictive distributions over numeric magnitude (smooth pdf-derived probabilities), reflecting the approximate-number/mental-number-line intuition; probabilities decrease rapidly with increasing decimal precision, consistent with continuous-variable theory (probability of exact value is near zero). Offline-fitting of mixture means/variances captures multiple granularities; mixture weights depend on context h_t.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tends to be wrongly selected for numerals that are labels/identifiers (e.g., catalogue indices, years) where magnitude semantics are inappropriate; had the worst MdAPE (i.e., less effective on median small-percentage errors) in some cases while reducing larger percentage errors; reliance on pre-fit Gaussian components may misrepresent label-like numerals.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>No parameter-scaling study; empirical finding: MoG reduces mean absolute percentage error substantially vs other strategies, but combination model did not outperform MoG, suggesting MoG captures magnitude information not captured by compositional digit models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6672.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6672.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>combination</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Combination (mixture-of-strategies) model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-model that mixes multiple numeral-generation strategies (hierarchical softmax, d-RNN, MoG) by learning context-dependent selection weights to choose among them for generating numerals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>combination (mixture of h-softmax, d-RNN, MoG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>recurrent neural network (LSTM) with a learned mixture over numeral-generation modules</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (D=50 hidden; additional parameters A in selection softmax over strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Same clinical and scientific corpora; selection weights A learned to depend on context hidden state h_t</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Numeral prediction on clinical and scientific corpora (this paper's datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>next-token prediction with mixture-of-strategies for numerals; numeric-value prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>contextual LM selecting among strategies via softmax over learned strategy logits</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>varied; aims to leverage complementary strengths of different numeral models</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>none (trained end-to-end to mix strategies via learned weights)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PP/APP; RMSE, MAE, MdAE, MAPE, MdAPE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Clinical numerals: PP=197.59 (APP=197.59). Clinical regression: RMSE=989.84, MAE=69.47, MdAE=9.00, MAPE=552.06%, MdAPE=17.86%. Combination had best overall APP on both datasets but did not outperform MoG on MAPE.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Combination model's selection weights reveal strategy specialisation: h-softmax chosen for years/dosages/integers with special function, d-RNN for two-digit integers/dimensions, MoG for continuous attributes and out-of-vocabulary continuous values; model visualizations and examples in the paper document these selection patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Mixture selection sometimes chooses MoG for label-like numerals (catalogue indices) where magnitude semantics are inappropriate, highlighting limitation of numeric-only evaluation; overall the combination did not surpass MoG on MAPE, indicating imperfect gating/selection.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>No explicit scaling analysis; mixing strategies yields best APP but not necessarily best numeric-value regression performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving general arithmetic word problems <em>(Rating: 2)</em></li>
                <li>Learning to solve arithmetic word problems <em>(Rating: 2)</em></li>
                <li>Learning to solve algebra word problems by semantic parsing <em>(Rating: 1)</em></li>
                <li>Generating sequences with recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Hierarchical probabilistic neural network language model <em>(Rating: 2)</em></li>
                <li>Gated word-character recurrent language model <em>(Rating: 2)</em></li>
                <li>Pointer networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6672",
    "paper_id": "paper-4eb8ac1f091befb9eebb60d5b80fdac5667fd758",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "softmax",
            "name_full": "Flat softmax language model",
            "brief_description": "Standard token-level LM that treats numerals as ordinary vocabulary items using a single softmax over a fixed vocabulary (with UNK tokens for OOVs). Used as a baseline in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "softmax (token-level LSTM LM)",
            "model_family": "recurrent neural network (LSTM)",
            "model_size": "not reported (token-level LSTM with D=50 hidden units)",
            "training_data_description": "Trained on two domain-specific corpora constructed for this work: (1) clinical records referencing tables from London Chest Hospital; (2) scientific paragraphs from arXiv referencing tables. Vocabularies: 1,000 (clinical) and 5,000 (scientific) most frequent tokens; numerals normalized (no thousands separators, no leading zeros).",
            "benchmark_name": "Numeral prediction on clinical and scientific corpora (this paper's datasets)",
            "task_type": "next-token prediction (language modeling) of numerals; also numeric-value evaluation on the number line",
            "problem_format": "contextual natural-language next-token prediction; candidate ranking over in-vocab numbers + percentile points; regression evaluation mapping predicted numeral to numeric value",
            "difficulty_level": "varied magnitudes including integers and continuous attributes; numbers in datasets range up to ~1e26 (scientific) and ~1e7 (clinical); decimals up to 3-4 places",
            "prompting_method": "none (standard LM training and conditional next-token prediction)",
            "performance_metric": "Perplexity (PP), Adjusted Perplexity (APP) for numerals; regression metrics RMSE, MAE, MdAE, MAPE, MdAPE on numeric value prediction",
            "performance_value": "Clinical numerals: PP=12.04, APP=58,443.72; overall token PP=4.28, APP=8.91. On numeric-value regression (clinical): RMSE=997.84, MAE=80.29, MdAE=12.70, MAPE=621.78%, MdAPE=22.41%. (See paper Table 2 & 3.)",
            "internal_analysis": "Embeddings place numerals close to UNK and dissimilar to words when trained in a single shared space (cosine-similarity heatmaps show numerals clustered together), indicating the model tends to memorise numeral tokens rather than exploit numeric structure.",
            "failure_modes": "High sensitivity to out-of-vocabulary numerals (huge APP values), poor handling of open-vocabulary numerals (maps unseen numerals to UNK), lacks smoothness over numeric magnitude; regression errors large for many numbers (high MAPE).",
            "scaling_trend": "No model-size scaling study reported; using a flat softmax with fixed vocabulary performs poorly on APP when OOV rates are high.",
            "uuid": "e6672.0",
            "source_info": {
                "paper_title": "Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "softmax+rn",
            "name_full": "Softmax with digit-RNN token output embeddings",
            "brief_description": "Flat softmax LM that augments output scoring with character/digit-level embeddings for numerals (concatenated/compositional digit representations) while still normalising over the fixed vocabulary.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "softmax+rn (token LSTM with digit/char-based output embeddings)",
            "model_family": "recurrent neural network (LSTM) + character-level RNN for numeral embeddings",
            "model_size": "not reported (token-level LSTM D=50; character-RNN components used for numeral embeddings)",
            "training_data_description": "Same clinical and scientific corpora described above; digit character set includes digits 0-9, decimal point and EOS token.",
            "benchmark_name": "Numeral prediction on clinical and scientific corpora (this paper's datasets)",
            "task_type": "next-token numeral prediction (LM)",
            "problem_format": "contextual natural-language next-token prediction with digit-aware scoring",
            "difficulty_level": "same varied magnitudes as other models",
            "prompting_method": "none (trained LM with combined token and character scoring at output)",
            "performance_metric": "PP, APP; RMSE, MAE, MdAE, MAPE, MdAPE on number-line evaluation",
            "performance_value": "Clinical numerals: PP=11.57, APP=56,164.81; overall PP=4.21, APP=8.77. Clinical regression: RMSE=991.38, MAE=74.44, MdAE=13.00, MAPE=503.57%, MdAPE=23.91%. (See Tables 2 & 3.)",
            "internal_analysis": "Character-aware output scoring slightly improves token perplexity vs flat softmax; digit embeddings show locality (each digit embedding similar to neighboring digits), but the model still operates in a shared token space mixing numerals and words.",
            "failure_modes": "Still suffers from very large APP due to high OOV numerals; memorisation remains dominant since normalisation is over fixed vocab; regression errors remain large for many magnitudes.",
            "scaling_trend": "Not analyzed; modest improvements over plain softmax reported, but OOV problem persists.",
            "uuid": "e6672.1",
            "source_info": {
                "paper_title": "Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "h-softmax",
            "name_full": "Hierarchical softmax (words vs numerals)",
            "brief_description": "Hierarchical two-branch LM that first predicts token class (word vs numeral) and then predicts token within the class using separately-normalised distributions, decoupling numeral modelling from words.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "h-softmax (hierarchical softmax with separate branches for words and numerals)",
            "model_family": "recurrent neural network (LSTM) with hierarchical output",
            "model_size": "not reported (D=50 hidden units; separate output embeddings for word and numeral branches)",
            "training_data_description": "Same clinical and scientific corpora; vocabularies as above; hierarchical branch for numerals allows separate handling of numeric tokens.",
            "benchmark_name": "Numeral prediction on clinical and scientific corpora (this paper's datasets)",
            "task_type": "next-token prediction with class-conditioned numeric generation",
            "problem_format": "contextual next-token LM with class (word/numeral) prediction then within-class softmax",
            "difficulty_level": "varied (integers, years, dosage values, continuous attributes)",
            "prompting_method": "none (hierarchical LM training)",
            "performance_metric": "PP, APP; regression metrics (RMSE, MAE, MdAE, MAPE, MdAPE)",
            "performance_value": "Clinical numerals: PP=11.78, APP=495.95 (much smaller APP than flat softmax). Overall clinical APP improved (overall token APP=6.05 vs 8.91 for softmax). Clinical regression: RMSE=1095.01, MAE=167.19, MdAE=14.00, MAPE=746.50%, MdAPE=25.00%. (See Tables 2 & 3.)",
            "internal_analysis": "Separating numerals into their own embedding/output space yields embeddings that reflect numeric magnitude structure (similarities concentrated along diagonal and fanning out with magnitude), and avoids compressing all unseen numerals into UNK-like representations.",
            "failure_modes": "Although APP is drastically improved relative to flat softmax, regression metrics indicate hierarchical softmax alone does not necessarily reduce numeric-value errors; some numerals with special symbolic meanings (years, percentiles) receive high probability spikes.",
            "scaling_trend": "No model scaling analysis; hierarchical separation itself yields large improvements in adjusted perplexity when OOV rates are high.",
            "uuid": "e6672.2",
            "source_info": {
                "paper_title": "Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "h-softmax+rn",
            "name_full": "Hierarchical softmax with digit-aware numeral branch",
            "brief_description": "Hierarchical softmax model where the numeral branch additionally uses digit/character-level embeddings for numerals (combines hierarchical decoupling with digit composition).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "h-softmax+rn (hierarchical LSTM with digit-RNN for numerals)",
            "model_family": "recurrent neural network (LSTM) + character-level RNN in numeral branch",
            "model_size": "not reported (D=50 hidden units; additional character RNN parameters)",
            "training_data_description": "Same clinical and scientific corpora; numeral characters modelled explicitly in numeral branch.",
            "benchmark_name": "Numeral prediction on clinical and scientific corpora (this paper's datasets)",
            "task_type": "next-token numeral prediction",
            "problem_format": "contextual LM with class prediction and digit-aware numeral decoding",
            "difficulty_level": "varied magnitudes and decimal precisions",
            "prompting_method": "none",
            "performance_metric": "PP, APP; RMSE, MAE, MdAE, MAPE, MdAPE",
            "performance_value": "Clinical numerals: PP=11.65, APP=490.14. Clinical regression: RMSE=1001.04, MAE=83.19, MdAE=12.30, MAPE=491.85%, MdAPE=23.44%. (See Tables 2 & 3.)",
            "internal_analysis": "Combining hierarchical class prediction with digit-aware representations yields further APP improvements vs flat models; numeral branch can exploit digit-level regularities while remaining separate from word embeddings.",
            "failure_modes": "Still limited on numeric-value regression; APP improvements do not always translate to lower MAPE or MAE; performance varies by context (e.g., years/dosages favored).",
            "scaling_trend": "Not explored.",
            "uuid": "e6672.3",
            "source_info": {
                "paper_title": "Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "d-RNN",
            "name_full": "Digit-RNN (digit-by-digit numeral generation)",
            "brief_description": "Open-vocabulary model that generates numerals one digit at a time by feeding the token-level hidden state into a character-level (digit-level) RNN, normalising over the small digit vocabulary and eliminating UNK for numerals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "d-RNN (token-level LSTM conditioning a character/digit-level RNN)",
            "model_family": "hierarchical RNNs: token-level LSTM + character-level RNN (LSTM)",
            "model_size": "not reported (token LSTM D=50; additional digit-RNN parameters)",
            "training_data_description": "Same clinical and scientific corpora. Digit vocabulary includes 0-9, decimal point, EOS; open-vocabulary numeral generation.",
            "benchmark_name": "Numeral prediction on clinical and scientific corpora (this paper's datasets)",
            "task_type": "open-vocabulary numeral generation; next-token digit-sequence generation; numeric-value ranking and regression",
            "problem_format": "contextual LM producing numerals as digit sequences; candidate ranking over union of in-vocab numbers and percentile points for numeric evaluation",
            "difficulty_level": "handles varying decimal precision (3-4 decimals used in evaluation to cover 90% of training numerals); numbers across many scales",
            "prompting_method": "none (trained end-to-end on LM objective)",
            "performance_metric": "PP/APP (equivalent for open-vocab), and regression metrics RMSE, MAE, MdAE, MAPE, MdAPE",
            "performance_value": "Clinical numerals: PP=263.22 (APP=263.22). Clinical regression: RMSE=1009.34, MAE=70.21, MdAE=9.00, MAPE=513.81%, MdAPE=17.90%. On scientific data MdAPE often best among models (e.g., scientific MdAPE=52.45%). (See Tables 2 & 3.)",
            "internal_analysis": "Digit embeddings learned by the digit-RNN show locality: each digit embedding is most similar to its previous and next digit; output probability distributions for numerals have a saw-tooth pattern by decimal precision; first-digit distribution follows dataset and approximates Benford's law.",
            "failure_modes": "Produces saw-tooth probabilities and can struggle with large-magnitude rare values; PP for numerals higher (worse) than hierarchical softmax in token PP but APP is fair since open-vocab models have no UNK; tends to be effective at reducing median absolute percentage errors but less effective at reducing large-percentage errors compared to MoG.",
            "scaling_trend": "Not studied; d-RNN is effective at reducing median errors (MdAPE) in some datasets but not necessarily mean/large-error statistics.",
            "uuid": "e6672.4",
            "source_info": {
                "paper_title": "Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "MoG",
            "name_full": "Mixture-of-Gaussians numeral model (continuous PDF)",
            "brief_description": "Novel open-vocabulary approach that models numerals via a continuous probability density over real numbers using a mixture of Gaussians conditioned on the token-level hidden state, combined with a discrete approximation by integrating over a precision-dependent bin (precision r).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MoG (mixture-of-Gaussians continuous numeral model + precision discretisation)",
            "model_family": "recurrent neural network (LSTM) conditioning a mixture density model over reals; pdf discretised to pmf per precision level",
            "model_size": "not reported (token LSTM D=50; mixture components K=256 fixed from offline EM fits)",
            "training_data_description": "Same clinical and scientific corpora. Mixture components (means and variances) fitted on all numbers from training data via EM and then fixed; mixture weights produced from token hidden state via softmax.",
            "benchmark_name": "Numeral prediction on clinical and scientific corpora (this paper's datasets)",
            "task_type": "open-vocabulary numeral generation via continuous density; numeric-value prediction and ranking",
            "problem_format": "contextual LM that predicts a mixture-weighted gaussian pdf over numeric values and separately models decimal precision; discrete pmf obtained by integrating pdf across rounding bin determined by precision r",
            "difficulty_level": "handles continuous-valued attributes and wide numeric ranges; particularly suitable for continuous attributes not seen in vocabulary",
            "prompting_method": "none (learned mixture weights conditioned on context; means/variances pre-fit on training numbers)",
            "performance_metric": "PP/APP (equivalent for open-vocab), RMSE, MAE, MdAE, MAPE, MdAPE",
            "performance_value": "Clinical numerals: PP=226.46 (APP=226.46). Clinical regression: RMSE=998.78, MAE=57.11, MdAE=6.92, MAPE=348.10%, MdAPE=13.64%. MoG achieved the best MAPE in both datasets and reduced MAPE by ~18% (clinical) and ~54% (scientific) relative to the second-best model (paper report).",
            "internal_analysis": "MoG produces smooth predictive distributions over numeric magnitude (smooth pdf-derived probabilities), reflecting the approximate-number/mental-number-line intuition; probabilities decrease rapidly with increasing decimal precision, consistent with continuous-variable theory (probability of exact value is near zero). Offline-fitting of mixture means/variances captures multiple granularities; mixture weights depend on context h_t.",
            "failure_modes": "Tends to be wrongly selected for numerals that are labels/identifiers (e.g., catalogue indices, years) where magnitude semantics are inappropriate; had the worst MdAPE (i.e., less effective on median small-percentage errors) in some cases while reducing larger percentage errors; reliance on pre-fit Gaussian components may misrepresent label-like numerals.",
            "scaling_trend": "No parameter-scaling study; empirical finding: MoG reduces mean absolute percentage error substantially vs other strategies, but combination model did not outperform MoG, suggesting MoG captures magnitude information not captured by compositional digit models.",
            "uuid": "e6672.5",
            "source_info": {
                "paper_title": "Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "combination",
            "name_full": "Combination (mixture-of-strategies) model",
            "brief_description": "A meta-model that mixes multiple numeral-generation strategies (hierarchical softmax, d-RNN, MoG) by learning context-dependent selection weights to choose among them for generating numerals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "combination (mixture of h-softmax, d-RNN, MoG)",
            "model_family": "recurrent neural network (LSTM) with a learned mixture over numeral-generation modules",
            "model_size": "not reported (D=50 hidden; additional parameters A in selection softmax over strategies)",
            "training_data_description": "Same clinical and scientific corpora; selection weights A learned to depend on context hidden state h_t",
            "benchmark_name": "Numeral prediction on clinical and scientific corpora (this paper's datasets)",
            "task_type": "next-token prediction with mixture-of-strategies for numerals; numeric-value prediction",
            "problem_format": "contextual LM selecting among strategies via softmax over learned strategy logits",
            "difficulty_level": "varied; aims to leverage complementary strengths of different numeral models",
            "prompting_method": "none (trained end-to-end to mix strategies via learned weights)",
            "performance_metric": "PP/APP; RMSE, MAE, MdAE, MAPE, MdAPE",
            "performance_value": "Clinical numerals: PP=197.59 (APP=197.59). Clinical regression: RMSE=989.84, MAE=69.47, MdAE=9.00, MAPE=552.06%, MdAPE=17.86%. Combination had best overall APP on both datasets but did not outperform MoG on MAPE.",
            "internal_analysis": "Combination model's selection weights reveal strategy specialisation: h-softmax chosen for years/dosages/integers with special function, d-RNN for two-digit integers/dimensions, MoG for continuous attributes and out-of-vocabulary continuous values; model visualizations and examples in the paper document these selection patterns.",
            "failure_modes": "Mixture selection sometimes chooses MoG for label-like numerals (catalogue indices) where magnitude semantics are inappropriate, highlighting limitation of numeric-only evaluation; overall the combination did not surpass MoG on MAPE, indicating imperfect gating/selection.",
            "scaling_trend": "No explicit scaling analysis; mixing strategies yields best APP but not necessarily best numeric-value regression performance.",
            "uuid": "e6672.6",
            "source_info": {
                "paper_title": "Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers",
                "publication_date_yy_mm": "2018-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving general arithmetic word problems",
            "rating": 2
        },
        {
            "paper_title": "Learning to solve arithmetic word problems",
            "rating": 2
        },
        {
            "paper_title": "Learning to solve algebra word problems by semantic parsing",
            "rating": 1
        },
        {
            "paper_title": "Generating sequences with recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Hierarchical probabilistic neural network language model",
            "rating": 2
        },
        {
            "paper_title": "Gated word-character recurrent language model",
            "rating": 2
        },
        {
            "paper_title": "Pointer networks",
            "rating": 1
        }
    ],
    "cost": 0.01828975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers</h1>
<p>Georgios P. Spithourakis<br>Department of Computer Science<br>University College London<br>g.spithourakis@cs.ucl.ac.uk</p>
<p>Sebastian Riedel<br>Department of Computer Science<br>University College London<br>s.riedel@cs.ucl.ac.uk</p>
<h4>Abstract</h4>
<p>Numeracy is the ability to understand and work with numbers. It is a necessary skill for composing and understanding documents in clinical, scientific, and other technical domains. In this paper, we explore different strategies for modelling numerals with language models, such as memorisation and digit-by-digit composition, and propose a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary. Our evaluation on clinical and scientific datasets shows that using hierarchical models to distinguish numerals from words improves a perplexity metric on the subset of numerals by 2 and 4 orders of magnitude, respectively, over nonhierarchical models. A combination of strategies can further improve perplexity. Our continuous probability density function model reduces mean absolute percentage errors by $18 \%$ and $54 \%$ in comparison to the second best strategy for each dataset, respectively.</p>
<h2>1 Introduction</h2>
<p>Language models (LMs) are statistical models that assign a probability over sequences of words. Language models can often help with other tasks, such as speech recognition (Mikolov et al., 2010; Prabhavalkar et al., 2017), machine translation (Luong et al., 2015; Glehre et al., 2017), text summarisation (Filippova et al., 2015; Gambhir and Gupta, 2017), question answering (Wang et al., 2017), semantic error detection (Rei and Yannakoudakis, 2017; Spithourakis et al., 2016a), and fact checking (Rashkin et al., 2017).</p>
<p>Numeracy and literacy refer to the ability to comprehend, use, and attach meaning to numbers and words, respectively. Language models exhibit literacy by being able to assign higher probabilities to sentences that
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Modelling numerals with a categorical distribution over a fixed vocabulary maps all out-ofvocabulary numerals to the same type, e.g. UNK, and does not reflect the smoothness of the underlying continuous distribution of certain attributes.
are both grammatical and realistic, as in this example:
'I eat an apple' (grammatical and realistic)
'An apple eats me' (unrealistic)
'I eats an apple' (ungrammatical)
Likewise, a numerate language model should be able to rank numerical claims based on plausibility:
'John's height is 1.75 metres' (realistic)
'John's height is 999.999 metres' (unrealistic)
Existing approaches to language modelling treat numerals similarly to other words, typically using categorical distributions over a fixed vocabulary.</p>
<p>However, this maps all unseen numerals to the same unknown type and ignores the smoothness of continuous attributes, as shown in Figure 1. In that respect, existing work on language modelling does not explicitly evaluate or optimise for numeracy. Numerals are often neglected and low-resourced, e.g. they are often masked (Mitchell and Lapata, 2009), and there are only $15,164(3.79 \%)$ numerals among GloVe's 400,000 embeddings pretrained on 6 billion tokens (Pennington et al., 2014). Yet, numbers appear ubiquitously, from children's magazines (Joram et al., 1995) to clinical reports (Bigeard et al., 2015), and grant objectivity to sciences (Porter, 1996).</p>
<p>Previous work finds that numerals have higher out-of-vocabulary rates than other words and proposes solutions for representing unseen numerals as inputs to language models, e.g. using numerical magnitudes as features (Spithourakis et al., 2016b,a). Such work identifies that the perplexity of language models on the subset of numerals can be very high, but does not directly address the issue. This paper focuses on evaluating and improving the ability of language models to predict numerals. The main contributions of this paper are as follows:</p>
<ol>
<li>We explore different strategies for modelling numerals, such as memorisation and digit-bydigit composition, and propose a novel neural architecture based on continuous probability density functions.</li>
<li>We propose the use of evaluations that adjust for the high out-of-vocabulary rate of numerals and account for their numerical value (magnitude).</li>
<li>We evaluate on a clinical and a scientific corpus and provide a qualitative analysis of learnt representations and model predictions. We find that modelling numerals separately from other words can drastically improve the perplexity of LMs, that different strategies for modelling numerals are suitable for different textual contexts, and that continuous probability density functions can improve the LM's prediction accuracy for numbers.</li>
</ol>
<h2>2 Language Models</h2>
<p>Let $s_{1}, s_{2}, \ldots, s_{L}$ denote a document, where $s_{t}$ is the token at position $t$. A language model estimates the probability of the next token given previous tokens, i.e. $p\left(s_{t} \mid s_{1}, \ldots, s_{t-1}\right)$. Neural LMs estimate this probability by feeding embeddings, i.e. vectors that represent each token, into a Recurrent Neural Network (RNN) (Mikolov et al., 2010).</p>
<p>Token Embeddings Tokens are most commonly represented by a $D$-dimensional dense vector that is unique for each word from a vocabulary $\mathcal{V}$ of known words. This vocabulary includes special symbols (e.g. 'UNK') to handle out-of-vocabulary tokens, such as unseen words or numerals. Let $w_{s}$ be the one-hot representation of token $s$, i.e. a sparse binary vector with a single element set to 1 for that token's index in the vocabulary, and $E \in \mathbb{R}^{D \times|\mathcal{V}|}$ be the token embeddings matrix. The token embedding for $s$ is the vector $e_{s}^{\text {token }}=E w_{s}$.</p>
<p>Character-Based Embeddings A representation for a token can be build from its constituent characters (Luong and Manning, 2016; Santos and Zadrozny, 2014). Such a representation takes into account the internal structure of tokens. Let $d_{1}, d_{2}, \ldots, d_{N}$ be the characters of token $s$. A character-based embedding for $s$ is the final hidden state of a $D$-dimensional character-level RNN: $e_{s}^{\text {chars }}=$ RNN $\left(d_{0}, d_{1}, \ldots d_{L}\right)$.</p>
<p>Recurrent and Output Layer The computation of the conditional probability of the next token involves recursively feeding the embedding of the current token $e_{s_{t}}$ and the previous hidden state $h_{t-1}$ into a $D$-dimensional token-level RNN to obtain the current hidden state $h_{t}$. The output probability is estimated using the softmax function, i.e.</p>
<p>$$
\begin{gathered}
p\left(s_{t} \mid h_{t}\right)=\operatorname{softmax}\left(\psi\left(s_{t}\right)\right)=\frac{1}{Z} e^{\psi\left(s_{t}\right)} \
Z=\sum_{s^{\prime} \in \mathcal{V}} e^{\psi\left(s^{\prime}\right)}
\end{gathered}
$$</p>
<p>where $\psi($.$) is a score function.$
Training and Evaluation Neural LMs are typically trained to minimise the cross entropy on the training corpus:</p>
<p>$$
\mathcal{H}<em s__t="s_{t">{\text {train }}=-\frac{1}{N} \sum</em>\right)
$$} \in \text { train }} \log p\left(s_{t} \mid s_{&lt;t</p>
<p>A common performance metric for LMs is per token perplexity (Eq. 3), evaluated on a test corpus. It can also be interpreted as the branching factor: the size of an equally weighted distribution with equivalent uncertainty, i.e. how many sides you need on a fair die to get the same uncertainty as the model distribution.</p>
<p>$$
P P_{\text {test }}=\exp \left(\mathcal{H}_{\text {test }}\right)
$$</p>
<h2>3 Strategies for Modelling Numerals</h2>
<p>In this section we describe models with different strategies for generating numerals and propose the</p>
<p>use of number-specific evaluation metrics that adjust for the high out-of-vocabulary rate of numerals and account for numerical values. We draw inspiration from theories of numerical cognition. The triple code theory (Dehaene et al., 2003) postulates that humans process quantities through two exact systems (verbal and visual) and one approximate number system that semantically represents a number on a mental number line. Tzelgov et al. (2015) identify two classes of numbers: i) primitives, which are holistically retrieved from long-term memory; and ii) non-primitives, which are generated online. An in-depth review of numerical and mathematical cognition can be found in Kadosh and Dowker (2015) and Campbell (2005).</p>
<h3>3.1 Softmax Model and Variants</h3>
<p>This class of models assumes that numerals come from a finite vocabulary that can be memorised and retrieved later. The softmax model treats all tokens (words and numerals) alike and directly uses Equation 1 with score function:</p>
<p>$$
\psi\left(s_{t}\right)=h_{t}^{T} e_{s_{t}}^{\text {token }}=h_{t}^{T} E_{\text {out }} w_{s_{t}}
$$</p>
<p>where $E_{\text {out }} \in \mathbb{R}^{D \times|V|}$ is an output embeddings matrix. The summation in Equation 1 is over the complete target vocabulary, which requires mapping any out-of-vocabulary tokens to special symbols, e.g. ' $\mathrm{UNK}<em _numeral="{numeral" _text="\text">{\text {word }}$ ' and ' $\mathrm{UNK}</em>$ '.}</p>
<p>Softmax with Digit-Based Embeddings The softmax $+r n n$ variant considers the internal syntax of a numeral's digits by adjusting the score function:</p>
<p>$$
\begin{aligned}
\psi\left(s_{t}\right) &amp; =h_{t}^{T} e_{s_{t}}^{\text {token }}+h_{t}^{T} e_{s_{t}}^{\text {chars }} \
&amp; =h_{t}^{T} E_{\text {out }} w_{s_{t}}+h_{t}^{T} E_{\text {out }}^{\mathrm{RNN}} w_{s_{t}}
\end{aligned}
$$</p>
<p>where the columns of $E_{\text {out }}^{\text {RNN }}$ are composed of character-based embeddings for in-vocabulary numerals and token embeddings for the remaining vocabulary. The character set comprises digits (0-9), the decimal point, and an end-of-sequence character. The model still requires normalisation over the whole vocabulary, and the special unknown tokens are still needed.</p>
<p>Hierarchical Softmax A hierarchical softmax (Morin and Bengio, 2005a) can help us decouple the modelling of numerals from that of words. The probability of the next token $s_{t}$ is decomposed to that of its class $c_{t}$ and the probability of the exact token from within the class:</p>
<p>$$
\begin{gathered}
p\left(s_{t} \mid h_{t}\right)=\sum_{c_{t} \in C} p\left(c_{t} \mid h_{t}\right) p\left(s_{t} \mid c_{t}, h_{t}\right) \
p\left(c_{t} \mid h_{t}\right)=\sigma\left(h_{t}^{T} b\right)
\end{gathered}
$$</p>
<p>where the valid token classes are $C=$ {word, numeral}, $\sigma$ is the sigmoid function and $b$ is a $D$-dimensional vector. Each of the two branches of $p\left(s_{t} \mid c_{t}, h_{t}\right)$ can now be modelled by independently normalised distributions. The hierarchical variants ( $h$-softmax and $h$-softmax $+r n n$ ) use two independent softmax distributions for words and numerals. The two branches share no parameters, and thus words and numerals will be embedded into separate spaces.</p>
<p>The hierarchical approach allows us to use any well normalised distribution to model each of its branches. In the next subsections, we examine different strategies for modelling the branch of numerals, i.e. $p\left(s_{t} \mid c_{t}=\right.$ numeral,$\left.h_{t}\right)$. For simplicity, we will abbreviate this to $p(s)$.</p>
<h3>3.2 Digit-RNN Model</h3>
<p>Let $d_{1}, d_{2} \ldots d_{N}$ be the digits of numeral $s$. A digit-bydigit composition strategy estimates the probability of the numeral from the probabilities of its digits:</p>
<p>$$
p(s)=p\left(d_{1}\right) p\left(d_{2} \mid d_{1}\right) \ldots p\left(d_{N} \mid d_{&lt;N}\right)
$$</p>
<p>The $d-R N N$ model feeds the hidden state $h_{t}$ of the token-level RNN into a character-level RNN (Graves, 2013; Sutskever et al., 2011) to estimate this probability. This strategy can accommodate an open vocabulary, i.e. it eliminates the need for an $\mathrm{UNK}_{\text {numeral }}$ symbol, as the probability is normalised one digit at a time over the much smaller vocabulary of digits (digits 0-9, decimal separator, and end-of-sequence).</p>
<h3>3.3 Mixture of Gaussians Model</h3>
<p>Inspired by the approximate number system and the mental number line (Dehaene et al., 2003), our proposed MoG model computes the probability of numerals from a probability density function (pdf) over real numbers, using a mixture of Gaussians for the underlying pdf:</p>
<p>$$
\begin{aligned}
q(v) &amp; =\sum_{k=1}^{K} \pi_{k} \mathcal{N}<em k="k">{k}\left(v ; \mu</em>\right) \
\pi_{k} &amp; =\operatorname{softmax}\left(B^{T} h_{t}\right)
\end{aligned}
$$}, \sigma_{k}^{2</p>
<p>where $K$ is the number of components, $\pi_{k}$ are mixture weights that depend on hidden state $h_{t}$ of the token-level RNN, $\mathcal{N}<em k="k">{k}$ is the pdf of the normal distribution with mean $\mu</em>$ is a matrix.} \in \mathbb{R}$ and variance $\sigma_{k}^{2} \in \mathbb{R}$, and $B \in \mathbb{R}^{D \times K</p>
<p>The difficulty with this approach is that for any continuous random variable, the probability that it equals a specific value is always zero. To resolve this,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Mixture of Gaussians model. The probability of a numeral is decomposed into the probability of its decimal precision and the probability that an underlying number will produce the numeral when rounded at the given precision.
we consider a probability mass function (pmf) that discretely approximates the pdf:</p>
<p>$$
\widetilde{Q}(v \mid r)=\int_{v-\epsilon_{r}}^{v+\epsilon_{r}} q(u) d u=F\left(v+\epsilon_{r}\right)-F\left(v-\epsilon_{r}\right)
$$</p>
<p>where $F($.$) is the cumulative density function of q($.$) ,$ and $\epsilon_{r}=0.5 \times 10^{-r}$ is the number's precision. The level of discretisation $r$, i.e. how many decimal digits to keep, is a random variable in $\mathbb{N}$ with distribution $p(r)$. The mixed joint density is:</p>
<p>$$
p(s)=p(v, r)=p(r) \widetilde{Q}(v \mid r)
$$</p>
<p>Figure 2 summarises this strategy, where we model the level of discretisation by converting the numeral into a pattern and use a RNN to estimate the probability of that pattern sequence:</p>
<p>$$
p(r)=p(\text { SOS INT_PART } . \overbrace{\mathrm{d} \ldots \backslash \mathrm{~d}}^{\text {r decimal digits }} \text { EOS })
$$</p>
<h3>3.4 Combination of Strategies</h3>
<p>Different mechanisms might be better for predicting numerals in different contexts. We propose a combination model that can select among different
strategies for modelling numerals:</p>
<p>$$
\begin{aligned}
p(s) &amp; =\sum_{\forall m \in M} \alpha_{m} p(s \mid m) \
\alpha_{m} &amp; =\operatorname{softmax}\left(A^{T} h_{t}\right)
\end{aligned}
$$</p>
<p>where $\mathrm{M}={$ h-softmax, d-RNN, MoG}, and $A \in \mathbb{R}^{D \times|M|}$. Since both d-RNN and MoG are openvocabulary models, the unknown numeral token can now be removed from the vocabulary of h-softmax.</p>
<h3>3.5 Evaluating the Numeracy of LMs</h3>
<p>Numeracy skills are centred around the understanding of numbers and numerals. A number is a mathematical object with a specific magnitude, whereas a numeral is its symbolic representation, usually in the positional decimal Hindu-Arabic numeral system (McCloskey and Macaruso, 1995). In humans, the link between numerals and their numerical values boosts numerical skills (Griffin et al., 1995).</p>
<p>Perplexity Evaluation Test perplexity evaluated only on numerals will be informative of the symbolic component of numeracy. However, model comparisons based on naive evaluation using Equation 3 might be problematic: perplexity is sensitive to out-of-vocabulary (OOV) rate, which might differ among models, e.g. it is zero for open-vocabulary models. As an extreme example, in a document where all words are out of vocabulary, the best perplexity is achieved by a trivial model that predicts everything as unknown.</p>
<p>Ueberla (1994) proposed Adjusted Perplexity (APP; Eq. 14), also known as unknown-penalised perplexity (Ahn et al., 2016), to cancel the effect of the out-of-vocabulary rate on perplexity. The APP is the perplexity of an adjusted model that uniformly redistributes the probability of each out-of-vocabulary class over all different types in that class:</p>
<p>$$
p^{\prime}(s)= \begin{cases}p(s) \frac{1}{\mid O O V_{c} \mid} &amp; \text { if } s \in O O V_{c} \ p(s) &amp; \text { otherwise }\end{cases}
$$</p>
<p>where $O O V_{c}$ is an out-of-vocabulary class (e.g. words and numerals), and $\left|O O V_{c}\right|$ is the cardinality of each OOV set. Equivalently, adjusted perplexity can be calculated as:</p>
<p>$$
\begin{gathered}
A P P_{\text {test }}=\exp \left(\mathcal{H}<em c="c">{\text {test }}+\sum</em>} \mathcal{H<em _adjust="{adjust" _text="\text">{\text {adjust }}^{c}\right) \
\mathcal{H}</em>
\end{gathered}
$$}}^{c}=-\sum_{t} \frac{\left|s_{t} \in O O V_{c}\right|}{N} \log \frac{1}{\left|O O V_{c}\right|</p>
<p>where $N$ is the total number of tokens in the test set and $\left|s \in O O V_{c}\right|$ is the count of tokens from the test set belonging in each OOV set.</p>
<p>Evaluation on the Number Line While perplexity looks at symbolic performance on numerals, this evaluation focuses on numbers and particularly on their numerical value, which is their most prominent semantic content (Dehaene et al., 2003; Dehaene and Cohen, 1995).</p>
<p>Let $v_{t}$ be the numerical value of token $s_{t}$ from the test corpus. Also, let $\hat{v}<em t="t">{t}$ be the value of the most probable numeral under the model $s</em>\right)\right)$. Any evaluation metric from the regression literature can be used to measure the models performance. To evaluate on the number line, we can use any evaluation metric from the regression literature. In reverse order of tolerance to extreme errors, some of the most popular are Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Median Absolute Error (MdAE):}=\operatorname{argmax}\left(p\left(s_{t} \mid h_{t}, c_{t}=\operatorname{num</p>
<p>$$
\begin{aligned}
e_{i} &amp; =v_{i}-\hat{v_{i}} \
R M S E &amp; =\sqrt{\frac{1}{N} \sum_{i=1}^{N} e_{i}^{2}} \
M A E &amp; =\frac{1}{N} \sum_{i=1}^{N}\left|e_{i}\right| \
M d A E &amp; =\operatorname{median}\left{\left|e_{i}\right|\right}
\end{aligned}
$$</p>
<p>The above are sensitive to the scale of the data. If the data contains values from different scales, percentage metrics are often preferred, such as the Mean/Median Absolute Percentage Error (MAPE/MdAPE):</p>
<p>$$
\begin{aligned}
p e_{i} &amp; =\frac{v_{i}-\hat{v_{i}}}{e_{i}} \
M A P E &amp; =\frac{1}{N} \sum_{i=1}^{N}\left|p e_{i}\right| \
M d A P E &amp; =\operatorname{median}\left{\left|p e_{i}\right|\right}
\end{aligned}
$$</p>
<h2>4 Data</h2>
<p>To evaluate our models, we created two datasets with documents from the clinical and scientific domains, where numbers abound (Bigeard et al., 2015; Porter, 1996). Furthermore, to ensure that the numbers will be informative of some attribute, we only selected texts that reference tables.</p>
<p>Clinical Data Our clinical dataset comprises clinical records from the London Chest Hospital. The records where accompanied by tables with 20 numeric attributes (age, heart volumes, etc.) that they partially describe, as well as include numbers not found in the tables. Numeric tokens constitute only a small proportion of each sentence (4.3\%), but account
for a large part of the unique tokens vocabulary ( $&gt;40 \%$ ) and suffer high OOV rates.</p>
<p>Scientific Data Our scientific dataset comprises paragraphs from Cornell's ARXIV ${ }^{1}$ repository of scientific articles, with more than half a million converted papers in 37 scientific sub-fields. We used the preprocessed ARXMLIV (Stamerjohanns et al., 2010; Stamerjohanns and Kohlhase, 2008) ${ }^{2}$ version, where papers have been converted from LATEX into a custom XML format using the LATEXML ${ }^{3}$ tool. We then kept all paragraphs with at least one reference to a table and a number.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Clinical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Scientific</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: center;">#inst</td>
<td style="text-align: center;">11170</td>
<td style="text-align: center;">1625</td>
<td style="text-align: center;">3220</td>
<td style="text-align: center;">14694</td>
<td style="text-align: center;">2037</td>
<td style="text-align: center;">4231</td>
</tr>
<tr>
<td style="text-align: center;">maxLen</td>
<td style="text-align: center;">667</td>
<td style="text-align: center;">594</td>
<td style="text-align: center;">666</td>
<td style="text-align: center;">2419</td>
<td style="text-align: center;">1925</td>
<td style="text-align: center;">1782</td>
</tr>
<tr>
<td style="text-align: center;">avgLen</td>
<td style="text-align: center;">210.1</td>
<td style="text-align: center;">209.1</td>
<td style="text-align: center;">206.9</td>
<td style="text-align: center;">210.1</td>
<td style="text-align: center;">215.9</td>
<td style="text-align: center;">212.1</td>
</tr>
<tr>
<td style="text-align: center;">\%word</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">96.0</td>
</tr>
<tr>
<td style="text-align: center;">\%nums</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">4.0</td>
</tr>
<tr>
<td style="text-align: center;">min</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">median</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">4.5</td>
</tr>
<tr>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">300.6</td>
<td style="text-align: center;">147.7</td>
<td style="text-align: center;">464.8</td>
<td style="text-align: center;">$\sim 10^{21}$</td>
<td style="text-align: center;">$\sim 10^{7}$</td>
<td style="text-align: center;">$\sim 10^{7}$</td>
</tr>
<tr>
<td style="text-align: center;">max</td>
<td style="text-align: center;">$\sim 10^{7}$</td>
<td style="text-align: center;">$\sim 10^{5}$</td>
<td style="text-align: center;">$\sim 10^{7}$</td>
<td style="text-align: center;">$\sim 10^{26}$</td>
<td style="text-align: center;">$\sim 10^{11}$</td>
<td style="text-align: center;">$\sim 10^{11}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistical description of the clinical and scientific datasets: Number of instances (i.e. paragraphs), maximum and average lengths, proportions of words and numerals, descriptive statistics of numbers.</p>
<p>For both datasets, we lowercase tokens and normalise numerals by omitting the thousands separator ("2,000" becomes "2000") and leading zeros ("007" becomes "7"). Special mathematical symbols are tokenised separately, e.g. negation ("-1" as "-", "1"), fractions ("3/4" as "3", " /", "4"), etc. For this reason, all numbers were non-negative. Table 1 shows descriptive statistics for both datasets.</p>
<h2>5 Experimental Results and Discussion</h2>
<p>We set the vocabularies to the 1,000 and 5,000 most frequent token types for the clinical and scientific datasets, respectively. We use gated token-character embeddings (Miyamoto and Cho, 2016) for the input of numerals and token embeddings for the input and output of words, since the scope of our paper is numeracy. We set the models' hidden dimensions to $D=50$ and initialise all token embeddings to pretrained GloVe (Pennington et al., 2014). All our</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Clinical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Scientific</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">words</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">numerals</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">total</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">words</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">numerals</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">total</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">APP</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">APP</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">APP</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">APP</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">APP</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">APP</td>
</tr>
<tr>
<td style="text-align: center;">softmax</td>
<td style="text-align: center;">4.08</td>
<td style="text-align: center;">5.99</td>
<td style="text-align: center;">12.04</td>
<td style="text-align: center;">58443.72</td>
<td style="text-align: center;">4.28</td>
<td style="text-align: center;">8.91</td>
<td style="text-align: center;">33.96</td>
<td style="text-align: center;">51.83</td>
<td style="text-align: center;">127.12</td>
<td style="text-align: center;">3505856.25</td>
<td style="text-align: center;">35.79</td>
<td style="text-align: center;">80.62</td>
</tr>
<tr>
<td style="text-align: center;">softmax+rn</td>
<td style="text-align: center;">4.03</td>
<td style="text-align: center;">5.91</td>
<td style="text-align: center;">11.57</td>
<td style="text-align: center;">56164.81</td>
<td style="text-align: center;">4.21</td>
<td style="text-align: center;">8.77</td>
<td style="text-align: center;">33.54</td>
<td style="text-align: center;">51.20</td>
<td style="text-align: center;">119.68</td>
<td style="text-align: center;">3300688.50</td>
<td style="text-align: center;">35.28</td>
<td style="text-align: center;">79.47</td>
</tr>
<tr>
<td style="text-align: center;">h-softmax</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">4.96</td>
<td style="text-align: center;">11.78</td>
<td style="text-align: center;">495.95</td>
<td style="text-align: center;">4.19</td>
<td style="text-align: center;">6.05</td>
<td style="text-align: center;">34.73</td>
<td style="text-align: center;">49.81</td>
<td style="text-align: center;">122.67</td>
<td style="text-align: center;">550.98</td>
<td style="text-align: center;">36.51</td>
<td style="text-align: center;">54.80</td>
</tr>
<tr>
<td style="text-align: center;">h-softmax+rn</td>
<td style="text-align: center;">4.03</td>
<td style="text-align: center;">4.99</td>
<td style="text-align: center;">11.65</td>
<td style="text-align: center;">490.14</td>
<td style="text-align: center;">4.22</td>
<td style="text-align: center;">6.09</td>
<td style="text-align: center;">34.04</td>
<td style="text-align: center;">48.83</td>
<td style="text-align: center;">120.83</td>
<td style="text-align: center;">542.70</td>
<td style="text-align: center;">35.80</td>
<td style="text-align: center;">53.73</td>
</tr>
<tr>
<td style="text-align: center;">d-RNN</td>
<td style="text-align: center;">3.99</td>
<td style="text-align: center;">4.95</td>
<td style="text-align: center;">263.22</td>
<td style="text-align: center;">263.22</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">5.88</td>
<td style="text-align: center;">34.08</td>
<td style="text-align: center;">48.89</td>
<td style="text-align: center;">519.80</td>
<td style="text-align: center;">519.80</td>
<td style="text-align: center;">37.98</td>
<td style="text-align: center;">53.70</td>
</tr>
<tr>
<td style="text-align: center;">MoG</td>
<td style="text-align: center;">4.03</td>
<td style="text-align: center;">4.99</td>
<td style="text-align: center;">226.46</td>
<td style="text-align: center;">226.46</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">5.88</td>
<td style="text-align: center;">34.14</td>
<td style="text-align: center;">48.97</td>
<td style="text-align: center;">683.16</td>
<td style="text-align: center;">683.16</td>
<td style="text-align: center;">38.45</td>
<td style="text-align: center;">54.37</td>
</tr>
<tr>
<td style="text-align: center;">combination</td>
<td style="text-align: center;">4.01</td>
<td style="text-align: center;">4.96</td>
<td style="text-align: center;">197.59</td>
<td style="text-align: center;">197.59</td>
<td style="text-align: center;">4.74</td>
<td style="text-align: center;">5.82</td>
<td style="text-align: center;">33.64</td>
<td style="text-align: center;">48.25</td>
<td style="text-align: center;">520.95</td>
<td style="text-align: center;">520.95</td>
<td style="text-align: center;">37.50</td>
<td style="text-align: center;">53.03</td>
</tr>
</tbody>
</table>
<p>Table 2: Test set perplexities for the clinical and scientific data. Adjusted perplexities (APP) are directly comparable across all data and models, but perplexities (PP) are sensitive to the varying out-of-vocabulary rates.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Clinical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Scientific</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MdAE</td>
<td style="text-align: center;">MAPE\%</td>
<td style="text-align: center;">MdAPE\%</td>
<td style="text-align: center;">MdAE</td>
<td style="text-align: center;">MAPE\%</td>
<td style="text-align: center;">MdAPE\%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">1043.68</td>
<td style="text-align: center;">294.95</td>
<td style="text-align: center;">245.59</td>
<td style="text-align: center;">2353.11</td>
<td style="text-align: center;">409.47</td>
<td style="text-align: center;">$\sim 10^{20}$</td>
<td style="text-align: center;">$\sim 10^{23}$</td>
<td style="text-align: center;">$\sim 10^{22}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">median</td>
<td style="text-align: center;">1036.18</td>
<td style="text-align: center;">120.24</td>
<td style="text-align: center;">34.52</td>
<td style="text-align: center;">425.81</td>
<td style="text-align: center;">52.05</td>
<td style="text-align: center;">4.20</td>
<td style="text-align: center;">8039.15</td>
<td style="text-align: center;">98.65</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">softmax</td>
<td style="text-align: center;">997.84</td>
<td style="text-align: center;">80.29</td>
<td style="text-align: center;">12.70</td>
<td style="text-align: center;">621.78</td>
<td style="text-align: center;">22.41</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">1947.44</td>
<td style="text-align: center;">80.62</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">softmax+rn</td>
<td style="text-align: center;">991.38</td>
<td style="text-align: center;">74.44</td>
<td style="text-align: center;">13.00</td>
<td style="text-align: center;">503.57</td>
<td style="text-align: center;">23.91</td>
<td style="text-align: center;">3.50</td>
<td style="text-align: center;">15208.37</td>
<td style="text-align: center;">80.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">h-softmax</td>
<td style="text-align: center;">1095.01</td>
<td style="text-align: center;">167.19</td>
<td style="text-align: center;">14.00</td>
<td style="text-align: center;">746.50</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">1652.21</td>
<td style="text-align: center;">80.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">h-softmax+rn</td>
<td style="text-align: center;">1001.04</td>
<td style="text-align: center;">83.19</td>
<td style="text-align: center;">12.30</td>
<td style="text-align: center;">491.85</td>
<td style="text-align: center;">23.44</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">2703.49</td>
<td style="text-align: center;">80.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">d-RNN</td>
<td style="text-align: center;">1009.34</td>
<td style="text-align: center;">70.21</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">513.81</td>
<td style="text-align: center;">17.90</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">1287.27</td>
<td style="text-align: center;">52.45</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MoG</td>
<td style="text-align: center;">998.78</td>
<td style="text-align: center;">57.11</td>
<td style="text-align: center;">6.92</td>
<td style="text-align: center;">348.10</td>
<td style="text-align: center;">13.64</td>
<td style="text-align: center;">2.10</td>
<td style="text-align: center;">590.42</td>
<td style="text-align: center;">90.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">combination</td>
<td style="text-align: center;">989.84</td>
<td style="text-align: center;">69.47</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">552.06</td>
<td style="text-align: center;">17.86</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">2332.50</td>
<td style="text-align: center;">88.89</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Test set regression evaluation for the clinical and scientific data. Mean absolute percentage error (MAPE) is scale independent and allows for comparison across data, whereas root mean square and mean absolute errors (RMSE, MAE) are scale dependent. Medians (MdAE, MdAPE) are informative of the distribution of errors.</p>
<p>RNNs are LSTMs (Hochreiter and Schmidhuber, 1997) with the biases of LSTM forget gate were initialised to 1.0 (Jzefowicz et al., 2015). We train using mini-batch gradient decent with the Adam optimiser (Kingma and Ba, 2014) and regularise with early stopping and 0.1 dropout rate (Srivastava, 2013) in the input and output of the token-based RNN.</p>
<p>For the mixture of Gaussians, we select the mean and variances to summarise the data at different granularities by fitting 7 separate mixture of Gaussian models on all numbers, each with twice as many components as the previous, for a total of $2^{7+1}-1=256$ components. These models are initialised at percentile points from the data and trained with the expectation-minimisation algorithm. The means and variances are then fixed and not updated when we train the language model.</p>
<h3>5.1 Quantitative Results</h3>
<p>Perplexities Table 2 shows perplexities evaluated on the subsets of words, numerals and all tokens of
the test data. Overall, all models performed better on the clinical than on the scientific data. On words, all models achieve similar perplexities in each dataset.</p>
<p>On numerals, softmax variants perform much better than other models in PP, which is an artefact of the high OOV-rate of numerals. APP is significantly worse, especially for non-hierarchical variants, which perform about 2 and 4 orders of magnitude worse than hierarchical ones.</p>
<p>For open-vocabulary models, i.e. d-RNN, MoG, and combination, PP is equivalent to APP. On numerals, d-RNN performed better than softmax variants in both datasets. The MoG model performed twice as well as softmax variants on the clinical dataset, but had the third worse performance in the scientific dataset. The combination model had the best overall APP results for both datasets.</p>
<p>Evaluations on the Number Line To factor out model specific decoding processes for finding the best next numeral, we use our models to rank a set</p>
<p>of candidate numerals: we compose the union of in-vocabulary numbers and 100 percentile points from the training set, and we convert numbers into numerals by considering all formats up to $n$ decimal points. We select $n$ to represent $90 \%$ of numerals seen at training, which yields $n=3$ and $n=4$ for the clinical and scientific data, respectively.</p>
<p>Table 3 shows evaluation results, where we also include two naive baselines of constant predictions: with the mean and median of the training data. For both datasets, RMSE and MAE were too sensitive to extreme errors to allow drawing safe conclusions, particularly for the scientific dataset, where both metrics were in the order of $10^{9}$. MdAE can be of some use, as $50 \%$ of the errors are absolutely smaller than that.</p>
<p>Along percentage metrics, MoG achieved the best MAPE in both datasets ( $18 \%$ and $54 \%$ better that the second best) and was the only model to perform better than the median baseline for the clinical data. However, it had the worst MdAPE, which means that MoG mainly reduced larger percentage errors. The d-RNN model came third and second in the clinical and scientific datasets, respectively. In the latter it achieved the best MdAPE, i.e. it was effective at reducing errors for $50 \%$ of the numbers. The combination model did not perform better than its constituents. This is possibly because MoG is the only strategy that takes into account the numerical magnitudes of the numerals.</p>
<h3>5.2 Learnt Representations</h3>
<p>Softmax versus Hierarchical Softmax Figure 3 visualises the cosine similarities of the output token embeddings of numerals for the softmax and h-softmax models. Simple softmax enforced high similarities among all numerals and the unknown numeral token, so as to make them more dissimilar to words, since the model embeds both in the same space. This is not the case for h-softmax that uses two different spaces: similarities are concentrated along the diagonal and fan out as the magnitude grows, with the exception of numbers with special meaning, e.g. years and percentile points.</p>
<p>Digit embeddings Figure 4 shows the cosine similarities between the digits of the d-RNN output mode. We observe that each primitive digit is mostly similar to its previous and next digit. Similar behaviour was found for all digit embeddings of all models.</p>
<h3>5.3 Predictions from the Models</h3>
<p>Next Numeral Figure 5 shows the probabilities of different numerals under each model for two
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Numeral embeddings for the softmax (top) and h-softmax (bottom) models on the clinical data. Numerals are sorted by value.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Cosine similarities for d-RNN's output digit embeddings trained on the scientific data.
examples from the clinical development set. Numerals are grouped by number of decimal points. The h-softmax model's probabilities are spiked, d-RNNs are saw-tooth like and MoG's are smooth, with the occasional spike, whenever a narrow component allows for it. Probabilities rapidly decrease for more decimal digits, which is reminiscent of the theoretical expectation that the probability of en exact value for a continuous variable is zero.</p>
<p>Selection of Strategy in Combination Model Table 4 shows development set examples with high selection probabilities for each strategy of the combination model, along with numerals with the highest average selection per mode. The h-softmax model is responsible for mostly integers with special functions,</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Clinical</th>
<th style="text-align: center;">Scientific</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">b-softmax</td>
<td style="text-align: center;">Examples: "late enhancement ( $&gt;75 \%$ )", "late gadolinium enhancement ( $&lt;25 \%$ )", "infarction ( 2 out of 17 segments )", "infarct with 4 out of 17 segments nonviable", "adenosine stress perfusion @ 140 mcg", "stress perfusion ( adenosine 140 mcg" Numerals: 50, 17, 100, 75, 25, 1, 140, 2012, 2010, 2011, 8, 5, 2009, 2013, 7, 6, 2, 3, 2008, 4...</td>
<td style="text-align: center;">Examples: "sharp et al . 2004", "li et al . 2003", " $3.5 \times 10^{-} 4$ ", " $0.3 \times 10^{-} 16$ " Numerals: 1992, 2001, 1995, 2003, 2009, 1993, 2010, 1994, 1998, 2002, 2006, 1997, 2005, 1990, 10, 2008, 2007, 2004, 1983, 1991...</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Examples: "aortic root is dilated ( measured $37 \times 37$ mm", "ascending aorta is not dilated ( $32 \times 31 \mathrm{~mm}$ " Numerals: 42, 33, 31, 43, 44, 21, 38, 36, 46, 37, 32, $39,26,28,23,29,45,40,49,94 \ldots$</td>
<td style="text-align: center;">Examples: "ngc 6334 stars", "ngc 2366 shows a wealth of small structures" Numerals: 294, 4000, 238, 6334, 2363, 1275, 2366, 602, 375, 1068, 211, 6.4, 8.7, $600,96,0.65,700,1.17,4861,270 \ldots$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Examples: "stroke volume 46.1 ml", "stroke volume 65.6 ml ", "stroke volume 74.5 ml ", "end diastolic volume 82.6 ml ", "end diastolic volume 99.09 ml ", "end diastolic volume 138.47 ml " Numerals: 74.5, 69.3, 95.9, 96.5, 72.5, 68.6, 82.1, 63.7, 78.6, 69.6, 69.5, 82.2, 68.3, 73.2, 63.2, 82.6, $77.7,80.7,70.7,70.4 \ldots$</td>
<td style="text-align: center;">Examples: "hip 12961 and gl 676 a are orbited by giant planets," "velocities of gl 676", "velocities of hip 12961" Numerals: 12961, 766, 7409, 4663, 44.3, 1819, 676, 1070, 5063, 323, 264, 163296, 2030, 77, 1.15, 196, 0.17, 148937, 0.43, 209458...</td>
</tr>
</tbody>
</table>
<p>Table 4: Examples of numerals with highest probability in each strategy of the combination model.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Example model predictions for the h-softmax (top), d-RNN (middle) and MoG (bottom) models. Examples from the clinical development set.
e.g. years, typical drug dosages, percentile points, etc. In the clinical data, d-RNN picks up two-digit integers (mostly dimensions) and MoG is activated for continuous attributes, which are mostly out of vocabulary. In the scientific data, d-RNN and MoG
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Distributions of significant digits from d-RNN model, data, and theoretical expectation (Benford's law).
showed affinity to different indices from catalogues of astronomical objects: d-RNN mainly to NGC (Dreyer, 1888) and MoG to various other indices, such as GL (Gliese, 1988) and HIP (Perryman et al., 1997). In this case, MoG was wrongly selected for numerals with a labelling function, which also highlights a limitation of evaluating on the number line, when a numeral is not used to represent its magnitude.</p>
<p>Significant Digits Figure 5 shows the distributions of the most significant digits under the d-RNN model</p>
<p>and from data counts. The theoretical estimate has been overlayed, according to Benford's law (Benford, 1938), also called the first-digit law, which applies to many real-life numerals. The law predicts that the first digit is 1 with higher probability (about 30\%) than $9(&lt;5 \%)$ and weakens towards uniformity at higher digits. Model probabilities closely follow estimates from the data. Violations from Benford's law can be due to rounding (Beer, 2009) and can be used as evidence for fraud detection (Lu et al., 2006).</p>
<h2>6 Related Work</h2>
<p>Numerical quantities have been recognised as important for textual entailment (Lev et al., 2004; Dagan et al., 2013). Roy et al. (2015) proposed a quantity entailment sub-task that focused on whether a given quantity can be inferred from a given text and, if so, what its value should be. A common framework for acquiring common sense about numerical attributes of objects has been to collect a corpus of numerical values in pre-specified templates and then model attributes as a normal distribution (Aramaki et al., 2007; Davidov and Rappoport, 2010; Iftene and Moruz, 2010; Narisawa et al., 2013; de Marneffe et al., 2010). Our model embeds these approaches into a LM that has a sense for numbers.</p>
<p>Other tasks that deal with numerals are numerical information extraction and solving mathematical problems. Numerical relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather produces an probabilistic estimate.</p>
<p>Much work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations, but could be extended to do so in future work.</p>
<p>In language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem. Gulcehre et al. (2016) and Gu et al. (2016) adopted pointer networks (Vinyals et al., 2015)
to copy unknown words from the source in translation and summarisation tasks. Merity et al. (2016) and Lebret et al. (2016) have models that copy from context sentences and from Wikipedia's infoboxes, respectively. Ahn et al. (2016) proposed a LM that retrieves unknown words from facts in a knowledge graph. They draw attention to the inappropriateness of perplexity when OOV-rates are high and instead propose an adjusted perplexity metric that is equivalent to APP. Other methods aim at speeding up LMs to allow for larger vocabularies (Chen et al., 2015), such as hierarchical softmax (Morin and Bengio, 2005b), target sampling (Jean et al., 2014), etc., but still suffer from the unknown word problem. Finally, the problem is resolved when predicting one character at a time, as done by the character-level RNN (Graves, 2013; Sutskever et al., 2011) used in our d-RNN model.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we investigated several strategies for LMs to model numerals and proposed a novel openvocabulary generative model based on a continuous probability density function. We provided the first thorough evaluation of LMs on numerals on two corpora, taking into account their high out-of-vocabulary rate and numerical value (magnitude). We found that modelling numerals separately from other words through a hierarchical softmax can substantially improve the perplexity of LMs, that different strategies are suitable for different contexts, and that a combination of these strategies can help improve the perplexity further. Finally, we found that using a continuous probability density function can improve prediction accuracy of LMs for numbers by substantially reducing the mean absolute percentage metric.</p>
<p>Our approaches in modelling and evaluation can be used in future work in tasks such as approximate information extraction, knowledge base completion, numerical fact checking, numerical question answering, and fraud detection. Our code and data are available at: https://github.com/uclmr/ numerate-language-models.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank the anonymous reviewers for their insightful comments and also Steffen Petersen for providing the clinical dataset and advising us on the clinical aspects of this work. This research was supported by the Farr Institute of Health Informatics Research and an Allen Distinguished Investigator award.</p>
<h2>References</h2>
<p>Sungjin Ahn, Heeyoul Choi, Tanel Prnamaa, and Yoshua Bengio. 2016. A neural knowledge language model. arXiv preprint arXiv:1608.00318 .</p>
<p>Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko Ohe. 2007. Uth: Svm-based semantic relation classification using physical sizes. In Proceedings of the 4th International Workshop on Semantic Evaluations. Association for Computational Linguistics, pages $464-467$.</p>
<p>TW Beer. 2009. Terminal digit preference: beware of benford's law. Journal of clinical pathology 62(2):192-192.</p>
<p>Frank Benford. 1938. The law of anomalous numbers. Proceedings of the American philosophical society pages 551-572.</p>
<p>Elise Bigeard, Vianney Jouhet, Fleur Mougin, Frantz Thiessard, and Natalia Grabar. 2015. Automatic extraction of numerical values from unstructured data in ehrs. In MIE. pages 50-54.</p>
<p>Jamie ID Campbell. 2005. Handbook of mathematical cognition. Psychology Press.</p>
<p>Welin Chen, David Grangier, and Michael Auli. 2015. Strategies for training large vocabulary neural language models. arXiv preprint arXiv:1512.04906 .</p>
<p>Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment: Models and applications. Synthesis Lectures on Human Language Technologies 6(4):1-220.</p>
<p>Dmitry Davidov and Ari Rappoport. 2010. Extraction and approximation of numerical attributes from the web. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, pages 1308-1317.</p>
<p>Marie-Catherine de Marneffe, Christopher D Manning, and Christopher Potts. 2010. Was it good? it was provocative. learning the meaning of scalar adjectives. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, pages 167-176.</p>
<p>Stanislas Dehaene and Laurent Cohen. 1995. Towards an anatomical and functional model of number processing. Mathematical cognition 1(1):83-120.</p>
<p>Stanislas Dehaene, Manuela Piazza, Philippe Pinel, and Laurent Cohen. 2003. Three parietal circuits for number processing. Cognitive neuropsychology 20(3-6):487-506.</p>
<p>John Louis Emil Dreyer. 1888. A new general catalogue of nebul and clusters of stars, being the catalogue of the late sir john fw herschel, bart, revised, corrected, and enlarged. Memoirs of the Royal Astronomical Society 49:1.</p>
<p>Katja Filippova, Enrique Alfonseca, Carlos A. Colmenares, Lukasz Kaiser, and Oriol Vinyals. 2015. Sentence compression by deletion with lstms. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015. pages 360-368.</p>
<p>Mahak Gambhir and Vishal Gupta. 2017. Recent automatic text summarization techniques: a survey. Artif. Intell. Rev. 47(1):1-66.</p>
<p>Wilhelm Gliese. 1988. The third catalogue of nearby stars. Stand. Star Newsl. 13, 1313.</p>
<p>Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 .</p>
<p>Sharon Griffin, Robbie Case, and Allesandra Capodilupo. 1995. Teaching for understanding: The importance of the central conceptual structures in the elementary mathematics curriculum. .</p>
<p>Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. 2016. Incorporating copying mechanism in sequenceto-sequence learning. arXiv preprint arXiv:1603.06393 .</p>
<p>Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. arXiv preprint arXiv:1603.08148 .</p>
<p>aglar Glehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, and Yoshua Bengio. 2017. On integrating a language model into neural machine translation. Computer Speech \&amp; Language 45:137-148.</p>
<p>Sepp Hochreiter and Jrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):17351780 .</p>
<p>Raphael Hoffmann, Congle Zhang, and Daniel S Weld. 2010. Learning 5000 relational extractors. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, pages 286-295.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 523-533.</p>
<p>Adrian Iftene and Mihai-Alex Moruz. 2010. Uaic participation at rte-6 .</p>
<p>Ander Intxaurrondo, Eneko Agirre, Oier Lopez De Lacalle, and Mihai Surdeanu. 2015. Diamonds in the rough: Event extraction from imperfect microblog data. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pages 641-650.</p>
<p>Sbastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2014. On using very large target vocabulary for neural machine translation. arXiv preprint arXiv:1412.2007 .</p>
<p>Elana Joram, Lauren B Resnick, and Anthony J Gabriele. 1995. Numeracy as cultural practice: An examination of numbers in magazines for children, teenagers, and adults. Journal for Research in Mathematics Education pages 346-361.</p>
<p>Rafal Jzefowicz, Wojciech Zaremba, and Ilya Sutskever. 2015. An empirical exploration of recurrent network architectures. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015. pages 2342-2350.</p>
<p>Roi Cohen Kadosh and Ann Dowker. 2015. The Oxford handbook of numerical cognition. Oxford Library of Psychology.</p>
<p>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 .</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics 3:585-597.</p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). volume 1, pages 271-281.</p>
<p>Rmi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. arXiv preprint arXiv:1603.07771 .</p>
<p>Iddo Lev, Bill MacCartney, Christopher D Manning, and Roger Levy. 2004. Solving logic puzzles: From robust processing to precise semantics. In Proceedings of the 2nd Workshop on Text Meaning and Interpretation. Association for Computational Linguistics, pages 9-16.</p>
<p>Fletcher Lu, J. Efrim Boritz, and H. Dominic Covvey. 2006. Adaptive fraud detection using benford's law. In Advances in Artificial Intelligence, 19th Conference of the Canadian Society for Computational Studies of Intelligence, Canadian AI 2006. pages 347-358.</p>
<p>Minh-Thang Luong and Christopher D Manning. 2016. Achieving open vocabulary neural machine translation with hybrid word-character models. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). volume 1, pages 1054-1063.</p>
<p>Thang Luong, Michael Kayser, and Christopher D. Manning. 2015. Deep neural language models for machine translation. In Proceedings of the 19th Conference on Computational Natural Language Learning, CoNLL 2015. pages 305-309.</p>
<p>Aman Madaan, Ashish Mittal, Ganesh Ramakrishnan, Sunita Sarawagi, et al. 2016. Numerical relation extraction with minimal supervision. In Thirtieth AAAI Conference on Artificial Intelligence.</p>
<p>Michael McCloskey and Paul Macaruso. 1995. Representing and using numerical information. American Psychologist 50(5):351.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843 .</p>
<p>Tomas Mikolov, Martin Karafit, Luks Burget, Jan Cernock, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association. pages 1045-1048.</p>
<p>Jeff Mitchell and Mirella Lapata. 2009. Language models based on semantic composition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1. Association for Computational Linguistics, pages 430-439.</p>
<p>Arindam Mitra and Chitta Baral. 2016. Learning to use formulas to solve simple arithmetic problems. In ACL.</p>
<p>Yasumasa Miyamoto and Kyunghyun Cho. 2016. Gated word-character recurrent language model. arXiv preprint arXiv:1606.01700 .</p>
<p>Frederic Morin and Yoshua Bengio. 2005a. Hierarchical probabilistic neural network language model. In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, AISTATS 2005.</p>
<p>Frederic Morin and Yoshua Bengio. 2005b. Hierarchical probabilistic neural network language model. In Aistats. Citeseer, volume 5, pages 246-252.</p>
<p>Katsuma Narisawa, Yotaro Watanabe, Junta Mizuno, Naoaki Okazaki, and Kentaro Inui. 2013. Is a 204 cm man tall or small? acquisition of numerical common sense from the web. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). volume 1, pages 382-391.</p>
<p>Truc-Vien T Nguyen and Alessandro Moschitti. 2011. End-to-end relation extraction using distant supervision from external semantic repositories. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2. Association for Computational Linguistics, pages 277-282.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). pages 1532-1543.</p>
<p>Michael AC Perryman, L Lindegren, J Kovalevsky, E Hoeg, U Bastian, PL Bernacca, M Crz, F Donati, M Grenon, M Grewing, et al. 1997. The hipparcos catalogue. Astronomy and Astrophysics 323:L49-L52.</p>
<p>Theodore M Porter. 1996. Trust in numbers: The pursuit of objectivity in science and public life. Princeton University Press.</p>
<p>Rohit Prabhavalkar, Kanishka Rao, Tara N. Sainath, Bo Li, Leif Johnson, and Navdeep Jaitly. 2017. A comparison of sequence-to-sequence models for speech recognition. In Interspeech 2017, 18th Annual Conference of the International Speech Communication Association. pages 939-943.</p>
<p>Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. Truth of varying shades: Analyzing language in fake news and political fact-checking. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017. pages 2931-2937.</p>
<p>Marek Rei and Helen Yannakoudakis. 2017. Auxiliary objectives for neural error detection models. In Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, BEA@EMNLP 2017. pages 33-43.</p>
<p>Subhro Roy and Dan Roth. 2016. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413 .</p>
<p>Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics 3:1-13.</p>
<p>Cicero D Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-of-speech tagging. In Proceedings of the 31st International Conference on Machine Learning (ICML-14). pages 1818-1826.</p>
<p>Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. 2015. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages $1466-1476$.</p>
<p>Shuming Shi, Yuehui Wang, Chin-Yew Lin, Xiaojiang Liu, and Yong Rui. 2015. Automatically solving number word problems by semantic parsing and reasoning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Lisbon, Portugal.</p>
<p>Georgios P. Spithourakis, Isabelle Augenstein, and Sebastian Riedel. 2016a. Numerically grounded language models for semantic error correction. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016. pages 987-992.</p>
<p>Georgios P Spithourakis, Steffen E Petersen, and Sebastian Riedel. 2016b. Clinical text prediction with numerically grounded conditional language models. EMNLP 2016 page 6.</p>
<p>Nitish Srivastava. 2013. Improving neural networks with dropout. University of Toronto 182.</p>
<p>Heinrich Stamerjohanns and Michael Kohlhase. 2008. Transforming the arxiv to xml. In International Conference on Intelligent Computer Mathematics. Springer, pages 574-582.</p>
<p>Heinrich Stamerjohanns, Michael Kohlhase, Deyan Ginev, Catalin David, and Bruce Miller. 2010. Transforming large collections of scientific publications to xml. Mathematics in Computer Science 3(3):299-307.</p>
<p>Ilya Sutskever, James Martens, and Geoffrey E Hinton. 2011. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 1017-1024.</p>
<p>Joseph Tzelgov, Dana Ganor-Stern, Arava Y Kallai, and Michal Pinhas. 2015. Primitives and non-primitives of numerical representations. Oxford library of psychology. The Oxford handbook of numerical cognition pages 45-66.</p>
<p>Joerg Ueberla. 1994. Analysing a simple language modelsome general conclusions for language models for speech recognition. Computer Speech \&amp; Language 8(2):153-176.</p>
<p>Shyam Upadhyay and Ming-Wei Chang. 2016. Annotating derivations: A new evaluation strategy and dataset for algebra word problems. arXiv preprint arXiv:1609.07197 .</p>
<p>Shyam Upadhyay, Ming-Wei Chang, Kai-Wei Chang, and Wen-tau Yih. 2016. Learning from explicit and implicit supervision jointly for algebra word problems. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pages 297-306.</p>
<p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances in Neural Information Processing Systems. pages 2692-2700.</p>
<p>Tong Wang, Xingdi Yuan, and Adam Trischler. 2017. A joint model for question answering and question generation. CoRR abs/1706.01450.</p>
<p>Lipu Zhou, Shuaixiang Dai, and Liwei Chen. 2015. Learn to solve algebra word problems using quadratic programming. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics (Lisbon, Portugal. pages 817-822.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ ARXIV.ORG. Cornell University Library at http://arxiv.org/, visited December 2016
${ }^{2}$ ARXMLIV. Project home page at http://arxmliv.kwarc.info/, visited December 2016
${ }^{3}$ LATEXML. http://dlmf.nist.gov, visited December 2016&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
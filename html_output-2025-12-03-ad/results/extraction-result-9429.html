<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9429 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9429</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9429</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-a46b06a4b8b4deecf96a4e42cd19b4696f999e66</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a46b06a4b8b4deecf96a4e42cd19b4696f999e66" target="_blank">Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The Anomaly Transformer achieves state-of-the-art results on six unsupervised time series anomaly detection benchmarks of three applications: service monitoring, space&earth exploration, and water treatment.</p>
                <p><strong>Paper Abstract:</strong> Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through learning pointwise representation or pairwise association, however, neither is sufficient to reason about the intricate dynamics. Recently, Transformers have shown great power in unified modeling of pointwise representation and pairwise association, and we find that the self-attention weight distribution of each time point can embody rich association with the whole series. Our key observation is that due to the rarity of anomalies, it is extremely difficult to build nontrivial associations from abnormal points to the whole series, thereby, the anomalies' associations shall mainly concentrate on their adjacent time points. This adjacent-concentration bias implies an association-based criterion inherently distinguishable between normal and abnormal points, which we highlight through the \emph{Association Discrepancy}. Technically, we propose the \emph{Anomaly Transformer} with a new \emph{Anomaly-Attention} mechanism to compute the association discrepancy. A minimax strategy is devised to amplify the normal-abnormal distinguishability of the association discrepancy. The Anomaly Transformer achieves state-of-the-art results on six unsupervised time series anomaly detection benchmarks of three applications: service monitoring, space&earth exploration, and water treatment.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9429.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9429.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anomaly Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based model that detects anomalies in multivariate time series by learning a per-timepoint association distribution (series-association) and comparing it to a learnable adjacent-concentrating prior (prior-association) via a symmetrized KL association discrepancy (AssDis); trained with a reconstruction objective and a minimax scheme to amplify normal/abnormal separation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Anomaly Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer variant with custom two-branch attention (Anomaly-Attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>d_model=512, L=3 layers, h=8 heads (parameter count not explicitly given)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Multivariate time series (sliding windows of fixed length, here window size = 100)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System/service monitoring (SMD, PSM), spacecraft telemetry (MSL, SMAP), industrial control / water treatment (SWaT), synthetic/benchmark scenarios (NeurIPS-TS, UCR)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous time points and anomalous segments (point-wise and pattern-wise anomalies; both point-global and pattern-contextual/seasonal/shapelet/trend types in benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Each time point i obtains two discrete distributions over all time indices: (1) prior-association P_i computed from a learnable Gaussian kernel G(|j-i|; sigma_i) (rescaled to a distribution) to encode adjacent-concentration bias; (2) series-association S_i from standard self-attention Softmax(QK^T / sqrt(d)). The Association Discrepancy AssDis_i is the symmetrized KL (KL(P||S)+KL(S||P)) averaged across layers. Training alternates a minimize phase (prior P is optimized to approximate a stopped-gradient S) and a maximize phase (S is optimized to increase discrepancy while minimizing reconstruction error), preventing trivial collapse of sigma. Final anomaly score = Softmax(-AssDis) elementwise-multiplied by reconstruction error per timepoint.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared extensively to 18 baselines including reconstruction-based (InterFusion, OmniAnomaly, LSTM-VAE, BeatGAN), density-estimation (DAGMM, MPPCACD, LOF), clustering/one-class (Deep-SVDD, ITAD, THOC), autoregression (VAR, LSTM, CL-MPPCA), classic methods (OC-SVM, IsolationForest), plus other change-point/segmentation baselines referenced in appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F1-score (per-dataset and averaged), ROC AUC (area under ROC curve); also visualization and per-point statistics of attention weights and learned sigma</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>State-of-the-art across evaluated benchmarks. Example dataset F1 scores from main table: SMD F1=92.33%, MSL F1=93.59%, SMAP F1=96.69%, SWaT F1=94.07%, PSM F1=97.89%; average (final ablation) F1 across five real datasets = 94.96%. NeurIPS-TS and UCR benchmarks: strong SOTA performance (UCR F1 reported 84.12%). AUC curves show highest AUC on compared datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Outperforms all listed baselines in the experiments; ablations show Anomaly Transformer improves over vanilla Transformer by ~18.34 percentage points absolute F1, and that AssDis alone already outperforms previous SOTA (THOC). Learnable prior and the minimax optimization each give further substantial gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Direct maximization of AssDis (without minimax) leads to collapse of Gaussian scale sigma (prior becomes meaningless), so minimax + stop-gradient is required; some failure cases shown in visualization (Figure 5) where criterion still mis-detects certain patterns; method requires sliding-window framing (window size, thresholding proportion r need selection though authors report stability over ranges); theoretical analysis relative to classic autoregressive/state-space models is left as future work; certain alternative discrepancy measures (Wasserstein, L2) performed poorly in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Key novel insight: anomalies tend to have self-attention distributions concentrated on adjacent time points (adjacent-concentration bias) because anomalies are rare and cannot form broad associations with normal dynamics; quantifying the discrepancy between a learnable local prior and the learned attention (AssDis) yields an inherently discriminative signal for anomalies. Combining AssDis (normalized) with reconstruction error multiplicatively gives superior detection. Minimax training (prior approximates S in minimize phase, S is pushed away in maximize phase under reconstruction) amplifies normal/abnormal distinguishability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9429.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9429.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anomaly-Attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anomaly-Attention (two-branch attention mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-branch attention mechanism introduced in this paper: a prior-association branch using a per-position learnable Gaussian kernel to encode adjacent-concentration, and a series-association branch using learned self-attention; both produce distributions whose symmetrized KL yields the Association Discrepancy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Anomaly-Attention</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Attention mechanism / Transformer modification (two-branch, multi-head)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Implemented multi-head; learned sigma per position and head (sigma shape N x h); used with h=8 heads in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Multivariate time series sequences</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>Same domains as Anomaly Transformer (server monitoring, spacecraft telemetry, water treatment, synthetic benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Point and segment anomalies in sequences</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prior branch: compute P_{i,j} = (1/(sqrt(2π) sigma_i)) * exp(-|j-i|^2 / (2 sigma_i^2)), row-normalized to a distribution; Series branch: compute S = Softmax(QK^T / sqrt(d_model)). Values V used for reconstruction as usual: Z_hat = S V. Store P and S per head/layer to compute AssDis.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Direct comparison to vanilla (single-branch) self-attention (Transformer) as ablation; also compared to fixed (non-learnable) prior and to a power-law prior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Same as model: precision/recall/F1 and AUC; ablation metrics show contribution of Anomaly-Attention</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Ablation: using AssDis with learnable prior and minimax (Anomaly-Attention) yields high performance (e.g., AssDis-only avg F1 91.55% across five datasets; final full model avg F1 94.96%). Substituting power-law prior gave slightly lower results than Gaussian prior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Anomaly-Attention improves over vanilla attention: ablation shows large gains, especially when combined with learnable sigma and minimax optimization; fixed prior or direct maximization yields worse performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>If trained with naive direct maximization of AssDis the learned sigma can collapse (very small) making prior uninformative; requires minimax + stop-gradient to stabilize; Gaussian prior chosen for optimization ease (power-law works but slightly worse).</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Encoding an inductive adjacent-concentration bias as a learnable local prior and explicitly comparing it to learned global attention gives a robust anomaly signal; tuning sigma per position/head adapts the prior to different temporal patterns and anomaly segment lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9429.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9429.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer (vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer (self-attention model as in Vaswani et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The canonical Transformer architecture based on multi-head self-attention; used here as a baseline and as the basis for the Anomaly Transformer modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention is All You Need</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (multi-head self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Configured comparably in experiments (d_model=512, L up to 3-4 layers, h=8) for ablation and baseline comparisons; original Transformer parameterization differs</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Multivariate time series (sequence data)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>Same evaluation domains (server metrics, spacecraft telemetry, industrial sensors, benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous time points/segments detected via reconstruction/prediction errors</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Used as a backbone trained with reconstruction loss; anomaly detection via reconstruction error (pointwise reconstruction/prediction criterion). Single-branch self-attention yields series-association but lacks an explicit prior branch to compute AssDis.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against many classical and deep baselines; also compared to the Anomaly Transformer variant</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Vanilla Transformer (reconstruction criterion) avg F1 reported ~76.62% (Table 2). Performance improves when using association mechanisms introduced by Anomaly Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Anomaly Transformer substantially outperforms vanilla Transformer (absolute F1 gain ~18.34 percentage points), showing the value of modeling prior-association and AssDis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Single-branch attention cannot simultaneously express an adjacent-concentrating prior and learned series-association distributions for direct comparison; pointwise reconstruction criteria can be dominated by normal patterns and are less discriminative for anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9429.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9429.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GTA (graph + transformer for anomaly)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning graph structures with transformer for multivariate time series anomaly detection in iot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior work that applies Transformer-based temporal modeling together with learned graph structures for multivariate time series anomaly detection; uses reconstruction criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning graph structures with transformer for multivariate time series anomaly detection in iot</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GTA</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Graph + Transformer (temporal encoder with graph learning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in current paper (refer to original)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Multivariate time series</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>IoT sensor networks / multivariate monitoring</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Time series anomalies (multivariate sensor anomalies)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as prior art: learns graph relationships among sensors and uses Transformer for temporal modeling; uses reconstruction-based anomaly criterion (in the referenced paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Referenced as a baseline/related approach (not directly evaluated against Anomaly Transformer here but cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not given here; see original paper for metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Cited as an example of prior Transformer use for anomaly detection; Anomaly Transformer differs by explicitly modeling and comparing attention distributions to a learnable prior (AssDis).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Shows prior art applying Transformers to multivariate anomaly detection; motivates the present work to renovate attention to capture association discrepancy explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9429.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9429.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (reference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited as an example of Transformer success in NLP; not used for anomaly detection in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer encoder (bidirectional)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various sizes in literature (base/large); not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned in related work as an example of successful Transformer application (NLP), motivating Transformer adoption for time series.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9429.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9429.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (reference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited (Brown et al., 2020) as an example of large-scale language model (GPT-3) success; not applied for anomaly detection in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Large-scale autoregressive Transformer LM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (up to 175B parameters in original paper) but not used here</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as background on Transformer-based language models' power; no application to lists/tabular anomalies in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning graph structures with transformer for multivariate time series anomaly detection in iot <em>(Rating: 2)</em></li>
                <li>Informer: Beyond efficient transformer for long sequence time-series forecasting <em>(Rating: 1)</em></li>
                <li>Autoformer: Decomposition transformers with Auto-Correlation for long-term series forecasting <em>(Rating: 1)</em></li>
                <li>Attention is All You Need <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9429",
    "paper_id": "paper-a46b06a4b8b4deecf96a4e42cd19b4696f999e66",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "Anomaly Transformer",
            "name_full": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
            "brief_description": "A Transformer-based model that detects anomalies in multivariate time series by learning a per-timepoint association distribution (series-association) and comparing it to a learnable adjacent-concentrating prior (prior-association) via a symmetrized KL association discrepancy (AssDis); trained with a reconstruction objective and a minimax scheme to amplify normal/abnormal separation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Anomaly Transformer",
            "model_type": "Transformer variant with custom two-branch attention (Anomaly-Attention)",
            "model_size": "d_model=512, L=3 layers, h=8 heads (parameter count not explicitly given)",
            "data_type": "Multivariate time series (sliding windows of fixed length, here window size = 100)",
            "data_domain": "System/service monitoring (SMD, PSM), spacecraft telemetry (MSL, SMAP), industrial control / water treatment (SWaT), synthetic/benchmark scenarios (NeurIPS-TS, UCR)",
            "anomaly_type": "Anomalous time points and anomalous segments (point-wise and pattern-wise anomalies; both point-global and pattern-contextual/seasonal/shapelet/trend types in benchmarks)",
            "method_description": "Each time point i obtains two discrete distributions over all time indices: (1) prior-association P_i computed from a learnable Gaussian kernel G(|j-i|; sigma_i) (rescaled to a distribution) to encode adjacent-concentration bias; (2) series-association S_i from standard self-attention Softmax(QK^T / sqrt(d)). The Association Discrepancy AssDis_i is the symmetrized KL (KL(P||S)+KL(S||P)) averaged across layers. Training alternates a minimize phase (prior P is optimized to approximate a stopped-gradient S) and a maximize phase (S is optimized to increase discrepancy while minimizing reconstruction error), preventing trivial collapse of sigma. Final anomaly score = Softmax(-AssDis) elementwise-multiplied by reconstruction error per timepoint.",
            "baseline_methods": "Compared extensively to 18 baselines including reconstruction-based (InterFusion, OmniAnomaly, LSTM-VAE, BeatGAN), density-estimation (DAGMM, MPPCACD, LOF), clustering/one-class (Deep-SVDD, ITAD, THOC), autoregression (VAR, LSTM, CL-MPPCA), classic methods (OC-SVM, IsolationForest), plus other change-point/segmentation baselines referenced in appendix.",
            "performance_metrics": "Precision, Recall, F1-score (per-dataset and averaged), ROC AUC (area under ROC curve); also visualization and per-point statistics of attention weights and learned sigma",
            "performance_results": "State-of-the-art across evaluated benchmarks. Example dataset F1 scores from main table: SMD F1=92.33%, MSL F1=93.59%, SMAP F1=96.69%, SWaT F1=94.07%, PSM F1=97.89%; average (final ablation) F1 across five real datasets = 94.96%. NeurIPS-TS and UCR benchmarks: strong SOTA performance (UCR F1 reported 84.12%). AUC curves show highest AUC on compared datasets.",
            "comparison_to_baseline": "Outperforms all listed baselines in the experiments; ablations show Anomaly Transformer improves over vanilla Transformer by ~18.34 percentage points absolute F1, and that AssDis alone already outperforms previous SOTA (THOC). Learnable prior and the minimax optimization each give further substantial gains.",
            "limitations_or_failure_cases": "Direct maximization of AssDis (without minimax) leads to collapse of Gaussian scale sigma (prior becomes meaningless), so minimax + stop-gradient is required; some failure cases shown in visualization (Figure 5) where criterion still mis-detects certain patterns; method requires sliding-window framing (window size, thresholding proportion r need selection though authors report stability over ranges); theoretical analysis relative to classic autoregressive/state-space models is left as future work; certain alternative discrepancy measures (Wasserstein, L2) performed poorly in this setting.",
            "unique_insights": "Key novel insight: anomalies tend to have self-attention distributions concentrated on adjacent time points (adjacent-concentration bias) because anomalies are rare and cannot form broad associations with normal dynamics; quantifying the discrepancy between a learnable local prior and the learned attention (AssDis) yields an inherently discriminative signal for anomalies. Combining AssDis (normalized) with reconstruction error multiplicatively gives superior detection. Minimax training (prior approximates S in minimize phase, S is pushed away in maximize phase under reconstruction) amplifies normal/abnormal distinguishability.",
            "uuid": "e9429.0",
            "source_info": {
                "paper_title": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Anomaly-Attention",
            "name_full": "Anomaly-Attention (two-branch attention mechanism)",
            "brief_description": "A two-branch attention mechanism introduced in this paper: a prior-association branch using a per-position learnable Gaussian kernel to encode adjacent-concentration, and a series-association branch using learned self-attention; both produce distributions whose symmetrized KL yields the Association Discrepancy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Anomaly-Attention",
            "model_type": "Attention mechanism / Transformer modification (two-branch, multi-head)",
            "model_size": "Implemented multi-head; learned sigma per position and head (sigma shape N x h); used with h=8 heads in experiments",
            "data_type": "Multivariate time series sequences",
            "data_domain": "Same domains as Anomaly Transformer (server monitoring, spacecraft telemetry, water treatment, synthetic benchmarks)",
            "anomaly_type": "Point and segment anomalies in sequences",
            "method_description": "Prior branch: compute P_{i,j} = (1/(sqrt(2π) sigma_i)) * exp(-|j-i|^2 / (2 sigma_i^2)), row-normalized to a distribution; Series branch: compute S = Softmax(QK^T / sqrt(d_model)). Values V used for reconstruction as usual: Z_hat = S V. Store P and S per head/layer to compute AssDis.",
            "baseline_methods": "Direct comparison to vanilla (single-branch) self-attention (Transformer) as ablation; also compared to fixed (non-learnable) prior and to a power-law prior.",
            "performance_metrics": "Same as model: precision/recall/F1 and AUC; ablation metrics show contribution of Anomaly-Attention",
            "performance_results": "Ablation: using AssDis with learnable prior and minimax (Anomaly-Attention) yields high performance (e.g., AssDis-only avg F1 91.55% across five datasets; final full model avg F1 94.96%). Substituting power-law prior gave slightly lower results than Gaussian prior.",
            "comparison_to_baseline": "Anomaly-Attention improves over vanilla attention: ablation shows large gains, especially when combined with learnable sigma and minimax optimization; fixed prior or direct maximization yields worse performance.",
            "limitations_or_failure_cases": "If trained with naive direct maximization of AssDis the learned sigma can collapse (very small) making prior uninformative; requires minimax + stop-gradient to stabilize; Gaussian prior chosen for optimization ease (power-law works but slightly worse).",
            "unique_insights": "Encoding an inductive adjacent-concentration bias as a learnable local prior and explicitly comparing it to learned global attention gives a robust anomaly signal; tuning sigma per position/head adapts the prior to different temporal patterns and anomaly segment lengths.",
            "uuid": "e9429.1",
            "source_info": {
                "paper_title": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Transformer (vanilla)",
            "name_full": "Transformer (self-attention model as in Vaswani et al.)",
            "brief_description": "The canonical Transformer architecture based on multi-head self-attention; used here as a baseline and as the basis for the Anomaly Transformer modifications.",
            "citation_title": "Attention is All You Need",
            "mention_or_use": "use",
            "model_name": "Transformer",
            "model_type": "Transformer (multi-head self-attention)",
            "model_size": "Configured comparably in experiments (d_model=512, L up to 3-4 layers, h=8) for ablation and baseline comparisons; original Transformer parameterization differs",
            "data_type": "Multivariate time series (sequence data)",
            "data_domain": "Same evaluation domains (server metrics, spacecraft telemetry, industrial sensors, benchmarks)",
            "anomaly_type": "Anomalous time points/segments detected via reconstruction/prediction errors",
            "method_description": "Used as a backbone trained with reconstruction loss; anomaly detection via reconstruction error (pointwise reconstruction/prediction criterion). Single-branch self-attention yields series-association but lacks an explicit prior branch to compute AssDis.",
            "baseline_methods": "Compared against many classical and deep baselines; also compared to the Anomaly Transformer variant",
            "performance_metrics": "Precision, Recall, F1",
            "performance_results": "Vanilla Transformer (reconstruction criterion) avg F1 reported ~76.62% (Table 2). Performance improves when using association mechanisms introduced by Anomaly Transformer.",
            "comparison_to_baseline": "Anomaly Transformer substantially outperforms vanilla Transformer (absolute F1 gain ~18.34 percentage points), showing the value of modeling prior-association and AssDis.",
            "limitations_or_failure_cases": "Single-branch attention cannot simultaneously express an adjacent-concentrating prior and learned series-association distributions for direct comparison; pointwise reconstruction criteria can be dominated by normal patterns and are less discriminative for anomalies.",
            "uuid": "e9429.2",
            "source_info": {
                "paper_title": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "GTA (graph + transformer for anomaly)",
            "name_full": "Learning graph structures with transformer for multivariate time series anomaly detection in iot",
            "brief_description": "A referenced prior work that applies Transformer-based temporal modeling together with learned graph structures for multivariate time series anomaly detection; uses reconstruction criterion.",
            "citation_title": "Learning graph structures with transformer for multivariate time series anomaly detection in iot",
            "mention_or_use": "mention",
            "model_name": "GTA",
            "model_type": "Graph + Transformer (temporal encoder with graph learning)",
            "model_size": "Not specified in current paper (refer to original)",
            "data_type": "Multivariate time series",
            "data_domain": "IoT sensor networks / multivariate monitoring",
            "anomaly_type": "Time series anomalies (multivariate sensor anomalies)",
            "method_description": "Referenced as prior art: learns graph relationships among sensors and uses Transformer for temporal modeling; uses reconstruction-based anomaly criterion (in the referenced paper).",
            "baseline_methods": "Referenced as a baseline/related approach (not directly evaluated against Anomaly Transformer here but cited in related work).",
            "performance_metrics": "Not given here; see original paper for metrics.",
            "performance_results": null,
            "comparison_to_baseline": "Cited as an example of prior Transformer use for anomaly detection; Anomaly Transformer differs by explicitly modeling and comparing attention distributions to a learnable prior (AssDis).",
            "limitations_or_failure_cases": null,
            "unique_insights": "Shows prior art applying Transformers to multivariate anomaly detection; motivates the present work to renovate attention to capture association discrepancy explicitly.",
            "uuid": "e9429.3",
            "source_info": {
                "paper_title": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "BERT (reference)",
            "name_full": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "brief_description": "Cited as an example of Transformer success in NLP; not used for anomaly detection in this paper.",
            "citation_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "mention_or_use": "mention",
            "model_name": "BERT",
            "model_type": "Transformer encoder (bidirectional)",
            "model_size": "various sizes in literature (base/large); not specified here",
            "data_type": null,
            "data_domain": null,
            "anomaly_type": null,
            "method_description": "Mentioned in related work as an example of successful Transformer application (NLP), motivating Transformer adoption for time series.",
            "baseline_methods": null,
            "performance_metrics": null,
            "performance_results": null,
            "comparison_to_baseline": null,
            "limitations_or_failure_cases": null,
            "unique_insights": null,
            "uuid": "e9429.4",
            "source_info": {
                "paper_title": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "GPT-3 (reference)",
            "name_full": "Language models are few-shot learners",
            "brief_description": "Cited (Brown et al., 2020) as an example of large-scale language model (GPT-3) success; not applied for anomaly detection in this study.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_type": "Large-scale autoregressive Transformer LM",
            "model_size": "various (up to 175B parameters in original paper) but not used here",
            "data_type": null,
            "data_domain": null,
            "anomaly_type": null,
            "method_description": "Cited as background on Transformer-based language models' power; no application to lists/tabular anomalies in this paper.",
            "baseline_methods": null,
            "performance_metrics": null,
            "performance_results": null,
            "comparison_to_baseline": null,
            "limitations_or_failure_cases": null,
            "unique_insights": null,
            "uuid": "e9429.5",
            "source_info": {
                "paper_title": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning graph structures with transformer for multivariate time series anomaly detection in iot",
            "rating": 2
        },
        {
            "paper_title": "Informer: Beyond efficient transformer for long sequence time-series forecasting",
            "rating": 1
        },
        {
            "paper_title": "Autoformer: Decomposition transformers with Auto-Correlation for long-term series forecasting",
            "rating": 1
        },
        {
            "paper_title": "Attention is All You Need",
            "rating": 1
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        }
    ],
    "cost": 0.018911499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ANOMALY TRANSFORMER: TIME SERIES ANOMALY DETECTION WITH ASSOCIATION DISCREPANCY</h1>
<p>Jiehui Xu, ${ }^{*}$ Haixu Wu, Jianmin Wang, Mingsheng Long ( $\boxtimes$ )<br>School of Software, BNRist, Tsinghua University, China<br>{xjh20,whx20}@mails.tsinghua.edu.cn, {jimwang,mingsheng}@tsinghua.edu.cn</p>
<h4>Abstract</h4>
<p>Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through learning pointwise representation or pairwise association, however, neither is sufficient to reason about the intricate dynamics. Recently, Transformers have shown great power in unified modeling of pointwise representation and pairwise association, and we find that the self-attention weight distribution of each time point can embody rich association with the whole series. Our key observation is that due to the rarity of anomalies, it is extremely difficult to build nontrivial associations from abnormal points to the whole series, thereby, the anomalies' associations shall mainly concentrate on their adjacent time points. This adjacent-concentration bias implies an association-based criterion inherently distinguishable between normal and abnormal points, which we highlight through the Association Discrepancy. Technically, we propose the Anomaly Transformer with a new Anomaly-Attention mechanism to compute the association discrepancy. A minimax strategy is devised to amplify the normal-abnormal distinguishability of the association discrepancy. The Anomaly Transformer achieves state-of-theart results on six unsupervised time series anomaly detection benchmarks of three applications: service monitoring, space \&amp; earth exploration, and water treatment.</p>
<h2>1 INTRODUCTION</h2>
<p>Real-world systems always work in a continuous way, which can generate several successive measurements monitored by multi-sensors, such as industrial equipment, space probe, etc. Discovering the malfunctions from large-scale system monitoring data can be reduced to detecting the abnormal time points from time series, which is quite meaningful for ensuring security and avoiding financial loss. But anomalies are usually rare and hidden by vast normal points, making the data labeling hard and expensive. Thus, we focus on time series anomaly detection under the unsupervised setting.</p>
<p>Unsupervised time series anomaly detection is extremely challenging in practice. The model should learn informative representations from complex temporal dynamics through unsupervised tasks. Still, it should also derive a distinguishable criterion that can detect the rare anomalies from plenty of normal time points. Various classic anomaly detection methods have provided many unsupervised paradigms, such as the density-estimation methods proposed in local outlier factor (LOF, Breunig et al. (2000)), clustering-based methods presented in one-class SVM (OC-SVM, Schölkopf et al. (2001)) and SVDD (Tax \&amp; Duin, 2004). These classic methods do not consider the temporal information and are difficult to generalize to unseen real scenarios. Benefiting from the representation learning capability of neural networks, recent deep models (Su et al., 2019; Shen et al., 2020; Li et al., 2021) have achieved superior performance. A major category of methods focus on learning pointwise representations through well-designed recurrent networks and are self-supervised by the reconstruction or autoregressive task. Here, a natural and practical anomaly criterion is the pointwise reconstruction or prediction error. However, due to the rarity of anomalies, the pointwise representation is less informative for complex temporal patterns and can be dominated by normal time points, making anomalies less distinguishable. Also, the reconstruction or prediction error is calculated point by point, which cannot provide a comprehensive description of the temporal context.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Another major category of methods detect anomalies based on explicit association modeling. The vector autoregression and state space models fall into this category. The graph was also used to capture the association explicitly, through representing time series with different time points as vertices and detecting anomalies by random walk (Cheng et al., 2008; 2009). In general, it is hard for these classic methods to learn informative representations and model fine-grained associations. Recently, graph neural network (GNN) has been applied to learn the dynamic graph among multiple variables in multivariate time series (Zhao et al., 2020; Deng \&amp; Hooi, 2021). While being more expressive, the learned graph is still limited to a single time point, which is insufficient for complex temporal patterns. Besides, subsequence-based methods detect anomalies by calculating the similarity among subsequences (Boniol \&amp; Palpanas, 2020). While exploring wider temporal context, these methods cannot capture the fine-grained temporal association between each time point and the whole series.</p>
<p>In this paper, we adapt Transfomers (Vaswani et al., 2017) to time series anomaly detection in the unsupervised regime. Transformers have achieved great progress in various areas, including natural language processing (Brown et al., 2020), machine vision (Liu et al., 2021) and time series (Zhou et al., 2021). This success is attributed to its great power in unified modeling of global representation and long-range relation. Applying Transformers to time series, we find that the temporal association of each time point can be obtained from the self-attention map, which presents as a distribution of its association weights to all the time points along the temporal dimension. The association distribution of each time point can provide a more informative description for the temporal context, indicating dynamic patterns, such as the period or trend of time series. We name the above association distribution as the series-association, which can be discovered from the raw series by Transformers</p>
<p>Further, we observe that due to the rarity of anomalies and the dominance of normal patterns, it is harder for anomalies to build strong associations with the whole series. The associations of anomalies shall concentrate on the adjacent time points that are more likely to contain similar abnormal patterns due to the continuity. Such an adjacent-concentration inductive bias is referred to as the prior-association. In contrast, the dominating normal time points can discover informative associations with the whole series, not limiting to the adjacent area. Based on this observation, we try to utilize the inherent normal-abnormal distinguishability of the association distribution. This leads to a new anomaly criterion for each time point, quantified by the distance between each time point's prior-association and its series-association, named as Association Discrepancy. As aforementioned, because the associations of anomalies are more likely to be adjacent-concentrating, anomalies will present a smaller association discrepancy than normal time points.</p>
<p>Go beyond previous methods, we introduce Transformers to the unsupervised time series anomaly detection and propose the Anomaly Transformer for association learning. To compute the Association Discrepancy, we renovate the self-attention mechanism to the Anomaly-Attention, which contains a two-branch structure to model the prior-association and series-association of each time point respectively. The prior-association employs the learnable Gaussian kernel to present the adjacentconcentration inductive bias of each time point, while the series-association corresponds to the selfattention weights learned from raw series. Besides, a minimax strategy is applied between the two branches, which can amplify the normal-abnormal distinguishability of the Association Discrepancy and further derive a new association-based criterion. Anomaly Transformer achieves strong results on six benchmarks, covering three real applications. The contributions are summarized as follows:</p>
<ul>
<li>Based on the key observation of Association Discrepancy, we propose the Anomaly Transformer with an Anomaly-Attention mechanism, which can model the prior-association and series-association simultaneously to embody the Association Discrepancy.</li>
<li>We propose a minimax strategy to amplify the normal-abnormal distinguishability of the Association Discrepancy and further derive a new association-based detection criterion.</li>
<li>Anomaly Transformer achieves the state-of-the-art anomaly detection results on six benchmarks for three real applications. Extensive ablations and insightful case studies are given.</li>
</ul>
<h1>2 Related Work</h1>
<h3>2.1 Unsupervised Time Series Anomaly Detection</h3>
<p>As a vital real-world problem, unsupervised time series anomaly detection has been widely explored. Categorizing by anomaly determination criterion, the paradigms roughly include the densityestimation, clustering-based, reconstruction-based and autoregression-based methods.</p>
<p>As for the density-estimation methods, the classic methods local outlier factor (LOF, Breunig et al. (2000)) and connectivity outlier factor (COF, Tang et al. (2002)) calculates the local density and local connectivity for outlier determination respectively. DAGMM (Zong et al., 2018) and MPPCACD (Yairi et al., 2017) integrate the Gaussian Mixture Model to estimate the density of representations.</p>
<p>In clustering-based methods, the anomaly score is always formalized as the distance to cluster center. SVDD (Tax \&amp; Duin, 2004) and Deep SVDD (Ruff et al., 2018) gather the representations from normal data to a compact cluster. THOC (Shen et al., 2020) fuses the multi-scale temporal features from intermediate layers by a hierarchical clustering mechanism and detects the anomalies by the multi-layer distances. ITAD (Shin et al., 2020) conducts the clustering on decomposed tensors.</p>
<p>The reconstruction-based models attempt to detect the anomalies by the reconstruction error. Park et al. (2018) presented the LSTM-VAE model that employed the LSTM backbone for temporal modeling and the Variational AutoEncoder (VAE) for reconstruction. OmniAnomaly proposed by Su et al. (2019) further extends the LSTM-VAE model with a normalizing flow and uses the reconstruction probabilities for detection. InterFusion from Li et al. (2021) renovates the backbone to a hierarchical VAE to model the inter- and intra- dependency among multiple series simultaneously. GANs (Goodfellow et al., 2014) are also used for reconstruction-based anomaly detection (Schlegl et al., 2019; Li et al., 2019a; Zhou et al., 2019) and perform as an adversarial regularization.</p>
<p>The autoregression-based models detect the anomalies by the prediction error. VAR extends ARIMA (Anderson \&amp; Kendall, 1976) and predicts the future based on the lag-dependent covariance. The autoregressive model can also be replaced by LSTMs (Hundman et al., 2018; Tariq et al., 2019).</p>
<p>This paper is characterized by a new association-based criterion. Different from the random walk and subsequence-based methods (Cheng et al., 2008; Boniol \&amp; Palpanas, 2020), our criterion is embodied by a co-design of the temporal models for learning more informative time-point associations.</p>
<h1>2.2 Transformers for Time Series Analysis</h1>
<p>Recently, Transformers (Vaswani et al., 2017) have shown great power in sequential data processing, such as natural language processing (Devlin et al., 2019; Brown et al., 2020), audio processing (Huang et al., 2019) and computer vision (Dosovitskiy et al., 2021; Liu et al., 2021). For time series analysis, benefiting from the advantage of the self-attention mechanism, Transformers are used to discover the reliable long-range temporal dependencies (Kitaev et al., 2020; Li et al., 2019b; Zhou et al., 2021; Wu et al., 2021). Especially for time series anomaly detection, GTA proposed by Chen et al. (2021) employs the graph structure to learn the relationship among multiple IoT sensors, as well as the Transformer for temporal modeling and the reconstruction criterion for anomaly detection. Unlike the previous usage of Transformers, Anomaly Transformer renovates the self-attention mechanism to the Anomaly-Attention based on the key observation of association discrepancy.</p>
<h2>3 Method</h2>
<p>Suppose monitoring a successive system of $d$ measurements and recording the equally spaced observations over time. The observed time series $\mathcal{X}$ is denoted by a set of time points $\left{x_{1}, x_{2}, \cdots, x_{N}\right}$, where $x_{t} \in \mathbb{R}^{d}$ represents the observation of time $t$. The unsupervised time series anomaly detection problem is to determine whether $x_{t}$ is anomalous or not without labels.</p>
<p>As aforementioned, we highlight the key to unsupervised time series anomaly detection as learning informative representations and finding distinguishable criterion. We propose the Anomaly Transformer to discover more informative associations and tackle this problem by learning the Association Discrepancy, which is inherently normal-abnormal distinguishable. Technically, we propose the Anomaly-Attention to embody the prior-association and series-associations, along with a minimax optimization strategy to obtain a more distinguishable association discrepancy. Co-designed with the architecture, we derive an association-based criterion based on the learned association discrepancy.</p>
<h3>3.1 Anomaly Transformer</h3>
<p>Given the limitation of Transformers (Vaswani et al., 2017) for anomaly detection, we renovate the vanilla architecture to the Anomaly Transformer (Figure 1) with an Anomaly-Attention mechanism.</p>
<p>Overall Architecture Anomaly Transformer is characterized by stacking the Anomaly-Attention blocks and feed-forward layers alternately. This stacking structure is conducive to learning underlying associations from deep multi-level features. Suppose the model contains $L$ layers with length- $N$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Anomaly Transformer architecture. Anomaly-Attention (left) models the prior-association and series-association simultaneously. In addition to the reconstruction loss, our model is also optimized by the minimax strategy with a specially-designed stop-gradient mechanism (gray arrows) to constrain the prior- and series- associations for more distinguishable association discrepancy.</p>
<p>input time series $\mathcal{X} \in \mathbb{R}^{N \times d}$. The overall equations of the $l$-th layer are formalized as:</p>
<p>$$
\begin{aligned}
&amp; \mathcal{Z}^{l}=\text { Layer-Norm }\left(\text { Anomaly-Attention }\left(\mathcal{X}^{l-1}\right)+\mathcal{X}^{l-1}\right) \
&amp; \mathcal{X}^{l}=\text { Layer-Norm }\left(\text { Feed-Forward }\left(\mathcal{Z}^{l}\right)+\mathcal{Z}^{l}\right)
\end{aligned}
$$</p>
<p>where $\mathcal{X}^{l} \in \mathbb{R}^{N \times d_{\text {model }}}, l \in{1, \cdots, L}$ denotes the output of the $l$-th layer with $d_{\text {model }}$ channels. The initial input $\mathcal{X}^{0}=\operatorname{Embedding}(\mathcal{X})$ represents the embedded raw series. $\mathcal{Z}^{l} \in \mathbb{R}^{N \times d_{\text {model }}}$ is the $l$-th layer's hidden representation. Anomaly-Attention( $\cdot$ ) is to compute the association discrepancy.</p>
<p>Anomaly-Attention Note that the single-branch self-attention mechanism (Vaswani et al., 2017) cannot model the prior-association and series-association simultaneously. We propose the AnomalyAttention with a two-branch structure (Figure 1). For the prior-association, we adopt a learnable Gaussian kernel to calculate the prior with respect to the relative temporal distance. Benefiting from the unimodal property of the Gaussian kernel, this design can pay more attention to the adjacent horizon constitutionally. We also use a learnable scale parameter $\sigma$ for the Gaussian kernel, making the prior-associations adapt to the various time series patterns, such as different lengths of anomaly segments. The series-association branch is to learn the associations from raw series, which can find the most effective associations adaptively. Note that these two forms maintain the temporal dependencies of each time point, which are more informative than point-wise representation. They also reflect the adjacent-concentration prior and the learned real associations respectively, whose discrepancy shall be normal-abnormal distinguishable. The Anomaly-Attention in the $l$-th layer is:</p>
<p>$$
\begin{aligned}
&amp; \text { Initialization: } \mathcal{Q}, \mathcal{K}, \mathcal{V}, \sigma=\mathcal{X}^{l-1} W_{\mathcal{Q}}^{l}, \mathcal{X}^{l-1} W_{\mathcal{K}}^{l}, \mathcal{X}^{l-1} W_{\mathcal{V}}^{l}, \mathcal{X}^{l-1} W_{\sigma}^{l} \
&amp; \text { Prior-Association: } \mathcal{P}^{l}=\operatorname{Rescale}\left(\left[\frac{1}{\sqrt{2 \pi} \sigma_{i}} \exp \left(-\frac{|j-i|^{2}}{2 \sigma_{i}^{2}}\right)\right]<em _model="{model" _text="\text">{i, j \in{1, \cdots, N}}\right) \
&amp; \text { Series-Association: } \mathcal{S}^{l}=\operatorname{Softmax}\left(\frac{\mathcal{Q} \mathcal{K}^{\mathrm{T}}}{\sqrt{d</em>\right) \
&amp; \text { Reconstruction: } \widehat{\mathcal{Z}}^{l}=\mathcal{S}^{l} \mathcal{V}
\end{aligned}
$$}}}</p>
<p>where $\mathcal{Q}, \mathcal{K}, \mathcal{V} \in \mathbb{R}^{N \times d_{\text {model }}}, \sigma \in \mathbb{R}^{N \times 1}$ represent the query, key, value of self-attention and the learned scale respectively. $W_{\mathcal{Q}}^{l}, W_{\mathcal{K}}^{l}, W_{\mathcal{V}}^{l} \in \mathbb{R}^{d_{\text {model }} \times d_{\text {model }}}, W_{\sigma}^{l} \in \mathbb{R}^{d_{\text {model }} \times 1}$ represent the parameter matrices for $\mathcal{Q}, \mathcal{K}, \mathcal{V}, \sigma$ in the $l$-th layer respectively. Prior-association $\mathcal{P}^{l} \in \mathbb{R}^{N \times N}$ is generated based on the learned scale $\sigma \in \mathbb{R}^{N \times 1}$ and the $i$-th element $\sigma_{i}$ corresponds to the $i$-th time point. Concretely, for the $i$-th time point, its association weight to the $j$-th point is calculate by the Gaussian kernel $G\left(|j-i| ; \sigma_{i}\right)=\frac{1}{\sqrt{2 \pi} \sigma_{i}} \exp \left(-\frac{|j-i|^{2}}{2 \sigma_{i}^{2}}\right)$ w.r.t. the distance $|j-i|$. Further, we use Rescale $(\cdot)$ to transform the association weights to discrete distributions $\mathcal{P}^{l}$ by dividing the row sum. $\mathcal{S}^{l} \in \mathbb{R}^{N \times N}$ denotes the series-associations. $\operatorname{Softmax}(\cdot)$ normalizes the attention map along the last dimension.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Minimax association learning. At the minimize phase, the prior-association minimizes the Association Discrepancy within the distribution family derived by Gaussian kernel. At the maximize phase, the series-association maximizes the Association Discrepancy under the reconstruction loss.</p>
<p>Thus, each row of $\mathcal{S}^{l}$ forms a discrete distribution. $\widehat{\mathcal{Z}}^{l} \in \mathbb{R}^{N \times d_{\text {model }}}$ is the hidden representation after the Anomaly-Attention in the $l$-th layer. We use Anomaly-Attention( $\cdot$ ) to summarize Equation 2.
In the multi-head version that we use, the learned scale is $\sigma \in \mathbb{R}^{N \times h}$ for $h$ heads. $\mathcal{Q}<em m="m">{m}, \mathcal{K}</em>}, \mathcal{V<em _model="{model" _text="\text">{m} \in$ $\mathbb{R}^{N \times \frac{d</em>}}}{h}}$ denote the query, key and value of the $m$-th head respectively. The block concatenates the outputs $\left{\widehat{\mathcal{Z}<em _model="{model" _text="\text">{m}^{l} \in \mathbb{R}^{N \times \frac{d</em>\right}}}}{h}<em _model="{model" _text="\text">{1 \leq m \leq h}$ from multiple heads and gets the final result $\widehat{\mathcal{Z}}^{l} \in \mathbb{R}^{N \times d</em>$.}}</p>
<p>Association Discrepancy We formalize the Association Discrepancy as the symmetrized KL divergence between prior- and series- associations, which represents the information gain between these two distributions (Neal, 2007). We average the association discrepancy from multiple layers to combine the associations from multi-level features into a more informative measure as:</p>
<p>$$
\operatorname{AssDis}(\mathcal{P}, \mathcal{S} ; \mathcal{X})=\left[\frac{1}{L} \sum_{l=1}^{L}\left(\operatorname{KL}\left(\mathcal{P}<em i_:="i,:">{i,:}^{l} | \mathcal{S}</em>}^{l}\right)+\operatorname{KL}\left(\mathcal{S<em i_:="i,:">{i,:}^{l} | \mathcal{P}</em>
$$}^{l}\right)\right)\right]_{i=1, \cdots, N</p>
<p>where $\operatorname{KL}(\cdot \mid \cdot)$ is the KL divergence computed between two discrete distributions corresponding to every row of $\mathcal{P}^{l}$ and $\mathcal{S}^{l} . \operatorname{AssDis}(\mathcal{P}, \mathcal{S} ; \mathcal{X}) \in \mathbb{R}^{N \times 1}$ is the point-wise association discrepancy of $\mathcal{X}$ with respect to prior-association $\mathcal{P}$ and series-association $\mathcal{S}$ from multiple layers. The $i$-th element of results corresponds to the $i$-th time point of $\mathcal{X}$. From previous observation, anomalies will present smaller $\operatorname{AssDis}(\mathcal{P}, \mathcal{S} ; \mathcal{X})$ than normal time points, which makes AssDis inherently distinguishable.</p>
<h1>3.2 Minimax Association Learning</h1>
<p>As an unsupervised task, we employ the reconstruction loss for optimizing our model. The reconstruction loss will guide the series-association to find the most informative associations. To further amplify the difference between normal and abnormal time points, we also use an additional loss to enlarge the association discrepancy. Due to the unimodal property of the prior-association, the discrepancy loss will guide the series-association to pay more attention to the non-adjacent area, which makes the reconstruction of anomalies harder and makes anomalies more identifiable. The loss function for input series $\mathcal{X} \in \mathbb{R}^{N \times d}$ is formalized as:</p>
<p>$$
\mathcal{L}<em _mathrm_F="\mathrm{F">{\text {Total }}(\widetilde{\mathcal{X}}, \mathcal{P}, \mathcal{S}, \lambda ; \mathcal{X})=|\mathcal{X}-\widetilde{\mathcal{X}}|</em>
$$}}^{2}-\lambda \times|\operatorname{AssDis}(\mathcal{P}, \mathcal{S} ; \mathcal{X})|_{1</p>
<p>where $\widetilde{\mathcal{X}} \in \mathbb{R}^{N \times d}$ denotes the reconstruction of $\mathcal{X} .|\cdot|<em k="k">{\mathrm{F}},|\cdot|</em>$ indicate the Frobenius and $k$-norm. $\lambda$ is to trade off the loss terms. When $\lambda&gt;0$, the optimization is to enlarge the association discrepancy. A minimax strategy is proposed to make the association discrepancy more distinguishable.</p>
<p>Minimax Strategy Note that directly maximizing the association discrepancy will extremely reduce the scale parameter of the Gaussian kernel (Neal, 2007), making the prior-association meaningless. Towards a better control of association learning, we propose a minimax strategy (Figure 2). Concretely, for the minimize phase, we drive the prior-association $\mathcal{P}^{l}$ to approximate the seriesassociation $\mathcal{S}^{l}$ that is learned from raw series. This process will make the prior-association adapt to various temporal patterns. For the maximize phase, we optimize the series-association to enlarge the association discrepancy. This process forces the series-association to pay more attention to the non-adjacent horizon. Thus, integrating the reconstruction loss, the loss functions of two phases are:</p>
<p>$$
\begin{aligned}
&amp; \text { Minimize Phase: } \mathcal{L}<em _detach="{detach" _text="\text">{\text {Total }}(\widetilde{\mathcal{X}}, \mathcal{P}, \mathcal{S}</em>) \
&amp; \text { Maximize Phase: } \mathcal{L}}},-\lambda ; \mathcal{X<em _detach="{detach" _text="\text">{\text {Total }}(\widetilde{\mathcal{X}}, \mathcal{P}</em>)
\end{aligned}
$$}}, \mathcal{S}, \lambda ; \mathcal{X</p>
<p>where $\lambda&gt;0$ and $*<em _detach="{detach" _text="\text">{\text {detach }}$ means to stop the gradient backpropagation of the association (Figure 1). As $\mathcal{P}$ approximates $\mathcal{S}</em>$ in the minimize phase, the maximize phase will conduct a stronger constraint to the series-association, forcing the time points to pay more attention to the non-adjacent area. Under the reconstruction loss, this is much harder for anomalies to achieve than normal time points, thereby amplifying the normal-abnormal distinguishability of the association discrepancy.}</p>
<p>Association-based Anomaly Criterion We incorporate the normalized association discrepancy to the reconstruction criterion, which will take the benefits of both temporal representation and the distinguishable association discrepancy. The final anomaly score of $\mathcal{X} \in \mathbb{R}^{N \times d}$ is shown as follows:</p>
<p>$$
\operatorname{AnomalyScore}(\mathcal{X})=\operatorname{Softmax}(-\operatorname{AssDis}(\mathcal{P}, \mathcal{S} ; \mathcal{X})) \odot\left[\left|\mathcal{X}<em i_:="i,:">{i,:}-\widetilde{\mathcal{X}}</em>\right|<em N="N" _cdots_="\cdots," i="1,">{2}^{2}\right]</em>
$$</p>
<p>where $\odot$ is the element-wise multiplication. $\operatorname{AnomalyScore}(\mathcal{X}) \in \mathbb{R}^{N \times 1}$ denotes the point-wise anomaly criterion of $\mathcal{X}$. Towards a better reconstruction, anomalies usually decrease the association discrepancy, which will still derive a higher anomaly score. Thus, this design can make the reconstruction error and the association discrepancy collaborate to improve detection performance.</p>
<h1>4 EXPERIMENTS</h1>
<p>We extensively evaluate Anomaly Transformer on six benchmarks for three practical applications.
Datasets Here is a description of the six experiment datasets: (1) SMD (Server Machine Dataset, Su et al. (2019)) is a 5-week-long dataset that is collected from a large Internet company with 38 dimensions. (2) PSM (Pooled Server Metrics, Abdulaal et al. (2021)) is collected internally from multiple application server nodes at eBay with 26 dimensions. (3) Both MSL (Mars Science Laboratory rover) and SMAP (Soil Moisture Active Passive satellite) are public datasets from NASA (Hundman et al., 2018) with 55 and 25 dimensions respectively, which contain the telemetry anomaly data derived from the Incident Surprise Anomaly (ISA) reports of spacecraft monitoring systems. (4) SWaT (Secure Water Treatment, Mathur \&amp; Tippenhauer (2016)) is obtained from 51 sensors of the critical infrastructure system under continuous operations. (5) NeurIPS-TS (NeurIPS 2021 Time Series Benchmark) is a dataset proposed by Lai et al. (2021) and includes five time series anomaly scenarios categorized by behavior-driven taxonomy as point-global, pattern-contextual, pattern-shapelet, pattern-seasonal and pattern-trend. The statistical details are summarized in Table 13 of Appendix.</p>
<p>Implementation details Following the well-established protocol in Shen et al. (2020), we adopt a non-overlapped sliding window to obtain a set of sub-series. The sliding window is with a fixed size of 100 for all datasets. We label the time points as anomalies if their anomaly scores (Equation 6) are larger than a certain threshold $\delta$. The threshold $\delta$ is determined to make $r$ proportion data of the validation dataset labeled as anomalies. For the main results, we set $r=0.1 \%$ for SWaT, $0.5 \%$ for SMD and $1 \%$ for other datasets. We adopt the widely-used adjustment strategy (Xu et al., 2018; Su et al., 2019; Shen et al., 2020): if a time point in a certain successive abnormal segment is detected, all anomalies in this abnormal segment are viewed to be correctly detected. This strategy is justified from the observation that an abnormal time point will cause an alert and further make the whole segment noticed in real-world applications. Anomaly Transformer contains 3 layers. We set the channel number of hidden states $d_{\text {model }}$ as 512 and the number of heads $h$ as 8 . The hyperparameter $\lambda$ (Equation 4) is set as 3 for all datasets to trade-off two parts of the loss function. We use the ADAM (Kingma \&amp; Ba, 2015) optimizer with an initial learning rate of $10^{-4}$. The training process is early stopped within 10 epochs with the batch size of 32 . All the experiments are implemented in Pytorch (Paszke et al., 2019) with a single NVIDIA TITAN RTX 24GB GPU.</p>
<p>Baselines We extensively compare our model with 18 baselines, including the reconstructionbased models: InterFusion (2021), BeatGAN (2019), OmniAnomaly (2019), LSTM-VAE (2018); the density-estimation models: DAGMM (2018), MPPCACD (2017), LOF (2000); the clusteringbased methods: ITAD (2020), THOC (2020), Deep-SVDD (2018); the autoregression-based models: CL-MPPCA (2019), LSTM (2018), VAR (1976); the classic methods: OC-SVM (2004), IsolationForest (2008). Another 3 baselines from change point detection and time series segmentation are deferred to Appendix I. InterFusion (2021) and THOC (2020) are the state-of-the-art deep models.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: ROC curves (horizontal-axis: false-positive rate; vertical-axis: true-positive rate) for five corresponding datasets. A higher AUC value (area under the ROC curve) indicates a better performance. The predefined threshold proportion $r$ is in ${0.5 \%, 1.0 \%, 1.5 \%, 2.0 \%, 10 \%, 20 \%, 30 \%}$.</p>
<p>Table 1: Quantitative results for Anomaly Transformer (Ours) in five real-world datasets. The $P$, $R$ and $F 1$ represent the precision, recall and F1-score (as \%) respectively. F1-score is the harmonic mean of precision and recall. For these three metrics, a higher value indicates a better performance.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>SMD</th>
<th></th>
<th></th>
<th>MSL</th>
<th></th>
<th></th>
<th>SMAP</th>
<th></th>
<th></th>
<th>SWaT</th>
<th></th>
<th></th>
<th>PSM</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Metric</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
</tr>
<tr>
<td>OCSVM</td>
<td>44.34</td>
<td>76.72</td>
<td>56.19</td>
<td>59.78</td>
<td>86.87</td>
<td>70.82</td>
<td>53.85</td>
<td>59.07</td>
<td>56.34</td>
<td>45.39</td>
<td>49.22</td>
<td>47.23</td>
<td>62.75</td>
<td>80.89</td>
<td>70.67</td>
</tr>
<tr>
<td>IsolationForest</td>
<td>42.31</td>
<td>73.29</td>
<td>53.64</td>
<td>53.94</td>
<td>86.54</td>
<td>66.45</td>
<td>52.39</td>
<td>59.07</td>
<td>55.53</td>
<td>49.29</td>
<td>44.95</td>
<td>47.02</td>
<td>76.09</td>
<td>92.45</td>
<td>83.48</td>
</tr>
<tr>
<td>LOF</td>
<td>56.34</td>
<td>39.86</td>
<td>46.68</td>
<td>47.72</td>
<td>85.25</td>
<td>61.18</td>
<td>58.93</td>
<td>56.33</td>
<td>57.60</td>
<td>72.15</td>
<td>65.43</td>
<td>68.62</td>
<td>57.89</td>
<td>90.49</td>
<td>70.61</td>
</tr>
<tr>
<td>Deep-SVDD</td>
<td>78.54</td>
<td>79.67</td>
<td>79.10</td>
<td>91.92</td>
<td>76.63</td>
<td>83.58</td>
<td>89.93</td>
<td>56.02</td>
<td>69.04</td>
<td>80.42</td>
<td>84.45</td>
<td>82.39</td>
<td>95.41</td>
<td>86.49</td>
<td>90.73</td>
</tr>
<tr>
<td>DAGMM</td>
<td>67.30</td>
<td>49.89</td>
<td>57.30</td>
<td>89.60</td>
<td>63.93</td>
<td>74.62</td>
<td>86.45</td>
<td>56.73</td>
<td>68.51</td>
<td>89.92</td>
<td>57.84</td>
<td>70.40</td>
<td>93.49</td>
<td>70.03</td>
<td>80.08</td>
</tr>
<tr>
<td>MMPCACD</td>
<td>71.20</td>
<td>79.28</td>
<td>75.02</td>
<td>81.42</td>
<td>61.31</td>
<td>69.95</td>
<td>88.61</td>
<td>75.84</td>
<td>81.73</td>
<td>82.52</td>
<td>68.29</td>
<td>74.73</td>
<td>76.26</td>
<td>78.35</td>
<td>77.29</td>
</tr>
<tr>
<td>VAR</td>
<td>78.35</td>
<td>70.26</td>
<td>74.08</td>
<td>74.68</td>
<td>81.42</td>
<td>77.90</td>
<td>81.38</td>
<td>53.88</td>
<td>64.83</td>
<td>81.59</td>
<td>60.29</td>
<td>69.34</td>
<td>90.71</td>
<td>83.82</td>
<td>87.13</td>
</tr>
<tr>
<td>LSTM</td>
<td>78.55</td>
<td>85.28</td>
<td>81.78</td>
<td>85.45</td>
<td>82.50</td>
<td>83.95</td>
<td>89.41</td>
<td>78.13</td>
<td>83.39</td>
<td>86.15</td>
<td>83.27</td>
<td>84.69</td>
<td>76.93</td>
<td>89.64</td>
<td>82.80</td>
</tr>
<tr>
<td>CL-MPPCA</td>
<td>82.36</td>
<td>76.07</td>
<td>79.09</td>
<td>73.71</td>
<td>88.54</td>
<td>80.44</td>
<td>86.13</td>
<td>63.16</td>
<td>72.88</td>
<td>76.78</td>
<td>81.50</td>
<td>79.07</td>
<td>56.02</td>
<td>99.93</td>
<td>71.80</td>
</tr>
<tr>
<td>ITAD</td>
<td>86.22</td>
<td>73.71</td>
<td>79.48</td>
<td>69.44</td>
<td>84.09</td>
<td>76.07</td>
<td>82.42</td>
<td>66.89</td>
<td>73.85</td>
<td>63.13</td>
<td>52.08</td>
<td>57.08</td>
<td>72.80</td>
<td>64.02</td>
<td>68.13</td>
</tr>
<tr>
<td>LSTM-VAE</td>
<td>75.76</td>
<td>90.08</td>
<td>82.30</td>
<td>85.49</td>
<td>79.94</td>
<td>82.62</td>
<td>92.20</td>
<td>67.75</td>
<td>78.10</td>
<td>76.00</td>
<td>89.50</td>
<td>82.20</td>
<td>73.62</td>
<td>89.92</td>
<td>80.96</td>
</tr>
<tr>
<td>BeatGAN</td>
<td>72.90</td>
<td>84.09</td>
<td>78.10</td>
<td>89.75</td>
<td>85.42</td>
<td>87.53</td>
<td>92.38</td>
<td>55.85</td>
<td>69.61</td>
<td>64.01</td>
<td>87.46</td>
<td>73.92</td>
<td>90.30</td>
<td>93.84</td>
<td>92.04</td>
</tr>
<tr>
<td>OmniAnomaly</td>
<td>83.68</td>
<td>86.82</td>
<td>85.22</td>
<td>89.02</td>
<td>86.37</td>
<td>87.67</td>
<td>92.49</td>
<td>81.99</td>
<td>86.92</td>
<td>81.42</td>
<td>84.30</td>
<td>82.83</td>
<td>88.39</td>
<td>74.46</td>
<td>80.83</td>
</tr>
<tr>
<td>InterFusion</td>
<td>87.02</td>
<td>85.43</td>
<td>86.22</td>
<td>81.28</td>
<td>92.70</td>
<td>86.62</td>
<td>89.77</td>
<td>88.52</td>
<td>89.14</td>
<td>80.59</td>
<td>85.58</td>
<td>83.01</td>
<td>83.61</td>
<td>83.45</td>
<td>83.52</td>
</tr>
<tr>
<td>THOC</td>
<td>79.76</td>
<td>90.95</td>
<td>84.99</td>
<td>88.45</td>
<td>90.97</td>
<td>89.69</td>
<td>92.06</td>
<td>89.34</td>
<td>90.68</td>
<td>83.94</td>
<td>86.36</td>
<td>85.13</td>
<td>88.14</td>
<td>90.99</td>
<td>89.54</td>
</tr>
<tr>
<td>Ours</td>
<td>89.40</td>
<td>95.45</td>
<td>92.33</td>
<td>92.09</td>
<td>95.15</td>
<td>93.59</td>
<td>94.13</td>
<td>99.40</td>
<td>96.69</td>
<td>91.55</td>
<td>96.73</td>
<td>94.07</td>
<td>96.91</td>
<td>98.90</td>
<td>97.89</td>
</tr>
</tbody>
</table>
<h1>4.1 MAIN RESULTS</h1>
<p>Real-world datasets We extensively evaluate our model on five real-world datasets with ten competitive baselines. As shown in Table 1, Anomaly Transformer achieves the consistent state-of-theart on all benchmarks. We observe that deep models that consider the temporal information outperform the general anomaly detection model, such as Deep-SVDD (Ruff et al., 2018) and DAGMM (Zong et al., 2018), which verifies the effectiveness of temporal modeling. Our proposed Anomaly Transformer goes beyond the point-wise representation learned by RNNs and models the more informative associations. The results in Table 1 are persuasive for the advantage of association learning in time series anomaly detection. In addition, we plot the ROC curve in Figure 3 for a complete comparison. Anomaly Transformer has the highest AUC values on all five datasets. It means that our model performs well in the false-positive and true-positive rates under various pre-selected thresholds, which is important for real-world applications.</p>
<p>NeurIPS-TS benchmark This benchmark is generated from well-designed rules proposed by Lai et al. (2021), which completely includes all types of anomalies, covering both the pointwise and pattern-wise anomalies. As shown in Figure 4, Anomaly Transformer can still achieve state-of-the-art performance. This verifies the effectiveness of our model on various anomalies.</p>
<p>Ablation study As shown in Table 2, we further investigate the effect of each part in our model. Our association-based criterion outperforms the widely-used reconstruction criterion consistently. Specifically, the association-based criterion brings a remarkable $18.76 \%(76.20 \rightarrow 94.96)$ averaged absolute F1-score promotion. Also, directly taking the association discrepancy as the criterion still achieves a good performance (F1-score: $91.55 \%$ ) and surpasses the previous state-of-the-art model</p>
<p>Figure 4: Results for NeurIPS-TS.</p>
<p>THOC (F1-score: 88.01% calculated from Table 1). Besides, the learnable prior-association (corresponding to $\sigma$ in Equation 2) and the minimax strategy can further improve our model and get 8.43% (79.05→87.48) and 7.48% (87.48→94.96) averaged absolute promotions respectively. Finally, our proposed Anomaly Transformer surpasses the pure Transformer by 18.34% (76.62→94.96) absolute improvement. These verify that each module of our design is effective and necessary. More ablations of association discrepancy can be found in Appendix D.</p>
<p>Table 2: Ablation results (F1-score) in anomaly criterion, prior-association and optimization strategy. Recon, AssDis and Assoc mean the pure reconstruction performance, pure association discrepancy and our proposed association-based criterion respectively. Fix is to fix Learnable scale parameter $\sigma$ of prior-association as 1.0. Max and Minimax ref to the strategies for association discrepancy in the maximization (Equation 4) and minimax (Equation 5) way respectively.</p>
<table>
<thead>
<tr>
<th>Architecture</th>
<th>Anomaly Criterion</th>
<th>Prior- Association</th>
<th>Optimization Strategy</th>
<th>SMD</th>
<th>MSL</th>
<th>SMAP</th>
<th>SWaT</th>
<th>PSM</th>
<th>Avg F1 (as %)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Transformer</td>
<td>Recon</td>
<td>$\times$</td>
<td>$\times$</td>
<td>79.72</td>
<td>76.64</td>
<td>73.74</td>
<td>74.56</td>
<td>78.43</td>
<td>76.62</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Anomaly</td>
<td>Recon</td>
<td>Learnable</td>
<td>Minmax</td>
<td>71.35</td>
<td>78.61</td>
<td>69.12</td>
<td>81.53</td>
<td>80.40</td>
<td>76.20</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Transformer</td>
<td>AssDis</td>
<td>Learnable</td>
<td>Minmax</td>
<td>87.57</td>
<td>90.50</td>
<td>90.98</td>
<td>93.21</td>
<td>95.47</td>
<td>91.55</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Assoc</td>
<td>Fix</td>
<td>Max</td>
<td>83.95</td>
<td>82.17</td>
<td>70.65</td>
<td>79.46</td>
<td>79.04</td>
<td>79.05</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Assoc</td>
<td>Learnable</td>
<td>Max</td>
<td>88.88</td>
<td>85.20</td>
<td>87.84</td>
<td>81.65</td>
<td>93.83</td>
<td>87.48</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>*final</td>
<td>Assoc</td>
<td>Learnable</td>
<td>Minmax</td>
<td>92.33</td>
<td>93.59</td>
<td>96.90</td>
<td>94.07</td>
<td>97.89</td>
<td>94.96</td>
</tr>
</tbody>
</table>
<h1>4.2 Model Analysis</h1>
<p>To explain how our model works intuitively, we provide the visualization and statistical results for our three key designs: anomaly criterion, learnable prior-association and optimization strategy.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Visualization of different anomaly categories (Lai et al., 2021). We plot the raw series (first row) from NeurIPS-TS dataset, as well as their corresponding reconstruction (second row) and association-based criteria (third row). The point-wise anomalies are marked by red circles and the pattern-wise anomalies are in red segments. The wrongly detected cases are bounded by red boxes.</p>
<p>Anomaly criterion visualization To get more intuitive cases about how association-based criterion works, we provide some visualization in Figure 5 and explore the criterion performance under different types of anomalies, where the taxonomy is from Lai et al. (2021). We can find that our proposed association-based criterion is more distinguishable in general. Concretely, the associationbased criterion can obtain the consistent smaller values for the normal part, which is quite contrasting</p>
<p>in point-contextual and pattern-seasonal cases (Figure 5). In contrast, the jitter curves of the reconstruction criterion make the detection process confused and fail in the aforementioned two cases. This verifies that our criterion can highlight the anomalies and provide distinct values for normal and abnormal points, making the detection precise and reducing the false-positive rate.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Learned scale parameter $\sigma$ for different types of anomalies (highlight in red).
Prior-association visualization During the minimax optimization, the prior-association is learned to get close to the series-association. Thus, the learned $\sigma$ can reflect the adjacent-concentrating degree of time series. As shown in Figure 6, we find that $\sigma$ changes to adapt to various data patterns of time series. Especially, the prior-association of anomalies generally has a smaller $\sigma$ than normal time points, which matches our adjacent-concentration inductive bias of anomalies.</p>
<p>Optimization strategy analysis Only with the reconstruction loss, the abnormal and normal time points present similar performance in the association weights to adjacent time points, corresponding to a contrast value closed to 1 (Table 3). Maximizing the association discrepancy will force the series-associations to pay more attention to the non-adjacent area. However, to obtain a better reconstruction, the anomalies have to maintain much larger adjacent association weights than normal time points, corresponding to a larger contrast value. But direct maximization will cause the optimization problem of Gaussian kernel, cannot strongly amplify the difference between normal and abnormal time points as expected (SMD:1.15 $\rightarrow 1.27$ ). The minimax strategy optimizes the prior-association to provide a stronger constraint to series-association. Thus, the minimax strategy obtains more distinguishable contrast values than direct maximization (SMD:1.27 $\rightarrow 2.39$ ) and thereby performs better.</p>
<p>Table 3: The statistical results of adjacent association weights for Abnormal and Normal time points respectively. Recon, Max and Minimax represent the association learning process that is supervised by reconstruction loss, direct maximization and minimax strategy respectively. A higher contrast value ( $\frac{\text { Abnormal }}{\text { Normal }}$ ) indicates a stronger distinguishability between normal and abnormal time points.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">SMD</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SMAP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SWaT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PSM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Optimization</td>
<td style="text-align: center;">Recon</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Recon</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Recon</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Recon</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Recon</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Ours</td>
</tr>
<tr>
<td style="text-align: center;">Abnormal (\%)</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">1.29</td>
<td style="text-align: center;">1.18</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.29</td>
</tr>
<tr>
<td style="text-align: center;">Normal (\%)</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">1.23</td>
<td style="text-align: center;">1.09</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">1.18</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: center;">Contrast ( $\frac{\text { Abnormal }}{\text { Normal }}$ )</td>
<td style="text-align: center;">1.15</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">2.39</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">1.10</td>
<td style="text-align: center;">1.59</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">1.14</td>
<td style="text-align: center;">1.76</td>
<td style="text-align: center;">1.03</td>
<td style="text-align: center;">1.04</td>
<td style="text-align: center;">2.64</td>
</tr>
</tbody>
</table>
<h1>5 CONCLUSION AND FUTURE WORK</h1>
<p>This paper studies the unsupervised time series anomaly detection problem. Unlike previous methods, we learn the more informative time-point associations by Transformers. Based on the key observation of association discrepancy, we propose the Anomaly Transformer, including an AnomalyAttention with the two-branch structure to embody the association discrepancy. A minimax strategy is adopted to further amplify the difference between normal and abnormal time points. By introducing the association discrepancy, we propose the association-based criterion, which makes the reconstruction performance and association discrepancy collaborate. Anomaly Transformer achieves the state-of-the-art results on an exhaustive set of empirical studies. Future work includes theoretical study of Anomoly Transformer in light of classic analysis for autoregression and state space models.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>This work was supported by the National Megaproject for New Generation AI (2020AAA0109201), National Natural Science Foundation of China (62022050 and 62021002), Beijing Nova Program (Z201100006820041), and BNRist Innovation Fund (BNR2021RC01002).</p>
<h2>REFERENCES</h2>
<p>Ahmed Abdulaal, Zhuanghua Liu, and Tomer Lancewicki. Practical approach to asynchronous multivariate time series anomaly detection and localization. $K D D, 2021$.</p>
<p>Ryan Prescott Adams and David J. C. MacKay. Bayesian online changepoint detection. arXiv preprint arXiv:0710.3742, 2007.
O. Anderson and M. Kendall. Time-series. 2nd edn. J. R. Stat. Soc. (Series D), 1976.</p>
<p>Paul Boniol and Themis Palpanas. Series2graph: Graph-based subsequence anomaly detection for time series. Proc. VLDB Endow., 2020.</p>
<p>Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, and Jörg Sander. LOF: identifying density-based local outliers. In SIGMOD, 2000.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.</p>
<p>Zekai Chen, Dingshuo Chen, Zixuan Yuan, Xiuzhen Cheng, and Xiao Zhang. Learning graph structures with transformer for multivariate time series anomaly detection in iot. ArXiv, abs/2104.03466, 2021.</p>
<p>Haibin Cheng, Pang-Ning Tan, Christopher Potter, and Steven A. Klooster. A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series. ICDM Workshops, 2008.</p>
<p>Haibin Cheng, Pang-Ning Tan, Christopher Potter, and Steven A. Klooster. Detection and characterization of anomalies in multivariate time series. In $S D M, 2009$.</p>
<p>Shohreh Deldari, Daniel V. Smith, Hao Xue, and Flora D. Salim. Time series change point detection with self-supervised contrastive predictive coding. In WWW, 2021.</p>
<p>Ailin Deng and Bryan Hooi. Graph neural network-based anomaly detection in multivariate time series. $A A A I, 2021$.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In $I C L R, 2021$.
I. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.</p>
<p>Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer. In $I C L R, 2019$.</p>
<p>Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Söderström. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. $K D D, 2018$.</p>
<p>Eamonn J. Keogh, Taposh Roy, Naik U, and Agrawal A. Multi-dataset time-series anomaly detection competition, Competition of International Conference on Knowledge Discovery \&amp; Data Mining 2021. URL https://compete.hexagon-ml.com/practice/competition/39/.</p>
<p>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR, 2020.</p>
<p>Kwei-Herng Lai, D. Zha, Junjie Xu, and Yue Zhao. Revisiting time series outlier detection: Definitions and benchmarks. In NeurIPS Dataset and Benchmark Track, 2021.</p>
<p>Dan Li, Dacheng Chen, Lei Shi, Baihong Jin, Jonathan Goh, and See-Kiong Ng. Mad-gan: Multivariate anomaly detection for time series data with generative adversarial networks. In ICANN, 2019a.</p>
<p>Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In NeurIPS, 2019b.</p>
<p>Zhihan Li, Youjian Zhao, Jiaqi Han, Ya Su, Rui Jiao, Xidao Wen, and Dan Pei. Multivariate time series anomaly detection and interpretation using hierarchical inter-metric and temporal embedding. $K D D, 2021$.
F. Liu, K. Ting, and Z. Zhou. Isolation forest. ICDM, 2008.</p>
<p>Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Ching-Feng Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. ICCV, 2021.</p>
<p>Aditya P. Mathur and Nils Ole Tippenhauer. Swat: a water treatment testbed for research and training on ICS security. In CySWATER, 2016.</p>
<p>Radford M. Neal. Pattern recognition and machine learning. Technometrics, 2007.
Daehyung Park, Yuuna Hoshi, and Charles C. Kemp. A multimodal anomaly detector for robotassisted feeding using an lstm-based variational autoencoder. $R A-L, 2018$.</p>
<p>Adam Paszke, S. Gross, Francisco Massa, A. Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Z. Lin, N. Gimelshein, L. Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019.</p>
<p>Mathias Perslev, Michael Jensen, Sune Darkner, Poul Jø rgen Jennum, and Christian Igel. U-time: A fully convolutional network for time series segmentation applied to sleep staging. In NeurIPS. 2019.</p>
<p>Lukas Ruff, Nico Görnitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Robert A. Vandermeulen, Alexander Binder, Emmanuel Müller, and M. Kloft. Deep one-class classification. In ICML, 2018.
T. Schlegl, Philipp Seeböck, S. Waldstein, G. Langs, and U. Schmidt-Erfurth. f-anogan: Fast unsupervised anomaly detection with generative adversarial networks. Med. Image Anal., 2019.
B. Schölkopf, John C. Platt, J. Shawe-Taylor, Alex Smola, and R. C. Williamson. Estimating the support of a high-dimensional distribution. Neural Comput., 2001.</p>
<p>Lifeng Shen, Zhuocong Li, and James T. Kwok. Timeseries anomaly detection using temporal hierarchical one-class network. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, MariaFlorina Balcan, and Hsuan-Tien Lin (eds.), NeurIPS, 2020.</p>
<p>Youjin Shin, Sangyup Lee, Shahroz Tariq, Myeong Shin Lee, Okchul Jung, Daewon Chung, and Simon S. Woo. Itad: Integrative tensor-based anomaly detection system for reducing false positives of satellite systems. CIKM, 2020.</p>
<p>Ya Su, Y. Zhao, Chenhao Niu, Rong Liu, W. Sun, and Dan Pei. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. $K D D, 2019$.</p>
<p>Jian Tang, Zhixiang Chen, A. Fu, and D. Cheung. Enhancing effectiveness of outlier detections for low density patterns. In PAKDD, 2002.</p>
<p>Shahroz Tariq, Sangyup Lee, Youjin Shin, Myeong Shin Lee, Okchul Jung, Daewon Chung, and Simon S. Woo. Detecting anomalies in space using multivariate convolutional lstm with mixtures of probabilistic pca. $K D D, 2019$.
D. Tax and R. Duin. Support vector data description. Mach. Learn., 2004.</p>
<p>Robert Tibshirani, Guenther Walther, and Trevor Hastie. Estimating the number of clusters in a dataset via the gap statistic. J. R. Stat. Soc. (Series B), 2001.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.</p>
<p>Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with Auto-Correlation for long-term series forecasting. In NeurIPS, 2021.</p>
<p>Haowen Xu, Wenxiao Chen, N. Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Y. Liu, Y. Zhao, Dan Pei, Yang Feng, Jian Jhen Chen, Zhaogang Wang, and Honglin Qiao. Unsupervised anomaly detection via variational auto-encoder for seasonal kpis in web applications. WWW, 2018.</p>
<p>Takehisa Yairi, Naoya Takeishi, Tetsuo Oda, Yuta Nakajima, Naoki Nishimura, and Noboru Takata. A data-driven health monitoring method for satellite housekeeping data based on probabilistic clustering and dimensionality reduction. IEEE Trans. Aerosp. Electron. Syst., 2017.</p>
<p>Hang Zhao, Yujing Wang, Juanyong Duan, Congrui Huang, Defu Cao, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, and Qi Zhang. Multivariate time-series anomaly detection via graph attention network. ICDM, 2020.</p>
<p>Bin Zhou, Shenghua Liu, Bryan Hooi, Xueqi Cheng, and Jing Ye. Beatgan: Anomalous rhythm detection using adversarially generated time series. In IJCAI, 2019.</p>
<p>Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In $A A A I, 2021$.</p>
<p>Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Dae-ki Cho, and Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In ICLR, 2018 .</p>
<h1>A PARAMETER SENSITIVITY</h1>
<p>We set the window size as 100 throughout the main text, which considers the temporal information, memory and computation efficiency. And we set the loss weight $\lambda$ based on the convergence property of the training curve.
Furthermore, Figure 7 provides the model performance under different choices of the window size and the loss weight. We present that our model is stable to the window size over extensive datasets (Figure 7 left). Note that a larger window size indicates a larger memory cost and a smaller sliding number. Especially, only considering the performance, its relationship to the window size can be determined by the data pattern. For example, our model performs better when the window size is 50 for the SMD dataset. Besides, we adopt the loss weight $\lambda$ in Equation 5 to trade off the reconstruction loss and the association part. We find that $\lambda$ is stable and easy to tune in the range of 2 to 4 . The above results verify the sensitivity of our model, which is essential for applications.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Parameter sensitivity for sliding window size (left) and loss weight $\lambda$ (right). The model with $\lambda=0$ still adopts the association-based criterion but only supervised by reconstruction loss.</p>
<h2>B IMPLEMENTATION DETAILS</h2>
<p>We present the pseudo-code of Anomaly-Attention in Algorithm 1.
Algorithm 1 Anomaly-Attention Mechanism (multi-head version).
Input: $\mathcal{X} \in \mathbb{R}^{N \times d_{\text {model }}}$ : input; $\mathcal{D}=\left((j-i)^{2}\right)<em _input="{input" _text="\text">{i, j \in{1, \cdots, N}} \in \mathbb{R}^{N \times N}$ : relative distance matrix
Layer params: $\mathrm{MLP}</em>}}$ : linear projector for input; $\mathrm{MLP<em _input="{input" _text="\text">{\text {output }}$ : linear projector for output
1: $\mathcal{Q}, \mathcal{K}, \mathcal{V}, \sigma=\operatorname{Split}\left(\operatorname{MLP}</em>$
2: for $\left(\mathcal{Q}}}(\mathcal{X}), \operatorname{dim}=1\right) \quad \triangleright \mathcal{Q}, \mathcal{K}, \mathcal{V} \in \mathbb{R}^{N \times d_{\text {model }}}, \sigma \in \mathbb{R}^{N \times h<em m="m">{m}, \mathcal{K}</em>}, \mathcal{V<em m="m">{m}, \sigma</em>}\right)$ in $(\mathcal{Q}, \mathcal{K}, \mathcal{V}, \sigma)$ : $\quad \triangleright \mathcal{Q<em m="m">{m}, \mathcal{K}</em>}, \mathcal{V<em _model="{model" _text="\text">{m} \in \mathbb{R}^{N \times \frac{d</em>$
3: $\quad \sigma_{m}=\operatorname{Broadcast}\left(\sigma_{m}, \operatorname{dim}=1\right)$
$\triangleright \sigma_{m} \in \mathbb{R}^{N \times N}$
4: $\quad \mathcal{P}}}}{h}}, \sigma_{m} \in \mathbb{R}^{N \times 1<em m="m">{m}=\frac{1}{\sqrt{2 \pi} \sigma</em>\right)$
$\triangleright \mathcal{P}}} \exp \left(-\frac{\mathcal{P}}{2 \sigma_{m}^{2}<em m="m">{m} \in \mathbb{R}^{N \times N}$
5: $\quad \mathcal{P}</em>}=\mathcal{P<em m="m">{m} / \operatorname{Broadcast}\left(\operatorname{Sum}\left(\mathcal{P}</em>=1\right)\right)$
$\triangleright$ Rescaled $\mathcal{P}}, \operatorname{dim<em m="m">{m} \in \mathbb{R}^{N \times N}$
6: $\quad \mathcal{S}</em>}=\operatorname{Softmax}\left(\sqrt{\frac{h}{d_{\text {model }}}} \mathcal{Q<em m="m">{m} \mathcal{K}</em>\right)$
$\triangleright \mathcal{S}}^{\mathrm{T}<em m="m">{m} \in \mathbb{R}^{N \times N}$
7: $\quad \widetilde{\mathcal{Z}}</em>}=\mathcal{S<em m="m">{m} \mathcal{V}</em>$
8: $\widetilde{\mathcal{Z}}=\operatorname{MLP}<em 1="1">{\text {output }}\left(\operatorname{Concat}\left(\left[\widetilde{\mathcal{Z}}</em>}, \cdots, \widetilde{\mathcal{Z}<em _model="{model" _text="\text">{h}\right], \operatorname{dim}=1\right)\right)$
$\triangleright \widetilde{\mathcal{Z}} \in \mathbb{R}^{N \times d</em>$
9: Return $\widetilde{\mathcal{Z}}$
$\triangleright$ Keep the $\mathcal{P}}}<em m="m">{m}$ and $\mathcal{S}</em>, m=1, \cdots, h$</p>
<h2>C MORE SHOWCASES</h2>
<p>To obtain an intuitive comparison of main results (Table 1), we visualize the criterion of various baselines. Anomaly Transformer can present the most distinguishable criterion (Figure 8). Besides,</p>
<p>for the real-world dataset, Anomaly Transformer can also detect the anomalies correctly. Especially for the SWaT dataset (Figure 9(d)), our model can detect the anomalies in the early stage, which is meaningful for real-world applications, such as the early warning of malfunctions.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Visualization of learned criterion for the NeurIPS-TS dataset. Anomalies are labeled by red circles and red segments (first row). The failure cases of the baselines are bounded by red boxes.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Visualization of the model learned criterion in real-world datasets. We select one dimension of the data for visualization. These showcases are from the test set of corresponding datasets.</p>
<h1>D Ablation of ASSOCIATION DISCREPANCY</h1>
<p>We present the pseudo code of the calculation in Algorithm 2.</p>
<h1>D. 1 Ablation of Multi-Level Quantification</h1>
<p>We average the association discrepancy from multiple layers for the final results (Equation 6). We further investigate the model performance under the single-layer usage. As shown in Table 4, the multiple-layer design achieves the best, which verifies the effectiveness of multi-level quantification.
Table 4: Model performance under difference selection of model layers for association discrepancy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">SMD</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SMAP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SWaT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PSM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">layer 1</td>
<td style="text-align: center;">87.15</td>
<td style="text-align: center;">92.87</td>
<td style="text-align: center;">89.92</td>
<td style="text-align: center;">90.36</td>
<td style="text-align: center;">94.11</td>
<td style="text-align: center;">92.19</td>
<td style="text-align: center;">93.65</td>
<td style="text-align: center;">99.03</td>
<td style="text-align: center;">96.26</td>
<td style="text-align: center;">92.61</td>
<td style="text-align: center;">91.92</td>
<td style="text-align: center;">92.27</td>
<td style="text-align: center;">97.20</td>
<td style="text-align: center;">97.50</td>
<td style="text-align: center;">97.35</td>
</tr>
<tr>
<td style="text-align: center;">layer 2</td>
<td style="text-align: center;">87.22</td>
<td style="text-align: center;">95.17</td>
<td style="text-align: center;">91.02</td>
<td style="text-align: center;">90.82</td>
<td style="text-align: center;">92.41</td>
<td style="text-align: center;">91.60</td>
<td style="text-align: center;">93.69</td>
<td style="text-align: center;">98.75</td>
<td style="text-align: center;">96.15</td>
<td style="text-align: center;">92.48</td>
<td style="text-align: center;">92.50</td>
<td style="text-align: center;">92.49</td>
<td style="text-align: center;">96.12</td>
<td style="text-align: center;">98.62</td>
<td style="text-align: center;">97.35</td>
</tr>
<tr>
<td style="text-align: center;">layer 3</td>
<td style="text-align: center;">87.27</td>
<td style="text-align: center;">93.89</td>
<td style="text-align: center;">90.46</td>
<td style="text-align: center;">91.61</td>
<td style="text-align: center;">88.81</td>
<td style="text-align: center;">90.19</td>
<td style="text-align: center;">93.40</td>
<td style="text-align: center;">98.83</td>
<td style="text-align: center;">96.04</td>
<td style="text-align: center;">88.75</td>
<td style="text-align: center;">91.22</td>
<td style="text-align: center;">89.96</td>
<td style="text-align: center;">77.25</td>
<td style="text-align: center;">94.53</td>
<td style="text-align: center;">85.02</td>
</tr>
<tr>
<td style="text-align: center;">Multiple-layer</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">95.45</td>
<td style="text-align: center;">92.33</td>
<td style="text-align: center;">92.09</td>
<td style="text-align: center;">95.15</td>
<td style="text-align: center;">93.59</td>
<td style="text-align: center;">94.13</td>
<td style="text-align: center;">99.40</td>
<td style="text-align: center;">96.69</td>
<td style="text-align: center;">91.55</td>
<td style="text-align: center;">96.73</td>
<td style="text-align: center;">94.07</td>
<td style="text-align: center;">96.91</td>
<td style="text-align: center;">98.90</td>
<td style="text-align: center;">97.89</td>
</tr>
</tbody>
</table>
<h2>D. 2 Ablation of Statistical Distance</h2>
<p>We select the following widely-used statistical distances to calculate the association discrepancy:</p>
<ul>
<li>Symmetrized Kullback-Leibler Divergence (Ours).</li>
<li>Jensen-Shannon Divergence (JSD).</li>
<li>Wasserstein Distance (Wasserstein).</li>
<li>Cross-Entropy (CE).</li>
<li>L2 Distance (L2).</li>
</ul>
<p>Table 5: Model performance under different definitions of association discrepancy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">SMD</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SMAP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SWaT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PSM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">L2</td>
<td style="text-align: center;">85.26</td>
<td style="text-align: center;">74.80</td>
<td style="text-align: center;">79.69</td>
<td style="text-align: center;">85.58</td>
<td style="text-align: center;">81.30</td>
<td style="text-align: center;">83.39</td>
<td style="text-align: center;">91.25</td>
<td style="text-align: center;">56.77</td>
<td style="text-align: center;">70.00</td>
<td style="text-align: center;">79.90</td>
<td style="text-align: center;">87.45</td>
<td style="text-align: center;">83.51</td>
<td style="text-align: center;">70.24</td>
<td style="text-align: center;">96.34</td>
<td style="text-align: center;">81.24</td>
</tr>
<tr>
<td style="text-align: center;">CE</td>
<td style="text-align: center;">88.23</td>
<td style="text-align: center;">81.85</td>
<td style="text-align: center;">84.92</td>
<td style="text-align: center;">90.07</td>
<td style="text-align: center;">86.44</td>
<td style="text-align: center;">88.22</td>
<td style="text-align: center;">92.37</td>
<td style="text-align: center;">64.08</td>
<td style="text-align: center;">75.67</td>
<td style="text-align: center;">62.78</td>
<td style="text-align: center;">81.50</td>
<td style="text-align: center;">70.93</td>
<td style="text-align: center;">70.71</td>
<td style="text-align: center;">94.68</td>
<td style="text-align: center;">80.96</td>
</tr>
<tr>
<td style="text-align: center;">Wasserstein</td>
<td style="text-align: center;">78.80</td>
<td style="text-align: center;">71.86</td>
<td style="text-align: center;">75.17</td>
<td style="text-align: center;">60.77</td>
<td style="text-align: center;">36.47</td>
<td style="text-align: center;">45.58</td>
<td style="text-align: center;">90.46</td>
<td style="text-align: center;">57.62</td>
<td style="text-align: center;">70.40</td>
<td style="text-align: center;">92.00</td>
<td style="text-align: center;">71.63</td>
<td style="text-align: center;">80.55</td>
<td style="text-align: center;">68.25</td>
<td style="text-align: center;">92.18</td>
<td style="text-align: center;">78.43</td>
</tr>
<tr>
<td style="text-align: center;">JSD</td>
<td style="text-align: center;">85.33</td>
<td style="text-align: center;">90.09</td>
<td style="text-align: center;">87.64</td>
<td style="text-align: center;">91.19</td>
<td style="text-align: center;">92.42</td>
<td style="text-align: center;">91.80</td>
<td style="text-align: center;">94.83</td>
<td style="text-align: center;">95.14</td>
<td style="text-align: center;">94.98</td>
<td style="text-align: center;">83.75</td>
<td style="text-align: center;">96.75</td>
<td style="text-align: center;">89.78</td>
<td style="text-align: center;">95.33</td>
<td style="text-align: center;">98.58</td>
<td style="text-align: center;">96.93</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">95.45</td>
<td style="text-align: center;">92.33</td>
<td style="text-align: center;">92.09</td>
<td style="text-align: center;">95.15</td>
<td style="text-align: center;">93.59</td>
<td style="text-align: center;">94.13</td>
<td style="text-align: center;">99.40</td>
<td style="text-align: center;">96.69</td>
<td style="text-align: center;">91.55</td>
<td style="text-align: center;">96.73</td>
<td style="text-align: center;">94.07</td>
<td style="text-align: center;">96.91</td>
<td style="text-align: center;">98.90</td>
<td style="text-align: center;">97.89</td>
</tr>
</tbody>
</table>
<p>As shown in Table 5, our proposed definition of association discrepancy still achieves the best performance. We find that both the CE and JSD can provide fairly good results, which are close to our definition in principle and can be used to represent the information gain. The L2 distance is not suitable for the discrepancy, which overlooks the property of discrete distribution. The Wasserstein distance also fails in some datasets. The reason is that the prior-association and series-association are exactly matched in the position indexes. Still, the Wasserstein distance is not calculated point by point and considers the distribution offset, which may bring noises to the optimization and detection.</p>
<p>Algorithm 2 Association Discrepancy $\operatorname{AssDis}(\mathcal{P}, \mathcal{S} ; \mathcal{X})$ Calculation (multi-head version).
Input: time series length $N$; layers number $L$; heads number $h$; prior-association $\mathcal{P}<em _all="{all" _text="\text">{\text {all }} \in$ $\mathbb{R}^{L \times h \times N \times N} ;$ series-association $\mathcal{S}</em>$;
1: $\mathcal{P}^{\prime}=\operatorname{Mean}(\mathcal{P}$, dim=1)
$\triangleright \mathcal{P}^{\prime} \in \mathbb{R}^{L \times N \times N}$
2: $\mathcal{S}^{\prime}=\operatorname{Mean}(\mathcal{S}$, dim=1)
$\triangleright \mathcal{S}^{\prime} \in \mathbb{R}^{L \times N \times N}$
3: $\mathcal{R}^{\prime}=\operatorname{KL}\left(\left(\mathcal{P}^{\prime}, \mathcal{S}^{\prime}\right)$, dim=-1$\right)+\operatorname{KL}\left(\left(\mathcal{S}^{\prime}, \mathcal{P}^{\prime}\right)$, dim=-1$\right)$
$\triangleright \mathcal{R}^{\prime} \in \mathbb{R}^{L \times N}$
4: $\mathcal{R}=\operatorname{Mean}\left(\mathcal{R}^{\prime}\right.$, dim $\left.=0\right)$
$\triangleright \mathcal{R} \in \mathbb{R}^{N \times 1}$
5: Return $\mathcal{R}$
$\triangleright$ Represent the association discrepancy of each time point}} \in \mathbb{R}^{L \times h \times N \times N</p>
<h1>D. 3 Ablation of Prior-Association</h1>
<p>In addition to the Gaussian kernel with a learnable scale parameter, we also try to use the power-law kernel $P(x ; \alpha)=x^{-\alpha}$ with a learnable power parameter $\alpha$ for prior-association, which is also a unimodal distribution. As shown in Table 6, power-law kernel can achieve a good performance in most of the datasets. However, because the scale parameter is easier to optimize than the parameter of power, Gaussian kernel still surpasses the power-law kernel consistently.</p>
<p>Table 6: Model performance under different definitions of prior-association. Our Anomaly Transformer adopts the Gaussian kernel as the prior. Power-law refers to the power-law kernel.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">SMD</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SMAP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SWaT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PSM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Power-law</td>
<td style="text-align: center;">89.41</td>
<td style="text-align: center;">92.46</td>
<td style="text-align: center;">90.91</td>
<td style="text-align: center;">90.95</td>
<td style="text-align: center;">85.87</td>
<td style="text-align: center;">88.34</td>
<td style="text-align: center;">91.95</td>
<td style="text-align: center;">58.24</td>
<td style="text-align: center;">71.31</td>
<td style="text-align: center;">92.52</td>
<td style="text-align: center;">93.29</td>
<td style="text-align: center;">92.90</td>
<td style="text-align: center;">96.46</td>
<td style="text-align: center;">98.15</td>
<td style="text-align: center;">97.30</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">95.45</td>
<td style="text-align: center;">92.33</td>
<td style="text-align: center;">92.09</td>
<td style="text-align: center;">95.15</td>
<td style="text-align: center;">93.59</td>
<td style="text-align: center;">94.13</td>
<td style="text-align: center;">99.40</td>
<td style="text-align: center;">96.69</td>
<td style="text-align: center;">91.55</td>
<td style="text-align: center;">96.73</td>
<td style="text-align: center;">94.07</td>
<td style="text-align: center;">96.91</td>
<td style="text-align: center;">98.90</td>
<td style="text-align: center;">97.89</td>
</tr>
</tbody>
</table>
<h2>E Ablation of Association-based Criterion</h2>
<h2>E. 1 CALCULATION</h2>
<p>We present the pseudo-code of association-based criterion in Algorithm 3.
Algorithm 3 Association-based Criterion AnomalyScore $(\mathcal{X})$ Calculation
Input: time series length $N$; input time series $\mathcal{X} \in \mathbb{R}^{N \times d}$; reconstruction time series $\widehat{\mathcal{X}} \in \mathbb{R}^{N \times d}$; association discrepancy $\operatorname{AssDis}(\mathcal{P}, \mathcal{S} ; \mathcal{X}) \in \mathbb{R}^{N \times 1}$;
1: $\mathcal{C}<em _mathrm_AD="\mathrm{AD">{\mathrm{AD}}=\operatorname{Softmax}(-\operatorname{AssDis}(\mathcal{P}, \mathcal{S} ; \mathcal{X}), \operatorname{dim}=0)$
$\triangleright \mathcal{C}</em>$
2: $\mathcal{C}}} \in \mathbb{R}^{N \times 1<em _Recon="{Recon" _text="\text">{\text {Recon }}=\operatorname{Mean}\left((\mathcal{X}-\widehat{\mathcal{X}})^{2}, \operatorname{dim}=1\right)$
$\triangleright \mathcal{C}</em>$
3: $\mathcal{C}=\mathcal{C}}} \in \mathbb{R}^{N \times 1<em _Recon="{Recon" _text="\text">{\mathrm{AD}} \times \mathcal{C}</em>$
$\triangleright \mathcal{C} \in \mathbb{R}^{N \times 1}$
4: Return $\mathcal{C}$
$\triangleright$ Anomaly score for each time point}</p>
<h2>E. 2 Ablation of Criterion Definition</h2>
<p>We explore the model performance under different definitions of anomaly criterion, including the pure association discrepancy, pure reconstruction performance and different combination methods for association discrepancy and reconstruction performance: addition and multiplication.</p>
<p>Association Discrepancy: $\operatorname{AnomalyScore}(\mathcal{X})=\operatorname{Softmax}(-\operatorname{AssDis}(\mathcal{P}, \mathcal{S} ; \mathcal{X}))$,
Reconstruction: AnomalyScore $(\mathcal{X})=\left[\left|\mathcal{X}<em i_:="i,:">{i,:}-\widehat{\mathcal{X}}</em>\right|<em N="N" _cdots_="\cdots," i="1,">{2}^{2}\right]</em>$,
Addition: AnomalyScore $(\mathcal{X})=\operatorname{Softmax}(-\operatorname{AssDis}(\mathcal{P}, \mathcal{S} ; \mathcal{X}))+\left[\left|\mathcal{X}<em i_:="i,:">{i,:}-\widehat{\mathcal{X}}</em>\right|<em N="N" _cdots_="\cdots," i="1,">{2}^{2}\right]</em>$,
Multiplication (Ours): AnomalyScore $(\mathcal{X})=\operatorname{Softmax}(-\operatorname{AssDis}(\mathcal{P}, \mathcal{S} ; \mathcal{X})) \odot\left[\left|\mathcal{X}<em i_:="i,:">{i,:}-\widehat{\mathcal{X}}</em>\right|<em N="N" _cdots_="\cdots," i="1,">{2}^{2}\right]</em>$.</p>
<p>From Table 7, we find that directly using our proposed association discrepancy can also achieve a good performance, which surpasses the competitive baseline THOC (Shen et al., 2020) consistently. Besides, the multiplication combination that we used in Equation 6 performs the best, which can bring a better collaboration to the reconstruction performance and association discrepancy.</p>
<p>Table 7: Ablation of criterion definition. We also include the state-of-the-art deep model THOC (Shen et al., 2020) for comparison. AssDis and Recon represent the pure association discrepancy and the pure reconstruction performance respectively. Ours refers to our proposed association-based criterion with the multiplication combination.</p>
<p>| Dataset | SMD | | | MSL | | | SMAP | | | SWaT | | | PSM | | | Avg |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Metric | P | R | F1 | P | R | F1 | P | R | F1 | P | R | F1 | P | R | F1 | F1(\%) |
| THOC | 79.76 | 90.95 | 84.99 | 88.45 | 90.97 | 89.69 | 92.06 | 89.34 | 90.68 | 83.94 | 86.36 | 85.13 | 88.14 | 90.99 | 89.54 | 88.01 |
| Recon | 78.63 | 65.29 | 71.35 | 79.15 | 78.07 | 78.61 | 89.38 | 56.35 | 69.12 | 76.81 | 86.89 | 81.53 | 69.84 | 94.73 | 80.40 | 76.20 |
| AssDis | 86.74 | 88.42 | 87.57 | 91.20 | 89.81 | 90.50 | 91.56 | 90.41 | 90.98 | 97.27 | 89.48 | 93.21 | 97.80 | 93.25 | 95.47 | 91.55 |
| Addition | 77.16 | 70.58 | 73.73 | 88.08 | 87.37 | 87.72 | 91.28 | 55.97 | 69.39 | 84.34 | 81.98 | 83.14 | 97.60 | 97.61 | 97.61 | 82.32 |
| Ours | 89.40 | 95.45 | 92.33 | 92.09 | 95.15 | 93.59 | 94.13 | 99.40 | 96.69 | 91.55 | 96.73 | 94.07 | 96.91 | 98.90 | 97.89 | 94.96 |</p>
<h1>F ConVERgence of Minimax Optimization</h1>
<p>The total loss of our model (Equation 4) contains two parts: the reconstruction loss and the association discrepancy. Towards a better control of association learning, we adopt a minimax strategy for optimization (Equation 5). During the minimization phase, the optimization trends to minimize the association discrepancy and the reconstruction error. During the maximization phase, the optimization trends to maximize the association discrepancy and minimize the reconstruction error.</p>
<p>We plot the change curve of the above two parts during the training procedure. As shown in Figures 10 and 11, both parts of the total loss can converge within limited iterations on all the five real-world datasets. This nice convergence property is essential for the optimization of our model.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Change curve of reconstruction loss $|\mathcal{X}-\widehat{\mathcal{X}}|_{F}^{2}$ in real-world datasets during training.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Change curve of association discrepancy $|\operatorname{AssDis}(\mathcal{P}, \mathcal{S} ; \mathcal{X})|_{1}$ in real-world datasets during the training process.</p>
<h2>G Model Parameter Sensitivity</h2>
<p>In this paper, we set the hyper-parameters $L$ and $d_{\text {model }}$ following the convention of Transformers (Vaswani et al., 2017; Zhou et al., 2021).</p>
<p>Furthermore, to evaluate model parameter sensitivity, we investigate the performance and efficiency under different choices for the number of layers $L$ and hidden channels $d_{\text {model }}$. Generally, increasing the model size can obtain better results but with larger memory and computation costs.</p>
<p>Table 8: Model performance under different choices of the number of layers $L$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">SMD</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SMAP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SWaT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PSM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">$L=1$</td>
<td style="text-align: center;">89.24</td>
<td style="text-align: center;">93.73</td>
<td style="text-align: center;">91.43</td>
<td style="text-align: center;">91.99</td>
<td style="text-align: center;">97.59</td>
<td style="text-align: center;">94.71</td>
<td style="text-align: center;">93.58</td>
<td style="text-align: center;">99.35</td>
<td style="text-align: center;">96.38</td>
<td style="text-align: center;">91.57</td>
<td style="text-align: center;">95.33</td>
<td style="text-align: center;">93.42</td>
<td style="text-align: center;">96.74</td>
<td style="text-align: center;">98.09</td>
<td style="text-align: center;">97.41</td>
</tr>
<tr>
<td style="text-align: center;">$L=2$</td>
<td style="text-align: center;">89.26</td>
<td style="text-align: center;">94.33</td>
<td style="text-align: center;">91.72</td>
<td style="text-align: center;">91.89</td>
<td style="text-align: center;">94.73</td>
<td style="text-align: center;">93.29</td>
<td style="text-align: center;">93.79</td>
<td style="text-align: center;">98.91</td>
<td style="text-align: center;">96.28</td>
<td style="text-align: center;">92.37</td>
<td style="text-align: center;">94.59</td>
<td style="text-align: center;">93.47</td>
<td style="text-align: center;">97.22</td>
<td style="text-align: center;">98.23</td>
<td style="text-align: center;">97.72</td>
</tr>
<tr>
<td style="text-align: center;">$L=3$</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">95.45</td>
<td style="text-align: center;">92.33</td>
<td style="text-align: center;">92.09</td>
<td style="text-align: center;">95.15</td>
<td style="text-align: center;">93.59</td>
<td style="text-align: center;">94.13</td>
<td style="text-align: center;">99.40</td>
<td style="text-align: center;">96.69</td>
<td style="text-align: center;">91.55</td>
<td style="text-align: center;">96.73</td>
<td style="text-align: center;">94.07</td>
<td style="text-align: center;">96.91</td>
<td style="text-align: center;">98.90</td>
<td style="text-align: center;">97.89</td>
</tr>
<tr>
<td style="text-align: center;">$L=4$</td>
<td style="text-align: center;">89.59</td>
<td style="text-align: center;">95.76</td>
<td style="text-align: center;">92.58</td>
<td style="text-align: center;">91.88</td>
<td style="text-align: center;">95.40</td>
<td style="text-align: center;">93.61</td>
<td style="text-align: center;">93.75</td>
<td style="text-align: center;">99.13</td>
<td style="text-align: center;">96.37</td>
<td style="text-align: center;">93.37</td>
<td style="text-align: center;">93.45</td>
<td style="text-align: center;">93.41</td>
<td style="text-align: center;">97.30</td>
<td style="text-align: center;">97.58</td>
<td style="text-align: center;">97.44</td>
</tr>
</tbody>
</table>
<p>Table 9: Model performance under different choices of the number of hidden channels $d_{\text {model }}$. Mem means the averaged GPU memory cost. Time is the averaged running time of 100 iterations during the training process.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">SMD</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SMAP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SWaT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PSM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mem Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">(GB) (s)</td>
</tr>
<tr>
<td style="text-align: center;">$d_{\text {model }}=256$</td>
<td style="text-align: center;">88.83</td>
<td style="text-align: center;">91.82</td>
<td style="text-align: center;">90.30</td>
<td style="text-align: center;">91.96</td>
<td style="text-align: center;">97.60</td>
<td style="text-align: center;">94.70</td>
<td style="text-align: center;">93.74</td>
<td style="text-align: center;">99.47</td>
<td style="text-align: center;">96.52</td>
<td style="text-align: center;">93.91</td>
<td style="text-align: center;">93.99</td>
<td style="text-align: center;">93.95</td>
<td style="text-align: center;">97.38</td>
<td style="text-align: center;">98.16</td>
<td style="text-align: center;">97.77</td>
<td style="text-align: center;">4.9</td>
</tr>
<tr>
<td style="text-align: center;">$d_{\text {model }}=512$</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">95.45</td>
<td style="text-align: center;">92.33</td>
<td style="text-align: center;">92.09</td>
<td style="text-align: center;">95.15</td>
<td style="text-align: center;">93.59</td>
<td style="text-align: center;">94.13</td>
<td style="text-align: center;">99.40</td>
<td style="text-align: center;">96.69</td>
<td style="text-align: center;">91.55</td>
<td style="text-align: center;">96.73</td>
<td style="text-align: center;">94.07</td>
<td style="text-align: center;">96.91</td>
<td style="text-align: center;">98.90</td>
<td style="text-align: center;">97.89</td>
<td style="text-align: center;">5.5</td>
</tr>
<tr>
<td style="text-align: center;">$d_{\text {model }}=1024$</td>
<td style="text-align: center;">89.44</td>
<td style="text-align: center;">96.33</td>
<td style="text-align: center;">92.76</td>
<td style="text-align: center;">91.80</td>
<td style="text-align: center;">94.99</td>
<td style="text-align: center;">93.37</td>
<td style="text-align: center;">93.58</td>
<td style="text-align: center;">99.47</td>
<td style="text-align: center;">96.43</td>
<td style="text-align: center;">92.02</td>
<td style="text-align: center;">95.01</td>
<td style="text-align: center;">93.49</td>
<td style="text-align: center;">95.78</td>
<td style="text-align: center;">98.12</td>
<td style="text-align: center;">96.94</td>
<td style="text-align: center;">6.6</td>
</tr>
</tbody>
</table>
<h1>H Protocol of Threshold Selection</h1>
<p>Our paper focuses on unsupervised time series anomaly detection. Experimentally, each dataset includes training, validation and testing subsets. Anomalies are only labeled in the testing subset. Thus, we select the hyper-parameters following the Gap Statistic method (Tibshirani et al., 2001) in K-Means. Here is the selection procedure:</p>
<ul>
<li>After the training phase, we apply the model to the validation subset (without label) and obtain the anomaly scores (Equation 6) of all time points.</li>
<li>We count the frequency of the anomaly scores in the validation subset. It is observed that the distribution of anomaly scores is separated into two clusters. We find that the cluster with a larger anomaly score contains $r$ time points. And for our model, $r$ is closed to $0.1 \%$, $0.5 \%, 1 \%$ for SWaT, SMD and other datasets respectively (Table 10).</li>
<li>Due to the size of the test subset being still inaccessible in real-world applications, we have to fix the threshold as a fixed value $\delta$, which can gaurantee that the anomaly scores of $r$ time points in the validation set are larger than $\delta$ and thus detected as anomalies.</li>
</ul>
<p>Table 10: Statistical results of anomaly score distribution on the validation set. We count the number of time points with corresponding values in several intervals.
(a) SMD, MSL and SWaT datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Anomaly Score Interval</th>
<th style="text-align: center;">SMD</th>
<th style="text-align: center;">MSL</th>
<th style="text-align: center;">SWaT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$(0,+\infty]$</td>
<td style="text-align: center;">141681</td>
<td style="text-align: center;">11664</td>
<td style="text-align: center;">99000</td>
</tr>
<tr>
<td style="text-align: center;">$\left[0,10^{-2}\right]$</td>
<td style="text-align: center;">140925</td>
<td style="text-align: center;">11537</td>
<td style="text-align: center;">98849</td>
</tr>
<tr>
<td style="text-align: center;">$\left(10^{-2}, 0.1\right]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">$(0.1,+\infty]$</td>
<td style="text-align: center;">754</td>
<td style="text-align: center;">119</td>
<td style="text-align: center;">134</td>
</tr>
<tr>
<td style="text-align: center;">Ratio of $(0.1,+\infty]$</td>
<td style="text-align: center;">$0.53 \%$</td>
<td style="text-align: center;">$1.02 \%$</td>
<td style="text-align: center;">$0.14 \%$</td>
</tr>
</tbody>
</table>
<p>(b) SMAP and PSM datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Anomaly Score Interval</th>
<th style="text-align: center;">SMAP</th>
<th style="text-align: center;">PSM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$(0,+\infty]$</td>
<td style="text-align: center;">27037</td>
<td style="text-align: center;">26497</td>
</tr>
<tr>
<td style="text-align: center;">$\left[0,10^{-3}\right]$</td>
<td style="text-align: center;">26732</td>
<td style="text-align: center;">26223</td>
</tr>
<tr>
<td style="text-align: center;">$\left(10^{-3}, 10^{-2}\right]$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">$\left(10^{-2},+\infty\right]$</td>
<td style="text-align: center;">305</td>
<td style="text-align: center;">269</td>
</tr>
<tr>
<td style="text-align: center;">Ratio of $\left(10^{-2},+\infty\right]$</td>
<td style="text-align: center;">$1.12 \%$</td>
<td style="text-align: center;">$1.01 \%$</td>
</tr>
</tbody>
</table>
<p>Note that, directly setting the $\delta$ is also feasible. According to the intervals in Table 10, we can fix the $\delta$ as 0.1 for the SMD, MSL and SWaT datasets, 0.01 for the SMAP and PSM datasets, which yield a quite close performance to setting $r$.</p>
<p>Table 11: Model performance. Choose by $\delta$ means that we fix $\delta$ as 0.1 for the SMD, MSL and SWaT datasets, 0.01 for the SMAP and PSM datasets. Choose by $r$ means that we select $r$ as $0.1 \%$ for SWaT, $0.5 \%$ for SMD and $1 \%$ for the other datasets.</p>
<p>| Dataset | SMD | | | MSL | | | SMAP | | | SWaT | | | PSM | | | | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | Metric | P | R | F1 | P | R | F1 | P | R | F1 | P | R | F1 | P | R | F1 | | Choose by $\delta$ | 88.65 | 97.17 | 92.71 | 91.86 | 95.15 | 93.47 | 97.69 | 98.24 | 97.96 | 86.02 | 95.01 | 90.29 | 97.69 | 98.24 | 97.96 | | Choose by $r$ | 89.40 | 95.45 | 92.33 | 92.09 | 95.15 | 93.59 | 94.13 | 99.40 | 96.69 | 91.55 | 96.73 | 94.07 | 96.91 | 98.90 | 97.89 | | In real-world applications, the number of selected anomalies is always decided up to human resources. Under this consideration, setting the number of detected anomalies by the ratio $r$ is more practical and easier to decide according to the available resources. | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<h1>L UCR DATASET</h1>
<p>UCR Dataset is a very challenging and comprehensive dataset provided by the Multi-dataset Time Series Anomaly Detection Competition of KDD2021 (Keogh et al., Competition of International Conference on Knowledge Discovery \&amp; Data Mining 2021). The whole dataset contains 250 subdatasets, covering various real-world scenarios. Each sub-dataset of UCR has only one anomaly segment and only has one dimension. These sub-datasets range in length from 6,684 to 900,000 and are pre-divided into training and test sets.
We also experiment on the UCR dataset for a wide evaluation. As show in Table 14, our Anomaly Transformer still achieves the state-of-the-art in this challenging benchmark.</p>
<p>Table 14: Quantitative results in UCR Dataset. IF refers to the IsolationForest (2008). Ours is our Anomaly Transformer. $P, R$ and $F 1$ represent the precison, recall and F1-score (\%) respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">LSTM-VAE</th>
<th style="text-align: center;">InterFusion</th>
<th style="text-align: center;">OmniAnomaly</th>
<th style="text-align: center;">THOC</th>
<th style="text-align: center;">Deep-SVDD</th>
<th style="text-align: center;">BeatGAN</th>
<th style="text-align: center;">LOF</th>
<th style="text-align: center;">OC-SVM</th>
<th style="text-align: center;">IF</th>
<th style="text-align: center;">Ours</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">P</td>
<td style="text-align: center;">62.08</td>
<td style="text-align: center;">60.74</td>
<td style="text-align: center;">64.21</td>
<td style="text-align: center;">54.61</td>
<td style="text-align: center;">47.08</td>
<td style="text-align: center;">45.20</td>
<td style="text-align: center;">41.47</td>
<td style="text-align: center;">41.14</td>
<td style="text-align: center;">40.77</td>
<td style="text-align: center;">72.80</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">97.60</td>
<td style="text-align: center;">95.20</td>
<td style="text-align: center;">86.93</td>
<td style="text-align: center;">80.83</td>
<td style="text-align: center;">88.91</td>
<td style="text-align: center;">88.42</td>
<td style="text-align: center;">98.80</td>
<td style="text-align: center;">94.00</td>
<td style="text-align: center;">93.60</td>
<td style="text-align: center;">99.60</td>
</tr>
<tr>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">75.89</td>
<td style="text-align: center;">74.16</td>
<td style="text-align: center;">73.86</td>
<td style="text-align: center;">65.19</td>
<td style="text-align: center;">61.56</td>
<td style="text-align: center;">59.82</td>
<td style="text-align: center;">58.42</td>
<td style="text-align: center;">57.23</td>
<td style="text-align: center;">56.80</td>
<td style="text-align: center;">$\mathbf{8 4 . 1 2}$</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal Contribution&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1670 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1670</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1670</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-253157500</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.15598v2.pdf" target="_blank">Provable Sim-to-real Transfer in Continuous Domain with Partial Observations</a></p>
                <p><strong>Paper Abstract:</strong> Sim-to-real transfer trains RL agents in the simulated environments and then deploys them in the real world. Sim-to-real transfer has been widely used in practice because it is often cheaper, safer and much faster to collect samples in simulation than in the real world. Despite the empirical success of the sim-to-real transfer, its theoretical foundation is much less understood. In this paper, we study the sim-to-real transfer in continuous domain with partial observations, where the simulated environments and real-world environments are modeled by linear quadratic Gaussian (LQG) systems. We show that a popular robust adversarial training algorithm is capable of learning a policy from the simulated environment that is competitive to the optimal policy in the real-world environment. To achieve our results, we design a new algorithm for infinite-horizon average-cost LQGs and establish a regret bound that depends on the intrinsic complexity of the model class. Our algorithm crucially relies on a novel history clipping scheme, which might be of independent interest.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1670.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1670.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robust Adversarial Training (for sim-to-real)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A min-max training paradigm that finds a policy minimizing the worst-case value gap across a class of simulators; analyzed here for continuous partially-observable linear-quadratic-Gaussian (LQG) domains and proven to produce policies with provable sim-to-real guarantees under realizability and stability assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>LQG control agents (history-dependent policies)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Controllers/policies for partially-observable linear dynamical systems (LQG) that select actions from histories/filtered beliefs; can be history-dependent and use Kalman-filter belief states.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics / continuous control (theoretical LQG setting)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Simulator class E (parameterized finite-horizon LQGs)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A parameterized set of finite-horizon linear-quadratic-Gaussian simulators, each specified by dynamics matrices (A,B,C) and Gaussian process/measurement noise; represents variability in physical parameters, delays, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>analytical linear dynamical model (mathematical LQG fidelity)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Linear state dynamics (A,B), linear observation model (C), Gaussian process and measurement noise, controller and Kalman-filter steady-state behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No nonlinear dynamics, no contact/impact/contact friction modeling, no photorealistic sensing, no complex unmodeled disturbances beyond Gaussian noise; real-world nonlinearity and unstructured noise are not modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>An unknown real-world LQG system Θ* assumed (for the analysis) to belong to the simulator class E (realizability): same linear structure, Gaussian noises, and stability properties as simulators in E.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Infinite-horizon average-cost or finite-horizon control policies (stabilizing optimal/near-optimal control) under partial observations.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Robust adversarial min-max training (find policy pi_RT = argmin_pi max_{Θ in E} [V^π(Θ)-V^*(Θ)]); analysis uses LQG-VTR algorithm to obtain regret guarantees used in reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Sim-to-real gap Gap(π(E)) = V^{π(E)}(Θ*) - V^*(Θ*); theoretical upper bound Õ(√(δ_E H)) and lower bound Ω(√H).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Intrinsic complexity δ_E of simulator class E, horizon length H, partial observation / belief-state estimation error, stability properties (spectral radius and strong stability constants), clipped history length, model-selection errors.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Realizability (Θ* ∈ E); Assumptions 1–3: open-loop or closed-loop stability, bounded norms, contractible closed-loop matrices, strong stability of predictor matrix A−F(Θ)C; tight model-selection confidence sets; history clipping (short clipped histories) to reduce model-class complexity; LQG-VTR regret guarantee (O(√(δ_E T))).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>The analysis requires the simulator class to include the real system (realizability) and to satisfy stability/strong-stability bounds; intrinsic complexity δ_E must be moderate (poly in dimensions, not growing with H) — history clipping reduces effective complexity to poly(log T). No absolute numeric % fidelity thresholds are given.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Robust adversarial training yields policies whose sim-to-real gap is bounded by Õ(√(δ_E H)); with the proposed LQG-VTR algorithm and history-clipping, δ_E can be made polylogarithmic in horizon (so gap scales ~√H and can be small for small intrinsic complexity); lower bound shows √H dependence is unavoidable, and realizability plus stability are necessary enabling conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Provable Sim-to-real Transfer in Continuous Domain with Partial Observations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1670.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1670.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LQG-VTR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LQG-Value-Targeted Regression (LQG-VTR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new algorithm proposed in this paper for infinite-horizon average-cost LQG control that uses model selection, value-targeted regression, clipped histories and low-switching updates to achieve regret O(√(δ_E T)) depending on intrinsic model complexity δ_E.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>LQG learning agent (regret-minimizing controller)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A history-dependent learning controller that interleaves model-selection warm-up, optimistic planning, episodic updates and value-targeted regression to estimate system parameters and produce control gains used online.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>theoretical continuous control / partially-observable LQG</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Simulator class E (finite-horizon/infinite-horizon LQGs) used for regression and planning</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Same as above: parameterized family of linear-Gaussian simulators; LQG-VTR collects trajectories from the unknown real system (assumed in E) while using simulators in E for planning/optimism.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>analysis-level LQG fidelity (mathematical linear dynamics with Gaussian noise); uses model-based estimates rather than physics-engine rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Belief-state evolution (Kalman filter), Riccati solutions (P, Σ), innovations distribution, steady-state covariances and Kalman gains.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Nonlinearities, contacts, non-Gaussian noise, and realistic sensor modalities (e.g., images) are not modeled; often assumes ability to represent system in (A,B,C) linear form.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>An LQG real-world system Θ* satisfying stability and controllability/observability assumptions; initial data gathered with randomized actions during warm-up for model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Regret-minimizing control policies for infinite-horizon average-cost LQG that are then used for finite-horizon tasks (reduction used to bound sim-to-real gap).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Model-based learning with value-targeted regression (Ayoub et al. style), optimistic planning, episodic updates, low-switching policy updates; uses clipped-history estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Regret(π; T) and sim-to-real Gap; LQG-VTR achieves Regret = O(√(δ_E T)), which via reduction implies sim-to-real gap bound Õ(√(δ_E H)).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Estimator error from full-history vs clipped-history (clipping error), size/dimension of function class F (Eluder dimension and covering number), number of episodes (switching cost), and model-selection warm-up length.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Accurate model-selection warm-up to ensure bounded belief states; use of clipped history length l = O(log(H n + H p)) to make intrinsic complexity small; confidence sets containing Θ* (optimism); low-switching updates to control regret.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Algorithm relies on LQG structure (Kalman filter behavior) and on boundedness/stability constants; clipping length chosen to make clipping error negligible (exponential decay κ^2(1−γ_2)^l), so effective fidelity required is that predictor dynamics be strongly stable.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LQG-VTR provides a principled way to achieve instance-dependent √T regret by (i) pruning unlikely simulators (model selection), (ii) estimating models via value-targeted regression with clipped histories to reduce intrinsic complexity, and (iii) limiting policy switching; this regret bound underpins the provable sim-to-real gap for robust adversarial training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Provable Sim-to-real Transfer in Continuous Domain with Partial Observations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1670.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1670.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HistoryClipping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>History Clipping (clipped-history estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique introduced in this work that replaces full histories with a fixed short clipped history when computing value-target regression targets; it reduces the intrinsic complexity δ_E from O(H) to poly(log T) and controls the clipping error via exponential decay in the predictor stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Belief-state based LQG controllers (Kalman-filtered agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Controllers that use belief states x_{t|t} computed from histories; history clipping approximates full belief computation by truncating the history used to compute the belief.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>partially-observable continuous control (LQG)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Simulator class E (finite-horizon LQGs) where histories are available</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Linear dynamical simulators that generate sequences of observations and actions; full histories determine Kalman-filter belief states exactly.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>mathematical LQG fidelity used for regression/estimation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Short-term belief dynamics and innovations; steady-state Kalman behavior approximated with clipped histories.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Long-run dependence on arbitrarily long histories is approximated away; residual from truncation is bounded but nonzero.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real LQG system whose belief state is approximated from clipped histories; clipping assumed valid due to strong stability (A−F C contractive).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Accurate model estimation for policy optimization and value regression enabling low-regret control transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Value-targeted regression using datasets with clipped-history features in LQG-VTR.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Clipping error Δ_f per sample bounded by O(κ^2 (1−γ_2)^l (n+p) log^2 H); overall regret and sim-to-real gap depend on this error being negligible relative to main terms.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Clipping length l (too-short increases bias), stability constants κ_2 and γ_2 (govern exponential decay), and bounding of belief-state norms X_2.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Strong stability of predictor dynamics (Assumption 3) so clipped-history residual term decays exponentially; choose l = O(log(H n + H p)) to make clipping error O(1/poly(H)).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Requires predictor form A−F(Θ)C to be (κ_2, γ_2)-strongly stable; with that, a short clipped history suffices and reduces required representation capacity of function class F.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>History clipping is critical to make the intrinsic model complexity δ_E small (polylogarithmic rather than linear in H); it trades a small, controllable bias (exponentially decaying with l) for drastically lower sample complexity and provable √T regret, enabling provable sim-to-real transfer bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Provable Sim-to-real Transfer in Continuous Domain with Partial Observations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1670.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1670.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimulatorClassE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulator class E (parameterized LQG simulators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formal set of parameterized finite-horizon LQG simulators used as the training domain; the real-world model Θ* is assumed to be an element of E (realizability), and E's intrinsic complexity δ_E (Eluder dimension × log covering number × F_∞^2) controls transfer difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Parameterizable LQG systems (A,B,C families)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A family of linear systems parameterized by physical/control parameters; used to generate diverse simulated environments for robust training/analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>theoretical continuous control / sim-to-real formalism</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>E (abstract set of LQG simulators)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Each element is a finite-horizon LQG defined by matrices (A,B,C), process and measurement Gaussian noise, and cost matrices Q,R; E captures variability in physical parameters, sensor transforms, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>model-level analytical fidelity (linear-Gaussian approximation); not a physics engine but an exact linear stochastic model.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>System matrices (A,B,C) variability; Gaussian noise statistics; Kalman-filter/optimal-control solutions (P,K,Σ).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Does not include nonlinearities, detailed contact/vision rendering, multi-physics phenomena; all models are linear-Gaussian and realizability requires the real system to lie within this set.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>An unknown LQG system Θ* assumed to belong to E; real world may be any physical system approximable by an LQG in E under the realizability assumption.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Policies robust across E that perform well on Θ* when trained with robust adversarial objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Robust min-max training over E, plus LQG-VTR for regret minimization and model estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Sim-to-real gap Gap(π) bounded by Õ(√(δ_E H)); δ_E defined as dim_E(F,1/H) log(N(F,1/H)) F_∞^2.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Size and structure of E (intrinsic complexity δ_E), whether Θ* ∈ E (realizability), stability and boundedness across E, presence/absence of low-rank structure, and availability of real data to reduce complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>If E is low-dimensional/low-rank or can be combined with a small amount of real data to fix Σ(Θ*), then δ_E is small and transfer gap tight; model-selection warm-up and clipping further reduce effective complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Realizability (Θ* ∈ E) is required for theoretical guarantees; small δ_E (e.g., low-rank parameterization or additional real covariances) makes transfer feasible; no absolute numeric fidelity percentages provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The complexity of the simulator class E (δ_E) — not naive horizon H dependence — determines the sim-to-real gap; low-rank parameterizations or use of a small amount of real data (to fix Σ) can drastically reduce δ_E and improve transfer guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Provable Sim-to-real Transfer in Continuous Domain with Partial Observations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1670.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1670.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DomainRandMention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used empirical technique that randomizes simulator parameters during training so policies generalize to the real world; discussed here as a contrast whose theoretical guarantees are limited in continuous domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Various robotic agents used in prior empirical sim-to-real work (cited examples)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Empirically: robots in manipulation and locomotion tasks trained with randomized simulators to improve zero-shot transfer; in this paper, domain randomization is discussed conceptually, not used.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics (manipulation, locomotion), sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Empirical simulators referenced (e.g., MuJoCo / custom simulators in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics engines or custom simulators whose parameters (masses, frictions, textures, lighting) are randomized during training.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>varies; often medium-fidelity physics with randomized parameters and sometimes photorealistic/noisy sensors in vision-based variants.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Physical parameters, sensor noise, visual appearance, dynamics parameters (when applied empirically).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>May not model all real-world complexities; success depends on probability of sampling models close to the real world which can be exponentially small in continuous high-dimensional domains.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Not specified in this paper; referenced empirical works test on physical robots (e.g., dexterous hands, quadrupeds).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>General policy behaviors for manipulation and locomotion in cited empirical works (not evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Randomize simulator parameters and train policies (empirical domain randomization); here discussed as alternative to robust adversarial training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>High-dimensional continuous parameter spaces make sampling a simulator sufficiently close to the real world exponentially unlikely; therefore domain randomization can fail without strong prior knowledge or appropriate distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Requires selection of a domain-randomization distribution d(E) that places sufficient mass near Θ*; otherwise performance guarantees are weak in continuous domains.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper highlights a limitation: Chen et al. (2021) result relies on domain randomization sampling models very close to the real world with constant probability, which is unrealistic in continuous domains.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain randomization is an important empirical baseline but lacks strong theoretical guarantees in continuous partial-observation domains; robust adversarial training is proposed as a theoretically justified alternative when domain randomization cannot reliably sample near-Θ* models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Provable Sim-to-real Transfer in Continuous Domain with Partial Observations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dexterous in-hand manipulation <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Robust adversarial reinforcement learning <em>(Rating: 2)</em></li>
                <li>Understanding domain randomization for sim-to-real transfer <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer in deep reinforcement learning for robotics: a survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1670",
    "paper_id": "paper-253157500",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "RAT",
            "name_full": "Robust Adversarial Training (for sim-to-real)",
            "brief_description": "A min-max training paradigm that finds a policy minimizing the worst-case value gap across a class of simulators; analyzed here for continuous partially-observable linear-quadratic-Gaussian (LQG) domains and proven to produce policies with provable sim-to-real guarantees under realizability and stability assumptions.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_system_name": "LQG control agents (history-dependent policies)",
            "agent_system_description": "Controllers/policies for partially-observable linear dynamical systems (LQG) that select actions from histories/filtered beliefs; can be history-dependent and use Kalman-filter belief states.",
            "domain": "general robotics / continuous control (theoretical LQG setting)",
            "virtual_environment_name": "Simulator class E (parameterized finite-horizon LQGs)",
            "virtual_environment_description": "A parameterized set of finite-horizon linear-quadratic-Gaussian simulators, each specified by dynamics matrices (A,B,C) and Gaussian process/measurement noise; represents variability in physical parameters, delays, etc.",
            "simulation_fidelity_level": "analytical linear dynamical model (mathematical LQG fidelity)",
            "fidelity_aspects_modeled": "Linear state dynamics (A,B), linear observation model (C), Gaussian process and measurement noise, controller and Kalman-filter steady-state behavior.",
            "fidelity_aspects_simplified": "No nonlinear dynamics, no contact/impact/contact friction modeling, no photorealistic sensing, no complex unmodeled disturbances beyond Gaussian noise; real-world nonlinearity and unstructured noise are not modeled.",
            "real_environment_description": "An unknown real-world LQG system Θ* assumed (for the analysis) to belong to the simulator class E (realizability): same linear structure, Gaussian noises, and stability properties as simulators in E.",
            "task_or_skill_transferred": "Infinite-horizon average-cost or finite-horizon control policies (stabilizing optimal/near-optimal control) under partial observations.",
            "training_method": "Robust adversarial min-max training (find policy pi_RT = argmin_pi max_{Θ in E} [V^π(Θ)-V^*(Θ)]); analysis uses LQG-VTR algorithm to obtain regret guarantees used in reduction.",
            "transfer_success_metric": "Sim-to-real gap Gap(π(E)) = V^{π(E)}(Θ*) - V^*(Θ*); theoretical upper bound Õ(√(δ_E H)) and lower bound Ω(√H).",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Intrinsic complexity δ_E of simulator class E, horizon length H, partial observation / belief-state estimation error, stability properties (spectral radius and strong stability constants), clipped history length, model-selection errors.",
            "transfer_enabling_conditions": "Realizability (Θ* ∈ E); Assumptions 1–3: open-loop or closed-loop stability, bounded norms, contractible closed-loop matrices, strong stability of predictor matrix A−F(Θ)C; tight model-selection confidence sets; history clipping (short clipped histories) to reduce model-class complexity; LQG-VTR regret guarantee (O(√(δ_E T))).",
            "fidelity_requirements_identified": "The analysis requires the simulator class to include the real system (realizability) and to satisfy stability/strong-stability bounds; intrinsic complexity δ_E must be moderate (poly in dimensions, not growing with H) — history clipping reduces effective complexity to poly(log T). No absolute numeric % fidelity thresholds are given.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Robust adversarial training yields policies whose sim-to-real gap is bounded by Õ(√(δ_E H)); with the proposed LQG-VTR algorithm and history-clipping, δ_E can be made polylogarithmic in horizon (so gap scales ~√H and can be small for small intrinsic complexity); lower bound shows √H dependence is unavoidable, and realizability plus stability are necessary enabling conditions.",
            "uuid": "e1670.0",
            "source_info": {
                "paper_title": "Provable Sim-to-real Transfer in Continuous Domain with Partial Observations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LQG-VTR",
            "name_full": "LQG-Value-Targeted Regression (LQG-VTR)",
            "brief_description": "A new algorithm proposed in this paper for infinite-horizon average-cost LQG control that uses model selection, value-targeted regression, clipped histories and low-switching updates to achieve regret O(√(δ_E T)) depending on intrinsic model complexity δ_E.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "LQG learning agent (regret-minimizing controller)",
            "agent_system_description": "A history-dependent learning controller that interleaves model-selection warm-up, optimistic planning, episodic updates and value-targeted regression to estimate system parameters and produce control gains used online.",
            "domain": "theoretical continuous control / partially-observable LQG",
            "virtual_environment_name": "Simulator class E (finite-horizon/infinite-horizon LQGs) used for regression and planning",
            "virtual_environment_description": "Same as above: parameterized family of linear-Gaussian simulators; LQG-VTR collects trajectories from the unknown real system (assumed in E) while using simulators in E for planning/optimism.",
            "simulation_fidelity_level": "analysis-level LQG fidelity (mathematical linear dynamics with Gaussian noise); uses model-based estimates rather than physics-engine rendering.",
            "fidelity_aspects_modeled": "Belief-state evolution (Kalman filter), Riccati solutions (P, Σ), innovations distribution, steady-state covariances and Kalman gains.",
            "fidelity_aspects_simplified": "Nonlinearities, contacts, non-Gaussian noise, and realistic sensor modalities (e.g., images) are not modeled; often assumes ability to represent system in (A,B,C) linear form.",
            "real_environment_description": "An LQG real-world system Θ* satisfying stability and controllability/observability assumptions; initial data gathered with randomized actions during warm-up for model selection.",
            "task_or_skill_transferred": "Regret-minimizing control policies for infinite-horizon average-cost LQG that are then used for finite-horizon tasks (reduction used to bound sim-to-real gap).",
            "training_method": "Model-based learning with value-targeted regression (Ayoub et al. style), optimistic planning, episodic updates, low-switching policy updates; uses clipped-history estimators.",
            "transfer_success_metric": "Regret(π; T) and sim-to-real Gap; LQG-VTR achieves Regret = O(√(δ_E T)), which via reduction implies sim-to-real gap bound Õ(√(δ_E H)).",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Estimator error from full-history vs clipped-history (clipping error), size/dimension of function class F (Eluder dimension and covering number), number of episodes (switching cost), and model-selection warm-up length.",
            "transfer_enabling_conditions": "Accurate model-selection warm-up to ensure bounded belief states; use of clipped history length l = O(log(H n + H p)) to make intrinsic complexity small; confidence sets containing Θ* (optimism); low-switching updates to control regret.",
            "fidelity_requirements_identified": "Algorithm relies on LQG structure (Kalman filter behavior) and on boundedness/stability constants; clipping length chosen to make clipping error negligible (exponential decay κ^2(1−γ_2)^l), so effective fidelity required is that predictor dynamics be strongly stable.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "LQG-VTR provides a principled way to achieve instance-dependent √T regret by (i) pruning unlikely simulators (model selection), (ii) estimating models via value-targeted regression with clipped histories to reduce intrinsic complexity, and (iii) limiting policy switching; this regret bound underpins the provable sim-to-real gap for robust adversarial training.",
            "uuid": "e1670.1",
            "source_info": {
                "paper_title": "Provable Sim-to-real Transfer in Continuous Domain with Partial Observations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "HistoryClipping",
            "name_full": "History Clipping (clipped-history estimation)",
            "brief_description": "A technique introduced in this work that replaces full histories with a fixed short clipped history when computing value-target regression targets; it reduces the intrinsic complexity δ_E from O(H) to poly(log T) and controls the clipping error via exponential decay in the predictor stability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Belief-state based LQG controllers (Kalman-filtered agents)",
            "agent_system_description": "Controllers that use belief states x_{t|t} computed from histories; history clipping approximates full belief computation by truncating the history used to compute the belief.",
            "domain": "partially-observable continuous control (LQG)",
            "virtual_environment_name": "Simulator class E (finite-horizon LQGs) where histories are available",
            "virtual_environment_description": "Linear dynamical simulators that generate sequences of observations and actions; full histories determine Kalman-filter belief states exactly.",
            "simulation_fidelity_level": "mathematical LQG fidelity used for regression/estimation",
            "fidelity_aspects_modeled": "Short-term belief dynamics and innovations; steady-state Kalman behavior approximated with clipped histories.",
            "fidelity_aspects_simplified": "Long-run dependence on arbitrarily long histories is approximated away; residual from truncation is bounded but nonzero.",
            "real_environment_description": "Real LQG system whose belief state is approximated from clipped histories; clipping assumed valid due to strong stability (A−F C contractive).",
            "task_or_skill_transferred": "Accurate model estimation for policy optimization and value regression enabling low-regret control transfer.",
            "training_method": "Value-targeted regression using datasets with clipped-history features in LQG-VTR.",
            "transfer_success_metric": "Clipping error Δ_f per sample bounded by O(κ^2 (1−γ_2)^l (n+p) log^2 H); overall regret and sim-to-real gap depend on this error being negligible relative to main terms.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Clipping length l (too-short increases bias), stability constants κ_2 and γ_2 (govern exponential decay), and bounding of belief-state norms X_2.",
            "transfer_enabling_conditions": "Strong stability of predictor dynamics (Assumption 3) so clipped-history residual term decays exponentially; choose l = O(log(H n + H p)) to make clipping error O(1/poly(H)).",
            "fidelity_requirements_identified": "Requires predictor form A−F(Θ)C to be (κ_2, γ_2)-strongly stable; with that, a short clipped history suffices and reduces required representation capacity of function class F.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "History clipping is critical to make the intrinsic model complexity δ_E small (polylogarithmic rather than linear in H); it trades a small, controllable bias (exponentially decaying with l) for drastically lower sample complexity and provable √T regret, enabling provable sim-to-real transfer bounds.",
            "uuid": "e1670.2",
            "source_info": {
                "paper_title": "Provable Sim-to-real Transfer in Continuous Domain with Partial Observations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "SimulatorClassE",
            "name_full": "Simulator class E (parameterized LQG simulators)",
            "brief_description": "A formal set of parameterized finite-horizon LQG simulators used as the training domain; the real-world model Θ* is assumed to be an element of E (realizability), and E's intrinsic complexity δ_E (Eluder dimension × log covering number × F_∞^2) controls transfer difficulty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Parameterizable LQG systems (A,B,C families)",
            "agent_system_description": "A family of linear systems parameterized by physical/control parameters; used to generate diverse simulated environments for robust training/analysis.",
            "domain": "theoretical continuous control / sim-to-real formalism",
            "virtual_environment_name": "E (abstract set of LQG simulators)",
            "virtual_environment_description": "Each element is a finite-horizon LQG defined by matrices (A,B,C), process and measurement Gaussian noise, and cost matrices Q,R; E captures variability in physical parameters, sensor transforms, etc.",
            "simulation_fidelity_level": "model-level analytical fidelity (linear-Gaussian approximation); not a physics engine but an exact linear stochastic model.",
            "fidelity_aspects_modeled": "System matrices (A,B,C) variability; Gaussian noise statistics; Kalman-filter/optimal-control solutions (P,K,Σ).",
            "fidelity_aspects_simplified": "Does not include nonlinearities, detailed contact/vision rendering, multi-physics phenomena; all models are linear-Gaussian and realizability requires the real system to lie within this set.",
            "real_environment_description": "An unknown LQG system Θ* assumed to belong to E; real world may be any physical system approximable by an LQG in E under the realizability assumption.",
            "task_or_skill_transferred": "Policies robust across E that perform well on Θ* when trained with robust adversarial objectives.",
            "training_method": "Robust min-max training over E, plus LQG-VTR for regret minimization and model estimation.",
            "transfer_success_metric": "Sim-to-real gap Gap(π) bounded by Õ(√(δ_E H)); δ_E defined as dim_E(F,1/H) log(N(F,1/H)) F_∞^2.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Size and structure of E (intrinsic complexity δ_E), whether Θ* ∈ E (realizability), stability and boundedness across E, presence/absence of low-rank structure, and availability of real data to reduce complexity.",
            "transfer_enabling_conditions": "If E is low-dimensional/low-rank or can be combined with a small amount of real data to fix Σ(Θ*), then δ_E is small and transfer gap tight; model-selection warm-up and clipping further reduce effective complexity.",
            "fidelity_requirements_identified": "Realizability (Θ* ∈ E) is required for theoretical guarantees; small δ_E (e.g., low-rank parameterization or additional real covariances) makes transfer feasible; no absolute numeric fidelity percentages provided.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "The complexity of the simulator class E (δ_E) — not naive horizon H dependence — determines the sim-to-real gap; low-rank parameterizations or use of a small amount of real data (to fix Σ) can drastically reduce δ_E and improve transfer guarantees.",
            "uuid": "e1670.3",
            "source_info": {
                "paper_title": "Provable Sim-to-real Transfer in Continuous Domain with Partial Observations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "DomainRandMention",
            "name_full": "Domain Randomization (mentioned)",
            "brief_description": "A widely-used empirical technique that randomizes simulator parameters during training so policies generalize to the real world; discussed here as a contrast whose theoretical guarantees are limited in continuous domains.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_system_name": "Various robotic agents used in prior empirical sim-to-real work (cited examples)",
            "agent_system_description": "Empirically: robots in manipulation and locomotion tasks trained with randomized simulators to improve zero-shot transfer; in this paper, domain randomization is discussed conceptually, not used.",
            "domain": "robotics (manipulation, locomotion), sim-to-real transfer",
            "virtual_environment_name": "Empirical simulators referenced (e.g., MuJoCo / custom simulators in cited works)",
            "virtual_environment_description": "Physics engines or custom simulators whose parameters (masses, frictions, textures, lighting) are randomized during training.",
            "simulation_fidelity_level": "varies; often medium-fidelity physics with randomized parameters and sometimes photorealistic/noisy sensors in vision-based variants.",
            "fidelity_aspects_modeled": "Physical parameters, sensor noise, visual appearance, dynamics parameters (when applied empirically).",
            "fidelity_aspects_simplified": "May not model all real-world complexities; success depends on probability of sampling models close to the real world which can be exponentially small in continuous high-dimensional domains.",
            "real_environment_description": "Not specified in this paper; referenced empirical works test on physical robots (e.g., dexterous hands, quadrupeds).",
            "task_or_skill_transferred": "General policy behaviors for manipulation and locomotion in cited empirical works (not evaluated here).",
            "training_method": "Randomize simulator parameters and train policies (empirical domain randomization); here discussed as alternative to robust adversarial training.",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "High-dimensional continuous parameter spaces make sampling a simulator sufficiently close to the real world exponentially unlikely; therefore domain randomization can fail without strong prior knowledge or appropriate distributions.",
            "transfer_enabling_conditions": "Requires selection of a domain-randomization distribution d(E) that places sufficient mass near Θ*; otherwise performance guarantees are weak in continuous domains.",
            "fidelity_requirements_identified": "Paper highlights a limitation: Chen et al. (2021) result relies on domain randomization sampling models very close to the real world with constant probability, which is unrealistic in continuous domains.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Domain randomization is an important empirical baseline but lacks strong theoretical guarantees in continuous partial-observation domains; robust adversarial training is proposed as a theoretically justified alternative when domain randomization cannot reliably sample near-Θ* models.",
            "uuid": "e1670.4",
            "source_info": {
                "paper_title": "Provable Sim-to-real Transfer in Continuous Domain with Partial Observations",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dexterous in-hand manipulation",
            "rating": 2,
            "sanitized_title": "learning_dexterous_inhand_manipulation"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Robust adversarial reinforcement learning",
            "rating": 2,
            "sanitized_title": "robust_adversarial_reinforcement_learning"
        },
        {
            "paper_title": "Understanding domain randomization for sim-to-real transfer",
            "rating": 2,
            "sanitized_title": "understanding_domain_randomization_for_simtoreal_transfer"
        },
        {
            "paper_title": "Sim-to-real transfer in deep reinforcement learning for robotics: a survey",
            "rating": 1,
            "sanitized_title": "simtoreal_transfer_in_deep_reinforcement_learning_for_robotics_a_survey"
        }
    ],
    "cost": 0.020826749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Provable Sim-to-real Transfer in Continuous Domain with Partial Observations
2 Mar 2023</p>
<p>Jiachen Hu 
School of Computer Science
Center for Data Science
Department of Electrical and Computer Engineering
School of Intelligence Science and Technology
Peking University Center for Data Science
Institute of Big Data Research
National Key Laboratory of General Artificial Intelligence
Peking University
Peking University
Princeton University
Peking University
Beijing</p>
<p>Han Zhong hanzhong@stu.pku.edu.cn 
School of Computer Science
Center for Data Science
Department of Electrical and Computer Engineering
School of Intelligence Science and Technology
Peking University Center for Data Science
Institute of Big Data Research
National Key Laboratory of General Artificial Intelligence
Peking University
Peking University
Princeton University
Peking University
Beijing</p>
<p>Chi Jin chij@princeton.edu 
School of Computer Science
Center for Data Science
Department of Electrical and Computer Engineering
School of Intelligence Science and Technology
Peking University Center for Data Science
Institute of Big Data Research
National Key Laboratory of General Artificial Intelligence
Peking University
Peking University
Princeton University
Peking University
Beijing</p>
<p>Liwei Wang wanglw@cis.pku.edu.cn 
School of Computer Science
Center for Data Science
Department of Electrical and Computer Engineering
School of Intelligence Science and Technology
Peking University Center for Data Science
Institute of Big Data Research
National Key Laboratory of General Artificial Intelligence
Peking University
Peking University
Princeton University
Peking University
Beijing</p>
<p>Provable Sim-to-real Transfer in Continuous Domain with Partial Observations
2 Mar 2023
Sim-to-real transfer trains RL agents in the simulated environments and then deploys them in the real world. Sim-to-real transfer has been widely used in practice because it is often cheaper, safer and much faster to collect samples in simulation than in the real world. Despite the empirical success of the sim-to-real transfer, its theoretical foundation is much less understood. In this paper, we study the sim-to-real transfer in continuous domain with partial observations, where the simulated environments and real-world environments are modeled by linear quadratic Gaussian (LQG) systems. We show that a popular robust adversarial training algorithm is capable of learning a policy from the simulated environment that is competitive to the optimal policy in the real-world environment. To achieve our results, we design a new algorithm for infinite-horizon average-cost LQGs and establish a regret bound that depends on the intrinsic complexity of the model class. Our algorithm crucially relies on a novel history clipping scheme, which might be of independent interest. * . Equal contribution</p>
<p>Introduction</p>
<p>Deep reinforcement learning has achieved great empirical successes in various real-world decisionmaking problems, such as Atari games (Mnih et al., 2015), Go (Silver et al., 2016, 2017, and robotics control (Kober et al., 2013). In addition to the power of large-scale deep neural networks, these successes also critically rely on the availability of a tremendous amount of data for training. For these applications, we have access to efficient simulators, which are capable of generating millions to billions of samples in a short time. However, in many other applications such as auto-driving (Pan et al., 2017) and healthcare (Wang et al., 2018), interacting with the environment repeatedly and collecting a large amount of data is costly and risky or even impossible.</p>
<p>A promising approach to solving the problem of data scarcity is sim-to-real transfer (Kober et al., 2013;Sadeghi and Levine, 2016;Tan et al., 2018;Zhao et al., 2020), which uses simulated environments to generate simulated data. These simulated data is used to train the RL agents, which will be then deployed in the real world. These trained RL agents, however, may perform poorly in real-world environments owing to the mismatch between simulation and real-world environments. This mismatch is commonly referred to as the sim-to-real gap.</p>
<p>To close such a gap, researchers propose various methods including (1) system identification (Kristinsson and Dumont, 1992), which builds a precise mathematical model for the real-world environment; (2) domain randomization (Tobin et al., 2017), which randomizes the simulation and trains an agent that performs well on those randomized simulated environments; and (3) robust adversarial training (Pinto et al., 2017b), which finds a policy that performs well in a bad or even adversarial environment. Despite their empirical successes, these methods have very limited theoretical guarantees. A recent work Chen et al. (2021) studies the domain randomization algorithms for sim-to-real transfer, but this work has two limitations: First, their results (Chen et al., 2021, Theorem 4) heavily rely on domain randomization being able to sample simulated models that are very close to the real-world model with at least constant probability, which is hardly the case for applications with continuous domain. Second, their theoretical framework do not capture the problem with partial observations. However, sim-to-real transfer in continuous domain with partial observations is very common. Take dexterous in-hand manipulation (OpenAI et al., 2018) as an example, the domain consists of angels of different joints which is continuous and the training on the simulator has partial information due to the observation noises (for example, the image generated by the agent's camera can be affected by the surrounding environment). Therefore, we ask the following question:</p>
<p>Can we provide a rigorous theoretical analysis for the sim-to-real gap in continuous domain with partial observations?</p>
<p>This paper answers the above question affirmatively. We study the sim-to-real gap of the robust adversarial training algorithm and address the aforementioned limitations. To summarize, our contributions are three-fold:</p>
<p>• We use finite horizon LQGs to model the simulated and real-world environments, and also formalize the problem of sim-to-real transfer in continuous domain with partial observations. Under this framework, the learner is assumed to have access to a simulator class E , each of which represents a simulator with certain control parameters. We analyze the sim-to-real gap (Eqn. (6)) of the robust adversarial training algorithm trained in simulator class E . Our results show that the sim-to-real gap of the robust adversarial training algorithm isÕ( √ δ E H). Here H is the horizon length of the real task, and δ E denotes some intrinsic complexity of the simulator class E . This result shows that the sim-to-real gap is small for simple simulator classes and short tasks, while it gets larger for complicated classes and long tasks. By establishing a nearly matching lower bound, we further show this sim-to-real gap enjoys a near-optimal rate in terms of H.</p>
<p>• To bound the sim-to-real gap of the robust adversarial training algorithm, we develop a new reduction scheme that reduces the problem of bounding the sim-to-real gap to designing a sample-efficient algorithm in infinite-horizon average-cost LQGs. Our reduction scheme for LQGs is different from the one in Chen et al. (2021) for MDPs because the value function (or more precisely, the optimal bias function) is not naturally bounded in a LQG instance, which requires more sophisticated analysis.</p>
<p>• To prove our results, we propose a new algorithm, namely LQG-VTR, for infinite-horizon average-cost LQGs with convex cost functions. Theoretically, we establish a regret bound O( √ δ E T ) for LQG-VTR, where T is the number of steps. To the best of our knowledge, this is the first "instance-dependent" result that depends on the intrinsic complexity of the LQG model class with convex cost functions, whereas previous works only provide worst-case regret bounds depending on the ambient dimensions (i.e., dimensions of states, controls, and observations).</p>
<p>At the core of LQG-VTR is a history clipping scheme, which uses a clipped history instead of the full history to estimate the model and make predictions. This history clipping scheme helps us reduce the intrinsic complexity δ E exponentially from O(T ) to O(poly(log T )) (cf. Appendix G).</p>
<p>Our theoretical results also have two implications: First, the robust training algorithm is provably efficient (cf. Theorem 1) in continuous domain with partial observations. One can turn to the robust adversarial training algorithm if the domain randomization has poor performance (cf. Appendix A.4). Second, for stable LQG systems, only a short clipped history needs to be remembered to make accurate predictions in the infinite-horizon average-cost setting.</p>
<p>Related Work</p>
<p>Sim-to-real Transfer Sim-to-real transfer, using simulated environments to train a policy that can be transferred to the real world, is widely used in many realistic scenarios such as robotics (e.g., Rusu et al., 2017;Tan et al., 2018;Peng et al., 2018;OpenAI et al., 2018;Zhao et al., 2020). To close the sim-to-real gap, various empirical algorithms are proposed, including robust adversarial training (Pinto et al., 2017b), domain adaptation (Tzeng et al., 2015), inverse dynamics methods (Christiano et al., 2016), progressive networks (Rusu et al., 2017), and domain randomization (Tobin et al., 2017). In this work, we focus on the robust adversarial training algorithm. Jiang (2018); Feng et al. (2019); Zhong et al. (2019) studies the sim-to-real transfer theoretically, but they require real-world samples to improve the policy during the training phase, while our work does not use any real-world samples. Our work is mostly related to Chen et al. (2021), which studies the benefits of domain randomization for sim-to-real transfer. As mentioned before, however, Chen et al. (2021) cannot tackle the sim-to-real transfer in continuous domain with partial observations, which is the focus of our work.  Dennis et al. (2020) show that the robust policy obtained by the robust adversarial training method can achieve good performance in the real world. But these works lack theoretical guarantees. Broadly speaking, the robust adversarial training method is also related to the min-max optimal control (Ma et al., 1999;Ma and Braatz, 2001) and robust RL (Morimoto and Doya, 2005;Iyengar, 2005;Xu and Mannor, 2010;Ho et al., 2018;Tessler et al., 2019;Mankowitz et al., 2019;Goyal and Grand-Clement, 2022).</p>
<p>Robust Adversarial Training Algorithm</p>
<p>LQR and LQG There is a line of works (Mania et al., 2019;Cohen et al., 2019;Simchowitz and Foster, 2020;Lale et al., 2020a;Chen and Hazan, 2021;Lale et al., 2022) 2021) derives a T 1/2 worst-case regret depending on the ambient dimensions, which is based on strong assumptions and complicated regret analysis. In contrast, with weaker assumptions and cleaner analysis, our results only depend on the intrinsic complexity of the model class, which might be potentially small (cf. Appendix G).</p>
<p>Preliminaries</p>
<p>Finite-horizon Linear Quadratic Gaussian</p>
<p>We consider the following finite-horizon linear quadratic Gaussian (LQG) model:
x h+1 = Ax h + Bu h + w h , y h = Cx h + z h ,(1)
where x h ∈ R n is the hidden state at step h; u h ∈ R m is the action at step h; w h ∼ N (0, I n ) is the process noise at step h; y h ∈ R p is the observation at step h; and z h ∼ N (0, I p ) is the measurement noise at step h. Here the noises are i.i.d. random vectors. The initial state x 0 is assumed to follow a Gaussian distribution. Moreover, we denote by Θ := (A ∈ R n×n , B ∈ R n×m , C ∈ R p×n ) the parameters of this LQG problem. The learner interacts with the system as follows. At each step h, the learner observes an observation y h , chooses an action u h , and suffers a loss c h = c(y h , u h ), which is defined by
c(y h , u h ) = y ⊤ h Qy h + u ⊤ h Ru h ,
where Q ∈ R p×p and R ∈ R m×m are known positive definite matrices. For the finite-horizon setting, the interaction ends after receiving the cost c H , where H is a positive integer. Let H h = {y 0 , u 0 , · · · , y h−1 , u h−1 , y h } be the history at step h. Given a policy π = {π h : H h → u h } H h=0 , its expected total cost is defined by
V π (Θ) = E π H h=0 y ⊤ h Qy h + u ⊤ h Ru h ,
where the expectation is taken with respect to the randomness induced by the underlying dynamics and policy π. The learner aims to find the optimal policy π ⋆ with minimal expected total cost, which is defined by π ⋆ = arg min π V π (Θ). For simplicity, we use the notation V ⋆ (Θ) = V π ⋆ (Θ).</p>
<p>Infinite-horizon Linear Quadratic Gaussian</p>
<p>For the infinite-horizon average-cost LQG, we use the t and J to denote the time step and expected total cost function respectively to distinguish them from the finite-horizon setting. Similar to the finite-horizon setting, the learner aims to find a policy π = {π t : H t → u t } ∞ t=0 that minimizes the expected total cost J π (Θ), which is defined by
J π (Θ) = lim T →∞ 1 T E π T t=0 y ⊤ t Qy t + u ⊤ t Ru t .
The optimal policy π ⋆ in is defined by π ⋆ in def = arg min π J π (Θ). We also use the notation J ⋆ (Θ) = J π ⋆ in (Θ). We measure the T -step optimality of the learner's policy π by its regret:
Regret(π; T ) = T t=0 E π [c t − J ⋆ (Θ)].(2)
Throughout this paper, when π is clear from the context, we may omit π from Regret(π; T ). It is known that the optimal policy for this problem is a linear feedback control policy, i.e., u t = −K(Θ)x t|t,Θ , where K(Θ) is the optimal control gain matrix andx t|t,Θ is the belief state at step t (i.e. the estimated mean of x t ). One can use the Kalman filter (Kalman, 1960) to obtain x t|t,Θ and dynamic programming to obtain K(Θ) when the system Θ is known. In particular, denote P (Θ) as the unique solution to the discrete-time algebraic Riccati equation (DARE):
P (Θ) = A ⊤ P (Θ)A + C ⊤ QC − A ⊤ P (Θ)B(R + B ⊤ P (Θ)B) −1 B ⊤ P (Θ)A,
then K(Θ) can be obtained using P (Θ). We also use Σ(Θ) to denote the steady-state covariance matrix of x t . More details are deferred to Appendix A.3.</p>
<p>The LQG instance (1) can be depicted in the predictor form (Kalman, 1960;Lale et al., 2020bLale et al., , 2021 x
t+1 = (A − F (Θ)C)x t + Bu t + F (Θ)y t , y t = Cx t + e t ,(3)
where F (Θ) = AL(Θ) and e t denotes a zero-mean innovation process. Bellman Equation We define the optimal bias function of Θ = (A, B, C) for (x t|t,Θ , y t ) as
h ⋆ Θ x t|t,Θ , y t def =x ⊤ t|t,Θ (P (Θ) − C ⊤ QC)x t|t,Θ + y ⊤ t Qy t .(4)
With this notation, the Bellman optimality equation (Lale et al., 2020c, Lemma 4.3) is given by
J ⋆ (Θ) + h ⋆ Θ x t|t,Θ , y t = min u c(y t , u) + E Θ,u h ⋆ Θ x t+1|t+1,Θ , y t+1 ,(5)
where the equality is achieved by the optimal control of Θ. Notation We use O(·) notation to highlight the dependency on H, n, m, p, yet omit the polynomial dependency on some complicated instance-dependent constants (Õ(·) further omits polylogarithmic factors). For function f :
X → R, its ℓ ∞ -norm f ∞ is defined by sup x∈X f (x). We also define F ∞ def = sup f ∈F f ∞ .
Let ρ(·) denote the spectral radius, i.e., the maximum absolute value of eigenvalues.</p>
<p>Sim-to-real Transfer</p>
<p>The principal framework of sim-to-real transfer works as follows: the learner trains a policy in the simulators of the environment, and then applies the obtained policy to the real world. We follow Chen et al. (2021) to present a formulation of the sim-to-real transfer. The simulators are modeled as a set of finite-horizon LQGs with control parameters (e.g., physical parameters, control delays, etc.), where different parameters correspond to different dynamics. We denote this simulator class by E . To make the problem tractable, we also impose the realizability assumption: the real-world environment Θ ⋆ ∈ E is contained in the simulator set. Now we describe the sim-to-real transfer paradigm formally. In the simulation phase, the learner is given the set E , each of which is a parameterized simulator (LQG system). During the simulation phase, the learner can interact with each simulator for arbitrary times. However, the learner does NOT know which one represents the real-world environment, which may cause the learned policy to perform poorly in the real world. This challenge is commonly referred to as the sim-to-real gap. Mathematically, assuming the learned policy in the simulation phase is π(E ), its sim-to-real gap is defined by
Gap(π(E )) = V π(E ) (Θ ⋆ ) − V ⋆ (Θ ⋆ ),(6)
which is the difference between the cost of simulation policy π(E ) on the real-world model and the optimal cost in the real world.</p>
<p>Robust Adversarial Training Algorithm</p>
<p>With our sim-to-real transfer framework defined above, we now formally define the robust adversarial training algorithm used in the simulator training procedure.</p>
<p>Definition 1 (Robust Adversarial Training Oracle). The robust adversarial training oracle returns a (history-dependent) policy π RT such that
π RT = arg min π max Θ∈E [V π (Θ) − V ⋆ (Θ)],(7)
where V ⋆ is the optimal cost, and V π is the cost of π, both on the LQG model Θ. Note that the real world model Θ ⋆ is unknown to the robust adversarial training oracle.</p>
<p>This robust adversarial training oracle aims to find a policy that minimizes the worst case value gap. This oracle can be achieved by many algorithms in min-max optimal control (Ma et al., 1999;Ma and Braatz, 2001) and robust RL (Morimoto and Doya, 2005;Iyengar, 2005;Xu and Mannor, 2010;Pinto et al., 2017a,b;Ho et al., 2018;Tessler et al., 2019;Mankowitz et al., 2019;Kuang et al., 2022;Goyal and Grand-Clement, 2022).</p>
<p>Main Results</p>
<p>Before presenting our results, we introduce several standard notations and assumptions for LQGs.
Assumption 1. The real world LQG system Θ ⋆ = (A ⋆ , B ⋆ , C ⋆ ) is open-loop stable, i.e., ρ(A ⋆ ) &lt; 1. Assume Φ(A ⋆ ) def = sup τ ≥0 (A ⋆ ) τ 2 ρ(A ⋆ ) τ &lt; +∞.
We also assume that
(A ⋆ , B ⋆ ) and (A ⋆ , F (Θ ⋆ )) are controllable, (A ⋆ , C ⋆ ) is observable.
For completeness, we provide the definitions of controllable, observable, and stable systems in Appendix A.2. Assumption 1 is common in previous works studying the regret minimization or system identification of LQG (Lale et al., 2021, 2020bOymak and Ozay, 2019). The bounded Φ(A ⋆ ) condition is a mild condition as noted in Lale et al. (2020c); Oymak and Ozay (2019); Mania et al. (2019), which is satisfied when A ⋆ is diagonalizable. We emphasize that the (A ⋆ , F (Θ ⋆ ))-controllable assumption here only helps us simplify the analysis (can be removed by more sophisticated analysis). It is not a necessary condition for our results.</p>
<p>In the framework of sim-to-real transfer, we have access to a parameterized simulator class E . It is natural to assume E has a bounded norm. The stability of the systems relies on the contractible property of the close-loop control matrix A − BK (Lale et al., 2020c(Lale et al., , 2021 . Thus, we make the following assumption, which is also used in Lale et al. (2020cLale et al. ( , 2021.
Assumption 2. The simulators Θ ′ = (A ′ , B ′ , C ′ ) ∈ E have contractible close-loop control matri- ces: K(Θ ′ ) 2 ≤ N K and A ′ − B ′ K(Θ ′ ) 2 ≤ γ 1 for fixed constant N K &gt; 0, 0 &lt; γ 1 &lt; 1. We assume there exists constant N S such that for any Θ ′ ∈ E , A ′ 2 , B ′ 2 , C ′ 2 ≤ N S .
As we study the partially observable setting with infinite-horizon average cost, we also require the belief states of any LQG model in E to be stable and convergent (Mania et al., 2019;Lale et al., 2021). Since the dynamics for belief states follow the predictor form in (3), we assume matrix A − F (Θ)C is stable as follows.</p>
<p>Assumption 3. Assume there exists constant (κ 2 , γ 2 ) such that for any
Θ ′ = (A ′ , B ′ , C ′ ) ∈ E , the matrix A ′ − F (Θ ′ )C ′ is (κ 2 , γ 2 )-strongly stable. Here A ′ − F (Θ ′ )C ′ is (κ 2 , γ 2 )-strongly stable means F (Θ ′ ) 2 ≤ κ 2 and there exists matrices L and G such that A ′ − F (Θ ′ )C ′ = GLG −1 with L 2 ≤ 1 − γ 2 , G 2 G −1 2 ≤ κ 2 .
We assume x 0 ∼ N (0, I) for simplicity. Note that the Kalman filter converges exponentially fast to its steady-state (Caines and Mayne, 1970;Chan et al., 1984), we omit the error brought by x 0 starting at covariance I instead of the Σ(Θ ⋆ ) (see e.g., Appendix G of Lale et al. (2021)) without loss of generality. Now we are ready to state our main theorem, which establishes a sharp upper bound for the sim-to-real gap of π RT . Theorem 1. Under Assumptions 1, 2, and 3, the sim-to-real gap of π RT satisfies
Gap(π RT ) ≤Õ δ E H ,
where δ E (defined in Theorem 4) characterizes the complexity of model class E .</p>
<p>Here we useÕ to highlight the dependency with respect to the dimensions of the problem (i.e., m, n, p, H) and hide the uniform constants, log factors, and complicated instance-dependent constants. We present the high-level ideas of our proof in Section 4. The full proof is deferred to Appendix E. The following proposition shows that δ E has at most logarithmic dependency on H. Proposition 1. Under Assumptions 1, 2, and 3, the intrinsic complexity δ E is always upper bounded by δ E =Õ (poly(m, n, p)) .</p>
<p>The proof is deferred to Appendix G.1. Note that the sim-to-real gap of any trivial stable control policy is as large as O(H) since stable policies incur at most O(1) loss (compared with the optimal policy) at each step. Therefore, Theorem 1 along with proposition 1 ensures that π RT is a highly non-trivial policy which only suffers O(H −1/2 ) loss per step. Moreover, we can show that δ E will be smaller if E has more properties such as the low-rank structure. See Appendicies G.2 and G.3 for details.</p>
<p>To demonstrate the optimality of the upper bound in Theorem 1, we establish the following lower bound, which shows that the √ H sim-to-real gap is unavoidable. The proof is given in Appendix H.</p>
<p>Theorem 2 (Lower Bound). Under Assumptions 1, 2, and 3, for any history-dependent policyπ, there exists a model class E and a choice of Θ ⋆ ∈ E such that :
Gap(π) ≥ Ω( √ H).</p>
<p>Analysis</p>
<p>We provide a sketched analysis of the sim-to-real gap of π RT in this section. In a nutshell, we first perform a reduction from bounding the sim-to-real gap to an infinite-horizon regret minimization problem. We further show that there exists a history-dependent policy achieving low regret bound in the infinite-horizon LQG problem. This immediately implies that the sim-to-real gap of π RT will be small by reduction. Before coming to the analysis, we note first that any simulator Θ ∈ E not satisfying Assumption 1 cannot be the real world system. Therefore, we can prune the simulator set E to remove the models that do not satisfy Assumption 1.</p>
<p>The Reduction</p>
<p>Now we introduce the reduction technique, which connects the sim-to-real gap defined in Eqn. (6) and the regret defined in Eqn.</p>
<p>(2).</p>
<p>Lemma 3 (Reduction). Under Assumptions 1, 2, and 3, the sim-to-real gap of π RT can be bounded by the H-step regret bound of any (history-dependent) policy π defined in Eqn. (2):
Gap(π RT ) ≤ Regret(π; H) + D h ,
where D h does not depend on H.</p>
<p>Although the idea of reduction is also used in Chen et al. (2021), our reduction here is very different from theirs. Their proof relies on the communication assumption, which ensures that the optimal bias function in infinite-horizon MDPs is uniformly bounded. In contrast, the optimal bias function defined in (4) is not naturally bounded, which requires much more complicated analysis. See Appendix D for a detailed proof.</p>
<p>By Lemma 3, it suffices to construct a history-dependent policyπ that has low regret. Since we can treat any history-dependent policy π as an algorithm, it suffices to design an efficient regret minimization algorithm for the infinite-horizon average-cost LQG systems asπ.</p>
<p>The Regret Minimization Algorithm</p>
<p>Motivated by the reduction in the previous subsection, we constructπ as a sample-efficient algorithm LQG-VTR (Algorithm 1). The key steps of LQG-VTR are summarized below.</p>
<p>Model Selection It has been observed by many previous works (e.g., Simchowitz and Foster (2020);Lale et al. (2021Lale et al. ( , 2020c; Tsiamis et al. (2020)) that an inaccurate estimation of the system leads to the actions that cause the belief state to explode (i.e., x t|t,Θ 2 becomes linear or even super linear as t grows). To alleviate this problem, we utilize a model selection procedure before the optimistic planning algorithm to stabilize the system. The model selection procedure rules out some unlikely systems from the simulator set, which ensures that the inaccuracies do not blow up during the execution of LQG-VTR. To this end, we collect a few samples with random actions, and use a modified system identification algorithm to estimate the system parameters (Lale et al., 2021;Oymak and Ozay, 2019). After the model selection procedure, we show that with high probability the belief states and observations stay bounded throughout the remaining steps (cf. Lemma 5 in Appendix C), so LQG-VTR does not terminate at Line 9. Estimate the Model with Clipped History As the simulator class E is known to the agent, we use the value-target model regression procedure (Ayoub et al., 2020) to estimate the real-world model Θ ⋆ at the end of each episode k. To be more concrete, suppose the agent has access to the regression dataset Z = {E t } |Z| t=1 at the end of episode k, where E t is t-th sample containing the belief statex t|t , action u t , observation y t , the estimated bias function h ⋆ Θ , and the regression target (x t+1|t+1 , y t+1 ). HereΘ is the optimistic model used in the t-th sample. Then, inspired by the Bellman equation in (5) we can estimate the model by minimizing the following least-squares losŝ
Θ ′ k+1 = arg min Θ∈U1 Et∈Z E Θ,ut h ⋆ Θ x ′ t+1|t+1 , y ′ t+1 |x t|t − h ⋆ Θ x t+1|t+1 , y t+1 2 ,(8)
wherex ′ t+1|t+1 , y ′ t+1 denotes the random belief state and observation at step t + 1. However, it requires the full history at step t to compute the expectation in Eqn. (8) as we show in Section 4.4, which leads to an O(H) intrinsic complexity (i.e., δ E = O(H)). Then the sim-to-real gap of π RT in Theorem 1 becomes O(H), which is vacuous. Fortunately, we can use a clipped history (Line 12 of Algorithm 1) to compute an approximation of the expectation. Let f Θ (x t|t , u,Θ)
def ≈ E Θ,u [h ⋆ Θ (x ′ t+1|t+1 , y ′ t+1 ) |x t|t ]
be the approximation of the expectation (see Section 4.4 for the formal definition), we use f to estimate Θ ⋆ with dataset Z:
Θ k+1 = arg min Θ∈U1 Et∈Z f Θ x t|t , u t ,Θ − h ⋆ Θ x t+1|t+1 , y t+1 2 .(9)
With this estimator, we can construct the following confidence set,
C Θ k+1 , Z = Θ ∈ U 1 : Et∈Z f Θ (x t|t , u t ,Θ) − fΘ k+1 (x t|t , u t ,Θ) 2 ≤ β ,(10)
Update with Low-switching Cost Since the regret bound of LQG-VTR has a linear dependency on the number of times the algorithm switches the control policies (cf. Appendix E.5), we have to ensure the low-switching property ( We only synchronize the current dataset with the new dataset and update the current policy when this value is greater than 1.</p>
<p>Regret Bound of LQG-VTR Theorem 4. Under Assumptions 1, 2, and 3, the regret (Eq.(2)) of LQG-VTR is bounded bỹ
O δ E H ,(11)
Algorithm 1 LQG-VTR 1: Initialize: set model selection period length T w by Eqn. (56).</p>
<p>2:</p>
<p>set maximum state allowed M x =X 1 (X 1 is defined in Lemma 5).</p>
<p>3:</p>
<p>set maximum number of episodeK by Eqn. (77).</p>
<p>4:</p>
<p>set ψ by Lemma 9, β by Eqn. (119), l = O(log(Hn + Hp)).</p>
<p>5:
set Z = Z new = ∅, initial state x 0 ∼ N (0, I), episode k = 1. 6: Compute U 1 = Model Selection(T w , E ) (Algorithm 2),Θ 1 = arg min Θ∈U1 J ⋆ (Θ). 7: for step t = T w + 1, · · · , H do 8: if x t|t,Θ k 2 &gt; M x then 9:
Take u t = 0 for the remaining steps and halt the algorithm.</p>
<p>10:</p>
<p>Compute the optimal action under systemΘ k : u t = −K(Θ k )x t|t,Θ k .</p>
<p>11:</p>
<p>Take action u t and observe y t+1 .</p>
<p>12:
Let l clip def = min(l, t), define τ t def = (y t , u t−1 , y t−1 , u t−2 , y t−2 , ..., y t−l clip +1 , u t−l clip ). 13: Add sample E t def = (τ t ,x t|t,Θ k ,Θ k , u t ,x t+1|t+1,Θ k , y t+1 ) to the set Z new .
14:</p>
<p>if importance score sup f1,f2∈F f1−f2 2 Znew f1−f2 2 Z +ψ ≥ 1 and k &lt;K then</p>
<p>15:</p>
<p>Add the history data Z new to the set Z, and reset Z new = ∅.</p>
<p>16:</p>
<p>CalculateΘ k+1 using Eqn. (9).</p>
<p>17:</p>
<p>Update the confidence set U k+1 = C(Θ k+1 , Z) by Eqn. (10).</p>
<p>18:
ComputeΘ k+1 = arg min Θ∈U k+1 J ⋆ (Θ); episode counter k = k + 1.
where the intrinsic model complexity δ E is defined as
δ E def = dim E (F , 1/H) log(N (F , 1/H)) F 2 ∞ .(12)
Here we use function class F to capture the complexity of E , which is defined in Section 4.4. Under Assumptions 1, 2, and 3, we obtain a √ H-regret as desired because δ E is at most O(poly(m, n, p)) (cf. Appendix G). Notably, if we do not adopt the history clipping technique, δ E will be linear in H, resulting in the regret bound becoming vacuous. Compared with existing works (Simchowitz et al., 2020;Lale et al., 2020cLale et al., , 2021, we achieve a √ H-regret bound for general model class E (depends on the intrinsic complexity of E ) with weaker assumptions and cleaner analysis. Theorem 1 is directly implied by Theorem 4 and the reduction in (75).</p>
<p>Construction of the Function Class F</p>
<p>In this subsection, we present the intuition and construction of F . To begin with, recall that the optimal bias function h ⋆ Θ of the optimistic systemΘ is
h ⋆ Θ x t+1|t+1,Θ , y t+1 =x ⊤ t+1|t+1,Θ P −C ⊤ QC x t+1|t+1,Θ + y ⊤ t+1 Qy t+1 ,
where (P ,C) are the parameters with respect toΘ. Given any underlying system Θ, we know the next step observation y t+1,Θ given belief statex t|t,Θ and action u is defined as
y t+1,Θ = Y x t|t,Θ , u, Θ def = CAx t|t,Θ + CBu + Cw t + CA(x t −x t|t,Θ ) + z t+1 .
Here we use Y to denote the stochastic process that generates y t+1,Θ under system Θ.</p>
<p>Thus, we can define function f ′ to be the one-step Bellman backup of the optimal bias function h ⋆ Θ under the transition of the underlying system Θ:
f ′ Θ τ t ,x t|t,Θ , u,Θ def = E h ⋆ Θ x t+1|t+1,Θ , y t+1,Θ , y t+1,Θ = Y x t|t,Θ , u, Θ .(13)
The next step belief statex t+1|t+1,Θ is determined by the Kalman filter aŝ
x t+1|t+1,Θ = I −LC Ãx t|t,Θ +Bu +Ly t+1,Θ .(14)
However, we need the full history τ t to computex t|t,Θ as shown below, which is unacceptable. To mitigate this problem, we compute the approximate belief statex c t|t,Θ with clipped length-l history τ l = {u t−l , y t−l+1 , · · · , u t−1 , y t }:
x t|t,Θ = (I − LC) Ax t−1|t−1,Θ + (I − LC) Bu t−1 + Ly t (require full history τ t to computex t|t,Θ ) = (A − LCA) lx t−l|t−l,Θ + l s=1 (A − LCA) s−1 ((I − LC)Bu t−s + Ly t−s+1 ) x c t|t,Θ .(15)
Thanks to Assumption 3, we can show that (
A − LCA) lx t−l|t−l,Θ is a small term whose ℓ 2 norm is bounded by O(κ 2 (1 − γ 2 ) l ) since (I − LC)A 2 = A(I − LC) 2 .
Therefore,x c t|t,Θ will be a good approximation ofx t|t,Θ , helping us control the error of clipping the history.</p>
<p>With the above observation, we formally define F , which is an approximation by replacing the full history with the clipped history. Let the domain of F be X
def = B m U × B p Y l × B n X1 × B m U × E , where B d v = {x ∈ R d : x 2 ≤ v} for any (d, v) ∈ N × R.
HereŪ ,Ȳ andX 1 will be specified in Lemma 5. We define F formally as follows:
F def = {f Θ : X → R | Θ ∈ E }, f Θ τ l ,x t|t,Θ , u,Θ def = E h ⋆ Θ x t+1|t+1,Θ , y c t+1,Θ , y c t+1,Θ = Y x c t|t,Θ , u, Θ .(16)
Herex t+1|t+1,Θ is determined similarly as in Eqn. (14), where the only difference is the next observation y t+1,Θ replaced by y c t+1,Θ .</p>
<p>Conclusion</p>
<p>In this paper, we make the first attempt to study the sim-to-real gap in continuous domain with partial observations. We show that the output policy of the robust adversarial algorithm enjoys the near-optimal sim-to-real gap, which only depends on the intrinsic complexity of the simulator class. To prove our theoretical results, we design a novel algorithm for infinite-horizon LQG and establish a sharp regret bound. Our algorithm features a history clipping mechanism. This work opens up several directions: for example, can we extend our results to the non-linear dynamics system? Moreover, though the robust adversarial training algorithm is proved to be efficient, is it possible to find or design provably efficient algorithms that enjoy better computational efficiency in the training stage? We leave them to future investigations.</p>
<p>Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. Advances in neural information processing systems, 21, 2008. </p>
<p>A. Missing Parts</p>
<p>A.1 Notation Table</p>
<p>For the convenience of the reader, we summarize some notations that will be used. Notation Explanation E , δ E model class and its complexity Θ = (A, B, C) dynamics parameters defined in (1)
V ⋆ (Θ), J ⋆ (Θ)
optimal cost of finite-horizon and infinite-horizon setting for Θ K, P, L, Σ see (18), (17), (19), and (20) F AL (cf. (3)) γ 1 , κ 2 , γ 2 strongly stable coefficients X 1 ,Ȳ ,Ū ,X 2 upper bounds ofx t|t,Θ k , y t , u t ,x t|t,Θ in (25), (26), and (27) 
N S max{ A , B , C } ≤ N S for all Θ ∈ E N U max{ Q 2 , R 2 } N K , N P , N Σ , N L
upper bounds of K, P, Σ, L, respectively (see Appendix B) K,K number of episodes of LQG-VTR, upper bound of K (see Lemma 9) S(k), T (k) start step and end step of episode k, respectivelỹ Θ t optimistic model used for planning at step t Θ k optimistic model used during episode k (Θ t =Θ k for S(k) ≤ t ≤ T (k)) x t|t,Θ belief state at step t measured under system Θ w,z high probability upper bound of w t 2 , z t 2 e t ,ẽ t Gaussian noises used in the induction stage 2 of the proof of Lemma 5 C 0 , C 1 , C 2 quantities defined in the induction proof of Lemma 5 T A , T B , T C , T L lower bound of T w for desired properties at induction stage 1 
T ′ A , T ′ B , T g0 , T g1 , T g2 ,− BK = HLH −1 with L 2 ≤ 1 − γ, H 2 H −1 2 ≤ κ.
Note that any stable LQG system is also strongly stable in terms of some parameters (Cohen et al., 2018), and a strongly stable system is also stable.</p>
<p>Definition 4 (Covering Number). We use N (F , ǫ) to denote the ǫ-covering number of a set F with respect to the ℓ ∞ norm, which is the minimum integer N such that their exists F ′ ∈ F with |F ′ | = N , and for any f ∈ F their exists f ′ ∈ F ′ satisfying f − f ′ ∞ ≤ ǫ.</p>
<p>Definition 5 (ǫ-Independent). For the function class F defined in Z, we say z is ǫ-independent of
{z 1 , · · · , z n } ∈ Z if there exist f, f ′ ∈ F satisfying n i=1 (f (z i ) − f ′ (z i )) 2 ≤ ǫ and f (z) − f ′ (z) ≥ ǫ.
Definition 6 (Eluder Dimension). For the function class F defined in Z, the ǫ-Eluder dimension is the longest sequence {z 1 , · · · , z n } ∈ Z such that there exists ǫ ′ ≥ ǫ where z i is ǫ ′ -independent of {z 1 , · · · , z i−1 } for any i ∈ [n].</p>
<p>A.3 Preliminaries about the optimal control and the Kalman Filter</p>
<p>It is known that the optimal policy for this problem is a linear feedback control policy, i.e., u t = −K(Θ)x t|t,Θ , where K(Θ) is the optimal control gain matrix andx t|t,Θ is the belief state at step t (i.e. the estimation of the hidden state).</p>
<p>Let P (Θ) be the unique solution to the discrete-time algebraic Riccati equation (DARE):
P (Θ) = A ⊤ P (Θ)A + C ⊤ QC − A ⊤ P (Θ)B(R + B ⊤ P (Θ)B) −1 B ⊤ P (Θ)A.(17)
Then K(Θ) can be calculated by
K(Θ) = (R + B ⊤ P (Θ)B) −1 B ⊤ P (Θ)A.(18)
The belief statex t|t,Θ is defined as the mean of x t under system Θ, which is decided by the system parameter Θ and history H t . Moreover, assumingx 0|−1,Θ = 0, the belief state can be calculated by the Kalman filter:
x t|t,Θ = (I − L(Θ)C)x t|t−1,Θ + L(Θ)y t , x t|t−1,Θ = Ax t−1|t−1,Θ + Bu t−1 , L(Θ) = Σ(Θ)C ⊤ (CΣ(Θ)C ⊤ + I) −1 ,(19)
where Σ(Θ) is the unique positive semidefinite solution to the following DARE:
Σ(Θ) = AΣ(Θ)A ⊤ − AΣ(Θ)C ⊤ (CΣ(Θ)C ⊤ + I) −1 CΣ(Θ)A ⊤ + I.(20)
We sometimes use L, P, K, Σ as the shorthand of L(Θ), P (Θ), K(Θ), Σ(Θ) when the system Θ is clear from the context. The predictor form is another formulation of LQG instance (1) (Kalman, 1960;Lale et al., 2020bLale et al., , 2021 x
t+1 = (A − F (Θ)C)x t + Bu t + F (Θ)y t , y t = Cx t + e t ,(21)
where F (Θ) = AL(Θ) and e t denotes a zero-mean innovation process. In the steady state, we have e t ∼ N (0, CΣ(Θ)C ⊤ + I). The dynamics ofx t|t−1,Θ follows exactly the predictor form.</p>
<p>A.4 Domain Randomization</p>
<p>Definition 7 (Domain Randomization Oracle (Chen et al., 2021)). The domain randomization oracle returns a (history-dependent) policy π DR such that
π DR = arg min π E Θ∼d(E ) [V π (Θ) − V ⋆ (Θ)] ,(22)
where V ⋆ is the optimal cost, V π is the cost of π, and d(E ) is a distribution over E .</p>
<p>As shown in Theorem 4 of Chen et al. (2021), even with an additional smooth assumption (Chen et al., 2021, Assumption 3), the performance of π DR depends crucially on d(E ). In high dimensional continuous domain, the probability of sampling an accurate model close to the real world model by uniform randomization is exponentially small. Thus, the learner needs to carefully choose the domain randomization distribution d(E ) with strong prior knowledge of the real world model. In contrast, the robust adversarial training oracle does not worry about this, and it just needs to solve (7) by some min-max optimal control or robust RL algorithms.</p>
<p>B. Uniform Bounded Simulator Set</p>
<p>Under Assumptions 1, 2, and 3, it is possible to provide a uniform upper bound on the spectral norm of P (Θ), Σ(Θ), K(Θ) and L(Θ) after the model selection procedure. Let N U def = max( Q 2 , R 2 ).</p>
<p>For any Θ = (A, B, C) ∈ E , we have K 2 ≤ N K by Assumption 2. By definition P is the unique solution to the equation
(A − BK) ⊤ P (A − BK) − P = C ⊤ QC + K ⊤ RK,
where ρ(A − BK) &lt; 1. Therefore, Lemma B.4 of Simchowitz et al. (2020) shows
P = ∞ k=0 (A − BK) ⊤ k (C ⊤ QC + K ⊤ RK)(A − BK) k . Since (A − BK) k 2 ≤ (1 − γ 1 ) k , P 2 ≤ N 2 S N U + N 2 K N U ∞ k=0 (1 − γ 1 ) 2k ≤ N P def = N U (N 2 S + N 2 K ) 2γ 1 − γ 2 1 .
Similarly, Σ is the unique solution to the equation
(A − F C)Σ(A − F C) ⊤ − Σ = I + F F ⊤ ,
where F = AL and A − F C is (κ 2 , γ 2 )-strongly stable under Assumption 3. Thus we know
Σ 2 ≤ N Σ def = κ 2 2 (1 + κ 2 2 ) 2γ 2 − γ 2 2 . Finally, we have L 2 = ΣC ⊤ (CΣC ⊤ + I) −1 2 ≤ N L def = N Σ N S since (CΣC ⊤ + I) −1 2 ≤ 1.
In this paper, we use N P , N Σ , N K , N L as the uniform upper bound on the spectral norm of P (Θ), Σ(Θ), K(Θ) and L(Θ) for any Θ ∈ E .</p>
<p>C. Details for the Model Selection Procedure</p>
<p>In this section we provide a complete description of the model selection procedure.</p>
<p>The main purpose of running a model selection procedure at the beginning of LQG-VTR is to obtain a more accurate estimateÂ,B,Ĉ,L of the real-world model Θ ⋆ . This accurate estimate is very useful to show that the belief states encountered in LQG-VTR will be bounded (see Lemma 5).</p>
<p>We 
whereĀ = A − F C = A − ALC.
The regression model for M can be established by
y t = Mφ t + e t + CĀH x t−H , where φ t = y ⊤ t−1 . . . y ⊤ t−H u ⊤ t−1 . . . u ⊤ t−H ⊤ .
Here e t is a Gaussian noise, and CĀ H x t−H is an exponentially small bias term. Thanks to Assumption 3,Ā is a (κ 2 , γ 2 )-strongly stable matrix. Hence ĀH 2 = O(κ 2 (1 − γ 2 )H ) = O(1/H 2 ), and the bias term will be negligible.</p>
<p>Algorithm 2 Model Selection</p>
<p>1: Input: model selection period length T w by Eqn. (56), simulator set E . 2: Set σ u = 1. 3: Execute action u t ∼ N (0, σ 2 u I) for T w steps, and gather dataset D init = {y t , u t } Tw t=1 . 4: Set the truncation lengthH = O(log(κ 2 H)/ log(1/γ 2 )). 
Â − T ⊤Ā T 2 ≤ β A , B − T ⊤B 2 ≤ β B , Ĉ −CT 2 ≤ β C , L − T ⊤ L(Θ) 2 ≤ β L .(23)
Here the unitary T is used because any LQG system transformed by a unitary matrix has exactly the same distribution of observation sequences as the original one (i.e., LQGs are equivalent under unitary transformations). Without loss of generality, we can assume T = I. β A , β B , β C , β L measure the width of the confidence set, and we have
β A , β B , β C , β L ≤ C w T w(24)
for some instance-dependent parameter C w according to Theorem 3.4 of Lale et al. (2021). Now we discuss how to decide the value of T w so that the following bounded state property holds after the model selection procedure, which is crucial to bound the regret of LQG-VTR. We define K as the number of episodes of LQG-VTR.</p>
<p>Lemma 5 (Bounded State). With probability at least 1 − δ, throughout the optimistic planning stage of LQG-VTR (i.e., after the model selection procedure), there exists constantsX 1 ,X 2 ,Ȳ ,Ū such that the belief statex t|t,Θ k measured inΘ k for any episode k ∈ [K] and any step t in episode k satisfy
x t|t,Θ k 2 ≤X 1 .(25)
Accordingly for any step t ∈ [H],
y t 2 ≤Ȳ , u t 2 ≤Ū .(26)
For any step t ∈ [H] and any system Θ = (A, B, C) ∈ E , the belief state measured under Θ with history (y 0 , u 0 , y 1 , u 1 , ..., y t−1 , u t−1 , y t ) is bounded
x t|t,Θ 2 ≤X 2 ,(27)
whereX 1 ,Ȳ ,Ū ,X 2 = O ( √ n + √ p) log H .</p>
<p>As a consequence, the algorithm does not terminate at Line 9 with probability 1 − δ.</p>
<p>Proof. Let S(k) and T (k) be the starting step and ending step of episode k. For simplicity, we usẽ Θ t = (Ã t ,B t ,C t ) to denoteΘ k for S(k) ≤ t ≤ T (k) for time step t (The first episode starts after the model selection procedure, S(1) = T w + 1). The core problem here is to show that after the model selection procedure (Algorithm 2), the belief statex t|t,Θt underΘ t stays bounded, then u t also stays bounded since u t = −K(Θ t )x t|t,Θt . Since Θ ⋆ ∈ C with probability at least 1 − δ/4, we assume this event happens. Before going through the proof, we define constantw,z andū such that with probability 1 − δ/4, w t 2 ≤w, z t 2 ≤z for any 0 ≤ t ≤ H, and u t 2 ≤ū for 0 ≤ t ≤ T w . Since w t , z t , u t are independent Gaussian variables, we knoww = O( n log(H)),z = O( p log(H)),ū = O( m log(T w )). For the model selection procedure (which contains the first T w steps), it holds that with probability 1 − δ/4, for any t ≤ T w , x t 2 ≤ O( n log(H)), u t 2 ≤ū, y t 2 ≤ O(N S n log(H) + p log(H)) by Lemma D.1 of Lale et al. (2020c). We assume this high probability event happens for now.</p>
<p>We use an inductive argument to show that for any episode k and step t therein, it holds that x t|t,Θt 2 ≤ C 0 (1 + 1/K) k for some C 0 . To guarantee action u t does not blow up, we set the upper bound of x t|t,Θt 2 asX 1 def = eC 0 ≥ C 0 (1 + 1/K) k for any k ≤ K &lt;K. Thus, LQG-VTR terminates as long as x t|t,Θt 2 exceeds that upper bound (Line 9 of Algorithm 1). For convenient, we call the model selection procedure as episode 0 (so T (0) = T w ). As the inductive hypothesis, suppose that x t|t,Θt 2 ≤ D 0 ≤ C 0 (1 + 1/K) k holds for some D 0 and step t ≤ T (k) + 1 when k ≥ 1. We define D 0 = C 0 for C 0 = O(( √ n + √ p) log H) specified later when k = 0. Then we know that the algorithm does not terminate in the first k episodes. Moreover, we have for any t ≤ T (k) it holds that u t 2 ≤ N K D 0 . Thus for t ≤ T (k) + 1, by definition of x t and y t we have
x t 2 ≤ Φ(A ⋆ ) 1 − ρ(A ⋆ ) · (N S N K D 0 +w), y t 2 ≤ N S x t 2 +z.(29)
Suppose D 0 ≥wΦ(A ⋆ )/(1 − ρ(A ⋆ )), so that
x t 2 ≤ 1 + N S N K Φ(A ⋆ ) 1 − ρ(A ⋆ ) D 0 , y t 2 ≤ 1 + N S + N 2 S N K Φ(A ⋆ ) 1 − ρ(A ⋆ ) D 0 .(30)
Hence, we can define three constants D u , D x , D y such that for any t ≤ T (k) it holds that u t 2 ≤ D u D 0 ; for any t ≤ T (k) + 1, we have x t 2 ≤ D x D 0 , y t 2 ≤ D y D 0 . Note that D u , D x , D y are all instance-dependent constants. To be more concrete, it suffices to set
D u = N K D x = 1 + N S N K Φ(A ⋆ ) 1 − ρ(A ⋆ ) D y = 1 + N S + N 2 S N K Φ(A ⋆ ) 1 − ρ(A ⋆ ) .
Induction Stage 1: fix t = T (k) + 1 and show x t|t,Θ k 2 is very close to x t|t,Θ k+1 2</p>
<p>In stage 1, we need to derive four terms T A , T B , T C , T L as the lower bound of T w to achieve the desired concentration property. This means as long as T w ≥ T A , T B , T C , T L , we can prove the induction stage 1 thanks to the tight confidence set C defined in Eqn. (23). Now fix t = T (k) + 1 = S(k + 1), for any Θ ∈ U 1 we havex
t|t,Θ = t s=1 A s−1 (Bu t−s + Ly t−s+1 ) + A t Ly 0 ,(31)
where
A def = (I − LC)A, B def = (I − LC)B.
Note that A s 2 ≤ κ 2 (1 − γ 2 ) s according to Assumption 3. Therefore, for Θ =Θ k and Θ =Θ k+1 (bothΘ k andΘ k+1 is chosen from the confidence set C by definition),
x t|t,Θ k −x t|t,Θ k+1 = t s=1Ã s−1 k B k u t−s +L k y t−s+1 +Ã t kLk y 0 − t s=1Ã s−1 k+1 B k+1 u t−s +L k+1 y t−s+1 +Ã t k+1Lk+1 y 0 .
To move on,
x t|t,Θ k −x t|t,Θ k+1 = t s=1Ã s−1 k B k u t−s +L k y t−s+1 − t s=1Ã s−1 k B k+1 u t−s +L k+1 y t−s+1 + t s=1Ã s−1 k B k+1 u t−s +L k+1 y t−s+1 − t s=1Ã s−1 k+1 B k+1 u t−s +L k+1 y t−s+1 +Ã t kLk y 0 −Ã t k+1Lk+1 y 0 .
We bound these three terms separately in the following. For the first term, observe that when
T w ≥ T L , we have t s=1Ã s−1 k (L k −L k+1 )y t−s+1 2 ≤ t s=1 κ 2 (1 − γ 2 ) s−1 · 2 √ C w √ T L · D y D 0 ≤ 2κ 2 D y √ C w γ 2 √ T L · D 0 .(32)
where the first inequality is due to the confidence set (Eqn. (23)), the induction hypothesis, and Assumption 3. Thus, as long as
T L ≥ (12κ 2 D y √ C wK /γ 2 ) 2 = O(K 2 ), we know t s=1Ã s−1 k (L k −L k+1 )y t−s+1 2 ≤ D 0 6K .(33)
Similarly, we have
B k −B k+1 2 ≤ C w 1 + N L N S √ T B + N 2 S √ T L + N L N S √ T C
by checking the definition ofB k andB k+1 . Therefore, as long as
T B ≥ 36κ 2 D u √ C w (1 + N L N S )K γ 2 2 = O(K 2 ) T L ≥ 36κ 2 D u √ C w N 2 SK γ 2 2 = O(K 2 ) T C ≥ 36κ 2 D u √ C w N L N SK γ 2 2 = O(K 2 ), we have B k −B k+1 2 ≤ γ 2 12κ 2 D uK .
Therefore, it holds that t s=1Ã s−1
k (B k −B k+1 )u t−s 2 ≤ D 0 6K .(34)
following the same reason as Eqn. (32).</p>
<p>To sum up, we require
T B ≥ O(K 2 ), T L ≥ O(K 2 ), T C ≥ O(K 2 )
to ensure that as long as T w ≥ max(T B , T L , T C ), the first term is bounded by
t s=1Ã s−1 k (B k −B k+1 )u t−s + (L k −L k+1 )y t−s+1 2 ≤ D 0 3K (35)
from Eqn. (33) and Eqn. (34). For the second term,
t s=1 Ã s−1 k −Ã s−1 k+1 B k+1 u t−s +L k+1 y t−s+1 (36) ≤ t s=1 ξ k · (s − 1)κ 2 2 (1 − γ 2 ) s−2 (N S + N 2 S N L )D u + N L D y D 0 (see Eqn. (148)) (37) ≤ ξ k · (κ 2 /γ 2 ) 2 t(1 − γ 2 ) t−1 (N S + N 2 S N L )D u + N L D y D 0 ,(38)
where
ξ k = Ã k −Ã k+1 2 . Note that C 1 def = max t≥0 t(1 − γ 2 ) t−1 is a constant. Similarly, we require T A , T C , T L ≥ O(K 2 ) (see Eqn.
(32) and Eqn. (33) for an example on how T A , T C , T L are derived) to ensure that as long as T w ≥ max(T A , T C , T L ), it holds that
ξ k · (κ 2 /γ 2 ) 2 C 1 (N S + N 2 S N L )D u + N L D y D 0 ≤ D 0 3K .(39)
Using the same argument, by setting an appropriate T L we have as long as T w ≥ T L , it holds that
Ã t kLk y 0 −Ã t k+1Lk+1 y 0 ≤ D 0 3K .(40)
As a consequence, combining Eqn. (35), (39), and (40) gives
x t|t,Θ k −x t|t,Θ k+1 2 ≤ D 0 K (41) x t|t,Θ k+1 2 ≤ D 0 1 + 1 K .(42)
For now we have proved the induction at the beginning step t = S(k + 1) of episode k + 1, it suffices to show that x t|t,Θ k+1 2 ≤ (1 + 1/K)D 0 for the rest of episode k + 1.</p>
<p>Induction Stage 2:</p>
<p>show that x t|t,Θ k+1 2 ≤ (1 + 1/K)D 0 holds for the whole episode k + 1.</p>
<p>Suppose LQG-VTR does not terminate during episode k + 1, then the algorithms follows the optimal control ofΘ k+1 . DenoteΘ k+1 = (Ã,B,C), we follow the Eqn. (46) of Lale et al. (2020c) to decomposex t|t,Θ k+1 for S(k + 1) + 1 ≤ t ≤ T (k + 1) + 1:
x t|t,Θ k+1 = Mx t−1|t−1,Θ k+1 +LC ⋆ x t|t−1,Θ ⋆ −x t|t−1,Θ +Lz t +LC ⋆ x t −x t|t−1,Θ ⋆ ,(43)
where M def = (Ã −BK −L(CÃ −CBK − C ⋆Ã + C ⋆BK )). The main idea to prove induction stage 2 is to show thatx t|t,Θ k+1 will also be bounded by (1 + 1/K)D 0 as long asx t−1|t−1,Θ k+1 is bounded by (1 + 1/K)D 0 , given the conclusionx S(k+1)|S(k+1),Θ k+1 ≤ (1 + 1/K)D 0 of the induction stage 1. To achieve this end, we divide the induction stage 2 into 2 phases. We first show that x t|t−1,Θ ⋆ −x t|t−1,Θ is small in the first phase, and prove the above main idea formally in the second phase.</p>
<p>Before coming to the main proof for induction stage 2, we first observe thatē t def = z t +C ⋆ x t −x t|t−1,Θ ⋆ is a (N S N Σ + 1)-subGaussian noise, so with probability at least 1 − δ/4 we know ē t 2 is bounded by C 2 (N S N Σ + 1) n log(H) for any 1 ≤ t ≤ H and some constant C 2 . Assume this high probability event happens for now.</p>
<p>Phase 1: boundx t|t−1,Θ ⋆ −x t|t−1,Θ We mainly follow the decomposition and induction techniques in Lale et al. (2020c) to finish phase 1. However, we note that our analysis here is much more complicated than the analysis in Lale et al. (2020c), because they have only one commit episode after the pure exploration stage (the agent chooses random actions in their pure exploration stage, just the same as the model selection procedure in LQG-VTR) but we have multiple episodes here.
Define ∆ t def =x t+S(k+1)|t+S(k+1)−1,Θ ⋆ −x t+S(k+1)|t+S(k+1)−1,Θ k+1 for 1 ≤ t ≤ T (k+1)−S(k+1)+1. For t = 1, we have ∆ 1 =x S(k+1)+1|S(k+1),Θ ⋆ −x S(k+1)+1|S(k+1),Θ k+1 = A ⋆x S(k+1)|S(k+1),Θ ⋆ + B ⋆ u S(k+1) −Ãx S(k+1)|S(k+1),Θ k+1 −Bu S(k+1) .(44)
Since we have assumed Θ ⋆ ∈ C, we know x S(k+1)|S(k+1),Θ ⋆ −x S(k+1)|S(k+1),Θ k+1 2 will be small according to induction stage 1. Specifically, we can set
T ′ A , T ′ B ≥ O((N L N S κ 3 ) 2 /(1 − γ 1 ) 2 ) so that as long as T w ≥ T ′ A , T ′ B , we have ∆ 1 2 ≤ D 0 (1 − γ 1 )/(8N L N S κ 3 )
for a constant κ 3 defined later. Now that we have come up with an upper bound on ∆ 1 2 , we hope to bound each ∆ t 2 and prove Eqn. (47).</p>
<p>For 1 &lt; t ≤ T (k + 1) − S(k + 1) + 1, we follow Eqn. (49) 
∆ t = G t−1 0 ∆ 1 + G 1 t−1 j=1 G t−1−j 0 G j−1 2xS(k+1)+1|S(k+1),Θ ⋆ + j−1 s=1 G j−s−1 2 G 3 ∆ s +ẽ t ,(45)
whereẽ t is a noise term, whose l 2 -norm is bounded by O( p log(H)) with probability 1 − δ/4 for any episode k and step t. We again assume this high probability event happens for now. We define ΛΘ def =Ã − A ⋆ −BK + B ⋆K and thus
G 0 = (A ⋆ + ΛΘ) (I −LC), G 1 = (A ⋆ + ΛΘ)L(C − C ⋆ ) − ΛΘ, G 2 = A ⋆ − B ⋆K + B ⋆KL (C − C ⋆ ), G 3 = (A ⋆ − B ⋆K )L ⋆ + B ⋆K (L ⋆ −L).(46)
Using a similar argument as in Lale et al. (2020c), we define T g0 , T g2 so that G 0 , G 2 are (κ 3 , γ 3 )strongly stable for some γ 3 ≤ min(γ 2 /2, (1 − γ 1 )/2) and κ 3 ≥ 1 as long as T w ≥ max(T g0 , T g2 ). This is possible because as long asΘ is close enough to Θ, G 0 will be strongly stable according to Assumption 3 while G 2 will be contractible in that G 2 2 ≤ (1 + γ 1 )/2 according to Assumption 2.</p>
<p>To be more concrete, we show how to construct T g2 in the following, and the construction of T g0 are analogous to that of T g2 . Observe that
G 2 − Ã −BK = A ⋆ −Ã + (B − B ⋆ )K + B ⋆KL (C − C ⋆ ).
By a similar argument used in Eqn. (32) and Eqn. (33), we know
A ⋆ −Ã + (B − B ⋆ )K + B ⋆KL (C − C ⋆ ) 2 ≤ 1 − γ 1 2 , as long as T w ≥ T g2 def = 36C w N 2 S N 2 K N 2 L /(1 − γ 1 ) 2 . This further implies G 2 2 ≤ Ã −BK 2 + 1 − γ 1 2 ≤ 1 + γ 1 2 ,
where the second inequality is by Assumption 3. Now we prove that
∆ t 2 ≤ 1 − γ 1 4N L N S D 0 (47) holds for all 1 ≤ t ≤ T (k + 1) − S(k + 1) + 1. First of all, we know ∆ 1 2 ≤ D 0 (1 − γ 1 )/(8N L N S κ 3 ) ≤ (1 − γ 1 )D 0 /(4N L N S ) according to Eqn. (44) and κ 3 ≥ 1. For any fixed t, suppose ∆ s 2 ≤ (1 − γ 1 )D 0 /(4N L N S ) for all 1 ≤ s ≤ t − 1, then the decomposition equation (45) implies ∆ t 2 = G t−1 0 ∆ 1 + G 1 t−1 j=1 G t−1−j 0 G j−1 2xS(k+1)+1|S(k+1),Θ ⋆ + j−1 s=1 G j−s−1 2 G 3 ∆ s +ẽ t 2 ≤ G 1 t−1 j=1 G t−1−j 0 G j−1 2xS(k+1)+1|S(k+1),Θ ⋆ + j−1 s=1 G j−s−1 2 G 3 ∆ s 2 + κ 3 (1 − γ 3 ) t−1 ∆ 1 2 + ẽ t 2 ≤ G 1 2 · (t − 1)κ 2 3 (1 − γ 3 ) t−1 · 2N S (1 + N K )D 0 + G 1 t−1 j=1 G t−1−j 0 j−1 s=1 G j−s−1 2 G 3 ∆ s 2 + κ 3 (1 − γ 3 ) t−1 ∆ 1 2 + ẽ t 2 ≤ G 1 2 · (t − 1)κ 2 3 (1 − γ 3 ) t−1 · 2N S (1 + N K )D 0 + κ 3 γ 3 2 G 1 G 3 2 · 1 − γ 1 4N L N S D 0 + κ 3 (1 − γ 3 ) t−1 ∆ 1 2 + ẽ t 2 .
The first inequality follows from the strong stability of G 0 and the concentration of noisesẽ t . The second inequality is due to the strong stability of G 0 , G 2 , and x S(k+1)+1|S(k+1),Θ ⋆ 2 ≤ (1 + N K )(N S + N S /K)D 0 ≤ 2N S (1 + N K )D 0 according to induction stage 1. The last inequality is because the assumption that ∆ s 2 ≤ (1 − γ 1 )D 0 /(4N L N S ) for all 1 ≤ s ≤ t − 1. When T w ≥ T g1 for some T g1 ≥ 1 we know that
G 1 2 ≤ N S + √ C w (1 + N K ) T g1 N L √ C w T g1 + √ C w (1 + N K ) T g1 ≤ √ C w N L N S + ( √ C w N L + 1)(1 + N K ) T g1 .
Since max t≥0 (t − 1)(1 − γ 3 ) t−1 and G 3 2 are both (instance-dependent) constants, there exists an instance-dependent constant T g1 that for T w ≥ T g1 , it holds that
G 1 2 · (t − 1)κ 2 3 (1 − γ 3 ) t−1 · 2N S (1 + N K )D 0 ≤ 1 − γ 1 24N L N S D 0 . (48) G 1 2 · κ 3 γ 3 2 G 3 2 · 1 − γ 1 4N L N S D 0 ≤ 1 − γ 1 24N L N S D 0 .(49)
Moreover, the bound on ∆ 1 2 from Eqn. (44) implies that
κ 3 (1 − γ 3 ) t−1 ∆ 1 2 ≤ 1 − γ 1 8N L N S D 0 .(50)
Note that we can set C 0 so that D 0 is large enough to guarantee
(1 − γ 1 )D 0 24N L N S ≥ ẽ t 2 = O( p log(H))(51)
for any episode k and step t in it. The proof of Eqn. (47) can be obtained by combining Eqn. (48), (49), (50), and (51). As a final remark, T g0 , T g1 , T g2 has no dependency on H.</p>
<p>Phase 2: finish induction stage 2 Now we come back to the main decomposition formula (43). Define T M = O(2 2 /(1 − γ 1 ) 2 ) so that C −C 2 is small enough to guarantee M 2 ≤ (1 + γ 1 )/2 (see the definition of M at the beginning of induction stage 2) as long as T w ≥ T M . Suppose we know x t−1|t−1,Θ k+1 2 ≤ (1 + 1/K)D 0 for time step t − 1, then as long as
D 0 ≥ 4C 2 N L (N S N Σ + 1) n log(H)/(1 − γ 1 ), we have x t|t,Θ k+1 2 ≤ M 2 1 + 1 K D 0 +LC ⋆ ∆ t−S(k+1) 2 +Lē t (52) ≤ 1 + γ 1 2 1 + 1 K D 0 + 1 − γ 1 4 D 0 + 1 − γ 1 4 D 0 (53) ≤ 1 + 1 K D 0 .(54)
Fortunately, we know that x S(k+1)|S(k+1),Θ k+1 2 ≤ (1 + 1/K)D 0 according to induction stage 1. Using the recursion argument above, we can show that x t|t,Θ k+1 2 ≤ (1+1/K)D 0 holds for S(k+1) ≤ t ≤ T (k + 1) + 1. We have proved the induction from episode k to k + 1 so far under the condition that LQG-VTR does not terminate at Line 9 during episode k. On the other hand, if for some step S(k + 1) + 1 ≤ t 0 ≤ T (k + 1) the algorithm entered Line 9 (note that it cannot be t 0 = S(k + 1) because induction stage 1 does not depend on whether the algorithm terminates), then the decomposition forx t|t,Θ k+1 (43) and decomposition for ∆ t (45) still works for steps t &lt; t 0 . Therefore, there must be some high probability event fails at step t 0 − 1 (i.e.,ē t0−1 and/orẽ t0−1 explode). This is impossible since we have assumed that all the high probability events happen.</p>
<p>Thus under the condition that all the high probability events happen, it suffices to choose the constant C 0 = O(( √ n + √ p) log H) so that Eqn. (30), (51), (52) all hold, and LQG-VTR does not terminate at Line 9 due to the definition ofX 1 = eC 0 . Since these high probability events happen with probability 1 − δ and recall that the total number of episodes is bounded byK, we know that with probability at least 1 − δ,
x t|t,Θt 2 ≤ C 0 1 + 1 K K ≤ eC 0 =X 1 = O(( √ n + √ p) log H).(55)
As a result, we know u t 2 ≤Ū def = N KX1 because u t = −K(Θ t )x t|t,Θt . Moreover,
y t = C ⋆ x t + z t = C ⋆x t|t−1,Θt−1 + C ⋆ x t −x t|t−1,Θt−1 + z t = C ⋆ Ã t−1x t−1|t−1,Θt−1 +B t−1 u t−1 + C ⋆ x t −x t|t−1,Θt−1 + z t = C ⋆ Ã t−1x t−1|t−1,Θt−1 +B t−1 u t−1 + C ⋆ x t −x t|t−1,Θ ⋆ +x t|t−1,Θ ⋆ −x t|t−1,Θt−1 + z t .
Observe that u t−1 2 ≤Ū , the l 2 -norm of noise term x t −x t|t−1,Θ ⋆ is bounded by C 2 (N S N Σ + 1) n log(H), and the last termx t|t−1,Θ ⋆ −x t|t−1,Θt−1 can be bounded through Eqn. (47), we have
y t 2 ≤Ȳ def = N 2 S (X 1 +Ū ) + C 2 (N S N Σ + 1) n log(H) + (1 − γ 1 )X 1 4N L .
Note thatx 0|0,Θ = L(Θ)y 0 , by Eqn. (31)
x t|t,Θ 2 ≤ κ 2 (1 − γ 2 ) t N LȲ + t s=1 κ 2 (1 − γ 2 ) s−1 (1 + N L N S )N SŪ + N LȲ ≤X 2 def = κ 2 (1 + N L N S )N SŪ + κ 2 (1 + γ 2 )N LȲ γ 2 .
At the end of this section, we set the value of T w so that the confidence set C is accurate enough to stabilize the LQG system:
T w def = max(T A , T B , T C , T L , T ′ A , T ′ B , T g0 , T g1 , T g2 , T M ) =Õ(K 2 ).(56)</p>
<p>D. Proof of Lemma 3</p>
<p>Proof of Lemma 3. The key observation to prove the lemma is that for any Θ ∈ E , the difference of H-step optimal cost between the infinite-horizon setting of Θ and finite-horizon setting of Θ is bounded:
|V ⋆ (Θ) − (H + 1)J ⋆ (Θ)| ≤ D h ,
where D h is independent of H. We now prove this statement. We first recall the optimal control of the finite horizon LQG problem. The cost for any policy π in a H-step finite horizon LQG problem is defined as
E π H h=0 y ⊤ h Qy h + u ⊤ h Ru h ,
where the initial state x 0 ∼ N (0, I).</p>
<p>The optimal control depends on the belief state E[x h |H h ], where H h is the history observations and actions up to time step h. It is well known that x h is a Gaussian variable, whose mean and covariance can be estimated by the Kalman filter.</p>
<p>Define the initial covariance estimate Σ 0|−1 def = I, then Kalman filter gives
Σ h+1|h = AΣ h|h−1 A ⊤ − AΣ h|h−1 C ⊤ (CΣ h|h−1 C ⊤ + I) −1 CΣ h|h−1 A ⊤ + I,(57)
and
Σ h|h = (I − L h C)Σ h|h−1 , where L h = Σ h|h−1 C ⊤ CΣ h|h−1 C ⊤ + I −1 .
The optimal control is linear in the belief state E[x h |H h ], with an optimal control matrix K h . This control matrix in turn depends on a Riccati difference equation as follows. Define P H def = C ⊤ QC, the Riccati difference equation for P h takes the form:
P h = A ⊤ P h+1 A + C ⊤ QC − A ⊤ P h+1 B(R + B ⊤ P h+1 B) −1 B ⊤ P h+1 A.(58)
The optimal control matrix is
K h = (R + B ⊤ P h B) −1 B ⊤ P h A.
Then the optimal cost of the finite horizon LQG problem V ⋆ (Θ) equals
V ⋆ (Θ) = H h=0 trace P h (Σ h|h−1 − Σ h|h ) + H h=0 trace C ⊤ QCΣ h|h .(59)
For the infinite-horizon view of the same LQG instance Θ, we denote the stable solution to two DAREs in (57) and (58) by Σ and P (i.e., we use Σ, P and L to represent Σ(Θ), P (Θ) and L(Θ)), then the optimal cost J ⋆ (Θ) equals
J ⋆ (Θ) = trace (P LCΣ) + trace C ⊤ QC(I − LC)Σ .(60)
Now let us check the difference between (H + 1)J ⋆ (Θ) and V ⋆ (Θ). The difference can be bounded as
V ⋆ (Θ) − (H + 1)J ⋆ (Θ) = H h=0 trace P h L h CΣ h|h−1 − trace (P LCΣ)(61)+ H h=0 trace C ⊤ QC(I − L h C)Σ h|h−1 − trace C ⊤ QC(I − LC)Σ(62)
Positive definite matrix Q can be decomposed as Q = Γ ⊤ Γ, where Γ ∈ R p×p is also positive definite. Therefore, for matrix C ⊤ QC = (ΓC) ⊤ ΓC, we can show that (A, ΓC) is also observable owing to (A, C) observable and Γ positive definite. Since (A, ΓC) is observable, and (A, B) is controllable, we know that the DARE for P has a unique positive semidefinite solution, and the Riccati difference sequence converges exponentially fast to that unique solution (see e.g., Chan et al. (1984, Theorem 4.2), Caines and Mayne (1970, Theorem 2.2), and the references therein). That is, there exists a uniform constant a 0 such that
P − P h 2 ≤ a 0 γ H−h 1 ,(63)
since A − BK 2 ≤ γ 1 for any Θ ∈ E . Similarly, Σ h|h−1 also converges to Σ exponentially fast in that there exists a (instance-dependent) constant b 0 (Chan et al., 1984;Lale et al., 2021) 
such that Σ − Σ h|h−1 2 ≤ b 0 κ 2 (1 − γ 2 ) h ,(64)
which further implies that L h also converges to L exponentially fast for some constant c 0 : To show the existence of b 0 , observe that
L − L h 2 ≤ c 0 κ 2 (1 − γ 2 ) h ,(65)Σ h|h−1 − Σ = (A − ALC) h (Σ 0|−1 − Σ)φ h ,(66)where φ h def = (A − AL 0 C) ⊤ (A − AL 1 C) ⊤ · · · (A − AL h−1 C) ⊤ .
In the proof of Theorem 4.2 of Chan et al. (1984) we have
Σ h|h−1 = φ ⊤ h Σ 0|−1 φ h + non-negative constant matrices.(67)
Theorem 4.1 of Caines and Mayne (1970) shows that for any Θ ∈ E and any 0 ≤ h ≤ H it holds that
Σ h|h−1 2 ≤ κ 2 2 (1 + κ 2 2 ) 2γ 2 − γ 2 2 = N Σ ,(68)
which helps us determine b 0 = (1 + N Σ ) √ N Σ . The existence of a 0 also relies on the state transition matrix (Zhang et al., 2021) 
φ ′ h = (A − BK H )(A − BK H−1 ) · · · (A − BK h+1 )
. As a minimum requirement to tackle the finite horizon LQR control problem, we shall assume φ ′ h 2 is uniformly upper bounded (i.e., the system Θ is stable) for any Θ (in fact, people often assume a stronger condition that it is exponentially stable). According to Assumption 2, we can now identify the existence of a 0 .</p>
<p>As a result, we have
|V ⋆ (Θ ⋆ ) − (H + 1)J ⋆ (Θ ⋆ )| = H h=0 trace P h L h CΣ h|h−1 − trace (P LCΣ)(69)+ H h=0 trace C ⊤ QC(I − L h C)Σ h|h−1 − trace C ⊤ QC(I − LC)Σ (70) ≤ H h=0 trace (P h − P )L h CΣ h|h−1 + H h=0 trace P L h CΣ h|h−1 − LCΣ(71)+ H h=0 trace C ⊤ QC Σ h|h−1 − Σ + LCΣ − L h CΣ h|h−1 (72) ≤ H h=0 O na 0 γ H−h 1 + nκ 2 (b 0 + c 0 )(1 − γ 2 ) h (73) ≤ D h def = O na 0 1 − γ 1 + nκ 2 (b 0 + c 0 ) γ 2 ,(74)
where the second inequality uses trace (X) ≤ n X 2 for any X ∈ R n×n . Now for any (history-dependent) policy π and any Θ ∈ E ,
V π (Θ) − V ⋆ (Θ) ≤ V π (Θ) − (H + 1)J ⋆ (Θ) Regret(π;H) +D h .
Combined with the definition of π RT in Eqn. (7), we have
Gap(π RT ) = V πRT (Θ ⋆ ) − V ⋆ (Θ ⋆ ) ≤ max Θ∈E (V π (Θ) − V ⋆ (Θ)) ≤ Regret(π; H) + D h ,(75)
where the first inequality is by the minimax property of π RT and Θ ⋆ ∈ E .</p>
<p>E. Proof of Theorem 4</p>
<p>For all the conclusions in this section, the bounded state property of LQG-VTR (Lemma 5) is a crucial condition. Let V be the event that for all 1 ≤ t ≤ H, x t|t,Θt 2 ≤X 1 (so that LQG-VTR does not terminate at Line 9), then by Lemma 5 we know P(V) ≥ 1 − δ. We provide the full proof of Theorem 4 in this section, but defer the proof of technical lemmas to Appendix F.
E.1 Bounded ℓ ∞ norm of F
To begin with, we show that F ∞ is well bounded under event V.</p>
<p>Lemma 6 (Bounded norm of F ). Suppose event V happens, then each sample
E t = (τ t ,x t|t,Θ k ,Θ k , u t ,x t+1|t+1,Θ k , y t+1 ) (Line 13 of Algorithm 1) satisfies for any 1 ≤ t ≤ H, (τ t ,x t|t,Θ k , u t ,Θ k ) ∈ X (recall that X is the domain of f defined in Section 4.4).
There exists a constant D such that for any Θ ∈ E ,
f Θ ∞ def = max E∈X f Θ (E) ≤ D = O((n + p) log 2 H), h ⋆ Θ ∞ ≤ D.</p>
<p>E.2 The Optimistic Confidence Set</p>
<p>Now that F ∞ is bounded, we can show the real-world model Θ ⋆ is contained in the confidence set with high probability. Note that this lemma requires a more complicated analysis since we are using a biased value target regression (Ayoub et al., 2020).</p>
<p>Lemma 7 (Optimism). It holds that with probability at least 1 − 2δ,
Θ ⋆ ∈ U k
for any episode k ∈ [K].</p>
<p>E.3 The Clipping Error</p>
<p>We also analyze the clipping error introduced by replacing f ′ with f (see Section 4.4) in the LQG-VTR.</p>
<p>Lemma 8. Suppose event V happens, there exists constant ∆ such that for any system Θ = (A, B, C) ∈ E and for any input (τ l , x, u,Θ) ∈ X at time step t,
∆ f τ t , x, u,Θ def = f ′ τ t , x, u,Θ − f τ l , x, u,Θ ≤ ∆ = O κ 2 (1 − γ 2 ) l (n + p) log 2 H ,(76)
where τ t = {y 0 , u 0 , y 1 , ..., y t } is the full history, and τ l = {u t−l , y t−l+1 , · · · , u t−1 , y t } is the clipped history.</p>
<p>E.4 Low Switching Property</p>
<p>At last, we observe that the episode switching protocol based on the importance score (Line 14 of Algorithm 1) ensures that the total number of episodes will be only O(log H).</p>
<p>Lemma 9 (Low Switching Cost). Suppose event V happens and setting ψ = 4D 2 + 1, the total number of episodes K of LQG-VTR(Algorithm 1) is bounded by
K &lt;K,
whereK is defined asK
def = C K dim E (F , 1/H) log 2 (DH)(77)
for some constant C K .</p>
<p>Proof. This lemma is implied by Lemma 6 and the proof of Chen et al. (2021, Lemma 9).</p>
<p>This lemma also implies that the number of episode will NOT reachK since K &lt;K but not K ≤K. This property is important as it guarantees that LQG-VTR switches to a new episode immediately as long as the importance score gets greater than 1.</p>
<p>E.5 Proof of Theorem 4</p>
<p>We have prepared all the crucial lemmas for the proof of the main theorem till now. Putting everything together, we provide the full proof for the Theorem 4 below.</p>
<p>Proof of Theorem 4. Define
Regret(H) def = H t=Tw+1 (y ⊤ t Qy t + u ⊤ t Ru t − J(Θ ⋆ ))
, then Regret(H) = E[ Regret(H)]+ O(T w ). We assume event V happens and the optimistic property of U k (Lemma 7) holds for now, leaving the failure of these two events at the end of the proof. As a consequence of V, the algorithm will not terminate at Line 9. As we know the regret incurred in the model selection stage is O(T w ) = O(K 2 ) is only logarithmic in H, it cannot be the dominating term of the regret. Denote the full history at time step t as τ t = {y 0 , u 0 , y 1 , ..., u t−1 , y t }, we decompose the regret as
Regret(H) = H t=Tw+1 y ⊤ t Qy t + u ⊤ t Ru t − J ⋆ (Θ ⋆ ) (78) ≤ H t=Tw +1 y ⊤ t Qy t + u ⊤ t Ru t − J ⋆ (Θ k ) (by Lemma 7) (79) = K k=1 T (k) t=S(k) E wt,zt+1 x ut⊤ t+1|t+1,Θ k P −C ⊤ QC x ut t+1|t+1,Θ k + y ut⊤ t+1,Θ k Qy ut t+1,Θ k (80) −x ⊤ t|t,Θ k P −C ⊤ QC x t|t,Θ k − y ⊤ t Qy t (by the Bellman equation of systemΘ k ) (81) = K k=1 T (k) t=S(k) E wt,zt+1 x ut⊤ t+1|t+1,Θ k P −C ⊤ QC x ut t+1|t+1,Θ k + y ut⊤ t+1,Θ k Qy ut t+1,Θ k (82) − f ′ Θ ⋆ τ t ,x t|t,Θ k , u t ,Θ k (83) + f ′ Θ ⋆ τ t ,x t|t,Θ k , u t ,Θ k −x ⊤ t|t,Θ k P −C ⊤ QC x t|t,Θ k − y ⊤ t Qy t .(84)
Here we denoteΘ k = (Ã,B,C), andP = P (Θ k ). For any k ∈ [K], the last term (84) of the regret decomposition can be bounded as
T (k) t=S(k) f ′ Θ ⋆ τ t ,x t|t,Θ k , u t ,Θ k −x ⊤ t|t,Θ k P −C ⊤ QC x t|t,Θ k − y ⊤ t Qy t (85) = T (k)−1 t=S(k) f ′ Θ ⋆ τ t ,x t|t,Θ k , u t ,Θ k −x ⊤ t+1|t+1,Θ k P −C ⊤ QC x t+1|t+1,Θ k − y ⊤ t+1 Qy t+1 (86) + f ′ Θ ⋆ τ T (k) ,x T (k)|T (k),Θ k , u T (k) ,Θ k −x ⊤ S(k)|S(k),Θ k P −C ⊤ QC x S(k)|S(k),Θ k − y ⊤ S(k) Qy S(k) .(87)
Observe that Eqn. (86) is a martingale difference sequence, and Eqn. (87) is bounded by 2D by Lemma 6. Summing over k we have with probability 1 − δ,
K k=1 T (k) t=S(k) f ′ Θ ⋆ τ t ,x t|t,Θ k , u t ,Θ k −x ⊤ t|t,Θ k P −C ⊤ QC x t|t,Θ k − y ⊤ t Qy t (88) ≤ D H log(1/δ) + 2DK.(89)
Now we focus on Eqn. (82) and (83). Observe that Eqn. (82) can be written as
E wt,zt+1 x ut⊤ t+1|t+1,Θ k P −C ⊤ QC x ut t+1|t+1,Θ k + y ut⊤ t+1,Θ k Qy ut t+1,Θ k (90) = f ′ Θ k τ t ,x t|t,Θ k , u t ,Θ k .(91)
Therefore, Eqn. (82) and (83) becomes
K k=1 T (k) t=S(k) f ′ Θ k τ t ,x t|t,Θ k , u t ,Θ k − f ′ Θ ⋆ τ t ,x t|t,Θ k , u t ,Θ k .(92)
Consider any episode 1 ≤ k ≤ K, define Z k to be the dataset used for the regression at the end of episode k (Z 0 = ∅). The construction of confidence set U k (Eqn. (10)) shows that
fΘ k − f Θ ⋆ 2 Z k−1 ≤ 2β.(93)
By the definition of importance score (Line 14 of Algorithm 1), we know for any S(k) ≤ t ≤ T (k), 
t s=S(k) fΘ k τ l s ,x s|s,Θ k , u s ,Θ k − f Θ ⋆ τ l s ,x s|s,Θ k , u s ,Θ k 2 ≤ 2β + ψ + 4D 2 ,(94)s=S(o) fΘ k τ l s ,x s|s,Θo , u s ,Θ o − f Θ ⋆ τ l s ,x s|s,Θo , u s ,Θ o 2 ≤ 4β + ψ + 4D 2 .(95)
Invoking Jin et al. (2021, Lemma 26) 
with G = F − F , g t = fΘ k − f Θ ⋆ , ω = 1/H, and µ s (·) = 1[· = (τ l s ,x s|s,Θo , u s ,Θ o )], we have K k=1 T (k) t=S(k) fΘ k τ l t ,x t|t,Θ k , u t ,Θ k − f Θ ⋆ τ l t ,x t|t,Θ k , u t ,Θ k (96) ≤ O dim E (F , 1/H) βH + D min(H, dim E (F , 1/H)) .(97)
The clipping error between f ′ and f can be bounded by Lemma 8:
K k=1 T (k) t=S(k) f ′ Θ k τ t ,x t|t,Θ k , u t ,Θ k − f ′ Θ ⋆ τ t ,x t|t,Θ k , u t ,Θ k (98) ≤ K k=1 T (k) t=S(k) fΘ k τ l t ,x t|t,Θ k , u t ,Θ k − f Θ ⋆ τ l t ,x t|t,Θ k , u t ,Θ k(99)+ K k=1 T (k) t=S(k) ∆ fΘ k τ t ,x t|t,Θ k , u t ,Θ k + ∆ f Θ ⋆ τ t ,x t|t,Θ k , u t ,Θ k (100) ≤ O dim E (F , 1/H) βH + κ 2 (1 − γ 2 ) l (n + p)H log 2 H .(101)
Therefore, we choose l = O(log(Hn + Hp)) to obtain 
K k=1 T (k) t=S(k) f ′ Θ k τ t ,x t|t,Θ k , u t ,Θ k − f ′ Θ ⋆ τ t ,x t|t,Θ k , u t ,Θ k ≤ O dim E (F , 1/H) βH ,(102)Regret(H) ≤ O dim E (F , 1/H) βH + D H log(1/δ) + 2DK(103)
as long as V happens, the optimistic property Θ ⋆ ∈ U k for any episode k, and the martingale difference (86) converges. The probability that any of these three events fail is 4δ, in which case the Regret(H) can be very large. The tail bound of the Gaussian variables (Abbasi-Yadkori and Szepesvári, 2011) indicates that for any q &gt; 0 and t &gt; 0,
P ( w t 2 ≥ q) ≤ 2n exp − q 2 2n , P ( v t 2 ≥ q) ≤ 2p exp − q 2 2p .
Therefore, a union bound implies
P (∃t, max( w t 2 , v t 2 ) ≥ q) ≤ 2Hn exp − q 2 2n + 2Hp exp − q 2 2p .
Note that as long as ∀0 ≤ t ≤ H, w t 2 , v t 2 ≤ q, we know that
Regret(H) ≤ C R (q 2 +Ū 2 ),
since u t 2 ≤Ū always hold for some instance dependent constant C R thanks to the terminating condition (Line 9 of LQG-VTR). Thus we know
P Regret(H) &gt; C R (q 2 +Ū 2 ) ≤ 2Hn exp − q 2 2n + 2Hp exp − q 2 2p .
We set q = Hnp and δ = C −1 R (q 2 +Ū 2 ) −1 so that
Regret(H) = E Regret(H) + O(T w )(104)≤ O dim E (F , 1/H) βH + 4C R δ(q 2 +Ū 2 ) + +∞ CR(q 2 +Ū 2 ) P Regret(H) &gt; x dx (105) ≤ O dim E (F , 1/H) βH .(106)
The integral above is a constant because P( Regret(H) &gt; x) is a exponentially small term. The theorem is finally proved since β =Õ(D 2 log N (F , 1/H)) and F ∞ ≤ D =Õ(n + p).</p>
<p>F. Proof of Technical Lemmas in Appendix E</p>
<p>F.1 Proof of Lemma 6</p>
<p>Proof of Lemma 6. The first part of the lemma is implied by the definition of V. By definition of X , we know that for any (τ l , x, u,Θ) ∈ X and Θ ∈ E , By the definition of optimal bias function in (4), we have
f Θ (τ l , x, u,Θ) = E et x ⊤ (P −C ⊤ QC)x + y c⊤ t+1,Θ Qy c t+1,Θ (by Eqn. (16) (107) = E et I −LC Ã x +Bu +L CAx c t|t,Θ + CBu + e t ⊤ (P −C ⊤ QC)× (108) I −LC Ã x +Bu +L CAx c t|t,Θ + CBu + e t(109)+ E et CAx c t|t,Θ + CBu + e t ⊤ Q CAx c t|t,Θ + CBu + e t(110)= L CAx c t|t,Θ +LCBu + I −LC Ã x +Bu ⊤ (P −C ⊤ QC)× (111) L CAx c t|t,Θ +LCBu + I −LC Ã x +Bu (112) + CAx c t|t,Θ + CBu ⊤ Q CAx c t|t,Θ + CBu(113)+ trace L ⊤ (P −C ⊤ QC)L CΣC ⊤ + I + trace Q CΣC ⊤ + I ,(114)h ⋆ Θ x t|t,Θ , y t = x ⊤ t|t,Θ (P (Θ) − C ⊤ QC)x t|t,Θ + y ⊤ t Qy t (115) ≤ D 2 def = (N P + N 2 S N U )X 2 2 + N UȲ 2 .(116)
It suffices to choose D def = max(D 1 , D 2 ) = O((n + p) log 2 H).</p>
<p>F.2 Proof of Lemma 7</p>
<p>Proof of Lemma 7. As a starting point, Θ ⋆ ∈ U 1 holds with probability 1 − δ/4 due to the warm up procedure (see Appendix C). We assume event V happens for now, and discuss the failure of V at the end of the proof. For episode 1 ≤ k ≤ K and step t in the episode, define X t
def = (τ t ,x t|t,Θ k , u t ,Θ k ), d t def = f ′ Θ ⋆ (X t ) − f Θ ⋆ (X t ), Y t def =x ⊤ t+1|t+1,Θ k (P −C ⊤ QC)x t+1|t+1,Θ k + y ⊤
t+1 Qy t+1 forP = P (Θ k ),C = C(Θ k ) (note that f Θ ⋆ only depends on the l-step nearest history τ l instead of the full history τ t ).</p>
<p>Let F t−1 be the filtration generated by (X 0 , Y 0 , X 1 , Y 1 , ..., X t−1 , Y t−1 , X t ), then we know that
f ′ Θ ⋆ (X t ) = E[Y t | F t−1 ] by definition. Define Z t def = Y t − f ′ Θ ⋆ (X t ), then Z t is a D/2-subGaussian
Here we take advantage of the D/2-subGaussian property of z t in that with probability 1 − 3δ/4 for any episode k,
Z Z ≤ D 2 2|Z| log (2|Z|(|Z| + 1)/δ) ≤ D 2 2H log (3H(H + 1)/δ).(118)
Merging Eqn. (117) and (118) with another union bound, we get that with probability 1 − δ for any episode k,
f * −f 2 Z ≤ 2D 2 log (2N α /δ) + 4∆DH + 2αH 2(D + ∆) + D 2 log (6H(H + 1)/δ) ,
where N α is the (α, · ∞ )− covering number of F . Finally, we set α = 1/H and β = 2D 2 log (2N (F , 1/H)) + 4∆DH + 4(D + ∆) + 4D 2 log(4H(H + 1)/δ).</p>
<p>It holds that Θ ⋆ ∈ U k for any 1 ≤ k ≤ K with probability 1 − δ/4 − 3δ/4 = 1 − δ as long as V happens. Since V happens with probability 1 − δ, we know the optimistic property holds with probability 1 − 2δ.</p>
<p>F.3 Proof of Lemma 8</p>
<p>Proof of Lemma 8. Define e c def = (A − LCA) lx t−l|t−l,Θ , thenx t|t,Θ =x c t|t,Θ + e c . Further, we know e c 2 ≤ κ 2 (1 − γ 2 ) lX 2 under event V and Assumption 3. By the decomposition rule for f and f ′ (Eqn. (111) - (114)), we know
f ′ Θ (τ t , x, u,Θ) − f Θ (τ l , x, u,Θ) (120) = 2e c⊤ A ⊤ C ⊤L⊤ (P −C ⊤ QC) L CAx c t|t,Θ +LCBu + I −LC Ã x +Bu (121) + e c⊤ A ⊤ C ⊤L⊤ (P −C ⊤ QC)LCAe c(122)+ 2e c⊤ A ⊤ C ⊤ Q CAx c t|t,Θ + CBu + e c⊤ A ⊤ C ⊤ QCAe c .(123)
Note thatX 2 = O(( √ n + √ p) log H), thereby there exists a constant ∆ such that
f ′ Θ (τ t , x, u,Θ) − f Θ (τ l , x, u,Θ) ≤ ∆ = O κ 2 (1 − γ 2 ) l (n + p) log 2 H .(124)</p>
<p>G. The Intrinsic Model Complexity δ E</p>
<p>In this section, we show how large the intrinsic model complexity δ E will be for different simulator classes E . An important message is that δ E does NOT have any polynomial dependency on H.</p>
<p>G.1 General Simulator Class</p>
<p>Without further structures, we can show that δ E is bounded by
δ E =Õ np 2 (n + m + p)(n + p) 2 (m + p) 2(125)
due to Proposition 2, 3 and the fact that F ∞ ≤ D =Õ(n + p).</p>
<p>Proposition 2. Under Assumption 1, 2, 3, the 1/H-Eluder dimension of F is bounded by
dim E (F , 1/H) =Õ p 4 + p 3 m + p 2 m 2 .(126)
Proof. The bound on Eluder dimension mainly comes from the fact that f Θ (τ l , x, u,Θ) can be regarded a linear function between features of Θ and features ofΘ. We use the "feature" of a system Θ to represent a quantity that only depends on Θ (independent fromΘ) and vice versa. It is well-known that the linear function class has bounded Eluder dimension. We formalize this idea below.</p>
<p>In the proof of Lemma 8, we know that for any (τ l , x, u,Θ) ∈ X and Θ ∈ E ,
f Θ (τ l , x, u,Θ) = L CAx c t|t,Θ +LCBu + I −LC Ã x +Bu ⊤ (P −C ⊤ QC)×(127)
L CAx c t|t,Θ +LCBu + I −LC Ã x +Bu (128)
+ CAx c t|t,Θ + CBu ⊤ Q CAx c t|t,Θ + CBu(129)+ trace L ⊤ (P −C ⊤ QC)L CΣC ⊤ + I + trace Q CΣC ⊤ + I ,(130)
Denote the input as E def = (τ l , x, u,Θ) ∈ X , and define three feature mappings ζ : X → R n×n , φ : X → R n and Φ : X → R p×p such that
ζ(E) =P −C ⊤ QC,(131)φ(E) = I −LC Ã x +Bu ,(132)Φ(E) =L ⊤ ζ(E)L + Q.(133)
Then we can move on to further write f as 
f Θ (E) = L CAx c t|t,Θ +LCBu + φ(E) ⊤ ζ(E) L CAx c t|t,Θ +LCBu + φ(E)(134)
Define two series of feature mappings ϕ s u , ϕ s y for 1 ≤ s ≤ l on E to be ϕ s u : E → R p×m and ϕ s y :
E → R p×p such that ϕ s u (Θ) = CA (A − LCA) s−1 (I − LC)B ϕ s y (Θ) = CA (A − LCA) s−1 L(137)
Consider the terms in Eqn. (134) 
= l s=1 ϕ s u (Θ)u t−s + ϕ s y (Θ)y t−s+1 ⊤L ⊤ ζ(E)L l s=1 ϕ s u (Θ)u t−s + ϕ s y (Θ)y t−s+1 .(138)
Note that ϕ s u (Θ) and ϕ s y (Θ) only depends on Θ (so they can be regarded as a feature of Θ), where u t−s , y t−s+1 ,L ⊤ ζ(E)L only depends on E. Consider any 1 ≤ s 1 , s 2 ≤ l in the summation, we pick the term below as an example:
(ϕ s1 u (Θ)u t−s1 ) ⊤L⊤ ζ(E)Lϕ s2 u (Θ)u t−s2 (140) = ϕ s2 u (Θ) ⊗ ϕ s1 u (Θ), vec(L ⊤ ζ(E)L) × vec(u t−s1 u ⊤ t−s2 ) ⊤ .(141)
Here ⊗ denotes the Kronecker product between two matrices, vec(X) for X ∈ R a×b denotes the vectorization of matrix X (i.e., vec(X) = X 11 X 21 ... X a1 X 12 ... X ab ⊤ ∈ R ab ), and ·, · is the inner product. In this way, we decompose the term (140) as the inner product of a p 2 m 2 -dimensional feature of Θ (i.e., ϕ s2 u (Θ) ⊗ ϕ s1 u (Θ)) and a p 2 m 2 -dimensional feature of E (i.e., vec(L ⊤ ζ(E)L) × vec(u t−s1 u ⊤ t−s2 ) ⊤ ). We then observe that such decomposition is valid for any 1 ≤ s 1 , s 2 ≤ l. Therefore, we can decompose each term in Eqn. (134) and (135) as the inner product of features of Θ and E. Gathering all these features as the aggregated feature mapping fea 1 for Θ and fea 2 for E, we have
f Θ (E) = fea 1 (Θ), fea 2 (E) .(142)
It is not hard to observe that the dimension of aggregated feature mappings is bounded by dim(fea 1 (Θ)) = dim(fea 2 (E)) =Õ p 4 + p 3 m + p 2 m 2 (143) since l = O(log(Hn + Hp)). Note the fea 1 (Θ) 2 , fea 2 (E) 2 are both upper bounded by the domain of E ∈ X and Θ ∈ E , then Proposition 2 of Osband and Van Roy (2014) shows that dim E (F , 1/H) =Õ p 4 + p 3 m + p 2 m 2 .</p>
<p>Proposition 3. Under Assumptions 1, 2, and 3, the logarithmic 1/H-covering number of F is bounded by log N (F , 1/H) =Õ n 2 + nm + np .</p>
<p>Proof. Under Assumption 2, the spectral norm of the system dynamics of Θ = (A, B, C) ∈ E is bounded by A 2 , B 2 , C 2 ≤ N S . Therefore, we can construct an ǫ 0 -net G ǫ0 (E ) such that for any Θ = (A, B, C), there existsΘ = (Ā,B,C) ∈ E satisfying max( A −Ā 2 , B −B 2 , C −C 2 ) ≤ ǫ 0 . By classic theory, we know |G ǫ0 (E )| ≤ O((1 + 2 √ nN S /ǫ 0 ) n 2 +nm+np ). To see this, observe that · 2 ≤ · F ≤ min(n, m) · 2 for any n × m matrix, so we can reduce the ǫ-cover with respect to the l 2 -norm to the ǫ-cover with respect to the Frobenius norm. To show the covering number of F , we check the gap f Θ − fΘ ∞ . By Lemma 3.1 of Lale et al. (2020c), we know that for small enough ǫ 0 there exists instance dependent constants C Σ and C L such that
Σ − Σ 2 ≤ C Σ ǫ 0 , L − L 2 ≤ C L ǫ 0 ,(146)
whereΣ = Σ(Θ),L = L(Θ). The constant C Σ is slightly different from Lemma 3.1 of their paper, because we do not assume M def = A − ALC is contractible (i.e., M 2 &lt; 1). Rather we know M is (κ 2 , γ 2 )-strongly stable. Therefore, the linear mapping T def = X → X − M XM ⊤ is invertible with T −1 2 ≤ κ 2 /(2γ 2 − γ 2 2 ) according to Lemma B.4 of Simchowitz and Foster (2020). As a result, we can set C Σ by replacing the 1/(1 − ν 2 ) term in the original constant of Lale et al. (2020c, Lemma 3.1) by κ 2 /(2γ 2 − γ 2 2 ).</p>
<p>For any E = (τ l , x, u,Θ) ∈ X , it is desired to compute the difference |f Θ (E) − fΘ(E)|. As a first step, we show the difference betweenx c t|t,Θ andx c t|t,Θ . Note that for any 1 ≤ s ≤ l,
(A − LCA) s−1 − (Ā −LCĀ) s−2 2 ≤ (s − 1)κ 2 2 (1 − γ 2 ) s−1 (1 + N 2 S + 2N L N S )ǫ 0 .(147)
This is because
A s−1 −Ā s−1 = s−2 o=0 A s−1−oĀo − A s−2−oĀo+1 = s−2 o=0 A s−2−o A −Ā Ā o (148) for A def = A − LCA,Ā def =Ā −LCĀ, where A −Ā 2 ≤ (1 + N 2 S + 2N L N S )ǫ 0 , A 2 , Ā 2 ≤ κ 2 (1 − γ 2 ).
Similarly we can bound (B − LCB) − (B −LCB) 2 . Follow the decomposition rule in Eqn. (136) and observe that u t−s 2 ≤Ū , y t−s+1 2 ≤Ȳ for 1 ≤ s ≤ l, we have
x c t|t,Θ −x c t|t,Θ 2 ≤ C x ǫ 0 ,(149)
where C x is a instance dependent constant (also depends on C Σ , C L ). By Eqn. (134) and (135), it holds that
f Θ (E) − fΘ(E) = trace Φ(E) CΣC ⊤ −CΣC ⊤ + Diff 1 + Diff 2 ,(150)
where Diff 1 = CAx c t|t,Θ + CBu </p>
<p>Checking each term in Diff 1 and Diff 2 , we conclude that there exists instance dependent constant C d1 , C d2 such that |Diff 1 | ≤ C d1 ǫ 0 , |Diff 2 | ≤ C d2 ǫ 0 .</p>
<p>Note that CΣC ⊤ −CΣC ⊤ 2 ≤ (2N S N Σ + N 2 S )ǫ 0 and Φ(E) 2 ≤ N 2 L (N P + N 2 S N U ) + N U , we have
|f Θ (E) − fΘ(E)| = trace Φ(E) CΣC ⊤ −CΣC ⊤ + Diff 1 + Diff 2 ≤ C f ǫ 0 ,(155)
where
C f def = p Φ(E) 2 (2N S N Σ + N 2 S )ǫ 0 + C d1 + C d2 . This means f Θ − fΘ ∞ ≤ C f ǫ 0 .
Therefore, the subset {fΘ |Θ ∈ G ǫ0 (E )} forms a C f ǫ 0 -covering set of F . Finally, if suffices to set ǫ 0 = C −1 f H −1 to induce a 1/H-covering set of F :
N (F , 1/H) ≤ G C −1 f H −1 (E ) = O 1 + 2 √ nN S C f H n 2 +nm+np ,(156)
which finishes the proof.</p>
<p>G.2 Low-Rank Simulator Class</p>
<p>In many sim-to-real tasks, there are only a few control parameters that affect the dynamics in the simulator class E (see e.g., Table 1 of OpenAI et al. (2018)). The number of control parameters is often a constant that is independent of the dimension of the task. This means our simulator class E is a low-rank class in these scenarios: E = {Θ(t 1 , t 2 , ..., t k ) | (t 1 , t 2 , ..., t k ) ∈ T } , where t 1 , t 2 , ..., t k represent k control parameters, T is the domain of control parameters. The dynamics of simulator Θ only depends on these control parameters t 1 , t 2 , ..., t k . It is a common situation that Θ(t 1 , t 2 , ..., t k ) is a continuous function of (t 1 , ..., t k ). For example, it is a continuous function when the control parameters are physical parameters like friction coefficients, damping coefficients. A simple example is when Θ is a linear combination of k base simulators:
Θ = (A, B, C) = k i=1 t i Θ i ,(157)
where Θ i = (A i , B i , C i ) is a fixed base simulator. This can be achieved if we approximate the effect of control parameters with some linear mappings on the states x t . We can set different values of control parameters t 1 , ..., t k to generate new simulators.</p>
<p>In such a continuous low rank class, the log covering number N (F , 1/H) will reduce to onlyÕ(k) as long as Θ is continuous with respect to the control parameters. It is straightforward to see this by checking the proof of Proposition 3. Unfortunately, it is unclear whether the Eluder dimension of such a continuous low-rank class will be smaller due to the existence of the Kalman gain matrix L. Overall, the intrinsic model complexity δ E will decrease fromÕ(np 2 (n + m + p)(n + p) 2 (m + p) 2 ) to at mostÕ (kp 2 (m + p) 2 (n + p) 2 ) for a small constant k.</p>
<p>G.3 Low-Rank Simulator Class with Real Data</p>
<p>Many works studying sim-to-real transfer also fine-tune the simulated policy with a few real data on the real-world model (Tobin et al., 2017;Rusu et al., 2017). Moreover, the observations are usually the perturbed input states with observation noises introduced by vision inputs (e.g., from the cameras) (Tobin et al., 2017;Peng et al., 2018;OpenAI et al., 2018). This means the observation y t equals x t + z t for random noise z t , without any linear transformation C. With these real data and noisy observations of the state, we can estimate the steady-state covariance matrix Σ(Θ ⋆ ) with random actions since it is independent of the policy! The intrinsic complexity of the simulator set will be further reduced for this low-rank simulator set (e.g., the simulator set defined by Eqn. (157)) combined with real data. In such low rank simulator classes, we know that Σ(Θ ⋆ ) is fixed as an estimatorΣ computed with noisy observations y t = x t + z t . By definition we know L(Θ ⋆ ) is also fixed asL =Σ(Σ+ I) −1 . Therefore, the Kalman dynamics matrix A−LCA = A−LA belongs to a k-dimensional space as A lies in a k dimensional space. As a result, the dimension of feature mappings ϕ s u (t 1 , t 2 , ..., t k ), ϕ s y (t 1 , t 2 , ..., t k ) (Eqn. (137)) reduces to O(k 2 l k ). The Eluder dimension of this low rank simulator class reduces tõ O(k 4 l 2k ) =Õ(k 4 log 2k (H)) for a small constant k.</p>
<p>Therefore, the intrinsic model complexity δ E for a low-rank simulator class fine-tuned with real data isÕ ((n + p) 2 k 5 log 2k (H)), which implies that the robust adversarial training algorithms are very powerful with a small sim-toreal gap in such simulator classes.</p>
<p>H. Proof of Theorem 2</p>
<p>Proof of Theorem 2. Throughout this proof, we assume H is sufficiently large and omit some (instancedependent) constants because we focus on the dependency of H. Note that LQR is a special case of LQG in the sense that the learner can observe the hidden state in LQR, it suffices to consider the case where E is a set of LQRs. Since C = I in LQRs, Assumption 3 holds naturally. Let (A ⋆ , B ⋆ ) be the parameters satisfying Assumptions 1, and 2, and E = {(A, B) : A − A ⋆ ∞ ≤ ǫ, B − B ⋆ ∞ ≤ ǫ}. Choosing ǫ ∝ H −1/2 where H is sufficiently large, by the same perturbation analysis in Simchowitz and Foster (2020, Appendix B), we have that all Θ ∈ E satisfy Assumptions 1 and 2. Fix policyπ, we have
max Θ∈E [Vπ(Θ) − V * (Θ)] ≥ min π max Θ∈E [V π (Θ) − V * (Θ)].(158)
Since we can treat a history-dependent policy as an algorithm, the lower bound in Simchowitz and Foster (2020, Corollary 1) implies that
min π max Θ∈E [V π (Θ) − V * (Θ)] ≥ Ω( √ H).(159)
Notably, Simchowitz and Foster (2020) only imposes the strongly stable assumption, which is weaker than our assumption, but their proof (Lemma B.7 in Simchowitz and Foster (2020)) still holds under our assumptions. Combining (158) and (159), we conclude the proof of Theorem 2.</p>
<p>There are many empirical works studying robust adversarial training algorithms. Pinto et al. (2017b) proposes the robust adversarial training algorithm, which trains a policy in the adversarial environment. Then Pinto et al. (2017a); Mandlekar et al. (2017); Pattanaik et al. (2017);</p>
<p>studying the infinite-horizon linear quadratic regulator (LQR), where the learner can observe the state. For the more challenging infinitehorizon LQG control, where the learner can only observe the noisy observations generated from the hidden state, Mania et al. (2019); Simchowitz et al. (2020); Lale et al. (2020c, 2021) propose various algorithms and establish regret bound for them. In specific, Mania et al. (2019); Lale et al. (2020b) study the strongly convex cost setting, where it is possible to achieve O(poly(log T )) regret bound. Simchowitz et al. (2020); Lale et al. (2020c, 2021) and our work focus on the convex cost, where Simchowitz et al. (2020); Lale et al. (2020c) establish a T 2/3 worst-case regret. Lale et al. (</p>
<p>Auer et al., 2008; Bai et al., 2019) of our algorithm. We follow the idea of Kong et al. (2021); Chen et al. (2021) that maintains two datasets Z and Z new , representing current data used in model regression and new incoming data. The importance score (Line 14 of Algorithm 1) measures the importance of the data in Z new with respect to the data in Z.</p>
<p>dim E (F , 1/H) denotes the 1/H-Eluder dimension of F , N (F , 1/H) is the 1/H-covering number of F . The definitions of Eluder dimension and covering number are deferred to Appendix A.2.</p>
<p>T M lower bound of T w for desired properties at induction stage 2 D h instance dependent constant used in reduction (see Lemma 3) a 0 , b 0 , c 0 instance dependent constant used to define D h D the · ∞ norm of F (see Lemma 6) ∆ the clipping error (difference between f ′ and f , see Lemma 8) A.2 Additional Definitions Definition 2 (Controllablity &amp; Observability). A LQG system Θ = (A, B, C) is (A, B) controllable if the controllability matrix B AB A 2 B . . . A n−1 B has full row rank. We say (A, C) is observable if the observability matrix C ⊤ (CA) ⊤ (CA 2 ) ⊤ . . . (CA n−1 ) ⊤ ⊤ has full column rank. Definition 3 (Stability (Cohen et al., 2018; Lale et al., 2022)). A LQG system Θ = (A, B, C) is stable if ρ(A − BK(Θ)) &lt; 1. It is strongly stable in terms of parameter (κ, γ) if K(Θ) 2 ≤ κ and there exists matrices L and H such that A</p>
<p>follow the warm up procedure used in Lale et al. (2021) to estimate Markov matrix M, and perform the model selection afterwards. The matrix M is M = CF CĀF . . . CĀH −1 F CB CĀB . . . CĀH −1 B ,</p>
<p>5 :
5EstimateM with D init following Eqn. (11) of Lale et al. (2021). 6: Run SYSID(H,Ĝ, n) (Algorithm 2 of Lale et al. (2021)) to obtain an estimateÂ,B,Ĉ,L of the real world system Θ ⋆ . 7: Return E ∩ C, where C is computed through Eqn. (23). Therefore, by estimatingM with random action set D init , we come to a similar confidence set as shown in the Theorem 3.4 of Lale et al. (2021): There exists a unitary matrix T ∈ R n×n such that with probability at least 1 − δ/4, the real world model Θ ⋆ is contained in the set C, where C def = Θ = (Ā,B,C) :</p>
<p>and Eqn. (50) of Lale et al. (2020c) to perform the decomposition</p>
<p>since Σ 0|−1 = I is positive definite (the techniques of Lemma 3.1 of Lale et al. (2020c) can be used here).</p>
<p>where τ l s = {u s−l , y s−l+1 , ..., u s−1 , y s } if l ≤ s, or τ l s denotes the full history at step s if l &gt; s. Summing up Eqn.(93)and(94)implies for any k</p>
<p>where β = O(D 2 log(N (F , 1/H)/δ)) is defined in Eqn. (119).Plugging it into Eqn. (92) combined with Eqn. (89), we can finally bound the regret as</p>
<p>whereP = P (Θ),L = L(Θ), and e t = Cw t + CA(x t −x t|t,Θ ) + z t+1 is the innovation noise such that e t ∼ N (0, CΣ(Θ)C ⊤ + I) (Zheng et al., 2021; Lale et al., 2021). . Note thatx c t|t,Θ =x t|t,Θ − (A − LCA) lx t−l|t−l,Θ , so x c t|t,Θ 2 ≤ (1 + κ 2 (1 − γ 2 ) l )X 2 . Therefore, each term in Eqn. (111) -(114) is bounded in at most O(( √ n + √ p) log H) besides x c t|t,Θ 2 = O(( √ n + √ p) log H). We can finally find a constant D 1 such that f Θ ∞ ≤ D 1 = O((n + p) log 2 H) for any Θ ∈ E .</p>
<p>+(
CAx c t|t,Θ + CBu ⊤ Q CAx c t|t,Θ + CBu + trace Φ(E) CΣC ⊤ + I . A − LCA) s−1 ((I − LC)Bu t−s + Ly t−s+1 ) .</p>
<p>LCCx c t|t,Θ +LCBu .</p>
<p>Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter Abbeel, and Wojciech Zaremba. Transfer from simulation to real world through learning deep inverse dynamics model. arXiv preprint arXiv:1610.03518, 2016. Alon Cohen, Avinatan Hasidim, Tomer Koren, Nevena Lazic, Yishay Mansour, and Kunal Talwar. Online linear quadratic control. In International Conference on Machine Learning, pages 1029-1038. PMLR, 2018. Alon Cohen, Tomer Koren, and Yishay Mansour. Learning linear-quadratic regulators efficiently with only √ T regret. In International Conference on Machine Learning, pages 1300-1309. PMLR, 2019. Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. Advances in neural information processing systems, 33:13049-13061, 2020. Fei Feng, Wotao Yin, and Lin F Yang. How does an approximate model help in reinforcement learning? arXiv preprint arXiv:1912.02986, 2019. Vineet Goyal and Julien Grand-Clement. Robust markov decision processes: Beyond rectangularity. Mathematics of Operations Research, 2022. Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Fast bellman updates for robust mdps. In International Conference on Machine Learning, pages 1979-1988. PMLR, 2018. Nan Jiang. Pac reinforcement learning with an imperfect model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. Advances in neural information processing systems, 34: 13406-13418, 2021. Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960. Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238-1274, 2013. Dingwen Kong, Ruslan Salakhutdinov, Ruosong Wang, and Lin F Yang. Online sub-sampling for reinforcement learning with general function approximation. arXiv preprint arXiv:2106.07203, 2021. Kristinn Kristinsson and Guy Albert Dumont. System identification and control using genetic algorithms. IEEE Transactions on Systems, Man, and Cybernetics, 22(5):1033-1046, 1992. Yufei Kuang, Miao Lu, Jie Wang, Qi Zhou, Bin Li, and Houqiang Li. Learning robust policy against disturbance in transition dynamics via state-conservative policy optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 7247-7254, 2022. Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, and Anima Anandkumar. Explore more and improve regret in linear quadratic regulators. arXiv preprint arXiv:2007.12291, 2020a. Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, and Anima Anandkumar. Logarithmic regret bound in partially observable linear dynamical systems. Advances in Neural Information Processing Systems, 33:20876-20888, 2020b. Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, and Anima Anandkumar. Regret minimization in partially observable linear quadratic control. arXiv preprint arXiv:2002.00082, 2020c. Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, and Anima Anandkumar. Adaptive control and regret minimization in linear quadratic gaussian (lqg) setting. In 2021 American Control Conference (ACC), pages 2517-2522. IEEE, 2021. Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, and Animashree Anandkumar. Reinforcement learning with fast stabilization in linear dynamical systems. In International Conference on Artificial Intelligence and Statistics, pages 5354-5390. PMLR, 2022. David L Ma and Richard D Braatz. Worst-case analysis of finite-time control policies. IEEE Transactions on Control Systems Technology, 9(5):766-774, 2001. David L Ma, Serena H Chung, and Richard D Braatz. Worst-case performance analysis of optimal batch control trajectories. AIChE journal, 45(7):1469-1476, 1999. Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Adversarially robust policy learning: Active construction of physically-plausible perturbations. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3932-3939. IEEE, 2017. Horia Mania, Stephen Tu, and Benjamin Recht. Certainty equivalent control of lqr is efficient. arXiv preprint arXiv:1902.07826, 2019. Daniel J Mankowitz, Nir Levine, Rae Jeong, Yuanyuan Shi, Jackie Kay, Abbas Abdolmaleki, Jost Tobias Springenberg, Timothy Mann, Todd Hester, and Martin Riedmiller. Robust reinforcement learning for continuous control with model misspecification. arXiv preprint arXiv:1906.07516, 2019. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529-533, 2015. Jun Morimoto and Kenji Doya. Robust reinforcement learning. OpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafa l Józefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning dexterous in-hand manipulation. CoRR, 2018. URL http://arxiv.org/abs/1808.00177. Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension. Advances in Neural Information Processing Systems, 27, 2014. Samet Oymak and Necmiye Ozay. Non-asymptotic identification of lti systems from a single trajectory. In 2019 American control conference (ACC), pages 5655-5661. IEEE, 2019. Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan, Evangelos Theodorou, and Byron Boots. Agile autonomous driving using end-to-end deep imitation learning. arXiv preprint arXiv:1709.07174, 2017. Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust deep reinforcement learning with adversarial attacks. arXiv preprint arXiv:1712.03632, 2017. Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE international conference on robotics and automation (ICRA), pages 3803-3810. IEEE, 2018. Lerrel Pinto, James Davidson, and Abhinav Gupta. Supervision via competition: Robot adversaries for learning tasks. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 1601-1608. IEEE, 2017a. Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In International Conference on Machine Learning, pages 2817-2826. PMLR, 2017b. Andrei A Rusu, Matej Večerík, Thomas Rothörl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-real robot learning from pixels with progressive nets. In Conference on robot learning, pages 262-270. PMLR, 2017. Fereshteh Sadeghi and Sergey Levine. Cad2rl: Real single-image flight without a single real image. arXiv preprint arXiv:1611.04201, 2016. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354-359, 2017. Max Simchowitz and Dylan Foster. Naive exploration is optimal for online lqr. In International Conference on Machine Learning, pages 8937-8948. PMLR, 2020. Max Simchowitz, Karan Singh, and Elad Hazan. Improper learning for non-stochastic control. In Conference on Learning Theory, pages 3320-3436. PMLR, 2020. Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018. Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applications in continuous control. In International Conference on Machine Learning, pages 6215-6224. PMLR, 2019. Runyu Zhang, Yingying Li, and Na Li. On the regret analysis of online lqr control with predictions. In 2021 American Control Conference (ACC), pages 697-703. IEEE, 2021. Yuren Zhong, Aniket Anand Deshmukh, and Clayton Scott. Pac reinforcement learning without real-world feedback. arXiv preprint arXiv:1909.10449, 2019.Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement 
learning with value-targeted regression. In International Conference on Machine Learning, pages 
463-474. PMLR, 2020. </p>
<p>Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efficient q-learning with low 
switching cost. Advances in Neural Information Processing Systems, 32, 2019. </p>
<p>Peter E Caines and David Q Mayne. On the discrete time matrix riccati equation of optimal control. 
International Journal of Control, 12(5):785-794, 1970. </p>
<p>Siew Chan, GC Goodwin, and Kwai Sin. Convergence properties of the riccati difference equation 
in optimal filtering of nonstabilizable systems. IEEE Transactions on Automatic Control, 29(2): 
110-118, 1984. </p>
<p>Xiaoyu Chen, Jiachen Hu, Chi Jin, Lihong Li, and Liwei Wang. Understanding domain randomiza-
tion for sim-to-real transfer. arXiv preprint arXiv:2110.03239, 2021. </p>
<p>Xinyi Chen and Elad Hazan. Black-box control for linear dynamical systems. In Conference on 
Learning Theory, pages 1114-1143. PMLR, 2021. </p>
<p>Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2): 
257-280, 2005. </p>
<p>Neural computation, 17(2):335-359, 
2005. </p>
<p>Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain 
randomization for transferring deep neural networks from simulation to the real world. In 2017 
IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23-30. IEEE, 
2017. </p>
<p>Anastasios Tsiamis, Nikolai Matni, and George Pappas. Sample complexity of kalman filtering for 
unknown systems. In Learning for Dynamics and Control, pages 435-444. PMLR, 2020. </p>
<p>Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Xingchao Peng, Sergey Levine, Kate Saenko, 
and Trevor Darrell. Towards adapting deep visuomotor representations from simulated to real 
environments. arXiv preprint arXiv:1511.07111, 2(3), 2015. </p>
<p>Lu Wang, Wei Zhang, Xiaofeng He, and Hongyuan Zha. Supervised reinforcement learning with 
recurrent neural network for dynamic treatment recommendation. In Proceedings of the 24th 
ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pages 2447-
2456, 2018. </p>
<p>Huan Xu and Shie Mannor. Distributionally robust markov decision processes. Advances in Neural 
Information Processing Systems, 23, 2010. </p>
<p>Wenshuai Zhao, Jorge Peña Queralta, and Tomi Westerlund. Sim-to-real transfer in deep rein-
forcement learning for robotics: a survey. In 2020 IEEE Symposium Series on Computational 
Intelligence (SSCI), pages 737-744. IEEE, 2020. </p>
<p>Yang Zheng, Luca Furieri, Maryam Kamgarpour, and Na Li. Sample complexity of linear quadratic 
gaussian (lqg) control for output feedback systems. In Learning for dynamics and control, pages 
559-570. PMLR, 2021. </p>
<p>and (135, each term can be written as the inner product of a feature of Θ and a feature of E. For example, trace Φ(E)(CΣC ⊤ + I) is the inner product of Φ(E) (feature of E) and CΣC ⊤ + I (feature of Θ, since it only depends on Θ). As a more complicated example, we check (LCAx c t|t,Θ ) ⊤ ζ(E)(LCAx c t|t,Θ ).L CAx c </p>
<p>t|t,Θ </p>
<p>⊤ </p>
<p>ζ(E)LCAx c </p>
<p>t|t,Θ </p>
<p>random variable conditioned on F t−1 by Lemma 6, and it is F t measurable (hence F -adapted). It holds that for the dataset Z at the end of episode kforIn order to show thatf is close to f * , it suffices to bound E(f ). For some fixedWe now start to bound these three terms. For any fixed f ∈ F , we know that 2 Z, f − f * Z is D f − f * Z -subGaussian. Then it holds with probability 1 − 3δ/4 for any episode k and λ &gt; 0 thatChoosing λ = 1/D 2 , we getBy a union bound argument we know with probability 1 − 3δ/4, the term max f ∈G(α) E(f ) is bounded by D 2 log(4|G(α)|/3δ) + 2∆DH.ForNote that by definition off it holds that√ H + D 2H log (3H(H + 1)/δ) · α √ H ≤ 2 (D + ∆) αH + αH · D 2 log (3H(H + 1)/δ).
Regret bounds for the adaptive control of linear quadratic systems. Yasin Abbasi, -Yadkori , Csaba Szepesvári, Proceedings of the 24th Annual Conference on Learning Theory. the 24th Annual Conference on Learning TheoryJMLR Workshop and Conference ProceedingsYasin Abbasi-Yadkori and Csaba Szepesvári. Regret bounds for the adaptive control of linear quadratic systems. In Proceedings of the 24th Annual Conference on Learning Theory, pages 1-26. JMLR Workshop and Conference Proceedings, 2011.</p>            </div>
        </div>

    </div>
</body>
</html>
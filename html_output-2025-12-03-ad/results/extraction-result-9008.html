<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9008 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9008</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9008</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-272832170</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.15890v1.pdf" target="_blank">HLB: Benchmarking LLMs' Humanlikeness in Language Use</a></p>
                <p><strong>Paper Abstract:</strong> As synthetic data becomes increasingly prevalent in training language models, particularly through generated dialogue, concerns have emerged that these models may deviate from authentic human language patterns, potentially losing the richness and creativity inherent in human communication. This highlights the critical need to assess the humanlikeness of language models in real-world language use. In this paper, we present a comprehensive humanlikeness benchmark (HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic experiments designed to probe core linguistic aspects, including sound, word, syntax, semantics, and discourse (see https://huggingface.co/spaces/XufengDuan/HumanLikeness). To anchor these comparisons, we collected responses from over 2,000 human participants and compared them to outputs from the LLMs in these experiments. For rigorous evaluation, we developed a coding algorithm that accurately identified language use patterns, enabling the extraction of response distributions for each task. By comparing the response distributions between human participants and LLMs, we quantified humanlikeness through distributional similarity. Our results reveal fine-grained differences in how well LLMs replicate human responses across various linguistic levels. Importantly, we found that improvements in other performance metrics did not necessarily lead to greater humanlikeness, and in some cases, even resulted in a decline. By introducing psycholinguistic methods to model evaluation, this benchmark offers the first framework for systematically assessing the humanlikeness of LLMs in language use.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9008.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9008.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HLB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Humanlikeness Benchmark (HLB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A psycholinguistic benchmark introduced in this paper that evaluates 20 LLMs on 10 classical psycholinguistic experiments spanning sound, word, syntax, semantics, and discourse; humanlikeness is measured by 1 - JS divergence between human and model response distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>HLB (aggregate: 20 LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Benchmark of 20 large language models (includes OpenAI GPT family, Meta Llama family, Mistral family, and others) evaluated using identical psycholinguistic tasks and prompt framing; responses collected via vendor APIs with default parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>HLB: 10 psycholinguistic experiments (sound, word, syntax, semantics, discourse)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A battery of 10 adapted psycholinguistic experiments (two per linguistic level) assessing phenomena such as sound symbolism, sound→gender inference, word length/predictivity, word-meaning priming, structural priming, syntactic ambiguity resolution, implausible-sentence reinterpretation, semantic illusions, implicit causality, and bridging vs. elaborative inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Varied across tasks and models; humanlikeness quantified as HS = 1 - JS(P_human, P_model). Scores reported per experiment and model (no single aggregate numeric HS provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human responses gathered from N = 1,905 valid participants; per-item human trial counts ranged on average from ~24 up to ~96 depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Aggregate: models differ substantially in humanlikeness; Llama family reported highest humanlikeness overall, OpenAI models relatively stable, Mistral family decreased in humanlikeness for recent release.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Each LLM provided 100 responses per item; humans provided ~50–100 responses per item (overall N=1,905); one-trial-per-run paradigm for both humans and LLMs; prompts for LLMs closely mirror human instructions; responses collected via OpenAI API and Hugging Face with default parameters (no temperature/top-k tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>HS values (1-JS) depend on auto-coding of responses (auto-coder validated, κ = 0.993). Models were run with default sampling parameters, which the authors treat as realistic but may affect response diversity; human sample restricted to native English speakers in UK/US and may not reflect global language use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "HLB: Benchmarking LLMs' Humanlikeness in Language Use", 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9008.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9008.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Llama-3.1-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A member of the Meta Llama family (instruct-tuned), reported in this paper as the top-performing Llama model on the HLB benchmark with notably large deviations from humans on some semantic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta-Llama-3.1-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-tuned Llama 3.1 model from Meta evaluated in this study; described as having improved humanlikeness relative to previous Llama releases.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>HLB (with notable results on Experiment 4: Word-meaning priming)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Word-meaning priming (Experiment 4) assesses whether recent context biases interpretation of ambiguous words (semantic priming for ambiguous word senses).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>In Experiment 4, Llama-3.1-70B showed a priming effect: 52% of responses associated 'post' with the job-related meaning after a word-meaning prime, and 38% after a synonym prime.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans: 20% associated 'post' with the job-related meaning after the word-meaning prime and 18% after the synonym prime (reported averages).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Llama-3.1-70B over-primes compared to humans (substantially larger priming effect), indicating divergence from human semantic-association patterns on ambiguous words.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Models given the same prime+target prompts formatted to mirror human instructions; 100 responses per item per model; responses auto-coded and compared to human response distributions via JS divergence; Experiment 4 highlighted as most non-humanlike (t = -116.32, p < .001).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Only priming percentages for specific conditions are reported; overall HS numeric values for this model on Experiment 4 are not explicitly listed. Over-priming is a relative observation against human frequencies; causes could include training data/statistical association strength not controlled here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "HLB: Benchmarking LLMs' Humanlikeness in Language Use", 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9008.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9008.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama family (Llama 2 / Llama 3 / Llama 3.1 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The family of Meta Llama models evaluated in HLB; overall the Llama family showed increases in humanlikeness relative to other families, with significant improvements from Llama-3-70B to Llama-3.1-70B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta Llama family (selected: Llama 2, Llama 3, Llama 3.1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A family of foundation and instruct-tuned models from Meta evaluated across the HLB experiments; specific models include Llama 2, Llama 3, and Llama 3.1 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>HLB: 10 psycholinguistic experiments (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same HLB battery as above, covering sound/word/syntax/semantics/discourse; used to compute per-model HS scores.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Llama models overall achieved higher humanlikeness scores than comparison families; Meta-Llama-3.1-70B-Instruct noted as highest within family.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human baseline: N = 1,905 participants; distributional baselines computed per item (varied by experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Llama models significantly outperformed Mistral models (t = 10.44, p < .001) and outperformed OpenAI models (t = 3.13, p = .002). Within-family upgrade from Meta-Llama-3-70B-Instruct to Meta-Llama-3.1-70B-Instruct showed significant humanlikeness increase (t = -4.85, p < .001).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Per-model data: 100 responses per item; one-trial-per-run; auto-coding + JS divergence to compute HS; comparisons performed across three selected models per family for statistical tests.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper reports statistical comparisons but does not provide full per-item HS numeric vectors in the text; reasons for Llama improvements (architecture vs. data vs. fine-tuning) are not disentangled here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "HLB: Benchmarking LLMs' Humanlikeness in Language Use", 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9008.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9008.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-Instruct-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Mistral-series instruction-tuned 7B model evaluated in the HLB; reported to show a decline in humanlikeness relative to earlier Mistral releases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-tuned release in the Mistral family evaluated via Hugging Face; described as scoring lower in humanlikeness than its predecessors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>HLB: 10 psycholinguistic experiments (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>HLB tasks spanning sound, word, syntax, semantics, and discourse to probe humanlike language use.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported as scoring lower in humanlikeness than earlier Mistral-7B-Instruct-v0.1; specific HS numeric scores not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human baseline: N = 1,905 participants (distributional baselines per task).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Mistral family underperformed Llama family significantly (t = 10.44, p < .001). Within Mistral, v0.3 showed a significant decrease in humanlikeness compared to v0.1 (t = 5.45, p < .001).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>100 responses per item collected via Hugging Face Inference API with default parameters; one-trial-per-run; auto-coding used to extract distributions for JS divergence-based HS.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No detailed breakdown of which specific experiments drove the Mistral decline is provided; the authors note that training methods/data variations may explain decreased alignment but provide no causal analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "HLB: Benchmarking LLMs' Humanlikeness in Language Use", 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9008.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9008.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI GPT family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT models (GPT-4o, GPT-3.5-turbo, GPT-4o-mini referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI models included in the HLB evaluation; described as showing relatively stable humanlikeness across tasks with no significant difference between GPT-3.5-turbo and GPT-4o in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI GPT series (GPT-3.5-turbo, GPT-4o, GPT-4o-mini referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT family models were included and queried via OpenAI API using default parameters to obtain 100 responses per item; described as maintaining consistent humanlikeness scores across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>HLB: 10 psycholinguistic experiments (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same HLB battery of psycholinguistic tasks probing multiple linguistic levels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Described as relatively stable across tasks; no significant difference between GPT-3.5-turbo and GPT-4o (t = -0.93, p = 0.352). Exact HS numeric values not reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human baseline: N = 1,905 participants; per-item distributions used for HS calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>OpenAI models performed consistently but were outperformed overall by the Llama family (Llama > OpenAI, t = 3.13, p = .002).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Responses collected via OpenAI API with default parameters; prompts mirrored human instructions; 100 responses per item; one-trial-per-run paradigm; auto-coding then JS divergence used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No per-experiment numeric performance breakdown shown in the main text for these models; model sizes/architectural details are not specified in this paper (left unspecified).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "HLB: Benchmarking LLMs' Humanlikeness in Language Use", 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9008.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9008.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experiment4_Aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experiment 4 — Word-meaning priming (aggregate LLMs vs humans)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A psycholinguistic task in the HLB measuring how prime context biases interpretation of ambiguous words; reported as the most non-humanlike experiment with large statistical divergence between human and LLM responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate LLM responses (all evaluated models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregate behavior across evaluated LLMs on the word-meaning priming task; example model-level contrasts (e.g., Llama-3.1-70B) were reported explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Word-meaning priming (Experiment 4)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Assesses semantic priming for ambiguous words (e.g., whether 'post' is interpreted as 'mail' vs 'job' after context primes); domain: lexical semantics / semantic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Large divergence from humans reported overall; example: Llama-3.1-70B: 52% (word-meaning prime) vs 38% (synonym prime).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans: 20% (word-meaning prime) vs 18% (synonym prime) for the same measure reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs (example: Llama-3.1) show substantially larger priming effects than humans (over-priming). Experiment 4 was characterized as the most non-humanlike (t = -116.32, p < .001).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Stimuli used ambiguous words with primes; participants (and models) given single-run prompts; responses coded by auto-coder and compared via JS divergence; 100 model responses per item, ~50–100 human per item dependent on task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Reported percentages are per-condition frequencies; JS divergence HS scores for the experiment are not enumerated in-text; causes of over-priming (data biases, associative strengths) are discussed qualitatively but not causally analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "HLB: Benchmarking LLMs' Humanlikeness in Language Use", 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do large language models resemble humans in language use? <em>(Rating: 2)</em></li>
                <li>Using cognitive psychology to understand GPT-3 <em>(Rating: 2)</em></li>
                <li>What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models <em>(Rating: 2)</em></li>
                <li>Language models show human-like content effects on reasoning tasks <em>(Rating: 2)</em></li>
                <li>Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9008",
    "paper_id": "paper-272832170",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "HLB",
            "name_full": "Humanlikeness Benchmark (HLB)",
            "brief_description": "A psycholinguistic benchmark introduced in this paper that evaluates 20 LLMs on 10 classical psycholinguistic experiments spanning sound, word, syntax, semantics, and discourse; humanlikeness is measured by 1 - JS divergence between human and model response distributions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "HLB (aggregate: 20 LLMs)",
            "model_description": "Benchmark of 20 large language models (includes OpenAI GPT family, Meta Llama family, Mistral family, and others) evaluated using identical psycholinguistic tasks and prompt framing; responses collected via vendor APIs with default parameters.",
            "model_size": null,
            "test_battery_name": "HLB: 10 psycholinguistic experiments (sound, word, syntax, semantics, discourse)",
            "test_description": "A battery of 10 adapted psycholinguistic experiments (two per linguistic level) assessing phenomena such as sound symbolism, sound→gender inference, word length/predictivity, word-meaning priming, structural priming, syntactic ambiguity resolution, implausible-sentence reinterpretation, semantic illusions, implicit causality, and bridging vs. elaborative inferences.",
            "llm_performance": "Varied across tasks and models; humanlikeness quantified as HS = 1 - JS(P_human, P_model). Scores reported per experiment and model (no single aggregate numeric HS provided in text).",
            "human_baseline_performance": "Human responses gathered from N = 1,905 valid participants; per-item human trial counts ranged on average from ~24 up to ~96 depending on task.",
            "performance_comparison": "Aggregate: models differ substantially in humanlikeness; Llama family reported highest humanlikeness overall, OpenAI models relatively stable, Mistral family decreased in humanlikeness for recent release.",
            "experimental_details": "Each LLM provided 100 responses per item; humans provided ~50–100 responses per item (overall N=1,905); one-trial-per-run paradigm for both humans and LLMs; prompts for LLMs closely mirror human instructions; responses collected via OpenAI API and Hugging Face with default parameters (no temperature/top-k tuning).",
            "limitations_or_caveats": "HS values (1-JS) depend on auto-coding of responses (auto-coder validated, κ = 0.993). Models were run with default sampling parameters, which the authors treat as realistic but may affect response diversity; human sample restricted to native English speakers in UK/US and may not reflect global language use.",
            "uuid": "e9008.0",
            "source_info": {
                "paper_title": "HLB: Benchmarking LLMs' Humanlikeness in Language Use",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Llama-3.1-70B",
            "name_full": "Meta-Llama-3.1-70B-Instruct",
            "brief_description": "A member of the Meta Llama family (instruct-tuned), reported in this paper as the top-performing Llama model on the HLB benchmark with notably large deviations from humans on some semantic tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Meta-Llama-3.1-70B-Instruct",
            "model_description": "An instruction-tuned Llama 3.1 model from Meta evaluated in this study; described as having improved humanlikeness relative to previous Llama releases.",
            "model_size": "70B",
            "test_battery_name": "HLB (with notable results on Experiment 4: Word-meaning priming)",
            "test_description": "Word-meaning priming (Experiment 4) assesses whether recent context biases interpretation of ambiguous words (semantic priming for ambiguous word senses).",
            "llm_performance": "In Experiment 4, Llama-3.1-70B showed a priming effect: 52% of responses associated 'post' with the job-related meaning after a word-meaning prime, and 38% after a synonym prime.",
            "human_baseline_performance": "Humans: 20% associated 'post' with the job-related meaning after the word-meaning prime and 18% after the synonym prime (reported averages).",
            "performance_comparison": "Llama-3.1-70B over-primes compared to humans (substantially larger priming effect), indicating divergence from human semantic-association patterns on ambiguous words.",
            "experimental_details": "Models given the same prime+target prompts formatted to mirror human instructions; 100 responses per item per model; responses auto-coded and compared to human response distributions via JS divergence; Experiment 4 highlighted as most non-humanlike (t = -116.32, p &lt; .001).",
            "limitations_or_caveats": "Only priming percentages for specific conditions are reported; overall HS numeric values for this model on Experiment 4 are not explicitly listed. Over-priming is a relative observation against human frequencies; causes could include training data/statistical association strength not controlled here.",
            "uuid": "e9008.1",
            "source_info": {
                "paper_title": "HLB: Benchmarking LLMs' Humanlikeness in Language Use",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Llama family",
            "name_full": "Meta Llama family (Llama 2 / Llama 3 / Llama 3.1 variants)",
            "brief_description": "The family of Meta Llama models evaluated in HLB; overall the Llama family showed increases in humanlikeness relative to other families, with significant improvements from Llama-3-70B to Llama-3.1-70B.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Meta Llama family (selected: Llama 2, Llama 3, Llama 3.1)",
            "model_description": "A family of foundation and instruct-tuned models from Meta evaluated across the HLB experiments; specific models include Llama 2, Llama 3, and Llama 3.1 variants.",
            "model_size": null,
            "test_battery_name": "HLB: 10 psycholinguistic experiments (aggregate)",
            "test_description": "Same HLB battery as above, covering sound/word/syntax/semantics/discourse; used to compute per-model HS scores.",
            "llm_performance": "Llama models overall achieved higher humanlikeness scores than comparison families; Meta-Llama-3.1-70B-Instruct noted as highest within family.",
            "human_baseline_performance": "Human baseline: N = 1,905 participants; distributional baselines computed per item (varied by experiment).",
            "performance_comparison": "Llama models significantly outperformed Mistral models (t = 10.44, p &lt; .001) and outperformed OpenAI models (t = 3.13, p = .002). Within-family upgrade from Meta-Llama-3-70B-Instruct to Meta-Llama-3.1-70B-Instruct showed significant humanlikeness increase (t = -4.85, p &lt; .001).",
            "experimental_details": "Per-model data: 100 responses per item; one-trial-per-run; auto-coding + JS divergence to compute HS; comparisons performed across three selected models per family for statistical tests.",
            "limitations_or_caveats": "Paper reports statistical comparisons but does not provide full per-item HS numeric vectors in the text; reasons for Llama improvements (architecture vs. data vs. fine-tuning) are not disentangled here.",
            "uuid": "e9008.2",
            "source_info": {
                "paper_title": "HLB: Benchmarking LLMs' Humanlikeness in Language Use",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Mistral-7B-v0.3",
            "name_full": "Mistral-7B-Instruct-v0.3",
            "brief_description": "A Mistral-series instruction-tuned 7B model evaluated in the HLB; reported to show a decline in humanlikeness relative to earlier Mistral releases.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct-v0.3",
            "model_description": "An instruction-tuned release in the Mistral family evaluated via Hugging Face; described as scoring lower in humanlikeness than its predecessors.",
            "model_size": "7B",
            "test_battery_name": "HLB: 10 psycholinguistic experiments (aggregate)",
            "test_description": "HLB tasks spanning sound, word, syntax, semantics, and discourse to probe humanlike language use.",
            "llm_performance": "Reported as scoring lower in humanlikeness than earlier Mistral-7B-Instruct-v0.1; specific HS numeric scores not provided in text.",
            "human_baseline_performance": "Human baseline: N = 1,905 participants (distributional baselines per task).",
            "performance_comparison": "Mistral family underperformed Llama family significantly (t = 10.44, p &lt; .001). Within Mistral, v0.3 showed a significant decrease in humanlikeness compared to v0.1 (t = 5.45, p &lt; .001).",
            "experimental_details": "100 responses per item collected via Hugging Face Inference API with default parameters; one-trial-per-run; auto-coding used to extract distributions for JS divergence-based HS.",
            "limitations_or_caveats": "No detailed breakdown of which specific experiments drove the Mistral decline is provided; the authors note that training methods/data variations may explain decreased alignment but provide no causal analysis.",
            "uuid": "e9008.3",
            "source_info": {
                "paper_title": "HLB: Benchmarking LLMs' Humanlikeness in Language Use",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "OpenAI GPT family",
            "name_full": "OpenAI GPT models (GPT-4o, GPT-3.5-turbo, GPT-4o-mini referenced)",
            "brief_description": "OpenAI models included in the HLB evaluation; described as showing relatively stable humanlikeness across tasks with no significant difference between GPT-3.5-turbo and GPT-4o in the study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI GPT series (GPT-3.5-turbo, GPT-4o, GPT-4o-mini referenced)",
            "model_description": "OpenAI's GPT family models were included and queried via OpenAI API using default parameters to obtain 100 responses per item; described as maintaining consistent humanlikeness scores across tasks.",
            "model_size": null,
            "test_battery_name": "HLB: 10 psycholinguistic experiments (aggregate)",
            "test_description": "Same HLB battery of psycholinguistic tasks probing multiple linguistic levels.",
            "llm_performance": "Described as relatively stable across tasks; no significant difference between GPT-3.5-turbo and GPT-4o (t = -0.93, p = 0.352). Exact HS numeric values not reported in text.",
            "human_baseline_performance": "Human baseline: N = 1,905 participants; per-item distributions used for HS calculation.",
            "performance_comparison": "OpenAI models performed consistently but were outperformed overall by the Llama family (Llama &gt; OpenAI, t = 3.13, p = .002).",
            "experimental_details": "Responses collected via OpenAI API with default parameters; prompts mirrored human instructions; 100 responses per item; one-trial-per-run paradigm; auto-coding then JS divergence used for comparison.",
            "limitations_or_caveats": "No per-experiment numeric performance breakdown shown in the main text for these models; model sizes/architectural details are not specified in this paper (left unspecified).",
            "uuid": "e9008.4",
            "source_info": {
                "paper_title": "HLB: Benchmarking LLMs' Humanlikeness in Language Use",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Experiment4_Aggregate",
            "name_full": "Experiment 4 — Word-meaning priming (aggregate LLMs vs humans)",
            "brief_description": "A psycholinguistic task in the HLB measuring how prime context biases interpretation of ambiguous words; reported as the most non-humanlike experiment with large statistical divergence between human and LLM responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Aggregate LLM responses (all evaluated models)",
            "model_description": "Aggregate behavior across evaluated LLMs on the word-meaning priming task; example model-level contrasts (e.g., Llama-3.1-70B) were reported explicitly.",
            "model_size": null,
            "test_battery_name": "Word-meaning priming (Experiment 4)",
            "test_description": "Assesses semantic priming for ambiguous words (e.g., whether 'post' is interpreted as 'mail' vs 'job' after context primes); domain: lexical semantics / semantic memory.",
            "llm_performance": "Large divergence from humans reported overall; example: Llama-3.1-70B: 52% (word-meaning prime) vs 38% (synonym prime).",
            "human_baseline_performance": "Humans: 20% (word-meaning prime) vs 18% (synonym prime) for the same measure reported in the paper.",
            "performance_comparison": "LLMs (example: Llama-3.1) show substantially larger priming effects than humans (over-priming). Experiment 4 was characterized as the most non-humanlike (t = -116.32, p &lt; .001).",
            "experimental_details": "Stimuli used ambiguous words with primes; participants (and models) given single-run prompts; responses coded by auto-coder and compared via JS divergence; 100 model responses per item, ~50–100 human per item dependent on task.",
            "limitations_or_caveats": "Reported percentages are per-condition frequencies; JS divergence HS scores for the experiment are not enumerated in-text; causes of over-priming (data biases, associative strengths) are discussed qualitatively but not causally analyzed.",
            "uuid": "e9008.5",
            "source_info": {
                "paper_title": "HLB: Benchmarking LLMs' Humanlikeness in Language Use",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do large language models resemble humans in language use?",
            "rating": 2,
            "sanitized_title": "do_large_language_models_resemble_humans_in_language_use"
        },
        {
            "paper_title": "Using cognitive psychology to understand GPT-3",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
            "rating": 2,
            "sanitized_title": "what_bert_is_not_lessons_from_a_new_suite_of_psycholinguistic_diagnostics_for_language_models"
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning tasks",
            "rating": 2,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning_tasks"
        },
        {
            "paper_title": "Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods",
            "rating": 2,
            "sanitized_title": "machine_psychology_investigating_emergent_capabilities_and_behavior_in_large_language_models_using_psychological_methods"
        }
    ],
    "cost": 0.01392775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HLB: Benchmarking LLMs' Humanlikeness in Language Use
24 Sep 2024</p>
<p>Xufeng Duan xufeng.duan@link.cuhk.edu.hk 
Department of Linguistics and Modern Languages
The Chinese University of Hong Kong</p>
<p>Bei Xiao 
Department of Linguistics and Modern Languages
The Chinese University of Hong Kong</p>
<p>Xuemei Tang 
Department of Linguistics and Modern Languages
The Chinese University of Hong Kong</p>
<p>Zhenguang G Cai 
Department of Linguistics and Modern Languages
The Chinese University of Hong Kong</p>
<p>Brain and Mind Institute
The Chinese University of Hong Kong</p>
<p>HLB: Benchmarking LLMs' Humanlikeness in Language Use
24 Sep 2024E9981F434F7E0E407F80ABAD48EBF72AarXiv:2409.15890v1[cs.CL]
As synthetic data becomes increasingly prevalent in training language models, particularly through generated dialogue, concerns have emerged that these models may deviate from authentic human language patterns, potentially losing the richness and creativity inherent in human communication.This highlights the critical need to assess the humanlikeness of language models in real-world language use.In this paper, we present a comprehensive humanlikeness benchmark (HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic experiments designed to probe core linguistic aspects, including sound, word, syntax, semantics, and discourse (see this link).To anchor these comparisons, we collected responses from over 2,000 human participants and compared them to outputs from the LLMs in these experiments.For rigorous evaluation, we developed a coding algorithm that accurately identified language use patterns, enabling the extraction of response distributions for each task.By comparing the response distributions between human participants and LLMs, we quantified humanlikeness through distributional similarity.Our results reveal fine-grained differences in how well LLMs replicate human responses across various linguistic levels.Importantly, we found that improvements in other performance metrics did not necessarily lead to greater humanlikeness, and in some cases, even resulted in a decline.By introducing psycholinguistic methods to model evaluation, this benchmark offers the first framework for systematically assessing the humanlikeness of LLMs in language use.</p>
<p>Introduction</p>
<p>In recent years, large language models (LLMs) have made significant advancements.Models like OpenAI's GPT series and Meta's Llama family can generate human-like text, engage in coherent dialogues, and answer complex questions, often producing responses that are indistinguishable from those of humans in certain evaluations (Tsubota and Kano, 2024).Cai et al. (2024) conducted a systematic evaluation of human-like language use in models such as ChatGPT and Vicuna, demonstrating that LLMs closely replicate human language patterns in many aspects.However, despite these successes, questions remain about how accurately these models capture the deeper, nuanced patterns of human language use.In other words, the full extent of their similarity to human behavior remains unclear.</p>
<p>The importance of evaluating humanlikeness in language use is further underscored by the increasing reliance on synthetic data for model training, particularly in dialogue models.While synthetic data generation facilitates efficient scaling of model training, it raises concerns about models diverging from real-world human language patterns(del Rio-Chanona et al., 2024).Studies have shown that synthetic data can degrade model performance after retraining (Shumailov et al., 2024).This makes it imperative to assess the humanlikeness of LLMs rigorously across various aspects of language use, to ensure that models do not lose the diversity and richness of human language data.</p>
<p>To address this challenge, we introduce a psycholinguistic benchmark designed to provide a systematic and comprehensive evaluation of how closely LLMs align with human linguistic behavior.</p>
<p>Although numerous benchmarks and leaderboards have been developed to assess the performance of LLMs on downstream NLP tasks, they often fail to capture the finer, human-like qualities of language use.Current NLP benchmarks typically focus on task-based accuracy or performance (Lewkowycz et al., 2022;Zhou et al., 2023;Peng et al., 2024;Hendrycks et al., 2021;Zellers et al., 2019), overlooking the broader psycholinguistic dimensions that characterize how humans process and produce language.Furthermore, few studies have systematically compared the language use of LLMs and human participants across multiple linguistic levels.This gap highlights the need for a new benchmark that can robustly measure the extent to which LLMs replicate human language behavior in real-world, diverse linguistic contexts.</p>
<p>In this paper, we address this gap by presenting a psycholinguistic benchmark study that evaluates the humanlikeness of 20 LLMs.Our benchmark consists of 10 representative psycholinguistic experiments, adapted from Cai et al. (2024), which cover five core linguistic aspects: sound, word, syntax, semantics, and discourse, with two experiments dedicated to each aspect (see 1).We collected approximately 50 to 100 responses per item from over 2,000 human participants.Additionally, we gathered 100 responses per item from each of the 20 LLMs, including well-known models such as GPT-4o, GPT-3.5, Llama 2, Llama 3, Llama 3.1, and other state-of-the-art models (see Table 1).To quantify humanlikeness, we developed an autocoding algorithm that efficiently and reliably extracts language use patterns from responses.The humanlikeness metric was then calculated based on the similarity between the response distributions of humans and LLMs, using a comparison of their probability distributions.</p>
<p>Our findings reveal significant, nuanced differences in how LLMs perform across various linguistic aspects, offering a new benchmark for evaluating the humanlikeness of LLMs in natural language use.This benchmark introduces psycholinguistic methods to model evaluation and provides the first framework for systematically assessing the humanlikeness of LLMs in language use.</p>
<p>Related Work</p>
<p>Recent advances in LLMs have led to the development of various benchmarks designed to evaluate their linguistic capabilities.Standard benchmarks like GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) assess models across a range of natural language processing (NLP) tasks, including sentence classification, textual entailment, and question answering.However, these benchmarks primarily focus on task-based accuracy and often overlook the more intricate aspects of humanlike language processing.While these evaluations provide valuable insights into model performance, they do not fully capture the extent to which LLMs comprehend and generate language in a humanlike manner.As Manning et al. ( 2020) note, LLMs are powerful statistical models that can identify patterns in vast datasets, but these benchmarks do not adequately test how well models replicate human patterns of language use due to the interplay of complex cognitive biases.</p>
<p>Psychological Experimentation on LLMs</p>
<p>A growing body of research has begun applying classical psychological experiments to evaluate LLMs in more domain-specific and cognitively demanding tasks.For example, Binz and Schulz (2023) and Dasgupta et al. (2023)   as seen in the work of Huang and Chang (2023)and Qiao et al. (2023), who analyzed reasoning patterns in LLMs.Hagendorff (2023) further provided a comprehensive review of LLM performance in psychological tests, showing that while LLMs demonstrate sophisticated behaviors, they often diverge from human cognition.These divergences highlight the need for more robust frameworks to understand the limitations of LLMs in mimicking human thought processes.</p>
<p>Psycholinguistic Experimentation on LLMs</p>
<p>Psycholinguistic approaches offer a deeper analysis by testing LLMs on how well they replicate the cognitive processes underlying human language processing.Ettinger (2020) and Futrell (2019) have subjected models like BERT to psycholinguistic tasks such as syntactic ambiguity resolution and structural priming, revealing both the strengths and limitations of LLMs in replicating human language processing.Michaelov and Bergen (2023)</p>
<p>Methodology</p>
<p>Human Experiments</p>
<p>Experimental Design The human experiments were conducted using Qualtrics, an online survey platform (Qualtrics, 2024).The study included ten psycholinguistic tasks that spanned various linguistic levels, from sound, word, syntax, and meaning to discourse comprehension, with two experiments for each level (see Appendix A for details).We exposed a participant to only one trial on each experiment, with a total of 10 trials across all the experiments.This setup minimized trial-level effects and facilitated direct comparisons with LLMs, which were tested under similar conditions (presenting instructions and stimuli in a single prompt) to avoid context effects within individual conversations.</p>
<p>Procedure After providing consent, participants completed the ten psycholinguistic tasks (presented in a random order); four attention checks were randomly interspersed among the trials to later identify participants for random responding.Each experimental task began with an instructional screen, some of which included examples to clarify task requirements.The examples were carefully designed to differ from the experimental stimuli to prevent potential priming effects.For instance, in a sentence-completion task, an illustrative example that did not resemble the experimental stimuli and did not induce target words for any stimuli was used.The priming tasks (which included pairs of priming and target stimuli) were spread across multiple pages to avoid strategic responses in case participants realise the relation between the prime and the target.The overall experimental procedure was streamlined for clarity and efficiency, with each session lasting approximately 8 to 10 minutes (mean = 8.336, SD = 4.171).</p>
<p>Participants Participants were recruited from the crowd-sourcing platform Quatrics and restricted to native English speakers residing in the UK and US, according to their registration on Prolific.They were required to use a desktop computer to complete the tasks.Among the 2,205 participants taking part in the experiments, 290 were excluded for not well adhering to the experimental instructions, including completing the study too quickly, showing low effort, or not finishing the experiment, according to the Qualtrics system.The remaining 1,915 participants were further checked for language nativeness and their accuracy with attention checks.After a thorough screening process-excluding those who were not native speakers, failed attention checks, or exhibited irregularities such as excessively short completion times or multiple participation attempts-the final valid sample consisted of 1,905 participants.The sample was composed of participants as follows: female (n = 1,051), male (n = 838), preferred not to disclose (n = 16), with an average age of 44.8 years (range: 18 to 89 years).Educational levels included: no formal education (n = 2), elementary school (n = 12), high school (n = 672), bachelor's degree (n = 862), and master's degree (n = 357).This sample of participants resulted in each item being tested in a minimum average of 24 trials (e.g., Word Length and Predictability) and up to an average of 96 trials (e.g., Sound-Shape Association Task).</p>
<p>LLM Experiments</p>
<p>Experimental Design To compare human responses with those generated by LLMs, we employed the same 10 psycholinguistic tasks designed for human participants.20 LLMs (See Table 2) were selected for evaluation, including models from prominent families like OpenAI's GPT series (GPT-4o, GPT-3.5),Meta's Llama series (Llama 2, Llama 3, Llama 3.1) and Mistral series (OpenAI et al., 2024;Touvron et al., 2023;AI, 2024).Each model provided 100 responses per item in each experiment, ensuring that the response data was comparable to the human data.Similar to the human experimental design, LLMs followed a one-trialper-run paradigm, ensuring that responses were generated independently for each item to prevent context effects.The input format for the LLMs closely mirrored the instructions provided to human participants.Careful modification of human prompts was performed to ensure that task instructions were clear and interpretable by LLMs.This allowed for a direct comparison between human and LLM performance on the same tasks under identical conditions.</p>
<p>Response Collection Procedure This closely mirror that in the human experiments.Each LLM was presented with the task instructions and the stimulus combined into a single prompt.We collected 100 responses (across different conditions) for each item in an experiment in order to ensure a sufficiently large dataset for robust analysis of the response distributions.For OpenAI models, responses were obtained through the OpenAI API, while models hosted on Hugging Face were accessed using the Hugging Face Inference API.All requests to the models were made using their default parameters to encourage variability in responses.The collected responses were stored and processed for subsequent coding and analysis.</p>
<p>Response Coding</p>
<p>Development and Validation</p>
<p>We employed an auto-coding algorithm across 10 experiments to assess agreement between human annotations and machine-generated labels.This algorithm utilized spaCy's en_core_web_trf-3.7.3 model for syntactic parsing (e.g., structural priming and syntactic ambiguity resolution tasks) and regular expressions to detect answer patterns in others.Across 20,953 trials of human response data, we computed Cohen's Kappa (κ), a measure that corrects for chance agreement between the results from manually coding and auto-coding algorithm, defined as:
K = P 0 − P e 1 − P e (1)
where Po is the observed agreement, and Pe is the expected agreement by chance.</p>
<p>The Kappa score was κ = 0.993, indicating nearperfect agreement (z = 451, p &lt; 0.001).This demonstrates the high accuracy of the auto-coding algorithm in replicating human annotations.</p>
<p>Humanlikeness Scoring</p>
<p>To quantify the humanlikeness of LLM responses, we used Jensen-Shannon (JS) divergence to compare the response distributions between human participants and LLMs.JS divergence, a symmetric measure of similarity between two probability distributions, is ideal for assessing how closely LLM responses mirror human behavior across linguistic levels.For each task, the auto-coding algorithm generated response distributions for both humans and LLMs.We computed humanlikeness score (HS) for each item as:
HS item = 1 − JS(P, Q) = 1 − 1 2 [KL(P ∥ M ) + KL(Q ∥ M )]
(2) where P and Q are the human and LLM response distributions, and M is their average.For each experiment, we average the scores across all items.The overall humanlikeness score across all experiments is then computed as:
HS Overall = 1 m m j=1 1 n j n j i=1 1 − 1 2 [KL(P i ∥ M i ) +KL(Q i ∥ M i )]))(3)</p>
<p>Result</p>
<p>The overall humanlikeness scores revealed notable variations in how well LLMs emulated human language use across the 10 psycholinguistic experiments.Here, we performed a concise analysis to explore the data.OpenAI's models, including GPT-3.5-turbo,GPT-4o-mini, and GPT-4o, exhibited relatively stable performance across tasks, maintaining consistent humanlikeness scores.In contrast, the Llama family of models showed an overall increase in humanlikeness scores, with Meta-Llama-3.1-70B-Instructachieving the highest performance among all Llama models.On the other hand, the Mistral family of models showed a slight decrease in humanlikeness, with Mistral-7B-Instruct-v0.3 scoring lower than its predecessors, indicating less alignment with human language use.</p>
<p>Comparative Analysis</p>
<p>Statistical comparisons between model families (three models selected per model family) revealed significant differences in performance.Notably, Llama models significantly outperformed Mistral models in humanlikeness (t = 10.44,p &lt;.001) highlighting the substantial gap between these two families.Furthermore, Llama models also outperformed OpenAI models (t = 3.13, p =.002) although this difference was less pronounced compared to the Llama vs. Mistral comparison.</p>
<p>Within the Llama family, the transition from Meta-Llama-3-70B-Instruct to Meta-Llama-3.1-70B-Instructshowed a significant increase in humanlikeness (t = -4.85,p &lt; .001),indicating improvements in model performance.In contrast, no significant differences were observed between GPT-3.5-turbo and GPT-4o (t = -0.93,p = 0.352), suggesting that OpenAI's models performed consistently across experiments.Interestingly, within the Mistral family, Mistral-7B-Instruct-v0.3 showed a significant decrease in humanlikeness compared to Mistral-7B-Instruct-v0.1 (t = 5.45, p &lt; .001).</p>
<p>These results underscore the varying abilities of different model families to approximate human language patterns, with Llama models demonstrating superior performance overall.</p>
<p>Case analysis</p>
<p>An in-depth analysis of individual experiments further highlights how LLMs' performance varies in replicating human-like responses.Experiment 4, which tested word meaning priming, emerged as the most non-humanlike among the tasks, with substantial differences between human and LLMs' responses (t = -116.32,p &lt; .001).In this experiment, we assessed whether humans and models tend to access, when reading an ambiguous word such as post, the meaning previously used in the prime of an ambiguous word.Human participants exhibited a modest priming effect, with 20% associating post with its job-related meaning after the word-meaning prime and 18% after the synonym prime.In contrast, the Llama-3.1-70Bmodel demonstrated a significantly higher priming effect, with 52% responding to the word-meaning prime and 38% to the synonym prime, revealing a stark divergence from human patterns.This case study emphasizes the challenges LLMs face in aligning their semantic associations with human interpretations, particularly when processing ambiguous or</p>
<p>Discussion</p>
<p>The results of this benchmark study highlight notable differences in how LLMs approximate human language use across various linguistic levels.The Llama family of models, particularly Meta-Llama-3.1-70B-Instruct,consistently outperformed both the OpenAI and Mistral models in terms of humanlikeness score.This finding suggests that recent advancements in the Llama models have led to more humanlike language behaviors, especially in terms of semantic and discourse processing.The OpenAI models, including GPT-4o and GPT-3.5turbo,showed relatively stable performance across tasks, with no significant differences between the models.This stability may reflect a plateau in the improvement of humanlikeness in these models, as compared to the more recent gains observed in the Llama family.On the other hand, the Mistral models demonstrated a decrease in humanlikeness scores, particularly in the transition to .This suggests that certain training methods and data quality in Mistral may have reduced their alignment with human language patterns.One of the key insights from this study is that models differ not only in their overall humanlikeness scores but also in how they handle specific linguistic phenomena.For instance, in Experiment 4 (word meaning priming), we observed a significant divergence in resposnes between humans and LLMs, with the latter showing a much larger priming effect.This over-priming suggests that while LLMs may excel in certain aspects of language generation, they often lack the subtle flexibility that humans display when processing ambiguous or context-dependent language.A major strength of this study is its use of psycholinguistic experiments to evaluate LLMs, which goes beyond traditional NLP benchmarks that focus on task accuracy.By systematically probing various linguistic levels-sound, word, syntax, semantics, and discourse-this benchmark provides a more comprehensive understanding of how LLMs process and generate language.</p>
<p>Conclusion</p>
<p>In this paper, we introduced a novel benchmark for evaluating the humanlikeness of LLMs in language use based on psycholinguistic experiments.Our study evaluated 20 LLMs, including OpenAI's GPT family, Meta's Llama family, the Mistral family and others, across 10 experiments that spanned key linguistic aspects such as sound, word, syntax, semantics, and discourse.Using responses from over 2,000 human participants as a baseline, the results revealed significant differences in model performance, with Llama models consistently out-  performing both OpenAI and Mistral models in terms of language use humanlikeness.These findings underscore the potential of psycholinguistic benchmarks to capture aspects of language that are often missed by traditional NLP evaluations.</p>
<p>This benchmark provides a framework for future research on LLMs, offering a more meaningful and comprehensive way to evaluate their performance in real-world language use.It also highlights areas where current LLMs diverge from human language patterns, particularly in tasks involving semantic priming and ambiguity resolution.By identifying these gaps, this study offers critical insights for the next generation of LLM development, paving the way for models that more closely mirror the intricacies of human communication.</p>
<p>Limitation</p>
<p>However, there are several limitations to this study.First, while the benchmark covers a wide range of linguistic tasks, it may not encompass the full complexity of human language use.Some linguistic phenomena, such as pragmatic reasoning, were not explored in this study.Second, we did not manipulate models' parameters, particularly the temperature or top k, to control the diversity of the generated responses.While using default parameters, particularly temperature, may seem limiting, this choice ensures that we evaluate models in their most typical and practical configurations.Default settings reflect how these models are commonly used in real-world applications, offering a fair and standardized comparison.Tuning parameters like temperature could introduce bias and variability across models, making it difficult to ensure con-sistent evaluation.By using default settings, we eliminate these concerns, allowing for a more reliable assessment of humanlikeness.Finally, while the study includes a large sample of human participants, the specific demographic characteristics (e.g., native English speakers from the UK and US) may not fully represent global language use patterns.Compared to previous benchmarks that focus on task-based performance, this study offers a more in-depth analysis of language models' alignment with human linguistic behavior.Similar studies, such as Ettinger (2020), have used psycholinguistic principles to probe LLMs, but our study stands out by incorporating a broader range of linguistic levels and by using a large-scale dataset of human responses for direct comparison.The significant differences found between model families, such as the higher humanlikeness of Llama models, provide valuable insights for the ongoing development and fine-tuning of LLMs.</p>
<p>A Appendix</p>
<p>This section introduces the ten psycholinguistic experiments used to evaluate the humanlikeness of LLMs across multiple linguistic levels.Each experiment was designed to test a specific linguistic phenomenon and compare the performance of LLMs to human participants.</p>
<p>Sounds: sound-shape association People often associate specific sounds with certain shapes, a phenomenon known as sound symbolism.We tested whether LLMs, like humans, tend to link spikysounding words such as takete or kiki with spiky objects and round-sounding words like maluma or bouba with round objects.</p>
<p>Sounds: sound-gender association People can often guess if an unfamiliar name is male or female based on its sound.In English, women's names more frequently end in vowels compared to men's names.In this task, we asked participants to complete a preamble containing either a consonantending name (e.g., Pelcrad in 1a) or a vowel-ending novel name (e.g., Pelcra in 1b).</p>
<p>1a. Consonant-ending name: Although Pelcrad was sick... 1b.Vowel-ending name: Although Pelcra was sick... Words: word length and predictivity Shorter words are suggested to make communication more efficient by carrying less information.If both humans and LLMs are sensitive to the relationship between word length and informativity, they should prefer shorter words over longer ones with nearly identical meanings when completing sentence preambles that predicted the meaning of the word (making it less informative; e.g., 2a), compared to neutral sentence preambles (e.g., 2b) 2a.Predictive context: Susan was very bad at algebra, so she hated... 1. math 2. mathematics 2b.Neutral context: Susan introduced herself to me as someone who loved... 1. math 2. mathematics Words: word meaning priming Many words have multiple meanings; for instance, post can refer to mail or a job.People update an ambiguous word's meaning based on recent exposure.We tested whether humans and LLMs similarly demonstrate word meaning priming phennomenon: Participants associated post with its job-related meaning more frequently after reading sentences using that context rather than synonyms' contexts (3a vs.3b).</p>
<p>3a. Word-meaning prime: The man accepted the post in the accountancy firm.</p>
<p>3b. Synonym prime: The man accepted the job in the accountancy firm.</p>
<p>Syntax: structural priming In structural priming, people tend to repeat syntactic structures they've recently encountered.We had participants complete prime preambles designed for either PO (prepositional-object dative structure, e.g., The racing driver gave helpful mechanic wrench to complete 4a) or DO (double-object dative structure, e.g., The racing driver gave torn overall his mechanic to complete 4b).Participants then completed target preamble which could be continued as either DO/PO.If structural priming is demonstrated, participants replicate structure of the prime preamble.</p>
<p>4a. DO-inducing prime preamble: The racing driver showed the helpful mechanic ... 4b.PO-inducing prime preamble: The racing driver showed the torn overall ... 4c.Target preamble: The patient showed ... Syntax: syntactic ambiguity resolution The way people parse words into syntactic structures has garnered significant attention in psycholinguistics.For instance, in VP/NP ambiguity (e.g., The ranger killed the poacher with the rifle), people usually interpret the ambiguous prepositional phrase (PP, with the rifle) as modifying the verb phrase (VP, killed the poacher) rather than the noun phrase (NP, the poacher).However, contextual information can modulate this resolution: People are more likely to interpret ambiguous PPs as modifying NPs when there are multiple possible referents (e.g., 5b) compared to when there is only a single referent (e.g., 5a).We examine how effectively LLMs use contextual information to resolve syntactic ambiguities and exhibit such modulation patterns.5a.Single referent: There was a hunter and a poacher.The hunter killed the dangerous poacher with a rifle not long after sunset.Who had a rifle, the hunter or the poacher?</p>
<p>5b.Multiple referents: There was a hunter and two poachers.The hunter killed the dangerous poacher with a rifle not long after sunset.Who had a rifle, the hunter or the poacher?</p>
<p>Meaning: implausible sentence interpretation Listeners often need to recover intended messages from noise-corrupted input.Errors in production or comprehension can make a plausible sentence implausible by omitting (e.g., to omitted, 6a) or inserting words (e.g., to inserted, 6b).People may interpret an implausible sentence nonliterally if they believe it is noise-corrupted.who found that people more frequently reinterpret implausible DO sentences than PO sentences due to the likelihood of omissions over insertions.We tested whether people and LLMs similarly assume that implausible sentences result from noise corruption, with omissions being more likely than insertions.</p>
<p>6a. Implausible DO: The mother gave the candle the daughter.</p>
<p>6b. Implausible PO: The mother gave the daughter to the candle.</p>
<p>6c. Question: Did the daughter receive something/someone?</p>
<p>Meaning: semantic illusions People often overlook obvious errors in sentences.For instance, when asked (7a), many fail to notice that the question should refer to Noah instead of Moses.Such semantic illusions suggest that processing sentence meanings involves partial matches in semantic memory.We tested whether LLMs and people alike produce semantic illusions and are more likely to catch a weak imposter (e.g., Adam, less similar to Noah, 7b) than a strong imposter (e.g.Morse, more similar to Noah, 7a).</p>
<p>7a. Strong: During the Biblical flood, how many animals of each kind did Moses take on the ark? 7b.Weak: During the Biblical flood, how many animals of each kind did Adam take on the ark?</p>
<p>Discourse: implicit causality Certain verbs prompt people to associate causality with either the subject or the object within a sentence.For instance, stimulus-experiencer verbs like scare typ-ically lead people to attribute causality to the subject (e.g., completing 8a as Gary scared Anna because he was violent), whereas experiencerstimulus verbs like fear generally lead people to attribute causality to the object (e.g., completing 8b as Gary feared Anna because she was violent).We assessed whether LLMs, like humans, show similar patterns of causal attribution based on verb type.</p>
<p>8a. Stimulus-experiencer verb: Gary scared Anna because... 8b.Experiencer-stimulus verb: Gary feared Anna because... Discourse: drawing inferences People make bridging inferences more frequently than elaborative inferences.Bridging inferences connect two pieces of information (after reading 9a, people infer that Sharon cut her foot) while elaborative inferences extrapolate from a single piece of information (people are less likely to make this inference after reading 9b).We examined how well an LLM aligns with human patterns of inference by comparing the bridging and elaborative conditions.9a.Bridging: While swimming in the shallow water near the rocks, Sharon stepped on a piece of glass.She called desperately for help, but there was no one around to hear her.</p>
<p>9b. Elaborative: While swimming in the shallow water near the rocks, Sharon stepped on a piece of glass.She had been looking for the watch that she misplaced while sitting on the rocks.</p>
<p>Question: Did she cut her foot?</p>
<p>Figure 1 :
1
Figure 1: The benchmark framework.The example prompt is taken from the sound-gender association task, where humans can infer the gender of a novel name (e.g., Pelcra for Female; Pelcrad for Male) based on phonology.</p>
<p>Figure 2 :
2
Figure 2: Humanlikeness scores of three LLM families</p>
<p>Table 1 :
1
The experiments in this benchmark.</p>
<p>Table 2 :
2
The humanlikness score for models in different experiments.</p>
<p>Mistral-7b-instruct-v0.3: An advanced instruction-based language model. Hugging Face Model Card. A I Mistral, 2024Released on May 22, 2024. Available at</p>
<p>Interaction with context during human sentence processing. Gerry Altmann, Mark Steedman, Cognition. 3031988</p>
<p>Using cognitive psychology to understand GPT-3. Marcel Binz, Eric Schulz, 10.1073/pnas.2218523120Proceedings of the National Academy of Sciences. 1206e22185231202023Proceedings of the National Academy of Sciences</p>
<p>G Zhenguang, Xufeng Cai, David A Duan, Shuqi Haslett, Martin J Wang, Pickering, 10.48550/arXiv.2303.08014ArXiv:2303.08014Do large language models resemble humans in language use? arXiv preprint. 2024</p>
<p>Inferring gender from name phonology. Kimberly Wright Cassidy, Michael H Kelly, Lee'at J Sharoni, Journal of Experimental Psychology: General. 12833621999</p>
<p>Language models show human-like content effects on reasoning tasks. Ishita Dasgupta, Andrew K Lampinen, C Y Stephanie, Hannah R Chan, Antonia Sheahan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, 10.48550/arXiv.2207.07051ArXiv:2207.070512023arXiv preprint</p>
<p>Large language models reduce public knowledge sharing on online Q&amp;A platforms. Maria Del Rio-Chanona, Nadzeya Laurentsyeva, Johannes Wachs ; Dorottya Demszky, Diyi Yang, David S Yeager, Christopher J Bryan, Margarett Clapper, Susannah Chandhok, Johannes C Eichstaedt, Cameron Hecht, Jeremy Jamieson, Meghann Johnson, Michaela Jones, Danielle Krettek-Cobb, Leslie Lai, Nirel Jonesmitchell, Desmond C Ong, Carol S Dweck, James J Gross, James W Pennebaker, 10.1038/s44159-023-00241-5Nature Reviews Psychology. 2112024. 2023PNAS Nexus. Publisher: Nature Publishing Group</p>
<p>From words to meaning: A semantic illusion. Td Erickson, Mattson, j verbal learn verbal behav. 2051981</p>
<p>What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Allyson Ettinger, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Neural language models as psycholinguistic subjects: Representations of syntactic state. Futrell, arXiv:1903.032602019arXiv preprint</p>
<p>Implicit causality in verbs. Catherine Garvey, Alfonso Caramazza, Linguistic inquiry. 531974</p>
<p>Rational integration of noisy evidence and prior semantic expectations in sentence interpretation. Edward Gibson, Leon Bergen, Steven T Piantadosi, Proceedings of the National Academy of Sciences. 110202013</p>
<p>Thilo Hagendorff, 10.48550/arXiv.2303.13988ArXiv:2303.13988Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. 2023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 10.48550/arXiv.2009.03300ArXiv:2009.03300Measuring Massive Multitask Language Understanding. 2021arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, 10.48550/arXiv.2212.10403ArXiv:2212.10403Towards Reasoning in Large Language Models: A Survey. 2023arXiv preprint</p>
<p>Large-scale benchmark yields no evidence that language model surprisal explains syntactic disambiguation difficulty. Kuan-Jung Huang, Suhas Arehalli, Mari Kugemoto, Christian Muxica, Grusha Prasad, Brian Dillon, Tal Linzen, 10.1016/j.jml.2024.104510Journal of Memory and Language. 1371045102024</p>
<p>Saketh Reddy, Karra , 10.48550/arXiv.2204.12000ArXiv:2204.12000Son The Nguyen, and Theja Tulabandhula. 2023. Estimating the Personality of White-Box Language Models. arXiv preprint</p>
<p>Wolfgang Köhler, Gestalt psychology. Psychologische forschung. XVIII-XXX196731</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra, 10.48550/arXiv.2206.14858ArXiv:2206.14858Solving Quantitative Reasoning Problems with Language Models. 2022arXiv preprint</p>
<p>Info/information theory: Speakers choose shorter words in predictive contexts. Kyle Mahowald, Evelina Fedorenko, Steven T Piantadosi, Edward Gibson, Cognition. 12622013</p>
<p>Emergent linguistic structure in artificial neural networks trained by self-supervision. D Christopher, Kevin Manning, John Clark, Urvashi Hewitt, Omer Khandelwal, Levy, 10.1073/pnas.1907367117Proceedings of the National Academy of Sciences. the National Academy of SciencesPublisher: Proceedings of the National Academy of Sciences2020117</p>
<p>James A Michaelov, Benjamin K Bergen, 10.48550/arXiv.2305.14681ArXiv:2305.14681Emergent inabilities? Inverse scaling over the course of pretraining. 2023arXiv preprint</p>
<p>Who is GPT-3? An Exploration of Personality, Values and Demographics. Marilù Miotto, Nicola Rossberg, Bennett Kleinberg, 10.48550/arXiv.2209.14338ArXiv:2209.143382022arXiv preprint</p>
<p>Giambattista Parascandolo. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mohammad Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Christopher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Madelaine Boiko, Anna-Luisa Boyd, Greg Brakman, Tim Brockman, Miles Brooks, Kevin Brundage, Trevor Button, Rosie Cai, Andrew Campbell, Brittany Cann, Chelsea Carey, Rory Carlson, Brooke Carmichael, Che Chan, Fotis Chang, Derek Chantzis, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chen, Chester Chess, Casey Cho, Hyung Won Chu, Dave Chung, Jeremiah Cummings, Yunxing Currier, Cory Dai, Thomas Decareaux, Noah Degry, Damien Deutsch, Arka Deville, David Dhar, Steve Dohan, Sheila Dowling, Adrien Dunning, Atty Ecoffet, Tyna Eleti, David Eloundou, Liam Farhi, Niko Fedus, Simón Felix, Juston Posada Fishman, Isabella Forte, Leo Fulford, Elie Gao, Christian Georges, Vik Gibson, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Heewoo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Kamali, Nitish Kanitscheider, Tabarak Shirish Keskar, Logan Khan, Jong Wook Kilpatrick, Christina Kim, Yongjik Kim, Jan Hendrik Kim, Jamie Kirchner, Matt Kiros, Daniel Knight, Łukasz Kokotajlo, Andrew Kondraciuk, Aris Kondrich, Kyle Konstantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Levy, Ming Chak, Rachel Li, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Anna Lue, Kim Makanju, Sam Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Mayne ; Aalok, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Ashvin Mishkin ; Mély, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Long Noh, Ouyang, O' Cullen, Jakub Keefe, Alex Pachocki, Joe Paino, Ashley Palermo, Kyla Pantuliano, Toki Sheppard, Jessica Sherbakov, Sarah Shieh, Pranav Shoker, Szymon Shyam, Eric Sidor, Maddie Sigler, Jordan Simens, Katarina Sitkin, Ian Slama, Benjamin Sohl, Yang Sokolowsky, C J Song ; Wei, Akila Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Lauren Wong, Sherwin Workman, Jeff Wu, Michael Wu, Kai Wu, Tao Xiao, Sarah Xu, Kevin Yoo, Qiming Yu, Wojciech Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Zhao, 10.48550/arXiv.2303.08774Adam Perelman, Filipe de Avila Belbute Peres. Michelle Pokorny, Pokrass, H Vitchyr, Tolly Pong, Alethea Powell, Boris Power, Elizabeth Power, Raul Proehl, Alec Puri, Jack Radford, Aditya Rae, Cameron Ramesh, Francis Raymond, Kendra Real, Carl Rimbach, Bob Ross, Henri Rotsted, Nick Roussez, Mario Ryder, Ted Saltarelli, Shibani Sanders, Girish Santurkar, Heather Sastry, David Schmidt, John Schnurr, Daniel Schulman, Selsam, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,; Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David; Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng; Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael; Natalie Staudacher; Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek; Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, JasonJuan Felipe Cerón UribeAndrea Vallone, Arun VijayvergiyaFelipe Petroski Such. Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical Report. arXiv preprint. ArXiv:2303.08774 [cs</p>
<p>HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization. Qiwei Peng, Yekun Chai, Xuhong Li, 10.48550/arXiv.2402.16694ArXiv:2402.166942024arXiv preprint</p>
<p>The representation of verbs: Evidence from syntactic priming in language production. J Martin, Holly P Pickering, Branigan, Journal of Memory and language. 3941998</p>
<p>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, 10.48550/arXiv.2212.09597ArXiv:2212.09597Reasoning with Language Model Prompting: A Survey. 2023arXiv preprint</p>
<p>Zhuang Qiu, Xufeng Duan, Zhenguang Garry, Cai, 10.31234/osf.io/qtbh9Pragmatic Implicature Processing in ChatGPT. 2023</p>
<p>Qualtrics and all other qualtrics product or service names are registered trademarks or trademarks of qualtrics. Qualtrics, 2024Provo, UT, USA</p>
<p>Long-term priming of the meanings of ambiguous words. Belen Jennifer M Rodd, Hannah Lopez Cutrin, Alessandra Kirsch, Matthew H Millar, Davis, Journal of Memory and Language. 6822013</p>
<p>Maarten Sap, Ronan Lebras, Daniel Fried, Yejin Choi, 10.48550/arXiv.2210.13312ArXiv:2210.13312Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. 2023arXiv preprint</p>
<p>Ai models collapse when trained on recursively generated data. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, Yarin Gal, Nature. 63180222024</p>
<p>Phantom recollection of bridging and elaborative inferences. Discourse Processes. Murray Singer, Jackie Spear, 201552</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, 10.48550/arXiv.2307.09288ArXiv:2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien RodriguezYuchen Zhang, Angela Fan. and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint</p>
<p>Do Large Language Models know what humans know?. Sean Trott, Cameron Jones, Tyler Chang, James Michaelov, Benjamin Bergen, 10.48550/arXiv.2209.01515ArXiv:2209.015152023arXiv preprint</p>
<p>Text Generation Indistinguishable from Target Person by Prompting Few Examples Using LLM. Yuka Tsubota, Yoshinobu Kano, Proceedings of the 2nd International AIWolfDial Workshop. the 2nd International AIWolfDial WorkshopTokyo, JapanAssociation for Computational Linguistics2024</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 201932</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.07461Glue: A multi-task benchmark and analysis platform for natural language understanding. 2018arXiv preprint</p>
<p>HellaSwag: Can a Machine Really Finish Your Sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, 10.48550/arXiv.1905.07830ArXiv:1905.078302019arXiv preprint</p>
<p>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li, 10.48550/arXiv.2308.07921Solving Challenging Math Word Problems Using GPT-4. 2023</p>
<p>Code Interpreter with Code-based Self-Verification. 10.48550/arXiv.2308.07921ArXiv:2308.07921 [cs] version: 1arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
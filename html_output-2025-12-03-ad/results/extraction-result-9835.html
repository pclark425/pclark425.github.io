<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9835 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9835</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9835</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-c3871abb91c8d0d97a5a0b46655a14392d2af91c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c3871abb91c8d0d97a5a0b46655a14392d2af91c" target="_blank">IDEA-Bench: How Far are Generative Models from Professional Designing?</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> By assessing models’ ability to comprehend and execute novel, complex tasks, IDEA-Bench paves the way toward the development of generative models with autonomous and versatile visual generation capabilities.</p>
                <p><strong>Paper Abstract:</strong> Recent advancements in image generation models enable the creation of high-quality images and targeted modifications based on textual instructions. Some models even support multimodal complex guidance and demonstrate robust task generalization capabilities. However, they still fall short of meeting the nuanced, professional demands of designers. To bridge this gap, we introduce IDEA-Bench, a comprehensive benchmark designed to advance image generation models toward applications with robust task generalization. IDEA-Bench comprises 100 professional image generation tasks and 275 specific cases, categorized into five major types based on the current capabilities of existing models. Furthermore, we provide a representative subset of 18 tasks with enhanced evaluation criteria to facilitate more nuanced and reliable evaluations using Multimodal Large Language Models (MLLMs). By assessing models’ ability to comprehend and execute novel, complex tasks, IDEA-Bench paves the way toward the development of generative models with autonomous and versatile visual generation capabilities.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9835",
    "paper_id": "paper-c3871abb91c8d0d97a5a0b46655a14392d2af91c",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0074252499999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>IDEA-Bench: How Far are Generative Models from PROFESSIONAL DESIGNING?</h1>
<p>Chen Liang ${ }^{2 <em>}$ Lianghua Huang ${ }^{1}$ Jingwu Fang ${ }^{3}$ Huanzhang Dou ${ }^{4 </em>}$ Wei Wang ${ }^{1}$<br>Zhi-Fan Wu ${ }^{1}$ Yupeng Shi ${ }^{1}$ Junge Zhang ${ }^{2}$ Xin Zhao ${ }^{3}$ Yu Liu ${ }^{1}$<br>${ }^{1}$ Tongyi Lab , ${ }^{2}$ CASIA , ${ }^{3}$ USTB , ${ }^{4}$ ZJU</p>
<h4>Abstract</h4>
<p>Real-world design tasks-such as picture book creation, film storyboard development using character sets, photo retouching, visual effects, and font transfer-are highly diverse and complex, requiring deep interpretation and extraction of various elements from instructions, descriptions, and reference images. The resulting images often implicitly capture key features from references or user inputs, making it challenging to develop models that can effectively address such varied tasks. While existing visual generative models can produce high-quality images based on prompts, they face significant limitations in professional design scenarios that involve varied forms and multiple inputs and outputs, even when enhanced with adapters like ControlNets and LoRAs. To address this, we introduce IDEA-Bench, a comprehensive benchmark encompassing 100 real-world design tasks, including rendering, visual effects, storyboarding, picture books, fonts, style-based, and identity-preserving generation, with 275 test cases to thoroughly evaluate a model's general-purpose generation capabilities. Notably, even the best-performing model only achieves 22.48 on IDEA-Bench, while the best general-purpose model only achieves 6.81 . We provide a detailed analysis of these results, highlighting the inherent challenges and providing actionable directions for improvement. Additionally, we provide a subset of 18 representative tasks equipped with multimodal large language model (MLLM)-based auto-evaluation techniques to facilitate rapid model development and comparison. We releases the benchmark data, evaluation toolkits, and an online leaderboard at https://github.com/ali-vilab/IDEA-Bench, aiming to drive the advancement of generative models toward more versatile and applicable intelligent design systems.</p>
<h2>1 Introduction</h2>
<p>Recent advancements in Text-to-Image (T2I) models [Ramesh et al., 2021, 2022, Esser et al., 2021, Rombach et al., 2022, Saharia et al., 2022, Betker et al., 2023, Podell et al., 2023, Esser et al., 2024a, Baldridge et al., 2024, Chen et al., 2023, Labs, 2024a] have significantly enhanced the ability to generate high-quality images from textual descriptions. Building on these successes, models such as ControlNet [Zhang et al., 2023a] and T2I-Adapter [Mou et al., 2024] integrate additional networks into text-to-image diffusion frameworks to incorporate visual conditioning. Similarly, InstructPix2Pix [Brooks et al., 2023] and Emu-Edit [Sheynin et al., 2024] are specifically trained on datasets tailored for complex image editing tasks. Despite the widespread popularity of models like DALL-E 3 [Ramesh et al., 2022] and FLUX-1 [Labs, 2024a], which attract millions of visits daily, professional users often rely on established workflows and specialized software for image creation. This reliance highlights a substantial gap between the capabilities of current image generation models and the demanding requirements of professional-grade image design. When most existing image generation models focus on academic task research, the rapid evolution of large language models (LLMs)</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of IDEA-Bench. IDEA-Bench comprises 5 categories, encompassing a total of 100 professionellevel subtasks, 275 cases, and 1,650 hierarchical evaluation questions. Each category provides subtask examples, quantitative statistics, and showcases a leaderboard of mainstream models.</p>
<p><em>[Radford et al., 2019, Raffel et al., 2020, Brown, 2020, Ouyang et al., 2022, Zhang et al., 2022, Touvron et al., 2023a, b, Dubey et al., 2024]</em> and multimodal language models (MLLMs) <em>[Li et al., 2024, Wang et al., 2024a, OpenAI, 2024, Team et al., 2023]</em> indicate that future image generation models will increasingly aim to achieve both professional-grade quality and task unification, enabling them to handle a diverse range of complex applications seamlessly.</p>
<p>Recent advances in image generation have demonstrated robust task unification and generalization through large-scale pre-training and arbitrary task learning <em>[Ge et al., 2023, Sun et al., 2023, 2024, Wang et al., 2024b, Chern et al., 2024, Huang et al., 2024]</em>. These models handle a wide range of academic tasks with a unified input-output format and can generalize to unseen tasks, showcasing in-context learning abilities. However, existing benchmarks <em>[Cho et al., 2023, Ghosh et al., 2024, Sheynin et al., 2024, Hu et al., 2023, Ruiz et al., 2023, Ku et al., 2023]</em> are often narrow in scope, focusing primarily on isolated academic tasks and lacking comprehensive criteria to evaluate the multifaceted demands of professional image design. Consequently, there is a critical need for more robust and versatile evaluation frameworks to effectively assess the diverse and sophisticated capabilities of contemporary generative models.</p>
<p>To bridge the gap between generative models and professional-grade design, we introduce IDEA-Bench (Intelligent Design Evaluation and Assessment Benchmark). We conduct a thorough review of real-world design and art tasks from diverse platforms, distilling 100 representative professional image generation tasks and 275 cases that comprehensively span the essential capabilities and effects for creating all forms of artwork. These tasks are systematically categorized into five distinct types based on required capabilities: text-to-image, image-to-image, images-to-image, text-to-images, and image(s)-to-images, as illustrated in fig. 1. Figure 2 provides an intuitive display of the definition of some tasks. Currently, only a few models <em>[Yang et al., 2024a, Wu et al., 2024a, Wang et al., 2024b, Chern et al., 2024]</em> can handle the most intricate category, which demands MLLM-level understanding of both image and text and the ability to produce variable-length image sequences. To evaluate generated images, we leverage MLLM to transform the evaluation into an image understanding task. This approach surpasses traditional metrics like FID <em>[Chong and Forsyth, 2020]</em> and CLIPScore <em>[Hessel et al., 2021]</em>, which fail to capture nuances in aesthetic quality, contextual relevance, and multimodal integration. IDEA-Bench includes six detailed evaluation questions per case, totaling 1,650 binary scoring items, and a representative subset of 18 tasks for more nuanced and reliable assessments, ensuring precise and objective evaluations aligned with professional design standards.</p>
<p>Our contributions can be summarized as follows:</p>
<ul>
<li>We introduce IDEA-Bench to bridge the gap between current generative model capabilities and the stringent demands of professional-grade image design, which consists of 100 carefully selected tasks.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Demonstration of task definitions in IDEA-Bench. IDEA-Bench encompasses a diverse range of professional image design tasks. Input formats include plain text, single-image input, and multi-image input, while output formats span both single-image and multi-image generation.</p>
<ul>
<li>We develop a task categorization of five distinct categories based on complexity and modality requirements, with a detailed evaluation framework comprising 1,650 binary scoring items, enabling precise and objective assessment.</li>
<li>We leverage MLLM-evaluation on a subset of 18 representative tasks. IDEA-Bench demonstrates that MLLMs have the capability to perform objective image assessment.</li>
</ul>
<h1>2 Related Work</h1>
<h3>2.1 Image Synthesis Models</h3>
<p>Recently, diffusion models such as Stable Diffusion [Podell et al., 2023, Esser et al., 2024b], DALL-E 3 [Ramesh et al., 2022], and FLUX-1 [Labs, 2024b] have gained significant popularity for generating photorealistic images and offering improved training stability. However, state-of-the-art models still struggle with intricate prompts involving multiple visual concepts and extensive textual information, similar to image editing models [Brooks et al., 2023, Zhang et al., 2024a, Sheynin et al., 2024], which find it challenging to interpret detailed commands requiring nuanced modifications for professional-grade tasks. To enhance generative models' capabilities, IDEA-Bench assesses existing image editing models against the rigorous demands of professional image design, aiming to emulate the creative and analytical processes of human designers. Additionally, image customization [Gal et al., 2022, Hu et al., 2021, Ruiz et al., 2022] is crucial for professionals needing varied personalized images. While techniques like Textual Inversion [Gal et al., 2022], LoRA [Hu et al., 2021], and DreamBooth [Ruiz et al., 2022] improve text-to-image models through fine-tuning, they lack adaptability to unseen subjects. IDEA-Bench aims at comprehensively evaluating models' customization capabilities across multi-image generation tasks, assessing consistency in content, style, identity, and conceptual aspects.</p>
<p>Table 1: Comparison with other image generation benchmarks. IDEA-Bench offers a broader range of task categories, multiple levels of model capabilities for evaluation, and longer and more complex prompts.</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Tasks</th>
<th>Evaluation Category</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Avg. Length</th>
<th>MLLM Eval.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>T2I</td>
<td>T2Is</td>
<td>I2I</td>
<td>Ix2I</td>
<td>I(x)2Is</td>
<td></td>
<td></td>
</tr>
<tr>
<td>PaintSkills [Cho et al., 2023]</td>
<td>3</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>10.44</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>GenEval [Ghosh et al., 2024]</td>
<td>6</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>7.61</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>EmuEdit [Sheynin et al., 2024]</td>
<td>12</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>8.57</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>TIFA [Hu et al., 2023]</td>
<td>12</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>10.46</td>
<td>GPT [Brown, 2020]</td>
</tr>
<tr>
<td>DreamBooth [Ruiz et al., 2023]</td>
<td>30</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>7.58</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>ImagenHub [Ku et al., 2023]</td>
<td>7</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
<td>8.61</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>IDEA-Bench</td>
<td>100</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>138.68</td>
<td>Gemini [Team et al., 2023]</td>
</tr>
</tbody>
</table>
<h1>2.2 Universal Generative Model</h1>
<p>Universal generative models have become pivotal in advancing AI's ability to perform a diverse array of tasks across both language and vision domains. LLMs [Radford et al., 2019, Raffel et al., 2020, Brown, 2020, Ouyang et al., 2022, Zhang et al., 2022, Touvron et al., 2023a,b, Dubey et al., 2024] have demonstrated remarkable versatility, excelling in tasks such as question answering and summarization through extensive pretraining on varied datasets. Building on this success, recent academic efforts [Team, 2024, Zhou et al., 2024, Sun et al., 2024, Wang et al., 2024b, Xiao et al., 2024] aim to adopt a more cohesive paradigm that addresses a wide range of visual generation tasks, including text-to-image and autoregressive image synthesis. Despite these advancements, there remains a significant gap in establishing a unified benchmark that comprehensively evaluates the multifaceted capabilities of these generative models. IDEA-Bench establishes a unified evaluation standard to assesses the strengths and limitations of existing generative foundation models, while guiding the development towards handling realistic and complex real-world image generation tasks at the same time.</p>
<h3>2.3 Benchmarks for Generative Models</h3>
<p>The development of benchmarks typically keeps pace with advancements in text-to-image (T2I) synthesis [Ramesh et al., 2021, 2022, Esser et al., 2021, Rombach et al., 2022, Saharia et al., 2022, Betker et al., 2023, Podell et al., 2023, Esser et al., 2024a, Baldridge et al., 2024, Chen et al., 2023, Labs, 2024a]. DrawBench was initially introduced by Imagen [Saharia et al., 2022], followed by DALL-EVAL [Cho et al., 2023], which proposed PaintSkills to evaluate visual reasoning and social bias capabilities. Recently, an increasing number of benchmarks have emerged [Petsiuk et al., 2022, Bakr et al., 2023, Huang et al., 2023, Lee et al., 2024, Ku et al., 2023]. While these benchmarks primarily focus on assessing image quality and alignment, DEsignBench [Lin et al., 2023] shares similarities with our approach by emphasizing scenarios within authentic design contexts. Differently, IDEA-Bench extends beyond this by addressing additional open challenges in professional creativity. Furthermore, IDEA-Bench is more closely aligned with the latest model capabilities, enabling the use of multi-image and complex instructions as diverse forms of guidance. As shown in table 1, IDEA-Bench greatly expand the prompt length and adde two evaluation dimensions: multi-image input and multi-image generation. This allows for the generation of both single and multiple images, thereby better mimicking the workflows of professional designers.</p>
<h2>3 IDEA-Bench</h2>
<p>We present IDEA-Bench, a comprehensive image generation benchmark that encompasses a wide range of professional tasks in image generation and rigorously challenges models across multiple dimensions of capability. In this section, we first introduce IDEA-Bench's data collection over four different model level in section 3.1, as well as the manual annotation pipeline in section 3.2, followed by a detailed explanation of both human evaluation and automated assessment methodologies, along with the specific metrics used, in section 3.3.</p>
<h3>3.1 Data Collection</h3>
<p>To ensure that the tasks evaluated in IDEA-Bench closely mirror real-world and professional scenarios, we source all task directives and data from the internet and professional designers. Leveraging the knowledge and capabilities of GPT-4o [OpenAI, 2024], we clearly define and classify the tasks, creating task variants aligned with existing model capabilities. The tasks in IDEA-Bench are categorized as follows:</p>
<p>Text-only Guided Image Generation While typical T2I users provide brief descriptions and rely on the model's discretion, professional users meticulously control every visual detail, often iterating multiple prompts to achieve the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Dataset construction process of IDEA-Bench. We categorize the task data from professional design websites and designers based on generative model capabilities and assign capability keywords to each category. For each specific task, we design image generation prompts and hierarchical evaluation questions. Evaluators then refine these evaluation questions on a representative subset.
desired outcome. Current T2I models [Ramesh et al., 2021, 2022, Esser et al., 2021, Rombach et al., 2022, Saharia et al., 2022, Betker et al., 2023, Podell et al., 2023, Esser et al., 2024a, Baldridge et al., 2024, Chen et al., 2023, Labs, 2024a] struggle with complex prompts that include numerous visual concepts and lengthy text. To address this, IDEA-Bench includes 11 tasks designed to evaluate models' ability to handle extensive visual components and incorporate text within images, essential for tasks like poster generation and business card generation.</p>
<p>Image / Multi-image Guided Image Generation Unlike ImagenHub [Ku et al., 2023], which categorizes conditional image generation tasks under specific definitions, we unify these into image-to-image and images-to-image. This consolidation promotes the development of models with enhanced task unification and generalization, similar to advancements in LLMs. IDEA-Bench includes numerous design tasks guided by both image and text inputs in realworld contexts, such as brand merchandise generation, package rendering, and image retouching. With the advent of models like GDT [Huang et al., 2024], Emu2 [Sun et al., 2024], and OmniGen [Xiao et al., 2024], we anticipate future models will exhibit strong multimodal understanding and generative capabilities.</p>
<p>Text / (Multi-)image Guided Multi-image Generation Distinct from previous benchmarks [Cho et al., 2023, Ghosh et al., 2024, Sheynin et al., 2024, Hu et al., 2023, Ruiz et al., 2023, Ku et al., 2023], IDEA-Bench introduces tasks that require generating multiple related images simultaneously. These tasks cater to professional demands such as creating series images around a subject, multi-view image generation, and illustrating storybooks. Unlike video generation [Zhao et al., 2024, Yang et al., 2024b], these image tasks involve significant visual differences while maintaining specific correlations. We further subdivide multi-image generation into text-to-images and image(s)-to-images categories to address models' limitations in handling diverse input modalities, thereby testing their ability to maintain detail, style, and content consistency.</p>
<h1>3.2 Data Annotation</h1>
<p>Prompt Generation After collecting all tasks and corresponding images, annotators provide clear task definitions for each specific case, which, along with input images, are fed to GPT-4o [OpenAI, 2024] to generate image prompts tailored to each case. To ensure consistent evaluation across tasks, we also use GPT-4o to create a standardized set of six evaluation questions per task, ranging from basic to advanced difficulty levels. Specifically, recognizing the inherent</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Visualization of model generated images on IDEA-Bench. We present the generation results of some models in representative task examples, including text-to-image, image-to-image, and images-to-image. In the case of no input image or no generated image, use " $\varnothing$ " instead.
subjectivity in both MLLM-based and human evaluations, we adopt a binary scoring system ( 0 or 1 ) for each question. This approach draws from methodologies in recent multimodal benchmarks [Bai et al., 2023, Liu et al., 2025, Li et al., 2023, Zhang et al., 2024b], where multiple-choice formats are used to mitigate interpretive bias.
To further enhance objectivity and reduce variability, we modify the typical multi-level scoring scale found in existing datasets and models, converting it into a set of six binary (true/false) questions. Each question is accompanied by clearly defined scoring criteria, with 0 indicating unmet standards and 1 representing success. For deeper granularity, questions 1-2 assess the model's understanding of basic task requirements, questions 3-4 evaluate the quality of task completion, and questions 5-6 examine the model's attention to detail and aesthetic quality in the generated images. This structured assessment methodology ensures a rigorous and objective evaluation framework, aligning with professional standards and reducing potential sources of instability in scoring.</p>
<p>Prompt Refinement However, despite our efforts to define evaluation questions as objectively as possible, multimodal large language models (MLLMs) [Li et al., 2024, Wang et al., 2024a, OpenAI, 2024, Team et al., 2023] still struggle to match human evaluators on complex, real-world tasks, limited by current models' gaps in image comprehension relative to human capabilities. To address this, we select a subset of tasks and refine the evaluation questions based on specific examples, ensuring that MLLMs can yield intuitively reasonable results.
One challenge in using MLLMs as evaluators is the sensitivity of models like GPT-4V/GPT-4o to the order of image presentation [Wang et al., 2022, Wu et al., 2024b, Zhang et al., 2023b, Zheng et al., 2023], which particularly makes these models less reliable for consistent comparative evaluation. MLLMs also fail to identify the sequence of multiple input images, whether it is input separately for multiple images or input as a collage. To mitigate this, we manually fine-tune evaluation questions for each case within the task subset, ensuring alignment with human intuition.
For example, in the task of generating a children's storybook with defined character images, we refine each evaluation angle by randomly sampling pairs of input and output images to test consistency. A typical question might ask if the primary character in the first input image appears with consistent features in the third output image. For testing, we use the Gemini 1.5 Pro [Team et al., 2023] to score model outputs and compared these results against human ratings. In cases where discrepancies arise, annotators iteratively refine the language of the evaluation questions until the MLLM evaluations align with human judgment. This approach enhances the robustness of our benchmark, ensuring that evaluations reflect both task-specific accuracy and consistency across diverse visual requirements.</p>
<p>Table 2: Experimental results on all categories of IDEA-Bench. Each task category is averaged across all its subtasks, with the top-ranked model scores for each task type highlighted in bold. Task types that a model cannot support are marked with " - " and are treated as 0 points in the average score calculation. " $\dagger$ " represents rephrasing using GPT-4o to adapt the model to all tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Param.</th>
<th style="text-align: center;">Scores on All Categories</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg. Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T2I</td>
<td style="text-align: center;">12I</td>
<td style="text-align: center;">Is2I</td>
<td style="text-align: center;">T2Is</td>
<td style="text-align: center;">$\mathrm{I}(x) 2 \mathrm{Is}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FLUX-1 $\dagger$ [Labs, 2024b]</td>
<td style="text-align: center;">12B</td>
<td style="text-align: center;">46.06</td>
<td style="text-align: center;">12.13</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">20.15</td>
<td style="text-align: center;">29.17</td>
<td style="text-align: center;">22.48</td>
</tr>
<tr>
<td style="text-align: center;">DALL-E $3 \dagger$ [Ramesh et al., 2022]</td>
<td style="text-align: center;">12B</td>
<td style="text-align: center;">24.34</td>
<td style="text-align: center;">6.95</td>
<td style="text-align: center;">5.27</td>
<td style="text-align: center;">14.36</td>
<td style="text-align: center;">14.44</td>
<td style="text-align: center;">13.07</td>
</tr>
<tr>
<td style="text-align: center;">SD3 $\dagger$ [Esser et al., 2024b]</td>
<td style="text-align: center;">2B</td>
<td style="text-align: center;">24.04</td>
<td style="text-align: center;">10.79</td>
<td style="text-align: center;">4.69</td>
<td style="text-align: center;">21.59</td>
<td style="text-align: center;">13.06</td>
<td style="text-align: center;">14.83</td>
</tr>
<tr>
<td style="text-align: center;">Pixart $\dagger$ [Chen et al., 2023]</td>
<td style="text-align: center;">0.6B</td>
<td style="text-align: center;">14.44</td>
<td style="text-align: center;">7.75</td>
<td style="text-align: center;">3.48</td>
<td style="text-align: center;">17.46</td>
<td style="text-align: center;">21.39</td>
<td style="text-align: center;">12.90</td>
</tr>
<tr>
<td style="text-align: center;">InstructPix2Pix [Brooks et al., 2023]</td>
<td style="text-align: center;">1B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.58</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.52</td>
</tr>
<tr>
<td style="text-align: center;">MagicBrush [Zhang et al., 2024a]</td>
<td style="text-align: center;">1B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">19.07</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.81</td>
</tr>
<tr>
<td style="text-align: center;">Anole [Chern et al., 2024]</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.74</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.48</td>
</tr>
<tr>
<td style="text-align: center;">Emu2 [Sun et al., 2024]</td>
<td style="text-align: center;">37B</td>
<td style="text-align: center;">17.98</td>
<td style="text-align: center;">7.05</td>
<td style="text-align: center;">8.98</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.81</td>
</tr>
<tr>
<td style="text-align: center;">OmniGen [Xiao et al., 2024]</td>
<td style="text-align: center;">3.8B</td>
<td style="text-align: center;">21.41</td>
<td style="text-align: center;">8.17</td>
<td style="text-align: center;">2.77</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.47</td>
</tr>
<tr>
<td style="text-align: center;">Emu2 $\dagger$ [Sun et al., 2024]</td>
<td style="text-align: center;">37B</td>
<td style="text-align: center;">17.98</td>
<td style="text-align: center;">7.05</td>
<td style="text-align: center;">8.98</td>
<td style="text-align: center;">15.53</td>
<td style="text-align: center;">12.78</td>
<td style="text-align: center;">12.46</td>
</tr>
<tr>
<td style="text-align: center;">OmniGen $\dagger$ [Xiao et al., 2024]</td>
<td style="text-align: center;">3.8B</td>
<td style="text-align: center;">21.41</td>
<td style="text-align: center;">8.17</td>
<td style="text-align: center;">2.77</td>
<td style="text-align: center;">23.52</td>
<td style="text-align: center;">21.39</td>
<td style="text-align: center;">15.45</td>
</tr>
</tbody>
</table>
<h1>3.3 Evaluation</h1>
<h3>3.3.1 Human Evaluation</h3>
<p>Evaluation Metric Each subtask in IDEA-Bench comprises 2 to 5 specific cases, with all cases within a subtask being evaluated using the same set of six binary ( 0 or 1 ) judgment questions. These evaluation questions are organized into three hierarchical levels, reflecting the priority of task completion over mere aesthetic quality. For each case, scoring progresses sequentially through these levels. If a model does not achieve a perfect score at a lower level, the scores for all higher levels are automatically set to 0 . This hierarchical approach ensures that while a model may generate high-quality and visually appealing images, it must also fully meet the task requirements to receive a high overall score. This methodology aligns the evaluation criteria with human design standards, emphasizing the importance of fulfilling task objectives.</p>
<p>Formally, let the dataset be denoted as $\mathcal{D}=\left{\mathcal{D}<em c="c">{c} \mid\left{\mathcal{D}</em>}=\left{\mathcal{T<em t="1">{t}\right}</em>\right}}^{T_{c}<em t="t">{c=1}^{C}\right.$. Each subtask $\mathcal{T}</em>$ is then determined by averaging the scores of all $C$ major categories.}$ consists of 2 to 5 specific cases, all evaluated using the same set of six binary ( 0 or 1 ) judgment questions. The score for each subtask is calculated by averaging the binary scores of its cases. Subsequently, each major category $\mathcal{D}_{c}$ receives its score by averaging the scores of its subtasks and converting the result to a percentage. The overall score for the entire dataset $\mathcal{D</p>
<p>Importantly, if a model fails to complete a specific task, it is assigned a score of 0 for that task, ensuring that incomplete performances are accurately reflected in the overall evaluation. This hierarchical scoring framework ensures a precise and objective assessment of model performance, closely aligning with the rigorous standards expected by professional designers.</p>
<h3>3.3.2 Automated Evaluation</h3>
<p>Automated evaluation is performed on a subset of IDEA-Bench comprising 18 tasks, named IDEA-Bench-mini. During the prompt refinement phase, annotators craft unique evaluation questions for each case, in contrast to the human evaluation setup where all cases within a subtask share the same 6 evaluation questions. This tailored approach allows for more precise and context-specific assessments. Scoring is conducted using Gemini 1.5 Pro [Team et al., 2023], adhering to the same hierarchical scoring methodology outlined in section 3.3.1. To enhance result stability and reliability, each case is evaluated three times independently, and the final score is computed as the average of these three evaluations. This repetition mitigates potential variability in automated assessments and ensures consistent and robust scoring outcomes.</p>
<p>By integrating both human and automated evaluation methods, IDEA-Bench provides a comprehensive framework for assessing the multifaceted capabilities of generative models. This dual approach not only leverages the nuanced judgment of human evaluators but also benefits from the scalability and consistency of automated tools, thereby offering a balanced and thorough evaluation of model performance.</p>
<p>Table 3: Experimental results on Text-to-Image. Each task category is averaged across all its subtasks, with the top-ranked model scores for each subtask highlighted in bold.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Subtasks Score</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Avg. Score</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Arch. Style</td>
<td>Bus. Card</td>
<td>Game UI</td>
<td>Inf. Chart</td>
<td>Int.</td>
<td>Paint.</td>
<td>Sculp.</td>
<td>Ticket</td>
<td>Land.</td>
<td>LOGO</td>
<td>Poster</td>
<td></td>
<td></td>
</tr>
<tr>
<td>FLUX-1 [Labs, 2024b]</td>
<td>100.00</td>
<td>38.89</td>
<td>5.56</td>
<td>0.00</td>
<td>66.67</td>
<td>61.11</td>
<td>16.67</td>
<td>16.67</td>
<td>83.33</td>
<td>61.11</td>
<td>56.67</td>
<td>46.06</td>
<td></td>
</tr>
<tr>
<td>DALL-E 3 [Ramesh et al., 2022]</td>
<td>22.22</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>11.11</td>
<td>100.00</td>
<td>11.11</td>
<td>0.00</td>
<td>38.89</td>
<td>61.11</td>
<td>23.33</td>
<td>24.34</td>
<td></td>
</tr>
<tr>
<td>SD3 [Esser et al., 2024b]</td>
<td>38.89</td>
<td>0.00</td>
<td>5.56</td>
<td>0.00</td>
<td>50.00</td>
<td>5.56</td>
<td>16.67</td>
<td>0.00</td>
<td>50.00</td>
<td>61.11</td>
<td>36.67</td>
<td>24.04</td>
<td></td>
</tr>
<tr>
<td>Pixart [Chen et al., 2023]</td>
<td>5.56</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>22.22</td>
<td>33.33</td>
<td>16.67</td>
<td>0.00</td>
<td>22.22</td>
<td>22.22</td>
<td>36.67</td>
<td>14.44</td>
<td></td>
</tr>
<tr>
<td>Anole [Chern et al., 2024]</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td></td>
</tr>
<tr>
<td>Emu2 [Sun et al., 2024]</td>
<td>22.22</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>38.89</td>
<td>38.89</td>
<td>16.67</td>
<td>0.00</td>
<td>44.44</td>
<td>16.67</td>
<td>20.00</td>
<td>17.98</td>
<td></td>
</tr>
<tr>
<td>OmniGen [Xiao et al., 2024]</td>
<td>38.89</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>27.78</td>
<td>50.00</td>
<td>16.67</td>
<td>0.00</td>
<td>38.89</td>
<td>33.33</td>
<td>30.00</td>
<td>21.41</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: Experimental results on Image-to-Image. Each task category is averaged across all its subtasks, with the top-ranked model scores for each subtask highlighted in bold. " $\dagger$ " represents the use of MLLM for prompt rephrasing.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Subtasks Score</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Avg. Score</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Anim. Atti.</td>
<td>Comp. Icon</td>
<td>ID Photo</td>
<td>Blue</td>
<td>Compl.</td>
<td>Ret.</td>
<td>No.</td>
<td>Light.</td>
<td>Enf.</td>
<td>Obj. Edit</td>
<td>Pack. Resul.</td>
<td>Style Edit</td>
<td>Brand Match.</td>
<td>Avg. Score</td>
</tr>
<tr>
<td>FLUX-11 [Labs, 2024b]</td>
<td>0.00</td>
<td>16.67</td>
<td>0.00</td>
<td>2.78</td>
<td>4.17</td>
<td>0.00</td>
<td>12.50</td>
<td>0.00</td>
<td>100.00</td>
<td>6.25</td>
<td>0.00</td>
<td>4.17</td>
<td>11.11</td>
<td>12.13</td>
</tr>
<tr>
<td>DALL-E 31 [Ramesh et al., 2022]</td>
<td>5.56</td>
<td>11.11</td>
<td>0.00</td>
<td>2.78</td>
<td>0.00</td>
<td>0.00</td>
<td>4.17</td>
<td>0.00</td>
<td>38.89</td>
<td>4.17</td>
<td>8.33</td>
<td>4.17</td>
<td>11.11</td>
<td>6.95</td>
</tr>
<tr>
<td>SD31 [Esser et al., 2024b]</td>
<td>8.33</td>
<td>27.78</td>
<td>0.00</td>
<td>2.78</td>
<td>4.17</td>
<td>0.00</td>
<td>8.33</td>
<td>0.00</td>
<td>55.56</td>
<td>4.17</td>
<td>8.33</td>
<td>4.17</td>
<td>16.67</td>
<td>10.79</td>
</tr>
<tr>
<td>Pixart1 [Chen et al., 2023]</td>
<td>5.56</td>
<td>55.56</td>
<td>4.17</td>
<td>2.78</td>
<td>4.17</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>11.11</td>
<td>4.17</td>
<td>0.00</td>
<td>2.08</td>
<td>11.11</td>
<td>7.75</td>
</tr>
<tr>
<td>InstructPix2Pix [Brooks et al., 2023]</td>
<td>44.44</td>
<td>16.67</td>
<td>16.67</td>
<td>13.89</td>
<td>25.00</td>
<td>54.17</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>10.42</td>
<td>25.00</td>
<td>16.67</td>
<td>5.56</td>
<td>17.58</td>
</tr>
<tr>
<td>MagicBrush [Zhang et al., 2024a]</td>
<td>33.33</td>
<td>16.67</td>
<td>12.50</td>
<td>25.00</td>
<td>25.00</td>
<td>41.67</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>35.42</td>
<td>8.33</td>
<td>33.33</td>
<td>16.67</td>
<td>19.07</td>
</tr>
<tr>
<td>Anole [Chern et al., 2024]</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>8.33</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.64</td>
</tr>
<tr>
<td>Emu2 [Sun et al., 2024]</td>
<td>11.11</td>
<td>0.00</td>
<td>4.17</td>
<td>5.56</td>
<td>0.00</td>
<td>0.00</td>
<td>20.83</td>
<td>0.00</td>
<td>16.67</td>
<td>4.17</td>
<td>8.33</td>
<td>4.17</td>
<td>16.67</td>
<td>7.05</td>
</tr>
<tr>
<td>OmniGen [Xiao et al., 2024]</td>
<td>16.67</td>
<td>0.00</td>
<td>4.17</td>
<td>5.56</td>
<td>0.00</td>
<td>4.17</td>
<td>4.17</td>
<td>0.00</td>
<td>0.00</td>
<td>16.67</td>
<td>0.00</td>
<td>10.42</td>
<td>44.44</td>
<td>8.17</td>
</tr>
</tbody>
</table>
<h1>4 Experiments</h1>
<h3>4.1 Experimental Details</h3>
<h3>4.1.1 Prompts Rephrasing</h3>
<p>Currently, only a few models [Yang et al., 2024a, Wu et al., 2024a, Wang et al., 2024b, Chern et al., 2024] are capable of generating multiple related images simultaneously, makes it difficult to analyse a model's ability to generate multiple images potentially. However, as DALL-E 3 [Ramesh et al., 2022] could perform multi-image generation owing to its MLLM GPT-4o [OpenAI, 2024], which could summarize multimodal input and then generate the corresponding prompt for each image. To empower other leading T2I models [Labs, 2024b, Esser et al., 2024a, Chen et al., 2023], we feed all input images and text prompt to GPT-4o and rephrase prompts for each image to be generated. By combining MLLM's multimodal understanding ability, we can approximately achieve the multi-image generation tasks even with basic T2I models.</p>
<h3>4.1.2 Model Reimplementation</h3>
<p>We evaluate the capabilities of four categories of models in our study. For leading text-to-image (T2I) models, we select FLUX-1 [Labs, 2024b], Stable Diffusion 3 (SD3) [Esser et al., 2024a], Pixart [Chen et al., 2023], and DALL-E 3 [Ramesh et al., 2022]. For image editing models, we choose InstructPix2Pix [Brooks et al., 2023] and MagicBrush Zhang et al. [2024a] To assess models with unified generation capabilities, we select Emu2 [Sun et al., 2024] and OmniGen [Xiao et al., 2024]. Additionally, for models that support interleaved text and image generation, we include Anole [Chern et al., 2024]. Notably, among the selected models, Anole [Chern et al., 2024] is the only one capable of handling an unrestricted number of input and output images. We endeavor to utilize the officially released model parameters and inference settings wherever possible; further details are provided in section 6.2.</p>
<h3>4.2 Results on IDEA-Bench</h3>
<p>Qualitative Analysis Figure 4 showcases the generated images across 7 models. For ease of comparison, we select several cases from three categories: text-to-image, image-to-image, and images-to-image. In fundamental text-toimage tasks, FLUX-1 [Labs, 2024b] demonstrates a clear advantage over other models, effectively comprehending the visual elements and professional-level requirements outlined in the prompts. Regarding image editing tasks, models that are not specifically designed for image editing struggle to preserve essential information from the original image. Emu2 [Sun et al., 2024] is the only model that roughly restores the shapes and compositions of objects from the original image. OmniGen [Xiao et al., 2024] excels in maintaining the identity of objects. However, for more complex tasks such as special effect synthesis, no current model sufficiently understands the intricate demands of these professional-level requirements.</p>
<p>Table 5: Experimental results on Images-to-Image. Each task category is averaged across all its subtasks, with the top-ranked model scores for each subtask highlighted in bold. " $\dagger$ " represents the use of MLLM for prompt rephrasing.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Subtasks Score</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg. Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human Attr.</td>
<td style="text-align: center;">Image Trans.</td>
<td style="text-align: center;">Pan.</td>
<td style="text-align: center;">Text Edit.</td>
<td style="text-align: center;">3D. Effect</td>
<td style="text-align: center;">Image Blend.</td>
<td style="text-align: center;">Spec. Effect</td>
<td style="text-align: center;">Real and Anime</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FLUX-11 [Labs, 2024b]</td>
<td style="text-align: center;">3.97</td>
<td style="text-align: center;">6.67</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">10.42</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">12.50</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4.89</td>
</tr>
<tr>
<td style="text-align: center;">DALL-E 31 [Ramesh et al., 2022]</td>
<td style="text-align: center;">4.76</td>
<td style="text-align: center;">3.33</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">6.25</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5.27</td>
</tr>
<tr>
<td style="text-align: center;">SD31 [Esser et al., 2024b]</td>
<td style="text-align: center;">4.76</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4.69</td>
</tr>
<tr>
<td style="text-align: center;">Pixart1 [Chen et al., 2023]</td>
<td style="text-align: center;">4.76</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.48</td>
</tr>
<tr>
<td style="text-align: center;">Anole [Chern et al., 2024]</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;">Emu2 [Sun et al., 2024]</td>
<td style="text-align: center;">6.35</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">2.08</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">9.26</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.98</td>
</tr>
<tr>
<td style="text-align: center;">OmniGen [Xiao et al., 2024]</td>
<td style="text-align: center;">1.59</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">2.08</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">11.11</td>
<td style="text-align: center;">7.41</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.77</td>
</tr>
</tbody>
</table>
<p>Table 6: Experimental results on Text-to-Images. Each task category is averaged across all its subtasks, with the top-ranked model scores for each subtask highlighted in bold. " $\dagger$ " represents the use of MLLM for prompt rephrasing.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Subtasks Score</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg. Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Anim. Grow.</td>
<td style="text-align: center;">Child. Book</td>
<td style="text-align: center;">Draw. Proc.</td>
<td style="text-align: center;">Hist. Num.</td>
<td style="text-align: center;">Movie Shots</td>
<td style="text-align: center;">Play. Lasso.</td>
<td style="text-align: center;">Plant Grow.</td>
<td style="text-align: center;">Style Group</td>
<td style="text-align: center;">Conc. Vis.</td>
<td style="text-align: center;">Comp. Icon.</td>
<td style="text-align: center;">Dyn. Chat.</td>
</tr>
<tr>
<td style="text-align: center;">FLUX-11 [Labs, 2024b]</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">70.83</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">7.08</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">22.92</td>
</tr>
<tr>
<td style="text-align: center;">DALL-E 31 [Ramesh et al., 2022]</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">41.67</td>
<td style="text-align: center;">32.92</td>
<td style="text-align: center;">12.50</td>
<td style="text-align: center;">41.67</td>
<td style="text-align: center;">16.67</td>
</tr>
<tr>
<td style="text-align: center;">SD31 [Esser et al., 2024b]</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">37.50</td>
<td style="text-align: center;">12.50</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">45.83</td>
<td style="text-align: center;">18.75</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">41.67</td>
<td style="text-align: center;">18.75</td>
</tr>
<tr>
<td style="text-align: center;">Pixart1 [Chen et al., 2023]</td>
<td style="text-align: center;">37.50</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">29.17</td>
<td style="text-align: center;">21.25</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">45.83</td>
<td style="text-align: center;">16.67</td>
</tr>
<tr>
<td style="text-align: center;">Anole [Chern et al., 2024]</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">2.08</td>
</tr>
</tbody>
</table>
<p>We first present the ranking of all models based on their scores across the entire dataset, as detailed in table 2. Leveraging GPT-4o’s [OpenAI, 2024] multimodal understanding and task translation capabilities, FLUX-1 [Labs, 2024b] achieve the highest score of 22.46, significantly outperforming other models. Specialized image editing models [Brooks et al., 2023, Zhang et al., 2024a] lead in the image-to-image tasks, while the general-purpose generation model Emu2 [Sun et al., 2024] excels in the images-to-image tasks. In contrast, Anole [Chern et al., 2024], which is capable of generating multiple images simultaneously, do not achieve an ideal final score. Furthermore, no model surpass a score of 10 without utilizing the comprehension capabilities of MLLMs, indicating that general-purpose image generation models still face substantial challenges in achieving professional-grade performance. This underscores the need for comprehensive evaluation standards to guide the development of more specialized and capable generative models.</p>
<p>To further quantify the impact of MLLM [OpenAI, 2024] assistance on model capabilities within the benchmark, we proceed text-to-images and image(s)-to-images tasks that not supported by Emu2 [Sun et al., 2024] and OmniGen [Xiao et al., 2024] using GPT-4o-rephrased prompts. The results are supplemented at the bottom of table 2. Despite enhancing Emu2 and OmniGen's capabilities with MLLMs, FLUX-1 [Labs, 2024b] remains the top performer, as shown in table 12. Notably, FLUX-1 even surpasses other models that support image input in image-to-image tasks. The advantage of T2I models [Labs, 2024b, Ramesh et al., 2022, Esser et al., 2024a, Chen et al., 2023] in this experiment lies in their ability to leverage MLLMs to understand different tasks. IDEA-Bench's task definitions are highly specialized, making it difficult for other models to comprehend these tasks without relying on MLLM. Universal generative models also have difficult ensuring the quality of generated images, resulting in lower scores finally. Overall, to achieve high scores across all benchmark tasks, a model must possess both multimodal input-output capabilities and robust MLLM-level multimodal understanding. The following section provides a detailed analysis of each category.</p>
<h1>4.2.1 Results on Text-to-Image</h1>
<p>All models' results on text-to-image tasks are presented in table 3. Notably, the FLUX-1 [Labs, 2024b] model significantly outperforms the others, effectively accomplishing the task objectives in the majority of tasks. Based on the task settings and each model's performance, we observe the following:</p>
<ul>
<li>Text Generation Challenges: Text generation tasks are a common pain point for all models. Even though models can generate specific visual elements with high quality and understand style instructions specified in the prompts, they struggle to incorporate text into images as seamlessly as human designers. For example, in the business card generation task, only FLUX-1 [Labs, 2024b] received scores.</li>
<li>Performance of Basic vs. General Models: Basic text-to-image models generally demonstrate higher task completion rates compared to general models like Emu2 [Sun et al., 2024] and OmniGen [Xiao et al., 2024]. This is expected, as general models prioritize enhancing task generalization and multimodal information processing capabilities, which inevitably compromises some aspects of image generation quality.</li>
<li>Poor Performance on Information Charts: All models perform poorly on the information chart generation task, receiving a score of 0 from human evaluators. The primary reason is that generative models struggle to</li>
</ul>
<p>Table 7: Experimental results on Image(s)-to-Images. Each task category is averaged across all its subtasks, with the top-ranked model scores for each subtask highlighted in bold. " $\dagger$ " represents the use of MLLM for prompt rephrasing.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Subtasks Score</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg. Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Anim. Grow.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Creat. Trans.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Layer Decomp.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Light. Effect</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Movie Shots</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-app.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-dec.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-view Trans.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FLUX-1 $\dagger$ [Labs, 2024b]</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66.67</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DALL-E 3 $\dagger$ [Ramesh et al., 2022]</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SD3 $\dagger$ [Esser et al., 2024b]</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Pixar $\dagger$ [Chen et al., 2023]</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">58.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">58.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Anole [Chen et al., 2024]</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 8: Experimental results on the subset IDEA-Bench-mini. Each task category is averaged across all its subtasks, with the top-ranked model scores for each subtask highlighted in bold. "G." represents evaluate using Gemini 1.5 pro [Team et al., 2023], "H." represents human evaluation. " $\dagger$ " represents the use of MLLM for prompt rephrasing.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">CLIPScore</th>
<th style="text-align: center;">Evaluation Category</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg. Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T2I</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">12I</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Is2I</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T2Is</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I(s)2Is</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">G.</td>
<td style="text-align: center;">H.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">G.</td>
<td style="text-align: center;">H.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">G.</td>
<td style="text-align: center;">H.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">G.</td>
<td style="text-align: center;">H.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">G.</td>
<td style="text-align: center;">H.</td>
<td style="text-align: center;">G.</td>
<td style="text-align: center;">H.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FLUX-1 $\dagger$ [Labs, 2024b]</td>
<td style="text-align: center;">0.3432</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">83.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">67.04</td>
<td style="text-align: center;">28.71</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">4.86</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6.25</td>
<td style="text-align: center;">32.29</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">28.13</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">37.50</td>
<td style="text-align: center;">38.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">37.48</td>
</tr>
<tr>
<td style="text-align: center;">DALL-E 3 $\dagger$ [Ramesh et al., 2022]</td>
<td style="text-align: center;">0.3462</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">55.56</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">41.11</td>
<td style="text-align: center;">27.78</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">20.37</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">29.17</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">21.88</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">29.17</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">32.41</td>
</tr>
<tr>
<td style="text-align: center;">SD3 $\dagger$ [Esser et al., 2024b]</td>
<td style="text-align: center;">0.3483</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">56.30</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">39.26</td>
<td style="text-align: center;">23.61</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.34</td>
<td style="text-align: center;">5.55</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">32.29</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.96</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">31.25</td>
<td style="text-align: center;">29.80</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">29.80</td>
</tr>
<tr>
<td style="text-align: center;">Pixar $\dagger$ [Chen et al., 2023]</td>
<td style="text-align: center;">0.3435</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">26.30</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">27.04</td>
<td style="text-align: center;">30.55</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">29.86</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.09</td>
<td style="text-align: center;">35.42</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.96</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">31.25</td>
<td style="text-align: center;">30.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">30.68</td>
</tr>
<tr>
<td style="text-align: center;">InstructPix2Pix [Brooks et al., 2023]</td>
<td style="text-align: center;">0.1194</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">27.78</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5.56</td>
</tr>
<tr>
<td style="text-align: center;">MagicBrush [Zhang et al., 2024a]</td>
<td style="text-align: center;">0.1212</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.72</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.34</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.94</td>
</tr>
<tr>
<td style="text-align: center;">Emu2 [Sun et al., 2024]</td>
<td style="text-align: center;">0.1821</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">35.80</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">27.04</td>
<td style="text-align: center;">43.52</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.34</td>
<td style="text-align: center;">34.72</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.09</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">22.81</td>
</tr>
<tr>
<td style="text-align: center;">OmniGen [Xiao et al., 2024]</td>
<td style="text-align: center;">0.1799</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">46.30</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">34.07</td>
<td style="text-align: center;">40.27</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">11.11</td>
<td style="text-align: center;">25.46</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">22.41</td>
</tr>
</tbody>
</table>
<p>accurately transfer large amounts of text from the input to the output image, which is a basic ability to human designers. Additionally, many models do not support ultra-long text inputs.</p>
<h1>4.2.2 Results on Image-to-Image \&amp; Images-to-Image</h1>
<p>The image-to-image tasks represent the strengths of image editing models. In table 4, MagicBrush [Zhang et al., 2024a] and InstructPix2Pix [Brooks et al., 2023] achieve the first and second highest evaluation scores, respectively. The images-to-image tasks involve more complex and diverse guidance or conditions, with Emu2 [Sun et al., 2024] attaining the highest score of 8.98 , as shwon in table 5 . From these results, we derive the following key insights:</p>
<ul>
<li>Pros and Cons of Image Editing Models: In local editing tasks such as image blur and image retouching, image editing models excel at preserving the original features of the image. However, these models are typically trained or fine-tuned on specific image editing datasets, which diminishes their ability to perform subject-driven tasks, such as branded merchandise generation.</li>
<li>In-Context Learning Capabilities: Emu2 [Sun et al., 2024] and OmniGen [Xiao et al., 2024] exhibit in-context learning abilities, as evidenced by OmniGen's significantly higher scores in tasks like branded merchandise generation and special effect synthesis, achieving ID consistency between input and output images. Emu2 demonstrates the ability to comprehend and partially complete the image straighten task, indicating that the model possesses an understanding of the semantic aspects of visual elements.</li>
<li>Challenges for Generative Models: General-purpose models perform poorly in tasks such as light condition editing. Although these tasks are frequently encountered by designers, the models require a certain level of understanding of real-world physical models, presenting a challenge for future generative models.</li>
</ul>
<h3>4.2.3 Results on Text-to-Images \&amp; Image(s)-to-Images</h3>
<p>Table 6 and table 7 present the evaluation results for text-to-images and select image(s)-to-images tasks, respectively. In both categories, SD3 [Esser et al., 2024a] and FLUX-1 [Labs, 2024b] achieve the highest scores. For tasks such as historical event generation, MLLMs can effectively convey detailed style requirements during the prompt rephrasing phase, resulting in relatively consistent style outcomes. However, for more complex tasks like children's book generation that involve sophisticated capabilities such as consistency and preservation over ID and style, existing models struggle to perform adequately even when assisted by MLLMs. Nonetheless, generating multiple related images holds significant value; for example, the generated movie shots can serve as guiding conditions for multi-shot video generation models. We aim for the task types designed in our benchmark to align generative models with the capabilities of human designers, thereby further advancing toward general AI.</p>
<h1>4.2.4 Results on IDEA-Bench-mini</h1>
<p>Table 8 presents the evaluation results for a subset of 18 tasks within our benchmark, where we calculate CLIPScore [Hessel et al., 2021], Gemini 1.5 Pro [Team et al., 2023] scores, and human evaluation scores. As shown in table 8, FLUX-1 [Labs, 2024b], leveraging MLLM's prompt rephrasing capabilities, achieve the highest scores in both MLLMbased and human evaluations within this subset. In contrast, when excluding the use of MLLMs, Emu2 [Sun et al., 2024] secure the top score. These experimental results are consistent with the overall benchmark dataset findings. Additionally, compared to CLIPScore, MLLM-based evaluations more closely align with human preferences in terms of professional evaluation data and assessment criteria. This suggests that as MLLM capabilities continue to evolve, utilizing MLLMs for image generation evaluation will become increasingly stable and reliable.</p>
<h2>5 Conclusion</h2>
<p>We introduce IDEA-Bench, a comprehensive benchmark designed to bridge the gap between current generative model capabilities and the stringent demands of professional-grade image design. IDEA-Bench encompasses 100 professional image generation tasks across five distinct categories, utilizing a detailed evaluation framework to ensure precise and objective assessments. Additionally, we build a representative subset IDEA-Bench-mini that facilitates automated evaluation using MLLMs. Our evaluations reveal significant gaps in existing models, particularly in handling intricate instructions and maintaining consistency in multi-image generation, underscoring the need for more advanced and versatile models. By aligning evaluation standards with the nuanced requirements of human designers, IDEABench serves as a pivotal tool for guiding the development of generative models toward achieving professional-grade performance and advancing toward general artificial intelligence with autonomous and sophisticated visual generation capabilities.</p>
<h1>References</h1>
<p>Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821-8831. Pmlr, 2021.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.
Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873-12883, 2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479-36494, 2022.
James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.</p>
<p>Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024a.
Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024.
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.
Black Forest Labs. Flux: Inference repository. https://github.com/black-forest-labs/flux, 2024a. Accessed: 2024-10-25.
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836-3847, 2023a.
Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 4296-4304, 2024.
Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392-18402, 2023.</p>
<p>Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8871-8879, 2024.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.
Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.</p>
<p>Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024. URL https://llava-vl. github.io/blog/2024-05-10-llava-next-stronger-llms/.
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a.
OpenAI. Gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023.
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. In The Twelfth International Conference on Learning Representations, 2023.
Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14398-14409, 2024.
Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b.
Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024.
Lianghua Huang, Wei Wang, Zhi-Fan Wu, Huanzhang Dou, Yupeng Shi, Yutong Feng, Chen Liang, Yu Liu, and Jingren Zhou. Group diffusion transformers are unsupervised multitask learners. arXiv preprint arXiv:2410.15027, 2024.
Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3043-3054, 2023.</p>
<p>Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024.
Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20406-20417, 2023.
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22500-22510, 2023.
Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models. arXiv preprint arXiv:2310.01596, 2023.
Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal long story generation with large language model. arXiv preprint arXiv:2407.08683, 2024a.
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Proceedings of the International Conference on Machine Learning, pages 53366-53397, 2024a.
Min Jin Chong and David Forsyth. Effectively unbiased fid and inception score and where to find them. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6070-6079, 2020.</p>
<p>Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024b. URL https://arxiv.org/abs/2403.03206.
Black Forest Labs. Flux.1, 2024b. URL https://github.com/black-forest-labs/flux.
Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36, 2024a.
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv, 2022.
Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024.
Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model, 2024.
Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024.
Vitali Petsiuk, Alexander E Siemenn, Saisamrit Surbehera, Zad Chin, Keith Tyser, Gregory Hunter, Arvind Raghavan, Yann Hicke, Bryan A Plummer, Ori Kerret, et al. Human evaluation of text-to-image models on a multi-task benchmark. arXiv preprint arXiv:2211.12112, 2022.
Eslam Mohamed Bakr, Pengzhan Sun, Xiaoqian Shen, Faizan Farooq Khan, Li Erran Li, and Mohamed Elhoseiny. Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20041-20053, 2023.
Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36: 78723-78747, 2023.
Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-image models. Advances in Neural Information Processing Systems, 36, 2024.
Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, and Lijuan Wang. Designbench: Exploring and benchmarking dall-e 3 for imagining visual design. arXiv preprint arXiv:2310.15144, 2023.
Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024.
Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideos: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024b.
Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin, Xinggang Wang, Chang Zhou, and Jingren Zhou. Touchstone: Evaluating vision-language models by language models. arXiv preprint arXiv:2308.16890, 2023.</p>
<p>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pages 216-233. Springer, 2025.
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.
Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution realworld scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024b.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022.
Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and Gordon Wetzstein. Gpt-4v (ision) is a human-aligned evaluator for text-to-3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22227-22238, 2024b.
Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda Ruth Petzold. Gpt-4v (ision) as a generalist evaluator for vision-language tasks. arXiv preprint arXiv:2311.01361, 2023b.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595-46623, 2023.</p>
<h1>6 Implementation Details</h1>
<p>In this section, we detail the methods used for IDEA-Bench construction and experimental anlyses to ensure reproducibility. Section 6.1 provides example instructions for utilizing GPT-4o [OpenAI, 2024] in the construction of IDEA-Bench, while section 6.2 outlines the experimental configurations.</p>
<h3>6.1 IDEA-Bench Construction Instruction</h3>
<p>Instruction for prompt rephrasing As mentioned in section 4.1.1, to closely align with real design scenarios, IDEABench includes multi-image generation tasks that most existing models do not support. To thoroughly evaluate current generative models' capabilities in these tasks, we utilize one of the most advanced MLLMs, GPT-4o [OpenAI, 2024], to rephrase multimodal inputs (which may include multiple images and complex long texts) into several text-to-image prompts. The specific rephrasing instruction is illustrated in fig. 7. However, transforming tasks through rephrasing is merely a workaround, as text alone cannot capture all the details of the given images. Human designers have the ability to autonomously extract information from images and transform it into outputs in a freeform manner. We aim for IDEA-Bench to drive future generative models to acquire this capability.</p>
<p>Instruction for evaluation question construction After collecting the task data, we generate evaluation questions in bulk by combining task keywords provided by human annotators with GPT-4o [OpenAI, 2024]. Figure 8 illustrates an example of the instruction for generating evaluation questions for image(s)-to-images tasks. In fig. 8, the red sections indicate prompts that need to be customized for each specific task, while the JSON format templates are omitted. Within the fixed prompts, we first outline the basic requirements for the evaluation questions, such as multi-level standards, the exclusive use of objective judgment questions, and the convention that a score of 1 signifies a better result compared to 0 . After incorporating the fundamental task definitions provided by annotators, the prompts also include frequently occurring evaluation capability keywords specific to multi-image generation tasks. This ensures that the evaluation questions defined by GPT-4o maintain a professional standard.</p>
<h3>6.2 Inference Configuration</h3>
<p>Table 9 details the configurations applied during inference for all models. To ensure fairness, all diffusion-based models employ 50 sampling steps (DALL-E 3 [Ramesh et al., 2022] utilizes the official API and is therefore excluded from the statistics). Notably, Anole's visual decoder is not diffusion-based [Chern et al., 2024]; instead, it employs a diffusion-free, token-based architecture. We adhere to the text guidance scale and image guidance scale recommended by the official project codes, as illustrated in table 9.</p>
<h2>7 Statistical Analysis</h2>
<p>Figure 1 visualizes the distribution of all subtasks across categories. In this section, we further conduct statistical analyses on the composition of the prompts and evaluation criteria of IDEA-Bench.</p>
<p>Distribution of prompt length. In fig. 5, we present the distribution of prompt lengths across the five task categories using histograms. According to the statistics in table 1, IDEA-Bench's prompts have an average length of approximately 139 words. Prompts shorter than the average are primarily found in the image-to-image and images-to-image tasks, as these tasks rely heavily on input images to guide the final generation, reducing the need for extensive textual descriptions. However, the prompt lengths for these two categories still significantly exceed those of other benchmarks [Cho et al., 2023, Ghosh et al., 2024, Sheynin et al., 2024, Hu et al., 2023, Ruiz et al., 2023, Ku et al., 2023]. Additionally, both text-to-image and image-to-images tasks feature excessively long prompts, due to the requirements for complex and rich visual elements or detailed descriptions for multiple generated images.</p>
<p>Distribution of evaluation ability. We conduct a statistical analysis of the evaluation dimensions involved in each subtask within every category, with the results illustrated in the figure. In fig. 6, a higher value for a dimension indicates that the category places greater emphasis on assessing the model's capabilities in that dimension. The analysis reveals that all five categories prioritize the evaluation of aesthetic aspects and the quality of the association between the generated images and the details in the prompts. Specifically, text-to-image tasks emphasize assessments of style, image composition, and text quality. In contrast, image-to-image and images-to-image tasks focus on evaluating the retention of elements between the input and output images. Meanwhile, text-to-images and image(s)-to-images tasks, which involve generating multiple images, concentrate on evaluating dimensions such as ID consistency and style consistency among the generated images.</p>
<p>Table 9: Inference details of the models being tested. "-" indicates either an API call or the absence of relevant parameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Param.</th>
<th style="text-align: center;">DiT based</th>
<th style="text-align: center;">Text Guid. Scale</th>
<th style="text-align: center;">Image Guid. Scale</th>
<th style="text-align: center;">Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FLUX-1 [Labs, 2024b]</td>
<td style="text-align: center;">12B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">DALL-E 3 [Ramesh et al., 2022]</td>
<td style="text-align: center;">12B</td>
<td style="text-align: center;">$\boldsymbol{\lambda}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">SD3 [Esser et al., 2024a]</td>
<td style="text-align: center;">2B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">Pixart [Chen et al., 2023]</td>
<td style="text-align: center;">0.6B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">InstructPix2Pix [Brooks et al., 2023]</td>
<td style="text-align: center;">1B</td>
<td style="text-align: center;">$\boldsymbol{\lambda}$</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">MagicBrush [Zhang et al., 2024a]</td>
<td style="text-align: center;">1B</td>
<td style="text-align: center;">$\boldsymbol{\lambda}$</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">Emu2 [Sun et al., 2024]</td>
<td style="text-align: center;">37B</td>
<td style="text-align: center;">$\boldsymbol{\lambda}$</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">OmniGen [Xiao et al., 2024]</td>
<td style="text-align: center;">3.8B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">Anole [Chern et al., 2024]</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 10: Experimental results on Image(s)-to-Images. Each task category is averaged across all its subtasks, with the top-ranked model scores for each task type highlighted in bold. Task types that a model cannot support are marked with "-". " $\dagger$ " represents the use of MLLM for prompt rephrasing.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Subtasks Score</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg. Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Paint. Undo</td>
<td style="text-align: center;">Same Pose</td>
<td style="text-align: center;">Three-view Trans.</td>
<td style="text-align: center;">Child. Book</td>
<td style="text-align: center;">Plant Growth</td>
<td style="text-align: center;">Prod. Usage Scen.</td>
<td style="text-align: center;">Stop-motion Anim.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FLUX-11 [Labs, 2024b]</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">45.83</td>
<td style="text-align: center;">41.67</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">29.17</td>
</tr>
<tr>
<td style="text-align: center;">DALL-E 31 [Ramesh et al., 2022]</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">37.50</td>
<td style="text-align: center;">58.30</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">14.44</td>
</tr>
<tr>
<td style="text-align: center;">Stable Diffusion 31 [Esser et al., 2024b]</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">29.17</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">13.06</td>
</tr>
<tr>
<td style="text-align: center;">Pixart1 [Chen et al., 2023]</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">37.50</td>
<td style="text-align: center;">41.67</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">16.67</td>
<td style="text-align: center;">21.39</td>
</tr>
<tr>
<td style="text-align: center;">InstructPix2Pix [Brooks et al., 2023]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MagicBrush [Zhang et al., 2024a]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Anole [Chern et al., 2024]</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;">Emu2 [Sun et al., 2024]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OmniGen [Xiao et al., 2024]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 11: Comparison of evaluation failure rates among different MLLMs. For each evaluation question, MLLMs score the model-generated outputs three times. If none of the three scores return the required value ( 0 or 1 ), the evaluation is considered a failure.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Eval. MLLM</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FLUX-1</td>
<td style="text-align: center;">DALL-E 3</td>
<td style="text-align: center;">SD3</td>
<td style="text-align: center;">Pixart</td>
<td style="text-align: center;">InstructPix2Pix</td>
<td style="text-align: center;">MagicBrush</td>
<td style="text-align: center;">Emu2</td>
<td style="text-align: center;">OmniGen</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Gemini 1.5 pro [Team et al., 2023]</td>
<td style="text-align: center;">$0.33 \%$</td>
<td style="text-align: center;">$1.63 \%$</td>
<td style="text-align: center;">$1.96 \%$</td>
<td style="text-align: center;">$0.00 \%$</td>
<td style="text-align: center;">$0.00 \%$</td>
<td style="text-align: center;">$0.00 \%$</td>
<td style="text-align: center;">$0.67 \%$</td>
<td style="text-align: center;">$1.33 \%$</td>
<td style="text-align: center;">$0.95 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o [OpenAI, 2024]</td>
<td style="text-align: center;">$52.29 \%$</td>
<td style="text-align: center;">$54.58 \%$</td>
<td style="text-align: center;">$52.95 \%$</td>
<td style="text-align: center;">$56.86 \%$</td>
<td style="text-align: center;">$16.67 \%$</td>
<td style="text-align: center;">$26.67 \%$</td>
<td style="text-align: center;">$54.00 \%$</td>
<td style="text-align: center;">$53.33 \%$</td>
<td style="text-align: center;">$52.84 \%$</td>
</tr>
</tbody>
</table>
<h1>8 Additional Experiments</h1>
<p>Supplementary results on image(s)-to-images Due to space constraints, we do not include all experimental results for the image(s)-to-images category in table 7. Supplementary results are provided in table 10. The current abilities of all models to achieve inter-image associations like ID consistency and style consistency stem from GPT-4o's [OpenAI, 2024] detailed rephrasing of each prompt, akin to the group image descriptions in GDT [Huang et al., 2024]. However, GDT employs a design where image tokens are concatenated during attention computation, whereas solely using MLLM rephrasing does not facilitate inter-image association modeling in the latent space. In the future, to enable multi-image generation tasks with complex associations, models will need to consider parallel generation of multiple images or utilize partially generated images as input conditions to guide the generation of subsequent images.</p>
<p>GPT or Gemini for Evaluation? On a subset of the dataset, we select Gemini 1.5 Pro [Team et al., 2023] to score the images generated by models based on the refined evaluation questions. However, MLLMs produce free-form textual outputs, making it challenging to ensure binary scores of 0 or 1 as human annotators do, potentially resulting in scoring failures. In table 11, we report the failure rates of Gemini 1.5 pro [Team et al., 2023] and GPT-4o [OpenAI, 2024], representing the proportion of evaluation questions where scoring failed. Specifically, we conduct three evaluations per question. If all three attempts do not yield a clear score, the evaluation is considered a failure. Across all models and evaluation questions, Gemini 1.5 pro exhibits a low failure rate of $0.95 \%$, whereas GPT-4o shows a high failure rate of $52.84 \%$, rendering it unsuitable as a reliable automated evaluation model. In practice, GPT-4o frequently responds with phrases such as "I'm sorry, I can't assist with that", whereas Gemini 1.5 pro provides more consistent responses.</p>
<p>Comparison of T2I capabilities across all models We also apply prompt rephrasing to all models in text-to-image generation settings. In this setup, all models have unified input comprehension capabilities, evaluating whether they can accurately translate prompts into high-quality generated images. The results are included in table 12, featuring Emu2</p>
<p>Table 12: Additional experimental results on all categories of IDEA-Bench. " $\dagger$ " represents the use of MLLM for prompt rephrasing. All models perform text-to-image generation on prompts rephrased by GPT-4o.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Scores on All Categories</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg. Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T2I</td>
<td style="text-align: center;">12I</td>
<td style="text-align: center;">Is2I</td>
<td style="text-align: center;">T2Is</td>
<td style="text-align: center;">$\mathrm{I}(\mathrm{s}) 2 \mathrm{Is}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FLUX-1| [Labs, 2024b]</td>
<td style="text-align: center;">46.06</td>
<td style="text-align: center;">12.13</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">20.15</td>
<td style="text-align: center;">29.17</td>
<td style="text-align: center;">22.48</td>
</tr>
<tr>
<td style="text-align: center;">DALL-E 3| [Ramesh et al., 2022]</td>
<td style="text-align: center;">24.34</td>
<td style="text-align: center;">6.95</td>
<td style="text-align: center;">5.26</td>
<td style="text-align: center;">14.36</td>
<td style="text-align: center;">14.44</td>
<td style="text-align: center;">13.07</td>
</tr>
<tr>
<td style="text-align: center;">Stable Diffusion 3| [Esser et al., 2024a]</td>
<td style="text-align: center;">24.04</td>
<td style="text-align: center;">10.79</td>
<td style="text-align: center;">4.69</td>
<td style="text-align: center;">21.59</td>
<td style="text-align: center;">13.06</td>
<td style="text-align: center;">14.83</td>
</tr>
<tr>
<td style="text-align: center;">Pixart $\dagger$ [Chen et al., 2023]</td>
<td style="text-align: center;">14.44</td>
<td style="text-align: center;">7.75</td>
<td style="text-align: center;">3.48</td>
<td style="text-align: center;">17.46</td>
<td style="text-align: center;">21.39</td>
<td style="text-align: center;">12.90</td>
</tr>
<tr>
<td style="text-align: center;">Anole-T2I $\dagger$ [Chern et al., 2024]</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">3.10</td>
<td style="text-align: center;">1.26</td>
<td style="text-align: center;">8.98</td>
<td style="text-align: center;">8.89</td>
<td style="text-align: center;">4.45</td>
</tr>
<tr>
<td style="text-align: center;">Emu2-T2I $\dagger$ [Sun et al., 2024]</td>
<td style="text-align: center;">17.98</td>
<td style="text-align: center;">3.15</td>
<td style="text-align: center;">2.34</td>
<td style="text-align: center;">15.53</td>
<td style="text-align: center;">12.78</td>
<td style="text-align: center;">10.36</td>
</tr>
<tr>
<td style="text-align: center;">OmniGen-T2I $\dagger$ [Xiao et al., 2024]</td>
<td style="text-align: center;">21.41</td>
<td style="text-align: center;">6.09</td>
<td style="text-align: center;">4.50</td>
<td style="text-align: center;">23.52</td>
<td style="text-align: center;">21.39</td>
<td style="text-align: center;">15.38</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Statistics of prompt lengths for all tasks in IDEA-Bench. Each of the five task categories is represented by a distinct color. Prompt lengths are divided into five intervals, and the y -axis shows the number of tasks that fall within each interval.
[Sun et al., 2024], OmniGen [Xiao et al., 2024], and Anole [Chern et al., 2024], each distinguished by the "-T2I" suffix. FLUX-1 [Labs, 2024b] remains the top-ranked model. FLUX-1 demonstrates a strong ability to convert prompts into images, maintaining stable image quality with only rare instances of failure.</p>
<h1>9 Data Examples</h1>
<p>Additional model-generated results are demonstrated in fig. 9 - fig. 13, including the input images and text prompts used. Some text prompts are omitted due to their length. Since different models support a limited number of task categories, we only showcase the models that are capable of handling each respective category in the generation results.
Examples of automated evaluations conducted using Gemini 1.5 pro [Team et al., 2023] are illustrated in fig. 14 and fig. 15. Due to the detailed definitions of the generation prompts and evaluation questions, the evaluation process can be effectively transformed into a multimodal understanding task, which MLLM excels at. In both presented examples, the model-generated results fail to fully meet the prompt requirements, resulting in a score of 0 .</p>
<h2>10 Limitations \&amp; Future Work</h2>
<p>Due to the current capabilities of multimodal large language models (MLLMs) still falling short of human performance, we are unable to apply automated MLLM evaluations to all tasks while meeting the evaluation standards of professional designers. Furthermore, the primary goal of IDEA-Bench is to bridge the gap between current generative models and professional tasks, pushing model capabilities toward a professional level. However, there remains a significant distance to match the proficiency of professional designers. In the future, we will focus on updating and maintaining IDEA-Bench, continuously refining automated evaluation methods in line with the real-time advancements of MLLMs, and expanding to more specialized tasks. This will ensure that the benchmark effectively supports the ongoing evolution of generative model capabilities.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Statistics of evaluation dimensions for all tasks in IDEA-Bench. Each of the five task categories is represented by a distinct color. A total of 12 evaluation dimensions are analyzed, with the radar chart values indicating the proportion of evaluation questions related to each dimension within each category.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: An instruction example for prompt rephrasing. The red sections indicate customization for different subtasks. The JSON file format templates within the instructions are not shown.</p>
<p>You are a professor at a design school, and the following is an exam question. You need to design 6 evaluation questions for a task where the model generates multiple images based on a text description and multiple referencing images to assess the output. The questions should follow these two principles:
a. There should be a total of 6 questions, and each question must be a clear, binary judgment ( 0 points or I point). Each question should first specify the evaluation aspect and details, followed by clear criteria for scoring 0 points and I point. A score of I point represents a better outcome, and 0 points represent a worse outcome.
b. The first two questions should focus on whether the model output meets the basic definition of the task. The next two questions should further evaluate whether the model output follows specific instructions from the text input. For example, if the text input specifies making the skin smoother, the model needs to clearly fulfill that directive. The last two questions should evaluate the images from a high-level professional perspective, assessing aspects like image detail, aesthetics, and so on.</p>
<p>Please design 6 evaluation questions for the "multi-appearance variant generation" task. This task is to generate multiple appearance color renderings of an object. Ensure the questions are created from a professional and comprehensive perspective, considering the following aspects:</p>
<ul>
<li>If the task involves partial modification, does the rest of the image remain unchanged?</li>
<li>The correlation between input and output images (content, style, ID, etc.)</li>
<li>Does it meet the requirements of the text description?</li>
<li>The quality of text editing within the image.</li>
<li>Chronological logo.</li>
<li>Consistency of image style.</li>
<li>Consistency of image ID (recognizable as the same person or object).</li>
<li>Logical consistency.</li>
<li>Alignment with the text description.</li>
<li>Is the output derived from the input image, or is it entirely unrelated?</li>
</ul>
<p>Don't limit yourself to these aspects, and also consider whether additional criteria are relevant to this task. Please provide the 6 evaluation questions you have defined. Output in JSON format. [omitted here]
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: An instruction example for generating evaluation questions. The red sections indicate customization for different subtasks. The JSON file format templates within the instructions are not shown.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*This work was done during interships at Tongyi Lab.
* Emails: liangchen2022@ia.ac.cn, xuangen.hlh@alibaba-inc.com, u202143361@xs.ustb.edu.cn, hzdou@zju.edu.cn, {ww413411, wuzhifan.wzf}@alibaba-inc.com, shiyupeng.syp@taobao.com, jgzhang@nlpr.ia.ac.cn, xinzhao@ustb.edu.cn, ly103369@alibabainc.com.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4380 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4380</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4380</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-281411574</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.15292v1.pdf" target="_blank">An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature</a></p>
                <p><strong>Paper Abstract:</strong> We propose an automated pipeline for performing literature reviews using semantic similarity. Unlike traditional systematic review systems or optimization based methods, this work emphasizes minimal overhead and high relevance by using transformer based embeddings and cosine similarity. By providing a paper title and abstract, it generates relevant keywords, fetches relevant papers from open access repository, and ranks them based on their semantic closeness to the input. Three embedding models were evaluated. A statistical thresholding approach is then applied to filter relevant papers, enabling an effective literature review pipeline. Despite the absence of heuristic feedback or ground truth relevance labels, the proposed system shows promise as a scalable and practical tool for preliminary research and exploratory analysis.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4380.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4380.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoLit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoLit: An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature Review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end automated pipeline introduced in this paper that uses transformer embeddings for retrieval, LLMs for keyword generation, section summarization, citation intent and contribution tagging, and automated synthesis to produce a literature-review paragraph and BibTeX entries from arXiv-sourced papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoLit</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AutoLit accepts a paper title and abstract, uses an LLM to generate 5–10 keywords, queries open-access repositories (arXiv) per keyword (max 20 papers per keyword), deduplicates results, computes sentence/document embeddings (all-MiniLM-L6-v2 by default) for query and candidate title+abstract, applies cosine similarity and an adaptive statistical threshold (Q3 + 0.5*IQR) to select highly similar papers, extracts text from PDFs (PyMuPDF) using regex-based section segmentation, summarizes each section with an LLM into structured fields (problem_statement, methodology, key_findings, conclusion_recommendations), tags citation intent and contribution types with the LLM, generates BibTeX from metadata, and synthesizes a literature-review paragraph from the structured summaries and tags.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gemini-2.0-flash (used for keyword generation, summarization, citation-intent and contribution tagging, and final synthesis); embeddings evaluated with all-MiniLM-L6-v2 and Specter2 (embedding models, not LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval (transformer embeddings + cosine similarity) for candidate selection; PDF text extraction via PyMuPDF; regex-based section segmentation; LLM-driven structured summarization (instructed schema producing problem_statement, methodology, key_findings, conclusion_recommendations).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based synthesis that consumes structured summaries, citation-intent tags, and contribution tags to produce a coherent literature-review paragraph; also generates BibTeX from metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>178 papers fetched from arXiv during evaluation (with per-keyword cap and deduplication; final retrieved sets per input: TF-IDF 19, all-MiniLM-L6-v2 20, Specter2 11 under chosen thresholds).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific papers fetched from arXiv (open-access scholarly literature); method intended as general-purpose for early-stage literature discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured section summaries (problem_statement, methodology, key_findings, conclusion_recommendations), citation intent tags, contribution tags, BibTeX entries, synthesized literature-review paragraph.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Cosine similarity score distributions (mean, skewness, IQR), count of retained documents after thresholding; qualitative metrics not used (no ground-truth relevance labels or human evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>System fetched 178 candidate papers; under evaluated embedding models and heuristic thresholding: TF-IDF retained 19 papers (threshold=0.204), all-MiniLM-L6-v2 retained 20 papers (threshold=0.659), Specter2 retained 11 papers (threshold=0.924). No human-ground-truth precision/recall reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared embedding models against each other (TF-IDF lexical baseline, all-MiniLM-L6-v2 general-purpose embeddings, Specter2 domain-tuned embeddings); no human expert baseline for end-to-end literature quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>all-MiniLM-L6-v2 provided broader conceptual coverage and convincing separation between relevant and less relevant documents compared to TF-IDF (which missed conceptual matches) and Specter2 (which produced high absolute scores with saturation requiring stricter thresholds). Quantitatively: TF-IDF retained 19, all-MiniLM-L6-v2 retained 20, Specter2 retained 11 under the applied thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A lightweight, LLM-augmented semantic-similarity pipeline can retrieve and summarize a concise set of relevant papers with minimal input; general-purpose embeddings (all-MiniLM-L6-v2) give balanced semantic coverage while domain-tuned Specter2 yields higher absolute similarity but score saturation requiring careful thresholding; TF-IDF finds lexical overlaps but misses semantically related work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>No human-annotated ground-truth relevance labels; heuristic thresholding (Q3 + 0.5*IQR) may exclude lexically distant but relevant papers or include irrelevant ones; embedding generalization limits in specialized domains; lack of qualitative/human evaluation; LLM summarization risks (not quantified) such as hallucinations are noted as a general concern.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper reports a single evaluation on 178 fetched candidates and discusses planned future work on adaptive thresholding, re-ranking, domain adaptation and integration with citation networks; no empirical scaling curves with increasing paper counts or model sizes are provided. Specter2 exhibited score clustering (saturation) that complicates thresholding as similarity values approach 1.0.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4380.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4380.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tang2024-LLM-Review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study referenced in this paper that evaluated the feasibility of using LLMs to automate literature-review sub-tasks (reference generation, abstract composition, and full review writing), finding good generative ability but notable hallucination and variable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Not a single system (evaluation study of LLM capabilities for literature-review tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Study investigates the capabilities of LLMs to perform discrete literature-review tasks (reference generation, abstract writing, full review composition) and evaluates failure modes such as hallucinated references and incomplete author lists, and variability across disciplines; no single integrated production system described in the text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based generation for review writing and reference generation (study of capabilities rather than a described extraction pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-driven composition of abstracts and literature reviews (end-to-end generation); no explicit multi-paper synthesis architecture detailed in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General literature-review automation across disciplines (paper reports variable performance across disciplines).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated references, abstracts, and complete review drafts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative evaluation of generated outputs; identification of error modes (hallucination, incomplete authorship lists); detailed metrics not provided in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported that LLMs show impressive generative results but suffer from hallucinated references, incomplete author lists, and variable performance across disciplines (no numeric performance values given in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implied comparison to human-authored references and traditional review quality expectations; no specific baselines detailed in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LLMs can produce readable outputs but fall short in factual reliability (hallucination) and fidelity to bibliographic truth; exact quantitative gaps not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs are powerful for drafting literature-review content but are prone to factual errors in citations and show inconsistent performance across domains, limiting their reliability for unsupervised literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Hallucination of references, incomplete author lists, and variable discipline-dependent performance; risk of producing misleading or incorrect bibliographic content.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in detail in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4380.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4380.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semi-automated system that integrates an LLM agent with human workflow guidance to generate comparative summaries between studies, relying on curated datasets and human-in-the-loop workflow control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ChatCite is described as an LLM-agent system augmented with explicit human workflow guidance; it produces comparative summaries across studies and integrates human steps to reduce some LLM failure modes. The system is semi-automated and depends on curated datasets for reliable summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based comparative summarization with human workflow orchestration; relies on curated inputs/datasets rather than fully automatic document retrieval as described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Comparative summarization via LLM agent that synthesizes structured comparisons across multiple studies; human guidance steers the agent and the summary construction.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Comparative literature summarization across scientific studies (domain-general but depends on curated datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comparative summaries between studies (semi-automated literature-review style outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Semi-automated human-in-the-loop workflow contrasted with prior fully automated LLM approaches (text notes reliance on curated datasets as a limitation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper notes ChatCite reduces some failure modes of earlier LLM approaches by including human workflow guidance, but it relies on curated datasets which may limit generalization—no numeric comparison provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Human workflow guidance within an LLM agent can improve reliability of comparative summaries, addressing some hallucination and factuality issues, but dependence on curated datasets limits out-of-domain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reliance on curated datasets (limits generalization); semi-automated nature requires human expertise for best results.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4380.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4380.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciReviewGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset and framework for automatic generation of literature reviews that summarizes multiple scientific papers using transformer-based models (example model: Fusion-in-Decoder), focusing on large-scale multi-document summarization for literature review tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciReviewGen</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SciReviewGen provides a large-scale dataset and an associated framework designed for training and evaluating transformer-based models to generate literature-review style summaries that aggregate information from multiple scientific papers; the framework has been used with multi-document transformer architectures (e.g., Fusion-in-Decoder variants) to produce reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Transformer-based multi-document summarization models (example cited: Fusion-in-Decoder); specific LLM sizes not detailed in the paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Multi-document transformer input feeding (passages or documents) followed by generative summarization (Fusion-in-Decoder style) to extract salient information across multiple papers.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Large-scale multi-document summarization using transformer decoder generation conditioned on concatenated or retrieved passages across papers (hierarchical/multi-input summarization approach).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed for large-scale summarization of multiple papers; specific numeric scale not provided in this paper's mention but described as 'large-scale dataset'.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature review generation across papers (dataset focused on literature-review style aggregation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated literature-review summaries (multi-paper summarizations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported challenges include hallucinations and missing detailed information; standard summarization metrics likely used in original work (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>The framework produces summaries but faces typical generative model issues such as hallucinations and omitted details; no numeric metrics reported in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to prior LLM-based summarization attempts; not detailed in this excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Produces large-scale summarizations but still challenged by hallucination and omission relative to ideal ground-truth; quantitative comparisons not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Scaling dataset and multi-document transformer approaches enable automated literature-review generation but do not eliminate hallucination and missing-detail failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Hallucination and missing details in generated summaries; dataset-based approaches may still struggle with factual accuracy and completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed to be large-scale (dataset scale), indicating applicability to many-paper synthesis; explicit scaling behavior/performance curves not reported in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4380.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4380.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-PR (Izacard & Grave)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation approach (passage retrieval + generative model) that demonstrates combining retrieval of relevant text passages with generative LLMs to answer open-domain questions, an architecture applicable to multi-document scientific synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (passage retrieval + generative model)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approach combines a passage retrieval layer that retrieves relevant text passages from a corpus and a generative model that conditions on those passages to produce answers; the retrieval component improves grounding and the generator composes multi-passage information—this paradigm is referenced as relevant to literature synthesis pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Generative transformer models used as decoders (specific model names/sizes not provided in this paper's excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Passage retrieval from a corpus followed by conditioning a generative model on retrieved passages (retrieval-augmented generation).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Generative composition of retrieved passages (multi-pass reasoning over retrieved evidence) to produce coherent answers/summaries; can be applied to synthesizing information from multiple documents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Open-domain QA and information synthesis; approach is general and applicable to scientific document synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Answer generation or synthesized summaries conditioned on retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to pure-generation models without retrieval in original work (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Retrieval-augmentation typically improves factual grounding and accuracy relative to non-retrieval generative baselines (general claim from literature; exact numbers not in this paper's mention).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining retrieval with generative models improves grounding and factuality and is a relevant architecture for multi-paper synthesis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality depends on retrieval accuracy; retrieval+generation can still hallucinate or omit relevant evidence if retrieval fails.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Approach inherently scales with corpus size but requires performant retrieval indexes; no specific scaling results provided in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4380.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4380.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wang2023-ChatGPT-Boolean</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study that evaluated ChatGPT's ability to generate and refine Boolean search queries for systematic literature searches and found that guided prompting substantially improved F1 and recall in query formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT-guided Boolean query refinement</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Study applies ChatGPT to refine and generate Boolean queries for systematic literature searches; demonstrates that prompt engineering (guided prompting) can substantially improve retrieval-oriented metrics for search query construction.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (referred to as Chat-GPT / ChatGPT in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-assisted query generation and refinement; not a direct extraction from papers but an approach to improve document retrieval via better Boolean queries.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not applicable (focus is on improving search query construction rather than multi-document synthesis), though improved queries enable better downstream retrieval for synthesis pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Systematic literature search/query construction across academic databases (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Boolean search queries (refined by LLM prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Recall and F1 for retrieval quality of generated Boolean queries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported that a refined query after single prompting achieved F1 = 0.0772, while guided prompting increased F1 to 0.5171; highest recall observed was 0.9128.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared ChatGPT-refined queries to base (unrefined) queries or less-guided prompting strategies in the context of systematic review search performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Guided prompting substantially improved F1 from 0.0772 to 0.5171, and achieved recall up to 0.9128, indicating large gains from structured prompting of ChatGPT for query construction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs like ChatGPT can meaningfully improve Boolean query construction for literature search when guided via appropriate prompting, increasing recall and F1 substantially compared to naive prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLMs still struggle to encode complex Boolean logic precisely; improvements depend on prompt design and human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in detail in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition <em>(Rating: 2)</em></li>
                <li>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary <em>(Rating: 2)</em></li>
                <li>SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation <em>(Rating: 2)</em></li>
                <li>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering <em>(Rating: 2)</em></li>
                <li>Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4380",
    "paper_id": "paper-281411574",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "AutoLit",
            "name_full": "AutoLit: An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature Review",
            "brief_description": "An end-to-end automated pipeline introduced in this paper that uses transformer embeddings for retrieval, LLMs for keyword generation, section summarization, citation intent and contribution tagging, and automated synthesis to produce a literature-review paragraph and BibTeX entries from arXiv-sourced papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AutoLit",
            "system_description": "AutoLit accepts a paper title and abstract, uses an LLM to generate 5–10 keywords, queries open-access repositories (arXiv) per keyword (max 20 papers per keyword), deduplicates results, computes sentence/document embeddings (all-MiniLM-L6-v2 by default) for query and candidate title+abstract, applies cosine similarity and an adaptive statistical threshold (Q3 + 0.5*IQR) to select highly similar papers, extracts text from PDFs (PyMuPDF) using regex-based section segmentation, summarizes each section with an LLM into structured fields (problem_statement, methodology, key_findings, conclusion_recommendations), tags citation intent and contribution types with the LLM, generates BibTeX from metadata, and synthesizes a literature-review paragraph from the structured summaries and tags.",
            "llm_model_used": "gemini-2.0-flash (used for keyword generation, summarization, citation-intent and contribution tagging, and final synthesis); embeddings evaluated with all-MiniLM-L6-v2 and Specter2 (embedding models, not LLMs).",
            "extraction_technique": "Embedding-based retrieval (transformer embeddings + cosine similarity) for candidate selection; PDF text extraction via PyMuPDF; regex-based section segmentation; LLM-driven structured summarization (instructed schema producing problem_statement, methodology, key_findings, conclusion_recommendations).",
            "synthesis_technique": "LLM-based synthesis that consumes structured summaries, citation-intent tags, and contribution tags to produce a coherent literature-review paragraph; also generates BibTeX from metadata.",
            "number_of_papers": "178 papers fetched from arXiv during evaluation (with per-keyword cap and deduplication; final retrieved sets per input: TF-IDF 19, all-MiniLM-L6-v2 20, Specter2 11 under chosen thresholds).",
            "domain_or_topic": "General scientific papers fetched from arXiv (open-access scholarly literature); method intended as general-purpose for early-stage literature discovery.",
            "output_type": "Structured section summaries (problem_statement, methodology, key_findings, conclusion_recommendations), citation intent tags, contribution tags, BibTeX entries, synthesized literature-review paragraph.",
            "evaluation_metrics": "Cosine similarity score distributions (mean, skewness, IQR), count of retained documents after thresholding; qualitative metrics not used (no ground-truth relevance labels or human evaluation).",
            "performance_results": "System fetched 178 candidate papers; under evaluated embedding models and heuristic thresholding: TF-IDF retained 19 papers (threshold=0.204), all-MiniLM-L6-v2 retained 20 papers (threshold=0.659), Specter2 retained 11 papers (threshold=0.924). No human-ground-truth precision/recall reported.",
            "comparison_baseline": "Compared embedding models against each other (TF-IDF lexical baseline, all-MiniLM-L6-v2 general-purpose embeddings, Specter2 domain-tuned embeddings); no human expert baseline for end-to-end literature quality.",
            "performance_vs_baseline": "all-MiniLM-L6-v2 provided broader conceptual coverage and convincing separation between relevant and less relevant documents compared to TF-IDF (which missed conceptual matches) and Specter2 (which produced high absolute scores with saturation requiring stricter thresholds). Quantitatively: TF-IDF retained 19, all-MiniLM-L6-v2 retained 20, Specter2 retained 11 under the applied thresholds.",
            "key_findings": "A lightweight, LLM-augmented semantic-similarity pipeline can retrieve and summarize a concise set of relevant papers with minimal input; general-purpose embeddings (all-MiniLM-L6-v2) give balanced semantic coverage while domain-tuned Specter2 yields higher absolute similarity but score saturation requiring careful thresholding; TF-IDF finds lexical overlaps but misses semantically related work.",
            "limitations_challenges": "No human-annotated ground-truth relevance labels; heuristic thresholding (Q3 + 0.5*IQR) may exclude lexically distant but relevant papers or include irrelevant ones; embedding generalization limits in specialized domains; lack of qualitative/human evaluation; LLM summarization risks (not quantified) such as hallucinations are noted as a general concern.",
            "scaling_behavior": "Paper reports a single evaluation on 178 fetched candidates and discusses planned future work on adaptive thresholding, re-ranking, domain adaptation and integration with citation networks; no empirical scaling curves with increasing paper counts or model sizes are provided. Specter2 exhibited score clustering (saturation) that complicates thresholding as similarity values approach 1.0.",
            "uuid": "e4380.0",
            "source_info": {
                "paper_title": "An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Tang2024-LLM-Review",
            "name_full": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
            "brief_description": "A study referenced in this paper that evaluated the feasibility of using LLMs to automate literature-review sub-tasks (reference generation, abstract composition, and full review writing), finding good generative ability but notable hallucination and variable performance.",
            "citation_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
            "mention_or_use": "mention",
            "system_name": "Not a single system (evaluation study of LLM capabilities for literature-review tasks)",
            "system_description": "Study investigates the capabilities of LLMs to perform discrete literature-review tasks (reference generation, abstract writing, full review composition) and evaluates failure modes such as hallucinated references and incomplete author lists, and variability across disciplines; no single integrated production system described in the text excerpt.",
            "llm_model_used": null,
            "extraction_technique": "LLM-based generation for review writing and reference generation (study of capabilities rather than a described extraction pipeline).",
            "synthesis_technique": "LLM-driven composition of abstracts and literature reviews (end-to-end generation); no explicit multi-paper synthesis architecture detailed in this paper's mention.",
            "number_of_papers": null,
            "domain_or_topic": "General literature-review automation across disciplines (paper reports variable performance across disciplines).",
            "output_type": "Generated references, abstracts, and complete review drafts.",
            "evaluation_metrics": "Qualitative evaluation of generated outputs; identification of error modes (hallucination, incomplete authorship lists); detailed metrics not provided in this paper's summary.",
            "performance_results": "Reported that LLMs show impressive generative results but suffer from hallucinated references, incomplete author lists, and variable performance across disciplines (no numeric performance values given in this paper).",
            "comparison_baseline": "Implied comparison to human-authored references and traditional review quality expectations; no specific baselines detailed in this paper's mention.",
            "performance_vs_baseline": "LLMs can produce readable outputs but fall short in factual reliability (hallucination) and fidelity to bibliographic truth; exact quantitative gaps not provided here.",
            "key_findings": "LLMs are powerful for drafting literature-review content but are prone to factual errors in citations and show inconsistent performance across domains, limiting their reliability for unsupervised literature synthesis.",
            "limitations_challenges": "Hallucination of references, incomplete author lists, and variable discipline-dependent performance; risk of producing misleading or incorrect bibliographic content.",
            "scaling_behavior": "Not discussed in detail in this paper's mention.",
            "uuid": "e4380.1",
            "source_info": {
                "paper_title": "An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "ChatCite",
            "name_full": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
            "brief_description": "A semi-automated system that integrates an LLM agent with human workflow guidance to generate comparative summaries between studies, relying on curated datasets and human-in-the-loop workflow control.",
            "citation_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
            "mention_or_use": "mention",
            "system_name": "ChatCite",
            "system_description": "ChatCite is described as an LLM-agent system augmented with explicit human workflow guidance; it produces comparative summaries across studies and integrates human steps to reduce some LLM failure modes. The system is semi-automated and depends on curated datasets for reliable summarization.",
            "llm_model_used": null,
            "extraction_technique": "LLM-based comparative summarization with human workflow orchestration; relies on curated inputs/datasets rather than fully automatic document retrieval as described in this paper.",
            "synthesis_technique": "Comparative summarization via LLM agent that synthesizes structured comparisons across multiple studies; human guidance steers the agent and the summary construction.",
            "number_of_papers": null,
            "domain_or_topic": "Comparative literature summarization across scientific studies (domain-general but depends on curated datasets).",
            "output_type": "Comparative summaries between studies (semi-automated literature-review style outputs).",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": "Semi-automated human-in-the-loop workflow contrasted with prior fully automated LLM approaches (text notes reliance on curated datasets as a limitation).",
            "performance_vs_baseline": "Paper notes ChatCite reduces some failure modes of earlier LLM approaches by including human workflow guidance, but it relies on curated datasets which may limit generalization—no numeric comparison provided here.",
            "key_findings": "Human workflow guidance within an LLM agent can improve reliability of comparative summaries, addressing some hallucination and factuality issues, but dependence on curated datasets limits out-of-domain generalization.",
            "limitations_challenges": "Reliance on curated datasets (limits generalization); semi-automated nature requires human expertise for best results.",
            "scaling_behavior": "Not discussed in this paper's mention.",
            "uuid": "e4380.2",
            "source_info": {
                "paper_title": "An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "SciReviewGen",
            "name_full": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation",
            "brief_description": "A dataset and framework for automatic generation of literature reviews that summarizes multiple scientific papers using transformer-based models (example model: Fusion-in-Decoder), focusing on large-scale multi-document summarization for literature review tasks.",
            "citation_title": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation",
            "mention_or_use": "mention",
            "system_name": "SciReviewGen",
            "system_description": "SciReviewGen provides a large-scale dataset and an associated framework designed for training and evaluating transformer-based models to generate literature-review style summaries that aggregate information from multiple scientific papers; the framework has been used with multi-document transformer architectures (e.g., Fusion-in-Decoder variants) to produce reviews.",
            "llm_model_used": "Transformer-based multi-document summarization models (example cited: Fusion-in-Decoder); specific LLM sizes not detailed in the paper's mention.",
            "extraction_technique": "Multi-document transformer input feeding (passages or documents) followed by generative summarization (Fusion-in-Decoder style) to extract salient information across multiple papers.",
            "synthesis_technique": "Large-scale multi-document summarization using transformer decoder generation conditioned on concatenated or retrieved passages across papers (hierarchical/multi-input summarization approach).",
            "number_of_papers": "Designed for large-scale summarization of multiple papers; specific numeric scale not provided in this paper's mention but described as 'large-scale dataset'.",
            "domain_or_topic": "Scientific literature review generation across papers (dataset focused on literature-review style aggregation).",
            "output_type": "Generated literature-review summaries (multi-paper summarizations).",
            "evaluation_metrics": "Reported challenges include hallucinations and missing detailed information; standard summarization metrics likely used in original work (not specified here).",
            "performance_results": "The framework produces summaries but faces typical generative model issues such as hallucinations and omitted details; no numeric metrics reported in this paper's summary.",
            "comparison_baseline": "Compared to prior LLM-based summarization attempts; not detailed in this excerpt.",
            "performance_vs_baseline": "Produces large-scale summarizations but still challenged by hallucination and omission relative to ideal ground-truth; quantitative comparisons not provided here.",
            "key_findings": "Scaling dataset and multi-document transformer approaches enable automated literature-review generation but do not eliminate hallucination and missing-detail failure modes.",
            "limitations_challenges": "Hallucination and missing details in generated summaries; dataset-based approaches may still struggle with factual accuracy and completeness.",
            "scaling_behavior": "Designed to be large-scale (dataset scale), indicating applicability to many-paper synthesis; explicit scaling behavior/performance curves not reported in this paper's mention.",
            "uuid": "e4380.3",
            "source_info": {
                "paper_title": "An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "RAG-PR (Izacard & Grave)",
            "name_full": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
            "brief_description": "A retrieval-augmented generation approach (passage retrieval + generative model) that demonstrates combining retrieval of relevant text passages with generative LLMs to answer open-domain questions, an architecture applicable to multi-document scientific synthesis.",
            "citation_title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation (passage retrieval + generative model)",
            "system_description": "Approach combines a passage retrieval layer that retrieves relevant text passages from a corpus and a generative model that conditions on those passages to produce answers; the retrieval component improves grounding and the generator composes multi-passage information—this paradigm is referenced as relevant to literature synthesis pipelines.",
            "llm_model_used": "Generative transformer models used as decoders (specific model names/sizes not provided in this paper's excerpt).",
            "extraction_technique": "Passage retrieval from a corpus followed by conditioning a generative model on retrieved passages (retrieval-augmented generation).",
            "synthesis_technique": "Generative composition of retrieved passages (multi-pass reasoning over retrieved evidence) to produce coherent answers/summaries; can be applied to synthesizing information from multiple documents.",
            "number_of_papers": null,
            "domain_or_topic": "Open-domain QA and information synthesis; approach is general and applicable to scientific document synthesis.",
            "output_type": "Answer generation or synthesized summaries conditioned on retrieved passages.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": "Compared to pure-generation models without retrieval in original work (not detailed here).",
            "performance_vs_baseline": "Retrieval-augmentation typically improves factual grounding and accuracy relative to non-retrieval generative baselines (general claim from literature; exact numbers not in this paper's mention).",
            "key_findings": "Combining retrieval with generative models improves grounding and factuality and is a relevant architecture for multi-paper synthesis tasks.",
            "limitations_challenges": "Quality depends on retrieval accuracy; retrieval+generation can still hallucinate or omit relevant evidence if retrieval fails.",
            "scaling_behavior": "Approach inherently scales with corpus size but requires performant retrieval indexes; no specific scaling results provided in this paper's mention.",
            "uuid": "e4380.4",
            "source_info": {
                "paper_title": "An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Wang2023-ChatGPT-Boolean",
            "name_full": "Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?",
            "brief_description": "A study that evaluated ChatGPT's ability to generate and refine Boolean search queries for systematic literature searches and found that guided prompting substantially improved F1 and recall in query formulation.",
            "citation_title": "Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?",
            "mention_or_use": "mention",
            "system_name": "ChatGPT-guided Boolean query refinement",
            "system_description": "Study applies ChatGPT to refine and generate Boolean queries for systematic literature searches; demonstrates that prompt engineering (guided prompting) can substantially improve retrieval-oriented metrics for search query construction.",
            "llm_model_used": "ChatGPT (referred to as Chat-GPT / ChatGPT in the paper).",
            "extraction_technique": "LLM-assisted query generation and refinement; not a direct extraction from papers but an approach to improve document retrieval via better Boolean queries.",
            "synthesis_technique": "Not applicable (focus is on improving search query construction rather than multi-document synthesis), though improved queries enable better downstream retrieval for synthesis pipelines.",
            "number_of_papers": null,
            "domain_or_topic": "Systematic literature search/query construction across academic databases (general).",
            "output_type": "Boolean search queries (refined by LLM prompting).",
            "evaluation_metrics": "Recall and F1 for retrieval quality of generated Boolean queries.",
            "performance_results": "Reported that a refined query after single prompting achieved F1 = 0.0772, while guided prompting increased F1 to 0.5171; highest recall observed was 0.9128.",
            "comparison_baseline": "Compared ChatGPT-refined queries to base (unrefined) queries or less-guided prompting strategies in the context of systematic review search performance.",
            "performance_vs_baseline": "Guided prompting substantially improved F1 from 0.0772 to 0.5171, and achieved recall up to 0.9128, indicating large gains from structured prompting of ChatGPT for query construction.",
            "key_findings": "LLMs like ChatGPT can meaningfully improve Boolean query construction for literature search when guided via appropriate prompting, increasing recall and F1 substantially compared to naive prompts.",
            "limitations_challenges": "LLMs still struggle to encode complex Boolean logic precisely; improvements depend on prompt design and human oversight.",
            "scaling_behavior": "Not discussed in detail in this paper's mention.",
            "uuid": "e4380.5",
            "source_info": {
                "paper_title": "An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature",
                "publication_date_yy_mm": "2025-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_literature_review_an_evaluation_of_reference_generation_abstract_writing_and_review_composition"
        },
        {
            "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
            "rating": 2,
            "sanitized_title": "chatcite_llm_agent_with_human_workflow_guidance_for_comparative_literature_summary"
        },
        {
            "paper_title": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation",
            "rating": 2,
            "sanitized_title": "scireviewgen_a_largescale_dataset_for_automatic_literature_review_generation"
        },
        {
            "paper_title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
            "rating": 2,
            "sanitized_title": "leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering"
        },
        {
            "paper_title": "Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?",
            "rating": 2,
            "sanitized_title": "can_chatgpt_write_a_good_boolean_query_for_systematic_review_literature_search"
        }
    ],
    "cost": 0.01417875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature Review
18 Sep 2025</p>
<p>Abhiyan Dhakal itsabhiyandhakal@gmail.com 
Kathmandu University Dhulikhel
Nepal</p>
<p>Kausik Paudel kausikpaudel@gmail.com 
Kathmandu University Dhulikhel
Nepal</p>
<p>Sanjog Sigdel sanjog.sigdel@ku.edu.np 
Sanjog Sigdel Kathmandu University Dhulikhel
Nepal</p>
<p>An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature Review
18 Sep 2025FDDBE1BFAD05318100625A15527B6278arXiv:2509.15292v1[cs.AI]
We propose an automated pipeline for performing literature reviews using semantic similarity.Unlike traditional systematic review systems or optimization-based methods, this work emphasizes minimal overhead and high relevance by using transformer-based embeddings and cosine similarity.By providing a paper's title and abstract, it generates relevant keywords, fetches relevant papers from open-access repositories (e.g., ArXiv), and ranks them based on their semantic closeness to the input.Three embedding models: TF-IDF, all-MiniLM-L6-v2, and Specter2 were evaluated.While TF-IDF struggled with capturing deeper semantic meaning, all-MiniLM-L6-v2 provided broader conceptual coverage.Specter2, specifically fine-tuned for scientific texts, exhibited score saturation in similarity scores.A statistical thresholding approach is then applied to filter relevant papers, enabling an effective literature review pipeline.Despite the absence of heuristic feedback or ground-truth relevance labels, the proposed system shows promise as a scalable and practical tool for preliminary research and exploratory analysis.</p>
<p>I. INTRODUCTION</p>
<p>Conducting a literature review is an important step in any research project.By examining existing studies, researchers understand the progress that has been made in their field.Literature reviews summarizes the current state of research, and identify gaps providing insights to the direction of future studies.</p>
<p>Traditional systematic literature review (SLR) methods are mostly time consuming and require extensive manual efforts to select papers, filter them, and organize relevant papers.Recent automated approaches, such as Swarm SLR [1] has attempted to address these challenges by introducing structured workflow and visualization for systematic reviews, however, it requires extensive configuration, user's expertise for proper parameter tuning and possesses challenges to perform quick, exploratory literature reviews on the provided context.</p>
<p>Considering the research gap, we propose AutoLit, an automated pipeline designed to perform literature reviews using semantic similarity.AutoLit operates on minimal input.It requires the title and abstract of the research paper, and generates a list of relevant papers based on transformer-based embeddings and cosine similarity.The pipeline automatically generates keywords from the input, retrieves papers from openaccess repositories like arXiv [2], and filters them based on semantic similarity using a customizable threshold.</p>
<p>This approach is useful for early-stage research and exploratory analysis, focusing on quick and relevant results rather than complete coverage.It minimizes manual intervention, and thus provides an efficient alternative to existing SLR systems for efficient literature discovery and reviews.</p>
<p>II. RELATED WORKS</p>
<p>With the development and innovation of Large Language Models (LLMs), semantic embeddings and workflow optimization techniques, automating the process of literature review has seen significant progress in the recent years.In replacement of traditional SLR, Wittenborg et al. [1] introduced SWARM-SLR that has effectively structured the systematic literature review process by defining clear requirements, establishing a modular workflow and creating a framework to evaluate the supporting tools.This has had impact in the early stages of research, such as searching other relevant papers and initial data processing.However, SWARM-SLR lacks the actual process of writing literature review.</p>
<p>In contrast to SWARM-SLR, Tang et al. [3] has explored the feasibility of LLMs in automating various processes of literature reviews such as reference generation, abstraction composition and complete review writing.Their evaluation showed that while LLMs showed impressive results, they often suffer from hallucination in references, favoring the tasks most cited.Additionally, generating incomplete author lists, and variable performance across various disciplines were major limitations.This is partly addressed by Li et al. [4] through their system ChatCite, which combines human workflow guidance within the LLM agent.It is a semi-automated system that generates comparative summaries between studies, which was not addressed by prior LLM based approaches.However, it relies on curated datasets and thus may not generalize summaries without further adaptation.</p>
<p>Reimers and Gurevych [5] introduced Sentence Bert (SBERT) for semantic similarity, which has become a baseline in many automated pipelines largely due to its efficient and accurate generation of sentence embeddings.SBERT enables precise comparison using cosine similarity.This significantly improves tasks such as relevant literature retrieval, ranking, and clustering which is useful for understanding the closeness of different papers.</p>
<p>Kasanishi et al. [6] proposed SciReviewGen, a large-scale dataset and framework designed for automatic generation of literature reviews by summarizing multiple scientific papers using transformer-based models like Fusion-in-Decoder [7] extended for literature review generation.It focuses on producing summaries but still faces challenges like hallucinations and missing detailed information.</p>
<p>Additionally, Wang et al. [8] evaluate the capability of Chat-GPT in generating Boolean queries for systematic literature searches.Their findings showed that query refinement using ChatGPT can improve the recall and F1 scores.The refined query after single prompting resulted an F1 score of 0.0772 while guided prompting increased the F1 score to 0.5171 which is a significant rise.Also the highest recall they were able to achieve was 0.9128.This indicates that combining base queries with ChatGPT refinement can generate effective Boolen queries suitable for rapid literature reviews where time constraints matter.Although LLMs can approximate human like query construction, they fall short in encoding complex Boolean logic effectively.</p>
<p>Despite these advances, many existing systems are based on multi-stage workflows, which can significantly hamper the performance of the system.In contrast, semantic pipelines that prioritize minimal input and provide relevant results would be beneficial for streamlining the literature review process.This approach addresses the current gap by focusing on semantic similarity-based classification and filtering of openaccess repositories such as arXiv.It emphasizes transformerbased embeddings and cosine similarity for semantic search that enables the extraction of a highly relevant set of papers while performing relevant literature review of the given paper.</p>
<p>III. METHODOLOGY</p>
<p>This section outlines the pipeline designed to automate the literature review process.An overview of the pipeline is illustrated in Figure 1.The pipeline consisted of seven key stages, each designed to efficiently and systematically process scholarly articles relevant to the research topic.</p>
<p>A. Keyword Generation</p>
<p>The initial stage involved generating a set of relevant keywords to guide the literature search.Given the title and abstract of the target paper, 5 to 10 keywords were generated using a LLM, gemini-2.0-flash[9], which recent studies have shown to be effective for zero-shot keyphrase extraction [10], to capture both explicit and semantically related terms, as illustrated in Figure 2.</p>
<p>B. Fetch Papers</p>
<p>For each keyword generated, a corpus of a maximum of 20 relevant papers was fetched from arXiv via keyword-based search using the arXiv API.Since the papers were being fetched using different keywords, there were some duplicates.Those duplicate papers were filtered out on the basis of their metadata ensuring that only unique papers remained in the corpus.</p>
<p>C. Retrieve Relevant Papers</p>
<p>To refine the selection of the papers obtained, a semantic similarity assessment was performed.The sentence transformer model all-MiniLM-L6-v2 [11] was used to generate 384-dimensional dense vector embeddings for the input query (derived from keywords) and the title and abstract of the papers obtained.The cosine similarities between the embeddings of the fetched papers and that of the input query were then calculated.</p>
<p>A statistical thresholding method based on the interquartile range (IQR) of the similarity scores distribution was applied to filter the papers.The IQR method is a robust, distribution-free approach for outlier detection that does not assume normality of the data [12,13].In line with recent NLP work [14], we adopt a conservative upper bound of Q3 + 0.5 × IQR to select only the highest-scoring papers, ensuring high precision while retaining adaptability across different similarity score distributions.</p>
<p>This adaptive thresholding approach accounted for the natural variance and skew in the similarity scores, resulting in a more reliable filtering of semantically relevant papers, as illustrated in Figure 3.</p>
<p>For comparison, we also evaluated other embedding models, including Specter2 [15], which is specifically trained on scientific citation data, and a traditional TF-IDF with cosine similarity baseline.Specter2 produced tightly clustered high similarity scores that required stricter thresholding to avoid false positives, while TF-IDF offered greater precision but lacked semantic coverage, often missing conceptually related papers.Based on this evaluation, all-MiniLM-L6-v2 was selected as a balanced choice for semantic retrieval in this pipeline.</p>
<p>D. Text Extraction from PDF</p>
<p>For each filtered paper, either the abstract or the full PDF was processed to extract structured textual content.We used PyMuPDF [16] library for full text extraction when PDFs were available.To structure the content of each paper, regular expression (regex) patterns were defined to identify and delineate different sections: Introduction, Methodology, Results, and Conclusion.The defined patterns were as follows:</p>
<p>• Abstract: The regex pattern (?i)\babstract\b was used to locate the abstract section of the paper.• Introduction: The pattern (?i)\bintroduction\b was applied to identify the introduction.• Methods: To capture the methodology section, the pattern (?i)\b(methodology|methods|approach)\b was used, accounting for various terminologies that might describe this section.</p>
<p>. Keyword Generation Processs</p>
<p>• Results: To capture the results section, the pattern (?i)\b(results|findings|experiments)\b was employed, which might also be referred to as findings or experiments.</p>
<p>• Conclusion:</p>
<p>The regex pattern (?i)\b(conclusion|discussion|summary)\b was defined to capture the conclusion, which might also appear under the names "discussion" or "summary" in some papers.</p>
<p>E. Summarization</p>
<p>The extracted text from each section of the relevant papers was then summarized using the gemini-2.0-flashLLM.To ensure consistent information extraction, the LLM was instructed to structure the summaries under the summary key, which contained four distinct categories:</p>
<p>problem_statement, methodology, key_findings, and conclusion_recommendations (see Listing 1).Each of these categories was designed to hold  Specifically, under the problem_statement category, the LLM would list the identified research problems or gaps addressed in the paper as individual bullet points.The methodology category would contain bullet points describing the research approach, methods, or techniques employed by the authors.The key_findings category would present the most significant results or discoveries reported in the paper in a bulleted format.Finally, the conclusion_recommendations category would list the main conclusions drawn by the authors, along with any recommendations for future work or applications of their findings.</p>
<p>F. Citation Intent and Contribution Tagging</p>
<p>To further analyze the nature and contributions of each selected paper, the Gemini LLM was employed to perform two key tasks: This approach follows prior research on citation function classification and contribution categorization in scientific literature [17,18,19] and recent advances employing LLMs for automated citation intent and contribution tagging.</p>
<p>G. BibTex Entry and Literature Review Generation</p>
<p>The overall pipeline for generating the literature review is shown in Figure 4.It begins with metadata retrieval from the arXiv API, followed by BibTeX entry generation, paper summarization and tagging using the Gemini LLM, and finally synthesis of the literature review paragraph based on these structured inputs.</p>
<p>The metadata of the relevant papers, retrieved from the arXiv API in earlier stages, was utilized to generate BibTeX entries for each paper.This ensured proper citation formatting for any inclusion of these sources in the final paper.For the generation of the literature review itself, the summaries, citation intent tags, and contribution type tags, previously obtained using the gemini-2.0-flashLLM, served as the primary input.The Gemini model was instructed to synthesize this information into a comprehensive overview of the existing literature related to the research topic.This step aimed to provide a high-level understanding of the current state-of-the-art and key contributions in the field, leveraging the automated analysis performed in the preceding stages of the pipeline.</p>
<p>IV. EVALUATION</p>
<p>To evaluate the effectiveness of embedding models in filtering relevant scientific papers for automated literature review, we experimented with three distinct approaches for research paper similarity:</p>
<p>• TF-IDF: Classical lexical vectorization.</p>
<p>• all-MiniLM-L6-v2: General-purpose transformer embeddings.• Specter2: Scientifically tuned transformer embeddings.A total of 178 papers were fetched from arXiv API.Each input paper's vector was compared to fetched candidates using cosine similarity.Table I summarizes the distributions.</p>
<p>A. TF-IDF</p>
<p>TF-IDF is a purely syntactic model, it lacks semantic encoding capabilities.With a calculated threshold of 0.204, it retained 19 papers.Although 19 articles were filtered, the cosine similarity distribution was highly positive and skewed toward the range [0.010, 0.294], indicating generally low similarity scores across the corpus, which is illustrated in Figure 5 and Figure 6.Also, the model failed to capture the semantic meaning of the input paper even among the selected relevant papers above the threshold.Thus, TF-IDF excels in high lexical overlap but misses conceptual matches.</p>
<p>B. all-MiniLM-L6-v2</p>
<p>The all-MiniLM-L6-v2 model is a general-purpose semantic embedding model.With a calculated threshold set at 0.659, it retrieved 20 papers.The cosine similarity scores was slightly positively skewed in the range [0.070, 0.804] with similarity score of 0.390.Performance reflects a moderately spread distribution of similarity scores.Figure 5 and Figure 6 shows that this model provided convincing separations between relevant and less relevant documents compared to both TF-IDF and specter2.Unlike TF-IDF, this model captured semantic relations, enabling retrieval of relevant papers even when exact terminology differed.</p>
<p>C. Specter2</p>
<p>Specter2 is a fine-tuned model explicitly for scientific document embeddings.As a result, it achieved the highest absolute similarity scores.A threshold of 0.924 resulted in the retrieval of 11 papers, with the score distribution skewed in the range [0.756, 0.945] with a negative skewness value of 0.963.Specter2 effectively captures both semantics and domain-specific knowledge.However, with our threshold set at 0.924, the distribution exhibited higher density near 1.0.All the values were clustered at a range of 0.75 to 0.95 requiring an extremely precise threshold, which might not always be the case for varied input papers.This can result in false positive values.</p>
<p>V. LIMITATIONS</p>
<p>A. Lack of Ground-Truth Relevance Labels</p>
<p>The current system operates without any human-annotated or expert-verified relevance labels.This makes it challenging to assess whether the papers retrieved by the semantic similarity filter are truly relevant to the input query.In the absence of proper benchmarks, only indirect metrics such as similarity score distributions can be used to approximate relevance.This may not always reflect actual correctness.</p>
<p>B. Generalization Constraints of Embedding Models</p>
<p>While models like Specter2 are trained on scientific text, general-purpose transformers such as all-MiniLM-L6-v2 are optimized for broad coverage across diverse domains rather than the scholarly literature specifically.Prior studies have shown that such models often underperform in specialized domains, as they may not capture domain-specific terminology, citation context, or methodological nuances with the same fidelity as models trained on in-domain corpora [20,21,22].This mismatch can lead to embeddings that overlook subtle but important conceptual relationships, reducing retrieval precision in fields with specialized jargon or structured discourse, such as medicine, chemistry, or legal research.</p>
<p>C. Heuristic Thresholding Method</p>
<p>The thresholding method employed, using the third quartile plus half the interquartile range (Q3+0.5•IQR) is a statistical heuristic designed to filter out low-similarity papers.However, this approach does not guarantee semantic relevance.It may exclude important but lexically distant papers and include irrelevant ones that happen to lie within the upper tail of the similarity distribution.The threshold's effectiveness is also dependent on the characteristics of the embedding model and the corpus.</p>
<p>D. Lack of Qualitative Evaluation</p>
<p>Evaluation of the retrieved papers depends entirely on quantitative metrics such as cosine similarity distributions, mean scores, and standard deviations.There is no qualitative assessment of the relevance of the chosen papers in terms of their content or research methods.Conducting such studies would require curated gold-standard datasets or involvement of subject matter experts, which was not feasible within the current resource.Without involvement of human reviewers or case studies, it remains uncertain how well the system supports real-world tasks like literature reviews or knowledge discovery.</p>
<p>VI. CONCLUSION</p>
<p>This work presents an automated pipeline for conducting targeted literature reviews using semantic similarity.By using transformer-based embeddings and large language models, it efficiently retrieves, filters, and summarizes academic literature with minimal human intervention.By integrating TF-IDF, all-MiniLM-L6-v2, and Specter2 embeddings, the system evaluated and filtered 178 papers based on their relevance to a given query derived from title and abstract prompts.</p>
<p>TF-IDF filters documents based on exact text matches, resulting in loss of semantic meaning.all-MiniLM-L6-v2 provides a balanced performance but lacks domain-specific tuning.Specter2 is best aligned with scientific language, however suffered from similarity saturation, due to rigid threshold calibration.</p>
<p>In future work, we plan to focus on implementing adaptive thresholding to dynamically adjust based on score distribution characteristics.Incorporating human-in-the-loop evaluation will provide qualitative insights and a ground-truth basis for refining automated decisions.Re-ranking techniques will be explored to help improve the precision and contextual relevance of the retrieved papers.</p>
<p>Beyond thresholding and ranking improvements, domain adaptation and model fine-tuning (e.g., on specialized corpora) will be investigated to better capture field-specific semantics.Integration with citation network data sources such as Semantic Scholar [23] can enable network-based relevance scoring (e.g., PageRank-style importance measures, co-citation patterns), which may enhance both retrieval accuracy and literature synthesis.Finally, optimizing query formulation techniques will be considered to increase recall while maintaining high relevance.</p>
<p>This work has shown the potential to reduce manual overhead in writing literature reviews, while maintaining relevance on topics, proving it is an scalable and accessible alternative to traditional SLR systems.</p>
<p>Fig. 1 .
1
Fig. 1.Pipeline Overview</p>
<p>Fig. 3 .
3
Fig. 3. Relevant Papers Retrieval Processs</p>
<p>( i )
i
Citation intent tagging, following a predefined taxonomy: Background, Comparison, Extension, Criticism, Application, Future Work, and Other.(ii) Contribution classification, assigning each paper to one of: Dataset, Algorithm, Framework, Review, Benchmark, Survey, System, Theoretical Analysis, or Other.</p>
<p>Fig. 4 .
4
Fig. 4. Litereature Review Synthesis</p>
<p>Fig. 5 .
5
Fig. 5. Line plot showing cosine similarity scores across models</p>
<p>Fig. 6 .
6
Fig. 6.Similarity distribution visualization of models using box plots</p>
<p>SWARM-SLR -Streamlined Workflow Automation for Machineactionable Systematic Literature Reviews. T Wittenborg, O Karras, S Auer, Proc. Int. Conf. Theory Pract. Int. Conf. Theory Pract2024</p>
<p>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition. X Tang, X Duan, Z G Cai, Proc. Int. Conf. Theory Pract. Int. Conf. Theory Pract2024</p>
<p>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary. Y Li, L Chen, A Liu, K Yu, L Wen, Proc. Int. Conf. Comput. Linguistics. Int. Conf. Comput. Linguistics2024</p>
<p>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. N Reimers, I Gurevych, Proc. Conf. Empirical Methods Nat. Conf. Empirical Methods Nat2019Lang. Process.</p>
<p>SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation. T Kasanishi, M Isonuma, J Mori, I Sakata, Proc. Annu. Meeting Assoc. Comput. Linguistics. Annu. Meeting Assoc. Comput. Linguistics2023</p>
<p>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. G Izacard, E Grave, arXiv:2007.012822021arXiv preprintOnline</p>
<p>Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?. S Wang, H Scells, B Koopman, G Zuccon, Proc. 46th Int. 46th Int2023</p>
<p>Gemini 2.0 Flash Large Language Model. Google, 2025</p>
<p>Empirical Study of Zero-shot Keyphrase Extraction with Large Language Models. B Kang, Y Shin, Proc. 31st Int. Conf. Comput. Linguistics. 31st Int. Conf. Comput. LinguisticsAbu Dhabi, UAEJan. 2025</p>
<p>all-MiniLM-L6-v2 Sentence Transformer. N Reimers, I Gurevych, HuggingFace. 2021</p>
<p>J W Tukey, Exploratory Data Analysis. Reading, MA, USAAddison-Wesley1977</p>
<p>Outlier Detection. I Ben-Gal, Data Mining and Knowledge Discovery Handbook. L Maimon, Rokach, Boston, MASpringer2005</p>
<p>Towards Multiple References Era -Addressing Data Leakage and Limited Reference Diversity in Machine Translation Evaluation. X Zeng, H He, H Shi, T Li, Findings Assoc. Comput. Linguistics: ACL 2024. 2024</p>
<p>SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. A Singh, M D'arcy, A Cohan, D Downey, S Feldman, Proc. Conf. Empirical Methods Nat. Lang. Process. Conf. Empirical Methods Nat. Lang. ess2022254018137</p>
<p>PyMuPDF: Python bindings for MuPDF, a high-performance PDF and document processing library. J X Mckie, Artifex Software. Version 1.26.3, 2025</p>
<p>Automatic classification of citation function. S Teufel, A Siddharthan, D Tidhar, Proc. Conf. Empirical Methods Nat. Conf. Empirical Methods NatSydney, Australia2006</p>
<p>Citation Intent Classification Using Deep Neural Networks. D Jurgens, T Finethy, J Mccorriston, Y Xu, D Ruths, Proc. 56th Annu. Meeting Assoc. Comput. Linguistics (ACL). 56th Annu. Meeting Assoc. Comput. Linguistics (ACL)2018</p>
<p>Structural Scaffolds for Citation Intent Classification in Scientific Publications. A Cohan, N Goharian, E Grave, Proc. 57th Annu. Meeting Assoc. Comput. Linguistics (ACL). 57th Annu. Meeting Assoc. Comput. Linguistics (ACL)2019</p>
<p>Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. Y Gu, 10.1145/3458754ACM Trans. Comput. Healthcare. 31Oct. 2021</p>
<p>SciBERT: A Pretrained Language Model for Scientific Text. I Beltagy, K Lo, A Cohan, 10.18653/v1/D19-1371Proc. Conf. Empirical Methods Nat. Lang. Process. (EMNLP-IJCNLP). Conf. Empirical Methods Nat. Lang. ess. (EMNLP-IJCNLP)Hong Kong, ChinaNov. 2019</p>
<p>SPECTER: Document-level Representation Learning using Citation-informed Transformers. A Cohan, S Feldman, I Beltagy, D Downey, D Weld, 10.18653/v1/2020.acl-main.207Proc. 58th Annu. Meeting Assoc. Comput. Linguistics. 58th Annu. Meeting Assoc. Comput. LinguisticsJul. 2020</p>
<p>Semantic Scholar. Allen Institute For Ai, Aug. 11, 2025</p>            </div>
        </div>

    </div>
</body>
</html>
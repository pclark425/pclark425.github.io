<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9562 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9562</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9562</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-273507995</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.16349v1.pdf" target="_blank">Large Language Models in Computer Science Education: A Systematic Literature Review</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are becoming increasingly better at a wide range of Natural Language Processing tasks (NLP), such as text generation and understanding. Recently, these models have extended their capabilities to coding tasks, bridging the gap between natural languages (NL) and programming languages (PL). Foundational models such as the Generative Pre-trained Transformer (GPT) and LLaMA series have set strong baseline performances in various NL and PL tasks. Additionally, several models have been fine-tuned specifically for code generation, showing significant improvements in code-related applications. Both foundational and fine-tuned models are increasingly used in education, helping students write, debug, and understand code. We present a comprehensive systematic literature review to examine the impact of LLMs in computer science and computer engineering education. We analyze their effectiveness in enhancing the learning experience, supporting personalized education, and aiding educators in curriculum development. We address five research questions to uncover insights into how LLMs contribute to educational outcomes, identify challenges, and suggest directions for future research.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9562",
    "paper_id": "paper-273507995",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004079,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models in Computer Science Education: A Systematic Literature Review
21 Oct 2024</p>
<p>Mohammed Latif Siddiq msiddiq3@nd.edu 
George</p>
<p>George</p>
<p>Nishat Raihan mraihan2@gmu.edu 
George</p>
<p>Joanna C S Santos joannacss@nd.edu 
George</p>
<p>Marcos 2025 Zampieri mzampier@gmu.edu 
George</p>
<p>Large 
George</p>
<p>Large Language Models in Computer Science Education: A Systematic Literature Review
21 Oct 20245171265F64DE95D3EC4112AB48DBCB74arXiv:2410.16349v1[cs.LG]Large Language ModelsCode GenerationCS Education
Large language models (LLMs) are becoming increasingly better at a wide range of Natural Language Processing tasks (NLP), such as text generation and understanding.Recently, these models have extended their capabilities to coding tasks, bridging the gap between natural languages (NL) and programming languages (PL).Foundational models such as the Generative Pre-trained Transformer (GPT) and LLaMA series have set strong baseline performances in various NL and PL tasks.Additionally, several models have been fine-tuned specifically for code generation, showing significant improvements in code-related applications.Both foundational and fine-tuned models are increasingly used in education, helping students write, debug, and understand code.We present a comprehensive systematic literature review to examine the impact of LLMs in computer science and computer engineering education.We analyze their effectiveness in enhancing the learning experience, supporting personalized education, and aiding educators in curriculum development.We address five research questions to uncover insights into how LLMs contribute to educational outcomes, identify challenges, and suggest directions for future research.CCS Concepts• Applied computing → Education; • Computing methodologies → Natural language processing; Natural language processing.</p>
<p>Introduction</p>
<p>Recent advances in Generative AI and LLMs, exemplified by GitHub Copilot [1] and ChatGPT [2], highlight their promising ability to address complex problems with human-like expertise.These advancements have a significant impact on education, where students may either benefit from or misuse these tools, compromising the integrity and quality of education [3].This issue is particularly important in introductory Computer Science (CS) courses, which are directly affected by the capabilities of LLMs [4].</p>
<p>The ability of LLMs to efficiently handle programming tasks allows them to successfully complete assignments typically given in beginner courses, making them highly attractive to students seeking effortless solutions.Researchers have been examining the role of LLMs in CS education, focusing on how these models perform with current datasets and past assignments [5].Identifying the use of AI tools in student work is another area of interest [6].However, current methods, including plagiarism detection software, often fail to deliver reliable performance when handling output from a recently introduced LLM [7].</p>
<p>Tools powered by LLMs offer interesting opportunities to improve CS education [8].When used responsibly and in the right way, they can be helpful for learning by providing students with quick feedback on coding assignments and creating different code examples to make programming concepts clearer [9].Furthermore, as Generative AI tools become more common in real-world jobs [10], it is important to teach students about these tools in CS classes.This ensures that students are well-prepared for careers where such tools are widely used.</p>
<p>With students already adopting these tools [11], we do not yet fully understand their impact on learning.Due to the many challenges and benefits these technologies bring, understanding the impact of LLMs in CS education is paramount to improving areas such as curriculum design and assessment.While recent surveys and literature reviews have been published on different topics related to LLMs, such as their use in programming exercise generation [12], implications to security and privacy [13], and software development [14], to the best of our knowledge, no comprehensive survey has been published on the impact of LLMs in CS education.A couple of surveys published on related educational topics are the one by Vierhauser et al. (2024) [15] that focuses on software engineering education and the one by Cambaz et al. (2024) [16] that focuses on programming alone.</p>
<p>In this paper, we fill this important gap in the literature by presenting the first comprehensive systematic literature review (SLR) investigating the impact of LLMs on CS education.We address five carefully crafted research questions (RQs) to understand how these models influence educational outcomes, pinpoint challenges, and suggest future research directions.Following Kitchenham and Charters (2007) guidelines [17] for conducting SLR, we use a thorough search strategy across multiple databases, applying strict inclusion and exclusion criteria to ensure the studies selected are relevant and of high quality.Our results reveal the transformative impact of LLMs in CS educational practices.To enable reproducibility, our scripts, and data are available in a GitHub repository 1 .</p>
<p>Methodology</p>
<p>To conduct this SLR, we follow the aforementioned guidelines [17] that suggest three main steps: planning the literature review, conducting the literature review, and reporting the results.During the planning phase, we set five clear RQs and created a detailed plan for our SLR.In the conducting phase, we search for relevant studies and select them based on specific criteria.Finally, in the reporting phase, we organize and present our findings in this paper.</p>
<p>Research Questions</p>
<p>The five RQs addressed in this SLR to understand the use and impact of LLMs in CS education are the following: RQ1: What are the educational levels in which LLMs are used?</p>
<p>In this RQ, we examine the education stages (e.g., undergraduate level, graduate level, etc.), in which LLMs are integrated.This examination aims to identify the most effective stages for introducing and integrating these models, enhancing learning outcomes, and guiding future educational practices.RQ2: What are the sub-disciplines of CS that are the focus of the studied papers?We investigate what are the specific sub-disciplines (e.g., CS1, software testing, etc.) that were the target of the studied works. 2his RQ aims to identify areas needing further research.RQ3: What research methodologies are mostly used in the papers?We explore the research methodologies, such as experimental designs and data analysis techniques, employed in the selected papers.This RQ aims to understand the field's research practices and standards with respect to this ML-based technology.RQ4: What are the most commonly used programming languages (PLs) in studies involving LLMs?In this RQ, we examine the programming languages that are the focus of the paper.Identifying the frequently targeted languages helps reveal trends and gaps in educational research and practice.RQ5: Which large language models (LLMs) are employed in these studies?</p>
<p>In this RQ, we study the LLMs most widely used in research papers.Cataloging the specific LLMs used provides insights into the diversity and rationale for model selection.</p>
<p>Search Method</p>
<p>To answer our RQs, we use the search query below to retrieve all primary works related to LLMs for CS education:</p>
<p>("software engineering" OR "programming" OR "software development" OR "computer science" OR "computer engineering") AND ("education" OR "teaching") AND ("LLM" OR "large language model") We apply this query to the following library databases3 : the ACM Digital Library, IEEE Xplore, Scopus, the ACL Anthology, ISI Web of Science, Springer Link, Science@Direct, and ArXiv.This search query results in a total of 1,735 papers.</p>
<p>Inclusion and Exclusion Criteria</p>
<p>We vet the papers to exclude those that do not meet our inclusion criteria or that meet our exclusion criteria.Our criteria, shown in Table 1, ensure the relevance and quality of the selected works.We begin with a total of 1,735 primary studies.First, we removed duplicated studies and papers not published within the past 5 years, obtaining a total of 1,423 papers.Subsequently, we inspect each paper's title, keywords, number of pages, and abstract to determine their relevance based on our inclusion and exclusion criteria.This screening process reduces the number of papers to 187.Finally, we apply the same criteria to the full text of these papers, leaving us with 125 papers included in this literature review.</p>
<p>Data Extraction</p>
<p>As we reviewed the papers, we extracted the key information we were looking for to answer our RQs: the educational level, the CS discipline, and programming languages that were the focus of the paper as well as the LLMs and research methodologies that were employed.This data was extracted by two of the authors and peerreviewed by the senior author.</p>
<p>Results</p>
<p>Upon carefully reviewing the 125 selected papers, we conducted a high-level analysis addressing the RQs presented in Section 2.1.</p>
<p>RQ1: Educational Levels</p>
<p>As shown in Figure 1, 111 of the studied papers focus on undergraduatelevel CS courses [1,6,9,11,.While 15 works explored advanced courses typically taught at the graduate level [21,28,30,39,52,89,92,111,114,[125][126][127][128][129][130], only 4 papers include PhD-level courses [21,30,92,127], and just 2 papers addressed K-12 education [131,132].There was one work [133] that examined how ChatGPT is used as a means for training employees in a software engineering workplace (professional context).This strong focus on undergraduate level is primarily due to the limited presence of CS courses at lower educational levels (K-12) and the current limitations of most LLMs in handling higher-level content [120,128].Although newer models like GPT-4 have shown promising results for some graduate courses [39,90], many earlier studies [100,128] did not have the chance to test these capabilities.Even in more recent works, GPT-4 is sometimes excluded due to its cost or other practical issues [99].</p>
<p>RQ1 Results: Over 80% of the papers focused on undergraduatelevel CS education.Further studies are needed to assess the effectiveness and helpfulness of LLMs for graduate students and professionals undergoing CS training.</p>
<p>RQ2: CS Sub-disciplines</p>
<p>As shown in Table 2, over half of the 125 observed studies focus on introduction to programming, predominantly in Python and occasionally in Java.This emphasis is expected, given that CS students frequently use LLMs for code generation [30].The emphasis on introductory programming arises from Python's widespread adoption as the first language in many CS curricula and Java's significant role in teaching object-oriented programming principles.</p>
<p>There were 19 works that focused on introductory CS concepts, featuring Q&amp;As and multiple-choice questions related to basic computer science topics [48,119,124].These studies not only assess LLM-generated code, solutions, and feedback but also explore the generation of tasks, assignments, and questions [9,136].More advanced courses, such as Data Science, present mixed findings regarding the effectiveness of LLMs; some studies claim that LLMs perform well [125], while others disagree [114,130].</p>
<p>Due to space constraints, disciplines with less than 3 papers are aggregated as "Others" in Table 2.These other advanced topics were Distributed Systems [21,69], Operating Systems [19,69], Computer Networks [69], Numerical Analysis [69], Interactive Systems [69], Real-Time Systems [69], Concurrent, Parallel and Distributed Computing [126], Software Testing [128], Information Technology [50], Computer Graphics [43], Human-computer Interaction [46], Databases [91], Automata Theory and Formal Languages [83], Bioinformatics [90], Software Security [74], and Data Visualization [65].RQ2 Results: 67% of works focus on introduction to programming and introduction to CS.There is limited focus on more advanced CS concepts, suggesting a need for further exploration on LLMs' ability in helping to teach advanced CS concepts.</p>
<p>RQ3: Research Methodologies</p>
<p>Table 3 summarizes the research methodologies followed by the studied papers.Our findings show that 38% of papers use case studies and ethnography as their research method, which means these papers are focused on how LLMs can be used in different use cases of CS education.Moreover, 24% of the papers used the action research method, where the researchers introduced novel LLM-based tools and techniques and applied them in a CS educational context.We also found 24 works in which researchers did experiments with students from various levels as described in RQ1 (Section 3.1).In 14% of the works we analyzed, we found researchers were involved in data-centric analysis, and in 12% of cases, they used grounded theory for qualitative analysis.12% of papers were involved in engineering research that invents and evaluates LLM-based artifacts for CS education.Researchers were also involved in interview studies and case studies.They also used multi-methodology and mixedmethods research to use LLMs in CS education.In 5% of works, they benchmarked LLMs for CS education tasks.The rest of the two works used Optimization Studies [9] and Repository Mining [121] as their research methodology.</p>
<p>While most studies conducted interviews with students, teachers as well as practitioners (humans), the work by Dengel et al. [35] conducted semi-structured interviews with LLMs.Their goal was to examine the applicability of qualitative research methods to interviews with LLMs.In their study, LLMs were asked questions related to the relevance of computer science in K-12 education.RQ3 Results: Around 60% works focus on case studies of using LLMs and action research for creating tools around LLMs for CS educational tasks.Researchers also did qualitative research by conducting surveys.Limited works explore data-centric analysis and benchmarking LLMs.</p>
<p>RQ4: Programming Languages</p>
<p>Figure 2 shows the top 10 programming languages that the reviewed papers focused on (TypeScript and R are tied for the 10th position).Among the programming languages, Python is the most studied, being mentioned in 55% of the papers.This prominence is likely because many studies focus on introductory CS courses, where Python is often the first language taught.The second most used language is Java [120,128], primarily taught as an object-oriented programming language.Our findings also reveal that LLMs are employed not only for code generation but also for code-related Q&amp;A tasks [82,116,119,124] and MCQs [48,49,105,109] across various CS courses.Moreover, some studies do not specify particular tasks but instead provide a broader overview, such as examining how LLMs are perceived by students [94] and teachers [111,122], evaluating course feedback [75,89,126], and generating scaffolds [31].</p>
<p>RQ4 Results: Most papers focused on Python and Java code generation while neglecting other commonly used languages such as JavaScript, C &amp; C++.</p>
<p>RQ5: Most commonly used LLMs</p>
<p>Most works used ChatGPT [140] without employing the API, primarily relying on the earlier GPT-3.5 model, with some using GitHub Copilot (OpenAI's Codex) [141] and GPT-4 [142] (see Figure 3).This trend is largely because many of the studies were conducted before the release of GPT-4 and the higher cost of GPT-4 compared to the mostly free GPT-3.5.Additionally, several works accessed GPT-3.5-Turbothrough OpenAI's API.Besides OpenAI models, Microsoft's BingAI4 , as well as Google's BARD 5 and Gemini6 , have been used occasionally.Code-finetuned models like StarCoder [143] and CodeBERT [144] were also used a few times.Unlike OpenAI's models, StarCoder and CodeBERT are free to use.There were 26 different models used only once across 12 papers.These include Anthropic's Claude7 and earlier LLMs like Mistral [145], Falcon [146], MPT [147] etc.</p>
<p>RQ5 Results: Most papers used commercial models in their studies, with ChatGPT being the most used model due to its popularity among students.However, papers mostly used its older version (GPT-3.5)instead of GPT-4 due to its costs.</p>
<p>Discussion</p>
<p>Along with the five RQs answered in this SLR, in our comprehensive analysis, we also identified four important discussion points on LLMs in CS Education as follows:</p>
<p>-Students' and instructors' sentiment about using LLMs in CS Education: Students generally have positive experiences with LLMs and LLM-based tools [21,93,[133][134][135].Studies have shown that students consider examples generated by LLMs helpful [60] and perceive that LLMs could enhance their knowledge [6] by providing helpful feedback [123].CS Students praised LLMs for providing explanations that were easy to understand [73] and thought that LLMs could be an additional agent with teaching assistants.However, studies have also shown that students expressed some frustration about crafting prompts that elicit the desired output [84].Furthermore, some studies indicate that students found it hard to find relevant or accurate responses from LLMs [21].From the perspective of the instructors, studies have indicated that CS instructors found a negative correlation between the usage of LLMs and students' grades [59].They found LLMs could negatively affect students' ability to solve programming tasks independently [59].They expressed concerns about proper learning, over-reliance on tools, and plagiarism when using LLMs in CS education [132].</p>
<p>-Task completion with LLMs and LLM-based tools: As described in the results of RQ2 and RQ4, LLMs and LLM-based tools were applied to solve assignments and programming problems in different PLs from different courses in CS.Regarding the successful completion of the tasks, we found mixed results regarding the use of LLMs.Studies have shown that LLMs can help students solve introductory programming problems, repair buggy code [68,80,117], and help write better code [62,64,93].According to the findings, LLMs are generally better at writing code than solving question-answer [116].They can also generate programming problems [101,112], MCQs [109,113], and detect AI-generated code despite having false positives [61].LLMs can also provide feedback to the student to improve their code [75,89].However, LLMs can partially help with data science [114,130] but hardly solve machine learning problems [39].They also suffer from problems in other languages other than English.For example, LLM performed poorly on Chinese Python question-answering problems [121].</p>
<p>-Adoption of LLMs in different use cases: LLMs are heavily adopted by students [52,82]; they often try ChatGPT to solve their problems but remain skeptical overall [82,96].The adoption of LLMs varied depending on the students' coding skills and prior experience [97].It is also significantly influenced by their perception of future career norms [88].There is potential for rapid iteration, creative ideation, and avoiding social pressures for using LLMs [54].Students used it as a chatbot [63], integrated with the IDE [71], and as a substitute for the teaching assistant [6].They usually read the generated code to solve a task and mostly understand them [115].</p>
<p>-Expectation and future direction of using LLMs in CS Education: Though LLMs can help solve assignments, provide feedback, and repair code, both students and instructors desire more than answers from LLMs [36].Students and instructors agree that LLMs should be welcome in academia [66,131] and that the integration of LLMs with teaching can lead to a better understanding [21].Instructors ask to change the curriculum as they can solve most of the data structure problems [129] but are urged to handle LLMs carefully [28].</p>
<p>Conclusion</p>
<p>In this paper, we presented the first comprehensive SLR on LLMs in CS education.We identified and analyzed 1,735 related papers.After applying well-defined inclusion and exclusion criteria, we described 125 relevant papers in this SLR -the most related SLR to ours [16] has covered 21 papers focusing only on programming.Taking the 125 relevant papers into consideration, we answered five important RQs related to educational levels, sub-disciplines, methodologies, and PLs.We also presented a brief discussion on the adoption of LLMs, students' sentiments, and future directions.</p>
<p>Our findings indicate that most current research focuses on undergraduate education and introductory programming courses.They also indicate that most research applies case-based studies, while the most widely-used PL is Python.All in all, although students are usually positive about using LLMs, instructors are worried about learning effectiveness because of potential over-reliance on them.Our SLR also indicates that educators are gradually adopting LLMs in their courses but that most CS curricula still need to be changed to accommodate recent advances in AI.We hope that the insights gained from this comprehensive SLR will help inform and enhance future research and applications of LLMs in CS education, contributing to a deeper understanding of their role and effectiveness in various educational contexts.</p>
<p>Figure 1 :
1
Figure 1: Educational Levels where LLMs are used.</p>
<p>Figure2shows the top 10 programming languages that the reviewed papers focused on (TypeScript and R are tied for the 10th position).Among the programming languages, Python is the most studied, being mentioned in 55% of the papers.This prominence is likely because many studies focus on introductory CS courses, where Python is often the first language taught.The second most used language is Java[120,128], primarily taught as an object-oriented programming language.C[63] and C++[73] receive comparatively less attention.Fewer works focus on web languages like JavaScript, HTML, and CSS[50,59,65,69,81,92,92,96,120].</p>
<p>Figure 2 :
2
Figure 2: Top 10 Programming Languages.</p>
<p>Figure 3 :
3
Figure 3: Most commonly used LLMs.Others include models that are used only once.A work may use more than one LLM.</p>
<p>Table 1 :
1
Inclusion and Exclusion criteria to Select Papers
Inclusion CriteriaExclusion Criteria
I1 Full papers (i.e., at least 4 full pages of text, excluding references).I2 Written in English.I3 Focus on or investigate the use of code LLMs to teach computing concepts.I4 Written between January 2019 -June 2024.E1 Duplicated studies E2 Not written in English.E3 Abstracts, posters, or extended abstracts with less than 4 full pages of text.E4: Survey and Systematic Literature Reviews (SLRs).</p>
<p>Table 2 :
2
CS Disciplines Explored by the Studied Papers.Introduction to Programming 65 [1, 11, 18-20, 22-27, 29, 31, 34, 36-38, 40, 41, 44, 45, 51, 53, 55, 57-64, 67, 68, 70, 72, 73, 77-80, 82, 84-86, 88, 93, 95, 100-108, 112, 116, 117, 121, 123, 132, 134, 135] Introduction to CS 19 [1, 11, 37, 38, 48, 49, 58, 82, 86, 87, 102, 104, 105, 109, 113, 115, 118-
CS DisciplineTotal References120]Data Science9 [47, 69, 71, 76, 114, 120, 125, 129,130]Software Engineering8 [6, 42, 52, 66, 69, 97, 99, 133]Object-Oriented Programming 4 [9, 32, 33, 69]Algorithms4 [56, 71, 83, 110]Web Development3 [69, 81, 96]Machine Learning3 [39, 98, 120]Computer architecture3 [83, 124, 127]
CS Education inGeneral 10 [28, 30, 35, 75, 89, 92, 94, 111, 122, 131] Others 14 [19, 21, 43, 46, 50, 65, 69, 69, 69, 74, 83, 90, 91, 126, 128]</p>
<p>Table 3 :
3
Research Methodologies used by the Papers.
MethodologyTotal ReferencesCase Study and Ethnogra-48[9, 18, 25-27, 29, 31, 33, 37, 39, 41-43, 45,phy46, 48, 49, 51, 54, 56, 57, 60, 62-64, 69, 70,74, 76-78, 83, 90, 91, 100, 103, 112, 114-116, 120, 121, 125, 129, 131, 134, 135, 137]Action Research30[20, 27, 29, 32, 34, 38, 41, 42, 44, 45, 50,53, 56, 58, 61-63, 67, 74, 80, 81, 102, 106,109, 119, 124, 130, 133, 135, 138]Experiments with Human24[18, 23, 24, 26, 35, 38, 47, 51, 63, 68, 69,Participants72, 74, 75, 85, 103, 106, 111, 115, 118, 123,131, 132, 134]Data Science18[1, 11, 24, 30, 33, 38, 47, 49, 55, 68, 70, 85,89, 106, 110, 111, 125, 126]Grounded Theory15[6, 19, 23, 28, 36, 37, 59, 66, 67, 72, 82, 84,98, 108, 137]Engineering Research (De-sign Science)Case Survey4[87, 92, 94, 127]Optimization Studies1[9]Repository Mining1[121]
15 [1, 25, 31, 40, 71, 73, 76, 93, 96, 99, 101, 104, 113, 117, 128] Qualitative Surveys 9 [21, 52, 55, 58, 65, 88, 118, 122, 123] Longitudinal Studies 8 [6, 11, 59, 79, 82, 86, 98, 105] Mixed Methods Research 7 [22, 43, 64, 82, 88, 91, 97] Benchmarking 6 [95, 99, 104, 107, 128, 139]</p>
<p>https://github.com/s2e-lab/llm-education-survey
As described in Section 2.2, we consider some sub-disciplines that are commonly within the scope of computer engineering along with CS sub-disciplines. The acronym used throughout the paper is CS.
portal.acm.org. IEEE Xplore: ieeexplore.ieee.org. Scopus: scopus.com. ACL: aclanthology.org. Web of Science: isiknowledge.com. Springer: link.springer.com. Science@Direct: sciencedirect.com. ArXiv: arxiv.org.
bing.com/chat
bard.google.com
gemini.google.com
claude.ai</p>
<p>Conversing with copilot: Exploring prompt engineering for solving cs1 problems using natural language. P Denny, V Kumar, N Giacaman, SIGCSE. 2023</p>
<p>OpenAI. Gpt-4 technical report. 2023</p>
<p>Generative ai for programming education: Benchmarking chatgpt, gpt-4, and human tutors. T Phung, Victor-Alexandru, 2023</p>
<p>the teachers are confused as well": A multiplestakeholder ethics discussion on large language models in computing education. K Z Zhou, Z Kilhoffer, 2024</p>
<p>Ai-ta: Towards an intelligent question-answer teaching assistant using open-source llms. Yann Hicke, Anmol Agarwal, Qianou Ma, Paul Denny, 2023</p>
<p>An exploratory study on upper-level computing students' use of large language models as tools in a semester-long project. B Tanay, L Arinze, S S Joshi, K A Davis, J C Davis, 2024</p>
<p>Chatgpt and large language models in academia: opportunities and challenges. J Meyer, R Urbanowicz, BioData Mining. 2023</p>
<p>Impact of guidance and interaction strategies for llm use on learner performance and perception. Harsh Kumar, Ilya Musabirov, 2024</p>
<p>Large language models (gpt) for automating feedback on programming assignments. M Pankiewicz, R S Baker, 2023</p>
<p>Survey reveals AI's impact on the developer experience | The GitHub Blog. GitHub Blog. Inbal Shani, June 2023</p>
<p>it's not like jarvis, but it's pretty close!" -examining chatgpt's usage among undergraduate students in computer science. R Budhiraja, I Joshi, ACE2024</p>
<p>A survey study on the state of the art of programming exercise generation using large language models. E Frankford, I Höhn, C Sauerwein, R Breu, 2024arXiv</p>
<p>Y Yao, J Duan, K Xu, Y Cai, Z Sun, Y Zhang, A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing. 2024100211</p>
<p>Large language models for software engineering: Survey and open problems. A Fan, B Gokkaya, M Harman, M Lyubarskiy, S Sengupta, S Yoo, J Zhang, ICSE. 2023</p>
<p>Towards integrating emerging ai applications in se education. M Vierhauser, I Groher, T Antensteiner, C Sauerwein, CSEE&amp;T. 2024</p>
<p>Use of ai-driven code generation models in teaching and learning programming: a systematic literature review. D Cambaz, X Zhang, SIGCSE. 2024</p>
<p>Guidelines for performing systematic literature reviews in software engineering. B A Kitchenham, S Charters, 2007Technical report</p>
<p>Leveraging chatgpt for adaptive learning through personalized prompt-based instruction: A cs1 education case study. M Abolnejadian, S Alipour, K Taeb, CHI. 2024</p>
<p>which llm should i use?": Evaluating llms for tasks performed by undergraduate computer science students. V Agarwal, M Garg, S Dharmavaram, D Kumar, 2024</p>
<p>Can chatgpt play the role of a teaching assistant in an introductory programming course?. A Anishka, Mehta, 2023</p>
<p>Analyzing llm usage in an advanced computing class in india. C Arora, U Venaik, P Singh, S Goyal, J Tyagi, S Goel, U Singhal, D Kumar, 2024</p>
<p>The impact of chatgpt on students' learning programming languages. I Aviv, M Leiba, H Rika, Y Shani, CHI. 2024</p>
<p>Ai-enhanced auto-correction of programming exercises: How effective is gpt-3.5? iJEP. I Azaiz, O Deckarm, S Strickroth, 2023</p>
<p>Feedback-generation for programming exercises with gpt-4. I Azaiz, N Kiesler, S Strickroth, 2024</p>
<p>Integrating LLMs in Higher Education, Through Interactive Problem Solving and Tutoring: Algorithmic Approach and Use Cases. N P Bakas, M Papadaki, E Vagianou, I Christou, S A Chatzichristofis, 2024Springer Nature Switzerland</p>
<p>Evaluating the quality of llm-generated explanations for logical errors in cs1 student programs. R Balse, V Kumar, P Prasad, J Madathil Warriem, COMPUTE. 2023</p>
<p>Investigating the potential of gpt-3 in providing feedback for programming assessments. R Balse, B Valaboju, S Singhal, J Madathil, P Warriem, Prasad, ITiCSE. 2023</p>
<p>Programming is hard -or at least it used to be: Educational opportunities and challenges of ai code generation. B A Becker, P Denny, J Finnie-Ansley, SIGCSE. 2023</p>
<p>Recommendations to create programming exercises to overcome chatgpt. J Berrezueta-Guzman, S Krusche, CSEE&amp;T. 2023</p>
<p>Text analysis on early reactions to chatgpt as a tool for academic progress or exploitation. U Ali Bukar, M Shohel, S Fatimah Sayeed, S Abdul Razak, O Ahmed Yogarayan, R Amodu, Azlina, Mahmood, SN Computer Science. 2024</p>
<p>Scaffolding cs1 courses with a large language model-powered intelligent tutoring system. C Cao, IUI. 2023</p>
<p>Gpt-3 vs object oriented programming assignments: An experience report. B Pereira Cipriano, P Alves, ITICSE. 2023</p>
<p>Llms still can't avoid instanceof: An investigation into gpt-3.5, gpt-4 and bard's capacity to handle object-oriented programming assignments. B Pereira Cipriano, P Alves, ITICSE. 2024</p>
<p>Evaluating automatically generated contextualised programming exercises. A Del, Carpio Gutierrez, P Denny, SIGCSE. 2024</p>
<p>Qualitative research methods for large language models: Conducting semi-structured interviews with chatgpt and bard on computer science education. A Dengel, R Gehrlein, D Fernes, 2023Informatics</p>
<p>P Denny, S Macneil, J Savelka, L Porter, A Luxton-Reilly, Desirable characteristics for ai teaching assistants in programming education. 2024arXiv</p>
<p>Prompt problems: A new programming exercise for the generative ai era. P Denny, J Leinonen, SIGCSE. 2024</p>
<p>A comparative study of ai-generated (gpt-4) and human-crafted mcqs in programming education. J Doughty, Z Wan, ACE2024</p>
<p>From human days to machine seconds: Automatically answering and generating machine learning final exams. I Drori, S J Zhang, R Shuttleworth, S Zhang, KDD. 2023</p>
<p>Chatgpt and python programming homework. M E Ellis, K Mike Casey, G Hill, 2024DSJIE</p>
<p>Exploring the potential of large language models in generating code-tracing questions for introductory programming courses. A Fan, H Zhang, EMNLP. 2023</p>
<p>Prompting Large Language Models to Power Educational Chatbots. J Carlos Farah, S Ingram, B Spaenlehauer, F Kim-Lan Lasne, D Gillet, 2023Springer NatureSingapore</p>
<p>More than meets the ai: Evaluating the performance of gpt-4 on computer graphics assessment questions. T Haoran Feng, P Denny, B Wuensche, A Luxton-Reilly, S Hooper, ACE2024</p>
<p>Cs1 with a side of ai: Teaching software verification for secure code in the era of generative ai. A S Fernandez, K A Cornell, SIGCSE. 2024</p>
<p>Ai-tutoring in software engineering education. E Frankford, C Sauerwein, P Bassner, S Krusche, R Breu, ICSE-SEET. 2024</p>
<p>May we consult chatgpt in our human-computer interaction written exam? an experience report after a professor answered yes. A Pimenta Freire, P Christina Figueira Cardoso, A De Lima, Salgado, IHC2024</p>
<p>The impact of structured prompt-driven generative ai on learning data analysis in engineering students. A Garg, R Rajendran, 2024CSEDU</p>
<p>Comparative quality analysis of gpt-based multiple choice question generation. C Grévisse, Applied Informatics. 2024</p>
<p>Docimological quality analysis of llm-generated multiple choice questions in computer science and medicine. C Grévisse, M Angeliki, S Pavlou, J G Schneider, SN Computer Science. 2024</p>
<p>Teaching it software fundamentals: Strategies and techniques for inclusion of large language models: Strategies and techniques for inclusion of large language models. S Gumina, T Dalton, J Gerdes, SIGITE. </p>
<p>Students' experiences of using chatgpt in an undergraduate programming course. P Haindl, G Weinberger, IEEE Access. 2024</p>
<p>On chatgpt: Perspectives from software engineering students. K Hanifi, O Cetin, C Yilmaz, In QRS</p>
<p>Detecting chatgpt-generated code submissions in a cs1 course using machine learning models. M Hoq, Y Shi, SIGCSE. 2024</p>
<p>The effects of generative ai on computing students' help-seeking preferences. I Hou, S Mettille, O Man, Z Li, C Zastudil, S Macneil, ACE2024</p>
<p>Evaluating the application of large language models to generate feedback in programming education. S Jacobs, S Jaschke, 2024</p>
<p>Teach ai how to code: Using large language models as teachable agents for programming education. H Jin, S Lee, H Shin, J Kim, CHI. 2024</p>
<p>Need a programming exercise generated in your native language? chatgpt's got your back: Automatic generation of non-english programming exercises using openai gpt-3.5. M Jordan, K Ly, A Gerald Soosai, Raj , SIGCSE. 2024</p>
<p>Chatgpt in the classroom: An analysis of its strengths and weaknesses for solving undergraduate computer science questions. I Joshi, R Budhiraja, H Dev, J Kadia, M Osama Ataullah, S Mitra, H D Akolekar, D Kumar, SIGCSE. 2024</p>
<p>The impact of large language models on programming education and student learning outcomes. G Jošt, V Taneski, S Karakatič, Applied Sciences. 2024</p>
<p>Evaluating llm-generated worked examples in an introductory programming course. B Jury, A Lorusso, J Leinonen, P Denny, A Luxton-Reilly, ACE2024</p>
<p>Detecting ai assisted submissions in introductory programming via code anomaly. O Karnalim, H Toba, M Christianti, Johan , 2024EAIT</p>
<p>Studying the effect of ai code generators on supporting novice learners in introductory programming. M Kazemitabaar, J Chow, C Ka, Ma, CHI. 2023</p>
<p>Codeaid: Evaluating a classroom deployment of an llm-based programming assistant that balances student and educator needs. M Kazemitabaar, R Ye, X Wang, A Henley, P Denny, M Craig, T Grossman, CHI. 2024</p>
<p>Large language models in introductory programming education: Chatgpt's performance and implications for assessments. N Kiesler, D Schiffner, 2023</p>
<p>N Wook Kim, H Ko, G Myers, B Bach, arXiv:2405.00748Chatgpt in data visualization education: A student perspective. 2024arXiv preprint</p>
<p>Software engineering education must adapt and evolve for an llm environment. V D Kirova, C S Ku, SIGCSE. 2024</p>
<p>Computer science education in chatgpt era: Experiences from an experiment in a programming course for novice programmers. T Kosar, D Ostojić, Y David Liu, M Mernik, 2024Mathematics</p>
<p>Training Language Models for Programming Feedback Using Automated Repair Tools. C Koutcheme, 2023Springer</p>
<p>Performance of large language models in a computer science degree program. T Krüger, M Gref, 20244</p>
<p>Using large language models for student-code guided test case generation in computer science education. N Kumar, A Lan, 2024</p>
<p>Kogi: A seamless integration of chatgpt into jupyter environments for programming education. K Kuramitsu, Y Obara, M Sato, M Obara, SPLASH-E. 2023</p>
<p>From "ban it till we understand it" to "resistance is futile": How university programming instructors plan to adapt as more students use ai code generation and explanation tools such as chatgpt and github copilot. S Lau, P Guo, ICER. 2023</p>
<p>Comparing code explanations created by students and large language models. J Leinonen, P Denny, S Macneil, ITiCSE2023</p>
<p>Evaluating the impact of chatgpt on exercises of a software security course. J Li, P Håkon Meland, 2023</p>
<p>The potential of large language models as tools for analyzing student textual evaluation: A differential analysis between cs and non-cs students. H Li, CEI. 2023</p>
<p>Codehelp: Using large language models with guardrails for scalable support in programming classes. M Liffiton, B Sheese, J Savelka, P Denny, Koli Calling. 2024</p>
<p>Beyond traditional teaching: Large language models as simulated teaching assistants in computer science. M Liu, F M Hiri, SIGCSE. 2024</p>
<p>Teaching cs50 with ai: Leveraging generative artificial intelligence in computer science education. R Liu, C Zenke, C Liu, SIGCSE. 2024</p>
<p>Evaluating the effectiveness of llms in introductory computer science education: A semesterlong field study. W Lyu, Y Wang, T Rachel, Y Chung, Y Sun, Zhang, 2024arXiv</p>
<p>How to teach programming in the ai era? using llms as a teachable agent for debugging. Q Ma, H Shen, K Koedinger, S Wu, AIED. 2024</p>
<p>Experiences from using code explanations generated by large language models in a web software development e-book. S Macneil, A Tran, A Hellas, SIGCSE. 2023</p>
<p>Examining student use of ai in cs1 and cs2. E D Manley, T Urness, A Migunov, M Alimoor Reza, J. Comput. Sci. Coll. 2024</p>
<p>Evaluating chatgpt-4 vision on brazil's national undergraduate computer science exam. N C Mendonça, ACM Trans. Comput. Educ. 2024</p>
<p>How beginning programmers and code llms (mis)read each other. S Nguyen, H Mclean Babe, Y Zi, A Guha, C Jane Anderson, M Feldman, CHI. 2024</p>
<p>Automated assessment of students' code comprehension using llms. P Oli, R Banjade, J Chapagain, V Rus, 2024</p>
<p>Beyond the hype: A cautionary tale of chatgpt in the programming classroom. G Oosterwyk, P Tsibolane, P Kautondokwa, A Canani, 2024</p>
<p>Detecting llm-generated text in computing education: A comparative study for chatgpt cases. M Sheinman Orenstrakh, O Karnalim, C Suarez, M Liut, 2023</p>
<p>Insights from social shaping theory: The appropriation of large language models in an undergraduate programming course. A Padiyath, X Hou, A Pang, D Viramontes, X Vargas, T Gu, Z Nelson-Fromm, M Wu, B Guzdial, Ericson, 2024arXiv</p>
<p>A large language model approach to educational survey feedback analysis. M J Parker, C Anderson, C Stone, Y Oh, 2024IJAIED</p>
<p>Evaluating a large language model's ability to solve programming exercises from an introductory bioinformatics course. S R Piccolo, P Denny, PLOS Computational Biology. 2023</p>
<p>Integrating llms into database systems education. K Prakash, S Rao, R Hamza, J Lukich, V Chaudhari, A Nandi, DataEd2024</p>
<p>The robots are here: Navigating the generative ai revolution in computing education. J Prather, P Denny, J Leinonen, B A Becker, I Albluwi, M Craig, H Keuning, N Kiesler, T Kohn, A Luxton-Reilly, S Macneil, A Petersen, R Pettit, B N Reeves, J Savelka, ITiCSE-WGR. 2023</p>
<p>Interactions with prompt problems: A new way to teach programming with large language models. J Prather, P Denny, J Leinonen, D H Smith, B N Reeves, S Macneil, B A Becker, A Luxton-Reilly, T Amarouche, B Kimmel, 2024</p>
<p>Chatgpt in computer science curriculum assessment: An analysis of its successes and shortcomings. B Qureshi, ICSLT2023</p>
<p>Cseprompts: A benchmark of introductory computer science prompts. N Raihan, D Goswami, S Sayara Chowdhury Puspo, C Newman, T Ranasinghe, M Zampieri, ISMIS2024</p>
<p>call me kiran" -chatgpt as a tutoring chatbot in a computer science course. J Rajala, J Hukkanen, M Hartikainen, P Niemelä, Mindtrek2023</p>
<p>An empirical study on usage and perceptions of llms in a software engineering project. S Rasnayaka, G Wang, R Shariffdeen, G Neelakanta Iyer, 2024</p>
<p>Bridging the Programming Skill Gap with ChatGPT: A Machine Learning Project with Business Students. M Reiche, J L Leidner, 2024Springer Nature Switzerland</p>
<p>Analysis of chatgpt performance in computer engineering exams. R Rodriguez-Echeverría, J D Gutiérrez, J M Conejero, 2024IEEE-RITA</p>
<p>Assessing ChatGPT's Proficiency in CS1-Level Problem Solving. M Sánchez, A Herrera, 2023Springer Nature Switzerland</p>
<p>Automatic generation of programming exercises and code explanations using large language models. S Sarsa, P Denny, A Hellas, J Leinonen, ICER2022</p>
<p>Enhancing e-learning experience through embodied ai tutors in immersive virtual environments: A multifaceted approach for personalized educational adaptation. F Sarshartehrani, E Mohammadrezaei, M Behravan, D Gracanin, CHI. 2024</p>
<p>Efficient classification of student help requests in programming courses using large language models. J Savelka, P Denny, M Liffiton, B Sheese, 2023</p>
<p>Thrilled by your progress! large language models (gpt-4) no longer struggle to pass assessments in higher education programming courses. J Savelka, A Agarwal, M An, ICER. </p>
<p>From gpt-3 to gpt-4: On the evolving efficacy of llms to answer multiple-choice questions for programming classes in higher education. J Savelka, A Agarwal, C Bogart, M Sakr, Computer Supported Education. 2024</p>
<p>Analyzing chat protocols of novice programmers solving introductory programming tasks with chatgpt. A Scholl, D Schiffner, N Kiesler, 2024</p>
<p>Can chatgpt pass a cs1 python course?. J S Sharpe, R E Dougherty, S J Smith, J. Comput. Sci. Coll. 2024</p>
<p>Patterns of student help-seeking when using a large language model-powered programming assistant. B Sheese, M Liffiton, J Savelka, P Denny, ACE2024</p>
<p>Automatic generation of multiple-choice questions for cs0 and cs1 curricula using large language models. T Song, Q Tian, Y Xiao, S Liu, Computer Science and Education. 2024</p>
<p>Automated analysis of algorithm descriptions quality, through large language models. A Sterbini, M Temperini, ITS. 2024</p>
<p>Acceptance and use of chatgpt in the academic community. A Strzelecki, K Cicha, M Rizun, P Rutecka, 2024EAIT</p>
<p>Exgen: Readyto-use exercise generation in introductory programming courses. N Binh Duong, T A , H Gia Phuc, G Nguyen, Swapna, ICCEC. Asia-Pacific Society for Computers in Education. 2023</p>
<p>Generating multiple choice questions for computing courses using large language models. A Tran, K Angelikas, E Rama, C Okechukwu, FIE2023</p>
<p>What should data science education do with large language models. X Tu, J Zou, W Su, L Zhang, 2023arXiv</p>
<p>Cs1-llm: Integrating llms into cs1 instruction. A Vadaparty, D Zingaro, D H Smith, M Padala, C Alvarado, J Gorson Benario, L Porter, 2024</p>
<p>Evaluating copilot on cs1 code writing problems with suppressed specifications. V Venkatesh, V Venkatesh, V Kumar, COMPUTE. 2023</p>
<p>Automated program repair for introductory programming assignments. H Wan, H Luo, M Li, X Luo, IEEE TLT. 2024</p>
<p>Exploring the role of ai assistants in computer science education: Methods, implications, and instructor perspectives. T Wang, D Díaz, VL/HCC. 2023</p>
<p>Enhancing image comprehension for computer science visual question answering. H Wang, P Qiang, H Tan, J Hu, CVPR. 2024</p>
<p>A qualitative assessment of chatgpt generated code in the computer science curriculum. J Wolfer, Towards a Hybrid, Flexible and Socially Engaged Higher Education. 2024</p>
<p>Qacp: An annotated question answering dataset for assisting chinese python programming learners. R Xiao, L Han, X Zhou, J Wang, N Zong, P Zhang, 2024</p>
<p>Generative ai in computing education: Perspectives of students and instructors. C Zastudil, M Rogalska, 2023</p>
<p>Students' perceptions and preferences of generative artificial intelligence feedback for programming. Z Zhang, Z Dong, 2023</p>
<p>Assistant teaching system for computer hardware courses based on large language model. D Zhang, Q Cao, Y Guo, L Wang, Computer Science and Education. 2024</p>
<p>Generative ai for data science 101: Coding without learning to code. J Bien, G Mukherjee, 2024arXiv</p>
<p>Evaluation of llm tools for feedback generation in a course on concurrent programming. I Estévez-Ayres, P Callejo, M Ángel Hombrados-Herrera, 2024IJAIED</p>
<p>Dual-submission homework in parallel computer architecture: An exploratory study in the age of llms. E F Gehringer, J George Wang, S Kumar Jilla, WCAE2024</p>
<p>Chatgpt and software testing education: Promises &amp; perils. S Jalil, S Rafi, T D Latoza, K Moran, W Lam, ICSTW2023</p>
<p>Implications of chatgpt for data science education. Y Shen, X Ai, SIGCSE. 2024</p>
<p>Chatgpt for teaching and learning: An experience from data science education. Y Zheng, SIGITE. 2023</p>
<p>Teaching ai to k-12 learners: Lessons, issues, and guidance. S Grover, SIGCSE. 2024</p>
<p>How novices use llm-based code generators to solve cs1 coding tasks in a self-paced learning environment. M Kazemitabaar, X Hou, Koli Calling. 2024</p>
<p>Incorporating generative ai into software development education. O Petrovska, L Clift, F Moller, R Pearsall, CEP2024</p>
<p>Potentiality of generative ai tools in higher education: Evaluating chatgpt's viability as a teaching assistant for introductory programming courses. Z Ahmed, S Sadat Shanto, A Islam Jony, 2024STEM Education</p>
<p>Next-step hint generation for introductory programming using large language models. L Roest, H Keuning, J Jeuring, ACE2024</p>
<p>Gpt-3 vs object oriented programming assignments: An experience report. B Cipriano, P Alves, ITICSE. 2023</p>
<p>An empirical study on how large language models impact software testing learning. S Mezzaro, A Gambi, G Fraser, EASE. 2024</p>
<p>Enhancing programming learning with llms: Prompt engineering and flipped interaction. B Cowan, Y Watanobe, A Shirafuji, ASSE2024</p>
<p>Studenteval: A benchmark of studentwritten prompts for large language models of code. H Mclean Babe, S Nguyen, Y Zi, 2023</p>
<p>Language models are few-shot learners. T Brown, B Mann, Advances in neural information processing systems. 2020</p>
<p>Evaluating large language models trained on code. Mark Chen, Jerry Tworek, arXiv2021</p>
<p>. J Achiam, S Adler, S Agarwal, arXiv:2303.087742023Gpt-4 technical report. arXiv preprint</p>
<p>R Li, L B Allal, Y Zi, N Muennighoff, D Kocetkov, C Mou, M Marone, C Akiki, J Li, J Chim, arXiv:2305.06161Starcoder: may the source be with you!. 2023arXiv preprint</p>
<p>Codebert: A pre-trained model for programming and natural languages. Z Feng, D Guo, EMNLP. 2020</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Mensch, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only. G Penedo, Q Malartic, D Hesslow, 2023</p>
<p>The MosaicML NLP Team. Mpt-30b: Raising the bar for open-source foundation models. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
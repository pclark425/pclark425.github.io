<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7754 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7754</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7754</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-273850415</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.03743v1.pdf" target="_blank">Automating Exploratory Proteomics Research via Language Models</a></p>
                <p><strong>Paper Abstract:</strong> With the development of artificial intelligence, its contribution to science is evolving from simulating a complex problem to automating entire research processes and producing novel discoveries. Achieving this advancement requires both specialized general models grounded in real-world scientific data and iterative, exploratory frameworks that mirror human scientific methodologies. In this paper, we present PROTEUS, a fully automated system for scientific discovery from raw proteomics data. PROTEUS uses large language models (LLMs) to perform hierarchical planning, execute specialized bioinformatics tools, and iteratively refine analysis workflows to generate high-quality scientific hypotheses. The system takes proteomics datasets as input and produces a comprehensive set of research objectives, analysis results, and novel biological hypotheses without human intervention. We evaluated PROTEUS on 12 proteomics datasets collected from various biological samples (e.g. immune cells, tumors) and different sample types (single-cell and bulk), generating 191 scientific hypotheses. These were assessed using both automatic LLM-based scoring on 5 metrics and detailed reviews from human experts. Results demonstrate that PROTEUS consistently produces reliable, logically coherent results that align well with existing literature while also proposing novel, evaluable hypotheses. The system's flexible architecture facilitates seamless integration of diverse analysis tools and adaptation to different proteomics data types. By automating complex proteomics analysis workflows and hypothesis generation, PROTEUS has the potential to considerably accelerate the pace of scientific discovery in proteomics research, enabling researchers to efficiently explore large-scale datasets and uncover biological insights.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7754.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7754.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROTEUS Eval Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROTEUS automatic + human evaluation pipeline for LLM-generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation pipeline introduced in this paper that combines automatic LLM-based scoring (primarily using GPT-4o) across five bespoke metrics with targeted human expert review to assess hypotheses produced by the PROTEUS system from proteomics data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70B-UltraMed (primary hypothesis generator) / GPT-4o (primary automatic evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (Llama-3.1 fine-tuned), GPT-4o (proprietary evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Proteomics / Computational Biology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis (biological hypotheses derived from proteomics data)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based scoring + human expert review</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Each hypothesis produced by PROTEUS is automatically scored by an LLM evaluator (GPT-4o in the main experiments) using detailed prompts for five metrics (Paper-Based Alignment, Literature-Based Alignment, Literature-Based Novelty, Logical Coherence, Evaluability) with scoring on a 0–5 integer scale; a subset of outputs was then reviewed by human proteomics experts using the same scoring instructions to validate automatic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Paper-Based Alignment; Literature-Based Alignment; Literature-Based Novelty; Logical Coherence; Evaluability (all scored 0–5)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Integer scale 0-5 where higher is better; each metric has detailed prompt-specific definitions (e.g., 0 = no alignment/contradiction up to 5 = excellent support/perfect alignment; Evaluability measures clarity and testability from 0 Not-evaluable to 5 Fully-evaluable).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SPDB (10 CyTOF single-cell datasets) and two clinical cohort mass-spectrometry datasets (HCC, GBM); evaluation corpus also included PubMed article sets retrieved per-hypothesis (10–20 articles).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Subset: 30 hypotheses randomly selected from two SPDB datasets (Datasets 3 and 4). Reviews performed by 4 human experts in proteomics following the same 5-metric instructions; scores compared to automatic evaluator; agreement reported visually (Fig.7) and mean comparisons reported (humans typically score higher except for Novelty).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Across 191 hypotheses: mean Paper-Based Alignment 2.13, Literature-Based Alignment 3.24, Literature-Based Novelty 3.29, Logical Coherence 3.32, Evaluability 3.55 (Table 6). Automatic vs. human scoring showed reasonable agreement; humans scored higher on most metrics except Novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Human expert scores on the examined subset were generally higher than automatic LLM scores for all metrics except Novelty; overall reasonable agreement within ±1 point for many items (visualized in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Paper-alignment is a poor proxy for hypothesis quality because original papers only cover a subset of dataset signals; automatic evaluators can be more sensitive to minor discrepancies than humans; no automated falsifiability tests or formal reproducibility experiments were performed; LLM context-length limits number of workflows/results retained for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating Exploratory Proteomics Research via Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7754.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7754.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Five Eval Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paper-Based Alignment; Literature-Based Alignment; Literature-Based Novelty; Logical Coherence; Evaluability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bespoke five-metric evaluation suite designed in this work to assess plausibility, alignment with the dataset's original paper, alignment and novelty with respect to general literature, logical/bio-plausibility, and the testability of each LLM-generated hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (automatic scoring), human experts (manual scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (proprietary), human experts (N/A)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Proteomics / Scientific hypothesis evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation criteria / scoring rubric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Five-metric scoring rubric (0–5 scale)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each hypothesis, evaluators (LLM or human) are provided the full hypothesis and optional reference info (original paper extract and 10–20 PubMed abstracts). The prompts define detailed criteria for each metric and request free-form analysis followed by an integer 0–5 score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Five discrete integer metrics: Paper-Based Alignment, Literature-Based Alignment, Literature-Based Novelty, Logical Coherence, Evaluability.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Each metric scored on integer 0–5: 0 = contradiction/no support/not evaluable; 1 = minimal; 2 = some support; 3 = moderate; 4 = strong; 5 = excellent/perfect (detailed prompt text provides specific guidance per metric).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to 191 PROTEUS hypotheses across SPDB (10 CyTOF datasets) and 2 clinical MS datasets (HCC, GBM); PubMed article sets (10–20 abstracts) retrieved per hypothesis for literature-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Same 5-metric rubric given to human experts; subset of 30 hypotheses scored by 4 experts for validation; comparison statistics reported (means and agreement visualization).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Means reported in Table 6: Paper-Based Alignment 2.13, Literature-Based Alignment 3.24, Literature-Based Novelty 3.29, Logical Coherence 3.32, Evaluability 3.55; distributions shown in Figures 2,3,4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Automatic metric scores were generally lower than human scores except for Novelty; inter-evaluator robustness shown by rerunning evaluation with multiple LLM evaluators (similar overall averages).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Scales are subjective despite prompt detail; paper-alignment metric conflates differences in research focus with low quality; literature-based metrics depend on the search query quality and retrieved abstracts; no objective ground truth for novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating Exploratory Proteomics Research via Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7754.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7754.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o Evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (used as the primary automatic hypothesis evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art proprietary large language model used in this work as the main automatic evaluator to score PROTEUS-generated hypotheses across the five evaluation metrics using detailed prompts and retrieved literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (not explicitly stated)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / biomedical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automatic evaluator (LLM-based scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM evaluator scoring (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>GPT-4o receives each hypothesis plus optional reference material (original paper excerpts and PubMed article metadata/abstracts) and executes prompt-guided scoring for each of the five metrics, returning free-form reasoning plus a 0–5 integer score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Five metrics (Paper-Based Alignment; Literature-Based Alignment; Literature-Based Novelty; Logical Coherence; Evaluability)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>0–5 integer scale per metric with explicit rubric embedded in evaluation prompts (see paper prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>191 hypotheses from 12 proteomics datasets (10 SPDB CyTOF + 2 clinical MS datasets); per-hypothesis PubMed retrieval (10–20 articles) used for literature-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used for automatic scoring; results compared to 4 human experts on subset of 30 hypotheses to validate automated scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Average scores reported (Table 6). Authors also ran evaluation using alternative evaluator LLMs (Gemini-flash-1.5, Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct) and observed similar overall averages, indicating robustness of automatic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Automatic GPT-4o scores were generally lower than human expert scores except for Novelty; reasonable agreement across metrics and between different LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Evaluator sensitivity to minor discrepancies; dependence on retrieval query quality and retrieved abstracts; proprietary model details not provided; automatic evaluation does not replace experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating Exploratory Proteomics Research via Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7754.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7754.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Expert Review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proteomics expert human scoring and case review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual evaluation by domain experts who scored a random subset of PROTEUS-generated hypotheses using the same five-metric rubric and provided open-ended critiques on biological plausibility, novelty, and experimental verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>human experts (domain reviewers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Proteomics / Biomedical research</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human evaluation / expert review</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert scoring using five-metric rubric + qualitative review</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Four proteomics experts scored 30 randomly chosen hypotheses (from two SPDB datasets) using the same 0–5 rubric and provided detailed feedback and free-text reviews assessing hypothesis quality, novelty, and biological implications, enabling validation of automatic scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Same five metrics (Paper-Based Alignment; Literature-Based Alignment; Literature-Based Novelty; Logical Coherence; Evaluability), plus qualitative commentary.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Integer 0–5 scale per metric following the prompt definitions used for automatic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Subset of PROTEUS outputs: 30 hypotheses from two SPDB datasets (Datasets 3 and 4).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>4 human experts; scored using the same instructions as LLM evaluator; provided detailed open-ended feedback; results compared against GPT-4o automatic scoring (Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Human scores were on average higher than automatic LLM scores for all metrics except Novelty; qualitative reviews identified plausible novel hypotheses and also highlighted cases where PROTEUS lacked necessary biological nuance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small human-evaluated subset (30 hypotheses) limits generalizability; inter-rater agreement not fully quantified (visual agreement reported); human evaluation is resource-intensive and subjective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating Exploratory Proteomics Research via Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7754.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7754.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Datasets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPDB CyTOF single-cell datasets and Clinical Cohort MS datasets (HCC, GBM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Datasets used to generate and evaluate hypotheses: 10 single-cell CyTOF datasets from the Single-cell Proteomic DataBase (SPDB) and two clinical bulk proteomics datasets (hepatocellular carcinoma HCC and glioblastoma GBM) sequenced by mass spectrometry, together producing 191 hypotheses for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spdb: a comprehensive resource and knowledgebase for proteomic data at the single-cell resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70B-UltraMed (PROTEUS backbone for generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (Llama-3.1 fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Proteomics / Clinical proteomics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>data sources used for hypothesis generation and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Dataset-driven hypothesis generation and dataset-aligned evaluation (paper-alignment & literature checks)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>PROTEUS ingests SingleCellExperiment objects (from SPDB) or raw protein+metadata CSVs (clinical MS) to produce hypotheses; evaluations reference the original paper for paper-alignment metric and use external PubMed retrieval for literature-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hypotheses produced on these datasets were evaluated across the five metrics (0–5 scale).</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Same 0–5 integer rubric per metric; paper-alignment compares hypothesis to the original dataset's published paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SPDB (10 CyTOF datasets) and clinical cohort MS datasets: HCC and GBM (total 12 datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human review subset drawn from SPDB datasets (Datasets 3 and 4); full automatic scoring done across all 191 hypotheses from the 12 datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>PROTEUS produced 147 hypotheses from CyTOF (SPDB) datasets and 44 from MS clinical datasets (total 191). Per-dataset average metric scores are reported in Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Original papers often only cover a fraction of dataset signals so paper-alignment may be low even for high-quality novel hypotheses; differences in data types required workflow adjustments (e.g., direct tool calls for clinical cohorts).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating Exploratory Proteomics Research via Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7754.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7754.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-run Selection Strategy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple LLM runs with automatic selection to improve hypothesis quality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedural method evaluated in the paper where PROTEUS performs multiple stochastic LLM runs on the same dataset and automatically selects the best hypotheses (by average score across four metrics) to improve overall evaluation scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70B-UltraMed (primary) / GPT-4o (evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (Llama-3.1 fine-tuned); GPT-4o (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Proteomics / ML evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>procedure to improve automatic selection of high-quality hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Iterated runs + automatic best-selection by evaluator scores</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Due to LLM sampling randomness, run PROTEUS multiple times (N iterations), evaluate hypotheses automatically (excluding paper-alignment since it's a poor overall indicator), compute average scores across the remaining four metrics for each generated hypothesis set, and select highest scoring outputs; authors found average score improved with multiple iterations (notably after 5+ runs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average across 4 metrics: Literature-Based Alignment, Literature-Based Novelty, Logical Coherence, Evaluability (0–5 scale); used as selection criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Mean of the four metric integer scores (0–5); selection based on higher mean score.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Tested on SPDB CyTOF datasets (10 datasets); improvement quantified in Figure 6 (average score improved by >0.2 after 5 iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not directly human-evaluated; improvement measured via automatic evaluator (GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Average score across four metrics rose by more than 0.2 when using 5+ iterations of PROTEUS runs and selecting the best outputs automatically (Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on the reliability of the automatic evaluator; increases compute cost; selection ignores paper-alignment metric intentionally because it's a poor proxy for overall quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating Exploratory Proteomics Research via Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Spdb: a comprehensive resource and knowledgebase for proteomic data at the single-cell resolution <em>(Rating: 2)</em></li>
                <li>Ultramedical: Building specialized generalists in biomedicine. <em>(Rating: 2)</em></li>
                <li>DREAM: a biomedical data-driven self-evolving autonomous research system. <em>(Rating: 1)</em></li>
                <li>MultiMedQA: a benchmark for medical question answering (composed of MedQA, PubMedQA, MedMCQA, biomedical MMLU categories) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7754",
    "paper_id": "paper-273850415",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "PROTEUS Eval Framework",
            "name_full": "PROTEUS automatic + human evaluation pipeline for LLM-generated hypotheses",
            "brief_description": "An evaluation pipeline introduced in this paper that combines automatic LLM-based scoring (primarily using GPT-4o) across five bespoke metrics with targeted human expert review to assess hypotheses produced by the PROTEUS system from proteomics data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70B-UltraMed (primary hypothesis generator) / GPT-4o (primary automatic evaluator)",
            "model_size": "70B (Llama-3.1 fine-tuned), GPT-4o (proprietary evaluator)",
            "scientific_domain": "Proteomics / Computational Biology",
            "theory_type": "hypothesis (biological hypotheses derived from proteomics data)",
            "evaluation_method_name": "LLM-based scoring + human expert review",
            "evaluation_method_description": "Each hypothesis produced by PROTEUS is automatically scored by an LLM evaluator (GPT-4o in the main experiments) using detailed prompts for five metrics (Paper-Based Alignment, Literature-Based Alignment, Literature-Based Novelty, Logical Coherence, Evaluability) with scoring on a 0–5 integer scale; a subset of outputs was then reviewed by human proteomics experts using the same scoring instructions to validate automatic evaluation.",
            "evaluation_metric": "Paper-Based Alignment; Literature-Based Alignment; Literature-Based Novelty; Logical Coherence; Evaluability (all scored 0–5)",
            "metric_definition": "Integer scale 0-5 where higher is better; each metric has detailed prompt-specific definitions (e.g., 0 = no alignment/contradiction up to 5 = excellent support/perfect alignment; Evaluability measures clarity and testability from 0 Not-evaluable to 5 Fully-evaluable).",
            "dataset_or_benchmark": "SPDB (10 CyTOF single-cell datasets) and two clinical cohort mass-spectrometry datasets (HCC, GBM); evaluation corpus also included PubMed article sets retrieved per-hypothesis (10–20 articles).",
            "human_evaluation_details": "Subset: 30 hypotheses randomly selected from two SPDB datasets (Datasets 3 and 4). Reviews performed by 4 human experts in proteomics following the same 5-metric instructions; scores compared to automatic evaluator; agreement reported visually (Fig.7) and mean comparisons reported (humans typically score higher except for Novelty).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Across 191 hypotheses: mean Paper-Based Alignment 2.13, Literature-Based Alignment 3.24, Literature-Based Novelty 3.29, Logical Coherence 3.32, Evaluability 3.55 (Table 6). Automatic vs. human scoring showed reasonable agreement; humans scored higher on most metrics except Novelty.",
            "comparison_to_human_generated": true,
            "comparison_results": "Human expert scores on the examined subset were generally higher than automatic LLM scores for all metrics except Novelty; overall reasonable agreement within ±1 point for many items (visualized in paper).",
            "limitations_noted": "Paper-alignment is a poor proxy for hypothesis quality because original papers only cover a subset of dataset signals; automatic evaluators can be more sensitive to minor discrepancies than humans; no automated falsifiability tests or formal reproducibility experiments were performed; LLM context-length limits number of workflows/results retained for evaluation.",
            "uuid": "e7754.0",
            "source_info": {
                "paper_title": "Automating Exploratory Proteomics Research via Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Five Eval Metrics",
            "name_full": "Paper-Based Alignment; Literature-Based Alignment; Literature-Based Novelty; Logical Coherence; Evaluability",
            "brief_description": "A bespoke five-metric evaluation suite designed in this work to assess plausibility, alignment with the dataset's original paper, alignment and novelty with respect to general literature, logical/bio-plausibility, and the testability of each LLM-generated hypothesis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (automatic scoring), human experts (manual scoring)",
            "model_size": "GPT-4o (proprietary), human experts (N/A)",
            "scientific_domain": "Proteomics / Scientific hypothesis evaluation",
            "theory_type": "evaluation criteria / scoring rubric",
            "evaluation_method_name": "Five-metric scoring rubric (0–5 scale)",
            "evaluation_method_description": "For each hypothesis, evaluators (LLM or human) are provided the full hypothesis and optional reference info (original paper extract and 10–20 PubMed abstracts). The prompts define detailed criteria for each metric and request free-form analysis followed by an integer 0–5 score.",
            "evaluation_metric": "Five discrete integer metrics: Paper-Based Alignment, Literature-Based Alignment, Literature-Based Novelty, Logical Coherence, Evaluability.",
            "metric_definition": "Each metric scored on integer 0–5: 0 = contradiction/no support/not evaluable; 1 = minimal; 2 = some support; 3 = moderate; 4 = strong; 5 = excellent/perfect (detailed prompt text provides specific guidance per metric).",
            "dataset_or_benchmark": "Applied to 191 PROTEUS hypotheses across SPDB (10 CyTOF datasets) and 2 clinical MS datasets (HCC, GBM); PubMed article sets (10–20 abstracts) retrieved per hypothesis for literature-based metrics.",
            "human_evaluation_details": "Same 5-metric rubric given to human experts; subset of 30 hypotheses scored by 4 experts for validation; comparison statistics reported (means and agreement visualization).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Means reported in Table 6: Paper-Based Alignment 2.13, Literature-Based Alignment 3.24, Literature-Based Novelty 3.29, Logical Coherence 3.32, Evaluability 3.55; distributions shown in Figures 2,3,4.",
            "comparison_to_human_generated": true,
            "comparison_results": "Automatic metric scores were generally lower than human scores except for Novelty; inter-evaluator robustness shown by rerunning evaluation with multiple LLM evaluators (similar overall averages).",
            "limitations_noted": "Scales are subjective despite prompt detail; paper-alignment metric conflates differences in research focus with low quality; literature-based metrics depend on the search query quality and retrieved abstracts; no objective ground truth for novelty.",
            "uuid": "e7754.1",
            "source_info": {
                "paper_title": "Automating Exploratory Proteomics Research via Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GPT-4o Evaluator",
            "name_full": "GPT-4o (used as the primary automatic hypothesis evaluator)",
            "brief_description": "A state-of-the-art proprietary large language model used in this work as the main automatic evaluator to score PROTEUS-generated hypotheses across the five evaluation metrics using detailed prompts and retrieved literature.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_size": "proprietary (not explicitly stated)",
            "scientific_domain": "General / biomedical evaluation",
            "theory_type": "automatic evaluator (LLM-based scoring)",
            "evaluation_method_name": "LLM evaluator scoring (GPT-4o)",
            "evaluation_method_description": "GPT-4o receives each hypothesis plus optional reference material (original paper excerpts and PubMed article metadata/abstracts) and executes prompt-guided scoring for each of the five metrics, returning free-form reasoning plus a 0–5 integer score.",
            "evaluation_metric": "Five metrics (Paper-Based Alignment; Literature-Based Alignment; Literature-Based Novelty; Logical Coherence; Evaluability)",
            "metric_definition": "0–5 integer scale per metric with explicit rubric embedded in evaluation prompts (see paper prompts).",
            "dataset_or_benchmark": "191 hypotheses from 12 proteomics datasets (10 SPDB CyTOF + 2 clinical MS datasets); per-hypothesis PubMed retrieval (10–20 articles) used for literature-based metrics.",
            "human_evaluation_details": "Used for automatic scoring; results compared to 4 human experts on subset of 30 hypotheses to validate automated scoring.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Average scores reported (Table 6). Authors also ran evaluation using alternative evaluator LLMs (Gemini-flash-1.5, Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct) and observed similar overall averages, indicating robustness of automatic evaluation.",
            "comparison_to_human_generated": true,
            "comparison_results": "Automatic GPT-4o scores were generally lower than human expert scores except for Novelty; reasonable agreement across metrics and between different LLM evaluators.",
            "limitations_noted": "Evaluator sensitivity to minor discrepancies; dependence on retrieval query quality and retrieved abstracts; proprietary model details not provided; automatic evaluation does not replace experimental validation.",
            "uuid": "e7754.2",
            "source_info": {
                "paper_title": "Automating Exploratory Proteomics Research via Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Human Expert Review",
            "name_full": "Proteomics expert human scoring and case review",
            "brief_description": "Manual evaluation by domain experts who scored a random subset of PROTEUS-generated hypotheses using the same five-metric rubric and provided open-ended critiques on biological plausibility, novelty, and experimental verifiability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "human experts (domain reviewers)",
            "model_size": "N/A",
            "scientific_domain": "Proteomics / Biomedical research",
            "theory_type": "human evaluation / expert review",
            "evaluation_method_name": "Human expert scoring using five-metric rubric + qualitative review",
            "evaluation_method_description": "Four proteomics experts scored 30 randomly chosen hypotheses (from two SPDB datasets) using the same 0–5 rubric and provided detailed feedback and free-text reviews assessing hypothesis quality, novelty, and biological implications, enabling validation of automatic scoring.",
            "evaluation_metric": "Same five metrics (Paper-Based Alignment; Literature-Based Alignment; Literature-Based Novelty; Logical Coherence; Evaluability), plus qualitative commentary.",
            "metric_definition": "Integer 0–5 scale per metric following the prompt definitions used for automatic evaluation.",
            "dataset_or_benchmark": "Subset of PROTEUS outputs: 30 hypotheses from two SPDB datasets (Datasets 3 and 4).",
            "human_evaluation_details": "4 human experts; scored using the same instructions as LLM evaluator; provided detailed open-ended feedback; results compared against GPT-4o automatic scoring (Figure 7).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Human scores were on average higher than automatic LLM scores for all metrics except Novelty; qualitative reviews identified plausible novel hypotheses and also highlighted cases where PROTEUS lacked necessary biological nuance.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Small human-evaluated subset (30 hypotheses) limits generalizability; inter-rater agreement not fully quantified (visual agreement reported); human evaluation is resource-intensive and subjective.",
            "uuid": "e7754.3",
            "source_info": {
                "paper_title": "Automating Exploratory Proteomics Research via Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Evaluation Datasets",
            "name_full": "SPDB CyTOF single-cell datasets and Clinical Cohort MS datasets (HCC, GBM)",
            "brief_description": "Datasets used to generate and evaluate hypotheses: 10 single-cell CyTOF datasets from the Single-cell Proteomic DataBase (SPDB) and two clinical bulk proteomics datasets (hepatocellular carcinoma HCC and glioblastoma GBM) sequenced by mass spectrometry, together producing 191 hypotheses for evaluation.",
            "citation_title": "Spdb: a comprehensive resource and knowledgebase for proteomic data at the single-cell resolution.",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70B-UltraMed (PROTEUS backbone for generation)",
            "model_size": "70B (Llama-3.1 fine-tuned)",
            "scientific_domain": "Proteomics / Clinical proteomics",
            "theory_type": "data sources used for hypothesis generation and evaluation",
            "evaluation_method_name": "Dataset-driven hypothesis generation and dataset-aligned evaluation (paper-alignment & literature checks)",
            "evaluation_method_description": "PROTEUS ingests SingleCellExperiment objects (from SPDB) or raw protein+metadata CSVs (clinical MS) to produce hypotheses; evaluations reference the original paper for paper-alignment metric and use external PubMed retrieval for literature-based metrics.",
            "evaluation_metric": "Hypotheses produced on these datasets were evaluated across the five metrics (0–5 scale).",
            "metric_definition": "Same 0–5 integer rubric per metric; paper-alignment compares hypothesis to the original dataset's published paper.",
            "dataset_or_benchmark": "SPDB (10 CyTOF datasets) and clinical cohort MS datasets: HCC and GBM (total 12 datasets).",
            "human_evaluation_details": "Human review subset drawn from SPDB datasets (Datasets 3 and 4); full automatic scoring done across all 191 hypotheses from the 12 datasets.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "PROTEUS produced 147 hypotheses from CyTOF (SPDB) datasets and 44 from MS clinical datasets (total 191). Per-dataset average metric scores are reported in Table 5.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Original papers often only cover a fraction of dataset signals so paper-alignment may be low even for high-quality novel hypotheses; differences in data types required workflow adjustments (e.g., direct tool calls for clinical cohorts).",
            "uuid": "e7754.4",
            "source_info": {
                "paper_title": "Automating Exploratory Proteomics Research via Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Multi-run Selection Strategy",
            "name_full": "Multiple LLM runs with automatic selection to improve hypothesis quality",
            "brief_description": "A procedural method evaluated in the paper where PROTEUS performs multiple stochastic LLM runs on the same dataset and automatically selects the best hypotheses (by average score across four metrics) to improve overall evaluation scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70B-UltraMed (primary) / GPT-4o (evaluation)",
            "model_size": "70B (Llama-3.1 fine-tuned); GPT-4o (proprietary)",
            "scientific_domain": "Proteomics / ML evaluation methodology",
            "theory_type": "procedure to improve automatic selection of high-quality hypotheses",
            "evaluation_method_name": "Iterated runs + automatic best-selection by evaluator scores",
            "evaluation_method_description": "Due to LLM sampling randomness, run PROTEUS multiple times (N iterations), evaluate hypotheses automatically (excluding paper-alignment since it's a poor overall indicator), compute average scores across the remaining four metrics for each generated hypothesis set, and select highest scoring outputs; authors found average score improved with multiple iterations (notably after 5+ runs).",
            "evaluation_metric": "Average across 4 metrics: Literature-Based Alignment, Literature-Based Novelty, Logical Coherence, Evaluability (0–5 scale); used as selection criterion.",
            "metric_definition": "Mean of the four metric integer scores (0–5); selection based on higher mean score.",
            "dataset_or_benchmark": "Tested on SPDB CyTOF datasets (10 datasets); improvement quantified in Figure 6 (average score improved by &gt;0.2 after 5 iterations).",
            "human_evaluation_details": "Not directly human-evaluated; improvement measured via automatic evaluator (GPT-4o).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Average score across four metrics rose by more than 0.2 when using 5+ iterations of PROTEUS runs and selecting the best outputs automatically (Figure 6).",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Relies on the reliability of the automatic evaluator; increases compute cost; selection ignores paper-alignment metric intentionally because it's a poor proxy for overall quality.",
            "uuid": "e7754.5",
            "source_info": {
                "paper_title": "Automating Exploratory Proteomics Research via Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Spdb: a comprehensive resource and knowledgebase for proteomic data at the single-cell resolution",
            "rating": 2,
            "sanitized_title": "spdb_a_comprehensive_resource_and_knowledgebase_for_proteomic_data_at_the_singlecell_resolution"
        },
        {
            "paper_title": "Ultramedical: Building specialized generalists in biomedicine.",
            "rating": 2,
            "sanitized_title": "ultramedical_building_specialized_generalists_in_biomedicine"
        },
        {
            "paper_title": "DREAM: a biomedical data-driven self-evolving autonomous research system.",
            "rating": 1,
            "sanitized_title": "dream_a_biomedical_datadriven_selfevolving_autonomous_research_system"
        },
        {
            "paper_title": "MultiMedQA: a benchmark for medical question answering (composed of MedQA, PubMedQA, MedMCQA, biomedical MMLU categories)",
            "rating": 1,
            "sanitized_title": "multimedqa_a_benchmark_for_medical_question_answering_composed_of_medqa_pubmedqa_medmcqa_biomedical_mmlu_categories"
        }
    ],
    "cost": 0.0167375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automating Exploratory Proteomics Research via Language Models
6 Nov 2024</p>
<p>Ning Ding 
Tsinghua University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Shang Qu 
Tsinghua University</p>
<p>Linhai Xie 
National Center for Protein Sciences (Beijing)
Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>International Academy of Phronesis Medicine (Guangdong)</p>
<p>Yifei Li 
Tsinghua University</p>
<p>Zaoqu Liu 
National Center for Protein Sciences (Beijing)
Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>Kaiyan Zhang 
Tsinghua University</p>
<p>FrontisAI</p>
<p>National Center for Protein Sciences (Beijing)
Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>Yibai Xiong 
Tsinghua University</p>
<p>FrontisAI</p>
<p>Yuxin Zuo 
Tsinghua University</p>
<p>Zhangren Chen 
FrontisAI</p>
<p>Ermo Hua 
Tsinghua University</p>
<p>FrontisAI</p>
<p>Xingtai Lv 
Tsinghua University</p>
<p>FrontisAI</p>
<p>Youbang Sun 
Tsinghua University</p>
<p>Yang Li 
National Center for Protein Sciences (Beijing)
Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>Dong Li 
National Center for Protein Sciences (Beijing)
Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>Fuchu He 
National Center for Protein Sciences (Beijing)
Beijing Proteome Research Center
State Key Laboratory of Medical Proteomics</p>
<p>International Academy of Phronesis Medicine (Guangdong)</p>
<p>Bowen Zhou 
Tsinghua University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Automating Exploratory Proteomics Research via Language Models
6 Nov 2024A0EE324A3A442215F7DD4970F3B381F0arXiv:2411.03743v1[cs.AI]
With the development of artificial intelligence, its contribution to science is evolving from simulating a complex problem to automating entire research processes and producing novel discoveries.Achieving this advancement requires both specialized general models grounded in real-world scientific data and iterative, exploratory frameworks that mirror human scientific methodologies.In this paper, we present PROTEUS, a fully automated system for scientific discovery from raw proteomics data.PROTEUS uses large language models (LLMs) to perform hierarchical planning, execute specialized bioinformatics tools, and iteratively refine analysis workflows to generate high-quality scientific hypotheses.The system takes proteomics datasets as input and produces a comprehensive set of research objectives, analysis results, and novel biological hypotheses without human intervention.We evaluated PROTEUS on 12 proteomics datasets collected from various biological samples (e.g.immune cells, tumors) and different sample types (single-cell and bulk), generating 191 scientific hypotheses.These were assessed using both automatic LLM-based scoring on 5 metrics and detailed reviews from human experts.Results demonstrate that PROTEUS consistently produces reliable, logically coherent results that align well with existing literature while also proposing novel, evaluable hypotheses.The system's flexible architecture facilitates seamless integration of diverse analysis tools and adaptation to different proteomics data types.By automating complex proteomics analysis workflows and hypothesis generation, PROTEUS has the potential to considerably accelerate the pace of scientific discovery in proteomics research, enabling researchers to efficiently explore large-scale datasets and uncover biological insights.</p>
<p>Introduction</p>
<p>Proteomics research [1], which focuses on the large-scale analysis of protein expression, functions, and interactions, is a crucial avenue for understanding biological processes and their underlying mechanisms.Modern technologies [2] have facilitated highthroughput proteomics sequencing and large-scale data collection.The resulting datasets hold copious information on proteins, cells, pathways, as well as their complicated relationships and interactions.When combined with scientific analysis methods and domain knowledge, they have the potential to reveal valuable biological insights, including novel biomarkers [3], disease mechanisms [4], and therapeutic targets [5].On the other hand, the sheer volume and complexity of proteomics data also pose challenges for conventional research techniques and paradigms.Current proteomics research relies heavily on human experts to design and perform data analysis using professional methods and tools, making decisions ranging from specific data manipulation to general research directions.This process brings forward two main issues.First, analysis can be extremely time-consuming, especially when it involves trial-and-error over large sets of possible proteins or sample groups.Second, the researcher's personal knowledge and habits may bias experimental design, potentially impeding comprehensive analysis and limiting its overall scope.</p>
<p>We propose that large language models (LLMs) [6][7][8][9], the cornerstone of generative artificial intelligence, can enable unprecedented extents of automation in proteomics research.State-of-the-art LLMs possess powerful instruction-following abilities and extensive general knowledge, which have expanded their use cases from simple language tasks to a myriad of professional domains [10,11].They have also demonstrated impressive competence and flexibility in realms such as planning complex tasks and calling diverse tools [12,13].Additionally, for knowledge-intensive or time-sensitive scenarios, augmenting LLMs with information retrieved from external sources effectively reduces hallucinations and improves accuracy [14,15].The primary advantage of LLMs over previous machine learning approaches for scientific tasks is their versatility: instead of being confined to a narrowly defined problem, LLMs can simulate a broad range of tasks integral to scientific discovery workflows.In other words, through representing all decision-making steps, intermediate results, and reasoning processes as sequences of tokens, we enable LLMs to generalize across these disparate steps within the research process.In the context of proteomics research, the generalization capabilities of LLMs, combined with specific data, information, and tools, can enable automated systems to advance from surface-level data analysis to in-depth scientific hypothesis proposal.This allows for more efficient, comprehensive, and insightful explorations of high-throughput proteomics data, and mitigates human experts' possible biases by uncovering hypotheses they might have overlooked.With this objective, we envision an end-to-end proteomics analysis and knowledge discovery system, where the input is raw data and the output is a set of scientific hypotheses derived from that data.</p>
<p>In this paper, we develop a fully automated LLM-based PROTeomics Exploration and Understanding System (PROTEUS) for proteomics analysis and scientific knowledge discovery.PROTEUS first receives raw omics data and basic dataset information, based on which it plans data-dependent analysis procedures across three hierarchies: research objectives, analysis workflows, and analysis tools.Guided by these plans, the system automatically executes a sequence of analysis steps using bioinformatics tools, then interprets the results.Furthermore, PROTEUS performs iterative refinement after completing each workflow or objective, updating subsequent plans based on the latest results.We demonstrate the capabilities of PROTEUS through both automatic and human evaluation.PROTEUS automatically analyzed 12 diverse proteomics datasets and produced a total of 191 high-level scientific hypotheses.We first used LLMs to score each hypothesis based on 5 metrics, with access to supplementary information such as the original research paper and related articles.To validate the automatic scoring results, we randomly selected a subset of results and obtained evaluation scores from human experts using the same metrics and instructions.Experts also provided detailed, open-ended feedback on the quality, novelty, and biological implications of the hypotheses.We show that PROTEUS can flexibly explore different types of proteomics data, consistently producing high-quality and novel scientific hypotheses.</p>
<p>Previous research on using LLMs to enhance omics research has largely been confined to isolated steps within the complex research process.Common tasks include batch effect correction [16], cell type annotation [16], and differential gene selection [17].While these methods provide valuable assistance to bioinformatics researchers, they lack the flexibility and comprehensiveness required for automating entire omics research procedures.In contrast, we enable PROTEUS to freely employ diverse methods and directly derive scientific hypotheses from raw data, bringing the system's performance closer to the iterative, exploratory research process of human experts.Most existing methods that similarly encompass the full bioinformatics research pipeline still rely heavily on human intervention, either requiring a pre-determined procedure to link single steps [18], or relying on frequent user inputs to guide the analysis [19] [20].DREAM [21], while eliminating human inputs, evaluates the system's outputs solely by judging whether the initial research question was resolved, lacking verification of the reliability and depth of the results.We address this gap by jointly employing human evaluation and a well-rounded suite of automatic evaluation methods and metrics that align with the open-ended nature of PROTEUS 's outputs.Therefore, our work represents a pioneering effort to incorporate both proteomics research and result evaluation in a fully automated, end-to-end manner, advancing AI-assisted efficient bioinformatics research.</p>
<p>Results</p>
<p>PROTEUS System</p>
<p>Towards the goal of automated proteomics research from raw data, we develop PROTEUS , a system that combines the general abilities of language models with the accuracy of domain-specialized analysis tools and knowledge sources.The system's input is a proteomics dataset consisting of protein expression data and cell or sample metadata.A large language model orchestrates the analysis process and arrives at a list of specific, data-grounded scientific hypotheses.</p>
<p>Large Language Models</p>
<p>Currently, the gap between proprietary and open-source language models is narrowing, with both showing capabilities for executing complex planning and reasoning tasks.Given the confidential and privacy-sensitive nature of proteomics data, as well as the need for iterative model improvements, we train a general-purpose model with a focus on the biomedical field.Specifically, we train models on biomedical instruction datasets to enhance their capabilities for analysis, planning, and knowledge in biomedicine, thereby improving their performance within our proteomics scientific discovery system.We predominantly adopt the state-of-the-art open-source LLM, Llama 3.1 [7], as our backbone architecture.With 70 billion parameters, it outperforms previous open-source models [6] across a range of tasks.For further domain specialization, we fine-tuned Llama 3.1 70B on the UltraMedical dataset [11], which contains diverse and high-quality biomedical instructions, including a wealth of open-ended questions on biomedical research and literature.The resulting models demonstrate superior performance on downstream tasks compared to other open-source models.</p>
<p>System Framework</p>
<p>PROTEUS arrives at a comprehensive and meaningful set of hypotheses through navigating possible objectives, executing statistical analysis, and iteratively improving its analysis plans.Considering the complexity and diversity of proteomics research, we devise a hierarchical planning framework consisting of three levels, ranging from general to specific: research objectives, analysis workflows, and analysis steps.This design increases flexibility and robustness in the LLM planning process, which</p>
<p>Step-level Results</p>
<p>Iterative refinement of workflows (a) Workflow-level Results</p>
<p>Iterative refinement of objectives</p>
<p>Workflows</p>
<p>Research Objective Objective Updating Workflows</p>
<p>Workflow Updating Workflow-level Results Steps</p>
<p>Workflow Results</p>
<p>Increase</p>
<p>Investigate Disease Mechanisms</p>
<p>Propose Potential Drug Targets</p>
<p>Identify Disease Biomarkers</p>
<p>The significant increase in CD20 in TmDC suggests a potential interaction or crosstalk between B cells and T cells.A novel hypothesis could be that the B cell signatures in TmDC are indicative of a broader immune response... Proteomics data collecting using mass cytometry, with a total of 42 proteins ...</p>
<p>The following cell types have significantly higher expression in tumor cells:</p>
<p>Seq. of Workflow Results</p>
<p>Clustering &amp; Annotation Statistical Testing Knowledge Mining</p>
<p>Investigate the disease mechanism of ...</p>
<p>Clustering + Annotation: Clusters cells based on protein expression levels, then annotates cell types based on top markers</p>
<p>Seq. of Obj.Results</p>
<p>Intermediate or Final Results</p>
<p>Research Processes</p>
<p>Large Language Models Textual Information</p>
<p>Result Analyzer</p>
<p>The following cell types have significantly increased expression in tumor cells: ... forms the backbone of PROTEUS .We also incorporate self-refinement and hypothesis proposal steps to further improve result quality.We detail the design of each module below.</p>
<p>Proteomics data collecting using mass cytometry, with</p>
<p>Research Objectives: Guiding the Trajectory of Proteomics Exploration.Proteomics research encompasses diverse objectives, such as establishing protein interactions, elucidating disease mechanisms, and identifying disease biomarkers.These high-level objectives determine the direction of data analysis and hypothesis proposal.In PROTEUS , we take advantage of the planning and reasoning capabilities of LLMs to dynamically generate and refine these research objectives.The LLM first generates a description of the input data, covering both protein expression data and relevant metadata, providing a comprehensive overview of the dataset.It includes information such as the number of proteins and cells sequenced, the conditions of the cell samples, and other important metadata paired with their possible values.Given the data description, the LLM proposes several potential research objectives to be considered sequentially and is encouraged to tailor them to data characteristics.For instance, it may highlight important cell markers or cell types based on general knowledge of the disease conditions mentioned in the description.Analysis Workflows: Streamlining Complex Bioinformatics Processes.Due to the rigorous dependencies and high specificity of many bioinformatics data analysis methods, we organize a large number of analysis tools into a set of analysis workflows, each consisting of one or more tools to be executed in sequence, as well as additional steps necessary for the analysis process.One example is the cell type annotation workflow, which calls the cell clustering tool, then the cluster annotation tool.For a certain research objective, we prompt the LLM with the objective, the data description, and a list of descriptions of all available data analysis workflows, then instruct it to plan a series of workflows.This design greatly reduces the probability that the system encounters errors caused by tool or data dependencies.It also reduces the difficulty and complexity of planning by reducing the total number of options given to the LLM.Analysis Steps: Enabling Professional, Data-Grounded Proteomics Analysis.Each analysis workflow includes both bioinformatics tools and additional analysis steps conducted by the LLM, which serves as an orchestrator for tool-related functionalities.We focus on two critical tasks: determining optimal tool parameters and interpreting complex execution results.</p>
<p>To automatically set tool parameters, the LLM is provided with the research objective, the data description, and an explanation of each parameter.For interpreting results, PROTEUS supports various formats of tool outputs, including text, data files, and visualization plots, and analyzes notable results within the context of the research objective.For example, after performing differential abundance analysis, PROTEUS summarizes key insights and biological implications based on the result file, identifying and emphasizing cell types that exhibit statistically significant changes and are related to the objective.The LLM is capable of providing further in-depth analysis, for instance regarding the functions of the notable cell types and the biological implications of their changes.Hierarchical Iterative Refinement.Proteomics research is an iterative process in which results from preliminary analysis stages can be conducive to deeper and more detailed exploration.Therefore, we enable PROTEUS to refine its plans after each execution stage.Following each workflow execution, the LLM refers to the newly obtained workflow results to update the original plan in preparation for subsequent workflow execution.It performs a similar step after analyzing each objective, using the latest results to refine future research objectives.These additional steps assist PROTEUS in both handling errors and deepening scientific inquiry.For instance, if initial analysis reveals that a particular cell type exhibits significant changes in abundance, the updated research objectives may propose identifying more fine-grained subtypes to clarify the observed trends.Hypothesis Proposal.Finally, PROTEUS proposes hypotheses from the completed analysis through integrating information on research objectives, executed workflows, and result interpretations.We instruct the LLM to emphasize the most significant and novel results among a large number of possible directions.The prompt specifies a fixed format for each hypothesis, consisting of an overview of proteins and cell conditions, a summary of statistical tests and corresponding numerical results, and a final scientific hypothesis.As a result, the summaries effectively link all proposed hypotheses to direct statistical observations derived from raw input data.This increases the interpretability of PROTEUS 's outputs and enables effective evaluation.</p>
<p>Features of PROTEUS</p>
<p>From Raw Data to Scientific Discoveries.PROTEUS exemplifies a paradigm shift in bioinformatics research by achieving a fully automated pipeline that produces scientific discoveries from raw data.Unlike traditional methods that rely heavily on human intervention and manual data processing, PROTEUS leverages the capabilities of LLMs to autonomously navigate the complete research process.This end-to-end pipeline ensures consistency, reduces the potential for human error, and significantly reduces the time spent between data acquisition and hypothesis generation.Scalable Integration.Due to its hierarchical planning framework, PROTEUS can seamlessly integrate diverse proteomics analysis tools and knowledge sources.The three-tiered structure, encompassing research objectives, analysis workflows, and individual tools, enables flexible adaptation to heterogeneous proteomics datasets and diverse research directions.Therefore, PROTEUS can conveniently incorporate new analysis methods and external data while maintaining a fixed system framework.Furthermore, the LLM's role in parameter assignment and result interpretation allows PROTEUS to execute specialized bioinformatics tools while maintaining a unified interface.This approach of scalable integration positions PROTEUS to evolve alongside advancements in bioinformatics methods and technologies, ensuring its long-term effectiveness.Dynamic Feedback Loop.PROTEUS implements an iterative refinement process that mimics the recursive process of scientific inquiry commonly employed by human experts.After each workflow execution and objective analysis, the system reevaluates and refines its subsequent plans based on newly obtained results.This dynamic approach allows PROTEUS to adapt to unexpected findings and pursue promising avenues of research that may not have been initially apparent.By incorporating feedback loops at multiple levels of analysis, PROTEUS can conduct thorough and nuanced investigations, uncovering insights that might be overlooked under linear analysis approaches.</p>
<p>Evaluation</p>
<p>Base Language Models</p>
<p>We aim for the base language model of the system to be a specialized generalist, achieving enhanced specialization in the biomedical field without compromising its generalization abilities on common tasks.We present the evaluation results of our custom Llama 3.1 models tailored to UltraMedical specifications.We primarily base our evaluation methodology on the protocols outlined in [11] and assess the models across widely recognized medical and general benchmarks.For medical benchmarking, we selected MultiMedQA, which has been extensively employed in MedPaLM-related studies [22,23].This benchmark comprises MedQA [24], PubMedQA [25], MedMCQA [26], and biomedical categories within MMLU [27].We select these tasks to assess the LLMs' application of biomedical knowledge.Additionally, for general instruction following and knowledge integration, we primarily evaluate the models across the comprehensive set of MMLU, GPQA [28], and Alpaca Eval 2 [29] benchmarks.</p>
<p>The overall results are listed in Table 2, and the detailed performance in medical domain is reported in Table 3.Our model demonstrates impressive performance across both biomedical and general domain benchmarks.On the MultiMedQA benchmark, our model achieves an average accuracy of 86.30%, surpassing other biomedical-focused models such as Med42-70B (70.74%),OpenBioLM-70B (86.06%), and Med-PaLM 2 (ER) (85.46%).Notably, it also outperforms general domain models including GPT-3.5-Turbo (67.80%) and comes close to GPT-4-Turbo (87.00%).On the Alpaca Eval 2 benchmark, our model shows strong performance with a win rate (WR) of 46.09% and a Likert score (LC) of 43.45%, considerably outperforming other biomedical models and many general domain models.The MMLU benchmark presents similar results.On the GPQA benchmark, our model demonstrates an accuracy of 45.76%, competitive with top-performing general models such as GPT-4-Turbo (49.10%) and Llama-3.1-70B-Instruct(46.70%).Our model acts as the core orchestrator within PROTEUS , supporting its comprehensive planning and reasoning, leading to novel scientific hypotheses.</p>
<p>Quantitative Evaluation</p>
<p>We conducted experiments and quantitative evaluation on two types of proteomics data.First, we used the Single-cell Proteomic DataBase (SPDB) [36] to obtain 10 single-cell datasets which used cytometry by time-of-flight (CyTOF) [37] sequencing technology.9 datasets were sequenced on various human tissues, and 1 dataset covered mouse brain tumor tissue.For all experiments on SPDB, PROTEUS 's input was a SingleCellExperiment [38] object directly downloaded from SPDB and a textual data description constructed using information from the data object and the SPDB website.For every dataset, we set the maximum number of total research objectives to 3 and instructed PROTEUS to generate 5 hypotheses for each objective.On several objectives, PROTEUS produced less than 5 hypotheses due to a lack of notable results from the analysis.We collected a total of 147 hypotheses for CyTOF data.In addition, to demonstrate the flexibility of PROTEUS , we obtained two clinical proteomics datasets from previous publications on hepatocellular carcinoma (HCC) [39] and glioblastoma (GBM) [40].These datasets contain bulk proteomics data sequenced using mass spectrometry (MS) [41] and cover significantly larger numbers of proteins than the previously described SPDB datasets.The system's input for each dataset was 2 files containing the raw protein  For the two clinical proteomics datasets, we adjusted the system in two main ways to address the differences in data characteristics.First, we allowed PROTEUS to directly call individual tools without introducing workflows.This is because bioinformatics analysis on clinical proteomics data is generally more flexible, with less reliance on fixed analysis pipelines.Second, since this data type requires distinct analysis methods, we replaced the set of analysis tools available to PROTEUS with bioinformatics tools commonly used for clinical proteomics data.Details on all analysis workflows and tools are provided in Table 1.The statistics of the datasets and hypotheses produced by our system are listed in Table 4a.In Table 4b, we provide the frequency each workflow was called during one experimental run over 10 CyTOF datasets.We used the state-of-the-art large language model GPT-4o to assist automatic quantitative evaluation.We evaluated each hypothesis separately according to 5 distinct metrics by prompting the LLM with the full hypothesis and optional refererence information.The prompts outlined detailed scoring criteria for the metrics and instructed GPT-4o to provide free-form analysis, followed by an integer score between 0 and 5.The 5 evaluation metrics are: Paper-Based Alignment, Literature-Based Alignment, Literature-Based Novelty, Logical Coherence, and Evaluability.These metrics are informative indicators of the plausibility, novelty, and potential for further exploration of a scientific hypothesis.</p>
<p>We next discuss the general evaluation results and provide detailed evaluation prompts in 4.2.Table 6 shows the overall scores of the 191 hypotheses, and Figure 2 provides score distributions on all hypotheses.Table 5 lists the different results on each dataset.Evaluation based on corresponding published research.For each dataset, we accessed the original research paper that published and analyzed the data.We extracted the text and located the most relevant text chunk for each hypothesis, after which Distributions of Automatic Evaluation Scores on SPDB Datasets Fig. 3: Score distributions on the 10 SPDB datasets, over 5 metrics.Specific dataset information corresponding to each index is provided in Table A1.GPT-4o evaluated the degree of overlap.We note that most proteomics datasets contain substantial amounts of information and potentially insightful trends, out of which only a subset is elaborated in the paper.PROTEUS is designed to conduct analyses in a comprehensive and unbiased manner, considering all possible research directions.Therefore, we expect that most responses will fall outside the scope of the original paper.A low score on this metric likely indicates discrepancies between the research focuses of PROTEUS and those of the original paper, rather than reflecting low quality of the results.</p>
<p>P L N C E Metric</p>
<p>Scores on this metric ranged from 0 to 5, with an average of 2.13, meaning that most hypotheses proposed by PROTEUS were not covered in the original research papers.Evaluation based on general literature.We next introduce two evaluation metrics that compare PROTEUS 's outputs against general biological literature.For each hypothesis, we automatically searched 10-20 of the most relevant articles on PubMed and extracted their PMIDs, titles, and abstracts.The search term was a concise query generated by GPT-4o based on the full output, comprising the most important biological entities present in the output.Using this information, GPT-4o individually scored the generated hypotheses according to two considerations: 1) Literature-Based Alignment, which assesses the degree of alignment between the generated results and existing research; and 2) Literature-Based Novelty, which evaluates their originality within the context of current biological literature.</p>
<p>PROTEUS scored an average of 3.24 on Literature-Based Alignment and 3.29 on Literature-Based Novelty.Hypotheses consistently scored 3 or 4 on Novelty.On the other hand, results for Literature-Based Alignment showed the largest deviation among all 5 metrics, with scores ranging from 0 to 5. Nonetheless, more than 78% of hypotheses scored at least 3 points, indicating that most results were partially concordant with existing research.Direct evaluation.Finally, we incorporate two metrics that can be evaluated solely based on the generated hypotheses, without additional information: 1) Logical Coherence, which assesses whether the output is logically plausible considering general biological principles; and 2) Evaluability, which determines whether the hypothesis can be further evaluated and verified through existing statistical methods or experimental procedures in the field.The LLM relies on its general domain knowledge and reasoning skills to judge the output.</p>
<p>PROTEUS 's results reliably reached satisfactory scores, despite the evaluation system being relatively strict.Average scores for Logical Coherence and Evaluability were 3.32 and 3.55 respectively.Notably, all hypotheses had Evaluability scores of 3 or 4, and only 5.24% of hypotheses had Logical Coherence scores of 2. In other words, an overwhelming majority of hypotheses were biologically plausible and at least partially evaluable.Comparison of base language models.To evaluate the impact of the choice of LLMs in our framework, we conducted experiments on the 10 CyTOF datasets using GPT-4o as the base LLM, keeping the system framework and available workflows unchanged.We performed automatic evaluation using the same prompts and present the results in Figure 5. Results were mixed across the 5 different metrics, with GPT-4o achieving a slightly higher average score.Nonetheless, as the backbone model of PROTEUS , our model demonstrated competitive performance compared to the state-of-the-art proprietary model.Performing multiple experimental runs.Due to the randomness in sampling outputs from an LLM, PROTEUS produces slightly different results for the same dataset with each run.Therefore, we investigated whether obtaining multiple sets of outputs and then automatically selecting the best hypotheses could improve the overall scores.</p>
<p>We present the results in Figure 6.Since degree of alignment with the original paper is a poor indicator of overall hypothesis quality, we calculated the average score across the remaining four metrics.All four metrics exhibited a general upward trend, with the average score improving by more than 0.2 starting from 5 iterations.These findings, combined with the efficiency of PROTEUS , demonstrate a promising approach for further enhancing result quality.</p>
<p>Verifying Evaluation Quality.</p>
<p>Comparison with human scoring.We randomly selected two datasets (Datasets 3 and 4) from SPDB, corresponding to 30 hypotheses.Human experts in proteomics research scored these hypotheses over the 5 metrics, following the same instructions provided to the LLM evaluator.These results, shown in Figure 7, demonstrate the rigor and validity of automatic scoring.For all metrics except Novelty, the average scores given by human evaluators were higher than those from LLM automatic evaluation, indicating that automatic evaluation was generally more sensitive to minor errors or discrepancies.In addition, automatic and human scoring showed reasonable levels of agreement across all metrics.Comparison of different evaluator LLMs.In all previous experiments, we used GPT-4o as the automatic evaluator.We subsequently reran the evaluation procedure on all SPDB results using three other language models as evaluators: Gemini-flash-1.5,Qwen2.5-7B-Instruct, and Llama-3.1-8B-Instruct.We present the average scores on the 5 metrics and the overall average scores in Figure 8.The resulting scores were close on all metrics, particularly for the overall average score, confirming the robustness of our automatic evaluation procedure.</p>
<p>Case Studies</p>
<p>We highlight PROTEUS 's ability to propose insightful and novel hypotheses through in-depth evaluation by human experts.We selected subsets of hypotheses from both types of proteomics data.Human experts reviewed the stated statistical trends and resulting hypotheses with reference to the original research paper and dataset.In the following section, we expand on several notable hypotheses to demonstrate PROTEUS 's advantages as well as limitations.PROTEUS identifies trends on rarely-studied biological topics and proposes novel hypotheses.In our experiments, PRO-TEUS often focused on proteins or cell types that were seldom studied in the considered context, enabling novel and valuable  results.On Dataset 2, the system observed significant changes (log 2 FC = 4.812894, adjusted p-value = 0.007) in mesothelioma cells between different stages of High-Grade Serous Ovarian Cancer (HGSC) and suggested that "the increase in mesothelioma cells from III A to IV stages could indicate a potential therapeutic target."Human experts provided positive feedback regarding the system's focus on the relationship between mesothelioma cells and HGSC, and judged the hypothesis as plausible, novel, and interesting.They noted that mesothelioma cells have seldom been studied in HGSC research.Further research via PubMed revealed less than 10 previous works that directly discussed this relationship, exploring the roles of the methothelial barrier [52], methothelial cells [53], and mesothelial genes [54,55], in HGSC progression and clinical outcomes.None of the existing works directly overlapped with PROTEUS 's proposed hypothesis.The biological plausibility of the hypothesis is supported by mesothelioma cells' known ability to aggravate cancer through creating immunosuppressive environments [56] and stimulating cancer cell growth.The former occurs through secretion of immunosuppressive factors (e.g.TGF-β, IL-10, PGE2) [57,58] and induction of regulatory T cells [59], the latter through secretion of growth factors (e.g.VEGF, FGF) [60] and influence on extracellular matrix (ECM) interactions [61].</p>
<p>On the other hand, experts noted that HGSC can increase mesothelioma abundance as it invades the abdominal cavity, where mesothelial cells naturally line the perotoneum.HGSC cells may induce their transformation into tumor-promoting phenotypes through mechanisms such as cell signaling pathways [62,63], ECM interations [64], and influence on mesothelialto-mesenchymal transition (MMT) [62].This bidirectional interplay makes possible a reinforcing cycle: mesothelioma cells promote HGSC growth, and HGSC invasion further increases mesothelioma abundance.Therefore, the observed trend does not establish that high mesothelioma presence is a definitive cause of HGSC, a crucial basis for considering their therapeutic potential.These complexities and possible alternative explanations underscore the need for more rigorous examination of the hypothesis.PROTEUS makes reasonable connections between different biological topics.We observed that PROTEUS was able to progress from identifying individual statistical trends to performing high-level analysis on biological mechanisms, often through connecting different biological topics.On Dataset 4, PROTEUS focused on the significant increase of CD20 expression in tumor microenvironment-derived cytokines (TmDCc) (log 2 FC = 0.726746, adjusted p-value = 0.041301), and proposed that this may indicate broader immune responses involving B and T Cell interactions, possibly mediated through CD20.</p>
<p>Human evaluators stated how this hypothesis is supported by relevant research, but has not yet been directly studied.Particularly, CD20's role in B and T Cell interactions has been touched on by existing research.CD20 can influence B Cells' role as antigen-presenting cells (APCs) [65], and its direct association with MHCII and CD40 [66] also suggest an impact on T Cell activation.CD20+ B Cells may influence T Cell differentiation through cytokine production (e.g.IL-2, IL-4) or recruit T Cells through chemokine release (e.g.CCL3, CCL4, CXCL10) [67].Additionally, CD20's crucial role in disease progression and therapeutics is reflected in research on anti-CD20 therapies, which can take effect through multiple mechanisms, including diminishing Regulatory T Cell populations [68].Considering that these mechanisms are yet to be studied under TmDC conditions, further exploring the proposed perspective may uncover novel mechanisms of interaction and deeper insights into disease responses.This result shows PROTEUS 's familiarity of commonly considered research directions and its ability to extend its analysis beyond individual, surface trends.PROTEUS carries out logically coherent bioinformatic pipelines in clinical cohort analysis and generates reliable hypothesis.While the above cases were all produced on SPDB datasets, we also noticed a large number of high-quality hypotheses among results from clinical cohort datasets.The data characteristics, common research themes, and analysis methods of clinical cohort data all differ considerably compared with SPDB datasets.Therefore, the following results indicate the versatility and flexibility of PROTEUS 's system design.</p>
<p>On the GBM dataset (Dataset 11), PROTEUS performed a comprehensive multi-step investigation of protein expression levels on GBM tumor and normal tissues.It first executed differential expression analysis (DEA) between GBM and normal samples to identify SNX32, along with VIM, LIMA1, NES, and S100A6, as significantly differentially expressed proteins.These proteins were filtered using log fold changes (| log 2 FC| &gt; 1) and false discovery rates (FDR &lt; 0.05), then identified by the LLM as proteins of interest based on both logFC values and P values.PROTEUS centered the subsequent analysis on SNX32, using correlation analysis (CA) to reveal that both VIM and NES exhibit prominent negative correlations with SNX32 (correlation coefficient &lt; −0.3, p &lt; 0.05).It then conducted gene set enrichment analysis (GSEA), after which it highlighted the downregulation of synaptic vesticle docking and the upregulation of integrin-mediated signaling.By integrating these multiple layers of evidence, PROTEUS formulated the hypothesis that SNX32 functions as a tumor suppressor protein, primarily through its inhibitory effect on VIM expression.</p>
<p>According to human evaluators, the above process demonstrates that the system can not only understand and apply various bioinformatics tools, but also construct a logically coherent bioinformatics pipeline.Regarding the proposed hypothesis, SNX32 (Sorting Nexin 32) has never been studied in the context of GBM, as no relevant literature was found.In contrast, VIM (Vimentin) is a well-studied structural protein that plays a crucial role in tumor progression by promoting epithelialmesenchymal transition [69].It has been reported to be a critical marker for glioma progression, associated with increased tumor invasiveness and poor prognosis [70,71].The GSEA results from PROTEUS aligned well with these known trends, and correlation analysis results provided a basis for connecting SNX32 and VIM.Although further experimental validation is needed to establish the proposed causal relationship, experts believed that obtaining this novel and biologically plausible hypothesis under fully automated experimental settings effectively demonstrates PROTEUS 's ability in knowledge discovery.Better leveraging the biological knowledge of LLMs can potentially improve PROTEUS 's performance.In Dataset 1, cell conditions were labeled as "GvHD" or "Normal", without information on whether the disease is acute or chronic.As a result, PROTEUS drew broad conclusions regarding GvHD, reporting a significant decrease in the abundance of cytotoxic T cells and deducing that this trend may contribute to the impaired immune response in GvHD patients.However, this claim overlooked crucial biological distinctions [72,73] between acute and chronic GvHD, regarding both Cytotoxic T Cell and Memory T Cell dynamics.</p>
<p>After pinpointing this flawed result, our subsequent experiments revealed that with accurate instructions, our base LLMs could correctly explain the above differences.This means that PROTEUS 's current framework and prompt designs have not fully exploited the knowledge base of LLMs.In the above example, this limitation caused it to establish proposals on identified trends without fully considering pertinent biological context and nuances.We conclude that improving the rigor of the hypotheses would require PROTEUS to more effectively elicit knowledge from LLMs at key steps of its analysis process.</p>
<p>Discussion</p>
<p>In this paper, we introduced PROTEUS , an end-to-end, fully automatic system for scientific discovery from raw proteomics data.An LLM acts as the core coordinator of the system, performing hierarchical planning, analysis tool calling, iterative feedback and refinement, and hypothesis proposal.We incorporated a large number of professional bioinformatics tools and organized them into analysis workflows that can be conveniently called by the system to investigate specific datasets.In this way, we have built upon the capabilities of the base LLM to form a system that better adheres to the empirically grounded and exploratory nature of scientific research.</p>
<p>We performed detailed evaluation on PROTEUS 's outputs both quantitatively and qualitatively.We constructed a set of 5 metrics and corresponding instructions, then used LLMs to perform large-scale automatic evaluation on a total of 191 hypotheses from two proteomics dataset types.Detailed reviews and scoring from 4 human experts corroborated the reliability and rigor of our automatic evaluation method.Experts also identified a number of novel hypotheses that point out promising directions for further research.Through examining these notable cases, we highlighted PROTEUS 's ability to pinpoint underexplored biological topics, couple specific quantitative results with general domain knowledge, and establish connections between multiple statistical trends or biological entities.Capabilities such as these empower the system to progress past surface-level observations to perform in-depth scientific reasoning and discovery.In general, results demonstrate that PROTEUS consistently produces reliable results, is capable of forming novel and insightful hypotheses, and can be easily adapted to different data types.Our work is the first to achieve both proteomics research and result evaluation in an effective, automated, and end-to-end manner.PROTEUS distinguishes itself from bioinformatics assistants that focus on single analysis steps, and through simulating the full scientific inquiry process, makes important advances towards new paradigms of bioinformatics research.</p>
<p>We identify several current limitations of PROTEUS .First, as elaborated in Section 2.2.4,facing flawed or incomprehensive results, PROTEUS has yet to fully elicit the base language model's knowledge to provide necessary explanations and qualifications.Refining prompting instructions within the system and augmenting it with LLM self-reflection mechanisms are potential methods for addressing this shortcoming.Second, since we provide the LLM with direct access to all previous analysis records, the number of workflows or tools called is limited by the context length of the language model.In most of our experiments, PROTEUS called a maximum of 5 workflows on SPDB datasets and 6-8 workflows on clinical cohort datasets.With the goal of enabling PROTEUS to handle more complicated analysis processes, a potential improvement is to develop more sophisticated memory management methods to concisely and dynamically provide the system with the most relevant analysis records at each given stage.Such improvements will potentially increase the complexity of the system's analysis by large margins and amplify the advantages of its iterative refinement mechanism, leading to more in-depth results.</p>
<p>We believe that PROTEUS charts a promising path towards more efficient and comprehensive research in bioinformatics.Its features, including using research objectives to guide analysis, flexibly calling professional analysis tools, and iteratively adjusting the research process, are highly generalizable to diverse directions of scientific discovery.Therefore, the design principles of PROTEUS can be extended beyond proteomics to multi-omics analysis or even other realms of biomedical research.Moreover, future developments in both general large language models and specialized bioinformatics analysis methods will continually improve the quality of PROTEUS 's analysis and results.</p>
<p>Method</p>
<p>Analysis Workflows and Tools</p>
<p>In this section, we provide an overview of the main analysis workflows and tools available to PROTEUS in our main experiments, explaining the role of language models in flexibly and correctly configuring the tools.</p>
<p>Analyzing CyTOF Data</p>
<p>Workflows in this section are tailored towards analyzing proteomics data obtained from CyTOF sequencing.FlowSOM Clustering and Cell Type Annotation This workflow clusters single cells based on protein expression, extracts highly expressed cell marker proteins of each cluster, then performs cell type annotation on the clusters.For clustering, we use the CATALYST [74] package to execute the FlowSOM [75] algorithm, a self-organizing map-based method designed for flow or mass cytometry data.We set the inital cluster number to 30.We use the scran [76] package to identify the top 10 cell markers of each cluster to prepare for automatic cell type labeling.</p>
<p>Previous research [77] has shown that GPT4 can generate cell type annotations given cell markers and the tissue type, achieving higher degrees of agreement with human annotations compared with conventional reference-based approaches.Therefore, we similarly use GPT-4o for labeling and designed the following prompt:</p>
<p>Prompt for Cell Type Annotation</p>
<p>Identify the cell type of <tissue name> using the following markers, arranged from highest to lowest expression levels.This means you should consider the first several markers in the list to be more important.Provide your result as the most specific cell type that is possible to be determined.Markers: <markers> Provide your output in the following format:</p>
<p>Analysis: <brief analysis of the cell markers and their relationship to cell types> Cell Type: <cell type name> Strictly adhere to this format and do not include any additional words or explanations.</p>
<p>Furthermore, we included an additional annotation refinement step to correct cell type name discrepancies that arose from using multiple LLM calls for the initial annotation process:</p>
<p>Prompt for Cell Type Annotations Refinement</p>
<p>You are a bioinformatics researcher.You will be given a list of <cell type number> cell type annotations.The annotations were performed individually, so there may be cases where the same cell type has been assigned slightly different names.Your task is to refine the list of cell type annotations to ensure that the same cell type is assigned the same name.For instance, if two annotations are 'Memory CD8 + T Cells' and 'CD8 + memory T cell' respectively, you may choose to change them both to 'Memory CD8 + T Cells'.Additionally, if you think some names are too specific, you can choose to make them more general so that they can be merged with other cell types in the list to facilitate future analysis.For instance, if there are many annotations of 'Memory CD8 + T Cells' and only one 'Central Memory CD8 + T Cells', you may change the latter to 'Memory CD8 + T Cells'.Ensure that the names are concise and specific.Provide your output in the same format as the input (a list of cell type annotations separated by commas, where each term is a refined cell type name).Do not include any additional words or explanations.Original annotations: <original cell type annotations> After cell type labeling, clusters that were assigned the same cell types were merged, and the final cell types were stored in the SingleCellExperiment object as an additional set of cluster codes.We also organize the full set of cell types and their corresponding cell markers used into a natural language description, which is stored in the system's history as the execution result of this workflow.Figure 9 illustrates the full process of this workflow.Clustering and Annotation Refinement This workflow provides the option to perform further clustering and more fine-grained cell type labeling on an existing cluster.It performs dimension reduction on the clustered cells based on protein expression levels, then generates a plot where the cells are colored according to cell type.The LLM interprets the plot and outputs one cell type to refine.We subset the data object to only include cells of the selected cell type, then implement the same clustering and labeling workflow as for the initial clusters.Finally, clusters and cell type labels are updated with the refined annotations.</p>
<p>For example, a large T Cell cluster may be refined into three sub-clusters: "CD4 + T Cells", "CD8 + T Cells", "Regulatory T Cells".Protein Abundance Visualization This workflow intends to provide general, qualitative information on the proteomic landscape of the dataset and includes analysis on two types of plots.First, it generates a heatmap of all protein expressions over different samples, with auxiliary labels of sample conditions provided.Second, it visualizes expression levels of individual proteins.Based on the research objective, the LLM selects several proteins of interest, and a plot is generated for each protein, with samples colored according to sample conditions.The LLM interprets both these images and generates a textual description of notable trends.Figure 10 shows two example plots generated by running this workflow on a CyTOF dataset.Differential Abundance Analysis of Cell Types The following workflows focus on identifying differentially expressed biological molecules.We use the edgeR [78] algorithm in the diffcyt [79] package to perform differential analysis of cell type abundances over different sample characteristics.</p>
<p>The LLM begins by selecting a metadata field to focus the analysis on, based on the research objective and data description, containing a list of available fields and their example values.Subsequently, it is given the full list of existing values of this metadata field and selects several contrasts to analyze.This step allows both comparing one group of cells against another and comparing one group against all remaining cells.We use these chosen parameters to construct a design matrix for calling diffcyt.The resulting data is stored in a csv file and interpreted by the LLM.</p>
<p>Differential Expression Analysis of Proteins Stratified by Cell Type</p>
<p>This workflow uses the limma [80] algorithm in the diffcyt package to calculate differential protein expression stratified by cell type.We similarly prompt the LLM to specify cell groupings for comparison.Here we include an additional step, where the LLM selects a subset of cell types to focus its analysis on, based on the research objective.After executing limma, we only keep data entries on the selected cell types and call the LLM to generate textual data interpretation.Differential Expression Analysis of Proteins Over All Cells For this workflow, we first merge all cells into a single cluster, then follow the steps in the previous workflow.This allows the system to get information on protein expression differences over the entire set of cells.</p>
<p>Analyzing Clinical Cohort Data</p>
<p>We provide a different set of workflows for PROTEUS to analyze clinical cohort data from mass spectrometry sequencing.All workflows excluding survival analysis and correlation analysis were conducted using the BioEnricher [81] package.Here we operate on a BioEnricher object that is constantly updated with each executed workflow, instead of a SingleCellExperiment object for CyTOF data.We create the data object using two csv files containing protein expression data and clinical metadata respectively, filtering out entries containing more than 25% missing values and imputing all remaining missing values using kNN.The system automatically generates the data description from the raw csv files and takes no additional input information.Differential Expression Analysis For parameter selection of Differential Expression Analysis (DEA), we adopt a similar procedure as differential analysis for CyTOF data, allowing PROTEUS to select a single metadata field, followed by one or more sets of conditions for comparison.We use the limma algorithm in BioEnricher to calculate top up-regulated and down-regulated proteins, as well as relevant statistics such as log fold changes and P values.Enrichment Analysis The Enrichment Analysis workflow first runs DEA on the data object if it has not already been performed.Based on the differentially expressed proteins, we identify enriched biological pathways by performing both over-representation analysis (ORA) and gene-set enrichment analysis (GSEA).Available pathway databases include GO, KEGG, Wiki, Reactome, MsigDB, etc.The result files of both algorithms are jointly provided to the LLM for analysis.Survival Analysis We implement two types of survival analysis using the Python package lifelines [82].First, we perform survival analysis on discrete classifications of low or high protein expression levels using log rank tests.We adjust the threshold for classification between the 20 and 80 percentiles of the protein expression data with intervals of 10 and select the threshold that yields the lowest p value for the final results.Second, we directly analyze continuous expression data using cox univariate regression and record key statistics such as correlation coefficients and p values.</p>
<p>When calling this workflow, PROTEUS selects the type of analysis to perform, as well as parameters including the metadata field to use for survival time, the field to use for event status, and a list of key molecules to focus on.Correlation Analysis on Clinical Features We implement correlation analysis in Python using scipy and mainly use Pearson correlation.This workflow calculates correlation levels between the expression of any molecule in the data and a clinical feature in the metadata.PROTEUS specifies the list of molecules, molecule type, and clinical trait to analyze when calling the workflow.Correlation Analysis Between Biological Molecules We similarly use scipy to investigate correlations between different biological molecules of same or different types (proteins, RNAs, phosphoproteins, etc.)This makes the workflow useful for both single and multi omics scenarios.PROTEUS calls the workflow by selecting two lists of molecules to analyze and their correspondingm molecule types.</p>
<p>Accessing External Data</p>
<p>We include additional workflows for PROTEUS to reference external information from datasets or databases based on biological molecules and diseases of interest.</p>
<p>Correlation Analysis on External Datasets We use the DataChat [83] package for integrated usage of external datasets such as The Cancer Genome Atlas (TCGA, https://www.cancer.gov/ccg/research/genome-sequencing/tcga)and the Clinical Proteomic Tumor Analysis Consortium (CPTAC) [84].Given the data description, PROTEUS searches for the required cancer type within available datasets in DataChat, locating the most relevant source database and datasets.DataChat provides diverse functions for correlation analysis between different molecule types (proteins, RNAs, phosphoproteins, etc.) PROTEUS selects relevant molecules and specifies molecule types to run correlation analysis, then automatically interprets the resulting plot.Survival Analysis on External Datasets This workflow is similarly based on an automatically selected dataset in DataChat.PROTEUS selects biological molecules on which to perform survival analysis and specifies the molecule type.The workflow then produces a Kaplan-Meier plot comparing the survival of patients with low and high expressions of the selected molecule, labeled with the calculated P value.The LLM judges whether the moelcule has significant impact on survival based on the plot.Both of these workflows intend to enhance the quality of hypotheses proposed by PROTEUS through providing an avenue to corroborate preliminary observations using other data sources.In most cases, we observe that PROTEUS correctly chooses data analysis workflows first, followed by external data validation workflows, and is able to identify notable, relevant molecules when calling DataChat.The Human Protein Atlas (THPA) Additionally, we design a workflow based on the THPA [85] API (proteinatlas.org) to provide PROTEUS with general biological information on a comprehensive set of human proteins.The workflow first calls the LLM to select a protein of interest based on its analysis history, then uses the API to fetch the following information: related protein classes; related biological pathways; related molecular functions; cancers in which the protein has favorable prognosis; cancers in which the protein has unfavorable prognosis.</p>
<p>Prompt Engineering in Automatic Evaluation</p>
<p>Here we provide the full prompts we used for performing automatic scoring on each of the 5 metrics.Partial alignment; some key proteins, cell types, or biological conditions match, but significant discrepancies in main findings 3: Moderate alignment; major findings and trends match, but differences in fine-grained names of protein markers or cell types, or divergences in deductions for further biological implications.Conclusions whose specific cell types overlap with those in the paper but exhibit notable differences should be in this category.4: Strong alignment; matches in main findings, key proteins or cell types, and most interpretations, with only minor differences in emphasis or detail.Conclusions whose specific cell types are close to those in the paper, with only minor differences, for instance in specificity, should be in this category.</p>
<p>Prompt for Auto-evaluation: Paper Alignment</p>
<p>of CD20 abundance in TmDC (logFC = 0.726746, p-value = 0.003199)</p>
<p>Fig. 1 :
1
Fig. 1: (a) The framework of the iterative refinement of PROTEUS .(b) A detailed illustration of the working process of PROTEUS .First, the Data Description module generates a precise description of the proteomics dataset, based on which the Objective Planner proposes a series of research objectives.Based on each objective, the Workflow Planner then plans a sequence of analytical workflows, such as analyzing expression differences or labeling cell types.These planned workflows are executed using specialized bioinformatics tools and follow a fixed sequence of steps.The Workflow Updater and Objective Updater analyze the system's latest results, based on which they refine the subsequent workflows and objectives.PROTEUS produces numerous scientific hypotheses for each research objective, and continues its analysis until it reaches a pre-determined maximum number of objectives.These framework designs facilitate a robust, end-to-end proteomics research pipeline.</p>
<p>Fig. 2 :
2
Fig. 2: Average scores and score distributions calculated on all 191 hypotheses, over 5 metrics.</p>
<p>Fig. 4 :
4
Fig.4: Score distributions on the 2 clinical cohort datasets (HCC and GBM), over 5 metrics.</p>
<p>Fig. 5 :Fig. 6 :
56
Fig. 5: Comparison of results obtained using our model and GPT-4o as the backbone of PROTEUS .</p>
<p>Fig. 7 :
7
Fig. 7: Comparison of automatic and human scoring results.The bar chart shows the average scores on each metric and the line chart denotes the agreement within a 1 point difference.</p>
<p>Fig. 8 :
8
Fig. 8: Comparison of automatic scoring results using four different LLM evaluators.</p>
<p>Fig. 9 :Step 1 :Fig. 10 :
9110
Fig. 9: The steps in the workflow FlowSOM Clustering and Cell Type Annotation.The procedure consists of performing FlowSOM clustering based on protein expression data, annotating and refining cell types based on top cell markers in each cluster, and finally assigning cell types as cluster names and merging clusters with the same cell types.The dataset used was Immune phenotyping of diverse syngeneic murine brain tumors identifies immunologically distinct types, downloaded from SPDB.</p>
<p>Conduct a thorough comparison between the AI-generated conclusion and the conclusion from an original research paper in the context of proteomics research.Assess the degree of concordance in terms of: a) Identification of key protein markers or cell types b) Reported biological processes or mechanisms c) Statistical tests and quantitative results (e.g., fold changes, p-values) d) Quality and alignment of statistical analysis workflows e) Proposed final conclusion or hypothesis f) Implications for the field of study Score on a scale of 0-5: 0: No alignment; AI conclusion contradicts or misses all key points from the original 1: Minimal alignment; only superficial similarities in general topic 2:</p>
<p>3 :
3
Assess the logical coherence and biological plausibility of a provided AI-generated proteomics conclusion based on fundamental principles of molecular biology, biochemistry, bioinformatics and proteomics.Evaluate: a) Consistency with known protein or cell type functions b) Adherence to established biological mechanisms and characteristics existent in the emphasized disease(s) c) Plausibility of proposed molecular mechanisms d) General logical coherence and consistency Score on a scale of 0-5: 0: Fundamentally flawed; violates basic principles of molecular biology or biochemistry 1: Major logical inconsistencies; proposed mechanisms highly unlikely based on current biological knowledge 2: Some logical gaps; parts of the conclusion are biologically plausible, but significant aspects are questionable Generally sound; mostly adheres to biological principles with a few minor logical leaps 4: Logically robust; aligns well with biological principles, only very minor questionable points 5: Exemplary logical coherence; fully adheres to all relevant biological principles and considers potential complexities in proteomics data interpretation AI-generated conclusion: [Insert AI conclusion here] Provide your output in the following format: Strengths in biological reasoning: Weaknesses or questionable aspects: Suggestions for improving biological plausibility: General assessment: Score (0-5): &lt;0/1/2/3/4/5&gt; Prompt for Auto-evaluation: Evaluability Assess the degree to which the AI-generated conclusion can be effectively evaluated based on current scientific knowledge, available data, and the nature of the claim.Consider the following factors: a) Clarity and specificity of the conclusion b) Adherence to statistical trends or biological conclusions presented in the original paper c) Existence of established methods to test the claim d) Presence of related studies in the literature e) Technological feasibility of verifying the conclusion through either external data or further experimentation f) Time frame required for potential validation (short-term vs. long-term implications) g) Ethical considerations for testing the conclusion Score on a scale of 0-5: 0: Not evaluable; Conclusion is too vague, ambiguous, or poorly formulated to be evaluated.No relevant data or established methods exist to assess the claim.Contradicts fundamental scientific principles or ethical considerations.</p>
<p>Table 1 :
1
Overview of analysis workflows.
TypeWorkflowPackagesParametersOutputDescriptionClustering and AnnotationCATALYST,NoneSCE ObjectPerforms FlowSOM clustering, top proteinscranmarker identification, and cell type labeling foreach cluster.Single-CellAnnotation RefinementCATALYST,Cell Type forSCE ObjectFurther clusters the selected cell types andDatascranRefinementannotates the subclusters with refined cellnames.VisualizationggplotList of ProteinsPlotsVisualizes protein abundances for general anal-ysis.DifferentialAbundancediffcytMetadata Field,FilesPerforms differential abundance analysis on all(Cell Types)Contrastslabeled cell types using the edgeR algorithm.StratifiedDifferentialdiffcytMetadata Field,FilesPerforms differential expression analysis on allExpression (Proteins)Contrastsproteins, stratified by cell type, using the limmaalgorithm.DifferentialExpressiondiffcytMetadata Field,FilesPerforms differential expression analysis on all(Proteins)Contrastsproteins, over all cells, using the limma algo-rithm.DifferentialExpressionBioEnricherMetadata Field,FilesPerforms differential expression analysis toBulk Clinical Cohort DataAnalysis Consensus ClusteringBioEnricherContrasts NoneBioEnricher Objectlocate up or down regulated proteins, using the limma algorithm. Performs NMF clustering on the data samples using all protein expressions. Selects the ideal cluster number by ensembling numerous met-rics.Enrichment AnalysisBioEnricherMetadata Field,FilesPerformsenrichmentanalysis(over-Contrastsrepresentation analysis and gene-set enrichmentanalysis) based on differential expression anal-ysis results.Survival AnalysislifelinesAnalysis Type,FilesPerforms either discrete or continuous survivalSurvival Timeanalysis to explore the relationship betweenField,Eventproteins and patient survival.Status FieldClinical CorrelationscipyListofFilesComputes the Pearson correlations betweenMolecules,biological molecule expression and a selectedClinical Fea-clinical feature.ture FieldMolecule CorrelationscipyListsofFilesComputes the pairwise Pearson correlationsMoleculesbetween two lists of biological molecules (pro-teins, RNAs, etc.)Correlation AnalysisDataChatDataset Name,FilesAnalyzes the pairwise correlations of theExternalListsofselected biological molecules using the speci-DatasetsMoleculesfied external dataset, often under the conditionof a certain cancer type.Survival AnalysisDataChatDatasetFilesPerforms survival analysis regarding theName, List ofselected biological molecules using the externalMoleculesdataset.THPANoneName of Pro-TextSearches for basic biological information on ateinselected protein using the API of The HumanProtein Atlas (THPA).</p>
<p>Table 2 :
2
Performance metrics of different large language models across various benchmarks.
DomainModelsMultiMedQA Avg. Acc (%)AlpacaEval 2 LC (%) WR (%)MMLU Acc (%)GPQA Acc (%)Mixtral-8x7B-Instruct [30]63.1723.7018.3070.6039.50Mixtral-8x22B-Instruct [30]79.1630.9022.2077.80-Qwen-1.5-72B-Chat [31]70.2436.6026.5075.6039.40GeneralQwen-2-72B-Chat [32]81.8138.1039.1082.3042.40DeepSeek-v2-Chat [33]77.90--78.50-Llama-3-70B-Instruct [7]82.6634.4033.2082.0039.50Llama-3.1-70B-Instruct [7]84.0538.1039.1086.0046.70GPT-3.5-Turbo67.7019.309.2070.0028.10GPT-4-Turbo87.0050.0050.0086.4049.10Med42-70B [34]70.74----OpenBioLM-70B [35]86.0630.8031.0060.1029.20BioMedMed-PaLM 2 (ER) [23]85.46----Llama-3-70B-UltraMed [11]85.8433.0032.1077.2039.70Llama-3.1-70B-UltraMed (Ours)86.2543.4546.0985.5845.76</p>
<p>Table 3 :
3
Main results on medical multiple-choice questions (MultiMedQA)
MMLUModel &amp; TaskMedQA (US 4-opt)MedMCQA (Dev) (Reason) PMQAClinical knowledgeMedical geneticsAnatomyProfess. medicineCollege biologyCollege medicineAvg.Mixtral-8x7B-Instruct52.849.746.271.770.062.271.077.867.163.17Mixtral-8x22B-Instruct73.163.371.484.289.077.088.288.278.079.16Qwen1.5-72B-Chat63.659.032.478.980.068.982.791.075.770.24Qwen2-72B-Chat75.366.668.885.793.080.789.794.482.181.81DeepSeek-v2-Chat68.661.571.083.090.073.386.888.978.077.90Llama-2-70B-Chat47.341.963.864.970.054.159.266.761.358.80Llama-3-70B-Instruct79.969.675.887.293.076.388.292.481.582.66Llama-3.1-70B-Instruct81.472.276.885.195.380.491.493.180.884.05GPT-3.5-Trubo57.772.753.874.774.065.972.872.964.767.70GPT-4-base (5-shot)86.173.780.488.797.085.293.897.280.987.00Med42-70B66.660.667.276.677.066.779.875.766.570.74OpenBioLLM-70B78.274.079.092.993.283.993.893.885.786.06Med-PaLM 2 (ER)85.472.375.088.792.084.492.395.883.285.46Llama-3-70B-UltraMed84.873.280.086.892.084.493.893.184.485.84Llama-3.1-70B-UltraMed85.272.977.887.295.083.794.995.184.486.25(a) Statistics of the datasets, workflows, and hypotheses used inour evaluation.# Datasets12WorkflowFrequency# Cytometry by Time-of-flight 10 # Clinical Cohort 2Clustering and Annotation Annotation Refinement46 21# Workflows15Visualization7# Hypotheses # Cytometry by Time-of-flight 147 191 # Mass Spectrometry 44Differential Abundance Stratified Differential Expression Differential Expression THPA23 28 9 10
(b) Frequency of workflow execution over one experimental run on 10 CyTOF datasets.</p>
<p>Table 4 :
4
Summary statistics and workflow execution frequency</p>
<p>Table 5 :
5
Average scores on 5 metrics across all datasets.Detailed dataset information is provided in Appendix A
Dataset# Results Paper Literature Novelty Coherence EvaluabilitySPDB (CyTOF)Human PBMCs, Leukemia [42]132.463.703.313.463.62Human HGSC tumor cells [43]151.802.732.873.203.57Human PBMCs, ICC [44]152.002.603.403.003.07Human T cells [45]152.273.333.133.073.40Human HCC tumor cells [46]152.133.403.403.273.60Mouse GBM tumor cells [47]152.673.403.273.533.67Human PBMCs, DLBCL [48]151.932.933.333.273.53Human GC cells [49]141.863.143.363.643.86Human HIV cells [50]152.203.333.333.473.33Human PBMCs, thyroid disease [51]151.802.733.333.273.67Clinical Cohorts (MS)Human GBM tissues [40]242.003.253.293.173.54Human HCC tissues [39]202.404.003.403.603.70</p>
<p>Table 6 :
6
Overview of the automatic evaluation on 5 metrics.
MetricMean Median Std Max MinPaper-based Alignment2.1320.854Literature-based Alignment3.2441.125Literature-based Novelty3.2930.524Logical Coherence3.3230.594Evaluability3.5540.504</p>
<p>Prompt for Auto-evaluation: Literature-Based Alignment Perform</p>
<p>a comprehensive literature review using PubMed to evaluate a provided AI-generated conclusion's alignment with existing proteomics research.Consider: a) Consistency with established trends in protein or cell type abundances with respect to similar biological condition comparisons b) Concordance with previously reported quantitative statistical results c) Consistency with systems biology perspectives and further general implications in the field Score on a scale of 0-5: 0: Contradicts well-established proteomics findings; multiple studies refute the conclusion 1: Limited support; mostly contradicts current literature with only minor points of agreement 2: Mixed support; some aspects align with literature but significant contradictions exist 3: Moderate support; generally aligns with literature, but some notable discrepancies or gaps.Conclusions whose specific cell types overlap with those in existing papers but exhibit notable differences should be considered to have notable gaps.4:Strong support; aligns well with multiple studies, only minor inconsistencies.Conclusions whose specific cell types are close to those in existing papers, with only minor differences, for instance in specificity, should be considered to have minor inconsistencies.Groundbreaking; presents an entirely new concept or approach that could significantly advance the field PubMed Articles: [Insert relevant PubMed article information here] AI-generated conclusion: [Insert AI conclusion here] Provide your output in the following format: Most closely related existing research (with PMIDs): Aspects that distinguish this conclusion from existing work: Potential impact on future proteomics research:
5: Excellent support; perfectly aligns with well-established findings across multiple studies and reviewsPubMed Articles: [Insert relevant PubMed article information here]AI-generated conclusion: [Insert AI conclusion here]Provide your output in the following format:Key supporting studies (with PMIDs):Key contradicting studies (if any, with PMIDs):Gaps in current literature relevant to the conclusion:General assessment:Score (0-5): &lt;0/1/2/3/4/5&gt;Prompt for Auto-evaluation: Literature-Based NoveltyConduct a thorough PubMed search to evaluate the novelty of the AI-generated conclusion in the context ofproteomics research. Consider:a) Identification of previously unknown disease biomarkers or immune signaturesb) Novel insights into protein functions, cell type functions, biological pathways or mechanismsc) Unique integration of proteomics data analysis results with general proteomics and biological knowledged) Innovative approaches to data interpretation in proteomicse) Potential for opening new avenues of research in the fieldScore on a scale of 0-5:0: Entirely unoriginal; all aspects have been extensively reported in multiple studies1: Minimal novelty; mostly reiterates known findings with only trivial new aspects2: Modest novelty; combines known concepts in a somewhat new way, but no significant new insights3: Moderate novelty; presents a fresh perspective on well-studied proteomics concepts or ideas4: High novelty; uncovers a previously unreported trend, idea, or interpretation in proteomics research5:5: Perfect alignment; AI conclusion captures all main points, key proteins and cell types, quantitative results,and biological interpretations from the original paperAI-generated conclusion: [Insert AI conclusion here]Original paper conclusion: [Insert original conclusion here]Provide your output in the following format:List of matching and divergent points:General assessment:Score (0-5): &lt;0/1/2/3/4/5&gt;</p>
<p>1: Minimally evaluable; Conclusion is mostly unclear but contains some assessable elements.Very limited relevant data or literature available.Would require significant technological advancements to test.2:Partially evaluable; Conclusion is somewhat clear but lacks crucial details.Some relevant data and methods exist, but significant gaps remain.Current methods can partially assess the claim, but with major limitations.3:Moderatelyevaluable; Conclusion is mostly clear and specific.Mostly sufficient relevant data and methods are available for a satisfactory evaluation.Current methods can largely assess the claim, with some limitations.4:Highlyevaluable; Conclusion is clear, specific, and well-formulated.Sufficient relevant data and methods are available for comprehensive evaluation.Established methods can thoroughly assess most aspects of the claim.5:Fullyevaluable; Conclusion is exceptionally clear, specific, and comprehensive.Extensive relevant data, literature, and established methods are readily available for well-rounded, in depth evaluation.Claim can be fully assessed with current scientific knowledge and technology.AI-generated conclusion: [Insert AI conclusion here] Provide your output in the following format: Key factors influencing evaluability: Suggested evaluation procedure, including necessary data and experimentation methods: Challenges in evaluation (if any): Suggestions for improving evaluability: General Assessment: Score (0-5): &lt;0/1/2/3/4/5&gt; Section B: Comparison with Published Papers unhelpful helpful How well do the results from PROTEUS align with the findings in the corresponding published papers?⃝ ⃝ Are there any significant differences between the automated analysis results and the scientist-obtained results from published papers?If so, please describe.In case of any inconsistencies, based on your professional background, please determine whether it is trivial, incorrect, or if it represents a new important and reasonable research direction.If a valuable new research direction emerges, please elaborate on its novelty and importance.Does the objective presented in the results comprehensively address the important research questions related to the corresponding published papers?Are there any additional research questions that you think should be addressed based on the results?Does the analysis in the hypotheses fulfill the planning of the objective?Is it evidence-based?⃝⃝Does the hypothesis present cell types and proteins that were not emphasized but are of great significance in the original paper?⃝⃝How strongly logical is the automated analysis process presented in the entire results, from objective design to hypothesis formulation?⃝⃝Calculaterelevant indicators according to the original literature and original data to determine whether the cell type and marker are correctly corresponding, and confirm that the values of logFC and P-value in the key statistics are within a reasonable range and show the correct trend.What improvements or modifications would you suggest for PRO-TEUS to enhance its performance?Are there any additional features or capabilities that you think should be added to the system?
⃝⃝⃝⃝⃝⃝Section C: Objectives &amp; HypothesesunhelpfulhelpfulIs the proposed objective reasonable?⃝⃝⃝⃝Section D: Hypotheses GeneratedunhelpfulhelpfulAre the hypotheses generated by PROTEUS reasonable and testable?⃝⃝How scientific is the proposed biological hypothesis?⃝⃝Do the hypotheses provide new directions for further research?⃝⃝Section E: Suggestions for Improvementunhelpfulhelpful⃝⃝
A List of Datasets
Proteomics: technologies and their applications. B Aslam, M Basit, M A Nisar, M Khurshid, M H Rasool, Journal of chromatographic science. 2016</p>
<p>Single-cell proteomics enabled by next-generation sequencing or mass spectrometry. H M Bennett, W Stephenson, C M Rose, S Darmanis, Nature Methods. 203</p>
<p>Plasma proteomics of acute tubular injury. I M Schmidt, A L Surapaneni, R Zhao, D Upadhyay, W.-J Yeo, P Schlosser, C Huynh, A Srivastava, R Palsson, T Kim, I E Stillman, D Barwinska, J Barasch, M T Eadon, T M El-Achkar, J Henderson, D G Moledina, S E Rosas, S E Claudel, A Verma, Y Wen, M Lindenmayer, T B Huber, S V Parikh, J P Shapiro, B H Rovin, I B Stanaway, N A Sathe, P K Bhatraju, J Coresh, E P Rhee, M E Grams, S S Waikar, the Kidney Precision Medicine Project. 157368</p>
<p>Neutrophil-specific interactome of ARHGAP25 reveals novel partners and regulatory insights. P Sasvári, A Pettkó-Szandtner, E Wisniewski, R Csépányi-Kömi, Scientific Reports. 14120106</p>
<p>Tensin-2 interactomics reveals interaction with GAPDH and a phosphorylation-mediated regulatory role in glycolysis. P Turkki, I Chowdhury, T Öhman, L Azizi, M Varjosalo, V P Hytönen, Scientific Reports. 14119862</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Pre-trained models: Past, present and future. X Han, Z Zhang, N Ding, Y Gu, X Liu, Y Huo, J Qiu, Y Yao, A Zhang, L Zhang, AI Open. 22021</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. 2021arXiv preprint</p>
<p>K Saab, T Tu, W.-H Weng, R Tanno, D Stutz, E Wulczyn, F Zhang, T Strother, C Park, E Vedadi, arXiv:2404.18416Capabilities of gemini models in medicine. 2024arXiv preprint</p>
<p>K Zhang, S Zeng, E Hua, N Ding, Z.-R Chen, Z Ma, H Li, G Cui, B Qi, X Zhu, arXiv:2406.03949Ultramedical: Building specialized generalists in biomedicine. 2024arXiv preprint</p>
<p>Y Qin, S Hu, Y Lin, W Chen, N Ding, G Cui, Z Zeng, Y Huang, C Xiao, C Han, Y R Fung, Y Su, H Wang, C Qian, R Tian, K Zhu, S Liang, X Shen, B Xu, Z Zhang, Y Ye, B Li, Z Tang, J Yi, Y Zhu, Z Dai, L Yan, X Cong, Y Lu, W Zhao, Y Huang, J Yan, X Han, X Sun, D Li, J Phang, C Yang, T Wu, H Ji, Z Liu, M Sun, Tool Learning with Foundation Models. </p>
<p>Augmenting large language models with chemistry tools. M Bran, A Cox, S Schilter, O Baldassari, C White, A D Schwaller, P , Nature Machine Intelligence. </p>
<p>Benchmarking retrieval-augmented generation for medicine. G Xiong, Q Jin, Z Lu, A Zhang, Findings of the Association for Computational Linguistics ACL 2024. Association for Computational Linguistics</p>
<p>GeneGPT: Augmenting large language models with domain tools for improved access to biomedical information. Q Jin, Y Yang, Q Chen, Z Lu, Bioinformatics. 402075 2304.09667 [cs, q-bio</p>
<p>CellAgent: An LLM-driven multi-agent framework for automated single-cell data analysis. Y Xiao, J Liu, Y Zheng, X Xie, J Hao, M Li, R Wang, F Ni, Y Li, J Luo, S Jiao, J Peng, bioRxiv 2024.05.13.5938612024</p>
<p>J Zhou, B Zhang, X Chen, H Li, X Xu, S Chen, W He, C Xu, X Gao, An AI Agent for Fully Automated Multi-omic Analyses. 2023</p>
<p>A data-intelligence-intensive bioinformatics copilot system for large-scale omics researches and scientific insights. Y Liu, R Shen, L Zhou, Q Xiao, J Yuan, Y Li, bioRxiv 2024.05.19.5948952024</p>
<p>BioInformatics agent (BIA): Unleashing the power of large language models to reshape bioinformatics workflow. Q Xin, Q Kong, H Ji, bioRxiv 2024.05.22.5952402024</p>
<p>scChat: A Large Language Model-Powered Co-Pilot for Contextualized Single. Y.-C Lu, A Varghese, R Nahar, H Chen, K Shao, X Bao, C Li, 10.1101/2024.10.01.616063Cell RNA Sequencing Analysis. 2024</p>
<p>DREAM: a biomedical data-driven self-evolving autonomous research system. L Deng, Y Wu, Y Ren, H Lu, arXiv:2407.136372024arXiv preprint</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, Nature. 62079722023</p>
<p>Towards expert-level medical question answering with large language models. K Singhal, T Tu, J Gottweis, R Sayres, E Wulczyn, L Hou, K Clark, S Pfohl, H Cole-Lewis, D Neal, arXiv:2305.096172023arXiv preprint</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, E Pan, N Oufattole, W.-H Weng, H Fang, P Szolovits, Applied Sciences. 111464212021</p>
<p>Q Jin, B Dhingra, Z Liu, W W Cohen, X Lu, arXiv:1909.06146Pubmedqa: A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. A Pal, L K Umapathi, M Sankarasubbu, Conference on Health, Inference, and Learning. PMLR2022</p>
<p>D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>D Rein, B L Hou, A C Stickland, J Petty, R Y Pang, J Dirani, J Michael, S R Bowman, arXiv:2311.12022Gpqa: A graduate-level google-proof q&amp;a benchmark. 2023arXiv preprint</p>
<p>Y Dubois, B Galambosi, P Liang, T B Hashimoto, arXiv:2404.04475Length-controlled alpacaeval: A simple way to debias automatic evaluators. 2024arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, C Bamford, D S Chaplot, D Casas, E B Hanna, F Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>A Yang, B Yang, B Hui, B Zheng, B Yu, C Zhou, C Li, C Li, D Liu, F Huang, arXiv:2407.10671Qwen2 technical report. 2024arXiv preprint</p>
<p>A Liu, B Feng, B Wang, B Wang, B Liu, C Zhao, C Dengr, C Ruan, D Dai, D Guo, arXiv:2405.04434Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. 2024arXiv preprint</p>
<p>C Christophe, P K Kanithi, P Munjal, T Raha, N Hayat, R Rajan, A Al-Mahrooqi, A Gupta, M U Salman, G Gosal, arXiv:2404.14779Med42-evaluating fine-tuning strategies for medical llms: Full-parameter vs. parameter-efficient approaches. 2024arXiv preprint</p>
<p>OpenBioLLMs: Advancing Open-Source Large Language Models for Healthcare and Life Sciences. Ankit Pal, M S , Hugging Face. 2024</p>
<p>Spdb: a comprehensive resource and knowledgebase for proteomic data at the single-cell resolution. F Wang, C Liu, J Li, F Yang, J Song, T Zang, J Yao, G Wang, Nucleic Acids Research. 52D12024</p>
<p>Mass cytometry: technique for real time single cell multitarget immunoassay based on inductively coupled plasma time-of-flight mass spectrometry. D R Bandura, V I Baranov, O I Ornatsky, A Antonov, R Kinach, X Lou, S Pavlov, S Vorobiev, J E Dick, S D Tanner, 10.1021/ac901049wAnalytical Chemistry. 8116</p>
<p>Orchestrating single-cell analysis with bioconductor. R A Amezquita, A T L Lun, E Becht, V J Carey, L N Carpp, L Geistlinger, F Marini, K Rue-Albrecht, D Risso, C Soneson, L Waldron, H Pagès, M L Smith, W Huber, M Morgan, R Gottardo, S C Hicks, 10.1038/s41592-019-0654-xNature Methods. 172</p>
<p>Proteomics identifies new therapeutic targets of early-stage hepatocellular carcinoma. Y Jiang, Nature Letters. 2019</p>
<p>Integrated pharmaco-proteogenomics defines two subgroups in isocitrate dehydrogenase wild-type glioblastoma with prognostic and therapeutic opportunities. S Oh, Nature Communications. 2020</p>
<p>Mass spectrometry and protein analysis. B Domon, R Aebersold, 10.1126/science.1124619Science. 5771</p>
<p>Comprehensive immune monitoring of clinical trials to advance human immunotherapy. F J Hartmann, J Babdor, P F Gherardini, E.-A D Amir, K Jones, B Sahaf, D M Marquez, P Krutzik, E O'donnell, N Sigal, H T Maecker, E Meyer, M H Spitzer, S C Bendall, Cell Reports. 283</p>
<p>Commonly occurring cell subsets in high-grade serous ovarian tumors identified by single-cell mass cytometry. V D Gonzalez, N Samusik, T J Chen, E S Savig, N Aghaeepour, D A Quigley, Y.-W Huang, V Giangarrà, A D Borowsky, N E Hubbard, S.-Y Chen, G Han, A Ashworth, T J Kipps, J S Berek, G P Nolan, W J Fantl, Cell reports. 2018</p>
<p>T Wu, Y.-C Yang, B Zheng, X.-B Shi, W Li, W.-C Ma, S Wang, Z.-X Li, Y.-J Zhu, J.-M Wu, K.-T Wang, Y Zhao, R Wu, C.-J Sui, S.-Y Shen, X Wu, L Chen, Z.-G Yuan, H.-Y Wang, Distinct immune signatures in peripheral blood predict chemosensitivity in intrahepatic cholangiocarcinoma patients. Engineering. 7</p>
<p>Multidimensional analyses of proinsulin peptide-specific regulatory t cells induced by tolerogenic dendritic cells. J S Suwandi, S Laban, K Vass, A Joosten, V Van Unen, B P F Lelieveldt, T Höllt, J J Zwaginga, T Nikolic, B O Roep, Journal of Autoimmunity. 107102361</p>
<p>Trajectory and functional analysis of PD-1high CD4+CD8+ t cells in hepatocellular carcinoma by single-cell cytometry and transcriptome sequencing. B Zheng, D Wang, X Qiu, G Luo, T Wu, S Yang, Z Li, Y Zhu, S Wang, R Wu, C Sui, Z Gu, S Shen, S Jeong, X Wu, J Gu, H Wang, L Chen, 20002247</p>
<p>Immune phenotyping of diverse syngeneic murine brain tumors identifies immunologically distinct types. J K Khalsa, N Cheng, J Keegan, A Chaudry, J Driver, W L Bi, J Lederer, K Shah, Nature Communications. 1113912</p>
<p>Single-cell phenotypic profiling to identify a set of immune cell protein biomarkers for relapsed and refractory diffuse large b cell lymphoma: A single-center study. Y Shi, W Ding, W Gu, Y Shen, H Li, Z Zheng, X Zheng, Y Liu, Y Ling, Journal of Leukocyte Biology. 1126</p>
<p>Immune profiling in gastric cancer reveals the dynamic landscape of immune signature underlying tumor progression. Y Wei, J Zhang, X Fan, Z Zheng, X Jiang, D Chen, Y Lu, Y Li, M Wang, M Hu, Q Du, L Yang, H Li, Y Xiao, Y Li, J Jin, D Wang, X Yuan, Q Li, Frontiers in Immunology. 13935552</p>
<p>Single-cell glycomics analysis by cytof-lec reveals glycan features defining cells differentially susceptible to hiv. T Ma, M Mcgregor, L Giron, G Xie, A F George, M Abdel-Mohsen, N R Roan, 1178870</p>
<p>Peripheral immunophenotyping of aitd subjects reveals alterations in immune cells in pediatric vs adult-onset aitd. Z C Stensland, B M Coleman, M Rihanek, R M Baxter, P A Gottlieb, E W Y Hsieh, V D Sarapura, K M Simmons, J C Cambier, M J Smith, 25103626</p>
<p>TRAIL-dependent apoptosis of peritoneal mesothelial cells by NK cells promotes ovarian cancer invasion. A M Steitz, C Schröder, I Knuth, C U Keber, L Sommerfeld, F Finkernagel, J M Jansen, U Wagner, S Müller-Brüsselbach, T Worzfeld, M Huber, V M Beutgen, J Graumann, E P V Strandmann, R Müller, S Reinartz, 10.1016/j.isci.2023.1084012024-10-3126</p>
<p>Exploring the clinical value of tumor microenvironment in platinum-resistant ovarian cancer 77. A Ghoneum, S Almousa, B Warren, A Y Abdulfattah, J Shu, H Abouelfadl, D Gonzalez, C Livingston, N Said, 10.1016/j.semcancer.2020.12.0242024-10-3183</p>
<p>Upregulation of mesothelial genes in ovarian carcinoma cells is associated with an unfavorable clinical outcome and the promotion of cancer. K Ojasalu, C Brehm, K Hartung, M Nischak, F Finkernagel, P Rexin, A Nist, E Pavlakis, T Stiewe, J M Jansen, U Wagner, S Gattenlöhner, A Bräuninger, S Müller-Brüsselbach, S Reinartz, R Müller, 10.1002/1878-0261.127492024-10-3114</p>
<p>Regulation of invasion and peritoneal dissemination of ovarian cancer by mesothelin manipulation. R Coelho, S Ricardo, A L Amaral, Y.-L Huang, M Nunes, J P Neves, N Mendes, M N López, C Bartosch, V Ferreira, R Portugal, J M Lopes, R Almeida, V Heinzelmann-Schwarz, F Jacob, L David, 10.1038/s41389-020-00246-22024-10-31961</p>
<p>The immune microenvironment of malignant pleural mesothelioma: A literature review. A.-L Désage, G Karpathiou, M Peoc'h, M E Froudarakis, 10.3390/cancers131332052024-10-3113</p>
<p>Modulating immunosuppression in the intrapleural space of malignant pleural mesothelioma and predictive biomarkers to guide treatment decisions. R M Wong, 10.1016/j.jtho.2016.07.0192024-10-31Elsevier11</p>
<p>Detection of circulating immunosuppressive cytokines in malignant pleural mesothelioma patients for prognostic stratification 146. 10.1016/j.cyto.2021.1556222024-10-31</p>
<p>Mesothelioma environment comprises cytokines and t-regulatory cells that suppress immune responses. J P J J Hegmans, A Hemmes, H Hammad, L Boon, H C Hoogsteden, B N Lambrecht, 10.1183/09031936.06.001353052024-10-3127</p>
<p>Angiogenic cytokines in mesothelioma: a study of VEGF, FGF-1 and -2, and TGF-β expression. S Kumar-Singh, J Weyler, M J H Martin, P B Vermeulen, E Van Marck, 10.1002/(SICI)1096-9896(199909)189:1&lt;72::AID-PATH401&gt;3.0.CO;2-0AID-PATH401&gt;3.0.CO;2-0 . _eprint2024-10-31189</p>
<p>Publisher: Frontiers. G J Chu, N Zandwijk, J E J Rasko, 10.3389/fonc.2019.01366The immune microenvironment in mesothelioma: Mechanisms of resistance to immunotherapy 9. 2024-10-31</p>
<p>Ovarian tumor cell-derived JAGGED2 promotes omental metastasis through stimulating the notch signaling pathway in the mesothelial cells. S S Islam, F H Al-Mohanna, I M Yousef, I A Al-Badawi, A Aboussekhra, 10.1038/s41419-024-06512-02024-11-01Nature Publishing Group15</p>
<p>Interactions of human peritoneal mesothelial cells with serous ovarian cancer cell spheroids-evidence for a mechanical and paracrine barrier function of the peritoneal mesothelium. S Stadlmann, H Feichtinger, G Mikuz, C Marth, A G Zeimet, M Herold, C Knabbe, F A Offner, 10.1097/IGC.0000000000000036BMJ Specialist Journals Section: Basic Science. 2422024-11-01</p>
<p>Ovarian cancer-driven mesothelial-to-mesenchymal. D Del Rio, I Masi, V Caprara, F Spadaro, F Ottavi, R Strippoli, P Sandoval, M López-Cabrera, R Cuesta, A Bagnato, L Rosanò, 10.3389/fcell.2021.764375Publisher: Frontiers. Accessed. 2024-11-01transition is triggered by the endothelin-1/β-arr1 axis 9</p>
<p>. Y S Kap, N Driel, J D Laman, P P Tak, B A Hart, 10.4049/jimmunol.1303125CD20+ b cell depletion alters t cell homing. 1929</p>
<p>The regulation and function of CD20: an "enigma" of b-cell biology and targeted therapy. G Pavlasova, M Mraz, Haematologica. 1056</p>
<p>Roles and mechanisms of tumourinfiltrating b cells in human cancer: a new force in immunotherapy. E Zhang, C Ding, S Li, X Zhou, B Aikemu, X Fan, J Sun, M Zheng, X Yang, 10.1186/s40364-023-00460-111</p>
<p>B-cell activation influences t-cell polarization and outcome of anti-CD20 b-cell depletion in central nervous system autoimmunity. M S Weber, T Prod'homme, J C Patarroyo, N Molnarfi, T Karnezis, K Lehmann-Horn, D M Danilenko, J Eastham-Anderson, A J Slavin, C Linington, C C A Bernard, F Martin, S S Zamvil, 10.1002/ana.2208168</p>
<p>Regulation of epithelial-mesenchymal transition through epigenetic and post-translational modifications 15. S J Serrano-Gomez, M Maziveyi, S K Alahari, 10.1186/s12943-016-0502-x2024-10-3018</p>
<p>M O Nowicki, J L Hayes, E A Chiocca, S E Lawler, 10.3390/cancers110404662024-10-30Proteomic analysis implicates vimentin in glioblastoma cell migration. 11</p>
<p>Vimentin promotes glioma progression and maintains glioma cell resistance to oxidative phosphorylation inhibition. Y Liu, S Zhao, Y Chen, W Ma, S Lu, L He, J Chen, X Chen, X Zhang, Y Shi, X Jiang, K Zhao, 10.1007/s13402-023-00844-346</p>
<p>T cell subsets in graft versus host disease and graft versus tumor. H Jiang, D Fu, A Bidgoli, S Paczesny, 10.3389/fimmu.2021.761448761448. 2024-10-3112</p>
<p>Naive and stem cell memory t cell subset recovery reveals opposing reconstitution patterns in CD4 and CD8 t cells in chronic graft vs. M V Soares, R I Azevedo, I A Ferreira, S Bucar, A C Ribeiro, A Vieira, P N G Pereira, R M Ribeiro, D Ligeiro, A C Alho, A S Soares, N Camacho, C Martins, F Lourenço, R Moreno, J Ritz, J F Lacerda, 10.3389/fimmu.2019.003342024-11-01Publisher: Frontiers</p>
<p>CATALYST: Cytometry dATa anALYSis Tools. H L Crowell, V R T Zanotelli, S Chevrier, M D Robinson, 10.18129/B9.bioc.CATALYST2024</p>
<p>Analyzing high-dimensional cytometry data using FlowSOM. K Quintelier, A Couckuyt, A Emmaneel, J Aerts, Y Saeys, S Van Gassen, Nature Protocols. 168</p>
<p>A step-by-step workflow for low-level analysis of single-cell rna-seq data with bioconductor. A T L Lun, D J Mccarthy, J C Marioni, 10.12688/f1000research.9501.2F1000Res. 5, 2122. 2016</p>
<p>Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis. W Hou, Z Ji, 10.1038/s41592-024-02235-4Nature Methods. 2182024</p>
<p>edger 4.0: powerful differential analysis of sequencing data with expanded functionality and improved support for small counts and larger datasets. Y Chen, L Chen, A T L Lun, P Baldoni, G K Smyth, 10.1101/2024.01.21.576131bioRxiv. 2024</p>
<p>Diffcyt: Differential Discovery in High-dimensional Cytometry Via High-resolution Clustering. L M Weber, 10.18129/B9.bioc.diffcyt2024</p>
<p>limma powers differential expression analyses for RNA-sequencing and microarray studies. M E Ritchie, B Phipson, D Wu, Y Hu, C W Law, W Shi, G K Smyth, 10.1093/nar/gkv007Nucleic Acids Research. 437472015</p>
<p>BioEnricher: Bioinformatics Analysis and Visualization Pipeline. Z Liu, 2024R package version 4.0.0</p>
<p>C Davidson-Pilon, 10.5281/zenodo.12549337Lifelines, Survival Analysis in Python. </p>
<p>DataChat: Comprehensive Gene Exploration in Public Databases. Z Liu, 2024R package version 2.0.0</p>
<p>Abstract LB-242: Proteomic data commons: A resource for proteogenomic analysis. R R Thangudu, P A Rudnick, M Holck, D Singhal, M J Maccoss, N J Edwards, K A Ketchum, C R Kinsinger, E Kim, A Basu, Cancer Research. 8016242</p>
<p>Tissue-based map of the human proteome. M Uhlén, L Fagerberg, B M Hallström, C Lindskog, P Oksvold, A Mardinoglu, A Sivertsson, C Kampf, E Sjöstedt, A Asplund, I Olsson, K Edlund, E Lundberg, S Navani, C A Szigyarto, .-K, J Odeberg, D Djureinovic, J O Takanen, S Hober, T Alm, P.-H Edqvist, H Berling, H Tegel, J Mulder, J Rockberg, P Nilsson, J M Schwenk, M Hamsten, K Feilitzen, M Forsberg, L Persson, F Johansson, M Zwahlen, G Heijne, J Nielsen, F Pontén, 10.1126/science.1260419Science. 347622012604192015</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1123 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1123</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1123</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-263331431</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.10289v1.pdf" target="_blank">Active Reinforcement Learning for Robust Building Control</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) is a powerful tool for optimal control that has found great success in Atari games, the game of Go, robotic control, and building optimization. RL is also very brittle; agents often overfit to their training environment and fail to generalize to new settings. Unsupervised environment design (UED) has been proposed as a solution to this problem, in which the agent trains in environments that have been specially selected to help it learn. Previous UED algorithms focus on trying to train an RL agent that generalizes across a large distribution of environments. This is not necessarily desirable when we wish to prioritize performance in one environment over others. In this work, we will be examining the setting of robust RL building control, where we wish to train an RL agent that prioritizes performing well in normal weather while still being robust to extreme weather conditions. We demonstrate a novel UED algorithm, ActivePLR, that uses uncertainty-aware neural network architectures to generate new training environments at the limit of the RL agent's ability while being able to prioritize performance in a desired base environment. We show that ActivePLR is able to outperform state-of-the-art UED algorithms in minimizing energy usage while maximizing occupant comfort in the setting of building control.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1123.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1123.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ActivePLR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Prioritized Level Replay</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An uncertainty-driven unsupervised environment design (UED) method that gradient-optimizes continuous environment configuration parameters to generate new training levels at the frontier of an RL agent's uncertainty, then adds those levels into a PLR replay buffer for prioritized training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ActivePLR (UED + PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PPO actor-critic RL agent (two hidden layers, 256 units each, dropout + ReLU) trained with a UED teacher that: (1) estimates critic uncertainty via Monte Carlo Dropout, (2) backpropagates gradients of that uncertainty to environment configuration parameters ϕ (the part of the initial state), and (3) uses constrained gradient ascent (Extragradient Adam with hard bounds and optional soft-distance penalty) to propose new realistic environments which are added to a Prioritized Level Replay buffer and sampled based on value-loss/staleness.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>active learning / uncertainty-driven adaptive environment design (gradient-based uncertainty maximization integrated with Prioritized Level Replay)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each environment-generation step, compute uncertainty L([ϕ, s0], θ) using Monte Carlo Dropout on the critic outputs (C stochastic forward passes). Perform gradient ascent on ϕ to maximize L subject to hard bounds (extragradient / Lagrangian projection) and an optional soft penalty −γ||ϕ−ϕ0||^2 to keep proposals near the base environment. The optimized ϕ is added to the PLR replay buffer; PLR then prioritizes levels by recorded value loss and staleness for sampling during PPO updates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Sinergym building HVAC simulation (EnergyPlus used for higher-fidelity evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Stochastic building/HVAC simulator with procedural weather generation (Ornstein-Uhlenbeck noise model); continuous 20-dimensional observed state (includes 5 outdoor weather vars + 15 indoor/occupancy/control vars); continuous-valued environment configuration ϕ ∈ R^5 (µ offsets for temperature, humidity, wind speed, wind direction, solar irradiance); non-stationary/weather-extreme regimes; reward dense (energy + comfort), not sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>State dimension = 20 (continuous); environment configuration ϕ dimension = 5 (continuous µ offsets; underlying OU process also has σ, τ but ϕ in the algorithm is 5-D); action space = HVAC setpoints (two setpoints: cooling and heating, continuous); training timesteps: experiments run up to multi-million timesteps (e.g., referenced 3M timesteps); training episodes operate at ∆t = 1 hour; evaluation also at ∆t = 0.25 h (actions repeated 4×).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>ActivePLR produced the strongest overall results: +9% reward over vanilla RL on base environment ϕ0, +24% over the rule-based controller (RBC) on the base environment, and ≈+3% average reward over vanilla RL across all six handcrafted environments (ϕ0 + 5 extreme weathers). Reduced days with ASHRAE thermal comfort violations by ~15% relative to vanilla RL. In Sim2Real (higher-fidelity EnergyPlus surrogate) ActivePLR had only a 3.1% relative drop in reward vs. its low-fidelity evaluation and achieved higher absolute reward than other data-driven baselines. Training/evaluation runtime: ~12 hours per ActivePLR trial; high-fidelity evaluation ~1 hour.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines: vanilla PPO trained on ϕ0 (no UED) and DR/RPLR. Vanilla PPO was competitive on the base environment but worse in extremes; DR and RPLR generally underperformed: DR and RPLR achieved ~9% higher reward than RBC on ϕ0 after 3M timesteps, but vanilla RL outperformed DR/RPLR on ϕ0 by ~8% (i.e., DR/RPLR sometimes hurt performance). In Sim2Real, vanilla RL had an average 8.5% relative drop in reward; DR/RPLR about 7% relative drop. Exact numeric baselines vary per experiment and environment.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported experiments used multi-million timestep training (examples reference 3M timesteps). Each ActivePLR trial took ~12 hours; ActivePLR required repeated inner gradient-ascent steps per level (N up to O(100) in hyperparams). No specific reduced-sample-count numbers reported; empirical gains are reported after the full training runs (multi-million steps).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration is driven by explicitly maximizing critic uncertainty (via MC Dropout variance) to produce novel but constrained environment configurations; exploitation occurs during standard PPO policy updates using collected trajectories. PLR replay sampling balances 'novel/new' (sample new ϕ via ActivePLR) vs. replaying previously seen levels with probability P_D (Bernoulli based on |Λ_seen|/N_PLR), and replay priorities combine value loss and staleness (weighted by ρ). The soft constraint γ controls how far exploration may stray from the base environment (realism vs. novelty tradeoff).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Baselines compared: vanilla PPO (trained on ϕ0), Domain Randomization (DR), Prioritized Level Replay (PLR) and Robust PLR (RPLR), Sampled Matched PLR (SAMPLR not used due to simulator cost), Rule-based controller (RBC), Random controller.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Uncertainty-driven gradient-based environment design (ActivePLR) yields better robustness to extreme weather and better Sim2Real transfer than DR, PLR/RPLR, and vanilla RL. ActivePLR finds realistic high-uncertainty environments (constrained by hard/soft constraints) that improve the policy's performance in both base and extreme test environments and reduces comfort violations. ActivePLR produced the smallest Sim2Real performance degradation (~3.1% relative drop) among data-driven methods and consistently outperformed baselines on a 120-US-weather-pattern benchmark (ActiveRL slightly ahead on that dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires a simulator and continuous environment configuration variables (cannot directly handle categorical ϕ without dequantization). Risk of generating unrealistic ϕ if constraints are weak (necessitating hard bounds and γ soft-penalty). Performance can be sensitive to hyperparameters (γ, η) though some robustness reported (γ large gave better results; η small improved stability). Possible attraction to local optima: ActivePLR and ActiveRL sometimes converge to similar behaviors, suggesting local (or global) minima that may not generalize across simulation fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Reinforcement Learning for Robust Building Control', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1123.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1123.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ActiveRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Reinforcement Learning (uncertainty-driven environment generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of ActivePLR which directly generates training environments by gradient-ascent on critic uncertainty (no replay buffer): per training iteration it optimizes ϕ to maximize uncertainty (with constraints) and immediately collects trajectories from the generated environment for PPO updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ActiveRL (uncertainty-guided environment generation + PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PPO actor-critic RL agent where at each training iteration the environment configuration ϕ is optimized via gradient ascent on Monte Carlo Dropout uncertainty of the critic (objective L([ϕ,s0],θ) − γ||ϕ−ϕ0||^2), using ExtragradientAdam to respect bounds, then the agent collects trajectories in that proposed environment and updates PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>active learning / uncertainty maximization via gradient-based optimization (no replay buffer)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>For each training iteration, extract base ϕ from the initial state, compute uncertainty of the critic via C forward passes with dropout, evaluate objective O = L - γ||ϕ−ϕ0||^2, perform constrained gradient ascent on ϕ (ExtragradientAdam) for N inner steps, set the environment to the optimized ϕ, collect trajectories, and update the policy via PPO. Thus the algorithm adapts the next training environment based on the agent's current model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Sinergym building HVAC simulation (same as ActivePLR)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same as ActivePLR: stochastic OU-noise weather model; continuous 20-D state; 5-D continuous environment configuration ϕ (weather µ offsets); continuous action setpoints; non-stationary potential due to weather extremes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>State dim 20; action dim ≈2 continuous setpoints; ϕ in R^5; training episodic timestep ∆t = 1 h; training runs are long (multi-million timesteps in reported experiments); inner optimization of ϕ uses up to N ~ O(100) gradient steps in hyperparam settings.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>ActiveRL performed similarly to ActivePLR in many experiments and, on the 120 US-weather benchmark, ActiveRL achieved higher reward than every baseline and slightly outperformed ActivePLR on that dataset. In Sim2Real transfer tests ActiveRL had a 6.1% relative drop in reward (worse than ActivePLR's 3.1%), but still smaller than vanilla RL (8.5%). ActiveRL reduced ASHRAE comfort violations by ~15% relative to vanilla RL (same number reported for ActivePLR/ActiveRL pair). Training + evaluation runtimes: ~12 hours per trial; high-fidelity evaluation ~1 hour.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Compared to vanilla PPO trained on ϕ0 and DR/RPLR baselines, ActiveRL produced higher average reward across handcrafted extreme environments and across 120 US weather patterns; baselines like DR and RPLR often underperformed or generated unrealistic environments that degraded generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Same experimental regime as ActivePLR: multi-million timesteps overall; inner optimization increases per-episode compute but no explicit sample-efficiency reduction numbers were reported. Empirical improvements are presented after full training (hours / millions of timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit exploration via choosing environments that maximize critic uncertainty; no replay buffer means newly generated environments are immediately exploited for learning via PPO updates. Soft constraint γ modulates exploration distance from ϕ0 to avoid unrealistic environments; learning rate η for ϕ controls granularity of exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to vanilla PPO (ϕ0), Domain Randomization (DR), PLR/RPLR, RBC, Random controller.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>ActiveRL (direct uncertainty-based environment generation) provides substantial robustness gains and generalization to diverse, realistic weather patterns (120 US-weather benchmark), outperforming baselines on every tested real-weather environment. On handcrafted extremes ActiveRL is comparable to ActivePLR; ActiveRL shows slightly higher performance on the 120-location test set but slightly worse Sim2Real relative drop than ActivePLR.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same limitations as ActivePLR: requires continuous ϕ, relies on a simulator, sensitive to γ and η (ablation showed γ large and η small helped), potential to generate unrealistic environments if constraints are weak. No explicit failure in partially observable regime is reported; no ability to handle categorical env vars without preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Reinforcement Learning for Robust Building Control', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1123.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1123.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLR / RPLR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prioritized Level Replay / Robust Prioritized Level Replay</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PLR is a UED sampling framework that prioritizes procedurally generated environment levels according to value loss (producing a curriculum of harder levels); RPLR is a variant that trains only on PLR-selected levels and stops gradients from randomly sampled levels to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prioritized Level Replay (Jiang, Grefenstette, and Rocktäschel 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PLR / RPLR (used with PPO agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A sampling/level-selection module used alongside PPO that maintains a replay buffer Λ_seen of seen environment configurations; new levels are sampled from the training distribution or from the buffer; levels in the buffer are prioritized by recorded value-loss magnitude and staleness; RPLR variant uses only PLR-selected levels for training updates.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>prioritized replay / curriculum generation based on value-loss (regret-like objective), resampling previously seen environments by priority</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>PLR either samples a new level from the training distribution or a previously seen level from Λ_seen (decision using PD). When sampling from Λ_seen, probabilities are proportional to recent value loss and staleness. RPLR further restricts training to PLR-selected levels and prevents gradient updates from randomly chosen levels.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Sinergym building HVAC simulation</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same building/HVAC stochastic continuous environment; PLR's operation assumes procedural environment configurations (ϕ) can be stored and replayed.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same: continuous 20-D state; ϕ in R^5; action setpoints continuous; training expensive resets make some advanced variants (e.g., SAMPLR) infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>RPLR sometimes outperformed DR and RBC over extremes but in the paper RPLR generally performed worse than vanilla RL on some tasks; over extreme weather conditions RPLR beat DR and RBC but performed worse than vanilla RL. PLR alone (vanilla PLR sampling new levels uniformly at random) was not as effective as ActivePLR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Compared to vanilla PPO and ActivePLR/ActiveRL, PLR/RPLR gave mixed results: DR/RPLR achieved ≈9% higher reward than RBC on ϕ0 after 3M timesteps, but vanilla RL outperformed DR/RPLR on ϕ0 by ≈8%. In Sim2Real transfer DR/RPLR had ≈7% relative drop, slightly better than vanilla RL's 8.5% drop but worse in absolute performance than ActivePLR.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Uses replay of previously collected levels to improve sample efficiency; however, the paper notes that populating the replay buffer via uniform sampling (DR) yields many unrealistic environments, limiting benefit. No explicit sample-count improvements reported.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration via sampling new levels from training distribution (often uniform DR) vs. exploitation via replaying high value-loss levels. The PD probability p = |Λ_seen| / N_PLR controls the balance of sampling new vs. replayed levels; RPLR biases training to prioritized (hard) levels only.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly against ActivePLR and ActiveRL, DR, vanilla PPO, RBC, Random controller.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>PLR-style prioritized replay improves curriculum learning when replay buffer contains realistic, informative levels; however, when the replay buffer is populated by uniform domain randomization across a wide or unrealistic ϕ space, PLR/RPLR can still suffer because many stored levels are unrealistic and unhelpful. Active generation of realistic uncertain levels (ActivePLR) outperformed PLR/RPLR in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>PLR/RPLR rely on the distribution used to populate the replay buffer; if initial sampling (e.g., DR) produces unrealistic environments, PLR cannot correct that and may prioritize unrealistic levels. SAMPLR (sampled matched PLR) was infeasible in this work due to simulator reset cost. RPLR combined with the authors' uncertainty-based environment proposer performed worse than ActivePLR in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Reinforcement Learning for Robust Building Control', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1123.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1123.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain Randomization (DR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach where environment configuration parameters ϕ are sampled uniformly at random across specified bounds to encourage generalization across diverse conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Domain Randomization + PPO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PPO agent trained on environments produced by uniformly sampling ϕ ∼ U(a,b) across predefined bounds for the 5 weather µ-offset parameters; no uncertainty-driven adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>randomized (non-adaptive) environment sampling</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>No adaptive strategy: environments are sampled uniformly at random from bounds at level-creation time (domain randomization).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Sinergym building HVAC simulation</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same stochastic continuous building/HVAC sim; DR attempts to cover wide ϕ space including extremes; OU-noise weather generation with µ offsets sampled uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>State dim 20, ϕ in R^5, action dim ~2; training across a wide range of environmental configurations increases effective problem complexity for the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>As a baseline (non-adaptive), DR performed poorly in many cases: DR did about as well as RBC over the 5 extreme environments and did not significantly improve over vanilla RL on the base environment; authors hypothesize DR produced unrealistic environments that harmed learning.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No specific sample-efficiency numbers; DR required the same multi-million-timestep PPO training used across experiments and did not produce faster learning in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>No explicit tradeoff mechanism beyond uniform coverage of ϕ; exploration is undirected and can include unrealistic regimes, leading to a weaker exploitation signal.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against vanilla PPO, PLR/RPLR, ActivePLR, ActiveRL, RBC, Random controller.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Domain Randomization (uniform sampling of weather offsets) can lead to unrealistic training environments that do not help the agent perform better in realistic extreme scenarios; DR underperformed compared to uncertainty-driven methods in this building-control domain.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Generates many unrealistic environment configurations when bounds are wide; can hurt generalization if training focuses on unrealistic or irrelevant scenarios; lacks a mechanism to prioritize realistic, informative environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Reinforcement Learning for Robust Building Control', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergent complexity and zero-shot transfer via unsupervised environment design <em>(Rating: 2)</em></li>
                <li>Prioritized Level Replay <em>(Rating: 2)</em></li>
                <li>Evolving curricula with regret-based environment design <em>(Rating: 2)</em></li>
                <li>Grounding Aleatoric Uncertainty in Unsupervised Environment Design <em>(Rating: 2)</em></li>
                <li>Sinergym: A Building Simulation and Control Framework for Training Reinforcement Learning Agents <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1123",
    "paper_id": "paper-263331431",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "ActivePLR",
            "name_full": "Active Prioritized Level Replay",
            "brief_description": "An uncertainty-driven unsupervised environment design (UED) method that gradient-optimizes continuous environment configuration parameters to generate new training levels at the frontier of an RL agent's uncertainty, then adds those levels into a PLR replay buffer for prioritized training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ActivePLR (UED + PPO)",
            "agent_description": "PPO actor-critic RL agent (two hidden layers, 256 units each, dropout + ReLU) trained with a UED teacher that: (1) estimates critic uncertainty via Monte Carlo Dropout, (2) backpropagates gradients of that uncertainty to environment configuration parameters ϕ (the part of the initial state), and (3) uses constrained gradient ascent (Extragradient Adam with hard bounds and optional soft-distance penalty) to propose new realistic environments which are added to a Prioritized Level Replay buffer and sampled based on value-loss/staleness.",
            "adaptive_design_method": "active learning / uncertainty-driven adaptive environment design (gradient-based uncertainty maximization integrated with Prioritized Level Replay)",
            "adaptation_strategy_description": "At each environment-generation step, compute uncertainty L([ϕ, s0], θ) using Monte Carlo Dropout on the critic outputs (C stochastic forward passes). Perform gradient ascent on ϕ to maximize L subject to hard bounds (extragradient / Lagrangian projection) and an optional soft penalty −γ||ϕ−ϕ0||^2 to keep proposals near the base environment. The optimized ϕ is added to the PLR replay buffer; PLR then prioritizes levels by recorded value loss and staleness for sampling during PPO updates.",
            "environment_name": "Sinergym building HVAC simulation (EnergyPlus used for higher-fidelity evaluation)",
            "environment_characteristics": "Stochastic building/HVAC simulator with procedural weather generation (Ornstein-Uhlenbeck noise model); continuous 20-dimensional observed state (includes 5 outdoor weather vars + 15 indoor/occupancy/control vars); continuous-valued environment configuration ϕ ∈ R^5 (µ offsets for temperature, humidity, wind speed, wind direction, solar irradiance); non-stationary/weather-extreme regimes; reward dense (energy + comfort), not sparse.",
            "environment_complexity": "State dimension = 20 (continuous); environment configuration ϕ dimension = 5 (continuous µ offsets; underlying OU process also has σ, τ but ϕ in the algorithm is 5-D); action space = HVAC setpoints (two setpoints: cooling and heating, continuous); training timesteps: experiments run up to multi-million timesteps (e.g., referenced 3M timesteps); training episodes operate at ∆t = 1 hour; evaluation also at ∆t = 0.25 h (actions repeated 4×).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "ActivePLR produced the strongest overall results: +9% reward over vanilla RL on base environment ϕ0, +24% over the rule-based controller (RBC) on the base environment, and ≈+3% average reward over vanilla RL across all six handcrafted environments (ϕ0 + 5 extreme weathers). Reduced days with ASHRAE thermal comfort violations by ~15% relative to vanilla RL. In Sim2Real (higher-fidelity EnergyPlus surrogate) ActivePLR had only a 3.1% relative drop in reward vs. its low-fidelity evaluation and achieved higher absolute reward than other data-driven baselines. Training/evaluation runtime: ~12 hours per ActivePLR trial; high-fidelity evaluation ~1 hour.",
            "performance_without_adaptation": "Baselines: vanilla PPO trained on ϕ0 (no UED) and DR/RPLR. Vanilla PPO was competitive on the base environment but worse in extremes; DR and RPLR generally underperformed: DR and RPLR achieved ~9% higher reward than RBC on ϕ0 after 3M timesteps, but vanilla RL outperformed DR/RPLR on ϕ0 by ~8% (i.e., DR/RPLR sometimes hurt performance). In Sim2Real, vanilla RL had an average 8.5% relative drop in reward; DR/RPLR about 7% relative drop. Exact numeric baselines vary per experiment and environment.",
            "sample_efficiency": "Reported experiments used multi-million timestep training (examples reference 3M timesteps). Each ActivePLR trial took ~12 hours; ActivePLR required repeated inner gradient-ascent steps per level (N up to O(100) in hyperparams). No specific reduced-sample-count numbers reported; empirical gains are reported after the full training runs (multi-million steps).",
            "exploration_exploitation_tradeoff": "Exploration is driven by explicitly maximizing critic uncertainty (via MC Dropout variance) to produce novel but constrained environment configurations; exploitation occurs during standard PPO policy updates using collected trajectories. PLR replay sampling balances 'novel/new' (sample new ϕ via ActivePLR) vs. replaying previously seen levels with probability P_D (Bernoulli based on |Λ_seen|/N_PLR), and replay priorities combine value loss and staleness (weighted by ρ). The soft constraint γ controls how far exploration may stray from the base environment (realism vs. novelty tradeoff).",
            "comparison_methods": "Baselines compared: vanilla PPO (trained on ϕ0), Domain Randomization (DR), Prioritized Level Replay (PLR) and Robust PLR (RPLR), Sampled Matched PLR (SAMPLR not used due to simulator cost), Rule-based controller (RBC), Random controller.",
            "key_results": "Uncertainty-driven gradient-based environment design (ActivePLR) yields better robustness to extreme weather and better Sim2Real transfer than DR, PLR/RPLR, and vanilla RL. ActivePLR finds realistic high-uncertainty environments (constrained by hard/soft constraints) that improve the policy's performance in both base and extreme test environments and reduces comfort violations. ActivePLR produced the smallest Sim2Real performance degradation (~3.1% relative drop) among data-driven methods and consistently outperformed baselines on a 120-US-weather-pattern benchmark (ActiveRL slightly ahead on that dataset).",
            "limitations_or_failures": "Requires a simulator and continuous environment configuration variables (cannot directly handle categorical ϕ without dequantization). Risk of generating unrealistic ϕ if constraints are weak (necessitating hard bounds and γ soft-penalty). Performance can be sensitive to hyperparameters (γ, η) though some robustness reported (γ large gave better results; η small improved stability). Possible attraction to local optima: ActivePLR and ActiveRL sometimes converge to similar behaviors, suggesting local (or global) minima that may not generalize across simulation fidelities.",
            "uuid": "e1123.0",
            "source_info": {
                "paper_title": "Active Reinforcement Learning for Robust Building Control",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ActiveRL",
            "name_full": "Active Reinforcement Learning (uncertainty-driven environment generation)",
            "brief_description": "A variant of ActivePLR which directly generates training environments by gradient-ascent on critic uncertainty (no replay buffer): per training iteration it optimizes ϕ to maximize uncertainty (with constraints) and immediately collects trajectories from the generated environment for PPO updates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ActiveRL (uncertainty-guided environment generation + PPO)",
            "agent_description": "PPO actor-critic RL agent where at each training iteration the environment configuration ϕ is optimized via gradient ascent on Monte Carlo Dropout uncertainty of the critic (objective L([ϕ,s0],θ) − γ||ϕ−ϕ0||^2), using ExtragradientAdam to respect bounds, then the agent collects trajectories in that proposed environment and updates PPO.",
            "adaptive_design_method": "active learning / uncertainty maximization via gradient-based optimization (no replay buffer)",
            "adaptation_strategy_description": "For each training iteration, extract base ϕ from the initial state, compute uncertainty of the critic via C forward passes with dropout, evaluate objective O = L - γ||ϕ−ϕ0||^2, perform constrained gradient ascent on ϕ (ExtragradientAdam) for N inner steps, set the environment to the optimized ϕ, collect trajectories, and update the policy via PPO. Thus the algorithm adapts the next training environment based on the agent's current model uncertainty.",
            "environment_name": "Sinergym building HVAC simulation (same as ActivePLR)",
            "environment_characteristics": "Same as ActivePLR: stochastic OU-noise weather model; continuous 20-D state; 5-D continuous environment configuration ϕ (weather µ offsets); continuous action setpoints; non-stationary potential due to weather extremes.",
            "environment_complexity": "State dim 20; action dim ≈2 continuous setpoints; ϕ in R^5; training episodic timestep ∆t = 1 h; training runs are long (multi-million timesteps in reported experiments); inner optimization of ϕ uses up to N ~ O(100) gradient steps in hyperparam settings.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "ActiveRL performed similarly to ActivePLR in many experiments and, on the 120 US-weather benchmark, ActiveRL achieved higher reward than every baseline and slightly outperformed ActivePLR on that dataset. In Sim2Real transfer tests ActiveRL had a 6.1% relative drop in reward (worse than ActivePLR's 3.1%), but still smaller than vanilla RL (8.5%). ActiveRL reduced ASHRAE comfort violations by ~15% relative to vanilla RL (same number reported for ActivePLR/ActiveRL pair). Training + evaluation runtimes: ~12 hours per trial; high-fidelity evaluation ~1 hour.",
            "performance_without_adaptation": "Compared to vanilla PPO trained on ϕ0 and DR/RPLR baselines, ActiveRL produced higher average reward across handcrafted extreme environments and across 120 US weather patterns; baselines like DR and RPLR often underperformed or generated unrealistic environments that degraded generalization.",
            "sample_efficiency": "Same experimental regime as ActivePLR: multi-million timesteps overall; inner optimization increases per-episode compute but no explicit sample-efficiency reduction numbers were reported. Empirical improvements are presented after full training (hours / millions of timesteps).",
            "exploration_exploitation_tradeoff": "Explicit exploration via choosing environments that maximize critic uncertainty; no replay buffer means newly generated environments are immediately exploited for learning via PPO updates. Soft constraint γ modulates exploration distance from ϕ0 to avoid unrealistic environments; learning rate η for ϕ controls granularity of exploration.",
            "comparison_methods": "Compared to vanilla PPO (ϕ0), Domain Randomization (DR), PLR/RPLR, RBC, Random controller.",
            "key_results": "ActiveRL (direct uncertainty-based environment generation) provides substantial robustness gains and generalization to diverse, realistic weather patterns (120 US-weather benchmark), outperforming baselines on every tested real-weather environment. On handcrafted extremes ActiveRL is comparable to ActivePLR; ActiveRL shows slightly higher performance on the 120-location test set but slightly worse Sim2Real relative drop than ActivePLR.",
            "limitations_or_failures": "Same limitations as ActivePLR: requires continuous ϕ, relies on a simulator, sensitive to γ and η (ablation showed γ large and η small helped), potential to generate unrealistic environments if constraints are weak. No explicit failure in partially observable regime is reported; no ability to handle categorical env vars without preprocessing.",
            "uuid": "e1123.1",
            "source_info": {
                "paper_title": "Active Reinforcement Learning for Robust Building Control",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "PLR / RPLR",
            "name_full": "Prioritized Level Replay / Robust Prioritized Level Replay",
            "brief_description": "PLR is a UED sampling framework that prioritizes procedurally generated environment levels according to value loss (producing a curriculum of harder levels); RPLR is a variant that trains only on PLR-selected levels and stops gradients from randomly sampled levels to improve robustness.",
            "citation_title": "Prioritized Level Replay (Jiang, Grefenstette, and Rocktäschel 2021)",
            "mention_or_use": "use",
            "agent_name": "PLR / RPLR (used with PPO agent)",
            "agent_description": "A sampling/level-selection module used alongside PPO that maintains a replay buffer Λ_seen of seen environment configurations; new levels are sampled from the training distribution or from the buffer; levels in the buffer are prioritized by recorded value-loss magnitude and staleness; RPLR variant uses only PLR-selected levels for training updates.",
            "adaptive_design_method": "prioritized replay / curriculum generation based on value-loss (regret-like objective), resampling previously seen environments by priority",
            "adaptation_strategy_description": "PLR either samples a new level from the training distribution or a previously seen level from Λ_seen (decision using PD). When sampling from Λ_seen, probabilities are proportional to recent value loss and staleness. RPLR further restricts training to PLR-selected levels and prevents gradient updates from randomly chosen levels.",
            "environment_name": "Sinergym building HVAC simulation",
            "environment_characteristics": "Same building/HVAC stochastic continuous environment; PLR's operation assumes procedural environment configurations (ϕ) can be stored and replayed.",
            "environment_complexity": "Same: continuous 20-D state; ϕ in R^5; action setpoints continuous; training expensive resets make some advanced variants (e.g., SAMPLR) infeasible.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "RPLR sometimes outperformed DR and RBC over extremes but in the paper RPLR generally performed worse than vanilla RL on some tasks; over extreme weather conditions RPLR beat DR and RBC but performed worse than vanilla RL. PLR alone (vanilla PLR sampling new levels uniformly at random) was not as effective as ActivePLR.",
            "performance_without_adaptation": "Compared to vanilla PPO and ActivePLR/ActiveRL, PLR/RPLR gave mixed results: DR/RPLR achieved ≈9% higher reward than RBC on ϕ0 after 3M timesteps, but vanilla RL outperformed DR/RPLR on ϕ0 by ≈8%. In Sim2Real transfer DR/RPLR had ≈7% relative drop, slightly better than vanilla RL's 8.5% drop but worse in absolute performance than ActivePLR.",
            "sample_efficiency": "Uses replay of previously collected levels to improve sample efficiency; however, the paper notes that populating the replay buffer via uniform sampling (DR) yields many unrealistic environments, limiting benefit. No explicit sample-count improvements reported.",
            "exploration_exploitation_tradeoff": "Exploration via sampling new levels from training distribution (often uniform DR) vs. exploitation via replaying high value-loss levels. The PD probability p = |Λ_seen| / N_PLR controls the balance of sampling new vs. replayed levels; RPLR biases training to prioritized (hard) levels only.",
            "comparison_methods": "Compared directly against ActivePLR and ActiveRL, DR, vanilla PPO, RBC, Random controller.",
            "key_results": "PLR-style prioritized replay improves curriculum learning when replay buffer contains realistic, informative levels; however, when the replay buffer is populated by uniform domain randomization across a wide or unrealistic ϕ space, PLR/RPLR can still suffer because many stored levels are unrealistic and unhelpful. Active generation of realistic uncertain levels (ActivePLR) outperformed PLR/RPLR in this domain.",
            "limitations_or_failures": "PLR/RPLR rely on the distribution used to populate the replay buffer; if initial sampling (e.g., DR) produces unrealistic environments, PLR cannot correct that and may prioritize unrealistic levels. SAMPLR (sampled matched PLR) was infeasible in this work due to simulator reset cost. RPLR combined with the authors' uncertainty-based environment proposer performed worse than ActivePLR in their experiments.",
            "uuid": "e1123.2",
            "source_info": {
                "paper_title": "Active Reinforcement Learning for Robust Building Control",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Domain Randomization (DR)",
            "name_full": "Domain Randomization",
            "brief_description": "A baseline approach where environment configuration parameters ϕ are sampled uniformly at random across specified bounds to encourage generalization across diverse conditions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Domain Randomization + PPO",
            "agent_description": "PPO agent trained on environments produced by uniformly sampling ϕ ∼ U(a,b) across predefined bounds for the 5 weather µ-offset parameters; no uncertainty-driven adaptation.",
            "adaptive_design_method": "randomized (non-adaptive) environment sampling",
            "adaptation_strategy_description": "No adaptive strategy: environments are sampled uniformly at random from bounds at level-creation time (domain randomization).",
            "environment_name": "Sinergym building HVAC simulation",
            "environment_characteristics": "Same stochastic continuous building/HVAC sim; DR attempts to cover wide ϕ space including extremes; OU-noise weather generation with µ offsets sampled uniformly.",
            "environment_complexity": "State dim 20, ϕ in R^5, action dim ~2; training across a wide range of environmental configurations increases effective problem complexity for the policy.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "As a baseline (non-adaptive), DR performed poorly in many cases: DR did about as well as RBC over the 5 extreme environments and did not significantly improve over vanilla RL on the base environment; authors hypothesize DR produced unrealistic environments that harmed learning.",
            "sample_efficiency": "No specific sample-efficiency numbers; DR required the same multi-million-timestep PPO training used across experiments and did not produce faster learning in practice.",
            "exploration_exploitation_tradeoff": "No explicit tradeoff mechanism beyond uniform coverage of ϕ; exploration is undirected and can include unrealistic regimes, leading to a weaker exploitation signal.",
            "comparison_methods": "Compared against vanilla PPO, PLR/RPLR, ActivePLR, ActiveRL, RBC, Random controller.",
            "key_results": "Domain Randomization (uniform sampling of weather offsets) can lead to unrealistic training environments that do not help the agent perform better in realistic extreme scenarios; DR underperformed compared to uncertainty-driven methods in this building-control domain.",
            "limitations_or_failures": "Generates many unrealistic environment configurations when bounds are wide; can hurt generalization if training focuses on unrealistic or irrelevant scenarios; lacks a mechanism to prioritize realistic, informative environments.",
            "uuid": "e1123.3",
            "source_info": {
                "paper_title": "Active Reinforcement Learning for Robust Building Control",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emergent complexity and zero-shot transfer via unsupervised environment design",
            "rating": 2,
            "sanitized_title": "emergent_complexity_and_zeroshot_transfer_via_unsupervised_environment_design"
        },
        {
            "paper_title": "Prioritized Level Replay",
            "rating": 2,
            "sanitized_title": "prioritized_level_replay"
        },
        {
            "paper_title": "Evolving curricula with regret-based environment design",
            "rating": 2,
            "sanitized_title": "evolving_curricula_with_regretbased_environment_design"
        },
        {
            "paper_title": "Grounding Aleatoric Uncertainty in Unsupervised Environment Design",
            "rating": 2,
            "sanitized_title": "grounding_aleatoric_uncertainty_in_unsupervised_environment_design"
        },
        {
            "paper_title": "Sinergym: A Building Simulation and Control Framework for Training Reinforcement Learning Agents",
            "rating": 2,
            "sanitized_title": "sinergym_a_building_simulation_and_control_framework_for_training_reinforcement_learning_agents"
        }
    ],
    "cost": 0.016236,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Active Reinforcement Learning for Robust Building Control
16 Dec 2023</p>
<p>Doseok Jang djang@berkeley.edu 
UC Berkeley</p>
<p>Larry Yan yanlarry@berkeley.edu 
UC Berkeley</p>
<p>Lucas Spangher spangher@berkeley.edu 
UC Berkeley</p>
<p>Costas Spanos spanos@berkeley.edu 
UC Berkeley</p>
<p>Active Reinforcement Learning for Robust Building Control
16 Dec 20234B929CE7CA4E1CBF5F04B0543ADD0752arXiv:2312.10289v1[cs.LG]
Reinforcement learning (RL) is a powerful tool for optimal control that has found great success in Atari games, the game of Go, robotic control, and building optimization.RL is also very brittle; agents often overfit to their training environment and fail to generalize to new settings.Unsupervised environment design (UED) has been proposed as a solution to this problem, in which the agent trains in environments that have been specially selected to help it learn.Previous UED algorithms focus on trying to train an RL agent that generalizes across a large distribution of environments.This is not necessarily desirable when we wish to prioritize performance in one environment over others.In this work, we will be examining the setting of robust RL building control, where we wish to train an RL agent that prioritizes performing well in normal weather while still being robust to extreme weather conditions.We demonstrate a novel UED algorithm, ActivePLR, that uses uncertainty-aware neural network architectures to generate new training environments at the limit of the RL agent's ability while being able to prioritize performance in a desired base environment.We show that ActivePLR is able to outperform state-of-the-art UED algorithms in minimizing energy usage while maximizing occupant comfort in the setting of building control.</p>
<p>Introduction</p>
<p>Reinforcement learning has demonstrated remarkable success in solving sequential decision-making tasks such as the game of Go (Silver et al. 2017), Atari games (Mnih et al. 2013), energy pricing (Jang et al. 2021;Gunn et al. 2022), and many others.However, RL agents often overfit to their training environment and fail to generalize to new environments (Zhang et al. 2018).This is a serious issue in tasks where we expect underlying dynamics in the environment not to stay static, e.g. when there is distribution shift between the training environment and the test environment.Here, we explore the use of RL in residential and commercial building control, where most often an agent is trained to optimize performance in normal weather conditions.When underlying weather conditions exhibit extremes or drift due to long term effects such as climate change, control often fails.We endeavor to address distribution shift by using uncertainty to select training environments such that the resulting RL agent performs well in average conditions and is also robust to uncommon but dangerous scenarios in the test environment.</p>
<p>Overview of HVAC Setpoint Control</p>
<p>Our focus is on robust RL for building energy consumption, which represent 73% of electricity usage and 40% of greenhouse gases in the US (U.S.Department of Energy-EIA 2020).Buildings are generating increasingly large amounts of sensory information that can be used to increase energy efficiency, such as temperature, airflow, humidity, occupancy, light, and energy usage (Hayat et al. 2019).</p>
<p>There are several ways buildings can be automatically controlled: heating, ventilation, and air conditioning (HVAC) units, energy storage systems, plug-in electric vehicles, photovoltaic power sources, and lights (Gong et al. 2022).We will focus on HVAC as it represents roughly a third of total building energy consumption (Wemhoff and Frank 2010).Traditionally, HVAC setpoint control has been approached through model-predictive control (MPC, Kou et al. 2021) or a heuristic rule-based-controller (RBC, Mathews et al. 2001).1 MPC is generally not scalable to high dimensional input or output spaces compared to RL, and heuristics are inflexible.</p>
<p>Recently, RL-based HVAC setpoint control has grown in popularity (Das et al. 2022).Rizvi and Pertzborn (2022) used Q-learning to control HVAC setpoints in the presence of unseen disturbances.Kurte et al. (2020) demonstrated how RL and meta-RL could train an RL HVAC agent that quickly adapts to different buildings.Xu et al. (2020) explored how to use transfer learning to train an RL HVAC controller that outperforms a RBC baseline across a variety of simulated buildings and climates.Figure 1 illustrates the flow of information in our RL HVAC control setup.</p>
<p>As climate change continues, we will experience more droughts, heat waves, rising temperatures, and cold snaps (Masson-Delmotte et al. 2021).These extreme weather events are likely underrepresented in the training data, but are essential to account for in reliable and safe building control.For example, consider an RL controller trained on the Figure 1: HVAC setpoint control in Sinergym.A simulated building sends sensor data as observations to an RL agent, which responds with HVAC setpoints as actions, and is rewarded according to energy use and thermal comfort.dry weather of the fictional country of Desertland.If the climate of Desertland changes to have increased humidity, the RL controller may react by raising temperature during a heat wave, which is energy-inefficient, unacceptable for occupant comfort, and may even threaten occupant health.</p>
<p>To our knowledge, the problem of training RL HVAC controllers that are robust to changing weather is understudied, and current work focuses on detecting climate change and retraining RL controllers as their performance drops over time (Naug, Quinones-Grueiro, and Biswas 2022;Deng, Zhang, and Qi 2022), or training RL controllers that can be transferred to buildings in other climates (Xu et al. 2020;Lissa, Schukat, and Barrett 2020).These works focus on RL training pipelines that are robust to long-term changes in climate, but will still underperform during short-term extreme weather events that are rare in their training distribution.</p>
<p>Unsupervised Environment Design</p>
<p>The process of automatically selecting areas of the state space to explore is known as active learning, or as optimal experiment design.Active learning has mostly been explored for supervised learning.For example, Cohn, Atlas, and Ladner (1994); Faria et al. (2022) find regions of uncertainty in the data distribution through misclassification rates and output entropy.Makili, Sánchez, and Dormido-Canto (2012) uses conformal prediction to quantify the similarity of new data points to their dataset.EVOP (Lynch 2003) uses the sequential simplex method in order to identify experiment configurations that can maximize information gain.Bouneffouf (2016) use random exploration to identify new, promising data samples.Some of these concepts are already in use in many RL algorithms; for example, the RL algorithm we use in this paper ( PPO Schulman et al. 2017), is incentivized to explore new data samples via random exploration and increasing the output action distribution entropy.</p>
<p>Our setting of Unsupervised Environment Design (UED, Dennis et al. 2020) is related, but different from active learning, in that we are not directly selecting training data points to sample, but selecting parameters of the environment that generates training data points.This is a more helpful problem setting in RL, as RL agents perform well with on-policy data that is collected as the RL agent explores the environment.UED is the problem of selecting new environment parameters that maximize the RL agent's generalization across diverse environments.2Parker-Holder et al. ( 2022); Dennis et al. (2020) find adversarial but feasible environment configurations with high regret; Jiang, Grefenstette, and Rocktäschel (2021) re-samples previously seen environments based on their 1-step TD error.These algorithms focus on training an RL agent that performs well across a distribution of similar tasks.SAMPLR (Jiang et al. 2022), a recent method, introduces the concept of curriculum induced covariate shift (CICS) and addresses it by launching several child simulations at each step to explore other step trajectories, but this approach does not scale computationally, especially when in an area such as building control, initializing state of the art building physics simulations comprises a large proportion of the overall computation.</p>
<p>Contributions</p>
<p>We present a novel gradient-based algorithm for UED in building control called ActivePLR that leverages agent uncertainty.To the best of our knowledge, this is the first time neural network uncertainty has been incorporated into the problem of UED; most current works focus on some form of regret; this is also the first time environment configuration variables have been directly optimized under gradient ascent rather than through some evolutionary process (Parker-Holder et al. 2022), resampling procedure (Jiang, Grefenstette, and Rocktäschel 2021), or training a separate teacher network to select new environments (Dennis et al. 2020).To the best of our knowledge, we are also the first to focus on training RL HVAC agents that are robust to short-term extreme weather events rather than long-term climate change.</p>
<p>We demonstrate how ActivePLR trains RL HVAC controllers that are (1) more performant overall, (2) robust to extreme weather conditions, and (3) more robust to the Sim2Real transfer than the current state-of-the-art in UED.</p>
<p>Methods</p>
<p>Reinforcement Learning (RL)</p>
<p>RL is a framework for finding the optimal policy in an environment.Environments are formalized by a Markov Decision Process (MDP), which consists of a tuple (S, A, T, R).State and Action spaces (S and A) consist of tuples of some fixed length indexed per timestep t; T : S × A → S is a transition function; and R : S × A × S → R is a reward function.Agents choose actions according to a probability distribution p π θ , determined by a policy π θ with parameters θ to optimize the RL objective J(θ), defined J(θ) = E π st,at∼pπ [r(s t , a t )] .(See Sutton and Barto 2018.)We use the PPO (Schulman et al. 2017) RL algorithm due to its performance and previous use in building control.Many works (Chen, Cai, and Bergés 2019;Zhang et al. 2021) have used PPO for HVAC control.Although we focus on PPO in this paper, our algorithm should easily extend to any actor-critic RL algorithm.</p>
<p>Uncertainty Estimation</p>
<p>To estimate the uncertainty of our RL agent, we use Monte Carlo Dropout (Gal and Ghahramani 2016), in which nodes in the neural network are set to zero ("dropped out") at random.This is used at inference time to generate multiple predictions for an individual input from different variations of the same model.The variance in these predictions is then used as a measure of the model's uncertainty.We use Monte Carlo Dropout as opposed to other methods of estimating neural network uncertainty such as bootstrapped ensembles (Lakshminarayanan, Pritzel, and Blundell 2017) or Bayesian neural networks (Wang et al. 2020) because Monte Carlo Dropout is simpler and cheaper to train than bootstrapped ensembles and Bayesian neural networks while still providing good quantifications of uncertainty.</p>
<p>Formally, suppose we have a neural network
f θ := R n → R m mapping n-dimensional input vectors to m-dimensional output vectors. f is parameterized by a list of l weight matri- ces θ := W i | l i=1
, where W i ∈ R Ni−1×Ni denotes the weight matrix for layer i of the neural network and N i denotes the number of neurons in that layer.We denote the dropout operation by d p : R k → R k , where d p (θ) zeros-out each column of W i in θ with probability p.We define the uncertainty L for input x ∈ R n as L(x, θ) = Var(f dp(θ) (x)), estimated by
L(x, θ) = 1 C C c=1 f dp(θ) (x) T f dp(θ) (x)− E f dp(θ) (x) T E f dp(θ) (x) (1)
Essentially, we conduct C independent stochastic forward passes through the model with dropout at inference time, and use the sample variance of the outputs as our uncertainty metric.To estimate the uncertainty of our RL agent, we use the uncertainty of the critic network similar to other works (An et al. 2021;Wu et al. 2021).</p>
<p>Robust Prioritized Level Replay</p>
<p>One key assumption we make in the design of this algorithm is that, at the beginning of each training episode, we interact with an RL environment that can change its dynamics in response to some configuration parameters; for example, in this work we focus on a building simulation environment that can change its simulated weather patterns in response to weather configuration variables that we provide at the be-ginning of each training episode.3Prioritized Level Replay (PLR, Jiang, Grefenstette, and Rocktäschel 2021) is a stateof-the-art framework for selectively sampling training levels in environments with procedurally generated content.Levels with higher value loss are prioritized, inducing an emergent curriculum of increasingly difficult levels. 4 For each episode, PLR samples d ∼ P D , to decide whether to sample a new level from the training distribution Λ train or pick one from the replay buffer Λ seen .Jiang, Grefenstette, and Rocktäschel (2021) parameterized P D as a Bernoulli distribution with probability p = |Λseen| |Λtrain| .Since we consider the setting where ϕ is continuous, |Λ train | is infinite, so we set the denominator as a hyperparameter N P LR , so p = |Λseen| N P LR .The probability of each level in the replay buffer being sampled is determined by the value loss, and how stale that estimate of the value loss is.If we do not sample a level from the replay buffer, we sample a new one from the training distribution and add it to the buffer.In this paper, vanilla PLR samples new levels to add to the buffer uniformly at random from the set of all possible training environments.Recently, Jiang et al. (2021) proposed Robust PLR, in which only training on the PLR-selected levels, and stopping gradient updates from the randomly selected levels, generally performs better; we will refer to this variant as RPLR.</p>
<p>ActivePLR</p>
<p>We present ActivePLR: a novel addition to PLR that samples new levels to add to the replay buffer through an uncertaintybased optimization procedure instead of at random. 5 In order to generate new environments at the frontier of the agent's uncertainty, we will backpropagate gradients from the uncertainty back to the state variable and do gradient ascent. 6his will allow us to find the state at which the agent is most uncertain, and generate a new environment that allows the agent to interact with the world at that state.Formally, assume we have a environment E with some parameters ϕ ∈ R k that are part of the agent's initial state space S. That is, the state s 0 ∈ R n can be divided into ϕ and s 0 , where s 0 ∈ R n−k is defined such that s 0 = [ϕ, s 0 ] (and [] is the concatenation operator).We can define an objective function that tries to maximize the uncertainty of the RL agent:
O(ϕ i , s 0 , θ) = L([ϕ i , s 0 ], θ)(2)
where L is our uncertainty estimate.We can then update ϕ:
ϕ i+1 = ϕ i + η∇ ϕi O(ϕ i , s 0 , θ)(3)
We use this optimization procedure to identify novel training environments to add to PLR's environment replay buffer.We call this procedure ActivePLR, as it is an active learning method that seeks to identify what data would be most useful for the RL agent. 7We can also use this optimization procedure to generate all the training environments instead of using a replay buffer. 8We denote this case as ActiveRL.</p>
<p>Trying to identify parameters ϕ by maximizing uncertainty can lead to unrealistic parameters that are outside of the test distribution and not useful for learning.Thus, we integrate both hard constraints and soft constraints on ϕ generation in ActivePLR.The hard constraints are useful when trying to train an RL agent that generalizes over a certain region of the ϕ space, and the soft constraints are useful when trying to train an RL agent that emphasizes performance near a particular ϕ 0 , which is still robust to different values of ϕ.We will refer to the latter setting as the ϕ 0neighborhood setting for brevity.</p>
<p>The hard constraints constrain the search space within some lower and upper limits specified by the user for ϕ using the extragradient (Korpelevich 1976) method.Suppose we have a lower bound constraint ϕ &gt; b for some b ∈ R k and an upper bound constraint ϕ &lt; a for some a ∈ R k .Then we can use Lagrangian optimization methods from the Cooper library (Gallego-Posada and Ramirez 2022) to search for a ϕ with high uncertainty within the bounds of a and b, helping to avoid unrealistic values of ϕ.9 Throughout this paper, we will use the implementation of Extragradi-entAdam from Gallego-Posada and Ramirez (2022), which adjusts the learning rate η for each parameter according to Adam (Kingma and Ba 2014).</p>
<p>In the ϕ 0 -neighborhood setting, the hard constraints are not enough -there is no guarantee that states near ϕ 0 will be sampled.Thus we introduce a soft constraint to O to minimize the Euclidean distance from ϕ to ϕ 0 .
O(ϕ i , s 0 , θ) = L([ϕ, s 0 ], θ) − γ||(ϕ − ϕ 0 )|| 2 (4)
where γ emphasizes the soft constraint.1967).Energy use is the total HVAC electricity demand rate in Watts (W).The reward can be formulated as:
R t = −ρ * λ E * P t − (1 − ρ) * λ P * P P D t * 1 (occupancyt&gt;0) * 1 P P Dt&gt;20 (5)
where ρ controls how much to weight comfort against energy use, P t is the electricity demand rate, λ E and λ P are scaling factors to account for varying units, 1 (occupancyt&gt;0) ensures there is no penalty for uncomfortable conditions when there are no occupants, and 1 P P Dt&gt;20 ensures the Algorithm 1: ActivePLR procedure ACTIVEPLR(θ, s 0 , N, T, η, γ, a, b, ρ, N P LR) ▷ θ : policy parameters ▷ s 0 : initial state to seed environment generation ▷ T : # of iterations to run PPO ▷ N : # of iterations to optimize ϕ ▷ η : Learning rate for optimizing ϕ ▷ γ : Weight on soft constraint ▷ a : ϕ lower bounds ▷ b : ϕ upper bounds ▷ c : Global episode counter ▷ Λ seen : Visited levels ▷ S : Global level scores ▷ C : Global level timestamps (when they were last sampled) ▷ ρ : PLR staleness weighting ▷ N P LR : hyperparameter of
P D ϕ 0 ← ExtractPhi(s 0 ) c ← c + 1 for t=0 to T do Sample replay decision d ∼ P D (N P LR ) if d == 1 then ScoreProb ← P S (ϕ|Λ seen , S) StaleProb ← P C (ϕ|Λ seen , C, c) Sample ϕ ∼ (1 − ρ) • ScoreProb + ρ • StaleProb else for i=0 to N do ϕ ← ExtractPhi(s 0 ) dist ← ||ϕ − ϕ 0 || 2 O ← UncertaintyEstimate(f θ , s 0 ) − γ • dist ϕ ← ExtragradientUpdate(ϕ, O, a, b) s 0 = Concatenate([ϕ, s 0 ]) end for end if Define new index i ← |S| + 1 Add ϕ i ← ϕ to Λ seen Add initial value S i = 0 to S and C i = 0 to C τ ← CollectTrajectories(E ϕ ) Update score S i ← PPOValueLoss(τ, θ) Update timestamp C i ← c θ ← PPOUpdate(τ, θ) end for Return θ end procedure
agent is not penalized if the PPD is below the ASHRAE guidelines' comfort threshold of 20% (ANSI and ASHRAE 2017).We use λ E = 0.0001, λ P = 0.1, ρ = 0.5.The state is a continuous, 20 dimensional vector that includes 5 outdoor weather variables: outdoor air temperature, outdoor relative humidity, wind speed, wind direction, solar irradiance, and 15 other variables: indoor air temperature, indoor relative humidity, clothing value, thermal comfort, current HVAC setpoints, total HVAC electricity demand rate, occupancy count, and date (year, month, day, hour).</p>
<p>To simulate outdoor weather, Sinergym takes as input a file with hourly measurements of each outdoor weather variable.Originally, Sinergym added noise to outdoor temperature through an Ornstein-Uhlenbeck (OU, Doob 1942) process, to help prevent overfitting the agent to a static weather pattern.We modified Sinergym so it could add this noise to the other outdoor weather variables as well.An OU process has three parameters: σ, µ, and τ .σ controls the variance of the added noise, µ is the average value of the noise, and τ determines how quickly the noise reverts to the mean.</p>
<p>We can obtain reasonable values for σ and τ for each weather variable from the original input weather file, so we have 5 remaining parameters to customize Sinergym: the µ offset parameters for each weather variable. 10Thus the ϕ for Sinergym that we vary to attempt to train a robust RL agent, is the 5 dimensional vector &lt; µ 1 , µ 2 , µ 3 , µ 4 , µ 5 &gt;.These essentially change the average outdoor temperature, relative humidity, wind speed, wind direction, and solar irradiance over the course of the simulation.Varying the environment configuration ϕ ∈ R 5 enables us to collect training data from diverse outdoor weather conditions.</p>
<p>All experiments used 24 Intel Xeon E5-2670 CPUs. 11</p>
<p>Baseline Algorithms for HVAC Control</p>
<p>Our basic RL baseline is a PPO agent composed of a neural network with two hidden layers, each with 256 neurons, using dropout and ReLU activations, that is trained on ϕ 0 .</p>
<p>The most common method of training agents that generalize across diverse environments is domain randomization (DR), where ϕ is selected uniformly at random.This method is often used so agents can transfer from simulation to the real world (Chen et al. 2021;Vuong et al. 2019;Tobin et al. 2017).In the buildings domain, Jang et al. (2021) used DR to train an RL energy pricing agent to be robust to the Sim2Real transfer.In our setting: we have some lower bounds a ∈ R 5 and upper bounds b ∈ R 5 for each variable, described in Table 1 in the appendix 12 .We sample ϕ ∼ U (a, b), where U is the uniform distribution.Jiang et al. (2022) propose Sampled Matched PLR (SAM-PLR) to generalize across diverse environments while combating the problem of curriculum-induced covariate shift (CICS), in which the distribution of training environments (ϕ ∼ P ) generated through UED may become too different from the distribution of test environments (ϕ test ∼ P ).Unfortunately we were not able to use SAMPLR as a baseline: SAMPLR resets the simulator at every timestep to collect fictitious trajectories; thus, SAMPLR is infeasible when a reset is expensive compared to a timestep, which is true for Sinergym and many other simulators. 13 We use a rule-based controller (RBC) based on Sinergym's RBC, and a random controller as baselines.The random controller outputs a random cooling setpoint a[0] ∼ U nif orm(22.5, 30.0) and a heating setpoint a[1] ∼ U nif orm(15, 22.5). 14Sinergym's RBC sets the desired temperature range (26-29°C) higher in the summer and lower in the winter (20-23.5°C) to minimize energy consumption.We added a rule that if no occupants are in the building, the RBC sets setpoints with a wide enough range that the HVAC system is turned off.We added this new occupancy-based rule for the sake of a fair comparison because we included occupancy information in the reward.</p>
<p>Our last RL baseline, RPLR, is described in Section 2. Hyperparameters are in Table 2 in the appendix.</p>
<p>Evaluating ActivePLR's Robustness to Extreme Weather Events and the Sim2Real Jump</p>
<p>In order to evaluate how robust agents trained by each algorithm are to extreme weather events, we evaluate each agent in a suite of 5 different extreme weather environments parameterized by 5 different ϕ weather configurations, as well as ϕ 0 for a total of 6 environments.The agent is trained on automatically generated environments according to each UED algorithm.It is then evaluated in the following 6 environments: ϕ 0 simulates realistic weather based on recordings from Arizona, USA, ϕ 1 simulates an extremely hot and dry drought, ϕ 2 simulates a wet and windy storm, ϕ 3 simulates a humid heatwave, ϕ 4 simulates a cold snap, and ϕ 5 simulates erratic weather.Our hypothesis is that using uncertainty to identify new environments to collect data from will allow us to train RL agents that are more robust to extreme weather conditions.</p>
<p>In order to test whether or not the RL policies trained in simulation can be extended to the real world, we evaluated each RL algorithm in each of the 6 environments by running the EnergyPlus simulator at a higher fidelity than the agents were trained on, thus simulating the "Sim2Real" jump with a more realistic simulator.The agents were trained on a simulator operating at a granularity of ∆t = 1hour per timestep, and we evaluate on a granularity of ∆t = 0.25hours per timestep.During evaluation, each of the RL agents' actions are simply repeated four times so that it still takes an action every hour.Our hypothesis is that by increasing the state space supported by the training distribution, Ac-tivePLR will help the agent be robust to compounding errors caused by the Sim2Real jump.</p>
<p>Results and Discussion</p>
<p>ActivePLR is Robust to Extreme Weather Events</p>
<p>In order to evaluate how robust agents trained by each algorithm are to extreme weather, we evaluate the agent in a suite of 6 different environments, parameterized by 5 different ϕ extreme weather configurations and ϕ 0 .Figure 4 shows the overall performance of each algorithm.See Figure 5 in the appendix for performance in specific environments.</p>
<p>Surprisingly, DR and RPLR did not have significant improvements over the vanilla RL algorithm.By the end of training, DR and RPLR achieved 9% higher reward than the RBC on the base environment ϕ 0 after 3M timesteps of training.However, the vanilla RL policy had a 8% improvement over the DR and RPLR policies with ϕ 0 .Over the 5 extreme weather environments, DR did about as well as the RBC.The unexpected lack of performance gain may mean the environments generated with DR were too unrealistic to learn how to perform in extreme weather conditions.</p>
<p>Over the extreme weather conditions, RPLR beat DR and the RBC, but performed worse than the vanilla RL policy.This indicates that as RPLR uses DR to sample new environments, it may still suffer from generating unrealistic environments.However, its weighted environment resampling procedure helps it generalize better than naive DR, even though it is resampling from unrealistic environments.We found that training the HVAC controller with ActivePLR resulted in agents that performed better in both the extreme environments and the base environment.</p>
<p>Generally, ActiveRL and ActivePLR performed similarly.In three out of the five extreme environments: (1) the hot drought, (2) the cold and windy, and (3) the cold snap environment, ActivePLR performed significantly better than all other baselines and performed competitively in the remaining two environments.In the base environment, Ac-tivePLR provides a 9% improvement over vanilla RL, and a 24% improvement over RBC.Over all 6 environments, it provides a 3% improvement over vanilla RL.We also found that over the 6 environments, ActivePLR and ActiveRL provided a 15% relative decrease in days with ASHRAE thermal comfort violations compared to vanilla RL (which was the best baseline in terms of thermal comfort), resulting in significantly more comfortable occupants even during extreme weather conditions. 15The fact that ActivePLR significantly outperforms all baselines indicates there is considerable value in seeking out realistic new training environments that maximize an agent's uncertainty rather than choosing environments at random or merely replaying old ones.</p>
<p>ActivePLR Generalizes from Simulation</p>
<p>One flaw in this work and UED algorithms in general is that a simulator is required to train the model in different environments.Thus, it is important to ask whether or not the RL policies trained in simulation can be extended to the real world.In order to approximate the Sim2Real gap, we conducted evaluated each RL algorithm in each of the 6 environments by running the EnergyPlus simulator at a higher fidelity than the agents were trained on, so that these test environments (1) had slightly different dynamics from the original training simulation, which should give rise to similar distribution shift issues as the Sim2Real gap, and (2) had dynamics that were as close to those of the real world as possible because the test simulation was run with higher fidelity and should therefore be more accurate to real dynamics than the training simulation.An illustration of the performances of each algorithm on each of the 6 handcrafted environments from Section 4 are shown in Figure 4.</p>
<p>When the agents are transferred from the simulation to our surrogate for the real world, we see there is a significant performance drop across all data-driven algorithms.Vanilla RL achieves a reward that is 8.5% lower on average across the 6 handcrafted environments when evaluated on the higher fidelity simulation.DR and RPLR have smaller relative drops of about 7%.However, since the 7% is relative to the performance of DR and RPLR in the original low fidelity simulation, vanilla RL still performs better in terms of absolute reward over the six environments.Random and RBC have C.The average drop in reward over all environments when evaluating in the higher fidelity simulation compared to evaluation in the lower fidelity simulation.Lower is better here.Note that none of the algorithms were trained on the specific extreme weather environments.Evaluating ActiveRL and ActivePLR on the higher fidelity simulation took about 1 hour per trial.very small or no relative performance degradation, which is to be expected because they are not data-driven models.They still perform the worst in terms of absolute reward.Ac-tiveRL and ActivePLR, however, achieve both smaller relative drops in performance and higher absolute reward across all the different extreme weather scenarios.ActiveRL has only a 6.1% relative drop in reward while ActivePLR has only a 3.1% relative drop.These are promising results that indicate that these algorithms would still perform well if deployed in the real world after being trained in simulation.Furthermore, these algorithms result in agents that are more robust to the Sim2Real transfer than other methods.</p>
<p>We found that ActivePLR trains agents that are more robust to the Sim2Real transfer than ActiveRL, which is surprising since ActivePLR was not significantly different from ActiveRL in the extreme weather experiment.There might be some attractive local optimum in the HVAC control task in the low fidelity simulation that both ActivePLR and Ac-tiveRL fall into that is not present in the high fidelity simulation, resulting insimilar performance in the experiments from Section 4 but better Sim2Real transfer.In addition, the recorded value loss that RPLR and ActivePLR use is likely a less noisy signal of environment curriculum value than the uncertainty over the value estimate that ActiveRL uses.The value loss is obtained by actually collecting data while the value uncertainty is estimated using only the model weights through Monte Carlo Dropout.</p>
<p>Conclusion and Limitations</p>
<p>We explored the utility of a novel uncertainty-driven, gradient based algorithm called ActivePLR for unsupervised environment design in the context of training RL building control agents that are robust to climate change.We found that incorporating uncertainty into UED through ActivePLR led to HVAC controllers that better optimized thermal comfort and energy usage, even in extreme weather scenarios that were never in the training distribution.Our experiments showed that other UED algorithms perform poorly when generating new environment configurations for weather patterns because they may output unrealistic weather patterns that do not help the RL agent perform well in more realistic weather scenarios.Furthermore, we showed that ActivePLR and its variant ActiveRL would have a much smaller degradation in performance when transferring from the simulated domain to the real world compared to other techniques, making them a practical option for training robust RL HVAC agents that are ready for real deployment.</p>
<p>This work has two primary limitations.The first is that we rely on simulations; this is a flaw that is shared by most work that focuses on UED as well as many works in building control, as access to real buildings is difficult to obtain.The second is that our method requires continuous environment configuration variables to conduct gradient ascent.Future work could explore how this could be mitigated by applying dequantization techniques (Das and Spanos 2022) that transform categorical variables into continuous variables.</p>
<p>A ActivePLR Hard Constraint Details</p>
<p>The hard constraints constrain the search space within some lower and upper limits specified by the user for ϕ using the extragradient (Korpelevich 1976) method.Suppose we have a lower bound constraint ϕ &gt; b for some b ∈ R k and an upper bound constraint ϕ &lt; a for some a ∈ R k .Then we can express the Lagrangian as
L(ϕ, s 0 , θ, λ, µ) = O(ϕ, s 0 , θ)+ i λ i (b i − ϕ i ) + i µ i (ϕ i − a i )(6)
Now we can express the extragradient update.First, define the joint variable ω = (ϕ, λ, µ).We omit s 0 and θ from ω and the parameters to L as they will be kept constant throughout the optimization process.Then, the extragradient optimization process can be described as:
F (ω) = [∇ ϕ L(ω), −∇ λ L(ω) − ∇ µ L(ω)] T(7)ω t+1/2 = P Ω <a href="8">ω t − ηF (ω t )</a>
ω t+1 = P Ω <a href="9">ω t − ηF (ω t+1/2 )</a>
where P Ω [•] is the projection onto the constraint set.</p>
<p>Algorithm 2: ActiveRL procedure ACTIVERL(θ, s 0 , N, T,
ϕ 0 ← ExtractPhi(s 0 ) for t=0 to T do for i=0 to N do ϕ ← ExtractPhi(s 0 ) O ← UncertaintyEstimate(f θ , s 0 ) − γ||ϕ − ϕ 0 || 2 ϕ ← ExtragradientUpdate(ϕ, O) s 0 = Concatenate([ϕ, s 0 ]) end for τ ← PPOCollectTrajectories(E ϕ ) θ ← PPOUpdate(τ, θ) end for Return θ end procedure</p>
<p>B How Sinergym Resets Weather Conditions</p>
<p>In order to simulate the outdoor weather, Sinergym takes as input a file that contains hourly measurements of each of the outdoor weather variables denoted with a '<em>' above, as well as several others.Originally, Sinergym added noise to the measured outdoor temperature through an Ornstein-Uhlenbeck (OU, Doob 1942) process, to help prevent overfitting the RL agent to the static weather pattern.We modified Sinergym so that it could add this noise to the other weather variables denoted with a '</em>' as well.An OU process has three parameters: σ, µ, and τ .If we have a noise vector x t , then
x t+1 = x t + ∆t * (−(x t − µ)/τ ) + σ * 2 τ Z(10)
where Z ∼ N ormal(0, 1).so σ controls the magnitude of the noise that is added, µ is the average value of the noise, and τ determines how quickly the noise reverts to the mean.Notably, if we have a recorded weather variable w t , then adding the noise results in Mean wt+xt = Mean wt + µ.For each of our 5 weather variables, we estimate realistic values for σ and τ by doing linear regression of the difference between that weather variable and its moving average.That is, if we assume our recorded weather variable w t was generated via adding noise generated through an OU process, then we can generate measurements x t = w t − MA(w t ), where MA is the moving average.By applying linear regression onto the generated x t 's, we can estimate values of σ and τ that will generate weather with a similar amount of noise to real weather conditions.A detailed description of this linear regression process is detailed in Appendix C.</p>
<p>C Evaluating ActivePLR's Generalization to US Weather Conditions Experiment Setup</p>
<p>Since we handcrafted the extreme weather environments in Section 3, it is possible that these environments are unrealistic.In order to properly assess the viability of our RL HVAC controller in a range of different weather scenarios, we constructed a dataset of 120 randomly sampled, recorded weather patterns from across the US.We deployed the HVAC controller for ActivePLR and each baseline in a building that simulated each of those 120 weather patterns.</p>
<p>To construct the dataset of 120 weather patterns, we first scraped the EnergyPlus weather data website to get recorded weather patterns from across the US. 19These were Typical Meteorological Year (TMY) weather patterns (Wilcox and Marion 2008) from NREL, which contain hourly meteorological information from specific weather stations over the course of 1 year (8760 hours). 20This meteorological information is specially collated from multiple historical recordings of the weather data in that location to present the range of weather phenomena that typically occur there, while still keeping to annual averages that are consistent with long term averages for that location.TMY weather data is used often for building simulations.</p>
<p>After we obtained a dataset of historical weather data recordings, we converted them into a realistic dataset of environment configuration parameters ϕ.We modeled each weather variable in each weather pattern as a variation generated by an OU process(Equation ( 10)) from the corresponding weather variable in the base environment configuration ϕ 0 .Formally, let us suppose we have some recorded weather variable y ∈ R 8760 , corresponding to the value of that weather variable for each hour in a year.We also have a recording corresponding to the base environment ϕ 0 ,   ActivePLR 17 ρ = 0.1, β = 0.1, N P LR = 100 y 0 ∈ R 8760 .Since Sinergym takes the parameters of an OU process as its environment configuration, we model the difference x t = y t − y 0 t as having been generated from an OU process, like in Equation (10).We rearrange the terms in Equation (10) as: Finally, we can estimate
x t+1 = (1 − ∆t τ )x t + µ∆t τ + σ * 2 τ Z(τ = ∆t 1−m , µ = bτ ∆t , σ = √ V ar(E) √ 2 τ .
We then repeat this process for each of the 120 US weather patterns, for each of the 5 weather variables that compose the environment configuration: outdoor humidity, air temperature, wind speed, wind direction, and solar irradiance.Thus we have a dataset X ∈ R 120×5×3 of environment configurations that Sinergym can take in and simulate. 2122 Our hypothesis is that by conducting an uncertaintydriven environment exploration that is constrained to realistic environments, ActivePLR will be able to generalize to different weather patterns across the US better than the baseline methods.In particular, our hypothesis for DR and RPLR is that they will end up training the RL algorithm to focus on performing well in unrealistic environments, and cause performance to degrade on this set of more realistic environments.</p>
<p>ActivePLR Generalizes to US Weather Conditions</p>
<p>Although the ActivePLR agent seems to perform well in the handcrafted extreme weather conditions from Section 4, it is possible that these environments are unrealistic.In order to properly assess the viability of our RL HVAC controller in a range of different weather scenarios, we deployed the HVAC controller for ActivePLR and each baseline in a building that simulated each of those 120 different weather patterns sampled from across the US.</p>
<p>On all 120 environments, ActiveRL achieves a higher reward than every baseline, showing that our uncertaintydriven UED approach can train an RL HVAC agent that is more robust to realistic extreme weather patterns than any of our baselines.Since ActiveRL outperforms all the baselines on every environment, we visualize how much better the reward achieved by ActiveRL is relative to each baseline in each of the 120 different weather patterns in Figure 6.The environment configuration that can be provided to Sinergym, as ϕ only contains the offset parameters µ.Interestingly, there is a very small improvement using Ac-tiveRL over ActivePLR; this, combined with the similar performance between ActiveRL and ActivePLR from Section 4 suggests that ActiveRL and ActivePLR have very similar behavior.One possible reason is that there may be some attractive local (or global) optimum that both algorithms fall into, resulting in them appearing to have similar performance.Another possible reason is that since PLR tries to sample environment configurations that result in high value loss, and ActiveRL tries to sample environment configurations that result in high value uncertainty, PLR and ActiveRL actually optimize for very similar objectives.Thus ActiveRL and Ac-tivePLR end up having similar behavior, at least with respect to their responses to weather.One advantage that ActiveRL has in optimizing for uncertainty rather than value loss is that it can be used to identify novel environments to learn from rather than having to sample from old ones, or worry about the staleness of the value loss estimates of the environments in the replay buffer.</p>
<p>There is a significant difference between both DR and RPLR, and vanilla RL.Both UED methods seem to perform poorly compared to vanilla RL.This seems to indicate that randomly sampling environment configuration parameters results in environments that are very unrealistic, causing poor generalization performance compared to the vanilla RL algorithm or ActiveRL.Although RPLR has the ability to control what environments in its replay buffer are sampled, its replay buffer is still populated through the same uniform random sampling process as used in DR, resulting in training on unrealistic environments.</p>
<p>D Ablations Exploring Components of ActivePLR</p>
<p>In order to further understand the driving factors behind the performance of ActivePLR, we conducted two ablation experiments.First, we explored the impact of the γ soft constraint term.Second, we explored the impact of the learning rate η on the performance of the algorithm.To better isolate the impact of each parameter without the added complexity of the PLR replay buffer, we conduct these ablation experiments on ActiveRL (which is just ActivePLR without the replay buffer).</p>
<p>Constraints</p>
<p>First, we explore the necessity of constraining ActiveRL from generating environment configurations ϕ that are too far away from ϕ 0 .γ shows up in Equation ( 4), as a coefficient that regulates how much the distance of the generated environment configuration ϕ from the base environment ϕ 0 contributes to the objective function of ActiveRL.As γ increases, the algorithm is encouraged to generate values of ϕ that are closer to ϕ 0 .We varied γ between four different values {0, 0.005, 0.05, 0.5} while keeping the other hyperparameters the same as our other experiments to better understand how the algorithm performs under different constraint strengths.</p>
<p>Our hypothesis with this experiment was that there would be a tradeoff between performance in the extreme environments, and performance in the base environment ϕ 0 that would be modulated by γ.</p>
<p>Learning Rate</p>
<p>The learning rate η determines the step-size used by the Adam optimizer when ActiveRL is conducting gradient ascent on ϕ.The smaller η is, the more fine-grained the search for an uncertain environment configuration becomes.We mainly explore this hyperparameter to assess how sensitive ActiveRL is to the user's choice of η; if there are many values of η that yield optimal performance, then ActiveRL becomes much easier to use for other problems.We varied η between five different values {0.0001, 0.001, 0.01, 0.1, 1.0} while keeping the other hyperparameters the same as our other experiments.</p>
<p>Our hypothesis for this experiment was that there would be some optimal value of η that yielded the best performance by striking the perfect balance between being large enough to avoid local minima, and being small enough to actually converge.</p>
<p>Results of Exploring Components of ActiveRL</p>
<p>In order to assess what factors contribute to the performance of ActiveRL, we ran some ablation experiments that show how ActiveRL changes as certain hyperparameters change.We look at the γ hyperparameter that controls the strength of the soft constraint on ActiveRL's environment design process, and the η parameter that controls the step-size of the optimization procedure used to generate new environments.The results of changing these two parameters can be seen in Figure 7, where panel A corresponds to testing different values of γ and panel B corresponds to different values of η.</p>
<p>Contrary to our original hypothesis that there would be some tradeoff between realism and robustness modulated by γ, we actually found that having a relatively high value of γ contributes to good performance in both the base environment and the extreme environments.There was a clear pattern that larger values of γ correlated well with better performance.This may be because ActiveRL will generate more unrealistic environment configurations ϕ with weaker regularization that are not similar enough to ϕ 0 and the extreme environments to aid performance in those settings.</p>
<p>We found that smaller values of η helped performance across all environments, but decreasing it below 0.001 did not change the agent's learning trajectory at all.It is possible that having a large η results in an unstable gradient ascent process which is unable to successfully find a ϕ that maximizes the agent's uncertainty.</p>
<p>Figure 2 :
2
Figure 2: The flow of data during ActivePLR training.</p>
<p>Figure 3 :
3
Figure 3: A. The ActivePLR environment generation process.B. The overall ActivePLR environment sampling process.</p>
<dl>
<dt>Figure 4 :</dt>
<dt>4</dt>
<dt>Figure 4: Performance of each algorithm on training an RL HVAC agent in Sinergym, tested on various weather patterns.ActiveRL and ActivePLR outperform all baselines.We report the standard error of the mean over 5 trials for each result.Each ActiveRL and ActivePLR trial took 12 hours to train and evaluate.A. Average reward of each algorithm on the base environment ϕ 0 throughout training.B. Average reward achieved by each algorithm averaged over all 6 environments throughout training.C.The average drop in reward over all environments when evaluating in the higher fidelity simulation compared to evaluation in the lower fidelity simulation.Lower is better here.Note that none of the algorithms were trained on the specific extreme weather environments.Evaluating ActiveRL and ActivePLR on the higher fidelity simulation took about 1 hour per trial.</dt>
<dd>
<p>policy parameters ▷ s 0 : initial state to seed environment generation ▷ T : number of iterations to run PPO ▷ N : number of iterations to optimize ϕ ▷ η : Learning rate for optimizing ϕ ▷ γ : Weight on soft constraint ▷ a : ϕ lower bounds ▷ b : ϕ upper bounds</p>
</dd>
</dl>
<p>Figure 5 :
5
Figure 5: Performance of each algorithm on training an RL HVAC agent, tested on various weather patterns.ActiveRL and ActivePLR outperform all baselines.We report the standard error of the mean over 5 trials for each result.A. Average reward achieved by each algorithm on the base environment ϕ 0 throughout training.B. Average reward achieved by each algorithm averaged over all 6 environments throughout training.C. Average reward achieved by each algorithm in each of the 5 extreme weather environments throughout training.Note that none of the algorithms were trained on these specific extreme weather environments.</p>
<p>ActiveRL γ = 0.5, η = 0.01, p dropout = 0.1, N = 91, C = 10 PLR ρ = 0.045, β = 0.0015, N P LR = 10</p>
<p>11) x t+1 = mx t + b + E (12) where m = (1 − ∆t τ ), b = µ∆t τ , and E = σ * 2 τ Z.We can then run linear regression to find what parameters m and b estimate x t+1 from x while minimizing the error term E. Once we have estimated m and b with linear regression, we can compute the residual error E = x t+1 −mx t −b and compute the standard deviation of E as an estimate for σ * 2 τ .</p>
<p>Figure 6 :
6
Figure6: Improvement in reward achieved by using ActivePLR instead of each baseline, on 120 randomly sampled weather patterns from across the US.A higher number here indicates that ActivePLR performs well in comparison to that baseline.On the other hand, a higher number indicates the baseline performs poorly.</p>
<p>Figure 7 :
7
Figure 7: Ablation A. We explore how the γ regularization parameter affects the performance of ActiveRL.Higher values of γ mean that ActiveRL is forced to propose training environment configurations ϕ that are close to the base environment ϕ 0 .The left graph shows the average reward obtained throughout training on the base environment ϕ 0 .The right graph shows the reward averaged over ϕ 0 and the 5 extreme weather environments.ActiveRL seems quite sensitive to γ. B. Similarly, we explore how the η learning rate parameter affects ActiveRL.η is the learning rate used by ActiveRL to conduct gradient ascent on ϕ.Higher values of η means a coarser-grained search for the ϕ that maximizes the agent's uncertainty.ActiveRL seems insensitive to η once it gets small enough (&lt; 0.1).</p>
<p>Table 1 :
1
Bounds for Sinergym environment configuration variables
Weather VariableLower BoundUpper BoundOutdoor Air Temperature (°C)−31.0560.7Outdoor Air Relative Humidity (%)3100Outdoor Wind Speed (m/s)0.023.1Outdoor Wind Direction (°)0360Direct Solar Radiation Rate (W)01033Table 2: Relevant hyperparametersAlgorithmHyperparameterslr= 0.00005, clip param= 0.3,PPO 16discount factor= 0.8, p dropout = 0.1, # ofinner SGD steps= 40</p>
<p>Table 3 :
3
Hyperparameter sweep ranges
AlgorithmHyperparameters included in Sweeplr∈ {0.0005, 0.00005, 0.000005},PPO 18clip param∈ {0.1, 0.2, 0.3}, discount factor∈ {0.8, 0.9, 0.99}, # of inner SGDsteps∈ 20, 30, 40ActiveRLγ ∈ {0, 0.0005, 0.005, 0.05, 0.5}, η ∈ [e −10 , 1], p dropout ∈ {0.1, 0.25, 0.5}, N ∈ [1, 100]PLRρ ∈ [e −8 , 1], β ∈ [e −8 , 1], N P LR ∈ {10, 50, 100, 200}γ ∈ {0, 0.0005, 0.005, 0.05, 0.5}, η ∈ [e −10 , 1],ActivePLRp dropout ∈ {0.1, 0.25, 0.5}, N ∈ [1, 100], ρ ∈ [e −8 , 1],β ∈ [e −8 , N P LR ∈ {10, 50, 100, 200}median improvement of ActiveRL over the RBC is 24%.Over vanilla RL, it is 5%.
"setpoints" are the numbers a human might input into their thermostat to tell the HVAC systems their desired temperature. "Setpoint control" is the problem of automatically determining these setpoints to optimize some objective.
Environments that can change behavior according to configuration parameters are often referred to as Procedural Content Generation environments(Risi and Togelius 2020) 
These environments are also known as Procedural Content Generation (PCG (Risi and Togelius 2020)) environments.
In our case, a "level" is just an environment configuration ϕ. We use the term "level" in describing PLR to be consistent with the original paper(Jiang, Grefenstette, and Rocktäschel 2021).
We also tried to use RPLR with our uncertainty-based approach, but we found it performs worse than ActivePLR
  6  We make the assumption that at least some of the environment configuration variables are continuous
Pseudocode for ActivePLR can be found in Algorithm 1, and an illustration can be found in Figure3.
This is equivalent to ActivePLR with PD assigning 100% probability to d = 0
A more detailed treatment of the constrained optimization problem can be found in Appendix A.
ASHRAE defines uncomfortable thermal conditions as at least 20% of occupants are predicted to be uncomfortable (PPD &gt; 20%)
https://energyplus.net/weather
e.g. temperature, humidity, wind, etc.
The final 3 dimension for X comes from the fact that we have 3 variables µ, σ, τ for the OU process for each weather variable.
Note that the environment configuration variables that go into ActiveRL, RPLR, or DR ϕ ∈ R 5 are a subset of the full R 5×3
AcknowledgementsThis research is funded by the Republic of Singapore's National Research Foundation through a grant to the Berkeley Education Alliance for Research in Singapore (BEARS) for the Singapore-Berkeley Building Efficiency and Sustainability in the Tropics (SinBerBEST) Program.BEARS has been established by the University of California, Berkeley as a center for intellectual excellence in research and education in Singapore.
Uncertainty-based offline reinforcement learning with diversified q-ensemble. G An, S Moon, J.-H Kim, H O Song, Advances in neural information processing systems. 202134</p>
<p>Standard 55-thermal environmental conditions for human occupancy. A Ansi, Ashrae , M , Amer. Soc. Heat, Refrigerat. Air Condition. Eng. 5112017. 1451992. Bouneffouf, D. 2016Computers</p>
<p>. G Brockman, Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.01540201610Openai gym. arXiv. arXiv preprint</p>
<p>Gnu-rl: A precocial reinforcement learning solution for building hvac control using a differentiable mpc policy. B Chen, Z Cai, M Bergés, Proceedings of the 6th ACM international conference on systems for energyefficient buildings, cities, and transportation. the 6th ACM international conference on systems for energyefficient buildings, cities, and transportation2019</p>
<p>Understanding domain randomization for sim-to-real transfer. X Chen, J Hu, C Jin, L Li, L Wang, arXiv:2110.032392021arXiv preprint</p>
<p>EnergyPlus: creating a new-generation building energy simulation program. D Cohn, L Atlas, R Ladner, D B Crawley, L K Lawrie, F C Winkelmann, W F Buhl, Y J Huang, C O Pedersen, R K Strand, R J Liesen, D E Fisher, M J Witte, Energy and buildings. 1541994. 2001Machine learning</p>
<p>H P Das, Y.-W Lin, U Agwan, L Spangher, A Devonport, Y Yang, J Drgona, A Chong, S Schiavon, C J Spanos, arXiv:2211.14889Machine Learning for Smart and Energy-Efficient Buildings. 2022arXiv preprint</p>
<p>Improved dequantization and normalization methods for tabular data pre-processing in smart buildings. H P Das, C J Spanos, Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation. the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation2022</p>
<p>Towards optimal HVAC control in non-stationary building environments combining active change detection and deep reinforcement learning. X Deng, Y Zhang, H Qi, Building and environment. 2111086802022</p>
<p>Emergent complexity and zero-shot transfer via unsupervised environment design. M Dennis, N Jaques, E Vinitsky, A Bayen, S Russell, A Critch, S Levine, Advances in neural information processing systems. 202033</p>
<p>The Brownian movement and stochastic equations. J L Doob, Annals of Mathematics. 1942</p>
<p>Calculation of thermal comfort: introduction of a basic comfort equation. P O Fanger, ASHRAE Trans, Part II. 731967</p>
<p>The Joint Role of Batch Size and Query Strategy in Active Learning-Based Prediction-A Case Study in the Heart Attack Domain. B Faria, D Perdigão, J Brás, L Macedo, Springer, Y Gal, Z Ghahramani, K Gong, J Yang, X Wang, C Jiang, Z Xiong, M Zhang, M Guo, R Lv, S Wang, S Zhang, Progress in Artificial Intelligence: 21st EPIA Conference on Artificial Intelligence, EPIA 2022. Pmlr, J Gallego-Posada, J Ramirez, Lisbon, Portugal2022. August 31-September 2, 2022. 2016. 2022Comprehensive review of modeling, structure. and integration techniques of smart buildings in the cyber-physical-social system</p>
<p>Adversarial poisoning attacks on reinforcement learning-driven energy pricing. S Gunn, D Jang, O Paradise, L Spangher, C J Spanos, Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation. the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation2022</p>
<p>The state-of-the-art of sensors and environmental monitoring technologies in buildings. H Hayat, T Griffiths, D Brennan, R P Lewis, M Barclay, C Weirman, B Philip, J R Searle, Sensors. 191736482019</p>
<p>Offline-online reinforcement learning for energy pricing in office demand response: lowering energy and data costs. D Jang, L Spangher, T Srivistava, M Khattar, U Agwan, S Nadarajah, C Spanos, M Dennis, J Parker-Holder, J Foerster, E Grefenstette, T Rocktäschel, Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation. the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation2021. 202134Advances in Neural Information Processing Systems</p>
<p>M Jiang, M Dennis, J Parker-Holder, A Lupu, H Küttler, E Grefenstette, T Rocktäschel, J Foerster, arXiv:2207.05219Grounding Aleatoric Uncertainty in Unsupervised Environment Design. 2022arXiv preprint</p>
<p>Sinergym: A Building Simulation and Control Framework for Training Reinforcement Learning Agents. M Jiang, E Grefenstette, T Rocktäschel, J Jiménez-Raboso, A Campoy-Nieves, A Manjavacas-Lucas, J Gómez-Romero, M Molina-Solana, D P Kingma, J Ba, arXiv:1412.6980Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation. X Kou, Y Du, F F Li, H Pulgar-Painemal, H Zandi, J Dong, M M Olama, the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and TransportationNew York, NY, USAAssociation for Computing Machinery2021. 2021. 2014. 197612arXiv preprintMatecon</p>
<p>Evaluating the Adaptability of Reinforcement Learning Based HVAC Control for Residential Houses. K R Kurte, J D Munk, O Kotevska, K Amasyali, R Smith, E Mckee, Y Du, B Cui, T Kuruganti, H Zandi, 2020Sustainability</p>
<p>Simple and scalable predictive uncertainty estimation using deep ensembles. B Lakshminarayanan, A Pritzel, C Blundell, 201730Advances in neural information processing systems</p>
<p>Transfer learning applied to reinforcement learning-based hvac control. P Lissa, M Schukat, E Barrett, SN Computer Science. 12020</p>
<p>EVOP design of experiments. D P Lynch, 2003CiteseerTechnical report</p>
<p>Active learning using conformal predictors: application to image classification. L E Makili, J A V Sánchez, S Dormido-Canto, Fusion Science and Technology. 6222012</p>
<p>Climate change 2021: the physical science basis. V Masson-Delmotte, P Zhai, A Pirani, S L Connors, C Péan, S Berger, N Caud, Y Chen, L Goldfarb, M Gomis, Contribution of working group I to the sixth assessment report of the intergovernmental panel on climate change. 2021</p>
<p>. E H Mathews, C P Botha, D C Arndt, A G Malan, </p>
<p>HVAC control strategies to enhance comfort and minimise energy usage. Energy and Buildings. 33</p>
<p>V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.5602Playing atari with deep reinforcement learning. 2013arXiv preprint</p>
<p>Deep reinforcement learning control for non-stationary building energy management. A Naug, M Quinones-Grueiro, G Biswas, Energy and Buildings. 2771125842022</p>
<p>Increasing generality in machine learning through procedural content generation. J Parker-Holder, M Jiang, M Dennis, M Samvelyan, J Foerster, E Grefenstette, T Rocktäschel, Pmlr, S Risi, J Togelius, International Conference on Machine Learning. 2022. 20202Evolving curricula with regret-based environment design</p>
<p>Experimental Results of a Disturbance Compensating Q-learning Controller for HVAC Systems. S A A Rizvi, A J Pertzborn, 2022 American Control Conference (ACC). IEEE2022</p>
<p>Mastering the game of go without human knowledge. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, arXiv:1707.06347Proximal policy optimization algorithms. 2017. 2017550arXiv preprint</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). 2017. 2020. 2020Table 18. Energy-Related Carbon Dioxide Emissions by Sector and Source</p>
<p>How to pick the domain randomization parameters for sim-to-real transfer of reinforcement learning policies?. Q Vuong, S Vikram, H Su, S Gao, H I Christensen, P Wang, N C Bouaynaya, L Mihaylova, J Wang, Q Zhang, R He, arXiv:1903.117742020 International Joint Conference on Neural Networks (IJCNN). 2019. 2020. 201042arXiv preprintPredictions of energy savings in HVAC systems by lumped models</p>
<p>Users manual for tmy3 data sets (revised). S Wilcox, W Marion, 2008Golden, CO (United StatesNational Renewable Energy Lab.(NREL)Technical report</p>
<p>Y Wu, S Zhai, N Srivastava, J Susskind, J Zhang, R Salakhutdinov, H Goh, arXiv:2105.08140Uncertainty weighted actor-critic for offline reinforcement learning. 2021arXiv preprint</p>
<p>One for many: Transfer learning for building hvac control. S Xu, Y Wang, Y Wang, Z O'neill, Q Zhu, Proceedings of the 7th ACM international conference on systems for energy-efficient buildings, cities, and transportation. the 7th ACM international conference on systems for energy-efficient buildings, cities, and transportation2020</p>
<p>C Zhang, O Vinyals, R Munos, S Bengio, arXiv:1804.06893A study on overfitting in deep reinforcement learning. 2018arXiv preprint</p>
<p>Grid-interactive multi-zone building control using reinforcement learning with global-local policy search. X Zhang, R Chintala, A Bernstein, P Graf, Jin , X , 2021 American Control Conference (ACC). IEEE2021</p>            </div>
        </div>

    </div>
</body>
</html>
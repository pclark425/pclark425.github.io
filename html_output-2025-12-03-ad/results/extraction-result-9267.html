<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9267 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9267</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9267</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-255942578</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2301.07069v2.pdf" target="_blank">Prompting Large Language Model for Machine Translation: A Case Study</a></p>
                <p><strong>Paper Abstract:</strong> Research on prompting has shown excellent performance with little or even no supervised training across many tasks. However, prompting for machine translation is still under-explored in the literature. We fill this gap by offering a systematic study on prompting strategies for translation, examining various factors for prompt template and demonstration example selection. We further explore the use of monolingual data and the feasibility of cross-lingual, cross-domain, and sentence-to-document transfer learning in prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the testbed show that 1) the number and the quality of prompt examples matter, where using suboptimal examples degenerates translation; 2) several features of prompt examples, such as semantic similarity, show significant Spearman correlation with their prompting performance; yet, none of the correlations are strong enough; 3) using pseudo parallel prompt examples constructed from monolingual data via zero-shot prompting could improve translation; and 4) improved performance is achievable by transferring knowledge from prompt examples selected in other settings. We finally provide an analysis on the model outputs and discuss several problems that prompting still suffers from.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9267.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9267.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot vs Few-shot MT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot and Few-shot Prompting for Machine Translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of zero-shot prompting (only instruction + input) and few-shot prompting (instruction + concatenated labeled examples) for MT using GLM-130B across FLORES/Wiki/WMT/Multi-Domain/PDC benchmarks, showing few-shot often improves average performance but with high variance and some settings where 1-shot can hurt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (FLORES/Wiki/WMT/Multi-Domain/PDC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate source sentences into target language; experiments on sentence- and document-level translation across En, De, Zh using standard MT benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot: template '[src]: X [tgt]:' (no examples). Few-shot: concatenate K labeled demonstration examples in format '[ptgt]: Y1 [psrc]: X1 [ptgt]: Y2 ... [src]: X [tgt]:' with K in {1,5,10,20}; beam search (beam size=2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot vs few-shot with varying K (1,5,10,20); random-sampled demonstrations to estimate variance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Wiki zero-shot BLEU: 24.08, COMET: 33.92; Wiki 1-shot (random) BLEU: 26.31, COMET: 48.29; Wiki 5-shot (random) BLEU: 27.46, COMET: 51.11 (values averaged across language pairs as reported in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Relative to zero-shot, 1-shot random: +2.23 BLEU, +14.37 COMET (Wiki); 5-shot random: +3.38 BLEU, +17.19 COMET (Wiki). WMT zero-shot BLEU: 20.38, COMET: 17.97; 1-shot random (WMT) BLEU: 21.27, COMET: 30.70 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+2–3 BLEU and large COMET gains (~+14 to +17) going from zero-shot to few-shot on average (variations across datasets/language pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>More examples provide stronger signals about mapping between source and target and prompt format, improving generation; however performance has high variance across specific demonstrations and 1-shot can sometimes underperform zero-shot, indicating sensitivity to example choice and that additional examples do not guarantee monotonic gains.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>GLM-130B INT4 quantized, beam size=2, experiments on Wiki Ablation/Full, WMT and Multi-Domain; K varied {1,5,10,20}; random sampling (100 draws) used to show distribution; evaluation metrics BLEU (SacreBLEU) and COMET (wmt20-comet-da). Increased K increases GPU memory and inference time per token (Figure 3).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9267.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9267.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt template wording & language</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Prompt Template Wording and Prompt Language on MT Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic comparison of multiple manual prompt templates and the language used for the template (English, German, Chinese), showing that small surface changes and template language substantially affect MT output quality; an English template in a simple form performed best overall for GLM-130B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (zero-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot translation where the test input X is wrapped with a textual template/instruction before generation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Six template variants were tested (labeled A–F), examples include A: '[src]: [input] [tgt]:' (simple tag-based), B: '[input] [tgt]:' (no source tag), C: '[input] Translate to [tgt]:', D: '[input] Translate from [src] to [tgt]:', and variants with/without line breaks. Templates were instantiated in English, German and Chinese prompt languages.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared 6 templates and three template languages (English/German/Chinese) across 6 language pairs (En↔De, En↔Zh, De↔Zh) in Wiki Ablation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Template A in English achieved the best overall performance (reported as best by BLEU and COMET in Table 10); German-language templates often substantially degraded performance; Chinese templates improved performance when translating into Chinese.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>German templates 'degenerate translation substantially' relative to English templates (large negative COMET differences in Table 1); Chinese template yields improved performance when target is Chinese but not overall best.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large negative effects when using German template on GLM-130B for many pairs; positive gains for Chinese template when target=Chinese; exact effect sizes vary by pair (see Table 1/Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The surface form/language of the template changes how the LLM parses the task: GLM-130B shows preference for English templates, likely because English is more represented/used as task-language in pretraining/usage and cross-lingual ability varies by language; language-specific templates help only when the LLM has stronger capability in that language.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot evaluation on Wiki Ablation sets; templates tested with and without line breaks; evaluations reported in BLEU and COMET. The paper recommends simple English tag-style template '[src]: X [tgt]:' as default for GLM-130B.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9267.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9267.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Example-selection features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Features for Selecting Prompt Demonstrations (LMScore, SemScore, TLength, ...)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigation of seven features of candidate examples (LMScore, MTScore, SemScore, CaseSemScore-Src/Tgt, SLength, TLength) to predict 1-shot prompting performance and design selection strategies based on them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (1-shot example selection)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select a single demonstration example for in-context learning from a candidate pool to maximize MT quality on a test instance; evaluated across Wiki Ablation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>1-shot prompting; features computed per demonstration: LMScore (GLM log-likelihood normalized by length), MTScore (COMET QE), SemScore (cosine similarity on LASER2 embeddings), CaseSemScore-Src/Tgt (similarity to test input), SLength/TLength (source/target token counts).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random selection baseline vs top-ranked selection by SemScore, LMScore, TLength, and combined strategies (different strategies for high- vs low-quality pools).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On Wiki (1-shot): Random BLEU 26.31 COMET 48.29; SemScore selection BLEU 26.73 COMET 49.34; LMScore BLEU 26.48 COMET 47.92; TLength BLEU 26.54 COMET 48.73 (Table 3 averaged across directions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>SemScore vs Random: +0.42 BLEU, +1.05 COMET (1-shot, Wiki); effects modest and inconsistent across pairs/datasets—SemScore often best across settings but not uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Small average gains (sub-1 to few points BLEU; around +1 COMET) when selecting by SemScore/LMScore/TLength versus random, but improvement is not consistent across language pairs/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Features such as semantic similarity, LLM likelihood and sequence length correlate significantly but weakly with prompting performance (Spearman ρ often <0.5). Long examples may provide more signals about input-output spaces. High-quality example pools reduce variance and correlation strength can be weak/fragile, making selection non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>1-shot experiments sampled 600 candidate demonstrations per pool; high-quality pool = default pool, low-quality pool = WikiMatrix.v1; statistical correlations (Spearman) computed; selection excludes examples with <10 or >100 tokens; 3-demo experiments concatenated top-5 in ascending order for multi-shot; metrics BLEU and COMET.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9267.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9267.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Monolingual vs Pseudo-parallel demos</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using Monolingual Demonstrations vs Pseudo-parallel (forward/back-translation) for Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of using raw monolingual data (random source-target pairing, source-only, target-only) as demonstrations versus constructing pseudo-parallel demonstrations by translating monolingual data with GLM-130B (forward/back-translation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (few-shot prompting with monolingual/pseudo-parallel examples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Investigate whether unlabeled monolingual data can be used as demonstration examples for few-shot prompting, and whether generating pseudo-parallel examples via zero-shot back-/forward-translation helps.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Demonstration formats tested: (1) Random example pairs assembled from monolingual sources/targets; (2) source-only examples (source text only); (3) target-only examples; (4) pseudo-parallel created by zero-shot GLM translations of monolingual sentences (forward: translate source→target; back: translate target→source).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Monolingual (random/source-only/target-only) vs pseudo-parallel (forward/back-translation) vs real parallel examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Monolingual demonstration almost always hurts translation (performance degrades as more monolingual examples used). Pseudo-parallel examples produced by back-translation improve prompting and can approach performance of real parallel examples (Figure 4); back-translation more robust than forward-translation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Monolingual random examples produce the worst results; back-translated pseudo-parallel yields notable improvements over monolingual and often moves performance toward that of true parallel demonstrations (exact numeric deltas depend on setting; plotted in Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large negative effect for raw monolingual demonstrations (degradation increases with number of examples). Back-translation yields positive gains (approaching real parallel), forward-translation less robust.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>MT requires preserving authentic source-target mapping in demonstrations; random or label-mismatched monolingual pairs mislead the model. Pseudo-parallel examples restore mapping and thus are beneficial even if auto-generated and imperfect; back-translation (target-side monolingual translated into source) is more effective likely because target-language monolinguals better represent the target distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>For monolingual experiments, 50 demonstrations sampled and averaged; pseudo-parallel created via GLM-130B zero-shot translation; evaluated on Wiki Ablation sets; metrics COMET (primary) and BLEU (appendix).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9267.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9267.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cross-setting transfer of demos</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transferability of Prompt Demonstrations Across Language Pairs, Domains, and Granularity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study of whether demonstrations selected in one setting (language pair, domain, sentence-level) transfer to others, measuring correlation of performance rankings and actual gains over zero-shot when using out-of-setting demos.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (transfer of demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate 1-shot prompting when demonstrations are selected from different source settings (other language pairs, domains, or sentence-level pool applied to document-level translation), using correlation (Spearman) of demo rankings and relative performance over zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>1-shot prompting; demonstrations sampled from source setting S1 and applied in target setting S2. Transfer settings include source-shared, target-shared, reversed direction, Wiki→WMT, Wiki→Multi-Domain, sentence→document.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>In-setting demonstrations (baseline) vs out-of-setting demonstrations (cross-lingual, cross-domain, cross-granularity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Using out-of-setting demonstrations can improve translation over zero-shot: target-shared and reversed settings easier to transfer; Table 4/5 report positive average COMET/BLEU gains in many transfer cells (e.g., target-shared ∆Quality +9.67 COMET in one aggregation). Transfer to document-level (PDC Zh→En) improved d-BLEU and document metrics (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Although positive gains exist, Spearman correlations of demonstration ranking across settings are weak and often insignificant (Table 4/5), indicating the demonstration superiority in one setting does not generalize reliably to another.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Positive deltas over zero-shot in some transfer settings (COMET gains up to ~+11 in aggregated cells; exacts vary by pair/domain), but unstable and can be negative in other runs (Table 19).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Transfer is possible (especially when source/target language overlap or when reversing direction) because demonstrations provide general signals about format and label space, but the optimality of specific examples is setting-dependent; therefore setting-specific demonstrations are usually needed for best results.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>1-shot prompting; correlations computed by sampling 300 demonstrations for cross-lingual transfer and 200 for cross-domain; measured Spearman ρ and relative quality (∆ against zero-shot); document-level transfer used chunks of ~4 sentences with 3 demonstrations selected by SemScore/LMScore; metrics BLEU and COMET and document metrics (d-BLEU, TC/CP/PT/TCP).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9267.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9267.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt trap & error modes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Trap and Error Modes Caused by Prompt/Input Presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed failure modes when the input contains prompt-template-like phrasing: the model copies template phrases, emits off-target or empty outputs, produces code-switched or garbled text, mistranslates entities and dates, or hallucinates, showing that presentation of input can severely alter behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation (robustness to input formatting)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Investigate how mixing prompt template language into the input or unexpected formatting leads to erroneous translations or model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Inputs that contain phrases identical or similar to the prompt template (e.g., 'Translate from English to Chinese:' inside source text) or other formatting that resembles prompt tokens; test both zero-shot and few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Normal input formatting vs input containing prompt-template-like phrases / adversarially crafted formats.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative examples in Table 7 show copying of prompt phrases instead of translating them; model output can be empty, off-target language, or contain garbled characters; no single numeric metric given but errors are frequent and problematic for Chinese targets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>When the input looks like a prompt, the model can confuse prompt-level and content-level instructions causing it to reproduce prompt text verbatim or to 'translate the prompt' rather than the intended content; this creates a security/robustness issue ('prompt trap').</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Case studies provided (Table 7) showing translations that copy templates or mistranslate dates/entities; noted especially when translating into Chinese (model outputs traditional Chinese and messy codes).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9267.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9267.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct vs Pivoting translation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct Non-English Translation vs Pivoting through English</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of direct non-English-to-non-English translation by prompting versus pivoting (source→English→target) showing pivoting via English substantially improves performance for language pairs where GLM has weaker direct cross-lingual ability (e.g., De↔Zh).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Non-English-to-non-English Machine Translation (De↔Zh)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate directly between German and Chinese versus translating German→English then English→Chinese (pivot) using the same LLM prompting setup.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Direct prompting for De↔Zh vs two-step pivoting prompting (source→English, then English→target) using 1-shot demonstrations (3 examples sampled for 1-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct translation prompt vs pivoted two-step prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On Wiki Full sets aggregated: Direct (0-shot) COMET for De→Zh = 2.80, Pivoting COMET = 19.23; in 1-shot settings pivoting also improved COMET substantially (e.g., direct 47.23 vs pivoting 48.25 for some 1-shot cells—see Table 8). For BLEU, pivoting often increased BLEU substantially (Table 18 shows BLEU improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Pivoting often yields large absolute gains for low-performing direct translation directions (e.g., De→Zh zero-shot COMET from 2.80 to 19.23 in one report), indicating pivoting is a practical workaround when direct non-English cross-lingual ability is weak.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Very large improvements in some cases (e.g., COMET +~16.4 for De→Zh zero-shot pivoting in Table 8); exact improvements vary by direction and shot-level.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>GLM-130B's cross-lingual ability centers around English, so translating via English leverages stronger source↔English and English↔target capabilities; suggests pretraining/finetuning regime lacks non-English-centric cross-lingual learning.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Pivoting evaluated on Wiki Full sets and Ablation sets; 1-shot used random sampling of 3 demos; metrics COMET and BLEU reported; experiments demonstrated both zero-shot and few-shot pivot benefits.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>Prompting palm for translation: Assessing strategies and performance <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 1)</em></li>
                <li>An information-theoretic approach to prompt engineering without ground truth labels <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9267",
    "paper_id": "paper-255942578",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Zero-shot vs Few-shot MT prompting",
            "name_full": "Zero-shot and Few-shot Prompting for Machine Translation",
            "brief_description": "Comparison of zero-shot prompting (only instruction + input) and few-shot prompting (instruction + concatenated labeled examples) for MT using GLM-130B across FLORES/Wiki/WMT/Multi-Domain/PDC benchmarks, showing few-shot often improves average performance but with high variance and some settings where 1-shot can hurt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GLM-130B",
            "model_size": "130B",
            "task_name": "Machine Translation (FLORES/Wiki/WMT/Multi-Domain/PDC)",
            "task_description": "Translate source sentences into target language; experiments on sentence- and document-level translation across En, De, Zh using standard MT benchmarks.",
            "presentation_format": "Zero-shot: template '[src]: X [tgt]:' (no examples). Few-shot: concatenate K labeled demonstration examples in format '[ptgt]: Y1 [psrc]: X1 [ptgt]: Y2 ... [src]: X [tgt]:' with K in {1,5,10,20}; beam search (beam size=2).",
            "comparison_format": "Zero-shot vs few-shot with varying K (1,5,10,20); random-sampled demonstrations to estimate variance.",
            "performance": "Wiki zero-shot BLEU: 24.08, COMET: 33.92; Wiki 1-shot (random) BLEU: 26.31, COMET: 48.29; Wiki 5-shot (random) BLEU: 27.46, COMET: 51.11 (values averaged across language pairs as reported in Table 3).",
            "performance_comparison": "Relative to zero-shot, 1-shot random: +2.23 BLEU, +14.37 COMET (Wiki); 5-shot random: +3.38 BLEU, +17.19 COMET (Wiki). WMT zero-shot BLEU: 20.38, COMET: 17.97; 1-shot random (WMT) BLEU: 21.27, COMET: 30.70 (Table 3).",
            "format_effect_size": "+2–3 BLEU and large COMET gains (~+14 to +17) going from zero-shot to few-shot on average (variations across datasets/language pairs).",
            "explanation_or_hypothesis": "More examples provide stronger signals about mapping between source and target and prompt format, improving generation; however performance has high variance across specific demonstrations and 1-shot can sometimes underperform zero-shot, indicating sensitivity to example choice and that additional examples do not guarantee monotonic gains.",
            "null_or_negative_result": true,
            "experimental_details": "GLM-130B INT4 quantized, beam size=2, experiments on Wiki Ablation/Full, WMT and Multi-Domain; K varied {1,5,10,20}; random sampling (100 draws) used to show distribution; evaluation metrics BLEU (SacreBLEU) and COMET (wmt20-comet-da). Increased K increases GPU memory and inference time per token (Figure 3).",
            "uuid": "e9267.0"
        },
        {
            "name_short": "Prompt template wording & language",
            "name_full": "Effect of Prompt Template Wording and Prompt Language on MT Prompting",
            "brief_description": "Systematic comparison of multiple manual prompt templates and the language used for the template (English, German, Chinese), showing that small surface changes and template language substantially affect MT output quality; an English template in a simple form performed best overall for GLM-130B.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GLM-130B",
            "model_size": "130B",
            "task_name": "Machine Translation (zero-shot prompting)",
            "task_description": "Zero-shot translation where the test input X is wrapped with a textual template/instruction before generation.",
            "presentation_format": "Six template variants were tested (labeled A–F), examples include A: '[src]: [input] [tgt]:' (simple tag-based), B: '[input] [tgt]:' (no source tag), C: '[input] Translate to [tgt]:', D: '[input] Translate from [src] to [tgt]:', and variants with/without line breaks. Templates were instantiated in English, German and Chinese prompt languages.",
            "comparison_format": "Compared 6 templates and three template languages (English/German/Chinese) across 6 language pairs (En↔De, En↔Zh, De↔Zh) in Wiki Ablation sets.",
            "performance": "Template A in English achieved the best overall performance (reported as best by BLEU and COMET in Table 10); German-language templates often substantially degraded performance; Chinese templates improved performance when translating into Chinese.",
            "performance_comparison": "German templates 'degenerate translation substantially' relative to English templates (large negative COMET differences in Table 1); Chinese template yields improved performance when target is Chinese but not overall best.",
            "format_effect_size": "Large negative effects when using German template on GLM-130B for many pairs; positive gains for Chinese template when target=Chinese; exact effect sizes vary by pair (see Table 1/Table 10).",
            "explanation_or_hypothesis": "The surface form/language of the template changes how the LLM parses the task: GLM-130B shows preference for English templates, likely because English is more represented/used as task-language in pretraining/usage and cross-lingual ability varies by language; language-specific templates help only when the LLM has stronger capability in that language.",
            "null_or_negative_result": true,
            "experimental_details": "Zero-shot evaluation on Wiki Ablation sets; templates tested with and without line breaks; evaluations reported in BLEU and COMET. The paper recommends simple English tag-style template '[src]: X [tgt]:' as default for GLM-130B.",
            "uuid": "e9267.1"
        },
        {
            "name_short": "Example-selection features",
            "name_full": "Features for Selecting Prompt Demonstrations (LMScore, SemScore, TLength, ...)",
            "brief_description": "Investigation of seven features of candidate examples (LMScore, MTScore, SemScore, CaseSemScore-Src/Tgt, SLength, TLength) to predict 1-shot prompting performance and design selection strategies based on them.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GLM-130B",
            "model_size": "130B",
            "task_name": "Machine Translation (1-shot example selection)",
            "task_description": "Select a single demonstration example for in-context learning from a candidate pool to maximize MT quality on a test instance; evaluated across Wiki Ablation sets.",
            "presentation_format": "1-shot prompting; features computed per demonstration: LMScore (GLM log-likelihood normalized by length), MTScore (COMET QE), SemScore (cosine similarity on LASER2 embeddings), CaseSemScore-Src/Tgt (similarity to test input), SLength/TLength (source/target token counts).",
            "comparison_format": "Random selection baseline vs top-ranked selection by SemScore, LMScore, TLength, and combined strategies (different strategies for high- vs low-quality pools).",
            "performance": "On Wiki (1-shot): Random BLEU 26.31 COMET 48.29; SemScore selection BLEU 26.73 COMET 49.34; LMScore BLEU 26.48 COMET 47.92; TLength BLEU 26.54 COMET 48.73 (Table 3 averaged across directions).",
            "performance_comparison": "SemScore vs Random: +0.42 BLEU, +1.05 COMET (1-shot, Wiki); effects modest and inconsistent across pairs/datasets—SemScore often best across settings but not uniformly.",
            "format_effect_size": "Small average gains (sub-1 to few points BLEU; around +1 COMET) when selecting by SemScore/LMScore/TLength versus random, but improvement is not consistent across language pairs/datasets.",
            "explanation_or_hypothesis": "Features such as semantic similarity, LLM likelihood and sequence length correlate significantly but weakly with prompting performance (Spearman ρ often &lt;0.5). Long examples may provide more signals about input-output spaces. High-quality example pools reduce variance and correlation strength can be weak/fragile, making selection non-trivial.",
            "null_or_negative_result": true,
            "experimental_details": "1-shot experiments sampled 600 candidate demonstrations per pool; high-quality pool = default pool, low-quality pool = WikiMatrix.v1; statistical correlations (Spearman) computed; selection excludes examples with &lt;10 or &gt;100 tokens; 3-demo experiments concatenated top-5 in ascending order for multi-shot; metrics BLEU and COMET.",
            "uuid": "e9267.2"
        },
        {
            "name_short": "Monolingual vs Pseudo-parallel demos",
            "name_full": "Using Monolingual Demonstrations vs Pseudo-parallel (forward/back-translation) for Prompting",
            "brief_description": "Evaluation of using raw monolingual data (random source-target pairing, source-only, target-only) as demonstrations versus constructing pseudo-parallel demonstrations by translating monolingual data with GLM-130B (forward/back-translation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GLM-130B",
            "model_size": "130B",
            "task_name": "Machine Translation (few-shot prompting with monolingual/pseudo-parallel examples)",
            "task_description": "Investigate whether unlabeled monolingual data can be used as demonstration examples for few-shot prompting, and whether generating pseudo-parallel examples via zero-shot back-/forward-translation helps.",
            "presentation_format": "Demonstration formats tested: (1) Random example pairs assembled from monolingual sources/targets; (2) source-only examples (source text only); (3) target-only examples; (4) pseudo-parallel created by zero-shot GLM translations of monolingual sentences (forward: translate source→target; back: translate target→source).",
            "comparison_format": "Monolingual (random/source-only/target-only) vs pseudo-parallel (forward/back-translation) vs real parallel examples.",
            "performance": "Monolingual demonstration almost always hurts translation (performance degrades as more monolingual examples used). Pseudo-parallel examples produced by back-translation improve prompting and can approach performance of real parallel examples (Figure 4); back-translation more robust than forward-translation.",
            "performance_comparison": "Monolingual random examples produce the worst results; back-translated pseudo-parallel yields notable improvements over monolingual and often moves performance toward that of true parallel demonstrations (exact numeric deltas depend on setting; plotted in Figure 4).",
            "format_effect_size": "Large negative effect for raw monolingual demonstrations (degradation increases with number of examples). Back-translation yields positive gains (approaching real parallel), forward-translation less robust.",
            "explanation_or_hypothesis": "MT requires preserving authentic source-target mapping in demonstrations; random or label-mismatched monolingual pairs mislead the model. Pseudo-parallel examples restore mapping and thus are beneficial even if auto-generated and imperfect; back-translation (target-side monolingual translated into source) is more effective likely because target-language monolinguals better represent the target distribution.",
            "null_or_negative_result": true,
            "experimental_details": "For monolingual experiments, 50 demonstrations sampled and averaged; pseudo-parallel created via GLM-130B zero-shot translation; evaluated on Wiki Ablation sets; metrics COMET (primary) and BLEU (appendix).",
            "uuid": "e9267.3"
        },
        {
            "name_short": "Cross-setting transfer of demos",
            "name_full": "Transferability of Prompt Demonstrations Across Language Pairs, Domains, and Granularity",
            "brief_description": "Study of whether demonstrations selected in one setting (language pair, domain, sentence-level) transfer to others, measuring correlation of performance rankings and actual gains over zero-shot when using out-of-setting demos.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GLM-130B",
            "model_size": "130B",
            "task_name": "Machine Translation (transfer of demonstrations)",
            "task_description": "Evaluate 1-shot prompting when demonstrations are selected from different source settings (other language pairs, domains, or sentence-level pool applied to document-level translation), using correlation (Spearman) of demo rankings and relative performance over zero-shot.",
            "presentation_format": "1-shot prompting; demonstrations sampled from source setting S1 and applied in target setting S2. Transfer settings include source-shared, target-shared, reversed direction, Wiki→WMT, Wiki→Multi-Domain, sentence→document.",
            "comparison_format": "In-setting demonstrations (baseline) vs out-of-setting demonstrations (cross-lingual, cross-domain, cross-granularity).",
            "performance": "Using out-of-setting demonstrations can improve translation over zero-shot: target-shared and reversed settings easier to transfer; Table 4/5 report positive average COMET/BLEU gains in many transfer cells (e.g., target-shared ∆Quality +9.67 COMET in one aggregation). Transfer to document-level (PDC Zh→En) improved d-BLEU and document metrics (Table 6).",
            "performance_comparison": "Although positive gains exist, Spearman correlations of demonstration ranking across settings are weak and often insignificant (Table 4/5), indicating the demonstration superiority in one setting does not generalize reliably to another.",
            "format_effect_size": "Positive deltas over zero-shot in some transfer settings (COMET gains up to ~+11 in aggregated cells; exacts vary by pair/domain), but unstable and can be negative in other runs (Table 19).",
            "explanation_or_hypothesis": "Transfer is possible (especially when source/target language overlap or when reversing direction) because demonstrations provide general signals about format and label space, but the optimality of specific examples is setting-dependent; therefore setting-specific demonstrations are usually needed for best results.",
            "null_or_negative_result": true,
            "experimental_details": "1-shot prompting; correlations computed by sampling 300 demonstrations for cross-lingual transfer and 200 for cross-domain; measured Spearman ρ and relative quality (∆ against zero-shot); document-level transfer used chunks of ~4 sentences with 3 demonstrations selected by SemScore/LMScore; metrics BLEU and COMET and document metrics (d-BLEU, TC/CP/PT/TCP).",
            "uuid": "e9267.4"
        },
        {
            "name_short": "Prompt trap & error modes",
            "name_full": "Prompt Trap and Error Modes Caused by Prompt/Input Presentation",
            "brief_description": "Observed failure modes when the input contains prompt-template-like phrasing: the model copies template phrases, emits off-target or empty outputs, produces code-switched or garbled text, mistranslates entities and dates, or hallucinates, showing that presentation of input can severely alter behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GLM-130B",
            "model_size": "130B",
            "task_name": "Machine Translation (robustness to input formatting)",
            "task_description": "Investigate how mixing prompt template language into the input or unexpected formatting leads to erroneous translations or model behavior.",
            "presentation_format": "Inputs that contain phrases identical or similar to the prompt template (e.g., 'Translate from English to Chinese:' inside source text) or other formatting that resembles prompt tokens; test both zero-shot and few-shot prompting.",
            "comparison_format": "Normal input formatting vs input containing prompt-template-like phrases / adversarially crafted formats.",
            "performance": "Qualitative examples in Table 7 show copying of prompt phrases instead of translating them; model output can be empty, off-target language, or contain garbled characters; no single numeric metric given but errors are frequent and problematic for Chinese targets.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "When the input looks like a prompt, the model can confuse prompt-level and content-level instructions causing it to reproduce prompt text verbatim or to 'translate the prompt' rather than the intended content; this creates a security/robustness issue ('prompt trap').",
            "null_or_negative_result": true,
            "experimental_details": "Case studies provided (Table 7) showing translations that copy templates or mistranslate dates/entities; noted especially when translating into Chinese (model outputs traditional Chinese and messy codes).",
            "uuid": "e9267.5"
        },
        {
            "name_short": "Direct vs Pivoting translation",
            "name_full": "Direct Non-English Translation vs Pivoting through English",
            "brief_description": "Comparison of direct non-English-to-non-English translation by prompting versus pivoting (source→English→target) showing pivoting via English substantially improves performance for language pairs where GLM has weaker direct cross-lingual ability (e.g., De↔Zh).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GLM-130B",
            "model_size": "130B",
            "task_name": "Non-English-to-non-English Machine Translation (De↔Zh)",
            "task_description": "Translate directly between German and Chinese versus translating German→English then English→Chinese (pivot) using the same LLM prompting setup.",
            "presentation_format": "Direct prompting for De↔Zh vs two-step pivoting prompting (source→English, then English→target) using 1-shot demonstrations (3 examples sampled for 1-shot).",
            "comparison_format": "Direct translation prompt vs pivoted two-step prompts.",
            "performance": "On Wiki Full sets aggregated: Direct (0-shot) COMET for De→Zh = 2.80, Pivoting COMET = 19.23; in 1-shot settings pivoting also improved COMET substantially (e.g., direct 47.23 vs pivoting 48.25 for some 1-shot cells—see Table 8). For BLEU, pivoting often increased BLEU substantially (Table 18 shows BLEU improvements).",
            "performance_comparison": "Pivoting often yields large absolute gains for low-performing direct translation directions (e.g., De→Zh zero-shot COMET from 2.80 to 19.23 in one report), indicating pivoting is a practical workaround when direct non-English cross-lingual ability is weak.",
            "format_effect_size": "Very large improvements in some cases (e.g., COMET +~16.4 for De→Zh zero-shot pivoting in Table 8); exact improvements vary by direction and shot-level.",
            "explanation_or_hypothesis": "GLM-130B's cross-lingual ability centers around English, so translating via English leverages stronger source↔English and English↔target capabilities; suggests pretraining/finetuning regime lacks non-English-centric cross-lingual learning.",
            "null_or_negative_result": false,
            "experimental_details": "Pivoting evaluated on Wiki Full sets and Ablation sets; 1-shot used random sampling of 3 demos; metrics COMET and BLEU reported; experiments demonstrated both zero-shot and few-shot pivot benefits.",
            "uuid": "e9267.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Prompting palm for translation: Assessing strategies and performance",
            "rating": 2,
            "sanitized_title": "prompting_palm_for_translation_assessing_strategies_and_performance"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 1,
            "sanitized_title": "calibrate_before_use_improving_fewshot_performance_of_language_models"
        },
        {
            "paper_title": "An information-theoretic approach to prompt engineering without ground truth labels",
            "rating": 1,
            "sanitized_title": "an_informationtheoretic_approach_to_prompt_engineering_without_ground_truth_labels"
        }
    ],
    "cost": 0.018302,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Prompting Large Language Model for Machine Translation: A Case Study</p>
<p>Biao Zhang b.zhang@ed.ac.uk 
School of Informatics
University of Edinburgh</p>
<p>Barry Haddow bhaddow@inf.ed.ac.uk 
School of Informatics
University of Edinburgh</p>
<p>Alexandra Birch a.birch@ed.ac.uk 
School of Informatics
University of Edinburgh</p>
<p>Prompting Large Language Model for Machine Translation: A Case Study</p>
<p>Research on prompting has shown excellent performance with little or even no supervised training across many tasks. However, prompting for machine translation is still underexplored in the literature. We fill this gap by offering a systematic study on prompting strategies for translation, examining various factors for prompt template and demonstration example selection. We further explore the use of monolingual data and the feasibility of cross-lingual, cross-domain, and sentence-todocument transfer learning in prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the testbed show that 1) the number and the quality of prompt examples matter, where using suboptimal examples degenerates translation; 2) several features of prompt examples, such as semantic similarity, show significant Spearman correlation with their prompting performance; yet, none of the correlations are strong enough; 3) using pseudo parallel prompt examples constructed from monolingual data via zero-shot prompting could improve translation; and 4) improved performance is achievable by transferring knowledge from prompt examples selected in other settings. We finally provide an analysis on the model outputs and discuss several problems that prompting still suffers from.</p>
<p>Introduction</p>
<p>Large language models (LLMs) pretrained on massive unlabeled corpora have shown impressive emergent abilities under model scaling which enable prompting for downstream applications (Brown et al., 2020;Kaplan et al., 2020;Wei et al., 2022b;Zhang et al., 2022a;Chowdhery et al., 2022). Different from task-specific finetuning, prompting constructs task-specific prompts by rephrasing test examples with descriptive task instructions and executes the task by feeding prompts to LLMs directly. It can be further enhanced through in-context learning by providing a few labeled examples (or prompt examples) as a demonstration (Brown et al., 2020). As a new paradigm, prompting LLMs has achieved state-of-the-art performance over a range of natural language processing (NLP) tasks (Chung et al., 2022;Goyal et al., 2022;Wei et al., 2022c;Chowdhery et al., 2022).</p>
<p>In this paper, we focus on prompting LLMs for machine translation (MT). MT represents a complex task requiring transforming a source input into its semantically equivalent target output in a different language, which combines sequence understanding and generation. It offers a unique platform to assess the cross-lingual generation capability of LLMs, and the assessment may shed light on pretraining/finetuning algorithm design for achieving universal LLMs (Chowdhery et al., 2022). While a few studies have reported translation results (Brown et al., 2020;Chowdhery et al., 2022), a systematic study on how prompting works for MT is still missing in the literature.</p>
<p>We aim at filling this gap by thoroughly examining different prompting setups using the recently released GLM-130B (Zeng et al., 2022), particularly concerning three aspects: the prompting strategy, the use of unlabeled/monolingual data, and the feasibility of transfer learning. Prompting has shown varying sensitivity to the choice of prompt templates and examples (Zhao et al., 2021). For MT, prior studies adopted different templates (Brown et al., 2020;Wei et al., 2022a;Chowdhery et al., 2022), and we reevaluate them to figure out the optimal one. We further design a set of features for prompt examples and explore which one(s) could explain the prompting performance, according to which we develop the example selection strategy.</p>
<p>Since leveraging monolingual data to improve MT has long been of interest, we would like to determine whether and how such data can be used in prompt example construction. We make a step in this direction by studying the effect of data aug-mentation using back-/forward-translation (Sennrich et al., 2016b;Zhang and Zong, 2016) via zero-shot prompting. In addition, neural MT and pretrained LLMs have shown encouraging transfer abilities (Devlin et al., 2019;Arivazhagan et al., 2019;Zhang et al., 2020;Xue et al., 2021) but transfer learning for prompting has received little attention. Whether prompt examples are transferable across different settings, such as from one domain/language pair to another and from sentencelevel examples to document-level translation, is yet to be addressed.</p>
<p>We address the above concerns with GLM-130B as the testbed and conduct extensive experiments on FLORES and WMT evaluation sets. We mainly study translation for three languages: English, German and Chinese. We also provide a quantitative and qualitative analysis to disclose problems when prompting for MT, which might offer insights for future study. Our main findings are listed as below:</p>
<p>• Prompting performance varies greatly across templates, and language-specific templates mainly work when translating into languages LLMs are pretrained on. An English template in a simple form works best for MT.</p>
<p>• Several features of prompt examples, such as sequence length, language model score, and semantic similarity, correlate significantly with its prompting performance while the correlation strength is weak in general. Selecting examples based on these features can outperform the random strategy, but not consistently.</p>
<p>• Using monolingual examples for prompting hurts translation. By contrast, constructing pseudo parallel examples via back-/forwardtranslation is a good option. Back-translation performs better and is more robust.</p>
<p>• Prompting shows some degree of transferability. Using demonstrations from other settings can improve translation over the zero-shot counterpart, while the superiority of a demonstration in one setting can hardly generalize to another.</p>
<p>• Prompting for MT still suffers from copying, mistranslation of entities, hallucination, inferior direct non-English translation, and prompt trap where translating the prompt itself via prompting becomes non-trivial.</p>
<p>Setup</p>
<p>Prompting for MT Given a pretrained and fixed LLM L, MT prompting first converts each test input X to a prompt according to a template T and then generate the translation Y by feeding the prompt to L. In this study, we consider zero-shot and few-shot prompting for translation. Zero-shot prompting only has access to the test input X, while few-shot prompting assumes that a few extra labeled examples (or prompt/demonstration examples)
D P = {X i , Y i } K i=1
are available and can be used as a demonstration. Particularly, we adopt the following template for zero-shot prompting based on the results in Section 3:</p>
<p><a href="[input]">src</a>: X <a href="38.78">tgt</a>:</p>
<p>(1)</p>
<p>where We also explore template language, which denotes the language in which the template is expressed. For example, the Chinese template " 中 文：X 英文：" represents the Chinese counterpart of the following English template "Chinese: X English: ".</p>
<p>Setting We experiment with GLM-130B, a LLM with 130B parameters pretrained on Chinese and English monolingual corpora, which was reported to outperform GPT-3 and OPT-175B on several NLP tasks (Zeng et al., 2022). Note GLM-130B is a raw LLM without any further finetuning. We use its INT4-quantized version, which is more affordable and suffers little performance degradation. We adopt beam search for MT with a beam size of 2, and perform experiments with 4 RTX 3090 and A100-40G GPUs.</p>
<p>We work on three languages: English (En), German (De), and Chinese (Zh). We perform major   and their prompting performance, we construct an Ablation set for Wiki, WMT and Multi-Domain (IT and Medical) based on the dev set of FLORES, WMT21 and Multi-Domain, separately, where we randomly sample 100 instances as the ablation test set and use the rest as the default example selection pool. To distinguish, we will refer to the official dev and test set as Full set. Detailed statistics are listed in Table 9, Appendix.</p>
<p>We evaluate translation performance using both a surface-based metric, detokenized BLEU↑ from SacreBLEU (Post, 2018), and a model-based metric, COMET↑ from unbabel-comet with wmt20comet-da (Rei et al., 2020).</p>
<p>Prompting Strategy for MT</p>
<p>To perform MT, prompting needs to cast the translation problem into a language modeling problem via the prompt. Thus, the format of the prompt, including its wording, directly affects how LLM understands the task and its behavior. For MT, we are interested in the following research questions:</p>
<p>• Which template should we use for MT prompting? And what language for the template?</p>
<p>• Does demonstration matter for MT prompting? How to select optimal prompt examples?</p>
<p>We address them through extensive experiments on Wiki Ablation sets.</p>
<p>Zero-shot prompting performance varies greatly across templates. We start with zeroshot prompting and explore the effect of different templates. Depending on how to describe MT and partially inspired by prior studies (Brown et al., 2020;Chowdhery et al., 2022;Wei et al., 2022a), we compare 6 templates and evaluate them on the Wiki Ablation sets covering 6 language pairs (En↔De, En↔Zh, De↔Zh). Table 1 shows the results (we list detailed results in    Language-specific template delivers mixed results. Table 1 also shows the prompting results of German and Chinese templates, which often largely underperform their English counterparts. Since German is not a major pretraining language in GLM-130B, a German template degenerates the translation substantially. By contrast, a Chinese template yields improved performance when translating into Chinese (see Table 10). Still, an English template works best on average.</p>
<p>The preference of GLM-130B to English template also shows that the level of language understanding and cross-lingual ability in GLM-130B varies across languages, even though it's pretrained on the same amount of monolingual Chinese and English tokens. This might be caused by the fact that English is used more globally than Chinese, but might also suggest that improving the language understanding of LLM requires more advanced training algorithms beyond scaling training data.</p>
<p>Using more prompt examples for demonstration improves translation significantly on aver- age. We next study few-shot prompting following the template A but in format (2) with K varying from 1 to 20. We evaluate multiple demonstrations for each K via random sampling to reduce data biases. Figure 1 shows that the more examples used, the better average performance (more results are shown in Figure 5, Appendix), albeit at the cost of using more GPU memory and increasing the inference time per token as in Figure 3.</p>
<p>The performance of demonstration is not stable. However, we also see high performance variance under the same K. It's possible that a demonstration with 5 examples outperforms its 10 or 20 counterpart. Figure 1 also shows that 1-shot prompting underperforms zero-shot prompting in many cases, even on average. This echoes with previous findings on other NLP tasks (Zhao et al., 2021;Liu et al., 2022) and also highlights the significance of developing effective example selection strategies.</p>
<p>Note that few-shot prompting greatly improves translation into Chinese. The reason based on our manual analysis is that the zero-shot baseline tends to translate into traditional Chinese with messy codes, where prompt examples help (the reference text is always simplified Chinese).</p>
<p>Several features correlate with prompting performance significantly yet weakly. We thus turn to explore example selection for prompting. Our idea is to extract a couple of diverse features from demonstration and examine whether any of them are informative enough to be used as an indicator for the selection. In this study, we simplify our analysis by focusing on 1-shot prompting, which ignores the ordering of prompt examples (we leave few-shot analysis to future). Particularly, we extract and analyze 7 features of a demonstration:  LMScore GLM-130B-based, length-normalized log likelihood of the demonstration;
S(T)Length
MTScore translation quality of the prompt example from COMET QE model wmt20-cometqe-da (Rei et al., 2020);</p>
<p>SemScore semantic score based on the cosine similarity of the demonstration's source and target sentence embeddings from LASER2 (Heffernan et al., 2022);</p>
<p>CaseSemScore-Src similarity to the input that averages over SemScores between the test input and the demonstration's source;</p>
<p>CaseSemScore-Tgt similar to CaseSemScore-Src but compares to demonstration's target;</p>
<p>We sample multiple demonstrations randomly and inspect the Spearman's correlation between feature values and prompting performance. We consider high-quality and low-quality pool for sampling. Table 2 summarizes the results and Figure 2 illustrates the relation between COMET and LMScore (more results are given in Table 11 and Figures 6, 7, Appendix). With the high-quality pool, different demonstrations yield similar translation results (see blue points) despite their feature values varying greatly. Several features show insignificant and inconsistent correlation, particularly for De→En and Zh→En. This suggests developing selection policy for high-quality example pool is non-trivial.</p>
<p>After mixing with demonstrations from the lowquality pool, the significance gets strengthened.</p>
<p>LMScore and CaseSemScore-Tgt shows the highest correlation on average followed by TLength and SemScore. MTScore behaves much worse which might be caused by its instability on sentence-level evaluation (Moghe et al., 2022). However, we didn't see significant difference in terms of Spearman's ρ between input-relevant and input-agnostic features (Agrawal et al., 2022), neither among surface-based, LLM-based or semantic-based features. Surprisingly, the simple feature, S/TLength, yields reasonably high correlation. We argue that long examples could offer LLM with more signals about the task's input and output space. This finding suggests that researchers should select long unlabeled sentences for annotation to improve prompting. Yet, most Spearman's ρs are much smaller than 0.5, indicating a weak/fragile relation.</p>
<p>In general, selecting prompt examples of high translation quality, high semantic similarity, high LLM likelihood, long sequence length and high similarity to test inputs are all preferable strategies. Unfortunately, none of them can guarantee optimal translation performance.</p>
<p>Using prompt examples selected based on the proposed features yields improved performance. We next verify the above findings on the Full sets. We explore selection strategies based on SemScore, LMScore and TLength (i.e. use topranked examples) as they show high average correlation. We didn't analyze CaseSemScore-Tgt as it's more complicated and doesn't make significant difference. Note we excluded too long (more than 100 tokens) or too short (less than 10 tokens) examples during selection. We also consider 5-shot prompting, where we concatenate top-ranked 5 examples in an ascending order (Liu et al., 2022). Table 3 shows that, with high-quality pool, adopting the feature-based strategy is likely to outperform the random baseline, and the SemScore-based strategy performs well across different settings (detailed results are available in Table 13 and 14, Appendix). These strategies also generalize to 5-shot prompting to some extent. For selection from lowquality pool, we propose a combined strategy: we  first choose top-11K examples according to Sem-Score to filter out poor examples, the top-1K of which are also dropped as they tend to be uninformative (see Table 12 in Appendix); then we re-rank the rest with LMScore and retain top-1K examples, upon which we further apply the TLength-based strategy. In Table 3, this combined strategy outperforms the random one by varying degrees.</p>
<p>Monolingual Data for Prompting</p>
<p>A longstanding concern in MT is how to utilize unlabeled data to improve translation. While prompting enables few-shot learning reducing the data requirement, exploring whether demonstration could benefit from monolingual examples is still valuable, both for MT study and for understanding of the role of demonstration in prompting. Min et al. (2022) argue that the key role of demonstration lies in its support of the input space, the label space and the prompt format, rather than the genuineness of the examples. They found that randomly replacing labels in demonstration barely hurts performance on classification tasks. We reexamine this argument in the context of MT by studying the following three prompting settings: 1) random examples constructing sentence pairs from monolingual sources and targets randomly; 2) source/target example only using monolingual source/target alone for prompting.</p>
<p>Directly using monolingual data for demonstration doesn't work. Figure 4 (top) shows a totally different story (see Figures 8 and 9 in Ap-pendix for more results): monolingual examplebased demonstration almost always hurts translation, and the more examples used, the more degeneration yielded. Using random examples misleads the prompting and performs the worst in general; compared to target-only examples, using source examples yields slightly better results except translating into Chinese. This indicates that the genuine source-target mapping should be retained in the demonstration, and also indicates that MT features unique challenges which deserves more attention when studying prompting.</p>
<p>Pseudo parallel examples by forward-/backtranslation benefits prompting. Inspired by data augmentation in MT (Sennrich et al., 2016b;Zhang and Zong, 2016), we next resort to constructing pseudo parallel data. We first adopt GLM-130B to translate the source or target examples via zero-shot prompting, and then use the generated parallel examples as demonstration. Despite low quality, Figure 4 (bottom) shows that this is an effective way to improve prompting, and using more examples often produces better results. We also observe that back-translation (i.e. translating target monolingual examples) performs better and behaves more robustly than forward-translation (i.e. translating source examples instead), which even approaches prompting with real parallel examples.  Table 4: Spearman's ρ and relative performance for crosslingual transfer under 1-shot prompting on Wiki Ablation sets (among En, De and Zh). When studying transfer from language pair S1 to S2, we randomly sample 300 demonstrations from the default pool of S1, and then evaluate them on the Ablation test sets for S1 and S2 respectively, based on which we compute the correlation. The performance is also averaged. ∆ Quality: relative quality against the zero-shot baseline. Blue cells indicate positive gains. Source/Target Shared: average result for transfer settings where the source/target language is shared; Reversed: average result for the same language pair but in different directions.</p>
<p>Transfer Learning for Prompting</p>
<p>could be transferred across different settings, especially from one domain/language pair to another and from sentence-level to document-level translation. While previous studies demonstrate the feasibility with continuous prompts on classification tasks , transfer for hard prompting on MT has never been investigated. Assume that demonstrations D 1 and D 2 are selected in setting S 1 and that D 1 performs better (i.e. D 1 &gt; D 2 ), We have the following research questions:</p>
<p>• Could we also expect D 1 &gt; D 2 in setting S 2 ?</p>
<p>• Whether using demonstrations from S 1 could outperform zero-shot prompting in S 2 ?</p>
<p>We next study these questions through experiments with 1-shot prompting.</p>
<p>The superiority of a demonstration doesn't generalize across settings. If the ranking D 1 &gt; D 2 holds across settings, the results of the same set of demonstrations in different settings should show high and significant Spearman's correlation. Unfortunately, the correlations in Table 4 and 5 are very weak and often insignificant (more results are given in Table 15, 16, and 17), even for the same language pairs in different directions (Reversed) and for similar domains (Wiki⇒WMT). This suggests that we will need setting-specific demonstration to get the optimal translation quality.</p>
<p>Using out-of-setting demonstrations can benefit translation. However, we can still gain from using out-of-setting demonstrations as demonstrated by the positive gains in Table 4 and 5, where we   find that transfer in target-shared and reversed settings is relatively easier, and that transfer across distant domains can be successful particularly when insetting example pool is of low quality. This is also supported by the transfer to document-level translation, where both BLEU and document-specific evaluation get improved as shown in Table 6. Results in Table 19 show that the transfer is unstable and could deliver negative results, i.e. worse than zero-shot prompting, partially resonating with previous findings (Lin et al., 2021). We leave the question of how to select prompt examples in transfer learning setups to future.</p>
<p>Discussion</p>
<p>Although prompting enables translation with decent performance, it still suffers from many (wellknown) problems. Here, we briefly explain the problems we observed from the model's outputs. Prompting sometimes rejects translating the input. Instead, it emits either empty or off-target outputs, i.e. translating in a wrong target language. This occurs frequently when translating into Chinese, where the model often translates into traditional Chinese with messy codes, causing unstable performance. Besides overly relying on a language Source 根据三江源国家公园管理局长江源园区可可西里管理处统计，藏羚羊回迁数量总体呈 逐年上升态势，2019年藏羚羊回迁数量为4860只，比2018年增加338只。</p>
<p>Reference</p>
<p>Statistics from the Sanjiangyuan National Park Administration Yangtze River Origin Park Hoh Xil Management Office show that the number of Tibetan antelopes on the return migration route has been increasing each year, with 4,860 counted in 2019, an increase of 338 over 2018.</p>
<p>GLM-130B (1-shot)</p>
<p>According to the三江源国家公园管理局长江源园区可可西里管理处, the total number of re-migration of the Tibetan antelope ::: has ::: been ::: on :: the ::: rise ::::: since :::: 2018, with 4,860 re-migrating in 2109, an increase of 338 compared to 2808.</p>
<p>Prompt in Prompt</p>
<p>English: Dominic Raab has defended the Government's decision to re-introduce quarantine measures on Spain at short notice. Translate from English to Chinese: Chinese:
Reference 针对政府突然做出重新对西班牙实施隔离措施的决定，Dominic Raab 做出了辩解。从英 文翻译成中文： GLM-130B (zero-shot) 多 米 尼 克·拉 布(Dominic Raab)对 政 府 决 定 重 新 引 入 西 班 牙 的 检 疫 措 施 表 示 支
持。Translate from English to Chinese:  model, prompting tends to under-translate the input, copy source phrases, produce code-switched output, mistranslate entities (e.g. dates) and generate hallucination, as illustrated in Table 7.</p>
<p>We also observe a phenomenon specific to prompting: prompt trap where prompting behaves unpredictable when its input is mixed with prompt template phrases. In the second case in Table  7, the model copies the template phrases, rather than translating them into Chinese. This means that translating prompt itself (not just the input) becomes non-trivial, and that users may attack prompting-based translation systems by manipulating the input format.</p>
<p>We find that the translation quality between German and Chinese is very poor (see Table 13). We argue that the cross-lingual ability of GLM-130B mainly centers around English (although GLM-130B was pretrained on Chinese as well), and thus explore pivoting translation instead. Table 8 shows that pivoting through English greatly improves non-English translation. It's still unclear whether the current LLM pretraining recipe could achieve promising non-English-centric cross-lingual ability. We might need to consider adding parallel data into the LLM pretraining or finetuning.</p>
<p>Related Work</p>
<p>The capability of prompting heavily depends on its surface representation, where small modifications to the prompt could cause high variance in its performance. This inspires researchers to develop advanced prompting strategies to get the most from LLMs. Gao et al. (2021) proposed to generate prompt templates automatically using T5 (Xue et al., 2021) rather than adopting manual templates. Liu et al. (2022) reported selecting prompt examples close to the test input via a kNNbased retriever, Sorensen et al. (2022) resorted to an information-theoretic approach based on mutual information, while Zhang et al. (2022b) formulated example selection as a sequential decision problem and solved it by reinforcement learning. For reasoning tasks, Wei et al. (2022c) developed chain-ofthought (CoT) prompting letting the model output the intermediate reasoning steps, which inspires researchers to further explore CoT selection (Fu et al., 2022) and decomposition (Zhou et al., 2022). In contrast to the studies just mentioned, which focus on NLP tasks other than MT, we explore prompting strategies exclusively for translation.</p>
<p>Prompting uses instructions to guide LLMs, which is closely related to neural MT with special prefixes. In multilingual NMT, a target language tag is often appended to the source input to indicate the translation direction (Johnson et al., 2017;Arivazhagan et al., 2019;Zhang et al., 2020). Special attribute tags can also be used to control properties of the model output, such as politeness (Sennrich et al., 2016a), diversity (Shu et al., 2019), and quality (Caswell et al., 2019). Besides, retrieved phrases and sentences can be augmented to the input to improve translation quality (Zhang et al., 2018;Gu et al., 2018). With the popularity of prompting LLMs, researchers see value in incorporating prompts into neural MT (Li et al., 2022;Tan et al., 2021;Garcia and Firat, 2022). Still, these methods rely on pretraining or finetuning the model rather than prompting frozen LLMs.</p>
<p>Very recently, concurrent to our work, Vilar et al. (2022) examined the capability of prompting PaLM for translation and discovered that prompting with high-quality examples even chosen randomly performs on par with or better than the one using input-relevant examples. By contrast, Agrawal et al. (2022) explored strategies to select inputspecific examples, and observed that input-relevant examples based on n-gram overlap significantly improves the capability of prompts. Our study resonates with both their findings and also explains their conflict: while the quality and input-based semantic similarity correlate with prompting performance significantly, the correlation strength is unfortunately not strong enough so using them as indicators to select examples may produce mixed results. Note that apart from example selection, we also studied using monolingual data and transfer learning for MT prompting, which, to the best of our knowledge, have never been explored before.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we presented a systematic study on prompting for MT, exploring topics ranging from prompting strategy, the use of unlabelled monolingual data, to transfer learning. We found that prompt template and demonstration example selection both have substantial impact on translation. Some prompt example features correlate significantly with prompting performance; treating them as criteria for example selection benefits translation to some extent but not consistently as the correlations are not strong enough.</p>
<p>Prompting for MT requires retaining the sourcetarget mapping signals in the demonstration. Directly applying monolingual data for prompting sounds interesting but doesn't work. Constructing pseudo parallel prompt examples by back-/forwardtranslation via zero-shot prompting is a simple yet effective solution. Regarding transfer learning, we saw positive results when applying a (sentencelevel) demonstration to other domains, other language pairs or document-level translation. Unfortunately, the optimality of the demonstration doesn't generalize across settings and the transfer perfor-mance is also unstable. We argue that MT provides a set of unique challenges and call for more efforts on evaluating prompting LLMs for MT.</p>
<p>Prompting also faces a number of other issues, like off-target generation and prompt traps, which we plan to address in the future. We are also interested in examining whether our findings can generalize to other LLMs, like GPT-3, OPT and PaLM. We would also like to explore further how to improve the cross-lingual ability in LLM.</p>
<p>Limitations</p>
<p>Our study heavily depends on the INT-4 quantized GLM-130B, which, unlike GPT and PaLM, was pretrained with both bidirectional and unidirectional training objectives. The quantization might weaken the model's capability and deteriorate some unknown aspects. It's unclear how our findings generalize to other pretrained LLMs. In addition, we mainly work on three languages due to resource constraints, and in experiments, results vary greatly across language pairs. Increasing the coverage of experimental languages would make the results more reliable.   72 -20.65 -81.28 -125.21 -137.02 -125.31 -108.45 -99.65 C 15.40 5.70 5.70 3.00 6.00 6.70 7. 08 -23.46 -80.15 -86.27 -104.10 -87.18 -58.23 -73 02 -6.41 -90.63 -12.10 -159.66 -9.24 -121.29 -66.55 C 11.40 3.20 14.30 0.40 20.80 5.00 9.18 -32.55 -114.57 -9.91 -140.54 2.89 -85.58 -63.38 D 17.10 6.40 15.90 0.20 19.60 1.90 10.18 -34.15 -101.69 -24.36 -166.15 -9.20 -125.20 -76.79 E 29.00 50 -11.65 -102.50 -63.95 -161.96 -46.84 -128.12 -85.84 C 12.10 3.00 13.80 0.80 21.20 9.90 10.13 -36.39 -105.55 -42.16 -151.06 -15.41 -74.90 -70.91 D 14.10 3.20 15.10 0.20 20.00 2.50 9.18 -19.15 -106.69 -19.34 -154.73 -11.51 -94.82 -67.71 E 28.60 8.00 26.50 0.90 32.30 21.40 19.62 8.71 -118.14 15.34 -124.30 21.18 14.91 -30.38 F 26.90 3.40 26.10 0.20 25.80 16.00 16.40 11.58 -120.31 10.33 -129.61 -21.19 -20.52 -44.95 </p>
<p>Figure 1 :
1COMET scores for few-shot prompting as a function of the number of prompt examples (K = 1, 5, 10, 20) on Wiki Ablation sets. For each setup, we randomly sample 100 times from the example pool and show the performance distribution via box plots. Dashed red line denotes the zero-shot baseline; blue curve and shadow area denote the mean and standard deviation. analysis on FLORES (Wiki domain, En-De-Zh, NLLB Team et al., 2022) and WMT21 (News domain, En-De, En-Zh, Akhbardeh et al., 2021), and also report results on Multi-Domain (IT, Law and Medical domain, De-En, Aharoni and Goldberg, 2020) to examine domain robustness and transfer ability, and PDC (News domain, Zh→En, Sun et al., 2022) for document-level translation. To understand the relation between prompt examples</p>
<p>Figure 2 :
2Visualization between COMET and LMScore for 1-shot prompting on Wiki Ablation sets. While correlations are significant, data points are scattered like clouds.</p>
<p>Figure 3 :
3Inference time per token in seconds for zero-/fewshot prompting on Wiki En-De Ablation sets. Numbers are averaged over 3 runs with 3 distinct demonstrations on 4 A100-40G GPUs.</p>
<p>Figure 4 :
4COMET scores for few-shot prompting with monolingual data on Wiki Ablation sets. Random Example: random sentence pairs; Source/Target Example Only: only use source or target data for prompting; Source/Target Example Aug: use pseudo-parallel data instead constructed via zero-shot prompting. For each setup, we randomly sample 50 demonstrations and report average performance.</p>
<p>Figure 5 :
5COMET (top) and BLEU (bottom) scores for few-shot prompting as a function of the number of prompt examples (K = 1, 5, 10, 20) on Wiki Ablation sets. For each setup, we randomly sample 100 times from the example pool and show the performance distribution via box plots. Dashed red line denotes the zero-shot baseline; blue curve and shadow area denote the mean and standard deviation.</p>
<p>Figure 6 :Figure 7 :
67Scatter plotting between BLEU and LMScore for 1-shot prompting on Wiki De↔En, En↔Zh Ablation sets. Scatter plotting between COMET/BLEU and LMScore for 1-shot prompting on Wiki De↔Zh Ablation sets.</p>
<p>Figure 8 :
8Results for few-shot prompting with monolingual data on Wiki Ablation sets for De↔Zh.</p>
<p>Figure 9 :
9BLEU scores for few-shot prompting with monolingual data on Wiki Ablation sets.</p>
<p>[psrc]:X 1 [ptgt]: Y 1 . . . [psrc]: X K<a href="[input]">src</a> and <a href="38.78">tgt</a> denote test language(s), 
i.e., the source and target language name of the test 
language pair, respectively. For few-show prompt-
ing, we concatenate the given prompt examples: </p>
<p>[ptgt]: Y K <a href="[input]">src</a>: X <a href="38.78">tgt</a>: 
(2) </p>
<p>where [psrc] and [ptgt] denote prompt lan-
guage(s), i.e., the source and target language name 
of the prompt example, respectively. By default, 
prompt examples and test data are in the same 
language pair. However, when considering cross-
lingual transfer for prompting, prompt examples 
might be in a different language pair. </p>
<p>Table 1: COMET scores averaged over 6 language pairs for zero-shot prompting with different templates and different template languages on Wiki Ablation sets. w/ and w/o denote whether adding line breaks into the template or not; indicates the position of the line break. <a href="[input]">src</a> and <a href="38.78">tgt</a> denote source and target test language name, respectively, and [input] denotes the test input; all of them are placeholders. English, German and Chinese indicate template languages. Best results are shown in bold.ID Template (in English) </p>
<p>English 
German 
Chinese </p>
<p>w/o 
w/ 
w/o 
w/ 
w/o 
w/ </p>
<p>A </p>
<p>31.17 
-26.15 
-16.48 
14.82 
-1.08 
B 
[input] 
<a href="38.78">tgt</a>: 
-88.62 -85.35 -135.97 
-99.65 -66.55 -85.84 
C 
[input] Translate to <a href="38.78">tgt</a>: 
-87.63 -68.75 -106.30 
-73.23 -63.38 -70.91 
D 
[input] Translate from <a href="[input]">src</a> to <a href="38.78">tgt</a>: 
-113.80 -89.16 -153.80 -130.65 -76.79 -67.71 
E 
<a href="[input]">src</a>: [input] Translate to <a href="38.78">tgt</a>: 
20.81 
16.69 
-24.33 
-5.68 
-8.61 -30.38 
F 
<a href="[input]">src</a>: [input] Translate from <a href="[input]">src</a> to <a href="38.78">tgt</a>: 
-27.14 
-6.88 
-34.36 
-9.22 -32.22 -44.95 </p>
<p>Table 10
10, </p>
<p>Table 2 :
2Spearman's ρ between demonstration features and 
their prompting performance for 1-shot prompting on Wiki 
Ablation sets. We randomly sample 600 demonstrations from 
each pool to calculate the correlation. HQ: examples are from 
the default high-quality pool; LQ: examples are from the low-
quality pool based on WikiMatrix.v1. </p>
<p>the number of source (target) tokens;Method </p>
<p>Wiki 
WMT </p>
<p>BLEU COMET BLEU COMET </p>
<p>Zero-Shot 
24.08 
33.92 
20.38 
17.97 </p>
<p>1-Shot Translation (high-quality pool) 
Random 
26.31 
48.29 
21.27 
30.70 
SemScore 
26.73 
49.34 
21.82 
31.28 
LMScore 
26.48 
47.92 
21.59 
30.81 
TLength 
26.54 
48.73 
21.29 
30.68 </p>
<p>5-Shot Translation (high-quality pool) 
Random 
27.46 
51.11 
21.82 
33.87 
SemScore 
27.36 
51.66 
22.37 
34.30 
LMScore 
27.17 
50.65 
22.04 
35.19 
TLength 
27.08 
50.50 
21.75 
34.29 </p>
<p>1-shot Translation (Low-quality Pool) 
Random 
24.75 
38.86 
22.06 
30.70 
Ours 
24.94 
39.88 
22.23 
30.87 </p>
<p>Table 3 :
3BLEU and COMET scores for zero-shot and few-shot 
prompting on Wiki and WMT Full sets with different selection 
strategies. Ours: the proposed combined strategy; Random: 
random sampling; SemScore, LMScore and TLength denote 
selecting top-ranked examples based on the corresponding 
feature values. We select 3 demonstrations for each translation 
direction and report average performance; the final score is 
further averaged over different language pairs. Underlined 
results denote the best in each section, while Bold results are 
the overall best. </p>
<p>After obtaining a performant demonstration, we are interested in to what extent its capabilitySetting </p>
<p>Correlation 
∆ Quality </p>
<p>BLEU COMET BLEU COMET </p>
<p>Source Shared 
0.08 
0.10 +0.59 
+7.03 
Target Shared 
0.20 
0.24 +1.32 
+9.67 
Reversed 
0.15 
0.06 +1.41 
+11.56 </p>
<p>Table 5 :
5Spearman's ρ and relative performance (in COMET) 
for cross-domain transfer under 1-shot prompting. We explore 
transfer from Wiki to Multi-Domain using the Ablation sets. 
Correlation and performance are calculated in the same way 
as in cross-lingual transfer, except that we sample 200 demon-
strations.  ‡ : statistically significant at p &lt; 0.01; Gray cells 
indicate insignificance. </p>
<p>Method 
d-BLEU 
TC 
CP 
PT TCP </p>
<p>Zero-Shot 
30.2 47.5 38.7 41.6 42.4 </p>
<p>SemScore 
30.5 53.0 34.4 43.2 42.9 
LMScore 
30.5 53.0 36.8 42.9 43.7 </p>
<p>Table 6 :
6Results for transfer learning from sentence-level demonstration to document-level translation under 1-shot prompting on PDC Zh→En Full sets. We split each test document in PDC into non-overlapped chunks, each of which contains about 4 sentences. SemScore/LMScore: prompt example selection strategy; we apply them to PDC's default pool. We select 3 demonstrations and report average performance. d-BLEU: document-level BLEU; TC/CP/PT/TCP(↑): document-specific metrics proposed in(Sun et al., 2022).</p>
<p>Table 7 :
7Case study of translation errors by prompting. Top: copying (in red), mistranslation of date (in blue), misunderstanding 
of source ( ::: 
wave :::: lines); Bottom: prompt trap where the model fails to translate the prompt phrase (in bold). </p>
<p>Setting 
0-shot 
1-shot </p>
<p>De→Zh Zh→De De→Zh Zh→De </p>
<p>Direct 
2.80 
10.05 
47.23 
11.75 
Pivoting 
19.23 
19.53 
48.25 
25.31 </p>
<p>Table 8 :
8COMET scores for direct vs. pivoting translation for De↔Zh on Wiki Full sets. In 1-shot prompting, we randomly sample 3 demonstrations and report average performance. Pivoting: source → English → target.</p>
<p>Table 9 :
9Statistics of Ablation sets and Full sets. Numbers in brackets denote the number of instances.: data from </p>
<p>Table 10 :
10Detailed zero-shot results for prompting with different templates and different template languages on Wiki Ablation sets. Template A in English achieves the overall best performance measured by BLEU and COMET. Avg: average result over different language pairs. Best results in each section are underlined; best results in each column are in bold.</p>
<p>11 ‡ 0.17 ‡ 0.11 ‡ 0.15 ‡ 0.10 ‡ 0.31 ‡ 0.16 0.12 ‡ 0.24 ‡ 0.42 ‡ 0.50 ‡ 0.17 ‡ 0.33 ‡ 0.30 CaseSemScore-Src -0.01 0.20 ‡ 0.22 ‡ 0.08 † 0.18 ‡ -0.03 0.11 0.08 ‡ 0.29 ‡ 0.53 ‡ 0.49 ‡ 0.26 ‡ 0.05 0.28 CaseSemScore-Tgt -0.01 0.22 ‡ 0.25 ‡ 0.14 ‡ 0.21 ‡ 0.05 0.14 0.09 ‡ 0.32 ‡ 0.53 ‡ 0.53 ‡ 0.27 ‡ 0.11 ‡ 0.31 13 ‡ 0.11 ‡ 0.15 ‡ 0.20 ‡ 0.25 ‡ 0.29 ‡ 0.19 0.13 ‡ 0.20 ‡ 0.45 ‡ 0.45 ‡ 0.28 ‡ 0.31 ‡ 0.30 CaseSemScore-Src 0.16 ‡ 0.15 ‡ 0.18 ‡ 0.03 0.28 ‡ 0.03 0.14 0.20 ‡ 0.29 ‡ 0.51 ‡ 0.36 ‡ 0.31 ‡ 0.07 ‡ 0.29 CaseSemScore-Tgt 0.14 ‡ 0.17 ‡ 0.16 ‡ 0.05 0.24 ‡ 0.09 † 0.14 0.18 ‡ 0.30 ‡ 0.49 ‡ 0.39 ‡ 0.29 ‡ 0.13 ‡ 0.30Method </p>
<p>High-quality Examples 
Plusll Low-quality Examples </p>
<p>De ↔ En 
De ↔ Zh 
En ↔ Zh 
Avg 
De ↔ En 
De ↔ Zh 
En ↔ Zh 
Avg 
→ 
← 
→ 
← 
→ 
← 
→ 
← 
→ 
← 
→ 
← </p>
<p>Correlation with COMET </p>
<p>SLength 
0.02 0.18  ‡ 0.24  ‡ 0.12  ‡ 0.26  ‡ 0.01 0.14 0.09  ‡ 0.20  ‡ 0.52  ‡ 0.44  ‡ 0.24  ‡ 0.10  ‡ 0.26 
TLength 
-0.01 0.23  ‡ 0.19  ‡ 0.27  ‡ 0.29  ‡ 0.06 0.17 0.06  † 0.35  ‡ 0.41  ‡ 0.57  ‡ 0.25  ‡ 0.13  ‡ 0.29 
LMScore 
0.06 0.23  ‡ 0.01 0.20  ‡ 0.12  ‡ 0.21  ‡ 0.14 0.19  ‡ 0.38  ‡ 0.35  ‡ 0.51  ‡ 0.16  ‡ 0.27  ‡ 0.31 
MTScore 
0.01 0.05 0.11  ‡ 0.12  ‡ 0.06 0.28  ‡ 0.11 0.13  ‡ 0.04 0.30  ‡ 0.23  ‡ 0.18  ‡ 0.28  ‡ 0.19 
SemScore 
0.Correlation with BLEU </p>
<p>SLength 
0.20  ‡ 0.27  ‡ 0.21  ‡ 0.11  ‡ 0.33  ‡ 0.12  ‡ 0.21 0.23  ‡ 0.30  ‡ 0.51  ‡ 0.35  ‡ 0.29  ‡ 0.18  ‡ 0.31 
TLength 
0.15  ‡ 0.32  ‡ 0.16  ‡ 0.22  ‡ 0.40  ‡ 0.12  ‡ 0.23 0.15  ‡ 0.38  ‡ 0.41  ‡ 0.47  ‡ 0.33  ‡ 0.19  ‡ 0.32 
LMScore 
0.14  ‡ 0.17  ‡ 0.10  ‡ 0.24  ‡ 0.27  ‡ 0.26  ‡ 0.20 0.23  ‡ 0.30  ‡ 0.39  ‡ 0.46  ‡ 0.27  ‡ 0.32  ‡ 0.33 
MTScore 
0.03 -0.05 0.04 0.09  † 0.03 0.12  ‡ 0.04 0.11  ‡ -0.04 0.26  ‡ 0.19  ‡ 0.17  ‡ 0.14  ‡ 0.14 
SemScore 
0.</p>
<p>Table 11 :
11Detailed Spearman's ρ between demonstration features and their prompting performance (COMET and BLEU) for 1-shot prompting on Wiki Ablation sets. We randomly sample 600 demonstrations from each pool to calculate the correlation. High-quality examples are from the default selection pool while Low-quality examples are from WikiMatrix.v1. † / ‡ : statistically significant at p &lt; 0.05/0.01. Gray cells indicate insignificance; Red cells indicate ρ &gt; 0.5. Source Coordinates: 19°43′10″S 63°18′00″E / 19.71944°S 63.30000°E / -19.71944; 63.30000 Target 坐标：19°43′10″S 63°18′00″E / 19.71944°S 63.30000°E / -19.71944; 63.30000 Source Brinton, Lauren and Leslie Arnovick. Target Brinton, Lauren und Leslie Arnovick.En→Zh </p>
<p>Source SAO 40012 is HD 277559. 
Target 
SAO 40012是HD 277559。 </p>
<p>En→De </p>
<p>Source 2002 and 2004. 
Target 
2002 und 2004. </p>
<p>Table 12 :
12Top-ranked parallel examples according to SemScore on WikiMatrix.v1 En-De and En-Zh. Despite showing high semantic similarity, these examples are not very informative. We thus dropped them at selection.−100 </p>
<p>−50 </p>
<p>0 
COMET(↑) </p>
<p>Wiki De→Zh </p>
<p>Zero-shot Baseline 
Random Example 
Source Example Only 
Target Example Only </p>
<p>Table 13 :
13Detailed test results for zero-shot and few-shot prompting on Wiki Full sets with different selection strategies. Ours: the proposed combined strategy; Random: random sampling; SemScore, LMScore and TLength denote selecting top-ranked examples based on the corresponding feature values. We select 3 demonstrations for each setup and report the average. Avg: average result over language pairs. Underlined results denote the best in each section, while Bold results are the overall best.Zero-Shot 28.30 15.70 20.70 16.80 20.38 46.01 13.32 4.63 7.92 17.97 1-Shot Translation (high-quality pool) Random 25.63 16.37 26.03 17.03 21.27 45.90 16.89 40.88 19.14 30.70 SemScore 26.90 16.03 26.30 18.07 21.82 46.39 15.13 41.13 22.49 31.28 LMScore 27.53 15.70 25.43 17.70 21.59 47.47 17.53 38.95 19.29 30.81 TLength 25.60 16.33 25.80 17.43 21.29 43.47 18.24 42.17 18.82 30.68 5-Shot Translation (high-quality pool) Random 26.40 17.10 26.23 17.53 21.82 48.36 20.19 43.97 22.95 33.87 SemScore 27.30 16.57 26.93 18.67 22.37 49.33 18.83 43.49 25.54 34.30 LMScore 25.90 16.87 26.47 18.93 22.04 47.77 20.83 44.76 27.41 35.19 TLength 25.80 17.03 26.55 17.63 21.75 47.34 20.78 45.17 23.85 34.29 1-shot Translation (Low-quality Pool) Random 27.33 15.53 25.30 20.07 22.06 45.29 14.21 36.83 26.49 30.70 Ours 27.63 15.97 25.23 20.10 22.23 47.16 15.01 34.48 26.82 30.87Method </p>
<p>BLEU 
COMET </p>
<p>De ↔ En 
En ↔ Zh 
Avg 
De ↔ En 
En ↔ Zh 
Avg 
→ 
← 
→ 
← 
→ 
← 
→ 
← </p>
<p>Table 14 :
14Detailed test results on WMT Full sets. En→Zh 0.01 -0.01 0.24 ‡ 0.25 ‡ -0.19 ‡ 0.04 -0.01 0.22 ‡ 0.21 ‡ -0.03 Zh→En 0.15 ‡ -0.16 ‡ 0.14 ‡ 0.34 ‡ 0.15 ‡ -0.25 ‡ 0.09 0.14 ‡ 0.21 ‡ 0.03 -Table 15: Detailed Spearman's ρ for cross-lingual transfer under 1-shot prompting on Wiki Ablation sets. Gray cells indicate insignificance.Method </p>
<p>BLEU 
COMET </p>
<p>De ↔ En 
De ↔ Zh 
En ↔ Zh 
De ↔ En 
De ↔ Zh 
En ↔ Zh </p>
<p>→ 
← 
→ 
← 
→ 
← 
→ 
← 
→ 
← 
→ 
← </p>
<p>Prompt Language </p>
<h2>De→En</h2>
<p>0.06 
0.08 0.12  † 0.13  † 0.13  † 
--0.02 
0.09 0.12  † -0.01 0.21  ‡ 
En→De 0.07 
-0.14  ‡ 0.19  ‡ 0.17  ‡ 0.11  † 0.01 
-0.07 0.21  ‡ 0.14  ‡ 0.17  ‡ 
De→Zh -0.08 
0.06 
-0.14  ‡ 0.24  ‡ -0.05 
0.02 0.15  ‡ 
-0.08 0.40  ‡ 0.02 
Zh→De 0.00 0.26  ‡ 0.26  ‡ 
-0.05 
0.01 -0.03 0.21  ‡ 0.22  ‡ 
-0.13  † 0.15  ‡ </p>
<p>Method BLEU COMET
BLEUDe ↔ En De ↔ Zh En ↔ Zh De ↔ En De ↔ Zh En ↔ ZhTable 16: Detailed translation results (relative against the zero-shot baseline) for cross-lingual transfer under 1-shot prompting on Wiki Ablation sets. Blue cells indicate positive gains.→ 
← 
→ 
← 
→ 
← 
→ 
← 
→ 
← 
→ 
← </p>
<p>Prompt Language </p>
<p>De→En 
--0.32 5.02 -0.86 1.29 0.00 
--1.08 35.04 2.71 
7.00 -0.01 
En→De -0.69 
-3.88 -0.69 1.21 -0.41 -0.46 
-26.01 1.56 
6.31 -2.40 
De→Zh -0.63 -0.48 
--0.65 4.38 0.04 0.92 -3.68 
-4.16 23.51 -0.34 
Zh→De -0.66 -0.86 6.84 
-3.23 0.19 0.71 -6.15 43.67 
-17.54 0.51 
En→Zh -1.54 -1.17 6.23 -1.44 
--1.50 -6.00 -4.47 41.77 -1.79 
--2.20 
Zh→En -1.12 -1.00 1.78 -1.11 4.81 
--2.63 -3.85 15.25 3.90 25.29 
-</p>
<p>Transfer from Wiki to ⇒ WMT 
IT 
Medical </p>
<p>Correlation 
En→De 
0.05 
0.11 
0.15  † 
De→En 
-0.25  ‡ 
0.19  ‡ 
0.07 </p>
<p>∆ Quality 
En→De 
-0.45 +0.88 
-0.21 
De→En 
-0.43 +1.00 
+0.77 </p>
<p>Table 17 :
17Spearman's ρ and relative performance (in BLEU) for cross-domain transfer under 1-shot prompting.Setting 
0-shot 
1-shot </p>
<p>De→Zh Zh→De De→Zh Zh→De </p>
<p>Direct 
21.70 
9.60 
28.70 
9.07 
Pivoting 
24.4 
11.5 
29.47 
11.47 </p>
<p>Table 18 :
18BLEU scores for direct vs. pivoting translation for De↔Zh on Wiki Full sets.Method </p>
<p>BLEU 
COMET </p>
<p>IT 
Law Medical 
Avg 
IT 
Law Medical 
Avg </p>
<p>Zero-Shot 
32.4 
28.5 
31.3 
30.7 12.39 32.85 
33.99 26.41 </p>
<p>1-shot Translation (Low-quality Pool) 
Random 
33.70 27.33 
30.80 30.61 29.12 30.22 
34.08 31.14 
Ours 
32.93 27.60 
33.23 31.26 29.95 29.60 
41.37 33.64 </p>
<p>Cross-domain Transfer 
Wiki⇒Multi-Domain 
32.90 26.73 
31.87 30.50 25.08 33.27 
37.85 32.07 
WMT⇒Multi-Domain 30.87 25.37 
31.43 29.22 12.98 30.34 
34.80 26.04 </p>
<p>Table 19 :
19Cross-domain transfer results on Multi-Domain Full sets under 1-shot prompting. We adopt the SemScorebased strategy for example selection using the default Wiki/WMT Full candidate pool. Results are averaged over 3 different demonstrations.
AcknowledgmentsThis work was funded by UK Research and Innovation (UKRI) under the UK government's Horizon Europe funding guarantee [grant number 10039436 -UTTER]. The computations described in this research were performed using the Baskerville Tier 2 HPC service (https://www.baskerville.ac.uk/). Baskerville was funded by the EPSRC and UKRI through the World Class Labs scheme (EP/T022221/1) and the Digital Research Infrastructure programme (EP/W032244/1) and is operated by Advanced Research Computing at the University of Birmingham.Method
A call for clarity in reporting BLEU scores. Matt Post, Proceedings of the Third Conference on Machine Translation: Research Papers. the Third Conference on Machine Translation: Research PapersBelgium, BrusselsAssociation for Computational LinguisticsMatt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186- 191, Belgium, Brussels. Association for Computa- tional Linguistics.</p>
<p>COMET: A neural framework for MT evaluation. Ricardo Rei, Craig Stewart, Ana C Farinha, Alon Lavie, 10.18653/v1/2020.emnlp-main.213Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural frame- work for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online. Association for Computational Linguistics.</p>
<p>Wiki-Matrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia. Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, Francisco Guzmán, 10.18653/v1/2021.eacl-main.115Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational LinguisticsHolger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzmán. 2021. Wiki- Matrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1351-1361, Online. Association for Computational Linguistics.</p>
<p>Controlling politeness in neural machine translation via side constraints. Rico Sennrich, Barry Haddow, Alexandra Birch, 10.18653/v1/N16-1005Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsRico Sennrich, Barry Haddow, and Alexandra Birch. 2016a. Controlling politeness in neural machine translation via side constraints. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 35-40, San Diego, California. Association for Computational Linguistics.</p>
<p>Improving neural machine translation models with monolingual data. Rico Sennrich, Barry Haddow, Alexandra Birch, 10.18653/v1/P16-1009Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics1Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016b. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86-96, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Generating diverse translations with sentence codes. Raphael Shu, Hideki Nakayama, Kyunghyun Cho, 10.18653/v1/P19-1177Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsRaphael Shu, Hideki Nakayama, and Kyunghyun Cho. 2019. Generating diverse translations with sentence codes. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1823-1827, Florence, Italy. Association for Computational Linguistics.</p>
<p>An information-theoretic approach to prompt engineering without ground truth labels. Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, David Wingate, 10.18653/v1/2022.acl-long.60Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics1Taylor Sorensen, Joshua Robinson, Christopher Ryt- ting, Alexander Shaw, Kyle Rogers, Alexia De- lorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 819-862, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Rethinking document-level neural machine translation. Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian Huang, Jiajun Chen, Lei Li, 10.18653/v1/2022.findings-acl.279Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational LinguisticsZewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian Huang, Jiajun Chen, and Lei Li. 2022. Rethinking document-level neural machine translation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3537- 3548, Dublin, Ireland. Association for Computa- tional Linguistics.</p>
<p>Msp: Multi-stage prompting for making pre-trained language models better translators. Zhixing Tan, Xiangwen Zhang, Shuo Wang, Yang Liu, arXiv:2110.06609arXiv preprintZhixing Tan, Xiangwen Zhang, Shuo Wang, and Yang Liu. 2021. Msp: Multi-stage prompting for mak- ing pre-trained language models better translators. arXiv preprint arXiv:2110.06609.</p>
<p>Prompting palm for translation: Assessing strategies and performance. David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, George Foster, arXiv:2211.09102arXiv preprintDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2022. Prompting palm for translation: Assessing strategies and performance. arXiv preprint arXiv:2211.09102.</p>
<p>TransPrompt: Towards an automatic transferable prompting framework for few-shot text classification. Chengyu Wang, Jianing Wang, Minghui Qiu, Jun Huang, Ming Gao, 10.18653/v1/2021.emnlp-main.221Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican Republic. Association for Computational LinguisticsChengyu Wang, Jianing Wang, Minghui Qiu, Jun Huang, and Ming Gao. 2021. TransPrompt: To- wards an automatic transferable prompting frame- work for few-shot text classification. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2792-2802, Online and Punta Cana, Dominican Republic. Asso- ciation for Computational Linguistics.</p>
<p>. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, M Andrew, Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.</p>
<p>Finetuned language models are zero-shot learners. Dai, Le, International Conference on Learning Representations. Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682arXiv preprintet al. 2022b. Emergent abilities of large language modelsJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022b. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022c. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, 10.18653/v1/2021.naacl-main.41Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational LinguisticsLinting Xue, Noah Constant, Adam Roberts, Mi- hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A mas- sively multilingual pre-trained text-to-text trans- former. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, arXiv:2210.02414Yuxiao Dong, and Jie Tang. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprintAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. 2022. Glm- 130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</p>
<p>Improving massively multilingual neural machine translation and zero-shot translation. Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich, 10.18653/v1/2020.acl-main.148Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsBiao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020. Improving massively multilingual neural machine translation and zero-shot translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628-1639, Online. Association for Computational Linguistics.</p>
<p>Exploiting source-side monolingual data in neural machine translation. Jiajun Zhang, Chengqing Zong, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingJiajun Zhang and Chengqing Zong. 2016. Exploit- ing source-side monolingual data in neural ma- chine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1535-1545.</p>
<p>Guiding neural machine translation with retrieved translation pieces. Jingyi Zhang, Masao Utiyama, Eiichro Sumita, Graham Neubig, Satoshi Nakamura, 10.18653/v1/N18-1120Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaLong Papers1Association for Computational LinguisticsJingyi Zhang, Masao Utiyama, Eiichro Sumita, Gra- ham Neubig, and Satoshi Nakamura. 2018. Guid- ing neural machine translation with retrieved transla- tion pieces. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1325- 1335, New Orleans, Louisiana. Association for Com- putational Linguistics.</p>
<p>. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, arXiv:2205.01068Mona Diab, Xian Li, Xi Victoria LinarXiv preprintet al. 2022a. Opt: Open pre-trained transformer language modelsSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De- wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022a. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Active example selection for in-context learning. Yiming Zhang, Shi Feng, Chenhao Tan, arXiv:2211.04486arXiv preprintYiming Zhang, Shi Feng, and Chenhao Tan. 2022b. Active example selection for in-context learning. arXiv preprint arXiv:2211.04486.</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, PMLRProceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine Learning139Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improv- ing few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR.</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, arXiv:2205.10625Quoc Le, and Ed Chi. 2022arXiv preprintDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reason- ing in large language models. arXiv preprint arXiv:2205.10625.</p>
<p>50 1-shot Translation (Low-quality Pool. 36.73 20.53 22.23 8.23 34.63 26.13 24.75 66.82 34.15 10.11 -1.94 57.97 66.08 38.86 Ours 37.90 21.27 20.50 9.37 34.47 26.17 24.94 68.46 33.78 0.19 12.07 58.05 66.75 39.8829TLength 38.57 22.00 29.50 10.00 35.90 26.53 27.08 68.94 37.16 50.80 15.80 63.01 67.29 50.50 1-shot Translation (Low-quality Pool) Random 36.73 20.53 22.23 8.23 34.63 26.13 24.75 66.82 34.15 10.11 -1.94 57.97 66.08 38.86 Ours 37.90 21.27 20.50 9.37 34.47 26.17 24.94 68.46 33.78 0.19 12.07 58.05 66.75 39.88</p>            </div>
        </div>

    </div>
</body>
</html>
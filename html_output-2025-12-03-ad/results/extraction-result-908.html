<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-908 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-908</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-908</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-7139a5f730652abbeabf9e140009907d2c7da3e5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7139a5f730652abbeabf9e140009907d2c7da3e5" target="_blank">VirtualHome: Simulating Household Activities Via Programs</a></p>
                <p><strong>Paper Venue:</strong> 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> This paper crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code, and implements the most common atomic actions in the Unity3D game engine, and uses them to "drive" an artificial agent to execute tasks in a simulated household environment.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to "drive" an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language descriptions.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e908.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e908.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq-LSTM (text->program)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Encoder-Decoder LSTM with attention for Text-to-Program generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence model (LSTM encoder + LSTM decoder with attention) that translates a natural language description into a procedural program (sequence of atomic actions with object arguments); pretrained with cross-entropy and fine-tuned with policy-gradient RL using a longest-common-subsequence (LCS) reward and an executability (simulator) reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Seq2Seq-LSTM (text->program)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LSTM encoder (word2vec input) + LSTM decoder with attention; step-level output over a discrete instruction vocabulary (action + up to two object arguments) using learned instruction embeddings (W_a, W_o, W_v). Trained with standard teacher-forcing (MLE) then policy-gradient fine-tuning. Uses word2vec embeddings and attention over encoder states.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Text-to-Program generation and program execution in VirtualHome</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>procedural program induction / multi-step reasoning and sequential decision-making (program execution in an embodied simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>On VirtualHome Activity (synthesized programs + natural descriptions): MLE baseline mean LCS = 0.729, executability = 38.6%; PG(LCS) mean LCS = 0.767, executability = 35.5%; PG(LCS+Sim) mean LCS = 0.774, executability = 39.8%. On ActivityPrograms (real activities; not executable in simulator) MLE mean LCS = 0.410, PG(LCS) mean LCS = 0.447.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Encoder-decoder LSTM with attention; instruction embedding layer; word2vec input embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised pretraining (cross-entropy / MLE) followed by Reinforcement Learning (policy-gradient) with rewards derived from normalized LCS and a binary simulator-executability signal.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (RL reward shaping / hybrid approach combining supervised pretraining and RL)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>After MLE pretraining, the model is fine-tuned with policy-gradient RL using two rewards: r_LCS = normalized longest-common-subsequence between sampled program and ground-truth; r_sim = binary reward indicating if the sampled program is executable in the VirtualHome simulator. Total reward = r_LCS + 0.1 * r_sim. The RL variants evaluated are PG(LCS) and PG(LCS+Sim).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Compared to MLE: PG(LCS) increased mean LCS from 0.729 to 0.767 but lowered executability from 38.6% to 35.5%; adding the simulator reward (PG(LCS+Sim)) further increased mean LCS slightly to 0.774 and improved executability to 39.8% (i.e., executability recovered and slightly exceeded MLE). On real ActivityPrograms (non-executable), PG(LCS) improved mean LCS from 0.410 (MLE) to 0.447.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The authors note a trade-off: MLE tends to generate shorter (simpler) programs which are more likely to be executable (an empty/short program is trivially executable), whereas RL optimizing LCS encourages longer/more accurate sequences that can reduce executability; balancing LCS fidelity and executability via a simulator reward is needed. They also note that real-program executability is limited by simulator action coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VirtualHome: Simulating Household Activities Via Programs', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e908.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e908.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Video->Program (TRN + Seq2Seq)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal Relation Network (video clip encoder) plus Seq2Seq LSTM decoder for Video-to-Program generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A video-to-program pipeline that first partitions video into 2s clips, uses DilatedNet for segmentation and a Temporal Relation Network (TRN) to predict step embeddings per clip, then feeds the clip-level predictions into an LSTM encoder-decoder (with attention) to generate the full program; trained with MLE then policy-gradient RL with LCS and simulator rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Video->Program (DilatedNet + TRN + Seq2Seq)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Video pipeline: frames -> DilatedNet semantic segmentation -> TRN (Temporal Relation Network) with 4-frame relations to predict action+object+object embeddings per clip; clip predictions serve as encoder inputs to an LSTM encoder, with an LSTM decoder (attention) generating program steps. Training: supervised MLE then policy-gradient RL using LCS and simulator-based rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Video-to-Program generation and program execution in VirtualHome</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>procedural program induction from visual demonstration / multi-step reasoning and sequential decision-making (embodied execution)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>On VirtualHome Activity (video + synthesized programs): MLE: mean LCS = 0.478, executability = 19.4%; PG(LCS): mean LCS = 0.502, executability = 19.0%; PG(LCS+Sim): mean LCS = 0.495, executability = 22.4%. Breakdown for PG(LCS+Sim): seen homes mean LCS = 0.645, executability = 24.6%; unseen homes mean LCS = 0.389, executability = 20.9%. Clip-level classification: overall mean per-class accuracy (action/object/step) on test videos = 30.34% (seen homes 45.42%, unseen homes 19.12%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>DilatedNet semantic segmentation frontend; Temporal Relation Network (TRN) for clip-level step embedding; LSTM encoder-decoder with attention; instruction embedding layers; use of simulator for executability signal in RL.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised MLE pretraining for both clip-level classifiers and seq2seq, followed by policy-gradient RL fine-tuning of seq2seq with LCS reward and optional simulator executability reward.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (RL reward shaping / hybrid approach combining supervised pretraining and RL); use of simulator-based reward as an auxiliary signal.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Fine-tuning the seq2seq model using policy-gradient where the reward is a combination of normalized LCS between predicted and ground-truth programs and a binary executability reward from the VirtualHome simulator; compared PG(LCS) and PG(LCS+Sim).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>PG(LCS) improved mean LCS over MLE (0.502 vs 0.478) but slightly decreased executability (19.0% vs 19.4%); adding simulator reward (PG(LCS+Sim)) recovered some LCS and improved executability to 22.4%. Performance degrades markedly on unseen homes vs seen homes (mean LCS 0.389 vs 0.645 under PG(LCS+Sim)).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper attributes lower video-based performance to high variability (many actions and objects) making clip-level prediction challenging, and to generalization gaps across environments (seen vs unseen homes). They also explain the RL-vs-MLE trade-off: MLE tends to produce shorter/easier-to-execute programs, whereas RL with LCS yields higher fidelity sequences but can reduce executability unless a simulator reward is included.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VirtualHome: Simulating Household Activities Via Programs', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inferring and executing programs for visual reasoning <em>(Rating: 2)</em></li>
                <li>AI2-THOR: an interactive 3d environment for visual AI <em>(Rating: 2)</em></li>
                <li>HOME: a household multimodal environment <em>(Rating: 2)</em></li>
                <li>Self-critical sequence training for image captioning <em>(Rating: 1)</em></li>
                <li>Sequence to sequence learning with neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-908",
    "paper_id": "paper-7139a5f730652abbeabf9e140009907d2c7da3e5",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "Seq2Seq-LSTM (text-&gt;program)",
            "name_full": "Encoder-Decoder LSTM with attention for Text-to-Program generation",
            "brief_description": "A sequence-to-sequence model (LSTM encoder + LSTM decoder with attention) that translates a natural language description into a procedural program (sequence of atomic actions with object arguments); pretrained with cross-entropy and fine-tuned with policy-gradient RL using a longest-common-subsequence (LCS) reward and an executability (simulator) reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Seq2Seq-LSTM (text-&gt;program)",
            "model_description": "LSTM encoder (word2vec input) + LSTM decoder with attention; step-level output over a discrete instruction vocabulary (action + up to two object arguments) using learned instruction embeddings (W_a, W_o, W_v). Trained with standard teacher-forcing (MLE) then policy-gradient fine-tuning. Uses word2vec embeddings and attention over encoder states.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Text-to-Program generation and program execution in VirtualHome",
            "interactive_task_type": "procedural program induction / multi-step reasoning and sequential decision-making (program execution in an embodied simulator)",
            "interactive_performance": "On VirtualHome Activity (synthesized programs + natural descriptions): MLE baseline mean LCS = 0.729, executability = 38.6%; PG(LCS) mean LCS = 0.767, executability = 35.5%; PG(LCS+Sim) mean LCS = 0.774, executability = 39.8%. On ActivityPrograms (real activities; not executable in simulator) MLE mean LCS = 0.410, PG(LCS) mean LCS = 0.447.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": false,
            "architectural_features": "Encoder-decoder LSTM with attention; instruction embedding layer; word2vec input embeddings.",
            "training_method": "Supervised pretraining (cross-entropy / MLE) followed by Reinforcement Learning (policy-gradient) with rewards derived from normalized LCS and a binary simulator-executability signal.",
            "intervention_type": "training method (RL reward shaping / hybrid approach combining supervised pretraining and RL)",
            "intervention_description": "After MLE pretraining, the model is fine-tuned with policy-gradient RL using two rewards: r_LCS = normalized longest-common-subsequence between sampled program and ground-truth; r_sim = binary reward indicating if the sampled program is executable in the VirtualHome simulator. Total reward = r_LCS + 0.1 * r_sim. The RL variants evaluated are PG(LCS) and PG(LCS+Sim).",
            "intervention_effect": "Compared to MLE: PG(LCS) increased mean LCS from 0.729 to 0.767 but lowered executability from 38.6% to 35.5%; adding the simulator reward (PG(LCS+Sim)) further increased mean LCS slightly to 0.774 and improved executability to 39.8% (i.e., executability recovered and slightly exceeded MLE). On real ActivityPrograms (non-executable), PG(LCS) improved mean LCS from 0.410 (MLE) to 0.447.",
            "hypothesized_cause_of_gap": "The authors note a trade-off: MLE tends to generate shorter (simpler) programs which are more likely to be executable (an empty/short program is trivially executable), whereas RL optimizing LCS encourages longer/more accurate sequences that can reduce executability; balancing LCS fidelity and executability via a simulator reward is needed. They also note that real-program executability is limited by simulator action coverage.",
            "uuid": "e908.0",
            "source_info": {
                "paper_title": "VirtualHome: Simulating Household Activities Via Programs",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Video-&gt;Program (TRN + Seq2Seq)",
            "name_full": "Temporal Relation Network (video clip encoder) plus Seq2Seq LSTM decoder for Video-to-Program generation",
            "brief_description": "A video-to-program pipeline that first partitions video into 2s clips, uses DilatedNet for segmentation and a Temporal Relation Network (TRN) to predict step embeddings per clip, then feeds the clip-level predictions into an LSTM encoder-decoder (with attention) to generate the full program; trained with MLE then policy-gradient RL with LCS and simulator rewards.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Video-&gt;Program (DilatedNet + TRN + Seq2Seq)",
            "model_description": "Video pipeline: frames -&gt; DilatedNet semantic segmentation -&gt; TRN (Temporal Relation Network) with 4-frame relations to predict action+object+object embeddings per clip; clip predictions serve as encoder inputs to an LSTM encoder, with an LSTM decoder (attention) generating program steps. Training: supervised MLE then policy-gradient RL using LCS and simulator-based rewards.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Video-to-Program generation and program execution in VirtualHome",
            "interactive_task_type": "procedural program induction from visual demonstration / multi-step reasoning and sequential decision-making (embodied execution)",
            "interactive_performance": "On VirtualHome Activity (video + synthesized programs): MLE: mean LCS = 0.478, executability = 19.4%; PG(LCS): mean LCS = 0.502, executability = 19.0%; PG(LCS+Sim): mean LCS = 0.495, executability = 22.4%. Breakdown for PG(LCS+Sim): seen homes mean LCS = 0.645, executability = 24.6%; unseen homes mean LCS = 0.389, executability = 20.9%. Clip-level classification: overall mean per-class accuracy (action/object/step) on test videos = 30.34% (seen homes 45.42%, unseen homes 19.12%).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": false,
            "architectural_features": "DilatedNet semantic segmentation frontend; Temporal Relation Network (TRN) for clip-level step embedding; LSTM encoder-decoder with attention; instruction embedding layers; use of simulator for executability signal in RL.",
            "training_method": "Supervised MLE pretraining for both clip-level classifiers and seq2seq, followed by policy-gradient RL fine-tuning of seq2seq with LCS reward and optional simulator executability reward.",
            "intervention_type": "training method (RL reward shaping / hybrid approach combining supervised pretraining and RL); use of simulator-based reward as an auxiliary signal.",
            "intervention_description": "Fine-tuning the seq2seq model using policy-gradient where the reward is a combination of normalized LCS between predicted and ground-truth programs and a binary executability reward from the VirtualHome simulator; compared PG(LCS) and PG(LCS+Sim).",
            "intervention_effect": "PG(LCS) improved mean LCS over MLE (0.502 vs 0.478) but slightly decreased executability (19.0% vs 19.4%); adding simulator reward (PG(LCS+Sim)) recovered some LCS and improved executability to 22.4%. Performance degrades markedly on unseen homes vs seen homes (mean LCS 0.389 vs 0.645 under PG(LCS+Sim)).",
            "hypothesized_cause_of_gap": "The paper attributes lower video-based performance to high variability (many actions and objects) making clip-level prediction challenging, and to generalization gaps across environments (seen vs unseen homes). They also explain the RL-vs-MLE trade-off: MLE tends to produce shorter/easier-to-execute programs, whereas RL with LCS yields higher fidelity sequences but can reduce executability unless a simulator reward is included.",
            "uuid": "e908.1",
            "source_info": {
                "paper_title": "VirtualHome: Simulating Household Activities Via Programs",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inferring and executing programs for visual reasoning",
            "rating": 2
        },
        {
            "paper_title": "AI2-THOR: an interactive 3d environment for visual AI",
            "rating": 2
        },
        {
            "paper_title": "HOME: a household multimodal environment",
            "rating": 2
        },
        {
            "paper_title": "Self-critical sequence training for image captioning",
            "rating": 1
        },
        {
            "paper_title": "Sequence to sequence learning with neural networks",
            "rating": 1
        }
    ],
    "cost": 0.0105165,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>VirtualHome: Simulating Household Activities via Programs</h1>
<p>Xavier Puig ${ }^{1}$, Kevin Ra ${ }^{2}$, Marko Boben ${ }^{3}$, Jiaman $\mathrm{Li}^{4}$, Tingwu Wang ${ }^{4}$,<br>Sanja Fidler ${ }^{4}$, Antonio Torralba ${ }^{1}$<br>${ }^{1}$ MIT ${ }^{2}$ McGill University ${ }^{3}$ University of Ljubljana ${ }^{4}$ University of Toronto<br>{xavierpuig, torralba}@csail.mit.edu kevin.ra@mail.mcgill.ca marko.boben@fri.uni-lj.si<br>{tingwuwang,ljm,fidler}@cs.toronto.edu</p>
<h4>Abstract</h4>
<p>In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to "drive" an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language descriptions.</p>
<h2>1. Introduction</h2>
<p>Autonomous agents need to know the sequences of actions that need to be performed in order to achieve certain goals. For example, we might want a robot to clean our room, make the bed, or cook dinner. One can define activities with procedural recipes or programs that describe how one can accomplish the task. A program contains a sequence of simple symbolic instructions, each referencing an atomic action (e.g. "sit") or interaction (e.g. "pick-up object") and a number of objects that the action refers to (e.g., "pick-up juice"). Assuming that an agent knows how to execute the atomic actions, programs provide an effective means of "driving" a robot to perform different, more complex tasks. Programs</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We first crowdsource a large knowledge base of household tasks, (top). Each task has a high level name, and a natural language instruction. We then collect "programs" for these tasks, (middle left), where the annotators "translate" the instruction into simple code. We implement the most frequent (inter)actions in a 3D simulator, called VirtualHouse, allowing us to drive an agent to execute tasks defined by programs. We propose methods to generate programs automatically from text (top) and video (bottom), thus driving an agent via language and a video demonstration.
can also be used as an internal representation of an activity shown in a video or described by a human (or another agent). Our goal in this paper is to automatically generate programs from natural language descriptions, as well as from video demonstrations, potentially allowing naive users to teach their robot a wide variety of novel tasks.</p>
<p>Towards this goal, one important missing piece is the lack of a database describing activities composed of multiple steps. We first crowdsource common-sense information about typical activities that happen in people's homes, forming the natural language know-how of how these activities are performed. We then adapt the Scratch [1] interface used for teaching kids how to code in order to collect programs that formalize the activity as described in the knowledge base. Note that these programs include all the steps required for the robot to accomplish a task, even those that are not mentioned in the language descriptions. We then implement the most common atomic (inter)actions in the Unity3D game engine, such as pick-up, switch on/off, sit, stand-up. By exploiting the physics, navigation and kinematic models in the</p>
<p>game engine we enable an artificial agent to execute these programs in a simulated household environment.</p>
<p>We first introduce our data collection effort and the program based representation of activities. In Sec. 5 we show how we can learn to automatically translate natural language instructions of activities into programs. In Sec. 4 we introduce the VirtualHome simulator that allows us to create a large activity video dataset with rich ground-truth by using programs to drive an agent in a synthetic world. Finally, we use the synthetic videos to train a system to translate videos of activities into the program being executed by the agent. Our VirtualHome opens an important "playground" for both vision and robotics, allowing agents to exploit language and visual demonstration to execute novel activities in a simulated environment. Our data is available online: http://virtual-home.org/.</p>
<h2>2. Related Work</h2>
<p>Actions as programs. A few works have defined activities as programs. In [30], the authors detect objects and actions in cooking videos and generate an "action plan" using a probabilistic grammar. By generating the plan, the robots were able to execute complex actions by simply watching videos. These authors further collected a tree bank of action plans from annotated cooking videos [29], creating a knowledge base of actions as programs for cooking. [18] tried to translate cooking recipes into action plans using an MRF. $[23,3]$ also argued for actions as a sequence of atomic steps. They aligned YouTube how-to videos with their narrations in order to parse videos into such programs. Most of these works were limited to either a small set of activities, or to a narrow domain (cooking). We go beyond this by creating a knowledge base about an exhaustive set of activities and tasks that people do in their homes.
[25] crowd-sourced scripts of people's actions at home in the form of natural language. These were mostly comprised of one or two sentences describing a short sequence of actions. While this is valuable information, language is very versatile and thus hard to convert into a usable program on a robot. We show how to do this in our work.</p>
<p>Code generation. There is increased interest in generating and interpreting source code [13]. Work most relevant to ours produces code given natural language inputs. [4] retrieves code snippets from Stackoverflow based on language queries. Given a sentence describing conditions, [19] produces If-This-Then-That code. [14] generates a program specifying the logic of a card game given a short description of the rules. In [9], the authors inferred programs to answer visual questions about images. Our work differs in the domain, and works with text or video as input.</p>
<p>Robotics. A subfield of robotics aims at teaching robots to follow instructions provided in natural language by a human tutor. However, most of the existing literature deals
with a constrained problem, for example, they learn to translate navigational instructions into a sequence of robotic actions [27, 15, 12, 16]. These instructions are typically simpler as they directly mention what to do next, and the action space is small. This is not the case in our work which also considers interactions with objects, and everyday activities which are typically far more complex.</p>
<p>Simulation. Simulations using game engines have been developed to facilitate training visual models for autonomous driving [8, 21, 7], quadcopter flying [24], or other robotic tasks [5]. Recent works have focused on simulating indoor environments, allowing for target-driven indoor navigation or interactive question answering [11, 28, 6, 22]. A few of these works [11, 6] include actionable objects, allowing to interact and change the environment. Our work focuses on simulating a wide range of human actions, both in terms of objects interactions and human poses, which allows to simulate common activities. We are not aware of simulators at the scale of objects and actions in a home, like ours. Lastly, we give credit to the popular game Sims which we draw our inspiration from. Sims is a strategic video game mimicking daily household activities. Unfortunately, the source of the game is not public and thus cannot be used for our purpose.</p>
<h2>3. KB of Household Activities for Robots</h2>
<p>Our goal is to build a large repository of common activities and tasks that we perform in our households in our daily lives. These tasks can include simple actions like "turning on TV" or complex ones such as "make coffee with milk". What makes our effort unique is that we are interested in collecting this information for robots. Unlike humans, robots need more direct instructions. For example, in order to "watch tv", one might describe it (to a human) as "Switch on the television, and watch it from the sofa". Here, the actions "grab remote control" and "sit/lie on sofa" have been omitted, since they are part of the commonsense knowledge that humans have. In our work, we aim to collect all the steps required for a robot to successfully execute a task, including the commonsense steps. In particular, we want to collect programs that fully describe activities.</p>
<p>Describing actions as programs has the advantage that it provides a clear and non-ambiguous description of all the steps needed to complete a task. Such programs can then be used to instruct a robot or a virtual character. Programs can also be used as a representation of a complex task that involves a number of simpler actions, providing a way to understand and compare activities and goals.</p>
<h3>3.1. Data Collection</h3>
<p>In this section, we describe our dataset collection using crowdsourcing. Describing actions as programs can be a challenging task as most annotators have no programing experience. We split the data collection effort in two parts.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: VirtualHome Activity Dataset is a video dataset of composite activities created with our simulator. We start by generating programs using a simple probabilistic grammar. We animate each program in VirtualHome by randomizing the selection of homes, agents, cameras, as well as the placement of a subset of the objects, the initial location of the agent, the speed of the actions, and choice of objects for interactions. Each program is shown to an annotator who is asked to describe it in natural language (top row). Videos have ground-truth: (second row) time-stamp for each atomic action, (bottom) 2D and 3D pose, class and object instance segmentation, depth and optical flow.</p>
<p>In the first part, we ask AMT workers to provide verbal descriptions of daily household activities. In particular, each worker is asked to come up with a common activity/task, give it a high level name, eg "make coffee", and describe it in detail. In order to cover a wide spectrum of activities we pre-specified in which scene the activity should start. Scenes were selected randomly from a list of 8 scenes (<em>living room</em>, <em>kitchen</em>, <em>dining room</em>, <em>bedroom</em>, <em>kids bedroom</em>, <em>bathroom</em>, <em>entrance hall</em>, and <em>home office</em>). An example of a described activity is shown in Fig. 3. Note that these descriptions may likely omit the commonsense steps, as they were written by "naive" workers that were describing these activities as they would to a (human) friend.</p>
<p>In the second stage, we showed the collected descriptions to the AMT workers and asked them to translate these descriptions into programs using a graphical programming language. We told them to produce a program that will "drive" a robot to successfully accomplish the described activity. Our interface builds on top of MIT's Scratch project [1] designed to teach young children to write symbolic code. We found that workers were capable of quickly learning to produce useful programs by providing them with a carefully designed tutorial. Fig. 3 shows a snapshot of the programming interface. Finally, we asked more qualified workers hired via Upwork crowdsourcing platform to double check the collected data.</p>
<p>Workers had to compose a program by composing a sequence of steps. Each instruction is a Scratch block from a predefined list of 77 possible blocks compiled by analyzing the frequency of verbs in the collected descriptions. Each step in the program is defined by a block. A block defines a syntactic frame with an action and a list of arguments (e.g., the block <em>walk</em> requires one argument to specify the destination, Fig. 3.c). To simplify the search for blocks they are organized according to 9 broad action categories (Fig. 3.b).</p>
<p>We required that the program contains all the steps, even those not explicitly mentioned in the description, but that could be inferred from common-sense. Fig. 3.d shows an example of a program. We also allowed annotators to use a "special" block for missing actions, where the step can be written as free-form text. Programs using this special block will not be used in the rest of the paper, but allowed us in identifying new blocks that needed to be added.</p>
<p>More precisely, step t in the program can be written as:</p>
<p>$$
\text{step}<em t_1="t,1">t = [\text{action}_t] \quad \langle \text{object}</em>} \rangle (id_{t,1}) \dots \langle \text{object<em t_n="t,n">{t,n} \rangle (id</em>)
$$</p>
<p>Here, id is an unique identifier (counter) of an object and helps in disambiguating different instances of objects that belong to the same class. An example of a program for "watch tv" would be:</p>
<div class="codehilite"><pre><span></span><code><span class="n">step</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">Walk</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">TELEVISION</span>
<span class="n">step</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">SwitchOn</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">TELEVISION</span>
<span class="n">step</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">Walk</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">SOFA</span>
<span class="n">step</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">Sit</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">SOFA</span>
<span class="n">step</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">Watch</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">TELEVISION</span>
</code></pre></div>

<p>Here, the programs defines that the television in steps 1, 2 and 5 refer to the same object instance.</p>
<h3>3.2. Dataset Analysis</h3>
<p>In the first part we collected 1814 descriptions. From those, we were able to collect programs for 1703 descriptions. Some of the programs contained several "special blocks" for missing actions, which we remove, resulting in 1257 programs. We finally selected a set of tasks and asked workers to write programs for them, obtaining 1564 additional programs. The resulting 2821 programs form our <em>ActivityPrograms</em> dataset. On average, the collected descriptions have 3.2 sentences and 21.9 words, and the resulting programs have 11.6 steps on average. The dataset statistics are summarized in Table 1.a.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: a) Description provided by a worker. b) User interface showing the list of block categories and 4 example blocks, c) Example of composition of a block by adding the arguments. Each block is like a Lego piece where the user can drop arguments inside and attach one block to another. d) Final program corresponding to the description from (a).</p>
<p>The dataset covers 75 atomic actions and 308 objects, making 2709 unique steps. Fig. 4.a shows a histogram of the 50 most common actions appearing in the dataset, and, Fig. 4.b, the 50 most common objects.</p>
<p>Our dataset contains activities with several examples, and we analyze their diversity by comparing their programs. Table 1.b analyzes 4 selected activities. We compute their similarities as the average length of the longest common subsequences computed between all pairs of programs.</p>
<p>We can also measure distances between activities by measuring the distance between programs. The similarity between two programs is measured as the length of their longest common subsequence of instructions divided by the length of the longest program. Table 1.c. shows the similarity matrix (sorted to better show the block diagonal structure) between different activities in our dataset.</p>
<p><strong>Completeness of programs.</strong> We analyze whether the collected programs contain all the necessary steps to execute the given task. We sample 100 collected programs and ask 5 AMT workers to rate whether the program is complete, missing minor steps (sitting in a chair before walking towards it) or important steps (filling a glass before drinking). Results show that 64% of the programs are complete, 28% are missing minor steps and 8% are missing crucial steps.</p>
<h3>4. VirtualHome: Simulator of Household Tasks</h3>
<p>The main motivation behind using programs to represent activities is to "drive" robots to perform tasks by having them executing these programs. As a proxy, we here use programs to drive characters in a simulated 3D environment. Simulations are useful as they define a playground for "robots", an environment where artificial agents can be taught to perform tasks. Here, we focus on building the simulator, and leave learning inside the simulator to future work. In particular, we will assume the agent has access to all 3D and semantic information about the environment, as well as to manually defined animations. Our focus will be to show that programs represent a good way of instructing such agents. Furthermore, our simulator will allow us to generate a large-scale video dataset of complex activities that is rich and diverse. We can create such a dataset by simply recording the agent executing programs in the simulator. The simulator then provides us with dense ground-truth information, eg semantic</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: a) Counts of actions in our <em>ActivityPrograms</em> dataset, b) object counts (zoom to read)</p>
<p>segmentation, depth, pose, etc. Fig. 2 showcases this dataset.</p>
<p>We implemented our <em>VirtualHome</em> simulator using the Unity3D game engine which allows us to exploit its kinematic, physics and navigation models, as well as user-contributed 3D models available through Unity's Assets store. We obtained six furnished homes and 4 rigged humanoid models from the web. On average, each home contains 357 object instances (86 per room). We collected objects from additional 30 object classes that appear in our collected programs yet are not available in the package, via the 3D warehouse [1]. To ensure visual diversity, we collected at least 3 different models per class. The apartments and agents are shown in Fig. 5 and Fig. 6.</p>
<h3>4.1. Animating Programs in VirtualHome</h3>
<p>Every step in the program requires us to animate the corresponding (inter)action in our virtual environment. We thus need to both, determine which object in the home (which we refer to as the <em>game object</em>) the step requires as well as properly animating the action. To get the former we need to solve an optimization problem by taking into account all steps in the program and finding a feasible path. For example, if the program requires the agent to switch on a computer and type on a keyboard, ideally the agent would type on the keyboard next to the chosen computer and not navigate to another keyboard attached to a different computer in possibly a different room. We now describe our simulator in more detail.</p>
<p><strong>Animating atomic actions.</strong> There is a huge variety and number of atomic actions that appear in the collected programs, as can be seen in Fig. 4. We implemented the 12 most frequent ones: <em>walk/run</em>, <em>grab</em>, <em>switch-on/off</em>, <em>open/close</em>, <em>place</em>, <em>look-at</em>, <em>sit/standup</em>, <em>touch</em>. Note that there is a large variability in how an action is performed depending on to which object it is applied to (e.g., opening a fridge is different than opening a drawer). We use Unity's NavMesh framework for navigation (path planner to avoid obstacles). For each action we compute the agent's target pose and animate the action using RootMotion FinalIK inverse kinematics package. We further animate certain objects the agent interacts with, e.g., we shake a coffee maker, animate toast in a toaster, show a (random) photo on a computer or TV</p>
<p><sup>1</sup>https://3dwarehouse.sketchup.com</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset</td>
<td># prog.</td>
<td>avg # steps</td>
<td>avg # sent.</td>
<td>avg # words</td>
<td></td>
</tr>
<tr>
<td>ActivityProg.</td>
<td>2821</td>
<td>11.6</td>
<td>3.2</td>
<td>21.9</td>
<td></td>
</tr>
<tr>
<td>SyntheticProg.</td>
<td>5193</td>
<td>9.6</td>
<td>3.4</td>
<td>20.5</td>
<td></td>
</tr>
</tbody>
</table>
<p>(a)</p>
<table>
<thead>
<tr>
<th>Action</th>
<th># Prog.</th>
<th>LCS</th>
<th>Norm. LCS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Make coffee</td>
<td>69</td>
<td>4.56</td>
<td>0.26</td>
</tr>
<tr>
<td>Fold laundry</td>
<td>11</td>
<td>1.29</td>
<td>0.08</td>
</tr>
<tr>
<td>Watch TV</td>
<td>128</td>
<td>3.65</td>
<td>0.40</td>
</tr>
<tr>
<td>Clean</td>
<td>42</td>
<td>0.76</td>
<td>0.04</td>
</tr>
</tbody>
</table>
<p>(b)</p>
<p>Table 1: (a) We analyze programs and natural language descriptions for both, real activities in ActivityPrograms (Sec. 3), and synthesized programs (with real descr.). (b) ActivityPrograms: Analyzing diversity in the same activity, by computing similarities across all pairs of the collected programs. LCS" denotes longest common subsequence. For norm.LCS", we normalize by max length of the two programs. (c) shows the similarity matrix (sorted to better show the block diagonal structure) between different activities in our dataset.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: 3D households in our VirtualHome. Notice the diversity in room and object layout and appearance. Each home has on average 357 objects. First 4 scenes are used for training, the fifth is also used in val, and all scenes are used when testing our video-to-script model.</p>
<p>Figure 6: Agents in VirtualHome. We use male 1 and female 1 in train., and all agents when testing our video-to-program model.</p>
<p>screen, light up a burner on a stove, and light up the lamps in the room, when these objects are switched on by the agent.</p>
<p><strong>Preparing the Scene.</strong> While every 3D home already contains many objects, the programs may still mention objects that are not present in the scene. To deal with this, we first "set" the scene by placing all missing objects that a program refers to in the home, before we try to execute the program. To be able to prepare a scene in a plausible way, we collect a knowledge base of possible object locations. The annotator is shown the class name and selects a list of other objects (including floor, wall) that are likely to support it.</p>
<p><strong>Executing a Program.</strong> To animate a program we need first to create a mapping between the objects in the program and the corresponding instances inside the virtual simulator. Furthermore, for each step in the program, we also need to compute the interaction position of the agent with respect to an object, and any additional information needed to animate the action (e.g., which hand to use, speed). We build a tree of all possibilities of assigning game objects to objects in the program, along with all interaction positions and attributes. To traverse the tree of possible states we use backtracking and stop as soon as a state executing the last step is found. Since the number of possible object mappings for each step is small, and we can prune the number of interaction positions to a few, our optimization runs in a few seconds, on average.</p>
<p><strong>Animation.</strong> We place 6-9 static cameras in each room, 26 per home on average. During recording, we switch between cameras based on agent's visibility. In particular, we randomly select a camera which sees the agent, and keep it until the agent is visible and within allowed distance. For agent-object interaction we also try to select a camera and adjust its field of view to enhance the visibility of the interaction. We further randomize the position, angle and field of view of each camera. Randomization is important when creating a dataset to ensure diversity of the final video data.</p>
<p><strong>VirtualHome Activity dataset.</strong> Since the programs in ActivityPrograms represent real activities that happen in households, they contain significant variability in actions and objects that appear in steps. While our ultimate aim is to be able to animate all these actions in our simulator, our current efforts only support the top 12 most frequent actions. We thus create another dataset that contains programs containing only these actions in their steps. The creation of this dataset is explained below.</p>
<p>We synthesized 5,193 programs using a simple probabilistic grammar, and had each one described in natural language by a human annotator. Although these programs were not given by annotators, they produced reasonable activities, creating a much larger dataset of paired descriptions-programs at a fraction of the cost. We then animated each program in our simulator, and automatically generated ground-truth which allows us to train and evaluate our video models. As can be seen from Table 1, descriptions in VirtualHome Activity dataset are of comparable length. However, the vocabulary here was biased towards that used in programs.</p>
<p>We animate the programs as described above, by randomizing the selection of home, an agent, cameras, placement of a subset of objects, initial location of the agent, speed of the actions, and choice of objects for interactions. We build on top of [2] to automatically generate groundtruth: 1) time-stamp of each step to video, 2) agent's 2D/3D pose, 3) class and instance segmentation, 4) depth, 5) optical flow, 6)</p>
<p>camera parameters. Example of data is shown in Fig. 2.</p>
<h2>5. From Videos and Descriptions to Programs</h2>
<p>We introduce a novel task using our dataset. In particular, we aim to generate a program for the activity from either a natural language description or from a video demonstration. We treat the task of transcribing an input (description or video) into a program as a translation problem. We adapt the seq2seq model [26] for our task, and train it with Reinforcement Learning that exploits the reward from the simulator.</p>
<p>Our model consists of an RNN encoder that encodes the input sequence into a hidden vector representation, and another RNN acting as a decoder, generating one step of the program at a time. We use LSTM with 100-dim hidden states as our encoder. At each step $t$, our RNN decoder decodes a step which takes the form of eq. (1). Let $\mathbf{x}<em i="i">{t}$ denote an input vector to our RNN decoder at step $t$, and let $h^{t}$ be the hidden state. Here, $h^{t}$ is computed as in the standard LSTM using $\tanh$ as the non-linearity. Let $a</em>$ of an instruction $i$ at step $t$ as:}$ be a one-hot encoding of an action $i$, and $o_{i}$ a one-hot encoding of an object. We compute the probability $p_{i}^{t</p>
<p>$$
\begin{aligned}
\tilde{a}<em a="a">{i}=W</em>} a_{i}, \quad \tilde{o<em o="o">{i, n} &amp; =W</em>} o_{i, n}, v_{i}=\operatorname{mean}\left(\tilde{a<em 1="1" i_="i,">{i}, \tilde{o}</em>}, \ldots, \tilde{o<em i="i">{i, n}\right) \
p</em>}^{t} &amp; =\operatorname{softmax<em i="i">{i}\left(\frac{v</em>\right)\right)
\end{aligned}
$$}}{\left|v_{i}\right|}^{T} \cdot W_{v}\left(h^{t} | \mathbf{x}_{t}^{a t t</p>
<p>where $W_{a}$ and $W_{o}$ and $W_{v}$ are learnable matrices, and $v_{i}$ denotes an embedding of an instruction. Note that here, $n$ is either 1 or 2 (our instructions have at most two objects).</p>
<p>The input vector $\mathbf{x}<em t="t">{t}$ concatenates multiple features. In particular, we use the embedding $v$ of the step with the highest probability from the previous time instance of the decoder. Following [26], we further use the attention mechanism over the encoder's states to get another feature $\mathbf{x}</em>$. In particular:}^{a t t</p>
<p>$$
\begin{aligned}
\alpha_{j}^{t} &amp; =\operatorname{softmax}<em a="a" t="t">{j}\left(v^{T}\left(W</em>\right)\right)\right) \
\mathbf{x}}\left(h^{t} | h_{e n c}^{j<em j="j">{t}^{a t t} &amp; =\sum</em>
\end{aligned}
$$} \alpha_{j}^{t} h_{e n c}^{j</p>
<p>where $W_{a t t}, v$ are learnable parameters. Our full model is visualized in Fig. 7.</p>
<p>Learning and inference. Our goal is to generate programs that are both close to the ground-truth programs in terms of their LCS (longest common subsequence) and are also executable by our renderer. To that end, we train our model in two phases. Firstly, we pre-train the model using cross-entropy loss at each time step of the RNN decoder. We follow the typical training strategy where we make a prediction at each time instance but feed in the ground-truth step to the next time instance. We use the word2vec [17] embeddings for matrices $W_{a}$ and $W_{o}$.</p>
<p>In the second stage, we treat program generation as an Reinforcement Learning problem, where the agent is learning a policy that generates steps to compose a program.</p>
<p>We follow [20], and use policy gradient optimization to train the model, using the greedy policy as the baseline estimator. We exploit two different kinds of reward $r\left(w^{s}, g\right)$ for RL training, where $w^{s}$ denotes the sampled program, and $g$ the ground-truth program. To ensure that the generated program is semantically correct (follows the description/video), we use the normalized LCS metric (length of the longest common subsequence) between the two programs as our first reward $r_{L C S}\left(w^{s}, g\right)$. The second reward comes from our simulator, and measures whether the generated program is executable or not. This reward, $r_{\text {sim }}\left(w^{s}\right)$, is a simple binary value. We carefully balance the total reward as, $r\left(w^{s}, g\right)=r_{L C S}\left(w^{s}, g\right)+0.1 \cdot r_{\text {sim }}\left(w^{s}\right)$.</p>
<p>So far we did not specify the input to the RNN encoder. Our model accepts either a language description or a video.</p>
<p>Textual Description. To encode a textual description our RNN encoder gets as input the word2vec [17] embedding of the word in the description at each time instance.</p>
<p>Video. To generate programs from videos, we partition each video into 2-second clips and train a model to predict the step at the middle frame. We use DilatedNet to obtain the semantic segmentation of each frame and use the Temporal Relation Network [31] with 4-frame relations to predict the embedding of an instruction (action+object+object). We use this embedding to obtain the likelihood of each instruction. The prediction at each clip is used as input to the RNN encoder for program generation.</p>
<h2>6. Experiments</h2>
<p>In our experiments we exploit both of our datasets: ActivityPrograms containing descriptions and programs for real activities, and VirtualHome Activity dataset that contains synthesized programs, yet natural descriptions to describe them. VirtualHome Activity dataset further contains videos animating the programs.</p>
<h3>6.1. Step (Instruction) Classification from Video</h3>
<p>We first evaluate our model for the task of video-based action and action-object-object (step/instruction in the program) classification. Here, we partition each video in 2-sec clips, and use the clip-based TRN to perform classification. We compute performance as the mean per-class accuracy across all 2-sec clips in test. To better understand the generalization properties of the video-based models, we further divide the test set into videos recorded in homes seen at train time, and videos in homes not seen at train time. We report the results in Table 2 (left). To set the lower bound, we also report a simple random retrieval baseline, in which a step is randomly retrieved from the training set. We can see that our model performs significantly better. However, a large number of actions and objects of interest, makes the prediction task challenging for the model.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Our encoder-decoder LSTM for generating programs from natural language descriptions or videos.</p>
<table>
<thead>
<tr>
<th></th>
<th>Action</th>
<th>Objects</th>
<th>Steps</th>
<th>Mean</th>
</tr>
</thead>
<tbody>
<tr>
<td>Rand. Retrieval</td>
<td>8.30%</td>
<td>1.50%</td>
<td>0.51%</td>
<td>3.43%</td>
</tr>
<tr>
<td>Seen homes</td>
<td>70.32 %</td>
<td>42.14 %</td>
<td>23.81 %</td>
<td>45.42%</td>
</tr>
<tr>
<td>Unseen homes</td>
<td>31.34%</td>
<td>14.55%</td>
<td>11.48%</td>
<td>19.12%</td>
</tr>
<tr>
<td>All</td>
<td>46.85%</td>
<td>25.76%</td>
<td>18.41%</td>
<td>30.34%</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th></th>
<th>Action</th>
<th>Objects</th>
<th>Steps</th>
<th>Mean</th>
<th>Simulator</th>
</tr>
</thead>
<tbody>
<tr>
<td>Rand. Retrieval</td>
<td>.473</td>
<td>.079</td>
<td>.071</td>
<td>.207</td>
<td>100.0%</td>
</tr>
<tr>
<td>MLE</td>
<td>.735</td>
<td>.359</td>
<td>.341</td>
<td>.478</td>
<td>19.4%</td>
</tr>
<tr>
<td>PG(LCS)</td>
<td>.761</td>
<td>.383</td>
<td>.364</td>
<td>.502</td>
<td>19.0%</td>
</tr>
<tr>
<td>PG(LCS+Sim)</td>
<td>.751</td>
<td>.377</td>
<td>.358</td>
<td>.495</td>
<td>22.4%</td>
</tr>
<tr>
<td>PG(LCS+Sim) Seen homes</td>
<td>.851</td>
<td>.556</td>
<td>.528</td>
<td>.645</td>
<td>24.6%</td>
</tr>
<tr>
<td>PG(LCS+Sim) Unseen homes</td>
<td>.680</td>
<td>.250</td>
<td>.236</td>
<td>.389</td>
<td>20.9%</td>
</tr>
</tbody>
</table>
<p>Table 2: Left: Accuracy of <em>video-based action classification</em> and <em>action-object-object</em> (step or instruction in the program) prediction in 2-sec clips from our VirtualHome Activity dataset. Right: <em>Video-based program generation</em>.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Action</th>
<th>Objects</th>
<th>Steps</th>
<th>Mean</th>
<th>Simulator (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Rand. Sampling</td>
<td>.226</td>
<td>.039</td>
<td>.020</td>
<td>.095</td>
<td>0.6%</td>
</tr>
<tr>
<td>Rand. Retrieval</td>
<td>.473</td>
<td>.079</td>
<td>.071</td>
<td>.207</td>
<td>100.0%</td>
</tr>
<tr>
<td>Skipthoughts</td>
<td>.642</td>
<td>.272</td>
<td>.252</td>
<td>.389</td>
<td>100.0%</td>
</tr>
<tr>
<td>MLE</td>
<td>.777</td>
<td>.723</td>
<td>.686</td>
<td>.729</td>
<td>38.6%</td>
</tr>
<tr>
<td>PG(LCS)</td>
<td>.803</td>
<td>.766</td>
<td>.732</td>
<td>.767</td>
<td>35.5%</td>
</tr>
<tr>
<td>PG(LCS+Sim)</td>
<td>.806</td>
<td>.775</td>
<td>.740</td>
<td>.774</td>
<td>39.8%</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Method</th>
<th>Action</th>
<th>Objects</th>
<th>Steps</th>
<th>Mean</th>
</tr>
</thead>
<tbody>
<tr>
<td>Rand. Sampling</td>
<td>.106</td>
<td>.018</td>
<td>.004</td>
<td>.043</td>
</tr>
<tr>
<td>Rand. Retrieval</td>
<td>.320</td>
<td>.037</td>
<td>.032</td>
<td>.130</td>
</tr>
<tr>
<td>Skipthoughts</td>
<td>.469</td>
<td>.297</td>
<td>.266</td>
<td>.344</td>
</tr>
<tr>
<td>MLE</td>
<td>.497</td>
<td>.392</td>
<td>.340</td>
<td>.410</td>
</tr>
<tr>
<td>PG(LCS)</td>
<td>.522</td>
<td>.433</td>
<td>.387</td>
<td>.447</td>
</tr>
</tbody>
</table>
<p>Table 3: Programs from descr.: Accuracy on (left) <em>VirtualHome Act.</em>, and (right) <em>ActivityPrograms</em>. We compute the length of longest common subsequence between a predicted script and GT and divide by max length of the two programs, mimicking IoU for programs. Since real programs are mainly not executable in our simulator due to the lack of implemented actions, we cannot report the executability metric.</p>
<h3>6.2. Program Generation</h3>
<p>We now evaluate the task of program generation.</p>
<p>Metrics. We evaluate program induction using a measure similar to IOU. We compute the longest common subsequence between a GT and a predicted program, where we allow gaps between the matching steps, but require their order to be correct. We obtain accuracy as the length of the subsequence divided by the max of the two programs' lengths. We also compute accuracies for <em>actions</em> and <em>objects</em> alone. Since LCS does not measure whether the program is valid, we report another metric that computes the percentage of generated programs that are executable in our simulator.</p>
<p>Language-based prediction. Since we have descriptions for all activities, we first evaluate how well our model translates natural language descriptions into programs. We report results on <em>ActivityPrograms</em> (real activities), as well as on <em>VirtualHome Activity</em> datasets (where we first only consider descriptions, not videos). We compare our models to four baselines: 1) <em>random sampling</em>, where we randomly pick both an action for each step and its arguments, 2) <em>random retrieval</em>, where we randomly pick a program from the training set, 3) <em>skipthoughts</em>, where we embed the description using [10, 32], retrieve the closest description from training set and take its program, 4) our model trained with MLE (no RL). Table 3 provides the results. We can see that our model outperforms all baselines on both datasets. Our RL model that exploits LCS reward outperforms the MLE model on both metrics (LCS and executability). Our model that uses both rewards slightly decreases the LCS score, but significantly improves the executability metrics.</p>
<p>Video-based prediction. We also report results on the most challenging task of video-based program generation. The results are shown in Table 2 (right). One can observe that RL training with LCS reward improves the overall accuracy over the MLE model (the generated programs are more meaningful given the description/video), however its executability score decreases. This is expected: MLE model typically generates shorter programs, which are thus more likely to be executable (an empty program is always executable). A careful balance of both metrics is necessary. RL with both the LCS and the simulator reward improves both LCS and the executability metrics over the LCS-only model.</p>
<p>Executing programs in VirtualHome. In Fig. 8 we show a few examples of our agent executing programs generated from natural descriptions. To understand the quality of our simulator as well as the plausibility of our program evaluation metrics, we perform a human study. We randomly selected 10 examples per level of performance: (a) [0.95  1], (b) [0.8  0.95], (c) [0.65  0.8], and (d) [0.5  0.65]. For each example we had 5 AMT workers judge the quality of the performed activity in our simulator, given its language description. Results are shown in Fig. 9. One can notice</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Description: Get an empty glass. Take milk from refrigerator and open it. Pour milk into glass.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Description: Go watch TV on the couch. Turn the TV off and grab the coffee pot. Put the coffee pot on the table and go turn the light on.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Description: Look at the clock then get the magazine and use the toilet. When done put the magazine on the table.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Description: Take the face soap to the kitchen counter and place it there. Turn toaster on and then switch it off. Place the pot on the stove.
Figure 8: Our agent executing generated programs from descriptions, in our VirtualHome. Top description is from ActivityPrograms, while the rest are from VirtualHome Activity dataset. Notice that the top agent uses his left to open the fridge and to grab an object since he already holds an item in his right. There are also some limitations, for example, in row 3 the agent sits on the toilet fully clothed. Furthermore, sometimes the carried item slightly penetrates into the character's body due to imprecisions of the colliders.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 9: Human evaluation of our agent executing described activities via program generation from text: x axis shows the program prediction accuracy, y axis is the human score.
agreement between our metrics and human scores. Generally, at perfect performance the simulations got high human scores, however, there are examples where this was not the case. This may be due to imperfect animation, an indication that further improvements to our simulator are possible.</p>
<p>Implications. The high performance of text-based activity animation opens exciting possibilities for the future. It would allow us to replace the more rigid program syn-
thesis that we used to create our dataset, by having annotators create these animations directly via natural language or crowd-sourcing scripts from existing text corpora.</p>
<h1>7. Conclusion</h1>
<p>We collected a large knowledge base of how-to for household activities specifically aimed for robots. Our dataset contains natural language descriptions of activities as well as programs, a formal symbolic representation of activities in the form of a sequence of steps. What makes these programs unique is that they contain all the steps necessary to perform an activity. We further introduced VirtualHome, a 3D simulator of household activities, which we used to create a large video activity dataset with rich ground-truth. We proposed a simple model that infers a program from either a video or a textual description, allowing robots to be "driven" by naive users via natural language or video demonstration. We showed examples of agents performing these programs in our simulator. There are many exciting avenues going forward, for example, training agents to perform tasks from visual observation alone using RL techniques.</p>
<p>Acknowledgements: We acknowledge partial support from "La Caixa" Fellowship, NSERC COHESA NETGP485577-15, Samsung, DARPA Explainable AI (XAI) program and IARPA D17PC00341. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government. We also gratefully acknowledge NVIDIA for donating several GPUs used in this research.</p>
<h2>References</h2>
<p>[1] https://scratch.mit.edu/.
[2] https://bitbucket.org/Unity-Technologies/ ml-imagesynthesis.
[3] J.-B. Alayrac, P. Bojanowski, N. Agrawal, I. Laptev, J. Sivic, and S. Lacoste-Julien. Unsupervised learning from narrated instruction videos. In CVPR, 2016.
[4] M. Allamanis, D. Tarlow, A. D. Gordon, and Y. Wei. Bimodal modelling of source code and natural language. In ICML, 2015.
[5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. In arXiv:1606.01540, 2016.
[6] S. Brodeur, E. Perez, A. Anand, F. Golemo, L. Celotti, F. Strub, J. Rouat, H. Larochelle, and A. C. Courville. Home: a household multimodal environment. CoRR, abs/1711.11017, 2017.
[7] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun. CARLA: An open urban driving simulator. In Proc. of the 1st Annual Conference on Robot Learning, pages 1-16, 2017.
[8] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig. Virtual worlds as proxy for multi-object tracking analysis. In CVPR, 2016.
[9] J. Johnson, B. Hariharan, L. van der Maaten, J. Hoffman, L. Fei-Fei, C. L. Zitnick, and R. Girshick. Inferring and executing programs for visual reasoning. In arXiv:1705.03633, 2017.
[10] R. Kiros, Y. Zhu, R. Salakhutdinov, R. Zemel, A. Torralba, R. Urtasun, and S. Fidler. Skip-thought vectors. NIPS, 2015.
[11] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi. AI2-THOR: an interactive 3d environment for visual AI. CoRR, abs/1712.05474, 2017.
[12] S. Lauria, G. Bugmann, T. Kyriacou, J. Bos, and A. Klein. Training personal robots using natural language instruction. Intelligent Systems, pages 38-45, 2001.
[13] C. Li, D. Tarlow, A. L. Gaunt, M. Brockschmidt, and N. Kushman. Neural program lattices. In ICML, 2017.
[14] W. Ling, E. Grefenstette, K. M. Hermann, T. Koisky, A. Senior, F. Wang, and P. Blunsom. Latent predictor networks for code generation. arXiv:1603.06744, 2016.
[15] M. MacMahon, B. Stankiewicz, and B. Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. In AAAI, 2006.
[16] H. Mei, M. Bansal, and M. R. Walter. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In AAAI, 2016.
[17] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
[18] D. Nyga and M. Beetz. Everything robots always wanted to know about housework (but were afraid to ask). In IROS, pages 243-250, 2012.
[19] C. Quirk, R. Mooney, and Y. M. Galley. Language to code: Learning semantic parsers for if-this-then-that recipes. In $A C L, 2015$.
[20] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel. Self-critical sequence training for image captioning. CoRR, abs/1612.00563, 2016.
[21] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In ECCV, 2016.
[22] M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun. MINOS: Multimodal indoor simulator for navigation in complex environments. arXiv:1712.03931, 2017.
[23] O. Sener, A. Zamir, S. Savarese, and A. Saxena. Unsupervised semantic parsing of video collections. In arXiv:1506.08438, 2015.
[24] S. Shah, D. Dey, C. Lovett, and A. Kapoor. Aerial Informatics and Robotics platform. Technical Report MSR-TR-2017-9, Microsoft Research, 2017.
[25] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and A. Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In ECCV, 2016.
[26] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In NIPS, 2014.
[27] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. J. Teller, and N. Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In AAAI, 2011.
[28] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian. Building generalizable agents with a realistic and rich 3d environment. CoRR, abs/1801.02209, 2018.
[29] Y. Yang, A. Guha, C. Fermuller, and Y. Aloimonos. Manipulation action tree bank: A knowledge resource for humanoids. In IEEE-RAS Intl. Conf. on Humanoid Robots, 2014.
[30] Y. Yang, Y. Li, C. Fermuller, and Y. Aloimonos. Robot learning manipulation action plans by "watching" unconstrained videos from the world wide web. In AAAI, 2015.
[31] B. Zhou, A. Andonian, and A. Torralba. Temporal relational reasoning in videos. CoRR, abs/1711.08496, 2017.
[32] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books. In ICCV, 2015.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Denotes equal contribution&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
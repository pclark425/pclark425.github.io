<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8514 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8514</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8514</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-f6007270b8aba3a69eac7c98375c99b7ca26c9b0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f6007270b8aba3a69eac7c98375c99b7ca26c9b0" target="_blank">Diversity of Thought Improves Reasoning Abilities of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8514.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8514.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIV-SE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DIVerse reasoning path Self-Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting+ensemble method that elicits multiple distinct high-level reasoning approaches (and optional personas) by varying the input prompt, runs each approach in separate inference calls, and aggregates outputs (majority vote) to improve reasoning accuracy without changing the model's decoding procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5 Turbo, LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Black-box instruction-tuned LLMs evaluated in the paper: GPT-4 and GPT-3.5 Turbo (OpenAI models), and LLaMA-2-70B (open-source) used with greedy decoding / quantization where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Prompt-conditioned diverse reasoning approaches (multiple high-level methods)', 'Few-shot chain-of-thought-style demonstrations augmented per approach', 'Ensemble aggregation (majority vote; optionally meta-reasoning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>DIV-SE first solicits the LLM to generate a set of distinct 'approaches' (e.g., visualization, working backwards, algebraic method) and personas. For each chosen approach/persona pair it augments few-shot examples to match that approach and issues a prompt; each approach is executed in a separate inference call (so multiple runs). The different approach-conditioned generations are then aggregated (majority vote by default). No decoding-time stochasticity is required (temperature set to 0 for DIV-SE runs in most experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared DIV-SE (multiple prompts, separate inference calls) against CoT baselines (ZS-CoT, FS-CoT) and Self-Consistency (SC-s) which samples multiple chain-of-thought decodings; ensembles sizes reported (DIV-SE typically uses size 3 for planning/graph-coloring/commonsenseQA and size 5 for AQUA/GSM8K). Evaluated error propagation and ensemble-size ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Arithmetic reasoning (AQUA‑RAT, GSM8K, MATH counting/probability), Planning (Blocksworld 3 and Blocksworld 4/5), Constraint satisfaction/optimization (Graph Coloring), CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Substantial gains over CoT and SC baselines across benchmarks. Representative numbers (GPT-4, ZS-CoT setting unless noted): Graph Coloring: DIV-SE 97.0% (vs CoT 15.0%, SC-10 23.0%). Blocksworld 3: DIV-SE 94.0% (CoT 70.0%, SC-10 73.0%). Blocksworld 4/5: DIV-SE 69.6% (CoT 40.0%, SC-10 41.2%). AQUA-RAT (GPT-4 ZS): DIV-SE 80.31% (CoT 70.47%). AQUA-RAT (GPT-3.5 ZS): DIV-SE 72.83% (CoT 59.0%). MATH counting/probability (GPT-4 ZS): DIV-SE 79.11% (CoT 62.24%). GSM8K: modest improvements on top of already high baseline (DIV-SE ~96.3% with GPT-4 vs ZS/FS baselines ~94-95%). LLaMA-2-70B (AQUA, FS-CoT): DIV-SE 39.9% vs CoT 29.1%. Ensemble sizes: used 5 for AQUA/GSM8K; 3 for planning/graph-coloring/commonsenseQA. Inference-cost vs accuracy: DIV-SE is Pareto-optimal compared to SC and CoT in reported cost model.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Diversity at the 'thought' or method level yields ensembles that are more independent and complementary than token-level stochastic ensembles: many improvements stem from approaches solving instances that other approaches fail. DIV-SE often achieves state-of-the-art for hard benchmarks (planning, graph coloring). Aggregation via majority vote already yields big gains; applying meta-reasoning for aggregation gives additional improvements. DIV-SE requires multiple inference calls and thus higher cost, but improves the accuracy-to-cost Pareto frontier.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Explicitly varying the input prompt to induce diverse reasoning methods (DIV-SE) outperforms varying decoding stochasticity (Self-Consistency) and vanilla chain-of-thought prompting on multiple hard reasoning benchmarks; thought-level diversity is a more direct and effective form of diversity than token-level sampling, and DIV-SE advances the cost-accuracy Pareto frontier.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8514.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8514.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IDIV-SE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-call DIVerse reasoning path Self-Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cost-reduced variant of DIV-SE that composes multiple distinct approach/persona-conditioned demonstrations into a single prompt and elicits multiple solutions within the same inference call, aggregating them to obtain ensemble benefits at reduced API cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5 Turbo, LLaMA-2-70B (limited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLM family as DIV-SE; IDIV-SE places multiple augmented few-shot exemplars (each exemplifying a different approach/persona) in one prompt so a single model run produces multiple reasoning paths sequentially.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['In-prompt multiple approach-conditioned chain-of-thoughts', 'Sequential in-context enumerated approaches (single autoregressive run)', 'Aggregation (majority vote; optional meta-reasoning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>IDIV-SE composes the set of augmented few-shot examples and the different approach/persona instructions into a single, larger prompt so that the model generates multiple approach-specific solutions within a single call. Aggregation is then applied across the generated solutions. Temperature is set to 0 for IDIV-SE runs in many experiments. Because outputs are generated sequentially in one context, IDIV-SE can be subject to error propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (multiple methods produced in a single run)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparison to DIV-SE (multi-call) and SC variants using same ensemble sizes (IDIV-SE ensembles were 3 for planning/graph-coloring and 5 for AQUA/GSM8K). Explicit measurement of error-propagation: cases where all approaches produced same incorrect answer were re-run to check for propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same as DIV-SE: AQUA-RAT, MATH (counting/probability), GSM8K, Blocksworld planning, Graph Coloring, CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>IDIV-SE often achieves accuracy close to DIV-SE but sometimes a bit lower, attributed to error propagation. Representative numbers: Graph Coloring (GPT-4 ZS): IDIV-SE 74.0% vs DIV-SE 97.0%. Blocksworld 3 (GPT-4 ZS): IDIV-SE 82.0% (vs DIV-SE 94.0%). AQUA-RAT (GPT-3.5 ZS): IDIV-SE 62.60% (CoT 59.0%) while DIV-SE 72.83%. MATH (GPT-3.5 ZS): IDIV-SE 42.50% (DIV-SE 44.94%). IDIV-SE reduces inference cost substantially compared to DIV-SE; reported to achieve comparable accuracy in some cases at lower cost. Error propagation measured on AQUA-RAT (FS-CoT): propagation rates ~6.2% for GPT-4 and ~5.5% for GPT-3.5. Aggregation via meta-reasoning sometimes improves IDIV-SE results (meta-reasoning gave GPT-3.5 AQUA accuracy 67.7% vs majority vote 64.47%).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>IDIV-SE reduces API/inference cost by batching approaches in a single prompt but introduces autoregressive error propagation where early wrong solutions can influence later outputs; measured propagation rates are non-negligible (~5–6%). When error propagation is small, IDIV-SE matches much of DIV-SE's gains; when propagation occurs, DIV-SE's independent calls are superior. IDIV-SE still demonstrates that in-prompt approach diversity is effective versus single-method prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Combining multiple diverse reasoning approaches within a single prompt (IDIV-SE) recovers much of the benefit of multi-call ensembles (DIV-SE) at lower cost, though IDIV-SE must manage error propagation; both strategies show that input-prompt diversity is an effective alternative to sampling-based diversity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8514.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8514.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (ensemble of sampled chain-of-thought decodings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding-time stochastic ensembling method that samples multiple chain-of-thought generations (via temperature sampling) and aggregates their final answers to improve robustness and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-Consistency is applied as a decoding strategy for chain-of-thought prompts; in this paper SC runs used temperature T=0.7, sampling up to s in [1,10] outputs (SC-s) with no top-k truncation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-thought with stochastic decoding (temperature sampling)', 'Aggregation of multiple sampled outputs']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>SC increases stochasticity of the decoding process to generate multiple distinct token-level continuations (chains-of-thought) and then ensembles answers, typically using majority voting. Diversity originates from decoding randomness rather than explicit changes to the input prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (diversity introduced via stochastic decoding at token level rather than distinct high-level approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared SC (SC-3, SC-5, SC-7, SC-10) with DIV-SE/IDIV-SE, using similar cost budgets to compare accuracy vs inference cost tradeoffs. Temperature and sample counts are varied to change SC diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AQUA-RAT, GSM8K, MATH, Blocksworld planning, Graph Coloring, CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>SC improves over single CoT baselines with larger sample counts: e.g., Graph Coloring (GPT-4 ZS) SC-10 = 23.0% (vs CoT 15.0%), Blocksworld 3 SC-10 = 73.0% (vs CoT 70.0%), Blocksworld 4/5 SC-10 = 41.2% (vs CoT 40.0%). However, DIV-SE and IDIV-SE outperform SC at comparable or lower costs on many tasks (e.g., Graph Coloring DIV-SE 97.0% vs SC-10 23.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>SC diversifies at token-output level; the paper argues this may not yield diverse independent solution approaches since token-level diversity does not necessarily equate to method-level diversity. SC can be impermissible in some deployment settings that disallow changing decoding strategy and can be more expensive if many sampled long CoT sequences are generated.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Although SC (sampling multiple CoT decodings) helps, it is less effective than intentionally generating diverse reasoning methods via prompt variations (DIV-SE/IDIV-SE) for many complex reasoning benchmarks; SC's token-level diversity doesn't guarantee the independence of solution strategies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8514.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8514.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that instructs the model to produce step-by-step intermediate reasoning (rationales) which can improve performance on complex reasoning tasks; evaluated in zero-shot ('Think step by step') and few-shot demonstration settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5 Turbo, LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT is applied by adding 'think step-by-step' or by providing demonstrations of intermediate reasoning (few-shot). The paper evaluates both zero-shot CoT (ZS-CoT) and few-shot CoT (FS-CoT) baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Step-by-step chain-of-thought prompting', 'Few-shot CoT demonstrations augmented per approach (in DIV-SE/IDIV-SE variants)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT encourages the model to produce internal reasoning traces. The authors use standard ZS-CoT and FS-CoT prompts (and augment FS examples per approach for DIV-SE/IDIV-SE). CoT is the base baseline against which SC, DIV-SE, and IDIV-SE are compared.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (single prompting style unless augmented with diverse demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>ZS-CoT and FS-CoT are used as baselines; DIV-SE augments FS-CoT demonstrations per approach to induce diversity. Comparative results reported across tasks and models.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AQUA-RAT, GSM8K, MATH, Blocksworld planning, Graph Coloring, CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Baseline CoT performances vary by task: e.g., AQUA-RAT GPT-3.5 ZS CoT 59.00%, GPT-4 ZS CoT 70.47%; GSM8K GPT-4 ZS/FS ~94-95%. On many hard tasks (Graph Coloring, Blocksworld), CoT baselines are substantially lower (e.g., Graph Coloring 15.0% with GPT-4 ZS-CoT). DIV-SE and IDIV-SE improve substantially over CoT baselines on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT is effective but generic; targeted conditioning of CoT demonstrations to express diverse high-level approaches (as in DIV-SE) yields further gains. The paper argues that simply asking 'think step by step' does not exploit the space of distinct problem-solving strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Chain-of-thought prompting is a necessary foundation but not sufficient: carefully designing prompts to induce diverse reasoning approaches (augmenting CoT demonstrations per approach/persona) leads to much larger gains on complex reasoning tasks than vanilla CoT alone.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Answering questions by meta-reasoning over multiple chains of thought <em>(Rating: 2)</em></li>
                <li>Making language models better reasoners with step-aware verifier <em>(Rating: 1)</em></li>
                <li>On the planning abilities of large language models - a critical investigation <em>(Rating: 2)</em></li>
                <li>Large language models still can't plan (a benchmark for llms on planning and reasoning about change) <em>(Rating: 2)</em></li>
                <li>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems <em>(Rating: 1)</em></li>
                <li>Making language models better reasoners with step-aware verifier <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8514",
    "paper_id": "paper-f6007270b8aba3a69eac7c98375c99b7ca26c9b0",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "DIV-SE",
            "name_full": "DIVerse reasoning path Self-Ensemble",
            "brief_description": "A prompting+ensemble method that elicits multiple distinct high-level reasoning approaches (and optional personas) by varying the input prompt, runs each approach in separate inference calls, and aggregates outputs (majority vote) to improve reasoning accuracy without changing the model's decoding procedure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5 Turbo, LLaMA-2-70B",
            "model_description": "Black-box instruction-tuned LLMs evaluated in the paper: GPT-4 and GPT-3.5 Turbo (OpenAI models), and LLaMA-2-70B (open-source) used with greedy decoding / quantization where applicable.",
            "reasoning_methods": [
                "Prompt-conditioned diverse reasoning approaches (multiple high-level methods)",
                "Few-shot chain-of-thought-style demonstrations augmented per approach",
                "Ensemble aggregation (majority vote; optionally meta-reasoning)"
            ],
            "reasoning_methods_description": "DIV-SE first solicits the LLM to generate a set of distinct 'approaches' (e.g., visualization, working backwards, algebraic method) and personas. For each chosen approach/persona pair it augments few-shot examples to match that approach and issues a prompt; each approach is executed in a separate inference call (so multiple runs). The different approach-conditioned generations are then aggregated (majority vote by default). No decoding-time stochasticity is required (temperature set to 0 for DIV-SE runs in most experiments).",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared DIV-SE (multiple prompts, separate inference calls) against CoT baselines (ZS-CoT, FS-CoT) and Self-Consistency (SC-s) which samples multiple chain-of-thought decodings; ensembles sizes reported (DIV-SE typically uses size 3 for planning/graph-coloring/commonsenseQA and size 5 for AQUA/GSM8K). Evaluated error propagation and ensemble-size ablations.",
            "task_or_benchmark": "Arithmetic reasoning (AQUA‑RAT, GSM8K, MATH counting/probability), Planning (Blocksworld 3 and Blocksworld 4/5), Constraint satisfaction/optimization (Graph Coloring), CommonsenseQA",
            "performance_results": "Substantial gains over CoT and SC baselines across benchmarks. Representative numbers (GPT-4, ZS-CoT setting unless noted): Graph Coloring: DIV-SE 97.0% (vs CoT 15.0%, SC-10 23.0%). Blocksworld 3: DIV-SE 94.0% (CoT 70.0%, SC-10 73.0%). Blocksworld 4/5: DIV-SE 69.6% (CoT 40.0%, SC-10 41.2%). AQUA-RAT (GPT-4 ZS): DIV-SE 80.31% (CoT 70.47%). AQUA-RAT (GPT-3.5 ZS): DIV-SE 72.83% (CoT 59.0%). MATH counting/probability (GPT-4 ZS): DIV-SE 79.11% (CoT 62.24%). GSM8K: modest improvements on top of already high baseline (DIV-SE ~96.3% with GPT-4 vs ZS/FS baselines ~94-95%). LLaMA-2-70B (AQUA, FS-CoT): DIV-SE 39.9% vs CoT 29.1%. Ensemble sizes: used 5 for AQUA/GSM8K; 3 for planning/graph-coloring/commonsenseQA. Inference-cost vs accuracy: DIV-SE is Pareto-optimal compared to SC and CoT in reported cost model.",
            "qualitative_findings": "Diversity at the 'thought' or method level yields ensembles that are more independent and complementary than token-level stochastic ensembles: many improvements stem from approaches solving instances that other approaches fail. DIV-SE often achieves state-of-the-art for hard benchmarks (planning, graph coloring). Aggregation via majority vote already yields big gains; applying meta-reasoning for aggregation gives additional improvements. DIV-SE requires multiple inference calls and thus higher cost, but improves the accuracy-to-cost Pareto frontier.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Explicitly varying the input prompt to induce diverse reasoning methods (DIV-SE) outperforms varying decoding stochasticity (Self-Consistency) and vanilla chain-of-thought prompting on multiple hard reasoning benchmarks; thought-level diversity is a more direct and effective form of diversity than token-level sampling, and DIV-SE advances the cost-accuracy Pareto frontier.",
            "uuid": "e8514.0"
        },
        {
            "name_short": "IDIV-SE",
            "name_full": "In-call DIVerse reasoning path Self-Ensemble",
            "brief_description": "A cost-reduced variant of DIV-SE that composes multiple distinct approach/persona-conditioned demonstrations into a single prompt and elicits multiple solutions within the same inference call, aggregating them to obtain ensemble benefits at reduced API cost.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5 Turbo, LLaMA-2-70B (limited)",
            "model_description": "Same LLM family as DIV-SE; IDIV-SE places multiple augmented few-shot exemplars (each exemplifying a different approach/persona) in one prompt so a single model run produces multiple reasoning paths sequentially.",
            "reasoning_methods": [
                "In-prompt multiple approach-conditioned chain-of-thoughts",
                "Sequential in-context enumerated approaches (single autoregressive run)",
                "Aggregation (majority vote; optional meta-reasoning)"
            ],
            "reasoning_methods_description": "IDIV-SE composes the set of augmented few-shot examples and the different approach/persona instructions into a single, larger prompt so that the model generates multiple approach-specific solutions within a single call. Aggregation is then applied across the generated solutions. Temperature is set to 0 for IDIV-SE runs in many experiments. Because outputs are generated sequentially in one context, IDIV-SE can be subject to error propagation.",
            "reasoning_diversity": "diverse (multiple methods produced in a single run)",
            "reasoning_diversity_experimental_setup": "Direct comparison to DIV-SE (multi-call) and SC variants using same ensemble sizes (IDIV-SE ensembles were 3 for planning/graph-coloring and 5 for AQUA/GSM8K). Explicit measurement of error-propagation: cases where all approaches produced same incorrect answer were re-run to check for propagation.",
            "task_or_benchmark": "Same as DIV-SE: AQUA-RAT, MATH (counting/probability), GSM8K, Blocksworld planning, Graph Coloring, CommonsenseQA",
            "performance_results": "IDIV-SE often achieves accuracy close to DIV-SE but sometimes a bit lower, attributed to error propagation. Representative numbers: Graph Coloring (GPT-4 ZS): IDIV-SE 74.0% vs DIV-SE 97.0%. Blocksworld 3 (GPT-4 ZS): IDIV-SE 82.0% (vs DIV-SE 94.0%). AQUA-RAT (GPT-3.5 ZS): IDIV-SE 62.60% (CoT 59.0%) while DIV-SE 72.83%. MATH (GPT-3.5 ZS): IDIV-SE 42.50% (DIV-SE 44.94%). IDIV-SE reduces inference cost substantially compared to DIV-SE; reported to achieve comparable accuracy in some cases at lower cost. Error propagation measured on AQUA-RAT (FS-CoT): propagation rates ~6.2% for GPT-4 and ~5.5% for GPT-3.5. Aggregation via meta-reasoning sometimes improves IDIV-SE results (meta-reasoning gave GPT-3.5 AQUA accuracy 67.7% vs majority vote 64.47%).",
            "qualitative_findings": "IDIV-SE reduces API/inference cost by batching approaches in a single prompt but introduces autoregressive error propagation where early wrong solutions can influence later outputs; measured propagation rates are non-negligible (~5–6%). When error propagation is small, IDIV-SE matches much of DIV-SE's gains; when propagation occurs, DIV-SE's independent calls are superior. IDIV-SE still demonstrates that in-prompt approach diversity is effective versus single-method prompting.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Combining multiple diverse reasoning approaches within a single prompt (IDIV-SE) recovers much of the benefit of multi-call ensembles (DIV-SE) at lower cost, though IDIV-SE must manage error propagation; both strategies show that input-prompt diversity is an effective alternative to sampling-based diversity.",
            "uuid": "e8514.1"
        },
        {
            "name_short": "Self-Consistency (SC)",
            "name_full": "Self-consistency (ensemble of sampled chain-of-thought decodings)",
            "brief_description": "A decoding-time stochastic ensembling method that samples multiple chain-of-thought generations (via temperature sampling) and aggregates their final answers to improve robustness and accuracy.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5 Turbo",
            "model_description": "Self-Consistency is applied as a decoding strategy for chain-of-thought prompts; in this paper SC runs used temperature T=0.7, sampling up to s in [1,10] outputs (SC-s) with no top-k truncation.",
            "reasoning_methods": [
                "Chain-of-thought with stochastic decoding (temperature sampling)",
                "Aggregation of multiple sampled outputs"
            ],
            "reasoning_methods_description": "SC increases stochasticity of the decoding process to generate multiple distinct token-level continuations (chains-of-thought) and then ensembles answers, typically using majority voting. Diversity originates from decoding randomness rather than explicit changes to the input prompt.",
            "reasoning_diversity": "similar (diversity introduced via stochastic decoding at token level rather than distinct high-level approaches)",
            "reasoning_diversity_experimental_setup": "Compared SC (SC-3, SC-5, SC-7, SC-10) with DIV-SE/IDIV-SE, using similar cost budgets to compare accuracy vs inference cost tradeoffs. Temperature and sample counts are varied to change SC diversity.",
            "task_or_benchmark": "AQUA-RAT, GSM8K, MATH, Blocksworld planning, Graph Coloring, CommonsenseQA",
            "performance_results": "SC improves over single CoT baselines with larger sample counts: e.g., Graph Coloring (GPT-4 ZS) SC-10 = 23.0% (vs CoT 15.0%), Blocksworld 3 SC-10 = 73.0% (vs CoT 70.0%), Blocksworld 4/5 SC-10 = 41.2% (vs CoT 40.0%). However, DIV-SE and IDIV-SE outperform SC at comparable or lower costs on many tasks (e.g., Graph Coloring DIV-SE 97.0% vs SC-10 23.0%).",
            "qualitative_findings": "SC diversifies at token-output level; the paper argues this may not yield diverse independent solution approaches since token-level diversity does not necessarily equate to method-level diversity. SC can be impermissible in some deployment settings that disallow changing decoding strategy and can be more expensive if many sampled long CoT sequences are generated.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Although SC (sampling multiple CoT decodings) helps, it is less effective than intentionally generating diverse reasoning methods via prompt variations (DIV-SE/IDIV-SE) for many complex reasoning benchmarks; SC's token-level diversity doesn't guarantee the independence of solution strategies.",
            "uuid": "e8514.2"
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that instructs the model to produce step-by-step intermediate reasoning (rationales) which can improve performance on complex reasoning tasks; evaluated in zero-shot ('Think step by step') and few-shot demonstration settings.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5 Turbo, LLaMA-2-70B",
            "model_description": "CoT is applied by adding 'think step-by-step' or by providing demonstrations of intermediate reasoning (few-shot). The paper evaluates both zero-shot CoT (ZS-CoT) and few-shot CoT (FS-CoT) baselines.",
            "reasoning_methods": [
                "Step-by-step chain-of-thought prompting",
                "Few-shot CoT demonstrations augmented per approach (in DIV-SE/IDIV-SE variants)"
            ],
            "reasoning_methods_description": "CoT encourages the model to produce internal reasoning traces. The authors use standard ZS-CoT and FS-CoT prompts (and augment FS examples per approach for DIV-SE/IDIV-SE). CoT is the base baseline against which SC, DIV-SE, and IDIV-SE are compared.",
            "reasoning_diversity": "similar (single prompting style unless augmented with diverse demonstrations)",
            "reasoning_diversity_experimental_setup": "ZS-CoT and FS-CoT are used as baselines; DIV-SE augments FS-CoT demonstrations per approach to induce diversity. Comparative results reported across tasks and models.",
            "task_or_benchmark": "AQUA-RAT, GSM8K, MATH, Blocksworld planning, Graph Coloring, CommonsenseQA",
            "performance_results": "Baseline CoT performances vary by task: e.g., AQUA-RAT GPT-3.5 ZS CoT 59.00%, GPT-4 ZS CoT 70.47%; GSM8K GPT-4 ZS/FS ~94-95%. On many hard tasks (Graph Coloring, Blocksworld), CoT baselines are substantially lower (e.g., Graph Coloring 15.0% with GPT-4 ZS-CoT). DIV-SE and IDIV-SE improve substantially over CoT baselines on many tasks.",
            "qualitative_findings": "CoT is effective but generic; targeted conditioning of CoT demonstrations to express diverse high-level approaches (as in DIV-SE) yields further gains. The paper argues that simply asking 'think step by step' does not exploit the space of distinct problem-solving strategies.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Chain-of-thought prompting is a necessary foundation but not sufficient: carefully designing prompts to induce diverse reasoning approaches (augmenting CoT demonstrations per approach/persona) leads to much larger gains on complex reasoning tasks than vanilla CoT alone.",
            "uuid": "e8514.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Answering questions by meta-reasoning over multiple chains of thought",
            "rating": 2
        },
        {
            "paper_title": "Making language models better reasoners with step-aware verifier",
            "rating": 1
        },
        {
            "paper_title": "On the planning abilities of large language models - a critical investigation",
            "rating": 2
        },
        {
            "paper_title": "Large language models still can't plan (a benchmark for llms on planning and reasoning about change)",
            "rating": 2
        },
        {
            "paper_title": "Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems",
            "rating": 1
        },
        {
            "paper_title": "Making language models better reasoners with step-aware verifier",
            "rating": 1
        }
    ],
    "cost": 0.0103707,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Diversity of Thought Improves Reasoning Abilities of LLMs</h1>
<p>Ranjita Naik ${ }^{\dagger}$<br>Microsoft</p>
<h2>Varun Chandrasekaran</h2>
<p>University of Illinois Urbana-Champaign</p>
<p>Mert Yuksekgonul<br>Stanford University</p>
<p>Hamid Palangi<br>Microsoft Research</p>
<h2>Besmira Nushi ${ }^{\dagger}$<br>Microsoft Research</h2>
<h2>Abstract</h2>
<p>Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps, or ensembling various generations through modifying decoding steps boosts performance. However, these methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we discuss how one can create and leverage variations of the input prompt as a means of diversity of thought. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that are apt for the problem. We then ensemble the diverse prompts in our method DIVSe (DIVerse reasoning path Self-Ensemble) across multiple inference calls, or use diverse approaches within a single inference call; we call the latter IDiv-Se (In-call DIVerse reasoning path Self-Ensemble). Apart from our approaches outperforming prior work, Div-SE (in particular) advances state-of-the-art performance on the challenging planning and graph coloring benchmarks. Our results improve the Pareto frontier of the accuracy-cost trade-off.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) exhibit state-of-the-art performance across a myriad of tasks, but their effectiveness is strongly influenced by prompt design (Anil et al., 2023; OpenAI, 2023a; Nori et al., 2023). For complex reasoning tasks, the right prompt can enable LLMs to capitalize on task structure (Guidance, 2024), such as by facilitating memory (by externalizing thought processes), or through tractable problem decomposition (Zhou et al., 2024). However, existing prompt design</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>either relies on iterative trial-and-error (White et al., 2023), or is expensive (Lester et al., 2021).</p>
<p>Previous works identified two simple, yet general prompting principles to enable complex reasoning: (i) Chain-of-Thought (CoT) prompting, and (ii) ensembling multiple solutions from diverse decoding paths. CoT prompting (Wei et al., 2022) improves performance by guiding the LLM to follow step-by-step reasoning. Selfconsistency (SC) (Wang et al., 2023) instead increases the stochasticity by modifying the decoding process and obtaining multiple completions, which are then ensembled.</p>
<p>However, combining the two principles raises limitations. First, inference is significantly more expensive due to numerous runs, each generating long completions with many reasoning steps. Next, it may be impermissible to modify the decoding process in some settings, such as commercial deployments. Finally, stochasticity-based methods do not directly guide the diversity at the level of thought or method, but rather at the token level. This poses limitations because linguistic token diversity does not always ensure diverse and independent solution approaches.</p>
<p>In this paper, we explore how to explicitly promote the diversity of thought while mitigating the aforementioned issues. Prior work by Li et al. (2023) highlights the importance of prompt diversity, but their notion of diversity is captured through variety in the few-shot examples provided with the prompt; ours focuses on the reasoning approach. We first solicit the LLM to produce multiple-highlevel reasoning approaches for problem-solving (e.g., method of elimination, visualization techniques etc. for math reasoning problems). We then leverage GPT-4 to augment few-shot examples used in prior work (Wei et al., 2022) into the corresponding approaches, whenever applicable.</p>
<p>We propose Div-SE (DIVerse reasoning path Self-Ensemble) to extract and aggregate responses</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Diversity of Thought enhances the inference cost vs. accuracy trade-off. We compare DIV-SE and IDIV-SE with SC <em>Wang et al. (2023)</em> and CoT <em>Wei et al. (2022)</em> across three benchmarks. The x-axis indicates the total inference cost (as defined in § 3) on the benchmark using the given method, while the y-axis represents the LLM's performance. The few-shot-CoT setting is represented by filled gray dots, while the zero-shot-CoT setting is indicated by unfilled dots. Notice that for a fixed cost, our approaches always give better performance.</p>
<p>(via majority vote) across multiple inference calls (§ 2.2). Since distinct approaches introduce diversity at the "thought" level, our methodology results in improved ensemble accuracy. In Fig. 1, we show that it yields more accurate results across multiple reasoning benchmarks at a fixed inference cost, without modifying the decoding procedure. For instance, in the BlockSwordD 4/5 task <em>Valmeekam et al. (2022)</em>, DIV-SE improves the performance by 29.6 percentage points (p.p). However, this method still leverages multiple inference calls, which could be costly.</p>
<p>To reduce inference costs, we build on the observation that the approaches are often mutually independent, and can be combined in a <em>single prompt</em> to solicit multiple solutions <em>Cheng et al. (2023)</em>. Based on this premise, we propose IDIV-SE (Incall DIVerse reasoning path Self-Ensemble; § 2.2), which combines all approaches within the same prompt and aggregates all resulting outputs to leverage diversity with a reduced cost. Fig. 1 demonstrates that this method obtains comparable accuracy to DIV-SE and better performance than prior work with lower inference costs.</p>
<p>We push the pareto frontier of the cost-accuracy trade-off of prompting strategies across multiple reasoning tasks (§ 4), outperforming both CoT and SC prompting on both GPT-3.5 and GPT-4. This is evident from Fig. 1 for the AQUA-RAT <em>Ling et al. (2017)</em>, planning <em>Valmeekam et al. (2023)</em>, and graph coloring <em>Stechly et al. (2023)</em> benchmarks, where there is a performance improvement of 16.52, 29.6, and 82.5 p.p respectively. These improvements, some of which are state-of-the-art, show the potential of thought diversity to extract complex reasoning abilities from LLMs that were impossible to leverage otherwise. We will open source our code upon publication to encourage further research.</p>
<h2>2 Diversity through LLM Interactions</h2>
<p>First, we introduce terms and notations that we use throughout the paper. We use upper case for sets, lower case for variables, and $[n]={1, \cdots, n}$.</p>
<p>Approach: These are reasoning strategies for problem solving, denoted with the variable <em>a</em>. For example, for the GSM8K <em>Cobbe et al. (2021)</em>, a benchmark of grade-school math problems, some of the (generated) approaches can include <em>a</em><sup>1</sup> = "using visualizations", <em>a</em><sup>2</sup> = "working backwards", <em>a</em><sup>3</sup> = "using direct calculation", and <em>a</em><sup>4</sup> = "method of elimination".</p>
<p>Persona: In addition to specifying "how" to solve a reasoning problem, specifying a persona can also influence how the LLM behaves <em>Salewski et al. (2023)</em>. We denote this with the variable <em>p</em> e.g., <em>p</em><sup>1</sup> = "Thinking like Alan Turing", <em>p</em><sup>2</sup> = "Thinking like Math Professor" for the GSM8K task. Note that both approaches and personas are dependent on the reasoning problem.</p>
<h3>2.1 Using the LLM as a guide</h3>
<p>Proposed method for creating prompts, which we term DIVERSEPROMPTING is presented in Algorithm 1. Below, we will describe each step in more detail. At a high-level, we solicit feedback from the LLM on how to solve tasks.</p>
<p>Step 1+2. Extracting Approaches &amp; Personas: Note that LLMs trained on internet-scale data encode a significant knowledge from multiple do-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Diversity of Thought. This illustration depicts CoT and IDIV-SE prompting strategies. Notice that both have a single example. However, IDIV-SE presents more diversity in terms of reasoning paths. This enables it to generate diverse completions, yielding more accurate responses.</p>
<p>mains (Liang et al., 2022; Bubeck et al., 2023). While LLMs may not be perfect at solving reasoning tasks, we hypothesize that they are helpful in providing high-quality intermediate feedback.</p>
<p>To extract approaches, we utilize the following methodology: (i) Randomly picking a question from the reasoning dataset $D$ we want to evaluate; and (ii) Creating an instruction prompt where we ask the LLM to generate the names of $b \in[1,5]$ approaches to solve the aforementioned question conforming to a predefined template (for easier post-processing). Refer to Figure 5 for an example of the prompt used.</p>
<p>We extract the part of the response that is compliant with the template and store it. We repeat this process $c$ times (obtaining of $c \cdot b$ candidate approaches), and pick the $n$ most frequent approaches to store in set $A^{1}$. This process is abstracted as method det_approaches( .).</p>
<p>One can repeat the above process used to extract relevant personas for a given reasoning task. However, we followed a simpler route and asked the model directly for relevant personas for a given task and then included them in the set of $m$ candidate personas $P$ used. This is abstracted as method det_personas( .). Note that no persona $(\phi)$ is also part of the persona set.</p>
<h2>Step 3. Choosing the Best Persona, Approach</h2>
<p>Pairs: The choice of persona and approaches introduces a principled way to promote diversity.</p>
<p>If the set of personas is $P$, and the set of approaches is $A$, the Cartesian product of $P$ and $A$ yields the total number of prompts. In practice, for each combination (denoted by $s_{i}$ ) of persona and approach, we evaluate the prompt formed using the composition on a small validation set $V^{1}$ and choose the best performing "size" elements on the given task ${ }^{2}$.
Step 4. Augmenting few-shot examples: Once the (subset of) approach and persona pairs are fixed, we ask the LLM to augment existing few-shot examples (denoted $F=\left{f_{1}, \cdots\right}$ ) with the given set of approaches. Specifically, we take the few-shot examples provided by Wei et al. (2022), and ask the LLM to solve them in the style of a chosen approach and persona pair (Fig.8); we term the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>task and then included them in the set of $m$ candidate personas $P$ used. This is abstracted as method det_personas( .). Note that no persona $(\phi)$ is also part of the persona set.</p>
<p>Step 3. Choosing the Best Persona, Approach Pairs: The choice of persona and approaches introduces a principled way to promote diversity.</p>
<p>If the set of personas is $P$, and the set of approaches is $A$, the Cartesian product of $P$ and $A$ yields the total number of prompts. In practice, for each combination (denoted by $s_{i}$ ) of persona and approach, we evaluate the prompt formed using the composition on a small validation set $V^{1}$ and choose the best performing "size" elements on the given task ${ }^{2}$.
Step 4. Augmenting few-shot examples: Once the (subset of) approach and persona pairs are fixed, we ask the LLM to augment existing few-shot examples (denoted $F=\left{f_{1}, \cdots\right}$ ) with the given set of approaches. Specifically, we take the few-shot examples provided by Wei et al. (2022), and ask the LLM to solve them in the style of a chosen approach and persona pair (Fig.8); we term the</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>DIVERSEPROMPTING: Prompt creation.
procedure DiversePrompt(size, type, $F, D, V$ )
$\triangleright$ Step 1: Identify different approaches to be used.
$A=\left{a_{1}, \ldots, a_{n}\right} \leftarrow$ det_approaches(D)
$\triangleright$ where $A$ is the set of approaches
$\triangleright$ Step 2: Identify different personas to be used.
$P=\left{\phi, p_{1}, \ldots, p_{m}\right} \leftarrow$ det_personas(D)
$\triangleright$ where $P$ is the set of personas
$\triangleright$ Step 3: Find the best combination.
$S=\left{s_{1}, \ldots s_{\text {size }}\right} \leftarrow$ combine $(A, P$, size, $V)$
$\triangleright$ where $S$ is the set of combined approaches and personas, and $s_{i}=\left(p, a_{i} \in A\right)$
$\triangleright$ Step 4: Augment the few-shot examples.
$T=\left{\hat{T}<em i_="i," j="j">{i, j}, \ldots\right} \leftarrow$ augment $(S, F)$
$\triangleright$ where $T$ is the set of augmented examples, and $\hat{T}</em> \in F ;|T|=$ size
$\triangleright$ Step 5: Compose the final prompt.
$O \leftarrow$ compose $(T, S$, type $)$
return $O$
$\triangleright$ Return the final output.
end procedure
output augmented few-shot examples. This is abstracted in method augment(.), where $\hat{T}}$ is formed using $s_{i} \in S$ and $f_{j<em i="i">{i, j}$ is the set of augmented few-shot examples corresponding to the approach and persona pair from $s</em>$. An example is visualized in the bottom left of Fig. 2, where the prompt contains different approaches for solving a math problem.}$ and example $f_{j</p>
<h3>2.2 Designing the Prompts</h3>
<p>Step 5. Prompt Composition: We create prompts for our approach using the best approach and persona pairs identified in step 3, and augmented fewshot examples from step 4 as shown in Fig. 2 and 4.</p>
<p>We now describe two techniques to generate prompts with the augmented demonstrations $(T)$ that have been accumulated.
Candidate 1. Div-SE: We first propose Div-SE (DIVerse reasoning path Self-Ensemble), a method to execute the diverse set of approaches in different inference calls and aggregate their solutions. Apart from the question to be solved and the augmented few-shot examples, the final prompt contains a persona, approach, and additional instructions. One example is visualized in Fig. 4 (please refer to appendix for more examples of prompts: Fig. 9 through 16). Diversity is ensured through running inference with multiple prompts, each with a different approach and persona pairs and augmented few-shot examples. However, since the approaches are executed separately, generating a solution (via
aggregation of multiple responses) requires multiple inference calls, which can be costly.
Candidate 2. IDIV-SE: To further reduce the inference costs while promoting diversity, we propose IDIV-SE (In-call DIVerse reasoning path SelfEnsemble). In IDIV-SE, the final prompt is a composition of all approach and persona pairs and corresponding augmented few-shot examples, and the question to be solved. An example is presented in Fig. 2 (bottom left). More examples of prompts are presented in the appendix in Fig. 9 through 16. This noticeably decreases the number of calls to be made, since all few-shot examples are presented within the same prompt. We note that there might be error propagation due to the autoregressive nature of models. We evaluate this in detail in § 4.3.
Practicality. Crucially, DiversePrompting finds approaches that are general and reusable across similar reasoning problems. We reused the strategies identified for solving AQUA-RAT and Planning benchmark respectively in the MATH (counting and probability) and Graph Coloring benchmarks. This also reduces the cost of repeated evaluation on a separate evaluation set.
Aggregation. We aggregate the responses via majority vote for both prompting strategies. Other aggregation strategies can also be leveraged, such as utilizing the LLM itself to aggregate responses or weighted aggregation. In $\S 4.4$, we consider an aggregation strategy proposed by Yoran et al. (2023) and describe how compatible it is with our prompting approaches.</p>
<h2>3 Experiments</h2>
<p>We consider the following reasoning benchmarks. Arithmetic Reasoning: We use: (i) AQUARAT (Ling et al., 2017), a suite of algebraic word problems, (ii) GSM8K (Cobbe et al., 2021), a benchmark of grade-school math problems described in natural language (involving elementary arithmetic operations), and (iii) MATH (Counting and Probability) (Hendrycks et al., 2021), a collection of math problems from which we choose only counting and probability as these are not covered by GSM8K and AQUA-RAT. For all datasets, we use the test split for evaluation, containing 254, 1319, and 474 questions respectively.
Planning Capabilities: We use the Blocksworld Planning benchmark proposed in Valmeekam et al. (2022, 2023). The benchmark has two datasets: one involves 3 blocks (BlockSWORLD 3, 100</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Diversity of Thought enhances the inference cost and accuracy trade-off. We compare DIV-SE and IDIV-SE with SC (Wang et al., 2023) and CoT (Wei et al., 2022) across three benchmarks. The x-axis indicates the total cost (as defined in § 3) of running inference with the LLM on the benchmark using the given method, while the y-axis represents the LLM's performance. The FS-CoT setting is represented by filled gray dots, while the ZS-CoT setting is indicated by unfilled dots. Notice that for BLOCKSWORLD 3, despite being in the ZS-CoT setting, our approaches are more performant than the SC-s (FS-CoT) baseline.</p>
<p>instances), while the other dataset involves 4 or 5 blocks (BLOCKSWORLD 4/5, 500 instances).</p>
<p><em>Constraint Satisfaction Optimization</em>: We use the GRAPH COLORING benchmark (Stechly et al., 2023) containing 100 examples to test reasoning for constraint satisfaction. <em>Commonsense Reasoning</em>: We use COMMONSENSEQA (Talmor et al., 2019) which consists of generic multiple-choice questions elicited for testing common sense reasoning. We use the validation split containing 1,221 questions.</p>
<p><strong>Language Models.</strong> We evaluate our proposed methods on both GPT-3.5 Turbo (OpenAI, 2022) and GPT-4 (OpenAI, 2023b). We also conduct an additional evaluation on LLaMA-2 70B (Touvron et al., 2023) to explore the performance of our technique on open-source LLMs. For the latter, we use meta-llama/Llama-2-70b-chat-hf through the Transformers library (Wolf et al., 2019).</p>
<p><strong>Baselines.</strong> We consider Chain-of-Thought (CoT) (Wei et al., 2022) and Self-Consistency (SC) (Wang et al., 2023) as our baselines. For CoT, we consider two settings: zero-shot (ZS) CoT (Kojima et al., 2022) (i.e., "Think step by step" is added to the prompt), and few-shot (FS) CoT (i.e., CoT with demonstrations). In our SC runs, we set the temperature T = 0.7 without top-k truncation and sample up to s ∈ [1, 10] outputs (denoted SC-s). For all other approaches, we set T = 0. We use ensembles of size 5 in IDIV-SE and DIV-SE for GSM8K and AQUA-RAT. For the planning, GRAPH COLORING, and COMMONSENSEQA benchmarks, we use a size of 3.</p>
<p><strong>Performance Metrics.</strong> We measure the accuracy on the task, and the generation inference cost. To measure the cost, we assume 1000 tokens are about 750 words<sup>3</sup>. For GPT-4 (8K) the input and output prices used to estimate inference cost are $0.03/1k tokens and $0.06/1k tokens, respectively. For GPT 3.5 Turbo (16K), the input and output prices used in the cost estimation are $0.003/1k (tokens) and $0.004/1k (tokens) respectively.</p>
<p><strong>Results Summary.</strong> include: Across most benchmarks we consider, our techniques provide substantial performance gains (e.g., 16.52, 82.5, and 14.3 p.p improvements for AQUA-RAT, GRAPH COLORING, and MATH respectively). They are also Pareto optimal (in terms of the utility vs. cost trade-off). For the challenging planning benchmark (BLOCKSWORLD 4/5), our techniques improve accuracy by 29.6 p.p achieving <em>state-of-the-art</em> performance. Using GPT-4 for BLOCKSWORLD 3, our approach (in the ZS-CoT setting) is substantially more effective than SC-10 (in the FS-CoT setting) at 4× lower cost (Figure 3 (center figure)).</p>
<p>Since prompts are chained together in IDIV-SE, error propagation is possible. Our evaluation on AQUA-RAT in § 4.3 suggests that even though error propagation is estimated as less than 6.5% for both models, these rates are comparable to differences in performance between DIV-SE and IDIV-SE. When combined with aggregation approaches that are capable of reasoning across the diverse generations (Yoran et al., 2023), we observe additional performance gains as shown in § 4.4. For the AQUA-RAT benchmark for instance, we see an accuracy of 67.7% for GPT-3.5 (3.23 p.p improvement to majority voting).</p>
<p><sup>3</sup>https://openai.com/pricing</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Graph Coloring</th>
<th>BW 3</th>
<th>BW 4/5</th>
</tr>
</thead>
<tbody>
<tr>
<td>CoT</td>
<td>15.0</td>
<td>70.00</td>
<td>40.00</td>
</tr>
<tr>
<td>SC-3</td>
<td>18.0</td>
<td>66.00</td>
<td>38.20</td>
</tr>
<tr>
<td>SC-5</td>
<td>20.0</td>
<td>70.00</td>
<td>38.40</td>
</tr>
<tr>
<td>SC-7</td>
<td>22.0</td>
<td>72.00</td>
<td>40.00</td>
</tr>
<tr>
<td>SC-10</td>
<td>23.0</td>
<td>73.00</td>
<td>41.20</td>
</tr>
<tr>
<td>IDIV-SE</td>
<td>74.00</td>
<td>82.00</td>
<td>57.00</td>
</tr>
<tr>
<td>DIV-SE</td>
<td>$\mathbf{9 7 . 0 0}$</td>
<td>$\mathbf{9 4 . 0 0}$</td>
<td>$\mathbf{6 9 . 6 0}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance on Graph Coloring and Blocksworld planning for GPT-4 in the ZS-CoT setting. We compare Div-SE and IDIV-SE with SC (Wang et al., 2023) and CoT (Wei et al., 2022).</p>
<h2>4 Results</h2>
<h3>4.1 Main Results</h3>
<p>We present the summary of results in Table 1 and 2. Detailed results are available in Appendix C. These also cover results on the impact of ensemble size in Appendix D.</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Method</th>
<th>AQuA</th>
<th>MATH</th>
<th>CQA</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5 ZS</td>
<td>CoT</td>
<td>59.00</td>
<td>31.90</td>
<td>71.40</td>
</tr>
<tr>
<td></td>
<td>SC-3</td>
<td>61.40</td>
<td>32.07</td>
<td>72.00</td>
</tr>
<tr>
<td></td>
<td>SC-5</td>
<td>63.37</td>
<td>38.19</td>
<td>72.80</td>
</tr>
<tr>
<td></td>
<td>IDIV-SE</td>
<td>62.60</td>
<td>42.50</td>
<td>74.00</td>
</tr>
<tr>
<td></td>
<td>DIV-SE</td>
<td>72.83</td>
<td>44.94</td>
<td>74.50</td>
</tr>
<tr>
<td>GPT-3.5 FS</td>
<td>CoT</td>
<td>57.48</td>
<td>30.38</td>
<td>79.4</td>
</tr>
<tr>
<td></td>
<td>IDIV-SE</td>
<td>64.57</td>
<td>44.10</td>
<td>80.00</td>
</tr>
<tr>
<td></td>
<td>DIV-SE</td>
<td>72.84</td>
<td>52.22</td>
<td>80.40</td>
</tr>
<tr>
<td>GPT-4 ZS</td>
<td>CoT</td>
<td>70.47</td>
<td>62.24</td>
<td>81.60</td>
</tr>
<tr>
<td></td>
<td>IDIV-SE</td>
<td>71.65</td>
<td>72.00</td>
<td>82.50</td>
</tr>
<tr>
<td></td>
<td>DIV-SE</td>
<td>80.31</td>
<td>79.11</td>
<td>81.70</td>
</tr>
<tr>
<td>GPT-4 FS</td>
<td>CoT</td>
<td>71.90</td>
<td>66.46</td>
<td>87.70</td>
</tr>
<tr>
<td></td>
<td>IDIV-SE</td>
<td>79.90</td>
<td>72.00</td>
<td>89.00</td>
</tr>
<tr>
<td></td>
<td>DIV-SE</td>
<td>84.25</td>
<td>80.76</td>
<td>88.00</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance on AQUA-RAT, MATH (Counting and Probability), and COMmonsenseQA for GPT3.5 Turbo and GPT-4 in the ZS-CoT and few-shot-CoT settings respectively.</p>
<h3>4.1.1 Arithmetic reasoning via AQUA-RAT</h3>
<p>GPT-4 Results: In Table 2, we observe that Div-SE achieves an accuracy increase of 9.84 and 14.6 p.p in the FS-CoT (baseline accuracy of $71.9 \%$ ) and ZS-CoT (baseline of $70.47 \%$ ) settings, respectively. While the gains from IDIV-SE are nominal in ZSCoT, it achieves a boost of 7.7 p.p for FS-CoT.
GPT-3.5 Results: In Table 2, we see that Div-SE yields a gain of 14.23 and 16.52 p.p in the FSCoT (baseline of $57.48 \%$ ) and ZS-CoT (baseline of $59 \%$ ) settings, respectively. Within the FS-CoT setting, IDIV-SE gets an absolute increase of 7 p.p.</p>
<p>Note that Fig. 1 also displays the total inference cost. Both IDIV-SE and Div-SE are Pareto optimal, indicating their capacity to achieve a higher
accuracy while maintaining low costs.</p>
<h3>4.1.2 Counting and probabilistic reasoning via MATH</h3>
<p>GPT-4 Results: From Table 2, we see that Div-SE achieves an accuracy increase of 14.3 and 16.87 p.p in the FS-CoT (baseline of $66.46 \%$ ) and ZS-CoT (baseline of $62.24 \%$ ) settings, respectively. On the other hand, IDIV-SE achieves a boost of 5.54 and 9.76 p.p in the FS-CoT and ZS-CoT settings, respectively, over the baseline.
GPT-3.5 Results: From Table 2, we see that DivSE yields a gain of 21.84 and 13.04 p.p in the FSCoT (baseline of $30.38 \%$ ) and ZS-CoT (baseline of $31.90 \%$ ) settings, respectively. Likewise IDIVSE achieves a boost of 13.72 and 10.60 p.p in the FS-CoT and ZS-CoT settings, respectively.</p>
<h3>4.1.3 Planning via Blocksworld</h3>
<p>Setup: The benchmark provides both natural language and Planning Definition and Domain Language prompts (McDermott et al., 1998). We use natural language prompts in all the experiments. For the baseline runs, we introduce minor alterations to the prompt originally proposed by Valmeekam et al. (2023). These changes involve incorporating an explicit directive to prevent under-block movement and resolving minor language ambiguities we observed to be problematic during initial investigation. Furthermore, we reposition the initial condition and goal state information to the beginning of the prompt. The modified improved prompt is presented in Fig. 9.</p>
<p>We aggregate the plans through majority voting and utilize string matching for comparing the plans. As a result, we optimize the plan by eliminating the redundant "no-op" steps.
GPT-4 Results: We note that GPT-4 performs slightly better in a ZS setting, and use this to run all experiments. From Fig. 1, we observe that for Blocksworld 3, ZS-CoT records an accuracy of $70 \%$, while SC-10 reaches an accuracy level of $73 \%$. IDIV-SE enhances the absolute accuracy by 12 p.p above the ZS-CoT baseline, while Div-SE produces an impressive state-of-the-art accuracy of $94 \%$. An analysis of the six unsuccessful instances suggests the capacity for further performance improvement by increasing the size of the ensemble, as already two out of five current approaches generate accurate plans. For the Blocksworld 4/5 case, the ZS-CoT accuracy is $40 \%$, while SC-10 has an accuracy of $41.2 \%$. Here, IDIV-SE results</p>
<p>in an absolute gain of 17 p.p above the ZS-CoT baseline, and Div-SE too enhances performance, leading to $69.6 \%$. As outlined in Fig. 1 and 3, both IDIV-SE and DIV-SE achieve Pareto optimality. GPT-3.5 Results: The baseline performance on Blocksworld 3 is $6 \%$, and on Blocksworld $4 / 5$ is $0.6 \%$. We do not see any additional improvement using both IDIV-SE and DIV-SE. Qualitatively, we observe that during plan generation, GPT-3.5 fails to follow the restrictions provided as part of the problem instructions too often, leading to either infeasible or incorrect plans. This shows instruction following capabilities are crucial to the success of the methods proposed here.</p>
<h3>4.1.4 Constraint Satisfaction via GRAPH COLORING</h3>
<p>There may exist numerous non-optimal yet valid colorings for a given graph. Since exact string matching is not usable for identifying the majority solution from the ensembles of IDIV-SE and DiVSE, we employ the external, sound verifier (Stechly et al., 2023) to pick the correct solution.
GPT-4 Results: From Fig. 1, it is observed that ZS-CoT achieves an accuracy of $15 \%$, whereas SC-10 attains an accuracy level of $23 \%$. IDIV-SE improves the absolute accuracy by 59 p.p above the ZS-CoT baseline. Remarkably, Div-SE delivers a state-of-the-art accuracy of $97 \%$. Given that GPT-4's performance plateaus in the ZS setting, we chose to omit conducting the few-shot experiments. Summary: Methods in this work often demonstrate state-of-the-art performance on reasoning tasks. This is most significant in the planning and constraint satisfaction benchmarks, where the corresponding authors claimed immense difficulty for existing LLMs. Our work shows that statusquo prompt design approaches including chain of thought are too generic for these problems, and prompt customization (via DIVERSEPROMPTING) can yield substantial gains by guiding the chain of thought to the general nature of the problem.</p>
<h3>4.2 Open Source Models</h3>
<p>Due to the limited computational budget, we only performed experiments with the AQUA-RAT benchmark. Please refer to Appendix B for further details. Table 3 demonstrates the results for LLaMA-2 70B with 8-bit quantization. Div-SE and IDIV-SE demonstrate an improvement of over 10 p.p over the baseline in the FS-CoT settings. However, the gain in the ZS-CoT setting has been negligible. We hypothesize that this is partly due to model's lack of capabilities to both follow instructions and the mentioned approach in the absence of examples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompting Strategy</th>
<th style="text-align: center;">ZS-CoT (\%)</th>
<th style="text-align: center;">FS-CoT (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">31.32</td>
<td style="text-align: center;">29.1</td>
</tr>
<tr>
<td style="text-align: left;">IDIV-SE</td>
<td style="text-align: center;">27.00</td>
<td style="text-align: center;">39.7</td>
</tr>
<tr>
<td style="text-align: left;">Div-SE</td>
<td style="text-align: center;">$\mathbf{3 2 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{3 9 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on AQUA-RAT and LLaMA-2 70B.</p>
<h3>4.3 Errors \&amp; Prompt Utility</h3>
<p>Error Propagation: Due to the autoregressive nature of LLM decoding, early incorrect answers in IDIV-SE may get propagated to the latter ones. To quantify this, we select examples where the solution is incorrect and all five approaches produce the same erroneous answer. We focus only on these cases to see if e.g., a wrong conclusion in the initial approaches leaks into the following ones. Next, we attempt the last two approaches again in a separate session: if the LLM generates the same outcomes as in the original session (i.e., IDIV-SE setup) within 3 attempts, we consider it as no error propagation. However, if it does not produce the same answer within the 3 attempts, we interpret this as a case of error propagation since the change in answer could be attributed to the initial approaches with wrong answers in the chain. We measure this phenomenon on AQUA-RAT (FSCoT) on both GPT-4 and GPT-3.5. We find that GPT-4 and GPT-3.5 have error propagation rates of $6.2 \%$ and $5.5 \%$ respectively, which are comparable to performance differences between Div-SE and IDIV-SE, making error propagation one of the main explanatory hypotheses for the differences between the two methods. Reducing these error rates remains a challenging problem given the autoregressive nature of current LLMs.
Beyond Thinking Step by Step: The diverse approaches and personas we utilize not only enhance the performance in IDIV-SE and IDIV-SE, but are also independently superior to ZS-CoT. Table 4 highlights this effect, which showcases the importance of conditioning the model for solutions via DIVERSEPROMPTING.</p>
<h3>4.4 Alternative Aggregation Strategies</h3>
<p>Our aggregation thus far relies on majority voting. Alternatively, we can also utilize the meta reasoning technique proposed by Yoran et al. (2023) to</p>
<table>
<thead>
<tr>
<th>Dataset, Model</th>
<th>Persona, Approach</th>
<th>Accuracy (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>AQUA-RAT, GPT-3.5</td>
<td>$\emptyset$,Think step by step</td>
<td>57.48</td>
</tr>
<tr>
<td></td>
<td>$\emptyset$, Using Algebra</td>
<td>60.24 (+2.76)</td>
</tr>
<tr>
<td></td>
<td>Thinking like Alan Turing, $\emptyset$</td>
<td>61.81 (+4.33)</td>
</tr>
<tr>
<td></td>
<td>Dr. Patel: A renowned mathematician, $\emptyset$</td>
<td>65.75 (+8.27)</td>
</tr>
<tr>
<td>BlockSWORLD 4/5, GPT-4</td>
<td>$\emptyset$, State tracking prompt (Valmeekam et al., 2022)</td>
<td>42.00</td>
</tr>
<tr>
<td></td>
<td>$\emptyset$, Finite State Machine</td>
<td>55.80 (+13.80)</td>
</tr>
<tr>
<td></td>
<td>Alan Turing, Action Rationale</td>
<td>57.80 (+15.80)</td>
</tr>
<tr>
<td></td>
<td>Alan Turing, Progressive Block Placement Approach</td>
<td>58.80 (+16.80)</td>
</tr>
</tbody>
</table>
<p>Table 4: Prompts, derived from approaches and personas, boost performance. Blue rows denote ZS-CoT prompts, while black lines denote FS-CoT prompts. $\emptyset$ denotes absence (of persona or approach respectively).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">GPT-4 (\%)</th>
<th style="text-align: center;">GPT-3.5 (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Majority Voting</td>
<td style="text-align: center;">$\mathbf{7 9 . 9 0}$</td>
<td style="text-align: center;">64.47</td>
</tr>
<tr>
<td style="text-align: left;">Meta Reasoning</td>
<td style="text-align: center;">79.24</td>
<td style="text-align: center;">$\mathbf{6 7 . 7 0}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Alternative aggregation strategies. Observe that, for the AQUA-RAT benchmark (FS-CoT), IDivSE produces more accurate results only with GPT-3.5.
accumulate the results and exploit the rich information present in the reasoning steps. To this end, we store the responses generated by IDIV-SE, and request the model to meta reason over them in a different prompt and session. Table 5 suggests that the proposed reasoning paths contain rich information that is effectively exploited by the meta reasoning aggregation. Future post-hoc techniques may consider to learn the accuracy of the diverse prompting approaches, and weigh them accordingly. Nevertheless, the fact that techniques presented here provide visible improvements even with simple approaches like majority voting, demonstrates their added value independently from different aggregation algorithms.</p>
<h2>5 Related Work</h2>
<p>Prompt Optimization: Pryzant et al. (2023) models the prompts as optimizable discrete variables, and minimizes the loss of the reasoning task. Jones et al. (2023) optimize over the prompt space, but to identify failure modes. However, optimizationbased approaches often require the task to have a differentiable loss function, which is a strong condition. In our work, we utilize feedback from the LLM (not through gradients) during prompt design. Similarly to Cheng et al. (2023), IDIV-SE batches the responses for multiple queries within a prompt. Decoding Optimizations and Tools: Wang et al. (2023) replace the naive greedy decoding by sampling a diverse set of reasoning paths (e.g., through temperature sampling), and then selects the most consistent answer. Chen et al. (2022) express the reasoning process as a program, which is then delegated to an external tool. In our work, we neither change the decoding process nor assume the existence of trusted tools. This makes our solution directly applicable to black-box models.
Prompting Strategies: Brown et al. (2020) note that demonstrations to prompts, encoded as inputoutput pairs, produce drastic performance increase in larger LLMs. Wei et al. (2022) encourage internal dialogue by forcing the LLM to generate a sequence of intermediate steps for reasoning problems. This improves reasoning performance on larger LLMs (Nye et al., 2021; Chung et al., 2022; Kojima et al., 2022). Zhou et al. (2022) automatically break a complex problem into simpler subproblems and then solve them in sequence. Across all these techniques, the common practice is to keep the prompts fixed, but aggregate responses across multiple trials by varying the temperature. In our work, we vary the input prompt itself. A work that is similar in spirit is that of Yoran et al. (2023), which instead of aggregating the response of multiple reasoning paths, forces the model to reason across them before aggregation. Another relevant work is that of Li et al. (2023), which shows the importance of prompt diversity. However, they rely on selecting few-shot demonstrations from a holdout set (which defines diversity in their method), without explicitly stating reasoning pathways.</p>
<h2>6 Conclusions</h2>
<p>In this work, we promoted diversity of thought as a principled prompting strategy and proposed methodologies that leverage the LLM as a guide to design a diverse set of approaches to solve complex reasoning tasks. Extracting solution approaches from LLMs themselves becomes a discovery mechanism that seeds and conditions generative solutions. Reported results on a variety of tasks confirm that there is a large space for improvement in com-</p>
<p>plex reasoning by uncovering the necessary skills and knowledge from LLMs through targeted and diverse prompting methods. These results demonstrated how promoting diversity can improve the Pareto frontier of accuracy-cost trade-off for current LLMs and yield state-of-the-art solutions for planning and mathematical reasoning tasks. We hope that future work will expand these results to complex tasks from other real-world applications.</p>
<h2>7 Limitations</h2>
<p>Our study mainly experimented with GPT-3.5 and GPT-4 models because of their instructionfollowing capabilities. While current open-source models have shown remarkable improvements to this end, they are still not able to reliably follow instructions relevant to complex reasoning tasks (e.g. state tracking, plan validity, constraint satisfaction). We hope that progress in the field will enable further experimentation in this direction.</p>
<p>In addition, we also observe that error propagation during autoregressive generation may sometimes negatively impact the performance of IDivSe, where all approaches are executed in order within the same prompt. Some of this could be addressed by explicitly instructing the model to forget about the previous solution but ultimately as long as previous generation history remains in context and short-term memory, error propagation risks may still need to be tracked and measured.</p>
<h2>References</h2>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates Inc.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,</p>
<p>Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.</p>
<p>Zhoujun Cheng, Jungo Kasai, and Tao Yu. 2023. Batch prompting: Efficient inference with large language model apis.
H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, and S. Brahma. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems.</p>
<p>Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022a. LIm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339.</p>
<p>Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2022b. 8-bit optimizers via block-wise quantization. 9th International Conference on Learning Representations, ICLR.</p>
<p>Guidance. 2024. A guidance language for controlling large language models. https://github.com/ guidance-ai/guidance.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset.</p>
<p>Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. 2023. Automatically auditing large language models via discrete optimization. arXiv preprint arXiv:2303.04381.
T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5315-5333.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Drew McDermott, Malik Ghallab, Adele E Howe, Craig A Knoblock, Ashwin Ram, Manuela M Veloso, Daniel S Weld, and David E Wilkins. 1998. Pddl-the planning domain definition language.</p>
<p>Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. 2023. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452.</p>
<p>Michael Nye, Anders J Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Marten Bosma, Daan Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>OpenAI. 2022. Introducing chatgpt.
OpenAI. 2023a. Gpt-4 technical report.
OpenAI. 2023b. Gpt-4 technical report.
Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. 2023. Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495.</p>
<p>Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata. 2023. In-context impersonation reveals large language models' strengths and biases. arXiv preprint arXiv:2305.14930.</p>
<p>Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. arXiv preprint arXiv:2310.12397.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. 2023. On the planning abilities of large language models - a critical investigation.</p>
<p>Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2022. Large language models still can't plan (a benchmark for llms on planning and reasoning about change). arXiv preprint arXiv:2206.10498.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Conference on Neural Information Processing Systems (NeurIPS).</p>
<p>Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<p>Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, HengTze Cheng, Quoc V Le, Ed H Chi, Denny Zhou, Swaroop Mishra, and Huaixiu Steven Zheng. 2024. Selfdiscover: Large language models self-compose reasoning structures. arXiv preprint arXiv:2402.03620.</p>
<h2>Appendix</h2>
<h2>Appendix</h2>
<h2>A Prompt used for DIVERSEPROMPTING</h2>
<p>Our diverse prompting strategy for IDIV-SE and DIV-SE is showcased in Fig. 2 and Fig. 4 respectively. The instrumental prompt template that determines our approaches is presented in Fig. 5.</p>
<h2>B Model Details</h2>
<h2>B. 1 Open-Source Models</h2>
<p>We perform the Llama-2 70B experiments with a single 80GB A100 GPU. To fit the 70B model to a single A100, we use 8-bit precision through bitsandbytes (Dettmers et al., 2022a,b). Further, Dettmers et al. (2022a) reports no performance drop with this quantization method.</p>
<p>As the system prompt, we use You are a helpful, respectful and honest assistant. We perform inference with greedy decoding, having temperature $T=0$.</p>
<h2>C Additional Results</h2>
<p>In this section, we provide additional results on COMMONSENSEQA and GSM8K benchmarks.</p>
<h2>C. 1 Common sense via COMMONSENSEQA</h2>
<p>Table 2 presents the results of the experiments. Overall, the improvements in accuracy are relatively modest. This is likely because answering questions in COMMONSENSEQA does not demand as much reasoning and thought diversity as is required in some other benchmarks. In addition, the dataset also contains a number of ambiguous questions, which if read verbatim may have many plausible answers but the ground truth contains only one answer.</p>
<h2>C. 2 Arithmetic reseasoning via GSM8K</h2>
<p>GPT-4 Results: As shown in Fig. 6, accuracy on GSM8K have nearly plateaued, with the ZS-CoT and FS-CoT baselines achieving accuracies of $94 \%$ and $95 \%$ respectively. IDIV-SE does not produce any significant gains in either setting. On the other hand, DIV-SE reaches accuracy of $96.3 \%$ in both FS-CoT and ZS-CoT settings, providing a modest improvement.
GPT-3.5 Results: Here, the gains are more substantial. Compared to the ZS-CoT baseline of $76.11 \%$, IDIV-SE provides an improvement of 5.31 p.p. DIV-SE goes a step further, enhancing the accuracy by 10.39 p.p. In the FS-CoT setting, DIV-SE posts an accuracy improvement of 7.68 p.p (with a baseline accuracy of $81.4 \%$ ).</p>
<p>Fig. 3 (rightmost) presents the cost vs. accuracy trade-offs between IDIV-SE, DIV-SE, and SC. While the performance of SC does improve with the expansion of reasoning paths, both IDIV-SE and DIV-SE offer better trade-offs.</p>
<h2>D Evaluating Ensemble Sizes</h2>
<p>Figure 6 depicts the average accuracy of different ensemble sizes on GSM8K for both ZS-CoT and FS-CoT settings, utilizing GPT-4 and GPT-3.5. Similarly, Figure 7 demonstrates the average accuracy of various ensemble sizes on AquA for both ZS-CoT and FS-CoT settings, using GPT-4 and GPT-3.5. It is noteworthy that in both AQuA and GSM8K, even an ensemble of size three yields significant performance improvements over the baseline, which we attribute to the high diversity and independence of reasoning paths.</p>
<h2>E Prompt Templates</h2>
<p>The following section provides a comprehensive visual representation of the prompts used in our study. These prompts, depicted in Figures 9 through 16, were used in different settings and for the planning, AQuA, and graph coloring benchmarks, and incorporate various personas and approaches.</p>
<p>Figure 9 illustrates the prompt used in the baseline run, Figure 10 demonstrates the prompt employed when applying a Finite State Machine approach, Figure 11 depicts the prompt used when incorporating the persona of Alan Turing and an Action Rationale approach, and lastly, Figure 12 shows the prompt used when applying the persona of Alan Turing and the Progressive Block Placement approach in the zero-shot setting for the planning benchmark.</p>
<p>Figure 13 illustrates the algebraic approach, while Figure 14 and Figure 15 demonstrate the prompts incorporating the personas of Alan Turing and Dr. Patel, a renowned mathematician, respectively, in the few-shot-CoT setting for the AQuA benchmark.</p>
<p>In Figure 16 we present the IDIV-SE prompt used in the zero-shot setting for the graph coloring benchmark.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Div-Se prompting.</p>
<p>Use five distinct approaches to solve the given problem accurately. If there is no exact match choose the closest option.
Q: {Question}
Use the following output format:
Approach $1&lt;$ name of the approach $&gt;:&lt;$ Details of Approach $1&gt;$
Approach $2&lt;$ name of the approach $&gt;:&lt;$ Details of Approach $2&gt;$
Approach $3&lt;$ name of the approach $&gt;:&lt;$ Details of Approach $3&gt;$
Approach $4&lt;$ name of the approach $&gt;:&lt;$ Details of Approach $4&gt;$
Approach $5&lt;$ name of the approach $&gt;:&lt;$ Details of Approach $5&gt;$</p>
<p>Figure 5: Prompt template for extracting diverse approaches for problem solving.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Average accuracy for different ensemble sizes on GSM8K for ZS-CoT and FS-CoT settings on GPT-4 and GPT-3.5. Note that all graphs are zoomed in.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Average accuracy for different ensemble sizes on AQUA-RAT for ZS-CoT and FS-CoT settings on GPT-4 and GPT-3.5. Note that all graphs are zoomed in.</p>
<h1>Input Prompt</h1>
<p>Solve the problem accurately using {approach name}.
Question: John found that the average of 15 numbers is 40 . If 10 is added to each number then the mean of the numbers
Answer Choices: (A) 50 (B) 45 (C) 65 (D) 78 (E) 64
A: If 10 is added to each number, then the mean of the numbers also increases by 10 . So the new mean would be 50 . The answer is $(A)$.</p>
<p>Approach <using \{approach name\} >:</p>
<h2>Augmented Few-shot Examples</h2>
<h2>Approach <using visualization>:</h2>
<p>To solve this problem using visualization, let's consider the given information and the effect of adding 10 to each number on the average.</p>
<ol>
<li>The average of 15 numbers is 40 . This means that the sum of these 15 numbers is 15 * $40=600$.</li>
<li>Now, we add 10 to each number. Since there are 15 numbers, the total added is $15 *$ $10=150$.</li>
<li>The new sum of the 15 numbers is $600+150=750$.</li>
<li>To find the new average, we divide the new sum by the number of values, which is $750 / 15=50$. So, the mean of the numbers after adding 10 to each number is 50 . The answer is: (A) 50</li>
</ol>
<h2>Approach <using logical reasoning>:</h2>
<p>We know that adding the same constant to each number in a set will increase the average by that constant. In this case, we are adding 10 to each number, so the average will increase by 10 . New average $=40+10=50$ The answer is: (A) 50</p>
<h2>Approach <using elimination strategy>:</h2>
<p>We know that adding 10 to each number will increase the average by 10 . So, we're looking for an answer choice that is 10 more than the original average of 40 .
(A) 50 is 10 more than 40 .
(B) 45 is only 5 more than 40 .
(C) 65 is 25 more than 40 .
(D) 78 is 38 more than 40 .
(E) 64 is 24 more than 40 .</p>
<p>Only (A) 50 is exactly 10 more than the original average of 40 , so that's the correct answer. The other choices can be eliminated because they are not 10 more than the original average.</p>
<h2>Approach <using algebra>:</h2>
<p>Let's denote the sum of the 15 numbers as $S$. Since the average of the 15 numbers is 40 , we can write the equation: $S / 15=40 S=40 * 15=600$ Now, we add 10 to each number, so the new sum is $S+15 * 10=600+150=750$. The new average is: New average $=750 / 15=50$ The answer is: (A) 50</p>
<p>I am playing with a set of blocks where I need to arrange the blocks into stacks.</p>
<h1>[STATEMENT]</h1>
<p>As initial conditions I have that, the orange block is clear, the hand is empty, the blue block is on top of the red block, the orange block is on top of the blue block and the red block is on the table. My goal is to have that the red block on top of the blue block and the orange block on top of the red block.</p>
<p>Here are the actions I can do:
Pick up a block from the table
Unstack a block from on top of another block
Put down a block on the table
Stack a block on top of another block
I have the following restrictions on my actions:
I can only pick up or unstack one block at a time.
I can only pick up or unstack a block if my hand is empty.
I can only pick up a block if the block is on the table and the block is clear. A block is clear if the block has no other blocks on top of it and if the block is not picked up.
I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block.
I can only unstack a block from on top of another block if the block I am unstacking is clear.
Once I pick up or unstack a block, I am holding the block.
I can only put down a block that I am holding.
I can only stack a block on top and not under of another block if I am holding the block being stacked.
I can only stack a block on top and not under of another block if the block onto which I am stacking the block is clear.
Once I put down or stack a block, my hand becomes empty.
Once you stack a block on top of a second block, the second block is no longer clear.
What is the plan to achieve my goal? Just give the actions in the plan.
[PLAN]</p>
<p>Figure 9: Zero-shot prompt used in the baseline run of the Planning - Blocksworld Domain</p>
<p>You are playing with a set of blocks where you need to arrange the blocks into stacks. What is the plan to achieve the goal?
<Initial State> : As initial conditions you have that, the orange block is clear, the hand is empty, the blue block is on top of the red block, the orange block is on top of the blue block and the red block is on the table.
<Goal State> : Your goal is to have that the red block on top of the blue block and the orange block on top of the red block.</p>
<p>Here are the actions you can do:
-Pick up a block from the table
-Unstack a block from on top of another block
-Put down a block on the table
-Stack a block on top of another block
Rules:</p>
<ol>
<li>You can only pick up or unstack one block at a time.</li>
<li>You can only pick up or unstack a block if your hand is empty.</li>
<li>You can only pick up a block if the block is on the table and the block is clear. A block is clear if the block has no other blocks on top of it and if the block is not picked up.</li>
<li>You can only unstack a block from on top of another block if the block you are unstacking was really on top of the other block.</li>
<li>You can only unstack a block from on top of another block if the block you are unstacking is clear.</li>
<li>Once you pick up or unstack a block, you are holding the block.</li>
<li>You can only put down a block that you are holding.</li>
<li>You can only stack a block on top and not under of another block if you are holding the block being stacked.</li>
<li>You can only stack a block on top and not under of another block if the block onto which you are stacking the block is clear.</li>
<li>Once you put down or stack a block, your hand becomes empty.</li>
<li>Once you stack a block on top of a second block, the second block is no longer clear.</li>
</ol>
<p>Using a finite state machine and a search algorithm what is the plan to achieve the goal? You can model each state of the blocks configuration on the table and the hand as a state. For each action step check that the step follows the rules and that the step brings you closer to the goal. After each action describe the state of the table and hand. Always check whether the final state satisfies the goal mentioned. <Goal State> : Your goal is to have that the red block on top of the blue block and the orange block on top of the red block.
[PLAN]</p>
<p>Figure 10: The Zero-shot prompt using Finite State Machine Approach for solving the Planning - Blocksworld Domain Problem.</p>
<p>You are playing with a set of blocks where you need to arrange the blocks into stacks.
<Initial State> : As initial conditions you have that, the orange block is clear, the hand is empty, the blue block is on top of the red block, the orange block is on top of the blue block and the red block is on the table.
<Goal State> : Your goal is to have that the red block on top of the blue block and the orange block on top of the red block.</p>
<p>Here are the actions you can do:
-Pick up a block from the table
-Unstack a block from on top of another block
-Put down a block on the table
-Stack a block on top of another block
Rules:</p>
<ol>
<li>You can only pick up or unstack one block at a time.</li>
<li>You can only pick up or unstack a block if your hand is empty.</li>
<li>You can only pick up a block if the block is on the table and the block is clear. A block is clear if the block has no other blocks on top of it and if the block is not picked up.</li>
<li>You can only unstack a block from on top of another block if the block you are unstacking was really on top of the other block.</li>
<li>You can only unstack a block from on top of another block if the block you are unstacking is clear.</li>
<li>Once you pick up or unstack a block, you are holding the block.</li>
<li>You can only put down a block that you are holding.</li>
<li>You can only stack a block on top and not under of another block if you are holding the block being stacked.</li>
<li>You can only stack a block on top and not under of another block if the block onto which you are stacking the block is clear.</li>
<li>Once you put down or stack a block, your hand becomes empty.</li>
<li>Once you stack a block on top of a second block, the second block is no longer clear.</li>
</ol>
<p>Thinking like Alan Turing starting from the <Initial State> build a plan to get to the <Goal State>. For each action step carefully check that the step follows the rules. <Goal State> : Your goal is to have that the red block on top of the blue block and the orange block on top of the red block.
output format for each step until you reach the goal state:
$&lt;$ state&gt; : <state>
$&lt;$ action&gt; : $&lt;$ action to be performed in this step $&gt;$
<assess the action> : &lt; are we building the stack bottom up, check carefully&gt;</p>
<p>Figure 11: The Zero-shot prompt used with the persona of Alan Turing and Action Rationale approach for solving the Planning - Blocksworld Domain Problem.</p>
<p>You are playing with a set of blocks where you need to arrange the blocks into stacks.
<Initial State> : As initial conditions you have that, the orange block is clear, the hand is empty, the blue block is on top of the red block, the orange block is on top of the blue block and the red block is on the table.
<Goal State> : Your goal is to have that the red block on top of the blue block and the orange block on top of the red block.</p>
<p>Here are the actions you can do:
-Pick up a block from the table
-Unstack a block from on top of another block
-Put down a block on the table
-Stack a block on top of another block
Rules:</p>
<ol>
<li>You can only pick up or unstack one block at a time.</li>
<li>You can only pick up or unstack a block if your hand is empty.</li>
<li>You can only pick up a block if the block is on the table and the block is clear. A block is clear if the block has no other blocks on top of it and if the block is not picked up.</li>
<li>You can only unstack a block from on top of another block if the block you are unstacking was really on top of the other block.</li>
<li>You can only unstack a block from on top of another block if the block you are unstacking is clear.</li>
<li>Once you pick up or unstack a block, you are holding the block.</li>
<li>You can only put down a block that you are holding.</li>
<li>You can only stack a block on top and not under of another block if you are holding the block being stacked.</li>
<li>You can only stack a block on top and not under of another block if the block onto which you are stacking the block is clear.</li>
<li>Once you put down or stack a block, your hand becomes empty.</li>
<li>Once you stack a block on top of a second block, the second block is no longer clear.</li>
</ol>
<p>Thinking like Alan Turing, starting from the <Initial State> build a plan to get to the <Goal State> . For each action step carefully check that the step follows the rules. Divide the task into smaller steps, starting with placing the bottom block first, followed by the middle blocks, and finally the top block. <Goal State> : Your goal is to have that the red block on top of the blue block and the orange block on top of the red block.
[PLAN]</p>
<p>Figure 12: The Zero-shot prompt used in the Alan Turing + Progressive Block Placement Approach for solving the Planning - Blocksworld Domain Problem.</p>
<p>As a math professor, you will solve the given problem accurately '<using algebra>'. If there is no exact match choose the closest option.</p>
<p>Question: John found that the average of 15 numbers is 40 . If 10 is added to each number then the mean of the numbers
Answer Choices: (A) 50 (B) 45 (C) 65 (D) 78 (E) 64
Approach: <using algebra>
Let's denote the sum of the 15 numbers as $S$. Since the average of the 15 numbers is 40 , we can write the equation: $S / 15=40 S=40 * 15=600$ Now, we add 10 to each number, so the new sum is $S+15 * 10=600+150=750$. The new average is: New average $=750 / 15=50$
The answer is: (A) 50
Question: If a / b = 3/4 and $8 a+5 b=22$, then find the value of a.
Answer Choices: (A) $1 / 2$ (B) $3 / 2$ (C) $5 / 2$ (D) $4 / 2$ (E) $7 / 2$
Approach: <using algebra>
To solve the given problem, we can use substitution. Since a / b = 3/4, we can write that as: $a=3 b / 4$ Now, substitute this expression for a into the second equation: $8(3 b / 4)+$ $5 b=22$ Simplify and solve for $b: 6 b+5 b=2211 b=22 b=2$ Now that we have the value of $b$, we can find the value of $a: a=3 b / 4 a=3(2) / 4 a=6 / 4 a=3 / 2$ So, the value of a is (B) $3 / 2$.
The answer is: (B) $3 / 2$
Question: A person is traveling at $20 \mathrm{~km} / \mathrm{hr}$ and reached his destiny in 2.5 hr then find the distance?
Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km
Approach: <using algebra>
Using the formula distance $=$ speed $\times$ time, we can calculate the distance as follows:
Distance $=20 \mathrm{~km} / \mathrm{hr} \times 2.5 \mathrm{hr}=50 \mathrm{~km}$ So, the closest answer choice is (e) 50 km .
The answer is: (e) 50 km
Question: How many keystrokes are needed to type the numbers from 1 to 500 ?
Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788
Approach: <using algebra>
Let's break down the number of keystrokes needed into groups based on the number of digits: One-digit numbers (1-9): There are 9 one-digit numbers, so we need 9 keystrokes. Two-digit numbers (10-99): There are 90 two-digit numbers, each requiring 2 keystrokes, so we need $90 * 2=180$ keystrokes. Three-digit numbers (100-500): There are 401 three-digit numbers ( $500-100+1$ ), each requiring 3 keystrokes, so we need $401 * 3=$ 1203 keystrokes. Now let's add up the keystrokes from all groups: $9+180+1203=$ 1392. Therefore, the closest option is (b) 1392.</p>
<p>The answer is: (b) 1392</p>
<p>Figure 13: AQuA few-shot-CoT prompt using algebraic approach.</p>
<p>Think like Alan Turing and solve the problem. If there is no exact match choose the closest option.</p>
<p>Question: John found that the average of 15 numbers is 40 . If 10 is added to each number then the mean of the numbers
Answer Choices: (A) 50 (B) 45 (C) 65 (D) 78 (E) 64
Approach : <think like Alan Turing>
Let's analyze the problem step by step: The average of 15 numbers is 40 . So, the sum of these 15 numbers is $15 * 40=600.10$ is added to each number. Now, since there are 15 numbers, the total increase in the sum will be $15 * 10=150$. The new sum of these 15 numbers will be $600+150=750$. To find the new mean, divide the new sum by the total number of values, which is 15 . New mean $=750 / 15=50$. Therefore, the closest option to the new mean is (A) 50 .
The answer is: (A) 50
Question: If a / b $=3 / 4$ and $8 \mathrm{a}+5 \mathrm{~b}=22$, then find the value of a.
Answer Choices: (A) $1 / 2$ (B) $3 / 2$ (C) $5 / 2$ (D) $4 / 2$ (E) $7 / 2$
Approach : <think like Alan Turing>
Let us analyze the given information and solve the problem step by step. We are given that a / b $=3 / 4$, which can be written as a $=(3 / 4) <em>$ b. We are also given the equation $8 \mathrm{a}+5 \mathrm{~b}=22$. Substitute the value of a from step 1 into the equation from step $2: 8^{</em>}$ $(3 / 4) * b+5 b=22$ Simplify the equation: $6 b+5 b=22$ Combine the terms and solve for b: $11 \mathrm{~b}=22 \mathrm{~b}=2$ Now that we have the value of b, let's find the value of a using the equation from step 1: $a=(3 / 4) * b a=(3 / 4) * 2$ Simplify to find the value of $a: a=$ $3 / 2$ Thus, the value of a is $3 / 2$, which corresponds to answer choice (B).
The answer is: (B)
Question: A person is traveling at $20 \mathrm{~km} / \mathrm{hr}$ and reached his destiny in 2.5 hr then find the distance?
Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km
Approach : <think like Alan Turing>
To find the distance, we can use the formula: distance $=$ speed $\times$ time. In this case, the speed is $20 \mathrm{~km} / \mathrm{hr}$ and the time is 2.5 hours. Distance $=20 \mathrm{~km} / \mathrm{hr} \times 2.5 \mathrm{hr}=50 \mathrm{~km}$ So, the closest option is (e) 50 km .
The answer is: (e) 50 km
Question: How many keystrokes are needed to type the numbers from 1 to 500?
Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788
Approach : <think like Alan Turing>
To determine the number of keystrokes needed to type the numbers from 1 to 500, let's break it down by digit count: One-digit numbers (1-9): There are 9 one-digit numbers, each requiring 1 keystroke. Total: 9 keystrokes. Two-digit numbers (10-99): There are 90 two-digit numbers, each requiring 2 keystrokes. Total: $90 * 2=180$ keystrokes. Three-digit numbers (100-500): There are 401 three-digit numbers, each requiring 3 keystrokes. Total: $401 * 3=1203$ keystrokes. Adding the keystrokes for each group, we get: $9+180+1203=1392$ keystrokes. Thus, the closest option is (b) 1392
The answer is: (b) 1392</p>
<p>Figure 14: AQuA few-shot-CoT prompt with the persona of Alan Turing.</p>
<p>Think like Dr. Patel, the mathematician who specializes in algebra and solve the problem. If there is no exact match choose the closest option.</p>
<p>Question: John found that the average of 15 numbers is 40 . If 10 is added to each number then the mean of the numbers
Answer Choices: (A) 50 (B) 45 (C) 65 (D) 78 (E) 64
Approach : <think like Dr. Patel>
Let's consider the problem step by step. John found that the average of 15 numbers is 40 . To find the sum of these 15 numbers, we can use the formula: Sum $=$ Average $\times$ Number of elements Sum $=40 \times 15$ Sum $=600$ Now, 10 is added to each of the 15 numbers. So, we can find the new sum by adding $10 \times 15=150$ to the original sum: New Sum $=600$ +150 New Sum $=750$ To find the new average (mean), we can use the formula: New Average $=$ New Sum $\div$ Number of elements New Average $=750 \div 15$ New Average $=50$ The answer is: (A) 50</p>
<p>Question: If a / b $=3 / 4$ and $8 \mathrm{a}+5 \mathrm{~b}=22$, then find the value of a.
Answer Choices: (A) $1 / 2$ (B) $3 / 2$ (C) $5 / 2$ (D) $4 / 2$ (E) $7 / 2$
Approach : <think like Dr. Patel>
To solve this problem, first we can rewrite the equation a / b $=3 / 4$ as a $=(3 / 4) \mathrm{b}$. Now we can substitute this expression for a into the second equation: $8 \mathrm{a}+5 \mathrm{~b}=22$. Substitute a with $(3 / 4) \mathrm{b}: 8(3 / 4) \mathrm{b}+5 \mathrm{~b}=22$ Simplify the equation: $6 \mathrm{~b}+5 \mathrm{~b}=22$ Combine the terms: $11 \mathrm{~b}=22$ Now, divide both sides by $11: \mathrm{b}=2$ Now that we have the value of b , we can find the value of a using the equation $\mathrm{a}=(3 / 4) \mathrm{b}: \mathrm{a}=(3 / 4)(2) \mathrm{a}=3 / 2$ The value of a is $3 / 2$. So, the correct answer is (B) $3 / 2$.
The answer is: (B) $3 / 2$
Question: A person is traveling at $20 \mathrm{~km} / \mathrm{hr}$ and reached his destiny in 2.5 hr then find the distance?
Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km
Approach : <think like Dr. Patel>
To find the distance, we can use the formula: Distance $=$ Speed $\times$ Time Here, the speed is given as $20 \mathrm{~km} / \mathrm{hr}$ and the time taken is 2.5 hours. So, we can plug these values into the formula: Distance $=20 \mathrm{~km} / \mathrm{hr} \times 2.5 \mathrm{hr}$ Distance $=50 \mathrm{~km}$
The answer is: (e) 50 km
Question: How many keystrokes are needed to type the numbers from 1 to 500?
Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788
Approach : <think like Dr. Patel>
To determine the number of keystrokes needed to type the numbers from 1 to 500, let's break down the problem by analyzing the number of digits in each range: For numbers $1-9$, there are 9 numbers with 1 digit each, requiring 9 keystrokes. For numbers 10-99, there are 90 numbers with 2 digits each, requiring $90 * 2=180$ keystrokes. For numbers 100-499, there are 400 numbers with 3 digits each, requiring $400 * 3=1200$ keystrokes. For the number 500, it has 3 digits, requiring 3 keystrokes. Now, we can sum up the keystrokes for each range: 9 (for 1-9) +180 (for 10-99) +1200 (for 100-499) +3 (for $500)=1392$ keystrokes.
The answer is: (b) 1392</p>
<p>Figure 15: AQuA few-shot-CoT prompt with the persona of Dr. Patel: A renowned mathematician</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ In practice, we set $c=100, b=5, n \in{3,5}$, and $|V|&lt;20$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ For a given reasoning task, we perform this process once (for GPT-3.5 Turbo), and re-use our selection across all LLMs we evaluate.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
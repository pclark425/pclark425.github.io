<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4476 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4476</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4476</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-265043839</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.eval4nlp-1.14.pdf" target="_blank">Which is better? Exploring Prompting Strategy For LLM-based Metrics</a></p>
                <p><strong>Paper Abstract:</strong> This paper describes the DSBA submissions to the Prompting Large Language Models as Explainable Metrics shared task, where systems were submitted to two tracks: small and large summarization tracks. With advanced Large Language Models (LLMs) such as GPT-4, evaluating the quality of Natural Language Generation (NLG) has become increasingly paramount. Traditional similarity-based metrics such as BLEU and ROUGE have shown to misalign with human evaluation and are ill-suited for open-ended generation tasks. To address this issue, we explore the potential capability of LLM-based metrics, especially leveraging open-source LLMs. In this study, wide range of prompts and prompting techniques are systematically analyzed with three approaches: prompting strategy, score aggregation, and explainability. Our research focuses on formulating effective prompt templates, determining the granularity of NLG quality scores and assessing the impact of in-context examples on LLM-based evaluation. Furthermore, three aggregation strategies are compared to identify the most reliable method for aggregating NLG quality scores. To examine explainability, we devise a strategy that generates rationales for the scores and analyzes the characteristics of the explanation produced by the open-source LLMs. Extensive experiments provide insights regarding evaluation capabilities of open-source LLMs and suggest effective prompting strategies.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4476.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4476.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HG prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Guideline prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt template adapted from human annotation guidelines (SummEval) that provides succinct, clear task description, aspect definitions and stepwise evaluation instructions for LLM-based evaluation of summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human Guideline (HG) prompt</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide the LLM with succinct, human-style annotation instructions adapted from SummEval including: a task description (short/long, optional expert role), explicit evaluation criteria broken into aspect definitions, and step-by-step evaluation steps (chain-of-thought style). The LLM outputs per-aspect scores (fine-grained) or an overall score (coarse). Variants modify task length and inclusion of expert role to probe sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relevance, Consistency/Factuality, Coherence, Fluency (aspect definitions AD; human-targeted criteria HT used in HG), plus the ability to produce rationales when asked.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B, Orca-13B, Hermes-13B, Platypus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation (summarization evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluative explanations/summaries (assessment of generated summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>HG prompt outperformed the baseline and MG prompts across models. Example Kendall's Tau (fine-grained + Direct): Orca-7B 0.3472, Orca-13B 0.4468 versus baseline Direct Orca-7B 0.2500 and Orca-13B 0.3040. HG prompt produced consistently best development-set performance and strong test-set results after Filtering/Binning.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: LLM-produced numeric scores are compared to human gold annotations; hybrid when rationales are generated and manually analyzed for error types.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation of model-produced scores with golden human annotation scores (segment-level Kendall's Tau) on SummEval (dev/train) and Eval4NLP test data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Prompt sensitivity (length/expert role effects vary by model size), potential for introducing bias via phrasing, dependence on human-style definitions (HT) that may not transfer to non-summary scientific theories.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SummEval (Fabbri et al., 2020) for development; Eval4NLP 2023 test set (MQM-annotated summaries) for testing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4476.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4476.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MG prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model Guideline prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A detailed, directive instruction template modeled after guidelines used for LLMs (e.g., GPT-4 in G-EVAL), containing explicit, model-targeted scoring criteria and stepwise evaluation instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Model Guideline (MG) prompt</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Give the LLM prescriptive, model-oriented instructions (MT style) with explicit scoring considerations and stepwise procedures for evaluating summaries; used to assess whether directive, algorithmic instructions improve alignment with human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same aspects as HG (Relevance, Consistency/Factuality, Coherence, Fluency) but phrased with model-targeted criteria (MT) rather than human-targeted HT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B, Orca-13B, Hermes-13B, Platypus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation (summarization evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluative explanations/summaries</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>MG prompt generally outperformed baseline but was worse than HG in many cases. Example (Direct, fine-grained): Orca-13B MG 0.3583 vs HG 0.4343 (table entries). MG showed improvements over baseline but less consistent than HG.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated LLM scoring compared to human gold annotations using Kendall's Tau.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation with SummEval human scores (Kendall's Tau).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>More complex, directive instructions did not always improve alignment; model-targeted phrasing sometimes underperformed simpler human-style instructions, indicating formulation-sensitive behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SummEval (dev/train) and Eval4NLP test set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4476.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4476.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Score granularity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granularity of scoring (coarse-grained vs fine-grained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two scoring schemes: coarse-grained produces a single holistic score; fine-grained produces per-aspect scores (relevance, factuality, coherence, fluency) which are averaged to an overall score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Fine-grained versus Coarse-grained scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Coarse-grained: ask LLM to assign one holistic quality score. Fine-grained: ask LLM to rate each defined aspect independently (1–5), then average aspect scores to obtain final score. Fine-grained permits more explicit criteria and diagnostic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-aspect: Relevance, Consistency/Factuality, Coherence, Fluency; Coarse is an aggregate of these implicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B, Orca-13B, Hermes-13B, Platypus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG / summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluative judgments of generated outputs (diagnostic explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Fine-grained scoring consistently outperformed coarse-grained across model sizes and prompts. Authors report that fine-grained removes ambiguity and yields higher Kendall's Tau (exact deltas vary by model; general increase across experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated scoring by LLMs compared to human annotations (Kendall's Tau).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Performance measured by segment-level Kendall's Tau correlation with human mean scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>More demanding prompt; requires LLM to reliably distinguish aspects; susceptible to aggregation biases (averaging may mask aspect-level errors).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SummEval (used for per-aspect human annotations) and Eval4NLP test set (MQM-based aspects).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4476.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4476.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Demonstration (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Learning demonstrations (examples with/without rationales; best/worst)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of one-shot in-context examples (source, summary, human score) or examples augmented with human/GPT-4-generated rationales, and categorizing examples as 'best' (score 5) or 'worst' (score 1) to study ICL effects and bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>In-Context Demonstrations (with/without rationale; best/worst examples)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Include a single demonstration example in the prompt due to input-length constraints: either (1) source+summary+score or (2) source+summary+score+rationale (rationale distilled from GPT-4). Examples are sampled per-aspect and categorized as 'best' or 'worst' to probe bias introduced by demonstration.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same summary aspects; also assesses whether demonstration content biases subsequent scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B, Orca-13B, Hermes-13B, Platypus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation (summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluative explanations and rationale generation</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Providing demonstrations often decreased performance, particularly for smaller models (likely due to limited ICL capability). 'Best' examples yielded higher performance than 'worst' across models but introduced upward bias (models tended to give higher scores when shown best examples). Example: demonstration decreased Orca-7B performance markedly; Orca-13B could benefit from rationale-containing demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated LLM outputs compared to human gold scores; authors also qualitatively analyze rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Kendall's Tau correlation with human annotations, plus qualitative error analysis of rationales (36 sampled instances).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Demonstration-induced bias (score skew), model-size dependent ICL limits, potential for misunderstandings from small models, and risk of overfitting to demonstration style rather than underlying criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4476.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4476.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct score aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregation method where the LLM directly outputs a discrete score (e.g., integer 1–5); that output is used as the final numeric score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Direct aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt LLM to output a score token (discrete integer rating). Use that returned score directly as the aspect score. Typically decoding temperature set to 0 to force deterministic selection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-aspect integer ratings (1–5) for relevance, factuality, coherence, fluency; final score may be average of aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B, Orca-13B, Hermes-13B, Platypus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>numeric evaluative judgments</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Direct aggregation performed strongly and often best or tied with Logprob. Direct yields integer outputs and fewer unique values which may benefit Kendall's Tau due to ties. Example: Human HG + Direct gave Orca-13B Kendall's Tau 0.4468 (one of top results).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated LLM scoring compared with human scores via Kendall's Tau.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Segment-level Kendall's Tau against SummEval gold; authors note possible metric bias towards tie-friendly methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Produces coarse discrete scores with limited resolution; may artificially inflate correlation metrics that reward ties (authors mention Xu et al. observation that Kendall Tau can favor ties).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4476.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4476.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logprob aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Log-probability weighted aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Compute a continuous score by weighting each discrete score by the model's generation probability for that score token, producing a probabilistic expectation over discrete ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Logprob aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For predefined discrete scores S={s1..sK}, compute score = sum_i p(s_i) * s_i where p(s_i) is the model's token generation probability for the token sequence representing s_i. This yields a continuous, probability-weighted expected score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-aspect rating mapped to discrete set (1–5) with probabilities used to create continuous final score.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B, Orca-13B, Hermes-13B, Platypus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>probabilistic evaluative scoring</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Logprob performed comparably to Direct and often better than Approximation; it produced continuous scores and achieved high Kendall's Tau in many prompt/model combinations (e.g., Humanox Logprob Orca-13B 0.4210).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated comparison to human scores using Kendall's Tau.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Kendall's Tau correlation with human annotations; compared across aggregation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires access to token-level probabilities (not always available across APIs), sensitivity to tokenization, and complexity in mapping multi-token score representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4476.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4476.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Approximation aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling-based approximation aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Estimate score distribution by sampling N outputs from the LLM and computing empirical frequencies of discrete scores to approximate token probabilities, then compute expected score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Approximation aggregation (sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Sample N completions, count occurrences of each discrete score s_i, compute g(s_i) = count(s_i)/N, then score = sum_i g(s_i) * s_i. Intended as an approximation to Logprob when direct probabilities are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-aspect discrete scores aggregated into expected score via sampling frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B, Orca-13B, Hermes-13B, Platypus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>stochastic evaluative scoring</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Approximation underperformed relative to Direct and Logprob across experiments; authors attribute lower performance to sampling noise (e.g., Humanox Approximation Orca-13B 0.4002 vs Humanox Direct 0.4468).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated sampling-based scores compared to human annotations via Kendall's Tau.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Kendall's Tau against SummEval gold; comparison across aggregation approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Sampling noise, higher compute cost (multiple samples), and sensitivity to sampling temperature (authors set temp=1 for Approximation causing more stochasticity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4476.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4476.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RG prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rationale Generation prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dedicated prompt to elicit explicit rationales/explanations for the numeric scores the LLM outputs, used to probe explainability of LLM-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Rationale Generation (RG) prompt</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt the LLM to produce not only a score but an accompanying rationale that explains reasons for the assigned score following prescribed steps; demonstrations with 'Reason-best' or 'Reason-worst' examples can be included to study influence on rationale quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same per-aspect criteria; additionally evaluates whether rationales are consistent, non-hallucinated, and aspect-focused.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-13B (analysis focused), Orca-7B etc.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B / 70B (detailed analysis reported for Orca-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation / explainability</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>explanatory rationales for judgments</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>RG prompt produced similar or slightly lower Kendall's Tau compared to HG prompts but better than MG in some configurations. It enabled analysis of rationales: in a sampled set of 36 instances, majority 'good' rationales 50–69% depending on demonstration; error types identified: inconsistent rationale vs score (11–17%), hallucination (6–36%), discussion of wrong aspect (6–8%).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated numeric scoring compared to human annotations; rationales analyzed qualitatively against source and human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Kendall's Tau for score alignment; manual error analysis of rationales against source text and assigned scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Rationale hallucinations, inconsistencies between rationale and assigned score, occasional focus on wrong aspect; model-size dependency (larger LLMs produce better rationales).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4476.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4476.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Filtering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Filtering (test-phase failure detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A binary filter prompt used during test-phase to detect failed/degenerate summaries (e.g., repeated phrases) and assign them the minimum score uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Filtering heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run a filtering prompt that labels a summary as 'redundant/failed' (Yes/No) focusing on structural failures; if labeled 'Yes', assign minimum score (e.g., 1) to avoid spurious high evaluation of poor outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Detect redundancy/failure in summary structure (not general quality); intervene by forcing lower score for clear generation failures.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-13B and Platypus-70B (test submissions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>post-hoc filtering of generated outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Applying Filtering improved test-set performance; authors observed discernible improvement when Filtering was applied.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated heuristic applied before scoring; final scores compared to human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Observed improvement in Kendall's Tau on test set after applying Filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Rule-based filter may be brittle and could misclassify marginal cases; assigning uniform minimum score could penalize recoverable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4476.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4476.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Binning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binning (score quantization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Post-processing strategy to reduce noise by grouping proximate model scores into a smaller number of discrete bins, thereby reducing spurious fine-grained distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Binning (score smoothing)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>After obtaining continuous or fine-grained scores, map scores into fewer discrete categories (bins) to reduce score variance/noise across same-quality samples; authors applied to test scores to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Reduces sensitivity to tiny numeric differences; intended to better align model score distributions with human annotation granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-13B, Platypus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>post-processing of numeric evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Binning improved test performance by reducing noise. After Binning, unique score counts reduced (Orca-13B: from 36 to 10; Platypus-70B: from 46 to 13) and Kendall's Tau improved in submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated post-processing then compared to human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Observed test-set Kendall's Tau increase after Binning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Choice of bin thresholds is heuristic; may reduce useful signal or mask meaningful distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4476.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4476.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation aspects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aspect-based evaluation criteria (Relevance, Consistency/Factuality, Coherence, Fluency)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses the four SummEval aspects (relevance, consistency/factuality, coherence, fluency) as the core evaluation dimensions, with variations in how criteria are phrased (AD, HT, MT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Aspect-based criteria (AD/HT/MT variants)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Define explicit aspect definitions (AD), human-targeted criteria (HT) used in HG prompts, and model-targeted criteria (MT) used in MG prompts. AD gives concise definitions; HT/MT provide scoring considerations and descriptions for each aspect to guide LLM judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relevance (captures central themes), Consistency/Factuality (accuracy relative to source), Coherence (logical ordering and flow), Fluency (grammatical quality/clarity).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Orca-7B, Orca-13B, Hermes-13B, Platypus-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B / 70B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>diagnostic evaluation of generated content</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Including AD (aspect definitions) consistently improved performance regardless of prompt template or model size; AD-GPT variants (generated by GPT-4) produced similar improvements, indicating robustness of concise aspect definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated scoring guided by aspect criteria compared with human per-aspect annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Kendall's Tau correlation per-aspect and averaged vs SummEval human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Aspects may not capture domain-specific dimensions for scientific theories (paper limited to summarization); cross-aspect averaging can mask trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4476.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4476.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kendall's Tau</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Segment-level Kendall's Tau correlation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The primary numeric performance metric used to assess alignment between LLM-produced scores and human gold annotations, computed at the segment (sentence) level.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Kendall's Tau (segment-level)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute Kendall's Tau rank correlation between per-segment scores produced by the LLM-based metric and the golden human annotation scores (averaged aspects for the task). Used to quantify monotonic alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Rank correlation of predicted vs human scores; sensitive to pairwise ordering and ties.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation methodology/statistics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>metric for evaluating evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used throughout to report results; e.g., HG+Direct Orca-13B Kendall's Tau 0.4468 (development), baseline Direct Orca-13B 0.3040. Authors discuss that Kendall's Tau may favor tie-prone aggregation like Direct.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Metric quantifying automated-vs-human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Standard statistical correlation with human annotations; authors note potential metric biases (ties).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Kendall's Tau can be affected by ties (Direct aggregation generates more ties), possibly inflating apparent correlation; does not measure calibration or absolute score differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4476.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e4476.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Datasets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SummEval and Eval4NLP test (MQM-annotated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SummEval benchmark used for training/development with human per-aspect scores; Eval4NLP 2023 test set composed of Wikipedia fragments and MQM annotations used for test evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SummEval (train/dev) and Eval4NLP test (MQM)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>SummEval: source-summary pairs with human annotations for relevance, consistency, coherence, fluency (used as golden human annotation scores). Eval4NLP test: Wikipedia fragments and model-generated summaries annotated under MQM for factuality/relevance/readability used for submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-aspect human annotation scores aggregated (SummEval average) used as gold; test MQM annotations used for factuality/relevance/readability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark datasets for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Development experiments used SummEval; test submissions used Eval4NLP test with MQM and showed improvements when Filtering and Binning applied. Authors averaged relevance, factuality, fluency (Direct) for final test scores.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated LLM scoring validated against human annotations in these datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation (Kendall's Tau) with dataset human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>SummEval human scores skew towards higher values (3–5) creating bias; MQM aspects differ from SummEval and required GPT-4 to generate factuality criteria for the test prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which is better? Exploring Prompting Strategy For LLM-based Metrics', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-eval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Gptscore: Evaluate as you desire <em>(Rating: 2)</em></li>
                <li>The eval4nlp 2023 shared task on prompting large language models as explainable metrics <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 1)</em></li>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4476",
    "paper_id": "paper-265043839",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "HG prompt",
            "name_full": "Human Guideline prompt",
            "brief_description": "A prompt template adapted from human annotation guidelines (SummEval) that provides succinct, clear task description, aspect definitions and stepwise evaluation instructions for LLM-based evaluation of summaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Human Guideline (HG) prompt",
            "evaluation_method_description": "Provide the LLM with succinct, human-style annotation instructions adapted from SummEval including: a task description (short/long, optional expert role), explicit evaluation criteria broken into aspect definitions, and step-by-step evaluation steps (chain-of-thought style). The LLM outputs per-aspect scores (fine-grained) or an overall score (coarse). Variants modify task length and inclusion of expert role to probe sensitivity.",
            "evaluation_criteria": "Relevance, Consistency/Factuality, Coherence, Fluency (aspect definitions AD; human-targeted criteria HT used in HG), plus the ability to produce rationales when asked.",
            "model_name": "Orca-7B, Orca-13B, Hermes-13B, Platypus-70B",
            "model_size": "7B / 13B / 70B",
            "scientific_domain": "Natural Language Generation (summarization evaluation)",
            "theory_type": "evaluative explanations/summaries (assessment of generated summaries)",
            "human_comparison": true,
            "evaluation_results": "HG prompt outperformed the baseline and MG prompts across models. Example Kendall's Tau (fine-grained + Direct): Orca-7B 0.3472, Orca-13B 0.4468 versus baseline Direct Orca-7B 0.2500 and Orca-13B 0.3040. HG prompt produced consistently best development-set performance and strong test-set results after Filtering/Binning.",
            "automated_vs_human_evaluation": "Automated: LLM-produced numeric scores are compared to human gold annotations; hybrid when rationales are generated and manually analyzed for error types.",
            "validation_method": "Correlation of model-produced scores with golden human annotation scores (segment-level Kendall's Tau) on SummEval (dev/train) and Eval4NLP test data.",
            "limitations_challenges": "Prompt sensitivity (length/expert role effects vary by model size), potential for introducing bias via phrasing, dependence on human-style definitions (HT) that may not transfer to non-summary scientific theories.",
            "benchmark_dataset": "SummEval (Fabbri et al., 2020) for development; Eval4NLP 2023 test set (MQM-annotated summaries) for testing.",
            "uuid": "e4476.0",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MG prompt",
            "name_full": "Model Guideline prompt",
            "brief_description": "A detailed, directive instruction template modeled after guidelines used for LLMs (e.g., GPT-4 in G-EVAL), containing explicit, model-targeted scoring criteria and stepwise evaluation instructions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Model Guideline (MG) prompt",
            "evaluation_method_description": "Give the LLM prescriptive, model-oriented instructions (MT style) with explicit scoring considerations and stepwise procedures for evaluating summaries; used to assess whether directive, algorithmic instructions improve alignment with human scores.",
            "evaluation_criteria": "Same aspects as HG (Relevance, Consistency/Factuality, Coherence, Fluency) but phrased with model-targeted criteria (MT) rather than human-targeted HT.",
            "model_name": "Orca-7B, Orca-13B, Hermes-13B, Platypus-70B",
            "model_size": "7B / 13B / 70B",
            "scientific_domain": "Natural Language Generation (summarization evaluation)",
            "theory_type": "evaluative explanations/summaries",
            "human_comparison": true,
            "evaluation_results": "MG prompt generally outperformed baseline but was worse than HG in many cases. Example (Direct, fine-grained): Orca-13B MG 0.3583 vs HG 0.4343 (table entries). MG showed improvements over baseline but less consistent than HG.",
            "automated_vs_human_evaluation": "Automated LLM scoring compared to human gold annotations using Kendall's Tau.",
            "validation_method": "Correlation with SummEval human scores (Kendall's Tau).",
            "limitations_challenges": "More complex, directive instructions did not always improve alignment; model-targeted phrasing sometimes underperformed simpler human-style instructions, indicating formulation-sensitive behavior.",
            "benchmark_dataset": "SummEval (dev/train) and Eval4NLP test set.",
            "uuid": "e4476.1",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Score granularity",
            "name_full": "Granularity of scoring (coarse-grained vs fine-grained)",
            "brief_description": "Two scoring schemes: coarse-grained produces a single holistic score; fine-grained produces per-aspect scores (relevance, factuality, coherence, fluency) which are averaged to an overall score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Fine-grained versus Coarse-grained scoring",
            "evaluation_method_description": "Coarse-grained: ask LLM to assign one holistic quality score. Fine-grained: ask LLM to rate each defined aspect independently (1–5), then average aspect scores to obtain final score. Fine-grained permits more explicit criteria and diagnostic reasoning.",
            "evaluation_criteria": "Per-aspect: Relevance, Consistency/Factuality, Coherence, Fluency; Coarse is an aggregate of these implicitly.",
            "model_name": "Orca-7B, Orca-13B, Hermes-13B, Platypus-70B",
            "model_size": "7B / 13B / 70B",
            "scientific_domain": "NLG / summarization evaluation",
            "theory_type": "evaluative judgments of generated outputs (diagnostic explanations)",
            "human_comparison": true,
            "evaluation_results": "Fine-grained scoring consistently outperformed coarse-grained across model sizes and prompts. Authors report that fine-grained removes ambiguity and yields higher Kendall's Tau (exact deltas vary by model; general increase across experiments).",
            "automated_vs_human_evaluation": "Automated scoring by LLMs compared to human annotations (Kendall's Tau).",
            "validation_method": "Performance measured by segment-level Kendall's Tau correlation with human mean scores.",
            "limitations_challenges": "More demanding prompt; requires LLM to reliably distinguish aspects; susceptible to aggregation biases (averaging may mask aspect-level errors).",
            "benchmark_dataset": "SummEval (used for per-aspect human annotations) and Eval4NLP test set (MQM-based aspects).",
            "uuid": "e4476.2",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Demonstration (ICL)",
            "name_full": "In-Context Learning demonstrations (examples with/without rationales; best/worst)",
            "brief_description": "Use of one-shot in-context examples (source, summary, human score) or examples augmented with human/GPT-4-generated rationales, and categorizing examples as 'best' (score 5) or 'worst' (score 1) to study ICL effects and bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "In-Context Demonstrations (with/without rationale; best/worst examples)",
            "evaluation_method_description": "Include a single demonstration example in the prompt due to input-length constraints: either (1) source+summary+score or (2) source+summary+score+rationale (rationale distilled from GPT-4). Examples are sampled per-aspect and categorized as 'best' or 'worst' to probe bias introduced by demonstration.",
            "evaluation_criteria": "Same summary aspects; also assesses whether demonstration content biases subsequent scoring.",
            "model_name": "Orca-7B, Orca-13B, Hermes-13B, Platypus-70B",
            "model_size": "7B / 13B / 70B",
            "scientific_domain": "NLG evaluation (summarization)",
            "theory_type": "evaluative explanations and rationale generation",
            "human_comparison": true,
            "evaluation_results": "Providing demonstrations often decreased performance, particularly for smaller models (likely due to limited ICL capability). 'Best' examples yielded higher performance than 'worst' across models but introduced upward bias (models tended to give higher scores when shown best examples). Example: demonstration decreased Orca-7B performance markedly; Orca-13B could benefit from rationale-containing demonstrations.",
            "automated_vs_human_evaluation": "Automated LLM outputs compared to human gold scores; authors also qualitatively analyze rationales.",
            "validation_method": "Kendall's Tau correlation with human annotations, plus qualitative error analysis of rationales (36 sampled instances).",
            "limitations_challenges": "Demonstration-induced bias (score skew), model-size dependent ICL limits, potential for misunderstandings from small models, and risk of overfitting to demonstration style rather than underlying criteria.",
            "uuid": "e4476.3",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Direct aggregation",
            "name_full": "Direct score aggregation",
            "brief_description": "Aggregation method where the LLM directly outputs a discrete score (e.g., integer 1–5); that output is used as the final numeric score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Direct aggregation",
            "evaluation_method_description": "Prompt LLM to output a score token (discrete integer rating). Use that returned score directly as the aspect score. Typically decoding temperature set to 0 to force deterministic selection.",
            "evaluation_criteria": "Per-aspect integer ratings (1–5) for relevance, factuality, coherence, fluency; final score may be average of aspects.",
            "model_name": "Orca-7B, Orca-13B, Hermes-13B, Platypus-70B",
            "model_size": "7B / 13B / 70B",
            "scientific_domain": "NLG evaluation",
            "theory_type": "numeric evaluative judgments",
            "human_comparison": true,
            "evaluation_results": "Direct aggregation performed strongly and often best or tied with Logprob. Direct yields integer outputs and fewer unique values which may benefit Kendall's Tau due to ties. Example: Human HG + Direct gave Orca-13B Kendall's Tau 0.4468 (one of top results).",
            "automated_vs_human_evaluation": "Automated LLM scoring compared with human scores via Kendall's Tau.",
            "validation_method": "Segment-level Kendall's Tau against SummEval gold; authors note possible metric bias towards tie-friendly methods.",
            "limitations_challenges": "Produces coarse discrete scores with limited resolution; may artificially inflate correlation metrics that reward ties (authors mention Xu et al. observation that Kendall Tau can favor ties).",
            "uuid": "e4476.4",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Logprob aggregation",
            "name_full": "Log-probability weighted aggregation",
            "brief_description": "Compute a continuous score by weighting each discrete score by the model's generation probability for that score token, producing a probabilistic expectation over discrete ratings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Logprob aggregation",
            "evaluation_method_description": "For predefined discrete scores S={s1..sK}, compute score = sum_i p(s_i) * s_i where p(s_i) is the model's token generation probability for the token sequence representing s_i. This yields a continuous, probability-weighted expected score.",
            "evaluation_criteria": "Per-aspect rating mapped to discrete set (1–5) with probabilities used to create continuous final score.",
            "model_name": "Orca-7B, Orca-13B, Hermes-13B, Platypus-70B",
            "model_size": "7B / 13B / 70B",
            "scientific_domain": "NLG evaluation",
            "theory_type": "probabilistic evaluative scoring",
            "human_comparison": true,
            "evaluation_results": "Logprob performed comparably to Direct and often better than Approximation; it produced continuous scores and achieved high Kendall's Tau in many prompt/model combinations (e.g., Humanox Logprob Orca-13B 0.4210).",
            "automated_vs_human_evaluation": "Automated comparison to human scores using Kendall's Tau.",
            "validation_method": "Kendall's Tau correlation with human annotations; compared across aggregation methods.",
            "limitations_challenges": "Requires access to token-level probabilities (not always available across APIs), sensitivity to tokenization, and complexity in mapping multi-token score representations.",
            "uuid": "e4476.5",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Approximation aggregation",
            "name_full": "Sampling-based approximation aggregation",
            "brief_description": "Estimate score distribution by sampling N outputs from the LLM and computing empirical frequencies of discrete scores to approximate token probabilities, then compute expected score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Approximation aggregation (sampling)",
            "evaluation_method_description": "Sample N completions, count occurrences of each discrete score s_i, compute g(s_i) = count(s_i)/N, then score = sum_i g(s_i) * s_i. Intended as an approximation to Logprob when direct probabilities are unavailable.",
            "evaluation_criteria": "Per-aspect discrete scores aggregated into expected score via sampling frequencies.",
            "model_name": "Orca-7B, Orca-13B, Hermes-13B, Platypus-70B",
            "model_size": "7B / 13B / 70B",
            "scientific_domain": "NLG evaluation",
            "theory_type": "stochastic evaluative scoring",
            "human_comparison": true,
            "evaluation_results": "Approximation underperformed relative to Direct and Logprob across experiments; authors attribute lower performance to sampling noise (e.g., Humanox Approximation Orca-13B 0.4002 vs Humanox Direct 0.4468).",
            "automated_vs_human_evaluation": "Automated sampling-based scores compared to human annotations via Kendall's Tau.",
            "validation_method": "Kendall's Tau against SummEval gold; comparison across aggregation approaches.",
            "limitations_challenges": "Sampling noise, higher compute cost (multiple samples), and sensitivity to sampling temperature (authors set temp=1 for Approximation causing more stochasticity).",
            "uuid": "e4476.6",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RG prompt",
            "name_full": "Rationale Generation prompt",
            "brief_description": "A dedicated prompt to elicit explicit rationales/explanations for the numeric scores the LLM outputs, used to probe explainability of LLM-based metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Rationale Generation (RG) prompt",
            "evaluation_method_description": "Prompt the LLM to produce not only a score but an accompanying rationale that explains reasons for the assigned score following prescribed steps; demonstrations with 'Reason-best' or 'Reason-worst' examples can be included to study influence on rationale quality.",
            "evaluation_criteria": "Same per-aspect criteria; additionally evaluates whether rationales are consistent, non-hallucinated, and aspect-focused.",
            "model_name": "Orca-13B (analysis focused), Orca-7B etc.",
            "model_size": "7B / 13B / 70B (detailed analysis reported for Orca-13B)",
            "scientific_domain": "NLG evaluation / explainability",
            "theory_type": "explanatory rationales for judgments",
            "human_comparison": true,
            "evaluation_results": "RG prompt produced similar or slightly lower Kendall's Tau compared to HG prompts but better than MG in some configurations. It enabled analysis of rationales: in a sampled set of 36 instances, majority 'good' rationales 50–69% depending on demonstration; error types identified: inconsistent rationale vs score (11–17%), hallucination (6–36%), discussion of wrong aspect (6–8%).",
            "automated_vs_human_evaluation": "Hybrid: automated numeric scoring compared to human annotations; rationales analyzed qualitatively against source and human scores.",
            "validation_method": "Kendall's Tau for score alignment; manual error analysis of rationales against source text and assigned scores.",
            "limitations_challenges": "Rationale hallucinations, inconsistencies between rationale and assigned score, occasional focus on wrong aspect; model-size dependency (larger LLMs produce better rationales).",
            "uuid": "e4476.7",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Filtering",
            "name_full": "Filtering (test-phase failure detection)",
            "brief_description": "A binary filter prompt used during test-phase to detect failed/degenerate summaries (e.g., repeated phrases) and assign them the minimum score uniformly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Filtering heuristic",
            "evaluation_method_description": "Run a filtering prompt that labels a summary as 'redundant/failed' (Yes/No) focusing on structural failures; if labeled 'Yes', assign minimum score (e.g., 1) to avoid spurious high evaluation of poor outputs.",
            "evaluation_criteria": "Detect redundancy/failure in summary structure (not general quality); intervene by forcing lower score for clear generation failures.",
            "model_name": "Orca-13B and Platypus-70B (test submissions)",
            "model_size": "13B / 70B",
            "scientific_domain": "NLG evaluation",
            "theory_type": "post-hoc filtering of generated outputs",
            "human_comparison": true,
            "evaluation_results": "Applying Filtering improved test-set performance; authors observed discernible improvement when Filtering was applied.",
            "automated_vs_human_evaluation": "Automated heuristic applied before scoring; final scores compared to human annotations.",
            "validation_method": "Observed improvement in Kendall's Tau on test set after applying Filtering.",
            "limitations_challenges": "Rule-based filter may be brittle and could misclassify marginal cases; assigning uniform minimum score could penalize recoverable outputs.",
            "uuid": "e4476.8",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Binning",
            "name_full": "Binning (score quantization)",
            "brief_description": "Post-processing strategy to reduce noise by grouping proximate model scores into a smaller number of discrete bins, thereby reducing spurious fine-grained distinctions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Binning (score smoothing)",
            "evaluation_method_description": "After obtaining continuous or fine-grained scores, map scores into fewer discrete categories (bins) to reduce score variance/noise across same-quality samples; authors applied to test scores to improve robustness.",
            "evaluation_criteria": "Reduces sensitivity to tiny numeric differences; intended to better align model score distributions with human annotation granularity.",
            "model_name": "Orca-13B, Platypus-70B",
            "model_size": "13B / 70B",
            "scientific_domain": "NLG evaluation",
            "theory_type": "post-processing of numeric evaluations",
            "human_comparison": true,
            "evaluation_results": "Binning improved test performance by reducing noise. After Binning, unique score counts reduced (Orca-13B: from 36 to 10; Platypus-70B: from 46 to 13) and Kendall's Tau improved in submissions.",
            "automated_vs_human_evaluation": "Automated post-processing then compared to human annotations.",
            "validation_method": "Observed test-set Kendall's Tau increase after Binning.",
            "limitations_challenges": "Choice of bin thresholds is heuristic; may reduce useful signal or mask meaningful distinctions.",
            "uuid": "e4476.9",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Evaluation aspects",
            "name_full": "Aspect-based evaluation criteria (Relevance, Consistency/Factuality, Coherence, Fluency)",
            "brief_description": "The paper uses the four SummEval aspects (relevance, consistency/factuality, coherence, fluency) as the core evaluation dimensions, with variations in how criteria are phrased (AD, HT, MT).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Aspect-based criteria (AD/HT/MT variants)",
            "evaluation_method_description": "Define explicit aspect definitions (AD), human-targeted criteria (HT) used in HG prompts, and model-targeted criteria (MT) used in MG prompts. AD gives concise definitions; HT/MT provide scoring considerations and descriptions for each aspect to guide LLM judgments.",
            "evaluation_criteria": "Relevance (captures central themes), Consistency/Factuality (accuracy relative to source), Coherence (logical ordering and flow), Fluency (grammatical quality/clarity).",
            "model_name": "Orca-7B, Orca-13B, Hermes-13B, Platypus-70B",
            "model_size": "7B / 13B / 70B",
            "scientific_domain": "NLG evaluation",
            "theory_type": "diagnostic evaluation of generated content",
            "human_comparison": true,
            "evaluation_results": "Including AD (aspect definitions) consistently improved performance regardless of prompt template or model size; AD-GPT variants (generated by GPT-4) produced similar improvements, indicating robustness of concise aspect definitions.",
            "automated_vs_human_evaluation": "Automated scoring guided by aspect criteria compared with human per-aspect annotations.",
            "validation_method": "Kendall's Tau correlation per-aspect and averaged vs SummEval human scores.",
            "limitations_challenges": "Aspects may not capture domain-specific dimensions for scientific theories (paper limited to summarization); cross-aspect averaging can mask trade-offs.",
            "uuid": "e4476.10",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Kendall's Tau",
            "name_full": "Segment-level Kendall's Tau correlation",
            "brief_description": "The primary numeric performance metric used to assess alignment between LLM-produced scores and human gold annotations, computed at the segment (sentence) level.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Kendall's Tau (segment-level)",
            "evaluation_method_description": "Compute Kendall's Tau rank correlation between per-segment scores produced by the LLM-based metric and the golden human annotation scores (averaged aspects for the task). Used to quantify monotonic alignment.",
            "evaluation_criteria": "Rank correlation of predicted vs human scores; sensitive to pairwise ordering and ties.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Evaluation methodology/statistics",
            "theory_type": "metric for evaluating evaluators",
            "human_comparison": null,
            "evaluation_results": "Used throughout to report results; e.g., HG+Direct Orca-13B Kendall's Tau 0.4468 (development), baseline Direct Orca-13B 0.3040. Authors discuss that Kendall's Tau may favor tie-prone aggregation like Direct.",
            "automated_vs_human_evaluation": "Metric quantifying automated-vs-human agreement.",
            "validation_method": "Standard statistical correlation with human annotations; authors note potential metric biases (ties).",
            "limitations_challenges": "Kendall's Tau can be affected by ties (Direct aggregation generates more ties), possibly inflating apparent correlation; does not measure calibration or absolute score differences.",
            "uuid": "e4476.11",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Datasets",
            "name_full": "SummEval and Eval4NLP test (MQM-annotated)",
            "brief_description": "SummEval benchmark used for training/development with human per-aspect scores; Eval4NLP 2023 test set composed of Wikipedia fragments and MQM annotations used for test evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "SummEval (train/dev) and Eval4NLP test (MQM)",
            "evaluation_method_description": "SummEval: source-summary pairs with human annotations for relevance, consistency, coherence, fluency (used as golden human annotation scores). Eval4NLP test: Wikipedia fragments and model-generated summaries annotated under MQM for factuality/relevance/readability used for submissions.",
            "evaluation_criteria": "Per-aspect human annotation scores aggregated (SummEval average) used as gold; test MQM annotations used for factuality/relevance/readability.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLG evaluation",
            "theory_type": "benchmark datasets for evaluation",
            "human_comparison": null,
            "evaluation_results": "Development experiments used SummEval; test submissions used Eval4NLP test with MQM and showed improvements when Filtering and Binning applied. Authors averaged relevance, factuality, fluency (Direct) for final test scores.",
            "automated_vs_human_evaluation": "Automated LLM scoring validated against human annotations in these datasets.",
            "validation_method": "Correlation (Kendall's Tau) with dataset human annotations.",
            "limitations_challenges": "SummEval human scores skew towards higher values (3–5) creating bias; MQM aspects differ from SummEval and required GPT-4 to generate factuality criteria for the test prompt.",
            "uuid": "e4476.12",
            "source_info": {
                "paper_title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-eval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire",
            "rating": 2,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        },
        {
            "paper_title": "The eval4nlp 2023 shared task on prompting large language models as explainable metrics",
            "rating": 2,
            "sanitized_title": "the_eval4nlp_2023_shared_task_on_prompting_large_language_models_as_explainable_metrics"
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 1,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 1,
            "sanitized_title": "large_language_models_are_stateoftheart_evaluators_of_translation_quality"
        }
    ],
    "cost": 0.01753975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Which is better? Exploring Prompting Strategy For LLM-based Metrics</p>
<p>Joonghoon Kim joonghoon_kim@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Saeran Park saeran_park@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Kiyoon Jeong kiyoon_jeong@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Sangmin Lee sangmin_lee@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Seung Hun 
Korea University
SeoulRepublic of Korea</p>
<p>Han Jiyoon Lee 
Korea University
SeoulRepublic of Korea</p>
<p>Pilsung Kang pilsung_kang@korea.ac.kr 
Korea University
SeoulRepublic of Korea</p>
<p>Which is better? Exploring Prompting Strategy For LLM-based Metrics
CDB68B2F165C159DB388EC4DB463D5A3
This paper describes the DSBA submissions to the Prompting Large Language Models as Explainable Metrics shared task, where systems were submitted to two tracks: small and large summarization tracks.With advanced Large Language Models (LLMs) such as GPT-4, evaluating the quality of Natural Language Generation (NLG) has become increasingly paramount.Traditional similarity-based metrics such as BLEU and ROUGE have shown to misalign with human evaluation and are illsuited for open-ended generation tasks.To address this issue, we explore the potential capability of LLM-based metrics, especially leveraging open-source LLMs.In this study, wide range of prompts and prompting techniques are systematically analyzed with three approaches: prompting strategy, score aggregation, and explainability.Our research focuses on formulating effective prompt templates, determining the granularity of NLG quality scores and assessing the impact of in-context examples on LLM-based evaluation.Furthermore, three aggregation strategies are compared to identify the most reliable method for aggregating NLG quality scores.To examine explainability, we devise a strategy that generates rationales for the scores and analyzes the characteristics of the explanation produced by the open-source LLMs.Extensive experiments provide insights regarding evaluation capabilities of open-source LLMs and suggest effective prompting strategies. 1</p>
<p>Introduction</p>
<p>As Large Language Models (LLMs) like GPT-4 continue to advance rapidly, the Natural Language Generation (NLG) capability is approaching a level of expertise comparable to that of a human.As a result, the precise evaluation of NLG has become increasingly paramount.However, traditional similarity-based metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which are widely used in NLG evaluations, tend to show a discrepancy from human assessments (Liu et al., 2023).Additionally, the reliance on reference texts for these metrics can hinder an accurate assessment of NLG quality, particularly for open-ended generation tasks.</p>
<p>Recent research has introduced methodologies that leverage LLMs as NLG evaluators, showcasing the potential of LLM-based metrics.These approaches are motivated from findings in recent research which revealed that LLM can directly evaluate NLG capabiltiy harnessing knowledge retained during the pre-train (Xu et al., 2023).These metrics have demonstrated notable correlation (Fu et al., 2023;Liu et al., 2023;Kocmi and Federmann, 2023;Fernandes et al., 2023) with human evaluations to learned evaluators (Chiang and yi Lee, 2023;Svikhnushina and Pu, 2023).</p>
<p>Concurrently, recent advancement of LLMs such as LLaMA (Touvron et al., 2023), Vicuna (Zheng et al., 2023), and Orca (Mukherjee et al., 2023), has paved a way for research on NLG evaluations utilizing open-source LLMs (Xu et al., 2023).However, there are few comprehensive studies that systematically evaluate the vast amount of possible prompts and prompting techniques for LLM-based metrics.Especially, research assessing the capabilities of open-source LLMs in the context of LLM-based metrics is even more scarce.Given the importance of enhancing the reproducibility of LLM-based metrics in metric research, there is a clear need for studies that explore effective prompts and prompting techniques specifically for open-source LLMs (Chiang and yi Lee, 2023).</p>
<p>In this work, we conduct a thorough exploration of various prompts and prompting techniques for effective deployment of open-source LLMs as metrics: analyze them in terms of prompting strategy, score aggregation, and explainability.</p>
<p>Within the scope of prompting strategies, we compare the effectiveness of human and model instruction templates for NLG evaluation.In addition, we explore granularity in score assignment to accurately evaluate NLG quality.Additionally, we gauge the influence of the open-source LLM's In-Context Learning (ICL) capability (Brown et al., 2020) in NLG evaluation by employing various types of demonstrated examples.For score aggregation, we compare three methodologies to discern the optimal strategy for aggregating NLG quality scores.To infer the explainability of opensource LLMs, we generate rationale when computing scores.These comprehensive experiments on prompting techniques for LLM-based metrics provide insights into the evaluation capabilities of open-source LLMs and guidelines for effective prompting strategies.</p>
<p>Furthermore, we provide insights derived from analysis of the features embedded in prompts and behaviors of open-source LLMs as LLM-based metrics.Additionally, we report our strategies and outcomes applied to the test set of summarization track in Eval4NLP 2023 shared task.</p>
<p>Related Work</p>
<p>Similarity-based Metrics</p>
<p>Similarity-based metrics evaluate the quality of NLG outputs by comparing reference and candidate text.They can be categorized into lexical-based and semantic-based metrics.Lexical-based metrics, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), utilize N-grams to measure lexical overlap between a reference and a candidate text.However, research has highlighted their inadequacy in accurately assessing the quality of generated outputs and identifying both syntactical and semantic discrepancies (Liu et al., 2023;Polišenská et al., 2021;Wu et al., 2021).On the other hand, semantic-based metrics, including BERTScore (Zhang et al., 2019) and MoverScore (Zhao et al., 2019), measure semantic similarity by comparing the embeddings of both reference and candidate texts.However, similar to lexical-based metrics, they face challenges when evaluating open-ended generation tasks due to their inherent dependence on reference text (Chiang and yi Lee, 2023;Guan et al., 2021;Gu et al., 2021).</p>
<p>LLM-based Metrics</p>
<p>The recent substantial advancement in the NLG capabilities of LLMs has motivated research interests related to LLM-based metrics.Consequently, the latest studies, primarily exploring various prompting approaches that do not require additional training of an LLM, has shown a correlation with human evaluation comparable to that of learned evaluators (Chiang and yi Lee, 2023;Svikhnushina and Pu, 2023).Also, building upon the foundational work of LLaMA (Touvron et al., 2023), research on the fine-tuning approach which constructs an evaluator by fine-tuning an LLM with suitable supervised data for the evaluation task, is being actively pursued (Bosselut et al., 2019;Xu et al., 2023).</p>
<p>Summarization Track</p>
<p>The summarization track of Eval4NLP 2023 shared task (Leiter et al., 2023) aims to propose a reference-free metric for summarization.Specifically, reference-free metric evaluates a given summary using only the provided source sentence or paragraph without additional human-written references.The objective of shared task is to develop LLM-based metrics by exploring effective prompting strategies for open-source LLMs.</p>
<p>Dataset</p>
<p>Train and Development Set</p>
<p>In this study, we utilize the SummEval benchmark dataset provided by Fabbri et al. (2020) as both train and development sets.While the original benchmark provides human annotation scores for each of four aspects, including relevance, consistency, coherence, and fluency, the summarization track adopts the average of these aspect scores as golden human annotation scores.The performance of the evaluation task is measured through sentence-level correlation with the golden human annotation scores.</p>
<p>Test Set</p>
<p>Dataset provided in the shared task (Leiter et al., 2023), consisting of sentences and fragments of paragraphs from English Wikipedia documents written after July 15, 2023, is used as the test set.Summaries in the test dataset were generated by a summary generation model that are annotated with reference to Multidimensional Quality Metrics (MQM) annotation for aspects like factuality, relevance, and readability.</p>
<p>2</p>
<p>Figure 1: Examples of Human Guideline (HG) prompt and Model Guideline (MG) prompt.HG prompt and MG prompt consists of task description, evaluation criteria, and evaluation steps.The HG prompt is used as the annotation guideline for summarization evaluation, serving as the basis for human annotators assessments.In contrast, the MG prompt was used as the instruction for the model.</p>
<p>Models</p>
<p>We use four out of six open-source LLMs provided in the Eval4NLP 2023 shared task.</p>
<p>• Hermes-13B -LLaMA-13B model trained on over 300,000 instructions.• Orca-7B -LLaMA2-7B model trained on Orca Style dataset.• Orca-13B -LLaMA2-13B model trained on Open-Platypus dataset and OpenOrca dataset.• Platypus-70B -LLaMA2-70B model trained by Lee et al. (2023).</p>
<p>Method</p>
<p>In this section, we address the prompting strategies and score aggregation methods, as well as approaches to assess the explainability of open-source LLMs.</p>
<p>Prompting Strategy</p>
<p>Prompting strategies consist of prompt template, granularity of score, and demonstration.</p>
<p>Prompt Template</p>
<p>We propose Human Guideline (HG) prompt and Model Guideline (MG) prompt for summary evaluation as illustrated in Figure 1.The HG prompt, adapted from the human evaluation guideline of SummEval (Fabbri et al., 2020), provides clear evaluation instructions and criteria for human annotators.</p>
<p>Conversely, the MG prompt, implemented from a guideline given to LLM such as GPT-4 for summary evaluation in G-EVAL (Liu et al., 2023), instructs LLM to assess summaries, offering detailed, directive instructions and criteria.</p>
<p>Both HG prompt and MG prompt consist of elements such as task description, evaluation criteria, and evaluation steps.To assess the impact of each element, we create variants by modifying each one.</p>
<p>Task Description</p>
<p>The task description provides instructions for the specified task.To explore the influence of its length, we craft short and long descriptions by varying sentence lengths, maintaining the original context.Additionally, we create an expert-role task description to study the effect of providing an expert role in the evaluation (e.g."you're an expert at summarizing news articles.").Each variant is developed for both HG and MG prompts, with details in Appendix D.</p>
<p>Evaluation Criteria</p>
<p>The evaluation criteria outlines the scoring standards for the given summary per aspect.It is categorized into three components, 1) Aspect Definition (AD) 2) Human-Targeted criteria (HT) 3) Model-Targeted criteria (MT).</p>
<p>AD, adopted from GPTScore (Fu et al., 2023), concisely describes the evaluation aspect definitions.HT and MT, used in HG and MG Prompts respectively, include scoring considerations and as-3 166 pect descriptions.</p>
<p>To investigate the effects of each components, we generate modified version of AD, HT, and MT for each aspect using GPT-4.We instruct GPT-4 to maintain a consistent format with the existing ones.Examples are provided in Appendix D.</p>
<p>Evaluation Steps</p>
<p>The evaluation steps, which could be considered as a Chain-of-Thought (CoT) (Zhang et al., 2023), provide step-by-step instructions for the evaluation task, enhancing the reasoning capabilities of LLM.To explore the impact of varied evaluation steps descriptions, we construct detailed complex evaluation steps for both HG and MG prompts.Examples are provided in Appendix D.</p>
<p>Granularity of Score</p>
<p>For assigning a score, we consider the following two scoring approaches: coarse-grained scoring and fine-grained scoring.Coarse-grained scoring yields a singular and holistic score that considers all evaluation aspects collectively, but does not provide scores for individual aspects.Conversely, finegrained scoring assigns the score for each aspect, deriving individual scores and then averaging them to yield the final singular score.This approach enables the LLMs to furnish both the overall score and specific aspect scores, granting a more nuanced understanding of for score derivation compared to the coarse-grained method.Given that NLG evaluations commonly score by jointly taking multiple aspects into account, adpoting fined-grained scoring when constructing variants of the prompt is naturally apt approach.</p>
<p>Demonstration</p>
<p>To examine the ICL capability of open-source LLMs in evaluation tasks, we craft two distinct types of demonstrated examples.</p>
<p>One set of examples includes raw source text, a summary, and a human annotation score.On the other hand, another set of examples incorporates a rationale derived from the assigned human annotation score, which has been distilled from GPT-42 , in addition to the components found in the former set of examples.Examples are provided in Appendix D.</p>
<p>Furthermore, we construct examples for each individual aspect and subsequently group them into 'worst' and 'best' categories based on human annotation scores.In our study, 'worst' examples are assigned a score of 1, while 'best' examples receiving a score of 5. Categorization is undertaken to investigate potential biases in the quality and the score of the provided examples.Due to the maximum input length constraint of the LLMs, we use only one example as demonstration per summary.</p>
<p>Score Aggregation</p>
<p>To derive scores for individual aspects, we propose the following three score aggregation methods: Direct, Logprob, and Approximation (see Figure 2).Direct This method is the most general scoring method.It leverages the score generated by the LLM directly.</p>
<p>Logprob This method calculates the score by summing the product of a pre-defined discrete score range (e.g. 1 to 5) and the generation probability of the corresponding tokens.This method is considered as a weighted summation approach, using each score's token probability as its weight.By incorporating the model's token generation probabilities, this method distinctively produces a more continuous score.</p>
<p>For a given set of pre-defined discrete scores S = {s 1 , ..., s K }, Logprob multiplies each discrete score s i by its token probability p(s i ).K in (1) is the number of pre-defined discrete scores.
score = K i=1 p(s i ) • s i (1)</p>
<p>Approximation</p>
<p>This method calculates the score by averaging N sampled scores generated by LLM.Intending to approximate the token probability distribution, we design Approximation method to distinguish it from the Logprob method, which directly uses the actual token probabilities.This aggregation is inspired by techniques explored in (Liu et al., 2023;Fu et al., 2023).</p>
<p>For a given set of pre-defined discrete scores S = {s 1 , ..., s K }, Approximation multiplies each discrete score s i by its approximated token probability g(s i ).In (2), count(s i ) denotes the number of count discrete score s i appears in N samples.</p>
<p>g(s
i ) = count(s i ) N (2) score = K i=1 g(s i ) • s i (3)</p>
<p>Explainability</p>
<p>Evaluations that employ the previously described methods yield only a sole scalar score with no additional explanation for the assigned score at all.Thus, we manually craft the Rationale Generation (RG) prompt to derive rationales for the scores.</p>
<p>Using this prompt, we aim to explore the explainability of open-source LLMs (see Figure 2).Furthermore, similar to the approach used in the demonstration section 4.1.3,we use examples to analyze the influence of demonstrated examples on rationale generation.Each example is divided into 'worst' and 'best' example to examine potential biases in the outputs.</p>
<p>Test phase</p>
<p>For the test set, we incorporate two supplementary approaches alongside the previously described prompting strategy, tailored to the attributes of the test set.</p>
<p>Filtering Although many summaries in the test set exhibit appropriate sentence structures, certain samples retain repetitive words or phrases (e.g."A family of four members, including a first member, a second member, a third member, and a fourth member.").We deem such instance as a failure to generate an appropriate summary and uniformly assigned them lowest score.To account such instances, we design a Filtering prompt that filters failed samples.For given summaries, when model generates a 'Yes' response, they are assigned the minimum score.Example of the Filtering prompt is provided in Appendix D.</p>
<p>Binning After analyzing the scores assigned by the model for the test data, we observe that opensource LLMs are generally adept at evaluating summaries.Nevertheless, we note the model's tendency of assigning excessively fine-grained scores among samples of equivalent quality (e.g.scores of 1 and 1.01).In light of these observations, we implement Binning to simplify the score distribution and mitigate noise, thereby integrating proximate scores into same categories.Detailed explanations can be found in the Appendix B.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Experiments are conducted using the development set of the summarization track provided in the shared task.We use the provided prompt template for the summarization track as the baseline prompt.</p>
<p>The baseline prompt contains a brief task description and score guide.Additionally, the HG and MG prompt in 5.2 are adapted from SummEval (Fabbri Moreover, we report the leaderboard results for the test set using Orca-13B and Platypus-70B for the small and large track, respectively.Test set experiments share the almost the same setting with development set experiments: same HG prompt, finegrained scoring, hyperparameters for Direct aggregation are implemented.For factuality evaluation criteria, not originally provided in SummEval (Fabbri et al., 2020), we use GPT-4 to generate it.Specifically, scores for relevance, factuality, and fluency, obtained from Direct aggregation, are averaged to compute the final score.Throughout our all experiments, segment-level Kendall's Tau correlation is used as the performance metric.For optimized inference with open-source LLMs, we employ Guidance3 and vLLM4 libraries.Details of experimental setup are provided in Appendix A.</p>
<p>Main Results</p>
<p>Prompting Strategy</p>
<p>We compare the performance with different types of the prompt templates.As shown in Prompting section of Table 1, regardless of the granularity of the score, we observe that HG and MG prompts, especially HG prompt, consistently outperform the baseline prompt.We hypothesize that a more detailed description of task provided in the HG and MG prompt allows LLM to understand and follow the instructions more clearly.Moreover, among all the prompts, the HG prompt achieves the best performance, indicating that succinct and clear instructions are better than complex ones.</p>
<p>As for granularity of the scoring, fine-grained scoring consistently outperforms coarse-grained scoring across various model sizes and prompt templates.The coarse-grained scoring may introduce ambiguity in the evaluation criteria by requiring the LLM to consider aspect-specific considerations in an integrated manner.Conversely, the fine-grained scoring removes such ambiguity by providing evaluation criteria of each aspect independently.</p>
<p>As shown in Demonstration section of Table 1, we observe that the use of demonstration leads to decrease in performance, likely due to the inherent bias introduced by the demonstrated example.Notably, the smaller model exhibits a significant decline in performance, which could be attributed to their limited ICL capabilities (Dong et al., 2022;Han et al., 2023;Wei et al., 2023), resulting in inaccurate understanding of in-context examples, and vice versa.The performance differs among models based on whether they are provided with examples containing only the score or examples with additional rationales.This discrepancy can be attributed to the superior ability of larger models in comprehending in-context examples, which leads to better understanding when explanations for scores are added.In contrast, the smaller model exhibits the opposite behavior.Furthermore, providing the 'best' examples consistently yields superior performance across all model sizes when compared to the 'worst' examples.After conducting an analysis of the model's score distribution, we observe a bias wherein the model tends to assign higher scores when provided with the 'best' example.We hypothesize that observed bias may be driven by the skewed distribution of human annotation scores in the development set, where human annotation scores are predominantly distributed towards higher values, mainly falling between 3 and 5.</p>
<p>Score Aggregation</p>
<p>We assess the performance based on the different score aggregation methods.Aggregation section of Table 1 illustrates that, across various model sizes and prompt templates, Direct and Logprob aggregation consistently demonstrates superior performance when compared to the Approximation aggregation.In both Direct and Logprob aggregation, the decoding temperature is set to 0. This likely leads the model to assign scores in a more deterministic manner compared to the Approximation, potentially resulting in superior performance.Specifically, since Approximation estimates the distribution of score token probability through sampling, sampling noise could account for its lower performance.Unlike other aggregation methods, Direct aggregation generates integer values ranging from 1 to 5, thereby offering a much fewer score range.On the other hand, Xu et al. (2023) suggest that Kendall Tau might favor tie pairs.Such tendency could explain the notably high correlation observed with Direct aggregation.</p>
<p>Explainability</p>
<p>We assess the LLM's ability to provide appropriate explanations for the scores.Examining Explainability section of Table 1, we observe that the RG prompt results in performance similar to or slightly lower than the HG prompt and better than the MG prompt.This suggests that generating rationales for scores can also aid the evaluation process itself.Furthermore, it is noteworthy that Orca-7B exhibits a slight performance decline when provided with a demonstrated example, in contrast to the performance of Orca-13B.The RG prompt is meticulously designed to facilitate the generation of rationales, possibly benefiting from the examples.Therefore, Orca-13B, with superior ICL capabilities as mentioned in 4.1, has outperformed the other smaller model.Analysis of the rationales generated by Orca-13B is discussed in 5.3.3.In Table 2, we report the performance of the HG prompt on the test set.Details of HG prompt applied for the test set are provided in Appendix D. As evident from the results of our development set experiments, the performance of the HG prompt on the test set is consistently satisfactory across all models.Furthermore, we observe a discernible improvement in performance when the Filtering is applied.This observation suggests that uniformly assigning lowest scores to inadequately generated summaries can enhance performance.Similarly, Binning enhances performance by reducing noise in the scores on the test set.This improvement is achieved by integrating closely related scores into same categories.While the Orca-13B model exhibits a slightly lower performance compared to the Platypus-70B with the base HG prompt, it shows superior performance after the application of Filtering and Binning.Details of test phase are provided in Appendix B.</p>
<p>Test</p>
<p>Analysis</p>
<p>The Effect of Different Model Sizes</p>
<p>We compare the performance depending on different model sizes: Orca-7B, Hermes-13B, Orca-13B, and Platypus-70B.As shown in Appendix Table 4 and Table 5, despite the same size with Orca-13B, the performance of Hermes-13B is significantly lower, even lower than Orca-7B.Except for Hermes-13B, generally positive correlation between model size and performance is observed.We speculate such outcome may be due to the differences in the backbone model's performance (e.g.LLaMA, LLaMA 2) and the type of datasets and approaches used for fine-tuning (Freitag et al., 2022).Insignificant performance gap between Platypus-70B and Orca-13B proves that Orca-13B is as effective as Platypus-70B for the evaluation task.</p>
<p>Comparisons of each Component</p>
<p>Task Description Types We investigate the impact of varying the length of task descriptions within the HG prompt and MG prompt on performance.Additionally, we compare performance when an expert role is assigned in the task description versus when it is not.As shown in Appendix Table 6, for Orca-7B, there is no significant performance difference based on length of task descriptions.However, for Orca-13B, we observe higher performance when a longer task description is employed.Such tendency suggests that, Orca-13B benefits from longer length of task descriptions in facilitating the execution of instructions, even when the content remains the same.Furthermore, when the expert role is assigned, there is a discernible performance improvement with Orca-7B.However, for Orca-13B, the performance difference between cases with and without the expert role is not substantial, indicating that this approach can be more effective for smaller models.</p>
<p>Evaluation Criteria Variants</p>
<p>We analyze the influence of various evaluation criteria, AD, HT, and MT.As shown in Appendix Table 7, utilizing aspect definitions consistently improves performance, regardless of the prompt template or model size.Furthermore, similar results are obtained even when evaluation criteria generated by GPT-4 are used.This suggests that providing a simple definition of each aspect is an effective approach when evaluating summary quality.</p>
<p>Complexity of Evaluation Steps</p>
<p>As shown in Table 8, there is no significant trend in performance between standard and complex evaluation steps both for the HG prompt and the MG prompt.This observation implies that while the evaluation steps are effective in offering step-by-step instructions to the model, the precise description or complexity level of the evaluation steps does not exert a significant influence on the evaluation of summaries.</p>
<p>Error Analysis</p>
<p>To investigate whether the model generates wellfounded rationales for the assigned scores, we perform an error analysis on the rationales generated using the RG prompt described in section 4.3.Specifically, we conduct such comparative analysis on 36 sampled instances for two different rationale generation method: one generated with Orca-13B and RG prompt, and another with RG prompt including demonstrated examples.</p>
<p>Our analysis reveals that, in general, the model exhibits the capability to provide rationales correctly.However, we identify several types of errors: (Error type 1) provided rationale is inconsistent with the assigned evaluation scores, (Error type 2) provided rationale shows hallucination where the rationale includes information not present in the source text or summary, (Error type 3) provided rationale describes explanation about aspect different from the designated one.Detailed descriptions and examples for each error type can be found in Appendix C. Addressing and mitigating these errors through further research efforts could significantly enhance the explainability and reliability of LLM-based metrics.</p>
<p>Conclusion</p>
<p>In this work, we conduct a systematic analysis of effective prompting techniques and strategies for LLM-based metrics in NLG evaluation.Our comprehensive experiments reveal that providing clear and straightforward instructions, akin to those explained to humans, proves to be more effective.Furthermore, we examine various score aggregation methods to achieve effective score assignments and show the potential for enhancing explainability within open-source LLMs.Additionally, we explore performance change relative to model size and scrutinize the influence of various elements within the prompt template.We hope that our research findings will furnish valuable insights for 8</p>
<p>A Experimental Setup</p>
<p>Library Version guidance 0.0.64 vllm 0.1.7torch 2.0.1 For optimized inference with open-source LLMs, we employ Guidance and vLLM libraries.The libraries and their respective versions used for the experiments can be found in Table 3.</p>
<p>B Test Phase</p>
<p>We submit the final results for the test set after equally applying Filtering and Binning to the HG prompt on both Orca-13B and Platypus-70B (for the small and large track, respectively).We use HT as the evaluation criteria of the factuality, generated using GPT-4.Scores for relevance, factuality, and fluency, obtained from Direct aggregation, are averaged to compute the final score.The hyperparameters for Direct aggregation is set identical to the development set, with top_p to 0.1 and temperature to 0, respectively.The prompts used for the test set can be found in Table 22, 23, and 24.</p>
<p>Filtering is applied using the Filtering prompt on both Orca-13B and Platypus-70B models.Example of the Filtering prompt is provided in Table 18.After applying Binning, the number of unique scores has been diminished from 36 to 10 and 46 to 13 for Orca-13B and Platypus-70B, respectively.</p>
<p>C Analysis C.1 The Effect of Different Model Sizes</p>
<p>We conduct experiments to analyze the performance differences depending on model sizes using Orca-7B, Hermes-13B, Orca-13B, and Platypus-70B.The experiments for Orca-7B, Hermes-13B, and Orca-13B are conducted using vLLM, while the Platypus-70B experiments are conducted using Guidance.In Table 4, we conduct experiments comparing performance across model sizes for different prompt templates and granularity of score.In Table 5, we carry out experiments to compare performance across model sizes for different prompt templates and score aggregations.</p>
<p>Template Fine-grained Demonstration Aggregation Orca-7B Orca-13B Hermes-13B Platypus-70B</p>
<p>C.2 Comparisons of each Component</p>
<p>Task description, evaluation criteria and evaluation steps of the prompt templates are slightly modified to ensure the suitability for each experiment.Examples are provided in Appendix D.</p>
<p>C.2.1 Task Description type</p>
<p>We investigate the impact of varying the length of task descriptions within the HG prompt and MG prompt on performance.Additionally, we compare performance when an expert role is assigned in the task description versus when it is not.Various task descriptions are manually crafted for each prompt 11</p>
<p>C.2.2 Evaluation Criteria variants</p>
<p>AD-GPT, HT-GPT, and MT-GPT are generated using GPT-4, tailored respectively to the AD, HT, and MT styles.The experimental results based on the types of the evaluation criteria can be found in Table 7.  Score 1</p>
<p>Rationale</p>
<p>The summary is coherent with the source text, as it captures the main points about Esteban Cambiasso's confidence in Leicester's chances of staying in the Premier League.However, it could be improved by including more details and quotes from the source text, making it more comprehensive and engaging.</p>
<p>Summary</p>
<p>holland america line said two guests were found dead inside their stateroom on the ship .the ship left tampa , florida , on march 29 on a 14-day southern caribbean cruise .it's currently in san juan , puerto rico Score 2</p>
<p>Rationale</p>
<p>The summary is not coherent with the source text.It omits crucial information, such as the cause of death, the identities of the passengers, and the cruise ship's itinerary.The summary also does not convey the urgency and seriousness of the situation, as indicated by the FBI's involvement and the ship's immediate return to port.The coherence of the summary is poor, as it does not represent the main events and context of the source text</p>
<p>Example Source</p>
<p>First blood to United after their Under 18s saw off City 1-0 in the 'mini-derby'.Kits aside, this was probably as far removed from Sunday's big match as you could imagine.For a start, no less than 13 out of the 22 players on show at kick-off were English.Of those, 10 were Mancunian.Callum Gribbin was the matchwinner for Manchester United with a delightful free-kick Ticket prices?Entry was free and close to 1,000 gathered on the seats and terraces of Moss Lane for a match that kicked off at 3pm on Saturday with half-and-half scarves nowhere to be seen.Altrincham's compact 6,000-capacity home may not be Old Trafford, but it does have a proud history of its own.It was certainly a grander stage than a windswept Saturday morning on an outfield at United Premier League end-of-season play-off series.FA chief Greg Dyke recently bemoaned the lack of first-team opportunities for homegrown talent between the ages of 18 and 21.On the evidence of this entertaining 90 minutes high on skill and commitment, there are talents on both sides that may well deserve their chance in the On the evidence of this entertaining 90 minutes high on skill and commitment, there are talents on both sides that may well deserve their chance in the coming years.</p>
<p>Summary callum gribbin was the matchwinner for manchester united with a free-kick .paul mcguinness's side claim victory in the 'mini derby' for a match.paul mcguinness and marcus wood were in the first team .Score 3</p>
<p>Rationale</p>
<p>The summary is not entirely consistent with the source document.It does mention Callum Gribbin as the matchwinner and the victory in the 'mini derby', but it omits important details such as the final score, the formation used by Manchester United, and the performance of Marcus Wood.</p>
<p>Table 12: Example of Error Type 3. The rationale does not discuss for consistency.</p>
<p>D Example Prompts</p>
<p>Figure 2 :
2
Figure 2: (a) Left -Score Aggregation An example of how the Score Aggregation is calculated.'Direct' uses scores directly generated by the model, 'Logprob' uses a weighted summation based on generation probabilities of pre-defined scores (e.g. 1 to 5), and 'Approximation' uses an average from N sampled scores.(b) Right -Rational Generation prompt An example of Rationale Generation (RG) prompt and the corresponding outputs.Using the RG prompt as input, the model provides a score for the quality of the summary and the corresponding rationale.</p>
<p>'s Carrington complex, where Paul McGuinness's Under 18s usually ply their trade.The young Reds coach wanted to make the experience closer to what his tyros could expect should they make the step up to the seniors.And his side lined up with three at the back and supporting wingbacks in a formation seen more than once this season in the first team.In an even first-half the impressive Marcus Wood, from just down the road in Sale, came closest for City with an audacious chip.United manager Paul McGuinness saw his side claim victory in the 'mini derby' For the home side towering centre-forward Marcus Rashford, another local lad from whom big things are expected, wasted two decent opportunities when put through.Just before the hour mark England Under 17 star Callum Gribbin made and scored the first.In March, the nifty-footed attacking midfielder went viral when United tweeted a clip of him showing outrageous skills to trick four Aston Villa players.He was at it again here, showing nifty footwork on the edge of the box and drawing a foul.After dusting himself down Gribbin stepped up to curl a delightful free-kick around the wall and beyond blonde Joe Hart-a-like Charlie Albinson in the City goal.Moments later it was almost two when full-back Cameron Borthwick-Jackson's rasping 25-yard drive struck the angle of the crossbar and post.The same man looked to have gotten away with one when he appeared to clip Buckley in the box before sub Aaron Nemane hit the side-netting as the visitors looked for an equaliser.Dutch winger Javairo Dilrosun curled a free kick onto the roof of the net in stoppage time for City but that was all she wrote.The result leaves United, who have won two of this year's cross-town clashes, two points behind l eaders Middlesbrough with a game in hand.Injury-ravaged City however, have now lost both matches in the Barclays Under 18s</p>
<p>Table 1 :
1
Main result.Experimental results of combination sets for each Prompting Strategy, Score Aggregation, and Explainability.'Human' and 'Model' mean Human Guideline prompt and Model Guideline prompt respectively.Also, 'Base-worst/best' and 'Reason-worst/best' are abbreviations of two types of demonstration that are distinguished, including rationale.Best results for each set of variants are in bold.
TemplateFine-grained DemonstrationAggregationOrca-7B Orca-13BPrompting BasexxDirect0.25000.3040HumanxxDirect0.30940.4343ModelxxDirect0.26510.3583BaseoxDirect0.27460.3891Human Modelo ox xDirect Direct0.3472 0.28640.4468 0.3844Demonstration HumanoBase-worstDirect0.17580.3690Human Humano oBase-best Reason-worstDirect Direct0.2854 0.23090.4092 0.3899Human Aggregation Humano oReason-best xDirect Approximation0.2733 0.32390.4133 0.4002HumanoxLogprob0.32960.4210Human Modelo ox xDirect Approximation0.3472 0.26870.4468 0.3530Model Modelo ox xLogprob Direct0.2926 0.28640.3851 0.3844Explainability Rationale Rationaleo ox Reason-worstDirect Direct0.3506 0.29150.4220 0.3876RationaleoReason-bestDirect0.32620.4330
(Liu et al., 2023)-EVAL(Liu et al., 2023)with minimal modification.Examples of prompts are provided in Appendix D. For scoring, we averaged the scores derived from the aspects of relevance, consistency, coherence, and fluency for finegrained scoring.For the demonstration experiments, we sample examples from the train set based on human annotation scores for each aspect.Rationales for the scores in the examples are generated using GPT-4.Throughout the entire score generation process, we set top_p to 0.1.For Direct and Logprob aggregation, the temperature is set to 0. Lastly, we set the temperature to 1 and n_samples to 20, respectively, for Approximation aggregation.</p>
<p>Table 3 :
3
Version of libraries used for the experiments.</p>
<p>Table 4 :
4
Comparison of Kendall's Tau correlation across various Prompt Templates and Models.Fine-grained denotes whether the fine-grained scoring is used or not.Aggregation denotes the type of Score Aggregation method used.
BasexxDirect0.25000.30400.15540.3956HumanxxDirect0.30940.43430.20410.4260ModelxxDirect0.26510.35830.19150.4383BaseoxDirect0.27460.38910.14020.4082HumanoxDirect0.34720.44680.20630.4354ModeloxDirect0.28640.37440.21700.4039</p>
<p>Table 5 :
5
Comparison of Kendall's Tau correlation across various Score Aggregation and Models.Fine-grained denotes whether the fine-grained scoring is used or not.Aggregation denotes the type of Score Aggregation method used.template,andexamplescan be found in Appendix D. The experimental results for the task description types can be found in Table6.
Template Fine-grained DemonstrationAggregationOrca-7B Orca-13B Hermes-13B Platypus-70BHumanoxApproximation0.32390.40020.21270.4041HumanoxLogprob0.32960.42100.20600.4305HumanoxDirect0.34720.44680.20630.4354ModeloxApproximation0.26870.35300.21520.4058ModeloxLogprob0.29260.38510.22500.4316ModeloxDirect0.28640.38440.21700.4039Template Task Description Orca-7B Orca-13BBase0.34720.4468HumanExpert Short0.3544 0.33390.4383 0.4239Long0.33830.4501Base0.28640.3744ModelExpert Short0.3302 0.27210.3881 0.3508Long0.27670.3891</p>
<p>Table 6 :
6
Comparison of Kendall's Tau correlation of cases using various types of task description on development set.Direct aggregation and fine-grained scoring are used for the experiment.Any demonstration is not provided.</p>
<p>Table 7 :
7
Comparison of Kendall's Tau correlation of cases using various types of evaluation criteria on development set.AD-GPT, HT-GPT, and MT-GPT denote AD, HT, and MT generated by GPT-4.Direct aggregation and finegrained scoring are used for the experiment.Any demonstrated example is not provided.Complex evaluation steps are crafted using GPT-4 for both HG and MG prompt.Examples are provided in Appendix D. The experimental results for the evaluation steps can be found in Table8.
Template Evaluation Criteria Orca-7B Orca-13BAD0.33430.4279AD-GPT0.33450.4336HumanHT HT-GPT0.3256 0.32930.4192 0.4192MT0.33030.4314MT-GPT0.33440.4297AD0.31160.4001AD-GPT0.31150.4066ModelHT HT-GPT0.3013 0.29870.3904 0.3894MT0.31410.4102MT-GPT0.30370.3949C.2.3 Complexity of evaluation stepsC.3 Error Analysis</p>
<p>Table 8 :
8
Comparison of Kendall's Tau correlation of base and complex evaluation steps on development set.Direct aggregation and fine-grained scoring are used for the experiment.No demonstrated example is provided to either method.
Error TypeBase Reason-best0Good50%69%1Inconsistent11%17%2Hallucination36%6%3 Different Aspect 6%8%</p>
<p>Table 9 :
9
Error Occurrence Ratio when RG prompt with and without 'Reason-best' demonstration are used.In this analysis, we use Orca-13B to generate a score and rationale for each aspect.Error Type 1 means that the rationale is inconsistent with the score.Error Type 2 means that the rationale includes hallucinated information not mentioned in the source text and/or summary.Error Type 3 means that the rationale is about different aspect rather than the designated aspect.Esteban Cambiasso has won all the major European competitions a player can during his illustrious career but revealed that keeping Leicester City in the Premier League would be up there with the best.The Foxes are currently seven points adrift at the bottom of the table, with only eight games remaining, knowing that time is running out to save themselves.Cambiasso refuses to give up and admits that keeping Leicester up will feel like winning a trophy.Esteban Cambiasso says that helping keep Leicester in the Premier League will feel like winning a trophy 'For me, it's like another cup,' he told BBC East Midlands Today.'Whenyoustartanotherseason you have an objective, and this is the objective for us.'Forme,winning a cup or winning the league with another team is the same now as having the possibility to save Leicester in the Premier League.'TheArgentinianmidfielderposes with the trophy after his team won the 2010 FIFA Club World Cup Cambiasso had an illustrious career at Inter Milan, winning an impressive 15 trophies during his stint River Plate(2001)(2002)Argentine Primera Division RealMadrid (2002Madrid ( -2004) )La Liga Super Cup Supercopa de Espana Inter Milan (2004-2014) Champions League Serie A (5) Coppa Italia (4) Supercoppa (4) FIFA Club World Cup Having not won a game since January, Nigel Pearson's men face West Ham United on Saturday and Cambiasso is still convinced they can avoid the drop.'I understood when I signed for Leicester it's not an easy job to stay in the Premier League,' he said.'It's a difficult situation but I think we have our chances to win matches.There's a quarter of the Premier League left to finish.'I think some people think for Leicester all is finished.But I'm sure, because I watch my team-mates every day, we can save Leicester and stay in the Premier League.'The former Inter Milan star signed for the Foxes in the summer, leaving Italy after ten years and embarking on a new challenge in England.After agreeing to a one-year-deal, Cambiasso has quickly established himself as a key player but it remains to be seen if he'll still be in the East Midlands at the start of next season.The former Real Madrid man was also successful during his short spell in Spain for Real Madrid Cambiasso played during Real's 'Galatico' era, with Luis Figo, Zinedine Zidane, Ronaldo and David Beckham 'Leicester always wanted me,' he added.'After these nine months or eight months, I'm very happy because my family is OK, and I'm good.'I want a few more points, but all the rest is perfect.'Cambiasso is happy in the East Midlands and could stay beyond his current one-year-deal Summary Esteban Cambiasso is confident Leicester can win the Premier League and keep themselves in the top flight.
Example</p>
<p>Table 10 :
10
Example of Error Type 1.The rationale is inconsistent with the score for the coherence.Two passengers found dead on a cruise ship in Puerto Rico appear to have died in a murder-suicide, the cruise line said.Holland America Line said two guests were found dead inside their stateroom on the ms Ryndam at 11:30 a.m.Thursday." "The cabin was immediately secured, and the authorities were notified, including the FBI,"" Holland America said.""We are cooperating fully with the investigation, and the authorities will make the official determination on what occurred.""FBI spokesman Moises Quiñones said authorities were on scene investigating.The ship left Tampa, Florida, on March 29 on a 14-day Southern Caribbean cruise.It's currently in San Juan, Puerto Rico.Puerto Rico Port Authority spokesman Efraín Santiago told El Nuevo Dia newspaper that the cleaning staff on the ship had discovered the deceased passengers after knocking on the cabin's door.
13 176</p>
<p>Table 11 :
11
Example of Error Type 2. Rationale includes hallucination for coherence.</p>
<p>Code for this paper is available at https://github.com/ kjhoon7686/Prompt4LLM-Eval.
https://openai.com/research/gpt-4
https://github.com/guidance-ai/guidance
https://github.com/vllm-project/vllm 6
Task Description TemplatePromptExpert HumanYou read and summarize a lot of news articles, and you're an expert at summarizing news articles.In this task you will evaluate the quality of a summary written for a news article.To correctly solve this task, follow these steps:Expert ModelYou read and summarize a lot of news articles, and you're an expert at summarizing news articles.You will be given one summary written for a news article.Your task is to evaluate the summary based on a specific metric, rating it on a scale from 1 (worst) to 5 (best).Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.Long HumanIn this task, you will evaluate the quality of a summary written for a news article.Please take your time to carefully evaluate the provided summary, and don't hesitate to refer back to this instruction document if you need clarification or guidance at any point during your evaluation.To correctly solve this task, follow these steps:Long ModelYou will be given one summary written for a news article.Your task is to evaluate the summary based on a specific metric, rating it on a scale from 1 (worst) to 5 (best).Please make sure you read and understand these instructions carefully.HT-GPT HumanRelevance:This rating assesses the extent to which the summary highlights the central themes of the original article.Evaluate if the summary encompasses the crucial elements while omitting any non-essential details.MT-GPT ModelRelevance -gauges the summary's alignment with the article's primary ideas.Check if the summary includes essential points and omits unrelated details.It may help to list the article's main points and verify their presence in the summary.AD Human,Model Relevance -How well is the generated text relevant to its source text?AD-GPT Human,Model Relevance -To what extent does the generated summary capture and reflect the core details of its source text?HumanIn this task, your primary aim is to conduct a thorough assessment of the summary provided for a news article.To effectively accomplish this task, please adhere to the following comprehensive steps:1. Initiate the evaluation process by engaging in an in-depth examination of the news article.Your aim here is to establish a profound understanding of the article's entire spectrum of content, ensuring you grasp its core message, nuances, and key elements.2. Proceed to scrutinize the proposed summary provided alongside the article.In this phase, your task is to meticulously evaluate the summary for its aspect.3. Assign a rating to each summary based on its aspect, utilizing a scale ranging from 1 (indicating the lowest quality) to 5 (signifying the highest quality).Complex Model 1. Thoroughly examine the provided summary and the source document with meticulous attention to detail.2. Conduct a comprehensive comparative analysis, scrutinizing the summary in relation to the source document to discern and delineate the primary focal points and pivotal elements elucidated within the article.3. Engage in a judicious evaluation to gauge the summary's efficacy in addressing and encompassing the central facets of the source document, concurrently assessing the presence of any extraneous or duplicative information that might detract from its relevance.4. Utilize a relevance rating scale, ranging from 1 (indicating minimal relevance) to 5 (indicating maximal relevance), for the purpose of assigning a numerical score.This score serves as a quantitative reflection of the extent to which the summary aligns with and encapsulates the core substance of the source document.World No 1 Williams eventually prevailed 4-6 7-6 ( 3 ) 6-3 against the dogged Italian to take her career record over her to 8-0 but the American was not impressed .The US were beaten 3-2 as Williams and Alison Riske were thrashed 6-0 6-3 in the doubles rubber by Errani and Flavia Pennetta , meaning they were relegated to World Group II .American tennis star Serena Williams fought back to beat Italian Sara Errani in the Fed Cup play-off on Sunday Tough weather conditions made it difficult for both players who had to keep on re-tossing their serves Errani gave Williams a real scare but in the end the world No 1 's power proved to be too much 'Today has been a big eye opener , ' Williams said afterwards .' I 'm totally not as ready for the claycourt season as I thought I was .Now I 'm in the mindset of , " You know what , I 'm not on hard court ." I 'm playing like I 'm on hard court and I 'm not .'So I have to play and be ready to hit a thousand shots if necessary .' Williams , 33 , won her 19th singles grand slam at the Australian Open and her dominance has raised talk of her claiming all the majors this year .The French Open has been her least successful of the four though despite claiming the title in Paris in 2002 and 2013 .Her doubles defeat on Sunday blotted an otherwise flawless Fed Cup record and left the US facing a battle to get back amongst the elite nations next year .'We have to work harder , ' US captain Mary Joe Fernandez said .'We came close today and need to just keep plugging away .'The good news is that we have a lot of players in the top 100 and , hopefully , we can get two wins next year and get back into the World Group .' Williams congratulates Italy captain Corrado Barazzutti after competing in America 's doubles defeat.Summary: Serena Williams beat Sara Errani 4-6 7-6 ( 3 ) 6-3 in the Fed Cup play-off .The US were beaten 3-2 as Williams and Alison Riske were thrashed in the doubles rubber .The doubles defeat saw the US relegated to World Group II .\u2019--Example Score: 5 Explanation: The summary effectively captures the key points from the article.It mentions Serena Williams' challenging match against Sara Errani and her eventual victory.The summary also highlights the US team's overall defeat and its consequence \u2013 relegation to World Group II.These details are central to the main storyline of the source text, making the summary highly relevant.Thus, a score of 5 (best) is appropriate for the summary's relevance.Template PromptRationaleYour task is to evaluate the relevance of a provided summary based on its source document.Follow these steps:1. Read the source document 2. Review the summary 3. Analyze for relevance 4. Assign a Score: Rate the summary on a scale of 1 to 5, where:-1 means the summary is not relevant with the source.-5 means the summary is entirely relevant with the source.5. Provide a Rationale: After assigning a score, explain your reasons based on your analysis.# Definition: Relevance:The rating measures how well the summary captures the key points of the article.Consider whether all and only the important aspects are contained in the summary."--Source text: Summary:FilteringIn this task you will evaluate the quality of a summary written for a document.Provided summary may include direct or rephrased repetitions of the same word or phrase.With that in mind do the following:1. Answer whether the summary is redundant or not.-Your answer must be in "Yes" or "No" format, where "Yes" means that the summary is redundant and "No" means that the summary is not redundant.2. Please provide brief explanation for your answer.-Your explanation should only discuss the redundancy of the summary, not the quality of the summary in general.-summary:Template PromptHumanIn this task you will evaluate the quality of a summary written for a document.To correctly solve this task, follow these steps:1. Carefully read the document, be aware of the information it contains.2. Read the proposed summary.3. Rate each summary on a scale from 1 (worst) to 5 (best) by its relevance.# Definition: Relevance: The rating measures how well the summary captures the key points of the article.Consider whether all and only the important aspects are contained in the summary.Source text:Summary:Score: In this task you will evaluate the quality of a summary written for a document.To correctly solve this task, follow these steps:1. Carefully read the document, be aware of the information it contains.2. Read the proposed summary.3. Rate each summary on a scale from 0 (worst) to 100 (best) by its Relevance.# Definition: Relevance: The rating measures how well the summary captures the key points of the article.Consider whether all and only the important aspects are contained in the summary.Source text:Summary:Score:Template PromptHuman Instruction:In this task you will evaluate the quality of a summary written for a document.To correctly solve this task, follow these steps:1. Carefully read the document, be aware of the information it contains.2. Read the proposed summary.3. Rate each summary on a scale from 0 (worst) to 100 (best) by its Factuality.# Definition: Factuality: This rating gauges the accuracy and truthfulness of the information presented in the summary compared to the original article.Scrutinize the summary to ensure it presents facts without distortion or misrepresentation, staying true to the source content's details and intent.Source text:Summary:Score: In this task you will evaluate the quality of a summary written for a document.To correctly solve this task, follow these steps:1. Carefully read the document, be aware of the information it contains.2. Read the proposed summary.3. Rate each summary on a scale from 0 (worst) to 100 (best) by its Fluency.# Definition: Fluency: This rating evaluates the clarity and grammatical integrity of each sentence in the summary.Examine each sentence for its structural soundness and linguistic clarity.Source text:Summary: Score:
COMET: commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, CoRR, abs/1906.053172019</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung Yi, Lee , 2023</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>Summeval: Re-evaluating summarization evaluation. Alexander R Fabbri, Wojciech Kryscinski, Bryan Mccann, Caiming Xiong, Richard Socher, Dragomir R Radev, 2020. 2007.12626</p>
<p>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, F T André, Graham Martins, Ankush Neubig, Jonathan H Garg, Markus Clark, Orhan Freitag, Firat, 2023</p>
<p>Results of WMT22 metrics shared task: Stop using BLEU -neural metrics are better and more robust. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-Kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, F T André, Martins, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, 2023</p>
<p>Perception score: A learned metric for open-ended text generation evaluation. Jing Gu, Qingyang Wu, Zhou Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, Minlie Huang, arXiv:2105.08920Openmeva: A benchmark for evaluating open-ended story generation metrics. 2021arXiv preprint</p>
<p>Chi Han, Ziqi Wang, Han Zhao, Heng Ji, arXiv:2305.12766-context learning of large language models explained as kernel regression. 2023arXiv preprint</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, 10.48550/ARXIV.2302.145202023</p>
<p>Platypus: Quick, cheap, and powerful refinement of llms. Ariel N Lee, Cole J Hunter, Nataniel Ruiz, 2023</p>
<p>The eval4nlp 2023 shared task on prompting large language models as explainable metrics. Christoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao, Rotem Dror, Steffen Eger, Proceedings of the 4th Workshop on Evaluation and Comparison for NLP systems. the 4th Workshop on Evaluation and Comparison for NLP systems2023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, G-eval: Nlg evaluation using gpt-4 with better human alignment. 2023</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, Orca: Progressive learning from complex explanation traces of gpt-4. 2023</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Effects of semantic plausibility, syntactic complexity and n-gram frequency on children's sentence repetition. Kamila Polišenská, Shula Chiat, Jakub Szewczyk, Katherine E Twomey, Journal of Child Language. 4822021</p>
<p>Approximating human evaluation of social chatbots with prompting. Ekaterina Svikhnushina, Pearl Pu, arXiv:2304.052532023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>Larger language models do in-context learning differently. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, arXiv:2303.038462023arXiv preprint</p>
<p>Exploring syntactic and semantic features for authorship attribution. Haiyan Wu, Zhiqiang Zhang, Qingfeng Wu, Applied Soft Computing. 1111078152021</p>
<p>Instructscore: Towards explainable text generation evaluation with automatic feedback. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang, Wang , Lei Li, 2023</p>
<p>Bertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, CoRR, abs/1904.096752019</p>
<p>Multimodal chain-of-thought reasoning in language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola, 2023</p>
<p>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, CoRR, abs/1909.026222019</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. </p>            </div>
        </div>

    </div>
</body>
</html>
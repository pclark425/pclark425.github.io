<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7007 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7007</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7007</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-256827163</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2302.05900v1.pdf" target="_blank">Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters</a></p>
                <p><strong>Paper Abstract:</strong> Text generation from Abstract Meaning Representation (AMR) has substantially benefited from the popularized Pretrained Language Models (PLMs). Myriad approaches have linearized the input graph as a sequence of tokens to fit the PLM tokenization requirements. Nevertheless, this transformation jeopardizes the structural integrity of the graph and is therefore detrimental to its resulting representation. To overcome this issue, Ribeiro et al. have recently proposed StructAdapt, a structure-aware adapter which injects the input graph connectivity within PLMs using Graph Neural Networks (GNNs). In this paper, we investigate the influence of Relative Position Embeddings (RPE) on AMR-to-Text, and, in parallel, we examine the robustness of StructAdapt. Through ablation studies, graph attack and link prediction, we reveal that RPE might be partially encoding input graphs. We suggest further research regarding the role of RPE will provide valuable insights for Graph-to-Text generation.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7007.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7007.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR linearization (sequence of node and edge labels)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A traversal-based serialization that converts an AMR graph into a flat sequence of node and edge labels (tokens) so it can be consumed by pretrained sequence models; this is the standard preprocessing step used to feed PLMs for AMR-to-text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Traversal-based AMR linearization (node/edge label sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The graph is turned into a token sequence by traversing the AMR and emitting node labels and relation labels in traversal order (a sequence of node and edge labels). The serialized sequence is then tokenized (e.g., into subword tokens) for input to a PLM. No special structural markup beyond the linearized ordering is required.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>lossy, sequential, token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Traversal-based serialization (graph traversal to produce ordered sequence of node and edge labels); exact traversal order not specified in this paper (standard AMR linearization used)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2020T02 (AMR 3.0)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-base (standard linearized input)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-base encoder-decoder pretrained model (uses relative position embeddings in its self-attention), fine-tuned for AMR-to-text with adapters; baseline uses a vanilla adapter (MLP) on top of token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SacreBLEU, METEOR, chrF++, TER, BERTScore, MF (M/F), human evaluation (meaning preservation, grammatical correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported comparisons in the paper: removing RPE from the encoder on the vanilla (MLP) adapter baseline caused a 25.3% absolute drop in BLEU (exact BLEU numbers by configuration are in the paper tables but not reproduced verbatim in the text).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Necessary baseline format to use PLMs; heavily reliant on positional signals (RPE) or additional structure-aware components to recover graph relationships because linearization loses adjacency information; without RPE, generation quality degrades substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Loss of structural integrity: adjacent graph nodes may be far apart in the serialized sequence, causing information loss; tokenization may split node labels leading to segmented nodes; plain linearized sequences require additional mechanisms (RPE, GNNs) to restore structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared unfavorably to StructAdapt (token-level graph reconstructed) when structural signals are available; however, when RPE are present the linearized input with a vanilla adapter can perform reasonably, showing that positional encodings can partly compensate for lost graph structure.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7007.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7007.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StructAdapt token-graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StructAdapt reconstructed token-level graph with reified relations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structure-aware adapter representation that reconstructs a graph after tokenization: relation labels are reified as new (non-chunked) nodes and subword tokens from node labels become nodes connected to the corresponding reified relation node, enabling GNNs to operate on token nodes within a PLM encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structural adapters in pretrained language models for AMR-to-Text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Token-level reconstructed AMR graph with reified relations (StructAdapt)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>After linearization and subword tokenization, StructAdapt builds a new graph where (1) relation labels are reified as distinct nodes and added to the vocabulary so they are not split into subwords, and (2) node labels that were split into subwords become individual token-nodes that are connected to the reified relation node representing their original (non-chunked) graph node. This token-level graph is input to a GNN (GCN/GAT/RGCN) inside an adapter module.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based, graph-structured (aims to preserve structure; partially lossy due to tokenization)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Linearize AMR → apply subword tokenization → reify relations as new nodes (relation labels kept as atomic tokens) → connect each subword token-node to the corresponding reified relation node → run a GNN adapter over this reconstructed token graph</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2020T02 (AMR 3.0)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-base with StructAdapt adapters (GCN, GAT, RGCN variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-base encoder-decoder pretrained model; encoder augmented with structural adapter that replaces the adapter's first MLP with a single-layer GNN (GCN, GAT, or RGCN). Bottleneck adapter dimension = 256 (≈4% of T5-base parameters). GAT used a single attention head; single GNN layer was used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SacreBLEU, METEOR, chrF++, TER, BERTScore, MF (M/F), link-prediction accuracy (probe), human evaluation (meaning preservation & linguistic correctness), attention–adjacency similarity (1-Wasserstein distance)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Key reported findings: (a) With RPE present, GNN-based StructAdapt improves generation vs. naive MLP-without-structure; (b) Removing encoder RPE causes GNN-based adapters to underperform a vanilla MLP adapter with RPE — a reported relative drop of ~12.5 BLEU points compared to the baseline; (c) In link-prediction probing, RGCN achieved up to ~76% accuracy (best), while other adapters remained under ~73%; (d) Adding RPE increases link-prediction performance by ~3.5% on average for MLP, GCN and GAT adapters.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables explicit propagation of adjacency/connectivity information to the PLM encoder via lightweight adapters and GNN layers, improving structure-aware encoding; however, the benefit depends strongly on positional signals (RPE) and correct adjacency input; training uses small adapter footprint and converges with modest compute (single-layer GNN).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires manual code modifications to insert adapters into pretrained models (not trivially scalable); still affected by tokenization-induced fragmentation of node labels; when encoder RPE are removed or adjacency is corrupted the method's advantage diminishes (GNNs alone did not fully compensate); experiments used single-layer GNN only.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to plain linearization + vanilla adapter: StructAdapt aims to preserve and leverage graph structure via GNNs; however, when encoder RPE are removed StructAdapt performs worse than a vanilla MLP adapter with RPE, indicating RPE can partly substitute for explicit GNN propagation. RGCN was stronger than GCN/GAT at reconstructing edges (link-prediction) likely due to relation-type modeling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7007.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7007.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RPE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative Position Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Relative position embeddings (RPE) encode pairwise positional relationships between tokens by adding learned relative-position biases to self-attention logits (per-head), and in this work are shown to partially capture graph adjacency information in linearized AMRs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-attention with relative position representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Relative position embeddings used as pairwise position signal</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>RPE provide per-token-pair positional information (relative offsets) that are added to self-attention logits (Shaw et al. style). In T5, RPE are computed per head and forwarded to the adapter; the RPE signal acts as an auxiliary token-pair encoding that can reflect ordering and—empirically—some graph adjacency patterns of the linearized input.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>positional encoding (auxiliary token-pair signals), sequence-level</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Compute relative distance buckets or relative offsets between token positions and use learned embeddings added to attention logits for each token pair (per-head RPE as in Shaw et al./T5 implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2020T02 (AMR 3.0)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (and probing for graph reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-base (uses RPE natively in self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-base encoder-decoder pretrained model that uses relative position embeddings in its self-attention layers; in experiments, encoder RPE were ablated in some settings while decoder RPE were kept.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SacreBLEU, METEOR, chrF++, TER, BERTScore, MF (M/F), link-prediction probe accuracy, attention–adjacency similarity (1-Wasserstein distance), human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported empirical effects: (a) Removing encoder RPE caused a 25.3% absolute drop in BLEU for the vanilla adapter baseline; (b) Adding RPE increased link-prediction probe accuracy by ~3.5% on average for MLP, GCN, and GAT adapters; (c) Using RPE with a corrupted adjacency matrix produced generation performance similar to using correct graph adjacency without RPE (exact metric values reported in paper tables). Attention matrices with RPE are closer to normalized adjacency matrices (lower 1-Wasserstein distance) than when RPE are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>RPE substantially improve generation from linearized graphs by providing token-pair relative signals; they can partially compensate for missing or corrupted graph adjacency and enable generation when structural information is erroneous or absent. They also affect the learned attention patterns to align with graph adjacency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>RPE can encode graph-like information implicitly, which may mask or confound explicit structure-aware components; they are shared across layers and the role may be entangled with learned traversal order, making causal attribution difficult. Position-shuffling (position attack) degrades performance, showing reliance on ordered relative positions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to absolute position embeddings (APE), RPE avoid fixed-position limits and the additive mixing of semantic and absolute position vectors; empirical evidence shows RPE can rival or substitute for explicit GNN-based adjacency encoding in some settings (i.e., linearized input with RPE can outperform StructAdapt without RPE).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structural adapters in pretrained language models for AMR-to-Text generation <em>(Rating: 2)</em></li>
                <li>Self-attention with relative position representations <em>(Rating: 2)</em></li>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Modeling graph structure via relative position for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>Investigating pretrained language models for graph-to-text generation <em>(Rating: 1)</em></li>
                <li>Densely connected graph convolutional networks for graph-to-sequence learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7007",
    "paper_id": "paper-256827163",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "AMR linearization",
            "name_full": "AMR linearization (sequence of node and edge labels)",
            "brief_description": "A traversal-based serialization that converts an AMR graph into a flat sequence of node and edge labels (tokens) so it can be consumed by pretrained sequence models; this is the standard preprocessing step used to feed PLMs for AMR-to-text.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Traversal-based AMR linearization (node/edge label sequence)",
            "representation_description": "The graph is turned into a token sequence by traversing the AMR and emitting node labels and relation labels in traversal order (a sequence of node and edge labels). The serialized sequence is then tokenized (e.g., into subword tokens) for input to a PLM. No special structural markup beyond the linearized ordering is required.",
            "representation_type": "lossy, sequential, token-based",
            "encoding_method": "Traversal-based serialization (graph traversal to produce ordered sequence of node and edge labels); exact traversal order not specified in this paper (standard AMR linearization used)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2020T02 (AMR 3.0)",
            "task_name": "AMR-to-text generation",
            "model_name": "T5-base (standard linearized input)",
            "model_description": "T5-base encoder-decoder pretrained model (uses relative position embeddings in its self-attention), fine-tuned for AMR-to-text with adapters; baseline uses a vanilla adapter (MLP) on top of token embeddings.",
            "performance_metric": "SacreBLEU, METEOR, chrF++, TER, BERTScore, MF (M/F), human evaluation (meaning preservation, grammatical correctness)",
            "performance_value": "Reported comparisons in the paper: removing RPE from the encoder on the vanilla (MLP) adapter baseline caused a 25.3% absolute drop in BLEU (exact BLEU numbers by configuration are in the paper tables but not reproduced verbatim in the text).",
            "impact_on_training": "Necessary baseline format to use PLMs; heavily reliant on positional signals (RPE) or additional structure-aware components to recover graph relationships because linearization loses adjacency information; without RPE, generation quality degrades substantially.",
            "limitations": "Loss of structural integrity: adjacent graph nodes may be far apart in the serialized sequence, causing information loss; tokenization may split node labels leading to segmented nodes; plain linearized sequences require additional mechanisms (RPE, GNNs) to restore structure.",
            "comparison_with_other": "Compared unfavorably to StructAdapt (token-level graph reconstructed) when structural signals are available; however, when RPE are present the linearized input with a vanilla adapter can perform reasonably, showing that positional encodings can partly compensate for lost graph structure.",
            "uuid": "e7007.0"
        },
        {
            "name_short": "StructAdapt token-graph",
            "name_full": "StructAdapt reconstructed token-level graph with reified relations",
            "brief_description": "A structure-aware adapter representation that reconstructs a graph after tokenization: relation labels are reified as new (non-chunked) nodes and subword tokens from node labels become nodes connected to the corresponding reified relation node, enabling GNNs to operate on token nodes within a PLM encoder.",
            "citation_title": "Structural adapters in pretrained language models for AMR-to-Text generation",
            "mention_or_use": "use",
            "representation_name": "Token-level reconstructed AMR graph with reified relations (StructAdapt)",
            "representation_description": "After linearization and subword tokenization, StructAdapt builds a new graph where (1) relation labels are reified as distinct nodes and added to the vocabulary so they are not split into subwords, and (2) node labels that were split into subwords become individual token-nodes that are connected to the reified relation node representing their original (non-chunked) graph node. This token-level graph is input to a GNN (GCN/GAT/RGCN) inside an adapter module.",
            "representation_type": "token-based, graph-structured (aims to preserve structure; partially lossy due to tokenization)",
            "encoding_method": "Linearize AMR → apply subword tokenization → reify relations as new nodes (relation labels kept as atomic tokens) → connect each subword token-node to the corresponding reified relation node → run a GNN adapter over this reconstructed token graph",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2020T02 (AMR 3.0)",
            "task_name": "AMR-to-text generation",
            "model_name": "T5-base with StructAdapt adapters (GCN, GAT, RGCN variants)",
            "model_description": "T5-base encoder-decoder pretrained model; encoder augmented with structural adapter that replaces the adapter's first MLP with a single-layer GNN (GCN, GAT, or RGCN). Bottleneck adapter dimension = 256 (≈4% of T5-base parameters). GAT used a single attention head; single GNN layer was used.",
            "performance_metric": "SacreBLEU, METEOR, chrF++, TER, BERTScore, MF (M/F), link-prediction accuracy (probe), human evaluation (meaning preservation & linguistic correctness), attention–adjacency similarity (1-Wasserstein distance)",
            "performance_value": "Key reported findings: (a) With RPE present, GNN-based StructAdapt improves generation vs. naive MLP-without-structure; (b) Removing encoder RPE causes GNN-based adapters to underperform a vanilla MLP adapter with RPE — a reported relative drop of ~12.5 BLEU points compared to the baseline; (c) In link-prediction probing, RGCN achieved up to ~76% accuracy (best), while other adapters remained under ~73%; (d) Adding RPE increases link-prediction performance by ~3.5% on average for MLP, GCN and GAT adapters.",
            "impact_on_training": "Enables explicit propagation of adjacency/connectivity information to the PLM encoder via lightweight adapters and GNN layers, improving structure-aware encoding; however, the benefit depends strongly on positional signals (RPE) and correct adjacency input; training uses small adapter footprint and converges with modest compute (single-layer GNN).",
            "limitations": "Requires manual code modifications to insert adapters into pretrained models (not trivially scalable); still affected by tokenization-induced fragmentation of node labels; when encoder RPE are removed or adjacency is corrupted the method's advantage diminishes (GNNs alone did not fully compensate); experiments used single-layer GNN only.",
            "comparison_with_other": "Compared to plain linearization + vanilla adapter: StructAdapt aims to preserve and leverage graph structure via GNNs; however, when encoder RPE are removed StructAdapt performs worse than a vanilla MLP adapter with RPE, indicating RPE can partly substitute for explicit GNN propagation. RGCN was stronger than GCN/GAT at reconstructing edges (link-prediction) likely due to relation-type modeling.",
            "uuid": "e7007.1"
        },
        {
            "name_short": "RPE",
            "name_full": "Relative Position Embeddings",
            "brief_description": "Relative position embeddings (RPE) encode pairwise positional relationships between tokens by adding learned relative-position biases to self-attention logits (per-head), and in this work are shown to partially capture graph adjacency information in linearized AMRs.",
            "citation_title": "Self-attention with relative position representations",
            "mention_or_use": "use",
            "representation_name": "Relative position embeddings used as pairwise position signal",
            "representation_description": "RPE provide per-token-pair positional information (relative offsets) that are added to self-attention logits (Shaw et al. style). In T5, RPE are computed per head and forwarded to the adapter; the RPE signal acts as an auxiliary token-pair encoding that can reflect ordering and—empirically—some graph adjacency patterns of the linearized input.",
            "representation_type": "positional encoding (auxiliary token-pair signals), sequence-level",
            "encoding_method": "Compute relative distance buckets or relative offsets between token positions and use learned embeddings added to attention logits for each token pair (per-head RPE as in Shaw et al./T5 implementation)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2020T02 (AMR 3.0)",
            "task_name": "AMR-to-text generation (and probing for graph reconstruction)",
            "model_name": "T5-base (uses RPE natively in self-attention)",
            "model_description": "T5-base encoder-decoder pretrained model that uses relative position embeddings in its self-attention layers; in experiments, encoder RPE were ablated in some settings while decoder RPE were kept.",
            "performance_metric": "SacreBLEU, METEOR, chrF++, TER, BERTScore, MF (M/F), link-prediction probe accuracy, attention–adjacency similarity (1-Wasserstein distance), human evaluation",
            "performance_value": "Reported empirical effects: (a) Removing encoder RPE caused a 25.3% absolute drop in BLEU for the vanilla adapter baseline; (b) Adding RPE increased link-prediction probe accuracy by ~3.5% on average for MLP, GCN, and GAT adapters; (c) Using RPE with a corrupted adjacency matrix produced generation performance similar to using correct graph adjacency without RPE (exact metric values reported in paper tables). Attention matrices with RPE are closer to normalized adjacency matrices (lower 1-Wasserstein distance) than when RPE are removed.",
            "impact_on_training": "RPE substantially improve generation from linearized graphs by providing token-pair relative signals; they can partially compensate for missing or corrupted graph adjacency and enable generation when structural information is erroneous or absent. They also affect the learned attention patterns to align with graph adjacency.",
            "limitations": "RPE can encode graph-like information implicitly, which may mask or confound explicit structure-aware components; they are shared across layers and the role may be entangled with learned traversal order, making causal attribution difficult. Position-shuffling (position attack) degrades performance, showing reliance on ordered relative positions.",
            "comparison_with_other": "Compared to absolute position embeddings (APE), RPE avoid fixed-position limits and the additive mixing of semantic and absolute position vectors; empirical evidence shows RPE can rival or substitute for explicit GNN-based adjacency encoding in some settings (i.e., linearized input with RPE can outperform StructAdapt without RPE).",
            "uuid": "e7007.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structural adapters in pretrained language models for AMR-to-Text generation",
            "rating": 2,
            "sanitized_title": "structural_adapters_in_pretrained_language_models_for_amrtotext_generation"
        },
        {
            "paper_title": "Self-attention with relative position representations",
            "rating": 2,
            "sanitized_title": "selfattention_with_relative_position_representations"
        },
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Modeling graph structure via relative position for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "modeling_graph_structure_via_relative_position_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation",
            "rating": 1,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        },
        {
            "paper_title": "Densely connected graph convolutional networks for graph-to-sequence learning",
            "rating": 1,
            "sanitized_title": "densely_connected_graph_convolutional_networks_for_graphtosequence_learning"
        }
    ],
    "cost": 0.013399499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters</p>
<p>Sebastien Montella sebastien.montella@huawei.com 
Alexis Nasr alexis.nasr@lis-lab.fr 
Johannes Heinecke johannes.heinecke@orange.com 
Frederic Bechet frederic.bechet@lis-lab.fr 
Lina M Rojas-Barahona linamaria.rojasbarahona@orange.com </p>
<p>Orange Innovation / Lannion
Aix-Marseille Univ. CNRS
LIS / MarseilleFrance, France</p>
<p>Orange Innovation / Lannion
AMU/CNRS/LIS
MarseilleFrance, France</p>
<p>Orange Innovation / Lannion
AMU/CNRS/LIS
MarseilleFrance, France</p>
<p>Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters</p>
<p>Text generation from Abstract Meaning Representation (AMR) has substantially benefited from the popularized Pretrained Language Models (PLMs). Myriad approaches have linearized the input graph as a sequence of tokens to fit the PLM tokenization requirements. Nevertheless, this transformation jeopardizes the structural integrity of the graph and is therefore detrimental to its resulting representation. To overcome this issue, Ribeiro et al.  (2021b)  have recently proposed StructAdapt, a structure-aware adapter which injects the input graph connectivity within PLMs using Graph Neural Networks (GNNs). In this paper, we investigate the influence of Relative Position Embeddings (RPE) on AMR-to-Text, and, in parallel, we examine the robustness of Struc-tAdapt. Through ablation studies, graph attack and link prediction, we reveal that RPE might be partially encoding input graphs. We suggest further research regarding the role of RPE will provide valuable insights for Graphto-Text generation.</p>
<p>Introduction</p>
<p>Earliest works on AMR-to-Text generation were mostly based on statistical methods. A common practice was to convert AMR-to-Text task into an already studied problems such as Tree-to-Text * Work done while at Orange Innovation (Lannion, France). Now at Huawei Edinburgh Research Centre (United Kingdom). (Flanigan et al., 2016;Lampouras and Vlachos, 2017), aligned text-to-text (Pourdamghani et al., 2016), Travel Sales Problems (Song et al., 2016) or Grammatical Framework (Ranta, 2011). Recently, most methods are neural-centered with an encoderdecoder architecture (Sutskever et al., 2014) as a backbone (Konstas et al., 2017;Takase et al., 2016;Cao and Clark, 2019). Unfortunately, this architecture coerces the AMR to be linearized as a sequence of tokens. This ends up in structural information loss. To tackle this issue, several strategies have attempted to integrate structure using message propagation (Song et al., 2018;Guo et al., 2019;Damonte and Cohen, 2019;Ribeiro et al., 2019;Zhang et al., 2020b;Zhao et al., 2020). A limitation of those is the absence of pretraining, as demonstrated by Ribeiro et al. (2021a). To this end, Ribeiro et al. (2021b) introduced StructAdapt for lightweight AMR-to-Text with structural adapters. As linearization and tokenization of the input graph are mandatory steps for PLMs, StructAdapt first defines a new graph where nodes are the resulting subwords from the tokenization. As a result, adapter can henceforth include GNN layers operating on the subsequent graph while leveraging pretrained representations.</p>
<p>However, although studies have been made to probe position embeddings (Wang and Chen, 2020;Wang et al., 2021;Dufter et al., 2022), their role on graph encoding has remained unanswered. In this paper, we are particularly interested in measuring the saliency of RPE with StructAdapt for AMR-to-Text generation. Our novelty is not in proposing a new method to encode graphs such as (Schmitt et al., 2021) but rather in revealing the interesting behaviours of RPE along with StructAdapt.</p>
<p>STRUCTADAPT: A Structural Adapter</p>
<p>A major issue in AMR-to-Text, and more generally Graph-to-Text with Transformers (Vaswani et al., 2017), is the linearization of the input structure. The linearization of the graph returns a sequence of node and edge labels according to a certain traversal of the graph. Nonetheless, adjacent nodes in the graph may be at multiple positions away from one another in the final serialization. To counteract this, Ribeiro et al. (2021b) introduced StructAdapt, a structure-aware (encoder) adapter. It solves the problem of segmented nodes labels by reconstructing a new graph from the resulting subwords. More specifically, the relations are primary reified as new nodes in the AMR graph. Furthermore, labels of those reified relations will be added in the vocabulary as new tokens and therefore will not be decomposed into subwords. However, the labels of the original nodes can still be chunked. To deal with this, each subword node will be connected independently to the reified relation of the initial (non-chunked) node. An example is outlined in Figure 1. As a consequence, the vanilla Adapter can now integrate any GNN-based neural network which operates on the new constructed graph (Figure 1), where nodes are the input tokens. Concretely, StructAdapt replaces the first stacked MLP of vanilla adapter with a GNN-based model as shown in Figure 2.</p>
<p>For AMR-to-Text, only the encoder is equipped with StructAdapt in order to encode AMR structure. The decoder layers adopt vanilla adapters. In our study, we consider three different GNN-based models which are Graph Convolutional Network (GCN) (Kipf and Welling, 2017), Graph Attention network (GAT) (Veličković et al., 2017) and Relational Graph Convolutional Network (RGCN) (Schlichtkrull et al., 2018). GCN computes a representation for each node a which is a (normalized) aggregation function of representation of its neighbor nodes noted N (a). GAT is akin to GCN but differs in that aggregation of neighbors embeddings are weighted using an attention mechanism. Unlike GAT and GCN, RGCN further captures the type of the relation between  (Ribeiro et al., 2021b). The details of the representations computation for each model can be found in Appendix A. The returned nodes embeddings will then be given as input features to the following MLP.</p>
<p>Relative Position Embeddings</p>
<p>Instead of adding Absolute Position Embeddings (APE) directly to the token embedding as in standard Transformer model, some models such as T5 make use of relative position embeddings inspired from Shaw et al. (2018). As an alternative to APE, RPE offer interesting features. A noteworthy limitation of APE is the need to set a limit of available positions. Therefore, long sequences may have  to be segmented. Furthermore, APE are directly added to the token representation leading to information inconsistency, i.e. position versus semantic information. To this extent, Shaw et al. (2018) came up with relative position encodings which are supplied to the self-attention mechanism by simply adding a scalar to the logits encoding the supposed relation between a current token i and a token j.</p>
<p>Experiments</p>
<p>Throughout our experiments, we make use of the LDC2020T02 dataset (AMR 3.0 release) 1 and use the T5 base model which employs RPE. The training and evaluation details can be found in Appendix B and C, respectively.</p>
<p>Exploring the Salience of RPE</p>
<p>In this section, we investigate the influence of RPE on the generation quality using structural adapter. RPE are computed in each Transformer head. Position information is then forwarded to the adapter module on top ( Figure 2). However, since connections between input nodes (i.e. tokens) are already given to structural adapters in encoder, it is legitimate to question the necessity for RPE on the encoder part but also how would the generation quality vary without such information. Hence, we propose to remove the RPE from the encoder heads to gauge their salience to structure encoding for downstream language generation. Since decoder is not encoding any graph structure, we leave 1 https://catalog.ldc.upenn.edu/LDC2020T02 RPE in decoder untouched. For better readability, MLP, GCN, GAT, and RGCN respectively denote: the vanilla adapter, StructAdapt with a GCN layer, StructAdapt with a GAT layer and Struc-tAdapt with a RGCN layer. MLP-based adapter with RPE is our baseline. Results are given in Table 1. A human evaluation is also provided for some encoder adapters in Table 2. First, it is apparent that using RPE systematically yields better generation performances. For the vanilla adapter (i.e. our baseline), we note a 25.3% absolute drop in BLEU when removing RPE. This can also be seen on human evaluation. More than one point is lost toward meaning preservation. The downturn for linguistic correctness is less important since T5 is pretrained and thus rarely prone to syntax errors. Such a result is not surprising for MLP-based adapter since it solely relies on RPE to differentiate tokens at different positions in the linearized AMR. However, a striking observation is that getting rid of RPE for GNN-based adapters leads to lower performances than our baseline. Indeed, when removing RPE when using structural adapter, we would have expected GNN-based approaches to be as competitive as a MLP-based adapter with RPE. We report a relative drop of 12.5 points in BLEU from the baseline. The same conclusion can be drawn from Table 2. This indicates that RPE are capturing relevant information for final generation. To further assess the impact and the role of RPE, we conduct a graph attack experiment. Instead of conveying the correct adjacency matrix, we propose to corrupt connectivity information. We randomly generate an adjacency matrix such that generated matrix does not contain any actual connection. We suppose that without RPE, structure-aware adapter will lead to a significant decrease in generated text due to the absence of information about the graph nor the position of nodes in the input sequence. We are especially interested to measure to which extent RPE might be able to take over the encoding of the graph for generation.   Results are shown in Table 3. Human evaluation for attacked GCN and RGCN adapters is given in Table 4. As hypothesized, providing erroneous connectivity without any position embeddings makes structural adapters no longer compelling. We observe that StructAdapt with RGCN is significantly more affected compared to GAT and GCN based adapters. Since RGCN adds direction information for each edge (direct and reverse), we conjecture that RGCN is much more bewildered. Interestingly, using RPE with corrupted graph (Table 3) leads to similar performance than using graph information without RPE (Table 1). This strongly demonstrates the usefulness of RPE to carry out the generation. We additionally provide a position attack experiment in Appendix E where RPE are shuffled randomly. Accordingly, we can further identify the saliency of RPE despite the available GNN. This raises the question of RPE encoding the input graph.
Adapter BLEU ↑ METEOR ↑ Chrf++ ↑ TER ↓ BERTScore ↑ M ↑ F ↑ GCN w/</p>
<p>Can the Graphs Be Reconstructed ?</p>
<p>As shown in Section 4.1, RPE seem to be as competitive as applying GNNs alone. If claiming that RPE also encode graphs is tempting, no strong evidence has been revealed. Indeed, better generation quality is not necessarily a consequence of better graph encoding. Therefore, we probe whether graphs can indeed be reconstructed from the learned hidden representations. To do so, we train a logistic regression, i.e. our probe, to perform link prediction as a binary classification. More specifically, given two nodes representations at a given layer l, our probe returns the probability that nodes are connected. To train our logistic regression model, we sample k positive connec- tions, i.e. two connected nodes, and k negative connections, i.e. two non-connected nodes, for each sample of training and test sets. 2 For our experiment, we choose k = 2 which leads to 109,490 and 3,770 samples for each class for training and testing respectively. We plot results in Figure 3. Firstly, we observe very high accuracy for the vanilla adapter without RPE. Accuracies over 60% are easily reached while no structure encoding nor positions information are supplied. This might be a side effect of our probe training. Nevertheless, this gives a lower bound for our experiment. We can see that adding RPE increases the link prediction performance for MLP, GCN and GAT-based adapters. We observe a constant gap of about 3.5% on average. However, we remark that RGCN is able to reconstruct edges on its own with best accuracy. We report a maximum accuracy about 76% while other models are not reaching 73%. This strengthens the idea that giving information of reverse connection may add robustness to graph encoding as shown in (Beck et al., 2018). Generally, we observe that the deeper the representation, the better the link prediction. We notice however that after the 10 th layer, significant drops in link prediction arise regardless of adapter type. We assume representations should lose some information about the structure to perform language generation. This may indicate that encoded representation for Graph-to-Text is not just graph-centered. Although counter-intuitive, encoder representations given to the decoder part may not have to encode input graph efficiently in order to verbalize it. We leave this research question for future work. We further provide an analysis on self-attention matrices in Appendix F.</p>
<p>Conclusion</p>
<p>In this paper, we have explored the effect of relative position embeddings on AMR-to-Text generation using structural adapters. We have shown that the generation process could be enabled by relative position embeddings when structure is erroneous or missing. In addition, we have demonstrated the capacity of those representations to encode the input graph to some extent. We have further revealed interesting robustness of RGCN model in graph reconstruction ability. For future work, we believe further experiments on other pretrained models and Graph-to-Text tasks may shed more light on the role of position embeddings.</p>
<p>Limitations</p>
<p>A limitation of our study is that we focus on the T5 model only. Since adapters are additional modules to add, it is required to manually implement and directly modify the original code of the pretrained model which is not easily scalable. In addition, we only evaluate on the LDC2020T02 dataset which is the cleanest AMR dataset available. </p>
<p>References</p>
<p>A Graph Neural Networks</p>
<p>A.1 Graph Convolutional Network (GCN)</p>
<p>To compute the node representation g l u ∈ R dg of the node u at layer l, GCN computes an aggregation of neighbors nodes embeddings as:
g l u = σ   v∈N (u) 1 deg(u) × deg(v) W l g l v   (1)
with σ an activation function and deg(x) the degree of the node x.</p>
<p>A.2 Graph Attention Network (GAT)</p>
<p>GAT applies an attention mechanism to determine importance of neighboors nodes regarding current node u. We have:
g l u = σ   v∈N ( u) softmax(e u,v )W l g l v   e u,v = LeakyReLu   a u,v v g l i i=u  (2)
with both σ and LeakyReLu non-linear activation functions and a u,v ∈ R 2×dg a learnable vector.  </p>
<p>A.3 Relational Graph Convolutional Network (RGCN)</p>
<p>RGCN takes into consideration the nature of the relation r ∈ R between nodes u and v. It performs convolution as the following:
g l u = σ   r∈R v∈Nr(u) c u,r W l r g l v  (3)
with N r (u) the direct neighbors of node u under the relation r, c u,r a normalization term and W l r ∈ R dg×dg a learnable relation-dependent parameter.</p>
<p>B Training Setting</p>
<p>We used the version 3.3.1 of the HuggingFace's Transformers library (Wolf et al., 2020). We use Adam optimizer with a linearly decreasing learning rate, without warm-up. The initial learning rate is set to 1 × 10 −4 . Batch size is set to 8. For decoding, a beam search of 5 is selected. A maximum length of 384 is used in case the end-of-sentence token is not encountered. We didn't use gradient clipping nor label smoothing. For GNNs, we make use of the version 1.7.2 of the PyTorch Geometric library (Fey and Lenssen, 2019). We only use a single layer for our GNNs networks, similarly to the vanilla adatper. For GAT, a single attention head was applied. The bottleneck dimension is set to 256 for all of our adapters. This is equivalent to about 4% only of the whole T5 base model's parameters to train. The training time for MLP, GCN, GAT and RGCN adapters are given in Table 5. Note that the total runtime depends on the convergence as we are using early stopping.</p>
<p>C Automatic Evaluation</p>
<p>For evaluation, we consider multiple automated metrics. We employ popular n-gram-based metrics which are SacreBLEU (Post, 2018), METEOR (Banerjee and Lavie, 2005), chrf++ (Popović, 2017), TER (Och, 2003) and the semantic-based BertScore metric (Zhang et al., 2020a). In addition, we also report both M and F pillars of the decomposable MF score recently proposed by Opitz and Frank (2021). The meaning preservation, noted M, measures how close the AMR of the generated sentence is to the reference sentence. To do so, both the generated sentence and target sentence are parsed with a pretrained Text-to-AMR model from Cai and Lam (2020). Then, their AMRs are compared using graph-based similarity measures such as Smatch (Cai and Knight, 2013). In contrast, the grammatical form, noted F, measures the linguistic quality of the generated text using stateof-the-art language model (Radford et al., 2019). We also report human evaluation in Section 4.1 to accurately assess the quality of the generated outputs. We ask annotators to evaluate the meaning preservation and linguistic correctness of our generated outputs compared to the references. Details of human evaluation are given in Appendix D.</p>
<p>D Human Evaluation</p>
<p>Since handcrafted annotation is extremely costly, we limited the number of samples and models to assess. We randomly selected 30 test samples per model to evaluate manually. We further limited evaluation to 8 configurations of adapters. This led us with 240 samples to score by multiple annotators. We asked 22 annotators to answer to these following questions with a rate on a 1-6 Likert scale, with 6 the best score:</p>
<p>• Is the generated sentence semantically close to the reference ?</p>
<p>• Is the generated sentence grammatically correct ?</p>
<p>Each annotator was given 50 samples to annotate. We adopted FlexEval (Fayet et al., 2020) to implement a flexible evaluation environment. This allows a streaming evaluation were annotators can stop at any moment and come back to the evaluation later, at one's will. Furthermore, different samples from different models are given to annotators in an balanced manner such that each sample of each model is exposed to at least 2 annotators. Note that one annotator is given some samples from multiple configuration. This avoids evaluation bias toward a single annotator. In our case, each sample from each configuration has been annotated by 4 distinct annotators.</p>
<p>E Position Attack</p>
<p>For a given graph, we also propose to corrupt RPE with a random shuffle. Since the model may learn the generated order through training, we systematically shuffle RPE for each epoch for all graphs. Results are given in Table 6.  </p>
<p>F Analysis of Attention Matrices</p>
<p>The self-attention matrix gives information about how much weight should be assigned to other tokens (i.e. nodes). Thus, attention matrix can be seen as a pseudo-adjacency matrix. We further propose to inspect the similarity between the attention matrix of each head at each layer with the adjacency matrix of the input graph. Since the input is considered as a sequence of nodes, the shape of the attention matrix and adjacency matrix are equal. However, a limitation of the similarity comparison lies in the respective type of those mathematical objects. Attention matrices are probabilities whereas adjacency matrices are not. To deal with this issue, we convert the adjacency matrix A s of a sample s to a normalized formÃ s such that each rowÃ s n is normalized and sums up to one, as described in Eq. 4. </p>
<p>This enables the rows of both attention and adjacency matrices to be probabilities distributions over input nodes. We can therefore measure the distance between those distributions to find out whether attention matrices are somehow close to the (normalized) adjacency matrix, and thus encoding graph connections. To do so, we compute the 1-Wasserstein distance W 1 3 between the attention distribution of each token n and its corresponding normalized n th row ofÃ. We average distances over tokens, and then over samples (Eq. 5). We noteÃ, S, and N s the adjacency matrices, 3 Equivalent to earth mover's distance. the total number of AMR samples and the length of sequence s, respectively. </p>
<p>For fair comparison, we also provide the distance between attention distribution head l i and the uniform distribution U which assigns a probability of 1 N to each token for a graph with N nodes. We plot distances as heatmaps on Figure 4 where each square indicates either W 1 (head l i ,Ã) or W 1 (head l i , U ). When using RPE, we can see that, for both MLP and RGCN-based adapters, the distribution of the attention scores are close to the normalized adjacency matrices of our graphs. Meanwhile, when removing them, we can witness that attention scores are much closer to the uniform distribution and thus smoother. This is in line with our previous results suggesting that RPE might partially encode graphs. Since RPE are shared across layers, we can easily detect similar distances across layers for same heads (visible vertical lines). We also find similar behavior we have previously noted where last layers tend to lose graph information as attention scores slightly move away from the normalized adjacency matrix distribution.</p>
<p>Figure 3 :
3Probing -Link Prediction Results using hidden representationsh i at different layers i.</p>
<p>two nodes. In our case, as AMR relations are reified and stand for new nodes of the graphs, our new relations can either be of direct (aa/and </p>
<p>o1/occidentalism 
o2/orientalism </p>
<p>:op1 
:op2 </p>
<p>and :op1 occidentalism :op2 orientalism </p>
<p>and :op1 occidental is m 
orient alism </p>
<p>1 
2 
3 
4 5 
6 
7 </p>
<p>:op2 </p>
<p>8 </p>
<p>Tokenization </p>
<p>LIN(..) </p>
<p>(a) AMR </p>
<p>(b) Linearization </p>
<p>(c) New Graph </p>
<p>Figure 1: Examples of AMR tokenization for the sen-
tence "Occidentalism and Orientialism.". The result-
ing input graph in (c) contains 8 nodes. </p>
<p>Multi-Head 
Attention </p>
<p>Add &amp; Norm </p>
<p>Feed-forward </p>
<p>Adapter 
Vanilla Adapter </p>
<p>StructAdapt </p>
<p>Layer Norm. </p>
<p>GNN </p>
<p>Feed-forward </p>
<p>Layer Norm. </p>
<p>Feed-forward </p>
<p>Feed-forward </p>
<p>Figure 2: Vanilla Adapter vs StructAdapt </p>
<p>d </p>
<p>− → b) or re-
verse (a </p>
<p>r </p>
<p>← − b) connections type as in </p>
<p>Table 2 :
2Human Evaluation. Mean scores (±s.d.)</p>
<p>Table 3 :
3Graph Attack -We corrupt the graph connectivity information given to the structural adapters in encoder. We report mean performances (±s.d.) over 3 seeds.Adapter 
Meaning 
Preservation </p>
<p>Linguistic 
Correctness </p>
<p>GCN w/ RPE 
5.0±1.2 
5.6±0.8 
RGCN w/ RPE 
5.2±1.1 
5.6±0.8 </p>
<p>Table 4 :
4Graph Attack -Human Evaluation. Mean scores (±s.d.)</p>
<p>to-sequence learning using gated graph neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 273-283, Melbourne, Australia. Association for Computational Linguistics.Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65-72, Ann Ar-
bor, Michigan. Association for Computational Lin-
guistics. </p>
<p>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 
2018. 
Graph-Deng Cai and Wai Lam. 2020. AMR parsing via graph-
sequence iterative inference. In Proceedings of the 
58th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1290-1301, Online. As-
sociation for Computational Linguistics. </p>
<p>Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short 
Papers), pages 748-752, Sofia, Bulgaria. Associa-
tion for Computational Linguistics. </p>
<p>Kris Cao and Stephen Clark. 2019. Factorising AMR 
generation through syntax. In Proceedings of the 
2019 Conference of the North American Chapter of 
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and 
Short Papers), pages 2157-2163, Minneapolis, Min-
nesota. Association for Computational Linguistics. </p>
<p>Marco Damonte and Shay B. Cohen. 2019. Structural 
neural encoders for AMR-to-text generation. In Pro-
ceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational 
Linguistics: Human Language Technologies, Vol-
ume 1 (Long and Short Papers), pages 3649-3658, 
Minneapolis, Minnesota. Association for Computa-
tional Linguistics. </p>
<p>Philipp Dufter, Martin Schmitt, and Hinrich Schütze. 
2022. Position information in transformers: An 
overview. Computational Linguistics, 48(3):733-
763. </p>
<p>Cédric Fayet, Alexis Blond, Grégoire Coulombel, 
Claude Simon, Damien Lolive, Gwénolé Lecorvé, 
Jonathan Chevelu, and Sébastien Le Maguer. 2020. 
FlexEval, création de sites web légers pour des cam-
pagnes de tests perceptifs multimédias (FlexEval, 
creation of light websites for multimedia perceptual 
test campaigns). In Actes de la 6e conférence con-
jointe Journées d'Études sur la Parole (JEP, 33e 
édition), Traitement Automatique des Langues Na-
turelles (TALN, 27e édition), Rencontre des Étudi-
ants Chercheurs en Informatique pour le Traitement 
Automatique des Langues (RÉCITAL, 22e édition). 
Volume 4 : Démonstrations et résumés d'articles in-
ternationaux, pages 22-25, Nancy, France. ATALA 
et AFCP. </p>
<p>Matthias Fey and Jan E. Lenssen. 2019. Fast graph 
representation learning with PyTorch Geometric. In 
ICLR Workshop on Representation Learning on 
Graphs and Manifolds. </p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and 
Jaime Carbonell. 2016. Generation from Abstract 
Meaning Representation using tree transducers. In 
Proceedings of the 2016 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, 
pages 731-739, San Diego, California. Association 
for Computational Linguistics. </p>
<p>Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei 
Lu. 2019. Densely connected graph convolutional 
networks for graph-to-sequence learning. Transac-
tions of the Association for Computational Linguis-
tics, 7:297-312. </p>
<p>Thomas N. Kipf and Max Welling. 2017. Semi-
Supervised Classification with Graph Convolutional 
Networks. In Proceedings of the 5th International 
Conference on Learning Representations, ICLR '17. </p>
<p>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin 
Choi, and Luke Zettlemoyer. 2017. Neural AMR: 
Sequence-to-sequence models for parsing and gener-
ation. In Proceedings of the 55th Annual Meeting of 
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 146-157, Vancouver, 
Canada. Association for Computational Linguistics. </p>
<p>Gerasimos Lampouras and Andreas Vlachos. 2017. 
Sheffield at SemEval-2017 task 9: Transition-based 
language generation from AMR. In Proceedings of 
the 11th International Workshop on Semantic Eval-
uation (SemEval-2017), pages 586-591, Vancouver, 
Canada. Association for Computational Linguistics. </p>
<p>Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of the 
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160-167, Sapporo, Japan. 
Association for Computational Linguistics. </p>
<p>Juri Opitz and Anette Frank. 2021. Towards a decom-
posable metric for explainable evaluation of text gen-
eration from AMR. In Proceedings of the 16th Con-
ference of the European Chapter of the Association 
for Computational Linguistics: Main Volume, pages 
1504-1518, Online. Association for Computational 
Linguistics. </p>
<p>Maja Popović. 2017. chrF++: words helping charac-
ter n-grams. In Proceedings of the Second Con-
ference on Machine Translation, pages 612-618, 
Copenhagen, Denmark. Association for Computa-
tional Linguistics. </p>
<p>Matt Post. 2018. A call for clarity in reporting BLEU 
scores. In Proceedings of the Third Conference on 
Machine Translation: Research Papers, pages 186-
191, Brussels, Belgium. Association for Computa-
tional Linguistics. </p>
<p>Nima Pourdamghani, Kevin Knight, and Ulf Herm-
jakob. 2016. Generating English from Abstract 
Meaning Representations. In Proceedings of the 9th </p>
<p>International Natural Language Generation confer-
ence, pages 21-25, Edinburgh, UK. Association for 
Computational Linguistics. </p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, 
Dario Amodei, and Ilya Sutskever. 2019. Lan-
guage Models are Unsupervised Multitask Learners. 
https://openai.com/blog/better-language-models/. </p>
<p>Aarne Ranta. 2011. Grammatical Framework -Pro-
gramming with Multilingual Grammars. CSLI Stud-
ies in Computational Linguistics. Cambridge Univer-
sity Press. </p>
<p>Leonardo F. R. Ribeiro, Claire Gardent, and Iryna 
Gurevych. 2019. Enhancing AMR-to-text genera-
tion with dual graph representations. In Proceed-
ings of the 2019 Conference on Empirical Methods 
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 3183-3194, Hong 
Kong, China. Association for Computational Lin-
guistics. </p>
<p>Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich 
Schütze, and Iryna Gurevych. 2021a. Investigating 
pretrained language models for graph-to-text gen-
eration. In Proceedings of the 3rd Workshop on 
Natural Language Processing for Conversational AI, 
pages 211-227, Online. Association for Computa-
tional Linguistics. </p>
<p>Leonardo F. R. Ribeiro, Yue Zhang, and Iryna 
Gurevych. 2021b. Structural adapters in pretrained 
language models for AMR-to-Text generation. In 
Proceedings of the 2021 Conference on Empirical 
Methods in Natural Language Processing, pages 
4269-4282, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics. </p>
<p>Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, 
Rianne van den Berg, Ivan Titov, and Max Welling. 
2018. Modeling relational data with graph convolu-
tional networks. In The Semantic Web, pages 593-
607, Cham. Springer International Publishing. </p>
<p>Martin Schmitt, Leonardo F. R. Ribeiro, Philipp Dufter, 
Iryna Gurevych, and Hinrich Schütze. 2021. Mod-
eling graph structure via relative position for text 
generation from knowledge graphs. In Proceedings 
of the Fifteenth Workshop on Graph-Based Methods 
for Natural Language Processing (TextGraphs-15), 
pages 10-21, Mexico City, Mexico. Association for 
Computational Linguistics. </p>
<p>Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 
2018. Self-attention with relative position represen-
tations. In Proceedings of the 2018 Conference of 
the North American Chapter of the Association for 
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 464-468, 
New Orleans, Louisiana. Association for Computa-
tional Linguistics. </p>
<p>Table 5 :
5Runtime -Runs are executed on a single NVIDIA GeForce RTX 3090 GPU.</p>
<p>Table 6 :
6Position Attack -Relative Position Embeddings are shuffled.
If k samples cannot be extracted for both positive and negative classes, the sample is discarded.
AcknowledgementsWe would like to thank annotators and reviewers for taking the time and effort necessary to share our contribution. This work was partially funded by the ANR Cifre conventions N°2020/0400 and Orange Innovation Research.
AMR-to-text generation as a traveling salesman problem. Linfeng Song, Yue Zhang, Xiaochang Peng, Zhiguo Wang, Daniel Gildea, 10.18653/v1/D16-1224Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsLinfeng Song, Yue Zhang, Xiaochang Peng, Zhiguo Wang, and Daniel Gildea. 2016. AMR-to-text gen- eration as a traveling salesman problem. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2084-2089, Austin, Texas. Association for Computational Lin- guistics.</p>
<p>A graph-to-sequence model for AMRto-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, 10.18653/v1/P18-1150Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaLong Papers1Association for Computational LinguisticsLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for AMR- to-text generation. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1616- 1626, Melbourne, Australia. Association for Compu- tational Linguistics.</p>
<p>Sequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, V Quoc, Le, Proceedings of the 27th International Conference on Neural Information Processing Systems. the 27th International Conference on Neural Information Processing SystemsCambridge, MA, USAMIT PressIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems -Vol- ume 2, NIPS'14, page 3104-3112, Cambridge, MA, USA. MIT Press.</p>
<p>Neural headline generation on Abstract Meaning Representation. Jun Sho Takase, Naoaki Suzuki, Tsutomu Okazaki, Masaaki Hirao, Nagata, 10.18653/v1/D16-1112Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsSho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hirao, and Masaaki Nagata. 2016. Neural head- line generation on Abstract Meaning Representation. In Proceedings of the 2016 Conference on Empiri- cal Methods in Natural Language Processing, pages 1054-1059, Austin, Texas. Association for Compu- tational Linguistics.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, volume 30. Curran Associates, Inc.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, Graph attention networks. 6th International Conference on Learning Representations. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2017. Graph attention networks. 6th International Conference on Learning Representations.</p>
<p>Qun Liu, and Jakob Grue Simonsen. 2021. On position embeddings in {bert}. Benyou Wang, Lifeng Shang, Christina Lioma, Xin Jiang, Hao Yang, International Conference on Learning Representations. Benyou Wang, Lifeng Shang, Christina Lioma, Xin Jiang, Hao Yang, Qun Liu, and Jakob Grue Simon- sen. 2021. On position embeddings in {bert}. In International Conference on Learning Representa- tions.</p>
<p>What do position embeddings learn? an empirical study of pre-trained language model positional encoding. Yu-An Wang, Yun-Nung Chen, 10.18653/v1/2020.emnlp-main.555Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Yu-An Wang and Yun-Nung Chen. 2020. What do position embeddings learn? an empirical study of pre-trained language model positional encoding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6840-6849, Online. Association for Computa- tional Linguistics.</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Drame, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsQuentin Lhoest, and Alexander RushOnline. Association for Computational LinguisticsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-art natural language process- ing. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Asso- ciation for Computational Linguistics.</p>
<p>Bertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020OpenReview.netTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020a. Bertscore: Eval- uating text generation with BERT. In 8th Inter- national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Lightweight, dynamic graph convolutional networks for AMR-to-text generation. Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B Cohen, Zuozhu Liu, Lidong Bing, 10.18653/v1/2020.emnlp-main.169Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsYan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B. Cohen, Zuozhu Liu, and Lidong Bing. 2020b. Lightweight, dynamic graph convolutional networks for AMR-to-text generation. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2162-2172, Online. Association for Computational Linguistics.</p>
<p>Line graph enhanced AMR-to-text generation with mix-order graph attention networks. Yanbin Zhao, Lu Chen, Zhi Chen, Ruisheng Cao, Su Zhu, Kai Yu, 10.18653/v1/2020.acl-main.67Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsYanbin Zhao, Lu Chen, Zhi Chen, Ruisheng Cao, Su Zhu, and Kai Yu. 2020. Line graph enhanced AMR-to-text generation with mix-order graph at- tention networks. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 732-741, Online. Association for Computational Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
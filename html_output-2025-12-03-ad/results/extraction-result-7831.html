<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7831 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7831</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7831</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-274776630</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.07791v9.pdf" target="_blank">Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge</a></p>
                <p><strong>Paper Abstract:</strong> LLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases - particularly position bias, the tendency to favor solutions based on their position within the prompt - compromise its reliability. This exploratory study evaluates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness. Our experiments, involving 15 LLM judges across MTBench and DevBench with 22 tasks and approximately 40 solution-generating models, result in over 150,000 evaluation instances. We identify Judge-Level, Candidate-Level, and Task-Level factors contributing to bias. The findings confirm that position bias is not due to random chance and varies significantly across judges and tasks. While position bias is weakly influenced by the length of prompt components, it is strongly affected by the quality gap between solutions. Our agreement and disagreement analysis among judges further provides insights into the distribution of judging difficulty across the dataset, and highlights the potential for dataset modifications.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7831.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7831.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior human-LLM agreement (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reported high agreement between LLM judges and human judgments in prior benchmark studies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior work reporting a high level of agreement between LLM-based judges and human evaluators and uses benchmarks (MTBench, DevBench) whose human evaluations are described as validating state-of-the-art LLM judges (e.g., GPT-4, GPT-4-Turbo). No numeric human-vs-LLM agreement numbers are provided in this paper itself.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Multiple tasks covered by MTBench and DevBench (e.g., coding, extraction, humanities, math, reasoning, roleplay, STEM, writing; UML / architecture metrics in DevBench)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MTBench and DevBench (benchmarks with prior human evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4; GPT-4-Turbo (examples cited as validated by benchmark human evaluations); general LLM judges (15 models evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>State-of-the-art closed-source models (GPT-4, GPT-4-Turbo, GPT-4o etc.) and other family models (Claude, Gemini, Llama); versions for API calls listed in Appendix F.1 for judges used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human evaluations from the original MTBench and DevBench benchmark studies (not specified in detail in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Position bias (LLM judges favoring prompt position) undermines accuracy; fairness issues (primacy/recency preference); reliability concerns when quality gaps are small; mitigation methods incomplete or impractical for closed-source models</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Authors state that prior work reports high human-LLM agreement and therefore selected MTBench and DevBench for experiments; this paper does not compute new human-LLM agreement scores but relies on those prior validations. The paper finds systematic position bias in LLM judges (judge- and task-dependent), and that hard-to-judge instances (small quality gap) lead to more disagreement among LLM judges and are the cases most prone to position bias.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Scalability, reproducibility, lower cost relative to human evaluation, and ability to run large numbers of consistent trials (enabled over 100k LLM judgments in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Paper uses pairwise and list-wise LLM-as-a-judge paradigms (double-blind, swap/permutation of answer order), Two-option/Three-option modes, repetition trials to compute Repetition Stability (RS), temperature=1; the paper does not perform new human evaluations but uses benchmarks previously validated with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7831.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7831.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No direct human comparison (limitation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Absence of direct head-to-head human vs. LLM judge comparisons in this study</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study did not collect new human judgments or perform direct quantitative comparisons between LLM judges and humans; instead it relied on benchmarks (MTBench, DevBench) that were previously human-evaluated and described as validated in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>The same diverse tasks as MTBench and DevBench used in the study (coding, extraction, reasoning, UML/architecture metrics, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MTBench and DevBench</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>15 LLM judges (various GPT, Claude, Gemini, and Llama models used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Mix of 12 closed-source and 3 open-source models; judge versions and API identifiers listed in Appendix F.1 (e.g., gpt-4-0613, gpt-4o-2024-05-13, claude-3-5-sonnet-20240620).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>No human evaluators were run for this paper; reliance on benchmark-provided human annotations (details not collected here)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Paper cannot report quantitative LLM-vs-human agreement metrics because it did not run direct human comparisons; therefore limitations include inability to quantify how position bias compares to human judgments and inability to report LLM vs human failure modes numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Authors explicitly note as a limitation that they evaluated only LLM judges and did not perform fresh human comparisons, relying on prior benchmarks' human validations; they emphasize that position bias findings are internal to LLM judges and that direct human comparison was outside the scope of this study.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>N/A for this limitation entry (the limitation is the lack of direct human comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Post hoc analysis: LLM judgments were collected first and then analyzed; no new human annotation experiments were performed in this study. The paper therefore cannot provide newly measured human-LLM agreement statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Devbench: A comprehensive benchmark for software development <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot rankers for recommender systems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7831",
    "paper_id": "paper-274776630",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "Prior human-LLM agreement (cited)",
            "name_full": "Reported high agreement between LLM judges and human judgments in prior benchmark studies",
            "brief_description": "The paper cites prior work reporting a high level of agreement between LLM-based judges and human evaluators and uses benchmarks (MTBench, DevBench) whose human evaluations are described as validating state-of-the-art LLM judges (e.g., GPT-4, GPT-4-Turbo). No numeric human-vs-LLM agreement numbers are provided in this paper itself.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge",
            "evaluation_task": "Multiple tasks covered by MTBench and DevBench (e.g., coding, extraction, humanities, math, reasoning, roleplay, STEM, writing; UML / architecture metrics in DevBench)",
            "dataset_name": "MTBench and DevBench (benchmarks with prior human evaluations)",
            "judge_model_name": "GPT-4; GPT-4-Turbo (examples cited as validated by benchmark human evaluations); general LLM judges (15 models evaluated in this paper)",
            "judge_model_details": "State-of-the-art closed-source models (GPT-4, GPT-4-Turbo, GPT-4o etc.) and other family models (Claude, Gemini, Llama); versions for API calls listed in Appendix F.1 for judges used in experiments.",
            "human_evaluator_type": "Human evaluations from the original MTBench and DevBench benchmark studies (not specified in detail in this paper)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "Position bias (LLM judges favoring prompt position) undermines accuracy; fairness issues (primacy/recency preference); reliability concerns when quality gaps are small; mitigation methods incomplete or impractical for closed-source models",
            "qualitative_findings": "Authors state that prior work reports high human-LLM agreement and therefore selected MTBench and DevBench for experiments; this paper does not compute new human-LLM agreement scores but relies on those prior validations. The paper finds systematic position bias in LLM judges (judge- and task-dependent), and that hard-to-judge instances (small quality gap) lead to more disagreement among LLM judges and are the cases most prone to position bias.",
            "advantages_of_llm_judge": "Scalability, reproducibility, lower cost relative to human evaluation, and ability to run large numbers of consistent trials (enabled over 100k LLM judgments in this study).",
            "experimental_setting": "Paper uses pairwise and list-wise LLM-as-a-judge paradigms (double-blind, swap/permutation of answer order), Two-option/Three-option modes, repetition trials to compute Repetition Stability (RS), temperature=1; the paper does not perform new human evaluations but uses benchmarks previously validated with human judgments.",
            "uuid": "e7831.0",
            "source_info": {
                "paper_title": "Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "No direct human comparison (limitation)",
            "name_full": "Absence of direct head-to-head human vs. LLM judge comparisons in this study",
            "brief_description": "The study did not collect new human judgments or perform direct quantitative comparisons between LLM judges and humans; instead it relied on benchmarks (MTBench, DevBench) that were previously human-evaluated and described as validated in prior work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge",
            "evaluation_task": "The same diverse tasks as MTBench and DevBench used in the study (coding, extraction, reasoning, UML/architecture metrics, etc.)",
            "dataset_name": "MTBench and DevBench",
            "judge_model_name": "15 LLM judges (various GPT, Claude, Gemini, and Llama models used in experiments)",
            "judge_model_details": "Mix of 12 closed-source and 3 open-source models; judge versions and API identifiers listed in Appendix F.1 (e.g., gpt-4-0613, gpt-4o-2024-05-13, claude-3-5-sonnet-20240620).",
            "human_evaluator_type": "No human evaluators were run for this paper; reliance on benchmark-provided human annotations (details not collected here)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "Paper cannot report quantitative LLM-vs-human agreement metrics because it did not run direct human comparisons; therefore limitations include inability to quantify how position bias compares to human judgments and inability to report LLM vs human failure modes numerically.",
            "qualitative_findings": "Authors explicitly note as a limitation that they evaluated only LLM judges and did not perform fresh human comparisons, relying on prior benchmarks' human validations; they emphasize that position bias findings are internal to LLM judges and that direct human comparison was outside the scope of this study.",
            "advantages_of_llm_judge": "N/A for this limitation entry (the limitation is the lack of direct human comparison).",
            "experimental_setting": "Post hoc analysis: LLM judgments were collected first and then analyzed; no new human annotation experiments were performed in this study. The paper therefore cannot provide newly measured human-LLM agreement statistics.",
            "uuid": "e7831.1",
            "source_info": {
                "paper_title": "Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Devbench: A comprehensive benchmark for software development",
            "rating": 2,
            "sanitized_title": "devbench_a_comprehensive_benchmark_for_software_development"
        },
        {
            "paper_title": "Large language models are zero-shot rankers for recommender systems",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_rankers_for_recommender_systems"
        }
    ],
    "cost": 0.01096725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge
11 Nov 2025</p>
<p>Lin Shi lin.shi.26@dartmouth.edu 
Dartmouth College</p>
<p>Chiyu Ma 
Dartmouth College</p>
<p>Wenhua Liang 
Dartmouth College</p>
<p>Xingjian Diao 
Dartmouth College</p>
<p>Weicheng Ma 
Dartmouth College</p>
<p>Soroush Vosoughi 
Dartmouth College</p>
<p>Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge
11 Nov 2025F07A875D453D8309CE108BB0A0A085DBarXiv:2406.07791v9[cs.CL]
LLM-as-a-Judge has emerged as a promising alternative to human evaluators across various tasks, yet inherent biases-particularly position bias, the tendency to favor solutions based on their position within the prompt-compromise its reliability.This exploratory study evaluates position bias in LLM judges across pairwise and list-wise comparison settings, introducing three metrics: repetition stability, position consistency, and preference fairness.Our experiments, involving 15 LLM judges across MT-Bench and DevBench with 22 tasks and approximately 40 solution-generating models, result in over 150,000 evaluation instances.We identify Judge-Level, Candidate-Level, and Task-Level factors contributing to bias.The findings confirm that position bias is not due to random chance and varies significantly across judges and tasks.While position bias is weakly influenced by the length of prompt components, it is strongly affected by the quality gap between solutions.Our agreement and disagreement analysis among judges further provides insights into the distribution of judging difficulty across the dataset, and highlights the potential for dataset modifications.</p>
<p>Introduction</p>
<p>In recent years, Large Language Models (LLMs) have emerged as evolutionary technologies, gathering global interest and stimulating substantial research into their applications.Evaluating LLMs has received increasing attention due to their advancing capabilities across diverse fields.While human assessment is considered the gold standard for aligning with human preferences, it lacks scalability in extensive evaluations (Zeng et al., 2023;Karpinska et al., 2021).To automate evaluations and reduce reliance on costly human evaluators, the LLM-as-a-Judge methodology emerged as a promising alternative across various tasks.Despite a high level of agreement with human judgments (Zheng et al., 2024b;Li et al., 2024a;Zhu et al., 2023), inherent biases, especially position bias, have undermined the accuracy, fairness, and reliability of these LLM evaluators.</p>
<p>Position bias refers to the tendency of LLM judges to favor certain positions within prompt components rather than evaluating content objectively, as shown in Fig. 1 (a).This bias has been observed across various types of LLM judges (Qin et al., 2024;Li et al., 2023c), raising concerns about their reliability.Prior studies have identified position bias alongside other biases and assessed its impact (Zheng et al., 2024a,b;Zeng et al., 2023), but these investigations remain largely preliminary and lack a focused, systematic exploration.Although mitigation strategies have been proposed, they often suffer from incomplete bias removal (Guo et al., 2024), added complexity (Li et al., 2024b;Khan et al., 2024;Chua et al., 2024), the introduction of new biases (Ohi et al., 2024), inconsistent effectiveness (Gallegos et al., 2024), or impracticality for closed-source models requiring access to model internals (Wang et al., 2025;Yu et al., 2025).Moreover, most empirical studies primarily examine position bias in pairwise settings (Wang et al., 2023;Zheng et al., 2024b), leaving its underlying factors and behavior in more complex list-wise paradigms underexplored.These gaps underscore the need for a more systematic, in-depth exploratory analysis of position bias to better understand its origins, manifestations, and implications.</p>
<p>In this study, we provide an in-depth and systematic investigation into position bias within the context of LLM-as-a-Judge.While evaluating with Position Consistency (Zheng et al., 2024b), we further introduce two novel metrics: Preference Fairness and Repetition Stability.Specifically, we move beyond simply assessing Position Consistency by incorporating Preference Fairness, which provides deeper insights into the specific answer directions where models exhibit unfair preferences.</p>
<p>The same prompt [Judgment]: {A} Solution [1] is better.</p>
<p>[Reasons]: Solution [1]… LLM Judge [Question]: Compute 1+...+100</p>
<p>[System Prompt]: Please act as an impartial judge and choose the better… Additionally, the measurement of Repetition Stability ensures that the observed position bias in the given model and tasks is not due to random variations, thus strengthening the reliability of the findings.</p>
<p>To investigate the underlying factors contributing to position bias, we categorized these factors into three levels: Judge-Level, Candidate-Level, and Task-Level.Our experiments are primarily conducted on pairwise comparisons, as LLM judges demonstrate superior performance in this setting.We further extend our study to more complicated list-wise comparison settings, involving evaluations of more than two candidate models by LLM judges.Our findings reveal several key insights: 1.The position bias of capable LLM judges is not a result of random variations.2. There is a high volatility in the direction of preference, even within the same LLM judge when applied to different tasks.3. Differences in answer quality among candidate models significantly influence position consistency.4. Position bias is very weakly correlated with the length of prompts generated by the candidate models.</p>
<p>Building on these findings, we conduct an agreement analysis among the LLM judges.The results reveal that, although measures of position consistency may appear similar in general, judgments on specific instances vary significantly among LLM judges, even when they demonstrate comparable capabilities.Instances where numerous LLMs agree are generally easier to judge, whereas in-stances with disagreements are more challenging to evaluate and more prone to position bias.This analysis provides insights into the distribution of judging difficulty across the dataset and highlights the potential for dataset modifications by incorporating more instances that are either easier or more difficult to judge.Future work could explore how to measure the likelihood of position bias arise from the datasets by identifying and quantifying such hard-to-judge instances before implementing LLM judges.</p>
<p>Evaluation Settings &amp; Definitions</p>
<p>We begin by outlining the settings for pairwise and list-wise comparisons employed in our experiments for LLM-as-a-Judge.Following this, we define the three metrics used in our evaluation: Position Consistency (PC), Preference Fairness (PF), and Repetition Stability (RS).Finally, we provide a detailed description of the factors we found that are related to position bias at the Judge-Level, Candidate-Level, and Task-Level.Our exploratory study was conducted post hoc, meaning the LLM judgments were collected first, and the factors influencing position bias were then identified and analyzed.</p>
<p>Pairwise &amp; List-wise Comparison</p>
<p>Pairwise Comparison: In the context of pairwise comparison, LLM judges are tasked with selecting the better solution provided by two candidate models in response to a given task question.As shown in Fig. 1 (a), the system prompt, option choices, task question, and solutions from two candidate models (original prompt) are presented to the LLM judges to select the better solution.The experiment is conducted in a double-blind setting.The identities of the candidate models are hidden from the LLM judges, and the candidate models are unaware that their solutions will be compared to another model when answering the question.Then, the prompt with solutions in a swapped position (swapped prompt) is given to the same LLM judge again, which results in a judgment pair.If the LLM judge consistently favors the same solution regardless of the swapped position, it is considered position consistent.Conversely, if the LLM judge selects different winners, position bias is observed, with the preference direction being either primacy (e.g.always choose {A}) or recency (e.g.always choose {B}).Example of measuring preference fairness with specific choice pairs is shown in Fig. 1 (b).To accommodate the possibility of ties, various option modes are employed: Two-Option mode restricts LLM judges to choosing between two options, labeled A for the first candidate and B for the second.Three-Option mode introduces an additional choice, C, allowing LLM judges to indicate a tie if neither solution is preferable, as illustrated in Fig. 1 (a).These option modes were explicitly specified in the system prompts to ensure clear guidance for the decision-making process of the LLM judges.</p>
<p>List-wise Comparison: Unlike pairwise settings, where LLM judges select the superior solution from two candidates, list-wise comparative approaches involve evaluating three or more candidates simultaneously, as shown in Fig. 1 (c).For efficiency, we prompt LLM judges to select the best candidate rather than ranking the entire list.The "swapped setting" used in pairwise evaluations is generalized to order permutations for list-wise judgments, ensuring that each candidate appears in every possible position exactly once.For a list of p candidates, this results in p permutations.In the i th permutation, the i th candidate is set to appear in the first position.Additionally, an option is provided to account for ties, allowing LLM judges to indicate if there is no certainly superior solution.</p>
<p>Evaluation Metrics</p>
<p>In our study, we first verify whether capable LLM judges exhibit high repetition stability and then evaluate their position bias in terms of position consistency and preference fairness.The metrics are introduced as follows:</p>
<p>Repetition Stability (RS) evaluates the reliability of LLM judges when presented with identical queries multiple times.It is essential to determine whether the judgments of LLMs, and consequently the observations of position bias, stem from a consistent evaluation pattern or by random variations.We measure this by calculating the percentage of the most frequent selections across multiple trials for each query, aggregated from all queries within each dataset.This metric is formalized as
RC = 1 N N j=1 1 n j max k∈S |C j k | ,(1)
where S = {A, B, C, . . .} refers to the set of choice options depending on the option mode, |C j k | denotes the counts of each choice option selected by the judge for the j th query, n j represents the total number of repeating trials for that query, and N is the total number of queries.The value of RS ranges from a small positive value depending on the option mode, indicating completely random decisions, to 1.0, indicating perfect stability.</p>
<p>Position Consistency (P C) quantifies how frequently LLM judges prefer the same solution after the order of solutions is permuted.It is calculated as the ratio of consistent evaluation series to the total number of valid evaluations, where a series is deemed consistent if the LLM judge prefers the same winning solution across permutations.Formally, it is calculated as
P C = 1 n n j=1 1 {(C j 1 ,...,C j p ,)∈V } ,(2)
where V is the set of choices that correspond to position consistency, and (C j 1 , . . ., C j p ) denotes the judgment series for the j th query when there are p candidate solutions in the list, and n represents the number of prompt series.An example of such series of choices under pairwise comparison setting can be found in Fig. 1 (b).This formula aims to provide a direct measure of a LLM judge's position bias and has been widely used in previous studies for its simplicity.</p>
<p>Preference Fairness (P F ) measures the extent to which LLM judges favor certain solution positions.In pairwise comparisons, an LLM judge may exhibit a preference for either primacy or recency.These terms replace the more verbose "preference/bias for the first/second candidate model" used in previous studies (Zheng et al., 2024b), ensuring clarity and generalization for future research.The examples of such preferences are demonstrated in Fig. 1 (b).Previous studies proposed two common ways to measure the preference fairness.One way is to count the primacy-preferred and recencypreferred judgment pairs, which we termed as primacy-count-number (pcn) and recency-countnumber (rcn).The counts are then normalized by the total number of prompt pairs (Zheng et al., 2024b;Zhu et al., 2023).However, the sensitivity of this measurement highly depends on the size of dataset, making comparisons across datasets unreliable, especially when the number of questions and instances varies for each task.</p>
<p>Alternatively, instead of normalizing over the complete dataset, studies like (Li et al., 2023b;Liusie et al., 2024) treat position inconsistent evaluation instances independently.They calculate the percentages of primacy-preferred and recencypreferred judgment pairs relative to the total number of position inconsistent pairs.We denote these as inconsistent primacy rates (ipr) and inconsistent recency rates (irr), where ipr +irr = 1.However, this approach overlooks the fact that "position consistent judgments are also preference fair", which leads to overly penalizing highly consistent LLMjudges.</p>
<p>To overcome these limitations, we introduce a more granular and scalable measurement that combines the strengths of both methods, to assess preference fairness.The P F score is formally calculated by
P F = P F raw − S − min S + max − S − min × 2 − 1, P F raw = (rcn × irr) − (pcn × ipr).(3)
where S − min and S + max are the minimum and maximum achievable P F raw scores for each judge on each task, respectively.This min-max scale ensures comparability across datasets by accounting for the range of achievable scores and centering the scale around zero.The P F score is interpreted as follows:
P F =               </p>
<p>1,</p>
<p>if P C = 0 and entirely recency-preferred x ∈ (0, 1), Recency-preferred 0, Preference Fair x ∈ (−1, 0), Primacy-preferred −1, if P C = 0 and entirely primacy-preferred</p>
<p>To extend this metric to list-wise comparisons, we employed a 'one vs. all' approach, defining primacy preference as favoring the first candidate solution while classifying all others as recencypreferred.This straightforward extension of the P F computation maintains consistency with pairwise setups.By providing a single and comprehensive metric that applies to all evaluation instances and list-wise settings, our proposed P F score ensures sensitivity across datasets, regardless of variations in the number of questions or instances, offering a significant improvement over previous methods.</p>
<p>Factors Affecting Position Bias</p>
<p>To investigate the factors influencing position bias in LLM judges, we categorized these factors into three groups: Judge-level, Candidate-level, and Task-level factors.Each group includes specific factors, that we hypothesize, may impact position bias, which we explore through a series of experiments.Table 1 lists the five factors we analyzed in this study.</p>
<p>Among the influencing factors, we selected "familial property" for Judge-level factors, as it reflects similar model sizes or training specifics, which are often proprietary and not publicly accessible for closed-source capable models.The familial categories of the models used in our studies are (1) GPT, (2) Claude, (3) Gemini, and (4) Llama allowing for straightforward grouping by company and version.More details and discussions about the familial property can be found in Appendix Sec. C.</p>
<p>Answer quality gap: While prior studies (Wang et al., 2023) explored quality disparities using "score gaps" in score-based LLM-as-a-Judge, this factor remains under-explored in comparative settings, which we address by introducing "answer quality gap" for both pairwise and list-wise evaluations.We define the quality of a candidate's solution by how effectively it addresses the question.Consequently, the answer quality gap refers to the disparity in quality between the solutions from one candidate model and the others to the same question and hence considered the Candidate-level factor.Ideally, when a reliable LLM judge is presented with a question and corresponding answer pairs or series, it would prefer the highest-quality answer, where the corresponding candidate is denoted as the winner selected by the LLM judge.</p>
<p>Following this assumption, we measure the answer quality gap by the win rates of candidates over an expected baseline on a set of tasks and questions.</p>
<p>However, if position bias occurs, the winner may be inconsistent when the order of candidate solutions is permuted in the query.Therefore, we categorize the LLM judgments into three groups: cases where the same winner is consistently chosen across all permutations (termed "consistent wins"), cases where there is no certain winner (termed "consistent ties"), and cases where different winners are selected after the solutions are permuted in the queries (termed "inconsistent judgment series").We denote these counts as the number of consistent wins (C w ), consistent ties (C t ), and inconsistent judgment series (C I ), respectively.Inspired by Zheng et al., we count inconsistent judgment pairs as ties for all candidate models, which is later calculated as a down-scaled win rate depending on the number of candidate models.</p>
<p>To calculate the win rates of candidate models for all three cases, we define the overall win rate (owr) of a model's solution over the other as:
owr = 1 n [C w + 1 p (C t + C I )],
where we have p candidates in the list and n judgment series.Then the answer quality gap (δ q ) is calculated as δ q = |owr − 1/p|, where 1/p is the expected baseline when all judgments are "ties".In contrast to using only consistent win rate (calculated as Cw nc , where n c is the number of position consistent judgment series) to quantify δ q (Zheng et al., 2024b;Li et al., 2023a;Raina et al., 2024), the adoption of overall win rate incorporates all data points and captures the "comparable quality" cases, where responses in similar quality might lead to position biased judgments, a scenario that the consistent win rate might overlook.</p>
<p>Experiment</p>
<p>Experiment Settings</p>
<p>In this study, we evaluated position bias of 15 models from the GPT (OpenAI, 2023), Claude (Anthropic, 2024), Gemini (Gemini Team, 2024), andLlama (Touvron et al., 2023) series using our framework.For datasets, we adopted the modified MTBench (Zheng et al., 2024b) and DevBench (Li et al., 2024a) due to their demonstrated high human-LLM agreement and the validated reliability of state-of-the-art LLM judges on the evaluation tasks.For pairwise comparisons, We fixed one of the candidates as vicuna-13b-v1.3 for MTBench and human for DevBench to serve as baselines, ensuring decent quality of solutions to the given questions.MTBench consists of 30 candidate models, 8 tasks, and 10 questions per task; for DevBench, we divide the general metric into more detailed ones and consider them as different tasks, resulting in 10 candidate models, 14 tasks, and 8 questions per task.We then paired solutions of these candidate models with that of the baseline candidate for evaluation by the LLM judges.We adopted Two-option mode for MTBench, and Three-option mode for DevBench.For listwise experiments, we randomly sampled 9 models to form three triple-candidate lists and evaluated four representative LLM judges on MTBench.The prompt templates we used are identical to those in the benchmarks for pairwise comparisons, with minor modifications to accommodate list-wise evaluations.More details about the models, tasks, and prompts can be found in Appendix.Sec.F.</p>
<p>To compute repetition stability, we sampled 3 questions per task and 4 candidate models, paired with baseline candidates, for each LLM judge to evaluate across 3 repetitive trials.This resulted in 576 instances per judge for MTBench and 432 instances per judge for DevBench.The temperature hyperparameter was set to 1 for all LLM judges to generate nontrivial results.To compute position consistency and preference fairness, the number of instances increased to 4,800 and 2,240, covering the entire MTBench and DevBench datasets.In total, more than 100,000 evaluation instances were analyzed in this study.</p>
<p>To identify significant factors contributing to position bias, we performed bidirectional stepwise regression on data from the two benchmarks.We used variables such as average lengths of input, output, and prompt; answer quality gap; LLM judge series; candidate identities; and task categories to predict P C and P F , respectively.Each model prunes non-significant variables based on the Akaike Information Criterion (AIC) score.This process involves both forward selection and backward elimination, with each "step" testing whether including or excluding a variable improves the model's AIC value.Further details about the process can be found in Appendix.Sec.E.</p>
<p>Empirical Results</p>
<p>The evaluation results of 12 close-source and 3 open-source LLM judges in terms of repetition stability, position consistency, and preference fairness on MTBench and DevBench are listed in Table 2.For each judge, we calculate its average RS, P C, and P F across all candidates and tasks.For  1, LLM judge series, candidate identities, and task categories significantly impact Position Consistency among all variables.Similarly, these factors also contribute significantly to Preference Fairness.Additionally, we found that average output length is a statistically significant predictor of P F .This finding is not surprising, as longer outputs are generally perceived as higher quality and more preferred.Quantitative results and more discussions can be found in Appendix.Sec.E.
RS Factor Judge-level Candidate-level Task-level Familial Property ✓<em> × × Answer Quality Gap × ✓</em> × Task Input Length × × ✓ Task Output Length × ✓<em> ✓</em> Prompt Length × ✓ ✓</p>
<p>Main Findings</p>
<p>Position Bias of Capable Judges are not Mere Random Variations: As shown in Table 2, the capable judges on the benchmark tasks, supported by minimal "Error" rates, generally exhibit RS values above 0.85.The most capable models, such as Claude-3.5-Sonnet,GPT-4, and Llama-3.3-70B,all achieve near-perfect RS scores over 0.95 on both benchmarks.These results confirm that judgments from capable LLM judges, and the resulting position bias, are not due to random variations.This strengthens confidence that one-time generated judgments by these validated LLMs accurately reflect their judging capabilities.</p>
<p>Position Bias Varies by Judge &amp; Task: As shown in Fig. 2(a), position bias among LLM judges varies significantly across different judges and tasks.For instance, GPT-4o demonstrates higher position consistency when evaluating coding tasks but performs less consistently on other tasks compared to GPT-4.Similarly, Gemini-1.5pro,while achieving higher P C than other Gemini models in most tasks, exhibits comparable consistency when judging extraction tasks.Similar findings can be observed in the DevBench results, as detailed in Appendix.Sec.D.2.</p>
<p>Variations in preference fairness are also evident.As shown in Table 2, GPT-4 and GPT-3.5-Turbodisplay different preference biases across datasets and tasks-being recency-preferred on MTBench but primacy-preferred on DevBench.Likewise, Claude-3.5-Sonnet,which is nearly preference-fair on MTBench (P F = 0.01), exhibits a strong recency-preferred position bias on DevBench (P F = 0.22).</p>
<p>While higher position consistency generally correlates with improved preference fairness (as demonstrated by the regression curve in Fig. 2(c)), consistency alone does not guarantee fairness.Certain LLM judges, despite achieving high P C, still exhibit significant and varied preference directions across different tasks, underscoring the need to evaluate both consistency and fairness when assessing LLM judges.</p>
<p>In list-wise comparisons, similar variations in position bias were observed across judges and tasks.Furthermore, Table 2 highlights that more capable models, such as GPT-4o and Claude-3.5-Sonnet,maintain high consistency when transitioning from pairwise to list-wise evaluations, while less capable models, such as GPT-3.5-Turbo,exhibit greater sensitivity to the increased number of candidates in list-wise tasks.</p>
<p>Therefore, the position bias of LLM judges is both judge-dependent and task-dependent.This observation is further confirmed by the bidirectional stepwise regression where judge identities and task categories are statistically significant predictors of P C and P F .In practice, when evaluating LLM judge's reliability or selecting suitable LLM judges, considering the balance between consistency and fairness, as well as accounting for task-specific variations, may be beneficial.</p>
<p>Position Bias Correlates to Answer Quality Gap: Intuitively, the difficulty of judging a pair Error Claude-3.5-Sonnet0.96 ± 0.07 0.82 ± 0.14 0.01 0.00 0.95 ± 0.09 0.76 ± 0.16 0.22 0.00 0.67 ± 0.19 0.17 ± 0.19 0.00 Claude-3-Opus 0.95 ± 0.08 0.70 ± 0.19 0.22 0.00 0.96 ± 0.07 0.69 ± 0.20 0.29 0.00 Claude-3-Sonnet 0.93 ± 0.11 0.59 ± 0.22 0.32 0.01 0.95 ± 0.09 0.71 ± 0.22 0.23 0.00 Claude-3-Haiku 0.89 ± 0.18 0.57 ± 0.18 0.18 0.00 0.90 ± 0.17 0.23 ± 0.14 0.75 0.00</p>
<p>Gemini-1.5-pro0.97 ± 0.09 0.62 ± 0.19 0.23 0.03 0.87 ± 0.17 0.84 ± 0.17 0.03 0.13 0.55 ± 0.20 0.33 ± 0.18 0.00 Gemini-1.0-pro0.89 ± 0.18 0.57 ± 0.18 0.30 0.00 0.85 ± 0.26 0.66 ± 0.20 -0.05 0.00 Gemini-1.5-flash1.00 ± 0.00 0.67 ± 0.17 0.07 0.00 0.04 ± 0.08 0.92 ± 0.39 0.00 0.96 GPT-4 0.97 ± 0.05 0.82 ± 0.15 0.02 0.00 0.97 ± 0.05 0.83 ± 0.15 -0.13 0.00 GPT-4-Turbo 0.94 ± 0.10 0.75 ± 0.16 0.02 0.00 0.97 ± 0.06 0.79 ± 0.18 0.16 0.00 GPT-4o</p>
<p>1.00 ± 0.02 0.76 ± 0.18 -0.12 0.00 0.98 ± 0.03 0.80 ± 0.16 -0.12 0.00 0.68 ± 0.22 0.18 ± 0.22 0.00 GPT-3.5-Turbo0.96 ± 0.07 0.70 ± 0.18 0.06 0.00 0.99 ± 0.02 0.76 ± 0.18 -0.02 0.00 0.34 ± 0.17 -0.05 ± 0.30 0.12 o1-mini 0.90 ± 0.07 0.76 ± 0.15 -0.04 0.00 0.93 ± 0.12 0.84 ± 0.13 -0.07 0.00 Llama-3.3-70B0.96 ± 0.06 0.80 ± 0.16 -0.05 0.00 0.99 ± 0.01 0.89 ± 0.12 -0.03 0.00 Llama-3.1-405B0.93 ± 0.10 0.77 ± 0.16 0.10 0.02 0.94 ± 0.10 0.79 ± 0.15 0.01 0.00 Llama-3.1-8B0.75 ± 0.32 0.69 ± 0.23 -0.03 0.25 0.79 ± 0.36 0.47 ± 0.18 0.25 0.00 of candidate answers is largely reflected by their difference in quality.In this study, as defined in Section 2.3, we quantify the quality gap (δ q ) between candidate solutions and expected baseline (calculated by 1/p for a p-candidate list) by the overall win rate (owr).Therefore, δ q increases as owr extends from baseline to 0 or 1. Fig. 2 (e) and (f) exhibit significant parabolic shapes, indicating that P C is positively proportional to δ q .This aligns with our intuition that the answer pairs or series with larger quality disparities are easier to achieve judgment consistency, whereas those of similar quality are difficult to judge, increasing the likelihood of position bias that leads to lower P C. The same relationship is observed for each individual judge and across benchmarks, as demonstrated in Appendix.Sec.D.</p>
<p>Similarly, as shown in Fig. 2 (d), judgments generally become more preference fair as δ q increases.However, the extent is not as significant as for P C. Also, the relationship varies by judge, as some LLMs maintain preference fairness regardless of δ q .For example, as shown in Appendix.Fig.5, P F of GPT models centered closely around 0 consistently, whereas that of Claude and Gemini-pro models exhibit a conspicuous proportional relationship on MTBench.These observations align with the right-arrow shape as demonstrated in Fig. 2 (c), where there is a general trend that judgments become preference fairer as position consistency increases.It also justifies the reasonableness of our quantification of preference fairness, as highly position consistent judges are not overly penalized and a perfect P C should result in P F = 0.</p>
<p>Together, we conclude that as the answer quality gap enlarges, judges generally become more position consistent and preference fair according to the regression curves.However, exceptions are common, as shown by the individual scatter points of these figures.This indicates that though the answer quality gap significantly influences the position bias of LLM judges, other factors also play important roles.Therefore, built on our findings, future studies may have better control over the answer quality gap when evaluating LLM judges, exploring other impacting factors on position bias, and seeking potential mitigation strategies.</p>
<p>Position Bias is weakly Length-dependent We investigate the impact of three different lengths on the position bias of LLM judges: the length of the question (task input length), the solution length of candidate models (task output length), and the length of the entire prompt (prompt length).By stepwise regression, we discovered that average task output length is only significant in predicting P F , adding a minimal change in AIC as shown in Appendix Table. 5.In other words, there is a very weak relationship between the lengths of prompt components and position bias.</p>
<p>LLM Agreement Analysis: We complement our investigation of position bias with an agreement and disagreement analysis among LLM judges.Rather than focusing exclusively on overall consistency or fairness, we examine how LLM judges converge and diverge in their assessments of individual instances.Agreement analysis quantifies  the percentage of instances where two LLM judges mutually agree on the outcome.Disagreement analysis counts the number of choices deviating from the mode for each instance among all judges.This further complies a "distribution of disagreement" across the dataset.</p>
<p>Our findings reveal that, despite exhibiting similar overall P C and P F scores, judges vary significantly in their judgments on individual instance.Disagreement analysis, in particular, highlights instances where consensus is either easily or difficultly achieved, reflecting the inherent complexity of the judgment task.For example, as shown in Fig. 3, more than half of the dataset can be considered relatively easy to judge, as over 80% of all 15 LLM judges agree with each other on these instances (disagreement ≤ 3).Conversely, fewer than 2% of instances represent the likely especially-hard-tojudge cases where a majority of LLM judges fail to reach consensus (disagreement ≥ 8).</p>
<p>Based on our observations of answer quality gaps and LLM agreement/disagreement patterns, this study offers practical insights for designing evaluator benchmarks that account for the varying difficulty levels of judgment tasks.Specifically, the most challenging instances to evaluate are characterized by: (1) frequent disagreements among LLM judges, (2) closely matched win rates and minimal quality gaps among candidate models, and (3) significant position bias exhibited by the majority of LLM judges.Further discussions and analyses can be found in Appendix Sec. C.</p>
<p>Conclusion</p>
<p>In conclusion, this paper provides an in-depth analysis of position bias in LLM judges, a critical challenge in automated evaluation.Using metrics such as repetition stability, position consistency, and preference fairness, we identify significant variations in position bias across judges and tasks, consistent across pairwise and list-wise comparison settings.Our findings show that position bias is weakly influenced by prompt length but strongly impacted by the quality gap between solutions.Furthermore, agreement and disagreement analysis highlights variability in judgment reliability, providing valuable insights for refining benchmarks.This study enhances understanding of position bias and contributes to the development of fairer and more reliable LLM evaluation frameworks.</p>
<p>Despite proposing scalable metrics and investigating key factors influencing position bias, our study has several limitations.</p>
<p>First, we evaluated only 12 commercial closedsource LLM judges for pairwise settings and 4 for list-wise paradigms across two benchmarks, limiting the list-wise comparisons to three-candidate lists.Among open-source models, we assessed only three Llama 3.1+ models of varying sizes, as earlier versions or other smaller models lacked sufficient context window lengths, making them unsuitable given the length of evaluation instances in our study.Additionally, while we used original benchmark prompt templates, exploring alternative prompting techniques could offer further insights.Future work could expand on this by incorporating more models, tasks, prompting strategies, and larger list-wise candidate pools to enhance the generalizability of our findings.</p>
<p>Second, data accessibility limitations prevented a direct analysis of Judge-level factors like architecture and parameter size of closed-source models.Instead, we approximated these factors by grouping models by family properties.While opensource models offer accessible architectural details for deeper analysis, our assessment of only three Llama models may not provide sufficient evidence for broader conclusions.Additionally, our analyses were conducted post hoc, relying on completed judgments before analysis.Future work could explore methods to estimate or control these factors pre-judgment, reducing computational costs and enabling proactive mitigation strategies.</p>
<p>Lastly, our focus was on evaluating and understanding position bias rather than mitigating it.While our findings provide a foundation for effective mitigation, further research is needed to address issues like maintaining consistency and fairness when answer quality gaps are minimal, where position bias is most pronounced.Multivariate analyses exploring interactions between factors like prompt length, task complexity, and answer quality gaps could also yield deeper insights and enhance mitigation approaches.</p>
<p>B Related</p>
<p>A Reproducibility</p>
<p>Our experiments were conducted primarily using API access, with a total cost of approximately 3,000 USD.The code repository for reproducing our results is available:https://anonymous.4open.science/r/Position-Bias-Analyzer-Demo-F7E3</p>
<p>B Related Work</p>
<p>B.1 LLM-as-a-Judge Large Language Models (LLMs) have become a transformative tool in automating evaluative tasks, offering scalability and reproducibility advantages over human assessments (Zheng et al., 2024b).The methodology of using LLMs as evaluators ("LLM-as-a-Judge") has been widely used for tasks such as open-ended story generation (Chiang and Lee, 2023a), adversarial attacks (Chiang and Lee, 2023b), summarization (Karpinska et al., 2021), machine translation (Kocmi and Federmann, 2023), and instruction following (Zeng et al., 2023), where models are tasked with scoring or ranking outputs.LLM-as-a-Judge paradigms range from pointwise, pairwise, and list-wise ranking approaches (Qin et al., 2024;Zhu et al., 2024) to score-based and relation-based (comparative or ranking) settings (Li et al., 2024b).Furthermore, explorations into multi-agent frameworks (Li et al., 2023b;Chan et al., 2023) and meta-thinking or meta-judge systems (Wu et al., 2024;Li et al., 2025) have emerged to enhance reliability of LLM evaluations.Despite their potential, inherent biases-particularly position bias-pose significant challenges to their reliability and fairness (Zheng et al., 2024b;Ye et al., 2024;Ma et al., 2025), even in the most effective pairwise comparative settings.</p>
<p>B.2 Position Bias</p>
<p>Position bias, the tendency of LLMs to favor certain positions within the prompt irrespective of actual content, is a pervasive issue observed across various domains and tasks (Zheng et al., 2024b;Qin et al., 2024;Li et al., 2024a,b).In the context of LLM-as-a-Judge, it typically refers to the position inconsistent scenarios where a LLM judge prefers different candidate solutions to a given question when the order of solutions in the prompt is permuted.To deal with such bias, the naive way is to exclude the position inconsistent judgments, which does not solve the fundamental issue and would likely result in data sparsity when position bias is frequently exhibited.Then, researchers proposed 'inconsistency-as-a-tie' for both candidate models in pairwise comparative settings to consider all judgments for further analysis (Zheng et al., 2024b;Li et al., 2023b).This approach, while practically useful for evaluations, does not mitigate position bias.</p>
<p>Given the significance of position bias, researchers have developed debiasing strategies, including bootstrapping (Hou et al., 2024), split-andmerge techniques (Li et al., 2023c), and multi-agent discussions (Li et al., 2023b;Khan et al., 2024).While these methods demonstrate potential, they are often costly, time-consuming, or insufficient to eliminate position bias.More recently, mechanistic approaches have been explored to address position bias by modifying model internals like positional embeddings and causal attention mechanisms (Yu et al., 2025;Wang et al., 2025).These methods, while effective in eliminating position bias, lack applicability to state-of-the-art closed-source models, which are frequently used in practice but model internals are inaccessible.Therefore, position bias remains a complex and challenging issue to fully resolve.</p>
<p>B.3 Evaluating Position Bias Evaluation</p>
<p>Parallel to developing debiasing strategies, a crucial line of research focuses on comprehensively assessing position bias and identifying its influencing factors, thereby fostering a deeper understanding of this phenomenon.The evaluation of position bias typically concentrates on two main aspects: the consistency of judgments when the order of candidate solutions in the prompt is changed, and the preference direction (e.g., primacy or recency bias) when inconsistencies arise.Metrics for these aspects in pairwise comparisons are relatively established; however, their extension to, and rigorous measurement in, list-wise settings remain less explored.</p>
<p>While prior works (Qin et al., 2024;Zhu et al., 2024) introduced list-wise ranking settings, they often become computationally expensive due to exponentially increasing number of pairs in the list or encounter significant degradation of performance due to ranking complexity.Moreover, the traditional preference direction measurement of position bias becomes increasingly complicated in list-wise paradigms (i.e., preferring the i th candidate solution for all i's).However, practically, "choosing the best out of an n-item list" is a simpler but common need compared to exhaustive ranking of the entire list, which is not yet fully explored by prior works.A unified single metric for effectively measuring preference direction is therefore necessary to facilitate position bias evaluation in a more comprehensive manner.</p>
<p>B.4 Factors influencing Position Bias</p>
<p>Position bias may be influenced by several factors, including judge attributes, task types, candidate solution lengths and qualities, and more.For example, (Wang et al., 2023) discovered that judgment conflict rate (i.e., frequency where position bias exhibits across the dataset) is negatively correlated with the score gap between the two candidate responses.However, quantifying and understanding the impact of this "answer quality gap" in comparative settings (both pairwise and list-wise) is an area that requires further investigation.</p>
<p>Besides, prior works often observe position bias and conduct subsequent analyses based on singleinstances judgments.However, observed biases might stem from random variations rather than systematic tendencies, hence compromising the reliability of downstream bias analyses.To mitigate this, researchers have employed multiple repetitions and used the mode judgment, an approach that, while robust, can be computationally expensive (Chen et al., 2024).In this sense, validating that position bias emerges from positional information rather than randomness becomes a crucial yet insufficiently-explored area.</p>
<p>These identified gaps -spanning from the validation of judgments to the evaluation of list-wise settings -highlight the need for a more comprehensive framework to evaluate and understand position bias.By addressing these dimensions collectively, our work provides a foundation for a deeper understanding of the consistency, fairness, and reliability of LLM evaluators.</p>
<p>C LLM Agreement Analysis</p>
<p>Besides the exploration of position bias with a broad lens by average P C and P F , instance-wise agreement between LLM judges is also insightful.Even two judges with the same P C and P F scores may not reach consensus on each instance.Therefore, this session investigates (1) what percentage of a set of evaluations do two LLM judges agree on each other?(2) how do the choices of all judges on an instance vary?</p>
<p>C.1 Mutual Agreement &amp; Familial Property</p>
<p>We compute the LLM judges' mutual agreement on the instances to explore how "alike" or consistent they are across a set of evaluations.We denote two judges agree on an instance if their judgment choices are identical.Then the mutual agreement between two LLM judges on a benchmark is defined as the proportion of their agreed instances.Figs.4(a) and (c) displays the mutual agreement heatmap for all judges on MTBench and DevBench, respectively.For MTBench that utilizes the 3-option mode, we also consider the "without tie" agreement since two judges are less disagreed when one chooses {C} while the other prefers a certain solution, compared to the case when they prefer different The "without tie" agreement heatmap of the twelve judges on MTBench is explored in Fig. 4(b).</p>
<p>The heatmaps reveal clear "familial patterns" in the judgment choices of these LLM judges.For instance, the GPT-4, GPT-4-Turbo, and GPT-4o series exhibit high agreement on MTBench, achieving over 70% with ties included and over 85% without.GPT-3.5-Turbodidn't agree with the GPT-4 series and o1-mini for around 40% of the instances, indicating that they are considerably different in judging capabilities.</p>
<p>For Claude-3 models, similar familial patterns could be observed.Claude-3-Opus highly agrees with Claude-3.5-Sonnet,probably due to their similar capabilities, while it also highly agrees with Claude-3-Sonnet, likely due to their similar model structure within the same series.Interestingly, Claude-3.5-Sonnetand Claude-3-Sonnet do not exhibit a significantly high agreement, indicating that the upgrade from series 3 to 3.5 considerably impacts their judging capabilities.</p>
<p>Gemini models exhibit rather low mutual agreement and "familial property" is minimal, but the most capable Gemini-1.5-proaligns more closely with other capable models like the GPT-4 series and Claude-3-Opus.</p>
<p>Llama models demonstrate a high agreement among capable family members (Llama-3.3-70Band Llama-405B) and with GPT series.However, significantly smaller and less capable model like Llama-3.1-8B does not strong agree with them.</p>
<p>These patterns suggest that familial similarities, possibly stemming from analogous model sizes, training data, and strategies, influence the positional preferences of these judges.In particular, the LLM judges could be primarily grouped by their capabilities; when judging capabilities are comparable, models within the same family series share a higher mutual agreement than across families.</p>
<p>Identifying such groupings provides valuable insights, as comparisons between judges from different groups, both adept at assessing LLM-generated content, can reveal distinct position biases and enrich our understanding of this phenomenon.</p>
<p>C.2 Disagreement &amp; Benchmark Design Insight</p>
<p>Since the mutual agreement between LLM judges is not perfect and usually a considerable proportion of instances are difficult for them to reach a consensus, disagreement analysis becomes crucial and insightful.Therefore, we define the disagreement of an evaluation instance to be the number of judgments different from the majority.By this definition, an instance with all judges reaching a consensus on the better solution will have a disagreement of 0; in contrast, an instance where judgments are widely varied will result in a high disagreement.</p>
<p>For our study where 15 judges are investigated, the maximum disagreement of an MTBench instance is 10, accounting for the 5{A}-5{B}-5{C} choice pattern by 3-option mode.On the other hand, for DevBench instances, since Gemini-1.5-flash is excluded due to insufficient data (as shown by high error rates in Tabel 2), the maximum possible disagreement for the remaining 14 judges is 7, representing the 7{A}-7{B} judgment distribution for the 2-option mode.</p>
<p>The distributions of instances with different disagreement values on MTBench and DevBench are shown in Fig. 3 and Fig. 4(d), respectively.From our disagreement analysis, at least 75% of the judges reached a choice consensus on more than half of the instances on both benchmarks.These are likely easy-to-evaluate instances, and the reliability of LLM judgments is enhanced by majority voting.</p>
<p>In comparison, the instances with the highest disagreement are likely the ones that are difficult to evaluate and where the position bias is most likely to occur.However, luckily, these instances are rare, occupying only less than 5% for both benchmarks respectively.In other words, majority voting of multiple capable LLM judges could be practically useful for over 95% of evaluation instances on both benchmarks.</p>
<p>Moreover, if we roughly consider the disagreement value of instances as their difficulty for judging, then Fig. 3 and Fig. 4(d) exhibit a balanced distribution of instances with varied difficulty.This is because, except for the instances with the highest disagreement, the numbers of other instances with varied disagreement do not vary significantly, indicating a smoothly increasing difficulty curve across the benchmark datasets.</p>
<p>To summarize, the practical implications of the disagreement analysis are three-fold.First, it helps identify the instances that are difficult or trivial to judge, benefiting benchmark designs to control the difficulty of evaluation by managing the number of these instances across the dataset.Second, it assists in filtering out instances where majority voting of LLM evaluators are likely to offer reliable judgments without direct comparison with humanannotated evaluations, enhancing the scalability of LLM judges especially when human evaluations are costly.In other words, if one-shot judgments from only one LLM judge are not enough reliable, multiple capable LLMs and the majoring voting strategy could be employed to make the evaluation more convincing.Last but not least, disagreement analysis provides a convenient way to make the difficulty variance of instances varied across the dataset tangible.Since the difficulty of an evaluation instance is closely related to the quality gap between the two solutions and hence position bias, the investigation of the instances where most judges particularly disagree with one another could provide more insights and inspiration for future benchmark designs and potential mitigation strategies for position bias.</p>
<p>D More Results of Position Bias and</p>
<p>Answer Quality Gap Measurement</p>
<p>Table 3 presents additional evaluation results for more open-source models, reinforcing our earlier observations.The scores consistently show that position bias varies across judges and tasks, while high repetition stability confirms that such biases are systematic rather than random.These extended results further validate our evaluation findings.</p>
<p>D.1 MTBench</p>
<p>As shown in Fig. 2(c), considering all judges together, a larger answer quality gap generally leads to better position consistency and preference fairness.In this session, we explore whether the discovery is consistent for each individual judge.Same as Section 2.3, we apply the overall win rate to reflect the answer quality gap for visualization.</p>
<p>As shown in Fig. 5 (a), the "parabolic shape" is observed for all individual judges, indicating that the argument "a higher answer quality gap generally results in higher position consistency applies to all models.However, Fig. 5 (b) reveals that preference fairness is more judge-dependent and the impact of the answer quality gap is neglectable for certain judges.For example, while Claude-3-opus and Claude-3-sonnet exhibit conspicuous "parabolic shape", GPT-4 and GPT-3.5 present nearly linear curves.In other words, while the former models align with the general tendency that a larger answer quality gap improves preference fairness, the latter ones preserve fairness regardless of the answer quality gap.This further demonstrates the necessity to investigate preference fairness in addition to consistency when evaluating a judge model's position bias.</p>
<p>D.2 DevBench</p>
<p>This session includes a similar baseline comparison analysis on DevBench as on MTBench.As shown in Fig. 6, position bias is judge-dependent and task-dependent on DevBench as well, as P C and P F vary significantly across judges and tasks.Similarly, although GPT-4 stands as the baseline model with a generally high P C across tasks, certain models achieve comparable or superior performances on certain tasks.For instance, for architecture design evaluations, GPT-4-Turbo, GPT-4o, and Gemini-1.5-proall surpass GPT-4.Gemini-1.5-pro is especially outstanding, also exceeding GPT-4 in uml class evaluations.However, GPT-4 is still the best-performing model on UML sequence evaluations, with only GPT-3.5-Turbo can achieve comparable performance regarding certain detailed metrics (e.g., interaction complexity).These discoveries, aligning with the findings on MTBench, further necessitate the need to consider the tradeoffs between positional consistency and fairness when selecting the optimal judge model for certain tasks.The AIC is given by:</p>
<p>E Variable Selection and Tests
AIC = 2k − 2 log(L), (4)
where L is the likelihood of the model and k is the number of parameters in the model, including the error variance σ 2 .For a linear regression model with independent and identically distributed (iid) errors, N (0, σ 2 ), fitted to n observations, the loglikelihood can be written as:
log(L) = − n 2 log(2π) − n 2 log(σ 2 ) − 1 2σ 2 n i=1 ê2 i , (5)
where êi is the residual for the ith observation, and σ 2 is the variance of the errors.The AIC, in this context, becomes:
AIC = 2k + n log(2π) + n log(σ 2 ) + 1 σ 2 n i=1 ê2 i . (6)
This form of the AIC balances the goodness of fit (as reflected by the residual sum of squares) and model complexity (as represented by k).</p>
<p>The operation of Bidirectional stepwise regression starts with either no predictors (forward selection) or all predictors (backward elimination), where the model iteratively adds or removes variables.Each step evaluates the impact on the AIC score.In forward selection, variables are added one by one, starting from the null model, such that the addition of each variable results in the largest decrease in AIC.In backward elimination, all variables are included in the model initially, and variables are removed one at a time, with the variable whose removal causes the smallest increase in AIC being dropped.</p>
<p>At each iteration, the change in AIC is computed as ∆AIC = AIC new − AIC current , where AIC new</p>
<p>Model MTBench Results</p>
<p>DevBench Results P C P F RS Error P C P F RS Error GPT-OSS-20B 0.79 ± 0.16 -0.04 ± 0.13 0.89 ± 0.17 0.06 0.84 ± 0.16 -0.04 ± 0.14 0.81 ± 0.25 0.08 Gemma-3n-E4B 0.68 ± 0.17 -0.07 ± 0.18 0.90 ± 0.17 0.00 0.41 ± 0.24 0.40 ± 0.35 0.93 ± 0.12 0.08 Llama-4-Scout-17B 0.79 ± 0.16 0.05 ± 0.15 0.95 ± 0.08 0.00 0.12 ± 0.18 0.88 ± 0.18 0.98 ± 0.03 0.00 Mistral-Small-24B 0.72 ± 0.19 -0.02 ± 0.17 0.87 ± 0.20 0.05 0.57 ± 0.18 0.27 ± 0.19 0.82 ± 0.31 0.02 Qwen2.5-72B0.77 ± 0.17 -0.01 ± 0.16 0.94 ± 0.09 0.00 0.81 ± 0.15 0.11 ± 0.16 0.93 ± 0.13 0.00 Qwen2.5-Coder-32B0.74 ± 0.18 0.01 ± 0.17 0.91 ± 0.14 0.00 0.81 ± 0.16 0.09 ± 0.17 0.92 ± 0.13 0.03 Qwen2.5-7B0.55 ± 0.18 -0.21 ± 0.23 0.87 ± 0.22 0.00 0.54 ± 0.21 -0.45 ± 0.21 0.93 ± 0.12 0.00  refers to the AIC after adding or removing a variable, and AIC current is the AIC of the current model.If ∆AIC &lt; 0, the model is improved by the addition or removal of the variable.The process terminates when neither adding nor removing variables results in a lower AIC, signifying that the most parsimonious model, based on AIC, has been reached.</p>
<p>E.2 Test results</p>
<p>We operated bidirectional stepwise regression on both benchmarks individually and together to identify the factors that are significantly contributing to position bias.Specifically, the variables include lengths (input, output, and prompt), answer quality gap, LLM judges, candidate models, and task categories to predict position consistency and preference fairness respectively.Through benchmark testing, we verified that LLM judges, task categories, and the answer quality gap significantly contribute to position bias in terms of both position consistency and preference fairness.These findings align with our empirical results, showing that position bias varies notably by judge and task, with the answer quality gap being a key influencing factor.The extent of this impact is reflected by the magnitude of change in AIC when the given variable is removed.It is worth noting that while task output length remains a significant predictor for P F and P C in both benchmarks, the change in AIC magnitude after removing this variable is very minimal.This is consistent across both benchmarks individually and combined.We therefore conclude that, although position bias is influenced by task output length, this dependency is minimal.</p>
<p>F Experiment Settings</p>
<p>This session specifies more detailed information about the judges, answer-generating models, tasks, and prompt templates used in this study.We choose to evaluate MTBench and DevBench for the following reasons:</p>
<p>(1) all necessary information about the benchmark models, tasks, and questions is publicly available, making modifications convenient (2) they include a wide variety of answer-generating models, tasks, and task questions for a comprehensive evaluation (3) their human evaluations validated the reliability of state-of-the-art judging models (GPT-4 and GPT-4-Turbo) on their evaluation instances, hence model untested by prior work, if reaching high agreement with these validated judges, can be perceived reliable as well.</p>
<p>F.1 Judges, Candidates, and Tasks</p>
<p>Judge In this study, we choose seven GPT, four Claude, and three Gemini models as the judges.</p>
<p>The specific versions for API call are specified as follows: o1-mini-2024-09-12 for o1-mini, gpt-4o-2024-05-13 for GPT-4o, gpt-4-1106-preview for GPT-4-Turbo, gpt-4-0613 for GPT-4, and gpt-3.5-turbo-1106for GPT-3.5-turbo;claude-3-5-sonnet-20240620, claude-3-opus-20240229, claude-3-sonnet-20240229, and claude-3-haiku-20240307 for Claude series.The other model names and versions are as they are.</p>
<p>Model The reference (or baseline) answergenerating models are vicuna-13b-v1.3for MT-Bench and human for DevBench.They are chosen to ensure a baseline quality of responses and an expected widely spread quality gap across evaluations.The other models that are compared to the reference models, namely "Model" in our context, are listed as follows.</p>
<p>• MTBench (30): alpaca-13b, baize-v2-13b, chatglm-6b, claude-instant-v1, claude-v1, dolly-v2-12b, falcon-40b-instruct, fastchat-t5-3b, gpt-3.5-turbo, gpt-4, gpt4all-13b-snoozy, guanaco-33b, guanaco-65b, h2ogpt-oasst-open-llama-13b, koala-13b, llama-13b, mpt-30b-chat, mpt-30b-instruct, mpt-7b-chat, nous-hermes-13b, oasst-sft-4-pythia-12b, oasst-sft-7-llama-30b, palm-2-chat-bison-001, rwkv-4-raven-14b, stablelm-tuned-alpha-7b, tulu-30b, vicuna-33b-v1.3, vicuna-7b-v1.3, wizardlm-13b, wizardlm-30b • DevBench (10): codellama-7b-instruct, codellama-13b-instruct, codellama-34b-instruct, deepseek-coder-1.3b-instruct, deepseek-coder-6.7b-instruct, deepseek-coder-33b-instruct, gpt-3.5-turbo-1106, gpt-4-0125-preview, gpt-4-0613, gpt-4-1106-preview The model names are exactly what MTBench (Zheng et al., 2024b) and DevBench (Li et al., 2024a) used in their studies.That is why for GPTs, DevBench specifies the exact version (e.g., gpt-4-0613) while MTBench doesn't (e.g., gpt-4).In this study, we directly use the provided answers of these models to the task questions to form answer pairs and queries for the LLM judges.</p>
<p>Task For tasks, we also follow the original studies of these two benchmarks, except for DevBench we separate the gerenal metrics into detailed ones and considered them as different tasks.In this sense, our study experiments on the following tasks to provide a comprehensive study on the positon bias of LLM-as-a-Judge:</p>
<p>• MTBench (8): coding, extraction, humanities, math, reasoning, roleplay, stem, and writing.</p>
<p>• Devbench ( 14):</p>
<p>-UML class (4): cohesion_and_decoupling, complexity, practicability, and faithfulness -UML sequence (5): cohe-sion_and_decoupling, interac-tion_complexity, practicability, uni-formity_and_integration, and faithfulness -architecture design (5): conformance, design_and_coding, practicability, unifor-mity_and_integration, and faithfulness</p>
<p>F.2 Prompt Settings</p>
<p>We follow the original prompt settings of MTBench and DevBench in our study of pairwise comparative LLM-as-a-Judge.</p>
<p>Though written differently, these prompts all share same key components:</p>
<p>• A system prompt explaining the judging task and the role the LLM should be playing.</p>
<p>• Emphasized "should" and"shouldn't"s.</p>
<p>• A prompt structure with placeholders for specific questions and model answers</p>
<p>• A specified output format for later judgment extraction</p>
<p>• Chain-of-Thought (Wei et al., 2022) prompts requiring the LLM judge to provide reasons for its judgment</p>
<p>The detailed prompt templates are specified below.</p>
<p>[Figure 1 :
1
Figure1: Overview of our experiment settings: (a) Position bias is observed when LLM judges consistently favor a specific position rather than evaluating the content, with repeated trials ensuring the deviations are not due to random variations.(b) Preference fairness is defined and measured through the distribution of choice pairs to assess the fairness of judgments.(c) The settings are extended from pairwise comparisons to list-wise comparisons, involving evaluations of more than two candidate models.</p>
<p>Figure 2 :
2
Figure 2: Judge performances on MTBench.Fig. (a)(b) are the radar charts for the P C comparison by family, judge, and task.Fig. (c) leverages linear regression to explore the general relationship between P C and P F .Fig.(d) to (f) investigate the impact of answer quality gap on position bias using overall win rates.Fig.(a) to (e) are for pairwise comparative settings, while Fig. (f) are obtained under list-wise evaluations.</p>
<p>Figure 2: Judge performances on MTBench.Fig. (a)(b) are the radar charts for the P C comparison by family, judge, and task.Fig. (c) leverages linear regression to explore the general relationship between P C and P F .Fig.(d) to (f) investigate the impact of answer quality gap on position bias using overall win rates.Fig.(a) to (e) are for pairwise comparative settings, while Fig. (f) are obtained under list-wise evaluations.</p>
<p>Figure 3 :
3
Figure 3: Distribution of disagreement on MTBench.The y-axis indicates the proportion of the dataset where the level of disagreement among LLM judges does not exceed a specific threshold.</p>
<p>Figure 4 :
4
Figure 4: Figures (a) to (c) are mutual agreement heatmap of LLM judges on MTBench and DevBench, where (b) is the agreement computation excluding the tie option {C}.Higher mutual agreement between two LLM judges is marked with brighter color.Figure (d), like Figure 3, is the distribution of disagreement on DevBench.</p>
<p>Figure 5 :
5
Figure 5: Position Consistency and Preference Fairness vs. overall win rate for each judge on MTBench.Figure (a) refers to the relationship investigation of P C and figure (b) for P F .</p>
<p>Figure 6 :
6
Figure 6: Baseline comparisons of position bias of LLM judges across tasks on DevBench.An asterisk marks the statistical significance by Student's t-tests.Figure (a) selects GPT-4 as the baseline, where asterisk demonstrates signficantly better or worse P C of other models compared to it.Figure (b) utilizes an absolute P F baseline of 0 and depicts preference fairness performances of LLM judges across tasks.Similar to findings on MTBench, position bias is significantly judge-dependent and task-dependent on DevBench as well.</p>
<p>Table 1 :
1
Factors influencing position bias.Significant factors, identified via bidirectional stepwise regression, are marked with * and highlighted in red based on empirical findings on both MTBench and DevBench results.Task Input refers to the question itself, while Task Output denotes the candidate model's answers, serving as both Candidate-level and Task-level factors.Prompt includes the full query presented to LLM judges: Task Input, Task Output, and system prompts.</p>
<p>and P C, higher values are preferable.A high RS value is particularly important as a prerequisite for meaningful computations of P C and P F , ensuring the LLM judge's choice patterns are not due to random variations.Fig.2(a)(b)demonstrate that position bias varies by judges and tasks significantly.Fig.2(c)explores the correlation between the metrics P C and P F .Fig.(d) to (f) further investigate the impact of the answer quality gap on position bias.These analyses were conducted by considering all judges together on MTBench.More analyses can be found in Appendix.Sec.D.Through bidirectional stepwise regression, as shown in Table</p>
<p>Table 2 :
2
Evaluation results for Repetition Stability (RS), Position Consistency (P C), and Preference Fairness (P F ) are presented for both pairwise and list-wise evaluation approaches, with the top 5 performances marked in bold.Errors arise from judgment failures (e.g., exceeding context window, not following output format).High error rates and low RS are marked red, rendering further evaluations invalid due to insufficient data.List-wise evaluation is conducted on four representative judges to validate scalability.</p>
<p>Table 3 :
3
More evaluation results for Position Consistency (P C), Preference Fairness (P F ), and Repetition Stability (RS) across MTBench and DevBench datasets.</p>
<p>Table 6
6
, 7 records the results of final step in stepwise regression for pre-dicting P C and P F , respectively.Table8, 9 serves for DevBench, and Table4, 5 is conducted on the integrated set of both benchmarks.The impact of variables on the model is ranked from highest to lowest, from bottom to top.Removed variables listed as None indicate the full model at this given step.</p>
<p>Table 5 :
5
Final results of stepwise model selection for both benchmarks: Preference Fairness
Removed Variables DF Sum of SqRSSAICNone61.974 -13312Task output length10.0553 62.029 -13311Candidate 291.6474 63.621 -13282Task71.5304 63.504 -13244Judge 1315.3637 77.338 -12594Quality gap115.6206 77.594 -12559</p>
<p>Table 6 :
6
Final results of stepwise model selection for MTBench: Position Consistency
Removed Variables DF Sum of SqRSSAICNone129.00 -10909.2Quality gap11.931 130.93 -10861.3Task79.295 138.29 -10689.4Judge 1358.847 187.85 -9672.5</p>
<p>Table 7 :
7
Final results of stepwise model selection for MTBench: Preference Fairness</p>
<p>Table 8 :
8
Final results of stepwise model selection for DevBench: Position Consistency
Removed Variables DF Sum of SqRSSAICNone55.382 -6940.2Task output length10.257 55.638 -6933.2Candidate91.514 56.896 -6905.4Quality gap113.128 68.510 -6525.3Judge 1384.760 140.141 -5146.6Removed Variables DF Sum of SqRSSAICNone60.104 -6753.9Task output length10.061 60.165 -6753.9Candidate90.731 60.834 -6748.2Task 131.305 61.408 -6737.8Quality gap11.783 61.886 -6698.6Judge 1380.875 140.979 -5108.9</p>
<p>Table 9 :
9
Final results of stepwise model selection for DevBench: Preference Fairness</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024</p>
<p>Chateval: Towards better llm-based evaluators through multi-agent debate. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.072012023Preprint</p>
<p>Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with visionlanguage benchmark. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, Lichao Sun, arXiv:2402.047882024Preprint</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023b</p>
<p>Bias-augmented consistency training reduces biased reasoning in chain-of-thought. James Chua, Edward Rees, Hunar Batra, R Samuel, Julian Bowman, Ethan Michael, Miles Perez, Turpin, arXiv:2403.055182024Preprint</p>
<p>Bias and fairness in large language models: A survey. Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K Ahmed, Computational Linguistics. 2024</p>
<p>Gemini: A family of highly capable multimodal models. Google Gemini, Team , arXiv:2312.118052024Preprint</p>
<p>Yufei Guo, Muzhe Guo, Juntao Su, Zhou Yang, Mengqiu Zhu, Hongfei Li, Mengyang Qiu, Shuo Shuo Liu, arXiv:2411.10915Bias in large language models: Origin, evaluation, and mitigation. 2024arXiv preprint</p>
<p>Large language models are zero-shot rankers for recommender systems. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian Mcauley, Wayne Xin Zhao, European Conference on Information Retrieval. Springer2024</p>
<p>The perils of using mechanical turk to evaluate open-ended text generation. Marzena Karpinska, Nader Akoury, Mohit Iyyer, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Debating with more persuasive llms leads to more truthful answers. Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Tim Samuel R Bowman, Ethan Rocktäschel, Perez, arXiv:2402.067822024arXiv preprint</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023</p>
<p>Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, arXiv:2403.08604Devbench: A comprehensive benchmark for software development. 2024aarXiv preprint</p>
<p>Generative judge for evaluating alignment. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Pengfei Liu, The Twelfth International Conference on Learning Representations. 2023a</p>
<p>Ruosen Li, arXiv:2307.02762Teerth Patel, and Xinya Du. 2023b. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint</p>
<p>Leveraging llms as meta-judges: A multi-agent framework for evaluating llm judgments. Yuran Li, Jama Hussein Mohamud, Chongren Sun, Di Wu, Benoit Boulet, arXiv:2504.170872025Preprint</p>
<p>Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu, arXiv:2310.01432Split and merge: Aligning position biases in large language model based evaluators. 2023carXiv preprint</p>
<p>Split and merge: Aligning position biases in llmbased evaluators. Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024b</p>
<p>Llm comparative assessment: Zero-shot nlg evaluation through pairwise comparisons using large language models. Adian Liusie, Potsawee Manakul, Mark Gales, Proceedings of the 18th Conference of the European Chapter. the 18th Conference of the European Chapterthe Association for Computational Linguistics20241Long Papers)</p>
<p>Judging with many minds: Do more perspectives mean less prejudice?. Chiyu Ma, Enpei Zhang, Yilun Zhao, Wenjun Liu, Yaning Jia, Peijun Qing, Lin Shi, Arman Cohan, Yujun Yan, Soroush Vosoughi, arXiv:2505.194772025arXiv preprint</p>
<p>Likelihood-based mitigation of evaluation bias in large language models. Masanari Ohi, Masahiro Kaneko, Ryuto Koike, Mengsay Loem, Naoaki Okazaki, arXiv:2402.159872024arXiv preprint</p>
<p>Openai, arXiv:2303.087742024. Gpt-4 technical report. 2023Preprint</p>
<p>Large language models are effective text rankers with pairwise ranking prompting. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, Michael Bendersky, arXiv:2306.175632024Preprint</p>
<p>Is llm-as-a-judge robust? investigating universal adversarial attacks on zero-shot llm assessment. Adian Vyas Raina, Mark Liusie, Gales, arXiv:2402.140162024Preprint</p>
<p>Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Preprint</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.179262023Preprint</p>
<p>Eliminating position bias of language models: A mechanistic approach. Ziqi Wang, Hanlin Zhang, Xiner Li, Kuan-Hao Huang, Chi Han, Shuiwang Ji, M Sham, Hao Kakade, Heng Peng, Ji, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Meta-rewarding language models: Self-improving alignment with llm-as-ameta-judge. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar, arXiv:2407.195942024Preprint</p>
<p>Justice or prejudice? quantifying biases in llm-as-a-judge. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh V Chawla, Xiangliang Zhang, arXiv:2410.027362024Preprint</p>
<p>Mitigate position bias in large language models via scaling a single dimension. Yijiong Yu, Huiqiang Jiang, Xufang Luo, Qianhui Wu, Chin-Yew Lin, Dongsheng Li, Yuqing Yang, Yongfeng Huang, Lili Qiu, 2025</p>
<p>Evaluating large language models at evaluating instruction following. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Large language models are not robust multiple choice selectors. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang, arXiv:2309.038822024aPreprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 2024b36</p>
<p>Starling-7b: Improving helpfulness and harmlessness with RLAIF. Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Karthik Ganesan, Wei-Lin Chiang, Jian Zhang, Jiantao Jiao, First Conference on Language Modeling. 2024</p>
<p>Judgelm: Fine-tuned large language models are scalable judges. Lianghui Zhu, Xinggang Wang, Xinlong Wang, arXiv:2310.176312023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
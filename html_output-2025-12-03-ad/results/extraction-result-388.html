<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-388 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-388</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-388</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-740063</p>
                <p><strong>Paper Title:</strong> A Survey on Transfer Learning</p>
                <p><strong>Paper Abstract:</strong> A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e388.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e388.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TrAdaBoost</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TrAdaBoost (Transfer AdaBoost)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of AdaBoost that iteratively reweights source-domain instances to emphasize source examples helpful for the target task while down-weighting harmful source examples, enabling inductive transfer when feature spaces match but distributions differ.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Boosting for Transfer Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>TrAdaBoost (instance reweighting boosting)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>TrAdaBoost trains a base classifier on a weighted mixture of source and target data. At each boosting round it updates weights for target examples using the standard AdaBoost rule (increasing weight of misclassified target examples) but applies a different update for source examples so that 'bad' source examples are progressively down-weighted and 'good' source examples are emphasized. The final classifier is an ensemble whose votes emphasize models that perform well on target examples.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / algorithm (instance reweighting / ensemble learning)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning (boosting / supervised classification)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>related supervised classification tasks with distribution shift between domains (e.g., text, web documents)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Modified the weight update scheme of AdaBoost: retained AdaBoost updates for target instances but introduced a different, decreasing-weight rule for source instances so that harmful source examples are suppressed across iterations; explicitly separates contributions of source vs. target during boosting.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — reported in cited experiments to improve target performance compared to non-transfer baselines when source and target are related but distributions differ; no absolute numeric metric is reported in this survey (results summarized from original evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Risk of negative transfer if source and target tasks/domains are too dissimilar; requirement that feature spaces and label spaces align (assumes same features/labels), sensitivity to proportion of target labeled examples.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of at least some labeled target examples to guide error calculation and weight updates; shared feature/label space between source and target; ensemble (boosting) framework that naturally accommodates reweighting.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires that source and target share the same feature representation and label set; needs a base learner compatible with boosting; labeled target examples must exist (inductive transfer setting).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderately generalizable across classification problems that meet assumptions (shared features/labels); performance depends on relatedness — not applicable when feature spaces differ without additional mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (algorithmic/technical knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e388.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e388.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instance reweighting / KMM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kernel Mean Matching (KMM) for instance reweighting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>KMM directly estimates instance importance weights by matching means of source and target distributions in an RKHS, avoiding explicit density estimation; used to adapt models under covariate shift/sample selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Correcting Sample Selection Bias by Unlabeled Data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Kernel Mean Matching (KMM) / instance importance weighting</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>KMM formulates a quadratic program to find non-negative weights for source instances so that the weighted mean of source data in a reproducing kernel Hilbert space matches the (unweighted) mean of available target (unlabeled) data; the resulting weights approximate the density ratio P_T(x)/P_S(x) and are used to reweight the loss in empirical risk minimization to correct covariate shift.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data analysis technique (importance weighting / density-ratio estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>statistical learning theory / kernel methods (machine learning)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>domain adaptation / transfer learning tasks across domains with differing input distributions (e.g., text, vision, WiFi localization)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (applied to domain adaptation problems)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied RKHS mean-matching objective to estimate instance weights rather than performing explicit density estimation; constrained optimization (QP) with box constraints and sum constraint to stabilize weights for use in reweighted ERM.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful in cited works for correcting sample selection bias and improving target performance when covariate shift assumptions hold; survey notes KMM's avoidance of direct density estimation as an advantage but does not provide quantitative metrics (referenced experimental improvements exist in original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Computational cost (solving QP), need for representative unlabeled target samples to estimate target mean; performance degrades if covariate-shift assumption (P(Y|X) invariant) is violated.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Shared feature representation between domains, availability of unlabeled target data, kernel choice enabling representation matching in RKHS.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to unlabeled target data; choice of kernel and regularization bounds; assumption that label conditional distributions are similar across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Generalizable to many transfer problems under covariate shift with same features; less applicable when feature spaces differ or when P(Y|X) differs across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit procedural steps (computational technique)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e388.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e388.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KLIEP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kullback-Leibler Importance Estimation Procedure (KLIEP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A direct density-ratio estimation method that estimates the importance weights P_T(x)/P_S(x) by minimizing the Kullback-Leibler divergence between the weighted source distribution and the target distribution, enabling importance weighting without separate density estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Direct Importance Estimation with Model Selection and its Application to Covariate Shift Adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>KLIEP (direct importance-ratio estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>KLIEP models the density ratio as a linear combination of basis functions and selects coefficients by minimizing the KL divergence from the target distribution to the weighted source distribution; coefficients are constrained to ensure nonnegativity and normalization, and model selection (e.g., via cross-validation) is used to choose hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data analysis technique (density-ratio estimation / importance weighting)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>statistical machine learning (density estimation / covariate shift correction)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>domain adaptation / transfer learning under covariate shift across datasets or tasks</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (applied to domain adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Direct estimation of density ratio rather than separate density estimation; integration with cross-validation for model selection in transfer-learning pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>reported as effective in referenced literature for covariate shift adaptation; survey notes KLIEP as an alternative to KMM that directly produces weights and can be integrated with cross-validation, but provides no numeric outcomes in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires representative samples from source and target distributions; choice of basis functions and regularization affects stability; may struggle with high-dimensional data without dimensionality reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of unlabeled target examples; ability to select/validate basis functions via cross-validation; theoretical grounding in KL divergence.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Sufficient unlabeled target data, computational resources for model selection, and appropriate basis function design.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Applicable broadly for covariate-shift scenarios where direct density-ratio estimation is preferable; less suited when feature spaces differ fundamentally.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and theoretical principles</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e388.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e388.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Correspondence Learning (SCL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised feature-representation transfer method for domain adaptation that identifies 'pivot' features common to source and target, learns linear predictors for these pivots on unlabeled data, and uses singular value decomposition to extract shared feature mappings that reduce domain divergence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain Adaptation with Structural Correspondence Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Structural Correspondence Learning (SCL)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>SCL selects pivot features (domain-agnostic, informative features), treats each pivot as a pseudo-label and trains linear classifiers predicting the pivot from the original features on unlabeled data from both domains, stacks the learned weight vectors W and computes an SVD to derive a low-dimensional linear mapping capturing correspondences; the mapping augments original features for downstream supervised learning on the target.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / feature-representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>natural language processing / text classification (domain adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>target-domain NLP tasks (e.g., sentiment classification on different product domains) or other cross-domain classification tasks where pivot features can be defined</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Use of pivot features as surrogate labels; learning multiple auxiliary prediction tasks followed by SVD to extract shared structure; later extension (SCL-MI) uses mutual information to select pivots rather than heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful in cited NLP experiments (survey cites improvements in cross-domain sentiment and tagging tasks); performance depends on pivot selection quality; survey notes SCL reduces domain difference experimentally but pivot selection is domain-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Selecting good pivot features is difficult and domain-dependent; assumes existence of meaningful pivots shared across domains; linearity assumption of auxiliary classifiers may limit representational power.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of abundant unlabeled data in both domains; presence of features that reliably correlate with labels across domains (pivots); SVD yields compact shared representation.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Large unlabeled corpora from both domains; domain expertise or statistics to select pivots (or mutual information computation); downstream supervised labeled target data for final classifier training.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Well-suited to NLP/text domain adaptation; conceptually extendable to other domains if pivot-like shared signals exist, but practicality depends on pivot identification.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and interpretive framework (feature correspondence mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e388.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e388.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCL-MI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCL with Mutual Information (SCL-MI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of SCL that selects pivot features using mutual information with source labels rather than heuristic selection, improving the relevance of pivots for domain adaptation in NLP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Biographies, Bollywood, Boom-Boxes and Blenders: Domain Adaptation for Sentiment Classification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>SCL-MI (pivot selection via mutual information)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>SCL-MI computes mutual information between candidate features and source labels to select pivot features that are highly predictive in the source; it then follows SCL's auxiliary task and SVD pipeline to derive shared feature mappings used to augment features for target classification.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / feature-representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>NLP / sentiment classification</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>cross-domain sentiment classification and other NLP adaptation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Replaced heuristic pivot selection with an MI-based ranking to choose features most informative of source labels, aiming to improve pivot relevance and transfer effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>reported to outperform heuristic pivot selection in cited work's sentiment classification experiments (survey summarizes comparative improvements but gives no numeric specifics here).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires accurate MI estimation which needs sufficient labeled source data; may still be sensitive to domain mismatch if MI-selected pivots are not predictive in target.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of labeled source data to compute MI; abundant unlabeled data for training auxiliary predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Sizable labeled source set for reliable MI estimates; computational resources for MI calculation and auxiliary training.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Primarily applicable in text/NLP where MI can be computed for features; concept may transfer to other modalities where analogous informative shared features exist.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and selection heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e388.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e388.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-taught / Sparse coding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Taught Learning using Sparse Coding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised feature-representation transfer approach where higher-level basis vectors (dictionary) are learned from abundant unlabeled source data via sparse coding, then target data are encoded with that dictionary and supervised learning is applied on the new representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Taught Learning: Transfer Learning from Unlabeled Data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Self-taught learning via sparse coding</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>First, learn a dictionary of basis vectors from large unlabeled source data by solving a sparse coding optimization that balances reconstruction error and sparsity; second, encode target-domain examples using the learned dictionary to obtain higher-level features, then train discriminative classifiers on the (few) labeled target examples in the new feature space.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / unsupervised feature learning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>unsupervised representation learning (sparse coding) often applied in vision and signal processing</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>supervised classification tasks in domains with scarce labels (e.g., image classification, text) where unlabeled source data are abundant</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (unsupervised->supervised transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Dictionary (basis) learned purely on source unlabeled data is reused as a fixed transform for target; target encoding step solves a sparse coding subproblem per target instance; assumes learned bases capture transferable high-level structure.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — cited as improving target classification in multiple experiments when learned bases capture transferable structure; survey notes potential drawback that source-learned bases may not suit target domain if domains are too dissimilar.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Mismatch between source-learned bases and target data statistics leading to limited transfer; choice of dictionary size and sparsity regularization critical; computational cost for coding target instances.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Large unlabeled source datasets to learn robust bases; target tasks that share underlying structure with source (e.g., similar visual primitives or language constructs).</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Efficient sparse coding solver; ability to choose regularizers and dictionary size; some labeled target examples to train classifier on new features.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>General framework applicable across modalities (vision, text, audio) where unsupervised bases capture transferable structure; success depends on domain relatedness.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural/technical (explicit algorithmic steps and representational knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e388.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e388.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMDE / TCA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Mean Discrepancy Embedding (MMDE) and Transfer Component Analysis (TCA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Dimensionality-reduction-based transfer methods that learn a latent subspace minimizing distribution discrepancy (MMD) between source and target; TCA is an efficient implementation to overcome MMDE computational costs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transfer Learning via Dimensionality Reduction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>MMDE (Maximum Mean Discrepancy Embedding) and TCA (Transfer Component Analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>MMDE optimizes an embedding that reduces the Maximum Mean Discrepancy (MMD) between source and target distributions while preserving data structure for downstream tasks. TCA approximates this objective with kernel-based feature extraction to find transfer components (a low-dimensional subspace) where source and target marginal distributions are closer; subsequent classifiers trained in this subspace generalize better to target.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / dimensionality reduction for domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>statistical learning / kernel methods / dimensionality reduction</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>domain adaptation problems across fields (text, WiFi localization, vision) needing distribution alignment</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (applied to domain adaptation and made computationally efficient)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Derived an objective based on MMD for embedding; TCA implements an efficient kernelized solution to the MMDE objective, trading off computational burden and embedding quality.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>reported successful in referenced experiments (survey cites TCA improving WiFi localization and other tasks compared to non-transfer baselines and kernel PCA); no absolute metrics provided here but original papers show empirical gains.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Computational cost for MMDE on large datasets (mitigated by TCA); choice of kernel and subspace dimensionality influences success; assumes same feature space across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Theoretical alignment measure (MMD) guiding the embedding; availability of unlabeled target data to estimate target marginal; kernel machinery enabling flexible embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to source and target data (unlabeled target accepted), kernel selection, computational resources for eigen-decomposition or kernel operations.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Broadly generalizable to problems with shared feature space and covariate shift; less applicable when features are heterogeneous without mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit algorithmic procedure</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e388.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e388.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAMAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAMAR (Transfer via Mapping and Revision for MLNs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage relational transfer algorithm that maps a Markov Logic Network learned in a source relational domain to a target domain using weighted pseudo log-likelihood, then revises the mapped structure via inductive logic programming (FORTE) to fit target data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mapping and Revising Markov Logic Networks for Transfer Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>TAMAR (MLN mapping and revision)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Stage 1: Construct a mapping from source MLN predicates/structures to target-domain predicates by scoring candidate mappings with a weighted pseudo log-likelihood (WPLL) measure. Stage 2: Apply FORTE, an ILP-based theory revision algorithm, to revise the mapped MLN in the target domain to better explain target data. The revised MLN serves as the relational model for inference in the target.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / relational knowledge-transfer / model mapping and revision</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>statistical relational learning (Markov Logic Networks) applied in source relational datasets</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>different relational domains with analogous roles/relations (e.g., academic to industrial organizational networks)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (structured mapping plus revision)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Explicit construction of cross-domain predicate/entity mappings using WPLL; incorporation of ILP-based revision to adapt structure to target-specific relations and data; two-stage pipeline instead of direct reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>reported effective in cited works for transferring relational knowledge across domains when mapping between analogous entities exists; survey summarizes the conceptual success but does not provide numeric outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Need to identify plausible entity/predicate mappings (may be nontrivial); domains must share relational analogies; computational complexity in structure search and revision.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Presence of analogous relational roles across domains (e.g., professor↔manager); expressive MLN formalism enabling mapping; ILP revision to adapt mapped structures.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to a learned source MLN and target relational data; ability to propose candidate mappings; ILP/revision tooling (FORTE) and compute resources.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Applicable to relational domains with structural analogies; less useful for non-relational or widely divergent relational structures.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>interpretive frameworks + explicit procedural steps (mapping heuristics and revision algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e388.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e388.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WiFi localization transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer of WiFi-based Indoor Localization Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of transfer learning methods to adapt WiFi signal–based localization models across time, space, and device changes to reduce costly recalibration, using techniques like adaptive temporal radio maps, multi-view learning and latent multi-task learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transfer Learning for WiFi-Based Indoor Localization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Transfer of WiFi localization models (adaptive radio maps / multi-view / latent multi-task)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Various works transfer localization models by (a) adapting radio maps over time (updating models with limited new labels), (b) multi-view learning to combine different sensor/device views, and (c) latent multi-task learning to share latent representations across devices/locations; the goal is to reuse source calibration data to predict location under changed conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>experimental protocol / applied computational method (model adaptation in sensor networks)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>sensor-network localization and supervised regression models for WiFi fingerprinting</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>new time periods, spatial areas, or different mobile devices with shifted WiFi signal distributions</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (practical model adaptation and recalibration reduction)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Incorporation of temporal adaptation (updating or reweighting samples), multi-view fusion of device-specific features, and latent task models to share structure across devices/periods; algorithms designed to tolerate nonstationary signal strengths and device heterogeneity.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>reported as beneficial in multiple cited studies—survey notes significant reduction in relabeling/calibration effort and improved localization compared to naive retraining, though specific metrics are reported in cited papers rather than in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>WiFi signal variability over time, device heterogeneity, nonstationarity and environmental changes leading to distribution shift; limited labeled data in target periods/devices.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Physical continuity of environment (some stable signal patterns), ability to collect some unlabeled or limited labeled target samples, multi-device data availability to learn shared latent structures.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to source calibration datasets, limited labeled target examples or abundant unlabeled target measurements, domain knowledge about signal dynamics, computational frameworks for model adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Highly applicable across indoor localization setups; techniques tailored to sensor-data modalities and may transfer to other sensor adaptation problems with similar nonstationarity.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural/technical and tacit practical know-how (how to adapt calibration procedures and modeling choices)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e388.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e388.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Translated Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Translated Learning (feature-space translation across heterogeneous domains)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach to transfer between entirely different feature spaces by learning mapping functions that translate features from one domain (e.g., images) into another (e.g., text), enabling cross-modal transfer learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translated Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Translated Learning (cross-feature-space mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Learn a mapping function (translation) between heterogeneous feature spaces so that models or knowledge in the source space can be applied in the target space; mappings may be learned from paired data or via bridging latent spaces that align semantics across modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / cross-modal mapping and transfer</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computer vision or textual feature spaces (e.g., image descriptors)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>different modality feature spaces (e.g., textual descriptors), enabling transfer between images and text or other heterogeneous domains</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>analogical transfer with significant changes (requires mapping functions)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Construction of explicit translation/mapping functions (possibly learned from paired examples or via shared latent spaces) to bridge disparate feature sets; changes include designing loss/objectives to preserve semantics across spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>presented as a new direction with promising outcomes in cited work; survey states translated learning enables transfer across entirely different feature spaces but does not report quantitative results here.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires paired or bridging data to learn mappings; semantic gap between modalities, difficulties in aligning heterogeneous features; risk of poor generalization if mapping is weak.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of cross-modal correspondences or latent shared semantics; flexible mapping models (e.g., latent-factor models) and sufficient training data for mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Paired data or auxiliary bridging datasets, model capacity to learn cross-space correspondences, careful objective formulation to preserve task-relevant structure.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Potentially general across heterogeneous transfer tasks (images↔text, audio↔text), but heavily dependent on quality of mapping data and semantic alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and representational/theoretical mapping knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e388.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e388.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMGM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rating-Matrix Generative Model (RMGM) for collaborative filtering transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent-mixture model that learns shared latent user and item clusters across rating matrices from different domains to transfer collaborative-filtering knowledge and address cold-start/sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transfer Learning for Collaborative Filtering via a Rating-Matrix Generative Model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>RMGM (shared latent user-item spaces for cross-domain collaborative filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>RMGM models each rating matrix via latent user and item cluster variables and learns a mixture model where users/items from different domains are mapped to shared latent clusters; the shared latent spaces enable transfer of rating patterns from auxiliary domains (e.g., books) to a target domain (e.g., movies).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / probabilistic generative model for transfer</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>recommender systems / collaborative filtering in one domain (e.g., books)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>collaborative filtering in another domain (e.g., movies) suffering from sparsity or cold-start</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>hybrid approach combining with existing methods (probabilistic latent modeling across datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Introduced shared latent user/item cluster variables and mapping procedures to align multiple rating matrices; uses generative assumptions to enable knowledge sharing and reconstruct target matrix via expanded codebook.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>reported to mitigate cold-start and sparsity in cited works; survey notes RMGM bridges rating matrices effectively when domains are related, but concrete metrics are in original papers.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Assumes latent cluster compatibility across domains; model misspecification or weak cross-domain alignment can limit benefits; requires sufficient auxiliary domain data.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Similarity in user/item behaviors across domains, abundant auxiliary ratings, and probabilistic modeling enabling shared latent structure.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Availability of one or more auxiliary rating matrices, EM/approximate inference tooling, and assumptions that latent clusters are meaningful across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Applicable to multi-domain recommender scenarios where latent patterns are shared; less effective when user/item semantics differ greatly between domains.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit probabilistic modeling steps</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e388.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e388.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Daumé's feature augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frustratingly Easy Domain Adaptation (feature augmentation / kernel mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple domain adaptation trick that augments original features with domain-specific and shared copies (or uses kernel mappings driven by domain knowledge) to allow discriminative learners to learn shared and domain-specific weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Frustratingly Easy Domain Adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Feature augmentation / kernel mapping for domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Construct augmented feature vectors by concatenating (1) domain-common features, (2) source-specific copies, and (3) target-specific copies (or apply a domain-aware kernel mapping), enabling standard discriminative classifiers to learn parameters that capture shared vs. domain-specific effects without complex modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / feature-engineering-based domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>NLP / discriminative classification</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>cross-domain NLP tasks and potentially other classification tasks with domain shifts</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with simple modification (feature augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Augmented original feature vectors to explicitly represent domain-common and domain-specific components; optionally implement via kernel mapping tailored to NLP features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>reported as an effective, easy-to-implement baseline that yields strong empirical performance in many domain-adaptation tasks; survey notes mapping is domain-knowledge driven and may not generalize trivially.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Relies on domain knowledge to construct useful mappings; may increase dimensionality substantially; limited when domain disparity is complex.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Simplicity of implementation, compatibility with off-the-shelf classifiers, and presence of shared signal across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Ability to augment feature representation and sufficient labeled data to train augmented-feature classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>A general baseline applicable across many domains; kernel mapping variants may need domain-specific design.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and pragmatic engineering heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Boosting for Transfer Learning <em>(Rating: 2)</em></li>
                <li>Correcting Sample Selection Bias by Unlabeled Data <em>(Rating: 2)</em></li>
                <li>Direct Importance Estimation with Model Selection and its Application to Covariate Shift Adaptation <em>(Rating: 2)</em></li>
                <li>Domain Adaptation with Structural Correspondence Learning <em>(Rating: 2)</em></li>
                <li>Biographies, Bollywood, Boom-Boxes and Blenders: Domain Adaptation for Sentiment Classification <em>(Rating: 2)</em></li>
                <li>Self-Taught Learning: Transfer Learning from Unlabeled Data <em>(Rating: 2)</em></li>
                <li>Transfer Learning via Dimensionality Reduction <em>(Rating: 2)</em></li>
                <li>Domain Adaptation via Transfer Component Analysis <em>(Rating: 2)</em></li>
                <li>Mapping and Revising Markov Logic Networks for Transfer Learning <em>(Rating: 2)</em></li>
                <li>Transfer Learning for WiFi-Based Indoor Localization <em>(Rating: 2)</em></li>
                <li>Translated Learning <em>(Rating: 2)</em></li>
                <li>Transfer Learning for Collaborative Filtering via a Rating-Matrix Generative Model <em>(Rating: 2)</em></li>
                <li>Frustratingly Easy Domain Adaptation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-388",
    "paper_id": "paper-740063",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "TrAdaBoost",
            "name_full": "TrAdaBoost (Transfer AdaBoost)",
            "brief_description": "An extension of AdaBoost that iteratively reweights source-domain instances to emphasize source examples helpful for the target task while down-weighting harmful source examples, enabling inductive transfer when feature spaces match but distributions differ.",
            "citation_title": "Boosting for Transfer Learning",
            "mention_or_use": "mention",
            "procedure_name": "TrAdaBoost (instance reweighting boosting)",
            "procedure_description": "TrAdaBoost trains a base classifier on a weighted mixture of source and target data. At each boosting round it updates weights for target examples using the standard AdaBoost rule (increasing weight of misclassified target examples) but applies a different update for source examples so that 'bad' source examples are progressively down-weighted and 'good' source examples are emphasized. The final classifier is an ensemble whose votes emphasize models that perform well on target examples.",
            "procedure_type": "computational method / algorithm (instance reweighting / ensemble learning)",
            "source_domain": "machine learning (boosting / supervised classification)",
            "target_domain": "related supervised classification tasks with distribution shift between domains (e.g., text, web documents)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Modified the weight update scheme of AdaBoost: retained AdaBoost updates for target instances but introduced a different, decreasing-weight rule for source instances so that harmful source examples are suppressed across iterations; explicitly separates contributions of source vs. target during boosting.",
            "transfer_success": "partially successful — reported in cited experiments to improve target performance compared to non-transfer baselines when source and target are related but distributions differ; no absolute numeric metric is reported in this survey (results summarized from original evaluations).",
            "barriers_encountered": "Risk of negative transfer if source and target tasks/domains are too dissimilar; requirement that feature spaces and label spaces align (assumes same features/labels), sensitivity to proportion of target labeled examples.",
            "facilitating_factors": "Availability of at least some labeled target examples to guide error calculation and weight updates; shared feature/label space between source and target; ensemble (boosting) framework that naturally accommodates reweighting.",
            "contextual_requirements": "Requires that source and target share the same feature representation and label set; needs a base learner compatible with boosting; labeled target examples must exist (inductive transfer setting).",
            "generalizability": "Moderately generalizable across classification problems that meet assumptions (shared features/labels); performance depends on relatedness — not applicable when feature spaces differ without additional mapping.",
            "knowledge_type": "explicit procedural steps (algorithmic/technical knowledge)",
            "uuid": "e388.0",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "Instance reweighting / KMM",
            "name_full": "Kernel Mean Matching (KMM) for instance reweighting",
            "brief_description": "KMM directly estimates instance importance weights by matching means of source and target distributions in an RKHS, avoiding explicit density estimation; used to adapt models under covariate shift/sample selection bias.",
            "citation_title": "Correcting Sample Selection Bias by Unlabeled Data",
            "mention_or_use": "mention",
            "procedure_name": "Kernel Mean Matching (KMM) / instance importance weighting",
            "procedure_description": "KMM formulates a quadratic program to find non-negative weights for source instances so that the weighted mean of source data in a reproducing kernel Hilbert space matches the (unweighted) mean of available target (unlabeled) data; the resulting weights approximate the density ratio P_T(x)/P_S(x) and are used to reweight the loss in empirical risk minimization to correct covariate shift.",
            "procedure_type": "computational method / data analysis technique (importance weighting / density-ratio estimation)",
            "source_domain": "statistical learning theory / kernel methods (machine learning)",
            "target_domain": "domain adaptation / transfer learning tasks across domains with differing input distributions (e.g., text, vision, WiFi localization)",
            "transfer_type": "adapted/modified for new context (applied to domain adaptation problems)",
            "modifications_made": "Applied RKHS mean-matching objective to estimate instance weights rather than performing explicit density estimation; constrained optimization (QP) with box constraints and sum constraint to stabilize weights for use in reweighted ERM.",
            "transfer_success": "successful in cited works for correcting sample selection bias and improving target performance when covariate shift assumptions hold; survey notes KMM's avoidance of direct density estimation as an advantage but does not provide quantitative metrics (referenced experimental improvements exist in original paper).",
            "barriers_encountered": "Computational cost (solving QP), need for representative unlabeled target samples to estimate target mean; performance degrades if covariate-shift assumption (P(Y|X) invariant) is violated.",
            "facilitating_factors": "Shared feature representation between domains, availability of unlabeled target data, kernel choice enabling representation matching in RKHS.",
            "contextual_requirements": "Access to unlabeled target data; choice of kernel and regularization bounds; assumption that label conditional distributions are similar across domains.",
            "generalizability": "Generalizable to many transfer problems under covariate shift with same features; less applicable when feature spaces differ or when P(Y|X) differs across domains.",
            "knowledge_type": "theoretical principles and explicit procedural steps (computational technique)",
            "uuid": "e388.1",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "KLIEP",
            "name_full": "Kullback-Leibler Importance Estimation Procedure (KLIEP)",
            "brief_description": "A direct density-ratio estimation method that estimates the importance weights P_T(x)/P_S(x) by minimizing the Kullback-Leibler divergence between the weighted source distribution and the target distribution, enabling importance weighting without separate density estimates.",
            "citation_title": "Direct Importance Estimation with Model Selection and its Application to Covariate Shift Adaptation",
            "mention_or_use": "mention",
            "procedure_name": "KLIEP (direct importance-ratio estimation)",
            "procedure_description": "KLIEP models the density ratio as a linear combination of basis functions and selects coefficients by minimizing the KL divergence from the target distribution to the weighted source distribution; coefficients are constrained to ensure nonnegativity and normalization, and model selection (e.g., via cross-validation) is used to choose hyperparameters.",
            "procedure_type": "computational method / data analysis technique (density-ratio estimation / importance weighting)",
            "source_domain": "statistical machine learning (density estimation / covariate shift correction)",
            "target_domain": "domain adaptation / transfer learning under covariate shift across datasets or tasks",
            "transfer_type": "adapted/modified for new context (applied to domain adaptation)",
            "modifications_made": "Direct estimation of density ratio rather than separate density estimation; integration with cross-validation for model selection in transfer-learning pipelines.",
            "transfer_success": "reported as effective in referenced literature for covariate shift adaptation; survey notes KLIEP as an alternative to KMM that directly produces weights and can be integrated with cross-validation, but provides no numeric outcomes in this survey.",
            "barriers_encountered": "Requires representative samples from source and target distributions; choice of basis functions and regularization affects stability; may struggle with high-dimensional data without dimensionality reduction.",
            "facilitating_factors": "Availability of unlabeled target examples; ability to select/validate basis functions via cross-validation; theoretical grounding in KL divergence.",
            "contextual_requirements": "Sufficient unlabeled target data, computational resources for model selection, and appropriate basis function design.",
            "generalizability": "Applicable broadly for covariate-shift scenarios where direct density-ratio estimation is preferable; less suited when feature spaces differ fundamentally.",
            "knowledge_type": "explicit procedural steps and theoretical principles",
            "uuid": "e388.2",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "SCL",
            "name_full": "Structural Correspondence Learning (SCL)",
            "brief_description": "An unsupervised feature-representation transfer method for domain adaptation that identifies 'pivot' features common to source and target, learns linear predictors for these pivots on unlabeled data, and uses singular value decomposition to extract shared feature mappings that reduce domain divergence.",
            "citation_title": "Domain Adaptation with Structural Correspondence Learning",
            "mention_or_use": "mention",
            "procedure_name": "Structural Correspondence Learning (SCL)",
            "procedure_description": "SCL selects pivot features (domain-agnostic, informative features), treats each pivot as a pseudo-label and trains linear classifiers predicting the pivot from the original features on unlabeled data from both domains, stacks the learned weight vectors W and computes an SVD to derive a low-dimensional linear mapping capturing correspondences; the mapping augments original features for downstream supervised learning on the target.",
            "procedure_type": "computational method / feature-representation learning",
            "source_domain": "natural language processing / text classification (domain adaptation)",
            "target_domain": "target-domain NLP tasks (e.g., sentiment classification on different product domains) or other cross-domain classification tasks where pivot features can be defined",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Use of pivot features as surrogate labels; learning multiple auxiliary prediction tasks followed by SVD to extract shared structure; later extension (SCL-MI) uses mutual information to select pivots rather than heuristics.",
            "transfer_success": "successful in cited NLP experiments (survey cites improvements in cross-domain sentiment and tagging tasks); performance depends on pivot selection quality; survey notes SCL reduces domain difference experimentally but pivot selection is domain-dependent.",
            "barriers_encountered": "Selecting good pivot features is difficult and domain-dependent; assumes existence of meaningful pivots shared across domains; linearity assumption of auxiliary classifiers may limit representational power.",
            "facilitating_factors": "Availability of abundant unlabeled data in both domains; presence of features that reliably correlate with labels across domains (pivots); SVD yields compact shared representation.",
            "contextual_requirements": "Large unlabeled corpora from both domains; domain expertise or statistics to select pivots (or mutual information computation); downstream supervised labeled target data for final classifier training.",
            "generalizability": "Well-suited to NLP/text domain adaptation; conceptually extendable to other domains if pivot-like shared signals exist, but practicality depends on pivot identification.",
            "knowledge_type": "explicit procedural steps and interpretive framework (feature correspondence mapping)",
            "uuid": "e388.3",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "SCL-MI",
            "name_full": "SCL with Mutual Information (SCL-MI)",
            "brief_description": "An extension of SCL that selects pivot features using mutual information with source labels rather than heuristic selection, improving the relevance of pivots for domain adaptation in NLP.",
            "citation_title": "Biographies, Bollywood, Boom-Boxes and Blenders: Domain Adaptation for Sentiment Classification",
            "mention_or_use": "mention",
            "procedure_name": "SCL-MI (pivot selection via mutual information)",
            "procedure_description": "SCL-MI computes mutual information between candidate features and source labels to select pivot features that are highly predictive in the source; it then follows SCL's auxiliary task and SVD pipeline to derive shared feature mappings used to augment features for target classification.",
            "procedure_type": "computational method / feature-representation learning",
            "source_domain": "NLP / sentiment classification",
            "target_domain": "cross-domain sentiment classification and other NLP adaptation tasks",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Replaced heuristic pivot selection with an MI-based ranking to choose features most informative of source labels, aiming to improve pivot relevance and transfer effectiveness.",
            "transfer_success": "reported to outperform heuristic pivot selection in cited work's sentiment classification experiments (survey summarizes comparative improvements but gives no numeric specifics here).",
            "barriers_encountered": "Requires accurate MI estimation which needs sufficient labeled source data; may still be sensitive to domain mismatch if MI-selected pivots are not predictive in target.",
            "facilitating_factors": "Availability of labeled source data to compute MI; abundant unlabeled data for training auxiliary predictors.",
            "contextual_requirements": "Sizable labeled source set for reliable MI estimates; computational resources for MI calculation and auxiliary training.",
            "generalizability": "Primarily applicable in text/NLP where MI can be computed for features; concept may transfer to other modalities where analogous informative shared features exist.",
            "knowledge_type": "explicit procedural steps and selection heuristics",
            "uuid": "e388.4",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "Self-taught / Sparse coding",
            "name_full": "Self-Taught Learning using Sparse Coding",
            "brief_description": "An unsupervised feature-representation transfer approach where higher-level basis vectors (dictionary) are learned from abundant unlabeled source data via sparse coding, then target data are encoded with that dictionary and supervised learning is applied on the new representations.",
            "citation_title": "Self-Taught Learning: Transfer Learning from Unlabeled Data",
            "mention_or_use": "mention",
            "procedure_name": "Self-taught learning via sparse coding",
            "procedure_description": "First, learn a dictionary of basis vectors from large unlabeled source data by solving a sparse coding optimization that balances reconstruction error and sparsity; second, encode target-domain examples using the learned dictionary to obtain higher-level features, then train discriminative classifiers on the (few) labeled target examples in the new feature space.",
            "procedure_type": "computational method / unsupervised feature learning",
            "source_domain": "unsupervised representation learning (sparse coding) often applied in vision and signal processing",
            "target_domain": "supervised classification tasks in domains with scarce labels (e.g., image classification, text) where unlabeled source data are abundant",
            "transfer_type": "adapted/modified for new context (unsupervised-&gt;supervised transfer)",
            "modifications_made": "Dictionary (basis) learned purely on source unlabeled data is reused as a fixed transform for target; target encoding step solves a sparse coding subproblem per target instance; assumes learned bases capture transferable high-level structure.",
            "transfer_success": "partially successful — cited as improving target classification in multiple experiments when learned bases capture transferable structure; survey notes potential drawback that source-learned bases may not suit target domain if domains are too dissimilar.",
            "barriers_encountered": "Mismatch between source-learned bases and target data statistics leading to limited transfer; choice of dictionary size and sparsity regularization critical; computational cost for coding target instances.",
            "facilitating_factors": "Large unlabeled source datasets to learn robust bases; target tasks that share underlying structure with source (e.g., similar visual primitives or language constructs).",
            "contextual_requirements": "Efficient sparse coding solver; ability to choose regularizers and dictionary size; some labeled target examples to train classifier on new features.",
            "generalizability": "General framework applicable across modalities (vision, text, audio) where unsupervised bases capture transferable structure; success depends on domain relatedness.",
            "knowledge_type": "procedural/technical (explicit algorithmic steps and representational knowledge)",
            "uuid": "e388.5",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "MMDE / TCA",
            "name_full": "Maximum Mean Discrepancy Embedding (MMDE) and Transfer Component Analysis (TCA)",
            "brief_description": "Dimensionality-reduction-based transfer methods that learn a latent subspace minimizing distribution discrepancy (MMD) between source and target; TCA is an efficient implementation to overcome MMDE computational costs.",
            "citation_title": "Transfer Learning via Dimensionality Reduction",
            "mention_or_use": "mention",
            "procedure_name": "MMDE (Maximum Mean Discrepancy Embedding) and TCA (Transfer Component Analysis)",
            "procedure_description": "MMDE optimizes an embedding that reduces the Maximum Mean Discrepancy (MMD) between source and target distributions while preserving data structure for downstream tasks. TCA approximates this objective with kernel-based feature extraction to find transfer components (a low-dimensional subspace) where source and target marginal distributions are closer; subsequent classifiers trained in this subspace generalize better to target.",
            "procedure_type": "computational method / dimensionality reduction for domain adaptation",
            "source_domain": "statistical learning / kernel methods / dimensionality reduction",
            "target_domain": "domain adaptation problems across fields (text, WiFi localization, vision) needing distribution alignment",
            "transfer_type": "adapted/modified for new context (applied to domain adaptation and made computationally efficient)",
            "modifications_made": "Derived an objective based on MMD for embedding; TCA implements an efficient kernelized solution to the MMDE objective, trading off computational burden and embedding quality.",
            "transfer_success": "reported successful in referenced experiments (survey cites TCA improving WiFi localization and other tasks compared to non-transfer baselines and kernel PCA); no absolute metrics provided here but original papers show empirical gains.",
            "barriers_encountered": "Computational cost for MMDE on large datasets (mitigated by TCA); choice of kernel and subspace dimensionality influences success; assumes same feature space across domains.",
            "facilitating_factors": "Theoretical alignment measure (MMD) guiding the embedding; availability of unlabeled target data to estimate target marginal; kernel machinery enabling flexible embeddings.",
            "contextual_requirements": "Access to source and target data (unlabeled target accepted), kernel selection, computational resources for eigen-decomposition or kernel operations.",
            "generalizability": "Broadly generalizable to problems with shared feature space and covariate shift; less applicable when features are heterogeneous without mapping.",
            "knowledge_type": "theoretical principles and explicit algorithmic procedure",
            "uuid": "e388.6",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "TAMAR",
            "name_full": "TAMAR (Transfer via Mapping and Revision for MLNs)",
            "brief_description": "A two-stage relational transfer algorithm that maps a Markov Logic Network learned in a source relational domain to a target domain using weighted pseudo log-likelihood, then revises the mapped structure via inductive logic programming (FORTE) to fit target data.",
            "citation_title": "Mapping and Revising Markov Logic Networks for Transfer Learning",
            "mention_or_use": "mention",
            "procedure_name": "TAMAR (MLN mapping and revision)",
            "procedure_description": "Stage 1: Construct a mapping from source MLN predicates/structures to target-domain predicates by scoring candidate mappings with a weighted pseudo log-likelihood (WPLL) measure. Stage 2: Apply FORTE, an ILP-based theory revision algorithm, to revise the mapped MLN in the target domain to better explain target data. The revised MLN serves as the relational model for inference in the target.",
            "procedure_type": "computational method / relational knowledge-transfer / model mapping and revision",
            "source_domain": "statistical relational learning (Markov Logic Networks) applied in source relational datasets",
            "target_domain": "different relational domains with analogous roles/relations (e.g., academic to industrial organizational networks)",
            "transfer_type": "adapted/modified for new context (structured mapping plus revision)",
            "modifications_made": "Explicit construction of cross-domain predicate/entity mappings using WPLL; incorporation of ILP-based revision to adapt structure to target-specific relations and data; two-stage pipeline instead of direct reuse.",
            "transfer_success": "reported effective in cited works for transferring relational knowledge across domains when mapping between analogous entities exists; survey summarizes the conceptual success but does not provide numeric outcomes.",
            "barriers_encountered": "Need to identify plausible entity/predicate mappings (may be nontrivial); domains must share relational analogies; computational complexity in structure search and revision.",
            "facilitating_factors": "Presence of analogous relational roles across domains (e.g., professor↔manager); expressive MLN formalism enabling mapping; ILP revision to adapt mapped structures.",
            "contextual_requirements": "Access to a learned source MLN and target relational data; ability to propose candidate mappings; ILP/revision tooling (FORTE) and compute resources.",
            "generalizability": "Applicable to relational domains with structural analogies; less useful for non-relational or widely divergent relational structures.",
            "knowledge_type": "interpretive frameworks + explicit procedural steps (mapping heuristics and revision algorithms)",
            "uuid": "e388.7",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "WiFi localization transfer",
            "name_full": "Transfer of WiFi-based Indoor Localization Models",
            "brief_description": "Application of transfer learning methods to adapt WiFi signal–based localization models across time, space, and device changes to reduce costly recalibration, using techniques like adaptive temporal radio maps, multi-view learning and latent multi-task learning.",
            "citation_title": "Transfer Learning for WiFi-Based Indoor Localization",
            "mention_or_use": "mention",
            "procedure_name": "Transfer of WiFi localization models (adaptive radio maps / multi-view / latent multi-task)",
            "procedure_description": "Various works transfer localization models by (a) adapting radio maps over time (updating models with limited new labels), (b) multi-view learning to combine different sensor/device views, and (c) latent multi-task learning to share latent representations across devices/locations; the goal is to reuse source calibration data to predict location under changed conditions.",
            "procedure_type": "experimental protocol / applied computational method (model adaptation in sensor networks)",
            "source_domain": "sensor-network localization and supervised regression models for WiFi fingerprinting",
            "target_domain": "new time periods, spatial areas, or different mobile devices with shifted WiFi signal distributions",
            "transfer_type": "adapted/modified for new context (practical model adaptation and recalibration reduction)",
            "modifications_made": "Incorporation of temporal adaptation (updating or reweighting samples), multi-view fusion of device-specific features, and latent task models to share structure across devices/periods; algorithms designed to tolerate nonstationary signal strengths and device heterogeneity.",
            "transfer_success": "reported as beneficial in multiple cited studies—survey notes significant reduction in relabeling/calibration effort and improved localization compared to naive retraining, though specific metrics are reported in cited papers rather than in this survey.",
            "barriers_encountered": "WiFi signal variability over time, device heterogeneity, nonstationarity and environmental changes leading to distribution shift; limited labeled data in target periods/devices.",
            "facilitating_factors": "Physical continuity of environment (some stable signal patterns), ability to collect some unlabeled or limited labeled target samples, multi-device data availability to learn shared latent structures.",
            "contextual_requirements": "Access to source calibration datasets, limited labeled target examples or abundant unlabeled target measurements, domain knowledge about signal dynamics, computational frameworks for model adaptation.",
            "generalizability": "Highly applicable across indoor localization setups; techniques tailored to sensor-data modalities and may transfer to other sensor adaptation problems with similar nonstationarity.",
            "knowledge_type": "procedural/technical and tacit practical know-how (how to adapt calibration procedures and modeling choices)",
            "uuid": "e388.8",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "Translated Learning",
            "name_full": "Translated Learning (feature-space translation across heterogeneous domains)",
            "brief_description": "Approach to transfer between entirely different feature spaces by learning mapping functions that translate features from one domain (e.g., images) into another (e.g., text), enabling cross-modal transfer learning.",
            "citation_title": "Translated Learning",
            "mention_or_use": "mention",
            "procedure_name": "Translated Learning (cross-feature-space mapping)",
            "procedure_description": "Learn a mapping function (translation) between heterogeneous feature spaces so that models or knowledge in the source space can be applied in the target space; mappings may be learned from paired data or via bridging latent spaces that align semantics across modalities.",
            "procedure_type": "computational method / cross-modal mapping and transfer",
            "source_domain": "computer vision or textual feature spaces (e.g., image descriptors)",
            "target_domain": "different modality feature spaces (e.g., textual descriptors), enabling transfer between images and text or other heterogeneous domains",
            "transfer_type": "analogical transfer with significant changes (requires mapping functions)",
            "modifications_made": "Construction of explicit translation/mapping functions (possibly learned from paired examples or via shared latent spaces) to bridge disparate feature sets; changes include designing loss/objectives to preserve semantics across spaces.",
            "transfer_success": "presented as a new direction with promising outcomes in cited work; survey states translated learning enables transfer across entirely different feature spaces but does not report quantitative results here.",
            "barriers_encountered": "Requires paired or bridging data to learn mappings; semantic gap between modalities, difficulties in aligning heterogeneous features; risk of poor generalization if mapping is weak.",
            "facilitating_factors": "Availability of cross-modal correspondences or latent shared semantics; flexible mapping models (e.g., latent-factor models) and sufficient training data for mapping.",
            "contextual_requirements": "Paired data or auxiliary bridging datasets, model capacity to learn cross-space correspondences, careful objective formulation to preserve task-relevant structure.",
            "generalizability": "Potentially general across heterogeneous transfer tasks (images↔text, audio↔text), but heavily dependent on quality of mapping data and semantic alignment.",
            "knowledge_type": "explicit procedural steps and representational/theoretical mapping knowledge",
            "uuid": "e388.9",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "RMGM",
            "name_full": "Rating-Matrix Generative Model (RMGM) for collaborative filtering transfer",
            "brief_description": "A latent-mixture model that learns shared latent user and item clusters across rating matrices from different domains to transfer collaborative-filtering knowledge and address cold-start/sparsity.",
            "citation_title": "Transfer Learning for Collaborative Filtering via a Rating-Matrix Generative Model",
            "mention_or_use": "mention",
            "procedure_name": "RMGM (shared latent user-item spaces for cross-domain collaborative filtering)",
            "procedure_description": "RMGM models each rating matrix via latent user and item cluster variables and learns a mixture model where users/items from different domains are mapped to shared latent clusters; the shared latent spaces enable transfer of rating patterns from auxiliary domains (e.g., books) to a target domain (e.g., movies).",
            "procedure_type": "computational method / probabilistic generative model for transfer",
            "source_domain": "recommender systems / collaborative filtering in one domain (e.g., books)",
            "target_domain": "collaborative filtering in another domain (e.g., movies) suffering from sparsity or cold-start",
            "transfer_type": "hybrid approach combining with existing methods (probabilistic latent modeling across datasets)",
            "modifications_made": "Introduced shared latent user/item cluster variables and mapping procedures to align multiple rating matrices; uses generative assumptions to enable knowledge sharing and reconstruct target matrix via expanded codebook.",
            "transfer_success": "reported to mitigate cold-start and sparsity in cited works; survey notes RMGM bridges rating matrices effectively when domains are related, but concrete metrics are in original papers.",
            "barriers_encountered": "Assumes latent cluster compatibility across domains; model misspecification or weak cross-domain alignment can limit benefits; requires sufficient auxiliary domain data.",
            "facilitating_factors": "Similarity in user/item behaviors across domains, abundant auxiliary ratings, and probabilistic modeling enabling shared latent structure.",
            "contextual_requirements": "Availability of one or more auxiliary rating matrices, EM/approximate inference tooling, and assumptions that latent clusters are meaningful across domains.",
            "generalizability": "Applicable to multi-domain recommender scenarios where latent patterns are shared; less effective when user/item semantics differ greatly between domains.",
            "knowledge_type": "theoretical principles and explicit probabilistic modeling steps",
            "uuid": "e388.10",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "Daumé's feature augmentation",
            "name_full": "Frustratingly Easy Domain Adaptation (feature augmentation / kernel mapping)",
            "brief_description": "A simple domain adaptation trick that augments original features with domain-specific and shared copies (or uses kernel mappings driven by domain knowledge) to allow discriminative learners to learn shared and domain-specific weights.",
            "citation_title": "Frustratingly Easy Domain Adaptation",
            "mention_or_use": "mention",
            "procedure_name": "Feature augmentation / kernel mapping for domain adaptation",
            "procedure_description": "Construct augmented feature vectors by concatenating (1) domain-common features, (2) source-specific copies, and (3) target-specific copies (or apply a domain-aware kernel mapping), enabling standard discriminative classifiers to learn parameters that capture shared vs. domain-specific effects without complex modeling.",
            "procedure_type": "computational method / feature-engineering-based domain adaptation",
            "source_domain": "NLP / discriminative classification",
            "target_domain": "cross-domain NLP tasks and potentially other classification tasks with domain shifts",
            "transfer_type": "direct application with simple modification (feature augmentation)",
            "modifications_made": "Augmented original feature vectors to explicitly represent domain-common and domain-specific components; optionally implement via kernel mapping tailored to NLP features.",
            "transfer_success": "reported as an effective, easy-to-implement baseline that yields strong empirical performance in many domain-adaptation tasks; survey notes mapping is domain-knowledge driven and may not generalize trivially.",
            "barriers_encountered": "Relies on domain knowledge to construct useful mappings; may increase dimensionality substantially; limited when domain disparity is complex.",
            "facilitating_factors": "Simplicity of implementation, compatibility with off-the-shelf classifiers, and presence of shared signal across domains.",
            "contextual_requirements": "Ability to augment feature representation and sufficient labeled data to train augmented-feature classifiers.",
            "generalizability": "A general baseline applicable across many domains; kernel mapping variants may need domain-specific design.",
            "knowledge_type": "explicit procedural steps and pragmatic engineering heuristics",
            "uuid": "e388.11",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning",
                "publication_date_yy_mm": "2010-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Boosting for Transfer Learning",
            "rating": 2,
            "sanitized_title": "boosting_for_transfer_learning"
        },
        {
            "paper_title": "Correcting Sample Selection Bias by Unlabeled Data",
            "rating": 2,
            "sanitized_title": "correcting_sample_selection_bias_by_unlabeled_data"
        },
        {
            "paper_title": "Direct Importance Estimation with Model Selection and its Application to Covariate Shift Adaptation",
            "rating": 2,
            "sanitized_title": "direct_importance_estimation_with_model_selection_and_its_application_to_covariate_shift_adaptation"
        },
        {
            "paper_title": "Domain Adaptation with Structural Correspondence Learning",
            "rating": 2,
            "sanitized_title": "domain_adaptation_with_structural_correspondence_learning"
        },
        {
            "paper_title": "Biographies, Bollywood, Boom-Boxes and Blenders: Domain Adaptation for Sentiment Classification",
            "rating": 2,
            "sanitized_title": "biographies_bollywood_boomboxes_and_blenders_domain_adaptation_for_sentiment_classification"
        },
        {
            "paper_title": "Self-Taught Learning: Transfer Learning from Unlabeled Data",
            "rating": 2,
            "sanitized_title": "selftaught_learning_transfer_learning_from_unlabeled_data"
        },
        {
            "paper_title": "Transfer Learning via Dimensionality Reduction",
            "rating": 2,
            "sanitized_title": "transfer_learning_via_dimensionality_reduction"
        },
        {
            "paper_title": "Domain Adaptation via Transfer Component Analysis",
            "rating": 2,
            "sanitized_title": "domain_adaptation_via_transfer_component_analysis"
        },
        {
            "paper_title": "Mapping and Revising Markov Logic Networks for Transfer Learning",
            "rating": 2,
            "sanitized_title": "mapping_and_revising_markov_logic_networks_for_transfer_learning"
        },
        {
            "paper_title": "Transfer Learning for WiFi-Based Indoor Localization",
            "rating": 2,
            "sanitized_title": "transfer_learning_for_wifibased_indoor_localization"
        },
        {
            "paper_title": "Translated Learning",
            "rating": 2,
            "sanitized_title": "translated_learning"
        },
        {
            "paper_title": "Transfer Learning for Collaborative Filtering via a Rating-Matrix Generative Model",
            "rating": 2,
            "sanitized_title": "transfer_learning_for_collaborative_filtering_via_a_ratingmatrix_generative_model"
        },
        {
            "paper_title": "Frustratingly Easy Domain Adaptation",
            "rating": 2,
            "sanitized_title": "frustratingly_easy_domain_adaptation"
        }
    ],
    "cost": 0.02155925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Transfer Learning</p>
<p>Sinno Jialin Pan sinnopan@cse.ust.hk 
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Clearwater BayKowloon, Hong Kong</p>
<p>Fellow, IEEEQiang Yang qyang@cse.ust.hk 
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Clearwater BayKowloon, Hong Kong</p>
<p>A Survey on Transfer Learning
16BF53EBE6402216803E11D87FC9CB2D10.1109/TKDE.2009.191received 13 Nov. 2008; revised 29 May 2009; accepted 13 July 2009; published online 12 Oct. 2009.Transfer learningsurveymachine learningdata mining
A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution.However, in many real-world applications, this assumption may not hold.For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution.In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts.In recent years, transfer learning has emerged as a new learning framework to address this problem.This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems.In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift.We also explore some potential future issues in transfer learning research.</p>
<p>INTRODUCTION</p>
<p>D ATA mining and machine learning technologies have already achieved significant success in many knowledge engineering areas including classification, regression, and clustering (e.g., [1], [2]).However, many machine learning methods work well only under a common assumption: the training and test data are drawn from the same feature space and the same distribution.When the distribution changes, most statistical models need to be rebuilt from scratch using newly collected training data.In many realworld applications, it is expensive or impossible to recollect the needed training data and rebuild the models.It would be nice to reduce the need and effort to recollect the training data.In such cases, knowledge transfer or transfer learning between task domains would be desirable.</p>
<p>Many examples in knowledge engineering can be found where transfer learning can truly be beneficial.One example is Web-document classification [3], [4], [5], where our goal is to classify a given Web document into several predefined categories.As an example, in the area of Webdocument classification (see, e.g., [6]), the labeled examples may be the university webpages that are associated with category information obtained through previous manuallabeling efforts.For a classification task on a newly created website where the data features or data distributions may be different, there may be a lack of labeled training data.As a result, we may not be able to directly apply the webpage classifiers learned on the university website to the new website.In such cases, it would be helpful if we could transfer the classification knowledge into the new domain.</p>
<p>The need for transfer learning may arise when the data can be easily outdated.In this case, the labeled data obtained in one time period may not follow the same distribution in a later time period.For example, in indoor WiFi localization problems, which aims to detect a user's current location based on previously collected WiFi data, it is very expensive to calibrate WiFi data for building localization models in a large-scale environment, because a user needs to label a large collection of WiFi signal data at each location.However, the WiFi signal-strength values may be a function of time, device, or other dynamic factors.A model trained in one time period or on one device may cause the performance for location estimation in another time period or on another device to be reduced.To reduce the recalibration effort, we might wish to adapt the localization model trained in one time period (the source domain) for a new time period (the target domain), or to adapt the localization model trained on a mobile device (the source domain) for a new mobile device (the target domain), as done in [7].</p>
<p>As a third example, consider the problem of sentiment classification, where our task is to automatically classify the reviews on a product, such as a brand of camera, into positive and negative views.For this classification task, we need to first collect many reviews of the product and annotate them.We would then train a classifier on the reviews with their corresponding labels.Since the distribution of review data among different types of products can be very different, to maintain good classification performance, we need to collect a large amount of labeled data in order to train the review-classification models for each product.However, this data-labeling process can be very expensive to do.To reduce the effort for annotating reviews for various products, we may want to adapt a classification model that is trained on some products to help learn classification models for some other products.In such cases, transfer learning can save a significant amount of labeling effort [8].</p>
<p>In this survey paper, we give a comprehensive overview of transfer learning for classification, regression, and cluster-ing developed in machine learning and data mining areas.There has been a large amount of work on transfer learning for reinforcement learning in the machine learning literature (e.g., [9], [10]).However, in this paper, we only focus on transfer learning for classification, regression, and clustering problems that are related more closely to data mining tasks.By doing the survey, we hope to provide a useful resource for the data mining and machine learning community.</p>
<p>The rest of the survey is organized as follows: In the next four sections, we first give a general overview and define some notations we will use later.We, then, briefly survey the history of transfer learning, give a unified definition of transfer learning and categorize transfer learning into three different settings (given in Table 2 and Fig. 2).For each setting, we review different approaches, given in Table 3 in detail.After that, in Section 6, we review some current research on the topic of "negative transfer," which happens when knowledge transfer has a negative impact on target learning.In Section 7, we introduce some successful applications of transfer learning and list some published data sets and software toolkits for transfer learning research.Finally, we conclude the paper with a discussion of future works in Section 8.</p>
<p>OVERVIEW 2.1 A Brief History of Transfer Learning</p>
<p>Traditional data mining and machine learning algorithms make predictions on the future data using statistical models that are trained on previously collected labeled or unlabeled training data [11], [12], [13].Semisupervised classification [14], [15], [16], [17] addresses the problem that the labeled data may be too few to build a good classifier, by making use of a large amount of unlabeled data and a small amount of labeled data.Variations of supervised and semisupervised learning for imperfect data sets have been studied; for example, Zhu and Wu [18] have studied how to deal with the noisy class-label problems.Yang et al. considered costsensitive learning [19] when additional tests can be made to future samples.Nevertheless, most of them assume that the distributions of the labeled and unlabeled data are the same.Transfer learning, in contrast, allows the domains, tasks, and distributions used in training and testing to be different.In the real world, we observe many examples of transfer learning.For example, we may find that learning to recognize apples might help to recognize pears.Similarly, learning to play the electronic organ may help facilitate learning the piano.The study of Transfer learning is motivated by the fact that people can intelligently apply knowledge learned previously to solve new problems faster or with better solutions.The fundamental motivation for Transfer learning in the field of machine learning was discussed in a NIPS-95 workshop on "Learning to Learn," 1 which focused on the need for lifelong machine learning methods that retain and reuse previously learned knowledge.</p>
<p>Research on transfer learning has attracted more and more attention since 1995 in different names: learning to learn, life-long learning, knowledge transfer, inductive transfer, multitask learning, knowledge consolidation, context-sensitive learning, knowledge-based inductive bias, metalearning, and incremental/cumulative learning [20].Among these, a closely related learning technique to transfer learning is the multitask learning framework [21], which tries to learn multiple tasks simultaneously even when they are different.A typical approach for multitask learning is to uncover the common (latent) features that can benefit each individual task.</p>
<p>In 2005, the Broad Agency Announcement (BAA) 05-29 of Defense Advanced Research Projects Agency (DARPA)'s Information Processing Technology Office (IPTO) 2 gave a new mission of transfer learning: the ability of a system to recognize and apply knowledge and skills learned in previous tasks to novel tasks.In this definition, transfer learning aims to extract the knowledge from one or more source tasks and applies the knowledge to a target task.In contrast to multitask learning, rather than learning all of the source and target tasks simultaneously, transfer learning cares most about the target task.The roles of the source and target tasks are no longer symmetric in transfer learning.</p>
<p>Fig. 1 shows the difference between the learning processes of traditional and transfer learning techniques.As we can see, traditional machine learning techniques try to learn each task from scratch, while transfer learning techniques try to transfer the knowledge from some previous tasks to a target task when the latter has fewer high-quality training data.</p>
<p>Today, transfer learning methods appear in several top venues, most notably in data mining (ACM KDD, IEEE ICDM, and PKDD, for example), machine learning (ICML, NIPS, ECML, AAAI, and IJCAI, for example) and applications of machine learning and data mining (ACM SIGIR, WWW, and ACL, for example). 3Before we give different categorizations of transfer learning, we first describe the notations used in this paper.</p>
<p>Notations and Definitions</p>
<p>In this section, we introduce some notations and definitions that are used in this survey.First of all, we give the definitions of a "domain" and a "task," respectively.</p>
<p>In this survey, a domain D consists of two components: a feature space X and a marginal probability distribution P ðXÞ, where X ¼ fx 1 ; . . .; x n g 2 X.For example, if our learning task is document classification, and each term is taken as a binary feature, then X is the space of all term vectors, x i is the ith term vector corresponding to some documents, and X is a particular learning sample.In general, if two domains are different, then they may have different feature spaces or different marginal probability distributions.</p>
<p>Given a specific domain, D ¼ fX; P ðXÞg, a task consists of two components: a label space Y and an objective predictive function fðÁÞ (denoted by T ¼ fY; fðÁÞg), which is not observed but can be learned from the training data, which consist of pairs fx i ; y i g, where x i 2 X and y i 2 Y.The function fðÁÞ can be used to predict the corresponding label, fðxÞ, of a new instance x.From a probabilistic viewpoint, fðxÞ can be written as P ðyjxÞ.In our document classification example, Y is the set of all labels, which is True, False for a binary classification task, and y i is "True" or "False."</p>
<p>For simplicity, in this survey, we only consider the case where there is one source domain D S , and one target domain, D T , as this is by far the most popular of the research works in the literature.More specifically, we denote the source domain data as D S ¼ fðx S 1 ; y S 1 Þ; . . .; ðx S n S ; y S n S Þg, where x S i 2 X S is the data instance and y S i 2 Y S is the corresponding class label.In our document classification example, D S can be a set of term vectors together with their associated true or false class labels.Similarly, we denote the target-domain data as D T ¼ fðx T1 ; y T1 Þ; . . .; ðx Tn T ; y Tn T Þg, where the input x Ti is in X T and y Ti 2 Y T is the corresponding output.In most cases, 0 n T ( n S .</p>
<p>We now give a unified definition of transfer learning.</p>
<p>Definition 1 (Transfer Learning).Given a source domain D S and learning task T S , a target domain D T and learning task T T , transfer learning aims to help improve the learning of the target predictive function f T ðÁÞ in D T using the knowledge in D S and T S , where D S 6 ¼ D T , or T S 6 ¼ T T .</p>
<p>In the above definition, a domain is a pair D ¼ fX; P ðXÞg.Thus, the condition D S 6 ¼ D T implies that either X S 6 ¼ X T or P S ðXÞ 6 ¼ P T ðXÞ.For example, in our document classification example, this means that between a source document set and a target document set, either the term features are different between the two sets (e.g., they use different languages), or their marginal distributions are different.</p>
<p>Similarly, a task is defined as a pair T ¼ fY; P ðY jXÞg.Thus, the condition T S 6 ¼ T T implies that either Y S 6 ¼ Y T or P ðY S jX S Þ 6 ¼ P ðY T jX T Þ.When the target and source domains are the same, i.e., D S ¼ D T , and their learning tasks are the same, i.e., T S ¼ T T , the learning problem becomes a traditional machine learning problem.When the domains are different, then either 1) the feature spaces between the domains are different, i.e., X S 6 ¼ X T , or 2) the feature spaces between the domains are the same but the marginal probability distributions between domain data are different; i.e., P ðX S Þ 6 ¼ P ðX T Þ, where X Si 2 X S and X Ti 2 X T .As an example, in our document classification example, case 1 corresponds to when the two sets of documents are described in different languages, and case 2 may correspond to when the source domain documents and the targetdomain documents focus on different topics.</p>
<p>Given specific domains D S and D T , when the learning tasks T S and T T are different, then either 1) the label spaces between the domains are different, i.e., Y S 6 ¼ Y T , or 2) the conditional probability distributions between the domains are different; i.e., P ðY S jX S Þ 6 ¼ P ðY T jX T Þ, where Y S i 2 Y S and Y T i 2 Y T .In our document classification example, case 1 corresponds to the situation where source domain has binary document classes, whereas the target domain has 10 classes to classify the documents to.Case 2 corresponds to the situation where the source and target documents are very unbalanced in terms of the userdefined classes.</p>
<p>In addition, when there exists some relationship, explicit or implicit, between the feature spaces of the two domains, we say that the source and target domains are related.</p>
<p>A Categorization of Transfer Learning Techniques</p>
<p>In transfer learning, we have the following three main research issues: 1) what to transfer, 2) how to transfer, and 3) when to transfer."What to transfer" asks which part of knowledge can be transferred across domains or tasks.Some knowledge is specific for individual domains or tasks, and some knowledge may be common between different domains such that they may help improve performance for the target domain or task.After discovering which knowledge can be transferred, learning algorithms need to be developed to transfer the knowledge, which corresponds to the "how to transfer" issue.</p>
<p>"When to transfer" asks in which situations, transferring skills should be done.Likewise, we are interested in knowing in which situations, knowledge should not be transferred.In some situations, when the source domain and target domain are not related to each other, brute-force transfer may be unsuccessful.In the worst case, it may even hurt the performance of learning in the target domain, a situation which is often referred to as negative transfer.Most current work on transfer learning focuses on "What to transfer" and "How to transfer," by implicitly assuming that the source and target domains be related to each other.However, how to avoid negative transfer is an important open issue that is attracting more and more attention in the future.</p>
<p>Based on the definition of transfer learning, we summarize the relationship between traditional machine learning and various transfer learning settings in Table 1, where we  1.In the inductive transfer learning setting, the target task is different from the source task, no matter when the source and target domains are the same or not.</p>
<p>In this case, some labeled data in the target domain are required to induce an objective predictive model f T ðÁÞ for use in the target domain.In addition, according to different situations of labeled and unlabeled data in the source domain, we can further categorize the inductive transfer learning setting into two cases: a.A lot of labeled data in the source domain are available.In this case, the inductive transfer learning setting is similar to the multitask learning setting.However, the inductive transfer learning setting only aims at achieving high performance in the target task by transferring knowledge from the source task while multitask learning tries to learn the target and source task simultaneously.b.No labeled data in the source domain are available.In this case, the inductive transfer learning setting is similar to the self-taught learning setting, which is first proposed by Raina et al. [22].In the self-taught learning setting, the label spaces between the source and target domains may be different, which implies the side information of the source domain cannot be used directly.Thus, it's similar to the inductive transfer learning setting where the labeled data in the source domain are unavailable.2. In the transductive transfer learning setting, the source and target tasks are the same, while the source and target domains are different.</p>
<p>In this situation, no labeled data in the target domain are available while a lot of labeled data in the source domain are available.In addition, according to different situations between the source and target domains, we can further categorize the transductive transfer learning setting into two cases.</p>
<p>a.The feature spaces between the source and target domains are different, X S 6 ¼ X T .b.The feature spaces between domains are the same, X S ¼ X T , but the marginal probability distributions of the input data are different,
P ðX S Þ 6 ¼ P ðX T Þ.
The latter case of the transductive transfer learning setting is related to domain adaptation for knowledge transfer in text classification [23] and sample selection bias [24] or covariate shift [25], whose assumptions are similar.3. Finally, in the unsupervised transfer learning setting, similar to inductive transfer learning setting, the target task is different from but related to the source task.However, the unsupervised transfer learning focus on solving unsupervised learning tasks in the target domain, such as clustering, dimensionality reduction, and density estimation [26], [27].In this case, there are no labeled data available in both source and target domains in training.The relationship between the different settings of transfer learning and the related areas are summarized in Table 2 and Fig. 2.</p>
<p>Approaches to transfer learning in the above three different settings can be summarized into four cases based on "What to transfer."Table 3 shows these four cases and brief description.The first context can be referred to as instance-based transfer learning (or instance transfer) approach [6], [28], [29], [30], [31], [24], [32], [33], [34], [35], which assumes that certain parts of the data in the source domain can be reused for learning in the target domain by reweighting.Instance reweighting and importance sampling are two major techniques in this context.</p>
<p>A second case can be referred to as feature-representation-transfer approach [22], [36], [37], [38], [39], [8], [40], [41], [42], [43], [44].The intuitive idea behind this case is to learn a "good" feature representation for the target domain.In this case, the knowledge used to transfer across domains is encoded into the learned feature representation.With the new feature representation, the performance of the target task is expected to improve significantly.</p>
<p>A third case can be referred to as parameter-transfer approach [45], [46], [47], [48], [49], which assumes that the source tasks and the target tasks share some parameters or prior distributions of the hyperparameters of the models.The transferred knowledge is encoded into the shared parameters or priors.Thus, by discovering the shared parameters or priors, knowledge can be transferred across tasks.</p>
<p>Finally, the last case can be referred to as the relationalknowledge-transfer problem [50], which deals with transfer learning for relational domains.The basic assumption behind this context is that some relationship among the data in the source and target domains is similar.Thus, the knowledge to be transferred is the relationship among the data.Recently, statistical relational learning techniques dominate this context [51], [52].</p>
<p>Table 4 shows the cases where the different approaches are used for each transfer learning setting.We can see that the inductive transfer learning setting has been studied in many research works, while the unsupervised transfer learning setting is a relatively new research topic and only studied in the context of the feature-representation-transfer case.In addition, the feature-representation-transfer problem has been proposed to all three settings of transfer learning.However, the parameter-transfer and the relational-knowledgetransfer approach are only studied in the inductive transfer learning setting, which we discuss in detail below.Based on the above definition of the inductive transfer learning setting, a few labeled data in the target domain are required as the training data to induce the target predictive function.As mentioned in Section 2.3, this setting has two cases: 1) labeled data in the source domain are available and 2) labeled data in the source domain are unavailable while unlabeled data in the source domain are available.Most transfer learning approaches in this setting focus on the former case.</p>
<p>INDUCTIVE TRANSFER LEARNING</p>
<p>Transferring Knowledge of Instances</p>
<p>The instance-transfer approach to the inductive transfer learning setting is intuitively appealing: although the source domain data cannot be reused directly, there are certain parts of the data that can still be reused together with a few labeled data in the target domain.</p>
<p>Dai et al. [6] proposed a boosting algorithm, TrAdaBoost, which is an extension of the AdaBoost algorithm, to address the inductive transfer learning problems.TrAdaBoost assumes that the source and target-domain data use exactly the same  set of features and labels, but the distributions of the data in the two domains are different.In addition, TrAdaBoost assumes that, due to the difference in distributions between the source and the target domains, some of the source domain data may be useful in learning for the target domain but some of them may not and could even be harmful.It attempts to iteratively reweight the source domain data to reduce the effect of the "bad" source data while encourage the "good" source data to contribute more for the target domain.For each round of iteration, TrAdaBoost trains the base classifier on the weighted source and target data.The error is only calculated on the target data.Furthermore, TrAdaBoost uses the same strategy as AdaBoost to update the incorrectly classified examples in the target domain while using a different strategy from AdaBoost to update the incorrectly classified source examples in the source domain.Theoretical analysis of TrAda-Boost in also given in [6].</p>
<p>Jiang and Zhai [30] proposed a heuristic method to remove "misleading" training examples from the source domain based on the difference between conditional probabilities P ðy T jx T Þ and P ðy S jx S Þ. Liao et al. [31] proposed a new active learning method to select the unlabeled data in a target domain to be labeled with the help of the source domain data.Wu and Dietterich [53] integrated the source domain (auxiliary) data an Support Vector Machine (SVM) framework for improving the classification performance.</p>
<p>Transferring Knowledge of Feature Representations</p>
<p>The feature-representation-transfer approach to the inductive transfer learning problem aims at finding "good" feature representations to minimize domain divergence and classification or regression model error.Strategies to find "good" feature representations are different for different types of the source domain data.If a lot of labeled data in the source domain are available, supervised learning methods can be used to construct a feature representation.This is similar to common feature learning in the field of multitask learning [40].If no labeled data in the source domain are available, unsupervised learning methods are proposed to construct the feature representation.</p>
<p>Supervised Feature Construction</p>
<p>Supervised feature construction methods for the inductive transfer learning setting are similar to those used in multitask learning.The basic idea is to learn a low-dimensional representation that is shared across related tasks.In addition, the learned new representation can reduce the classification or regression model error of each task as well.Argyriou et al. [40] proposed a sparse feature learning method for multitask learning.In the inductive transfer learning setting, the common features can be learned by solving an optimization problem, given as follows:
arg min A;U X t2fT ;Sg X n t i¼1
Lðy ti ; ha t ; U T x ti iÞ þ kAk 2 2;1
s:t: U 2 O d :ð1Þ
In this equation, S and T denote the tasks in the source domain and target domain, respectively.A ¼ ½a S ; a T 2 R dÂ2 is a matrix of parameters.U is a d Â d orthogonal matrix (mapping function) for mapping the original high-dimensional data to low-dimensional representations.The ðr; pÞnorm of A is defined as kAk r;p :¼ ð
P d i¼1 ka i k p r Þ 1 p
. The optimization problem (1) estimates the low-dimensional representations U T X T , U T X S and the parameters, A, of the model at the same time.The optimization problem (1) can be further transformed into an equivalent convex optimization formulation and be solved efficiently.In a follow-up work, Argyriou et al. [41] proposed a spectral regularization framework on matrices for multitask structure learning.</p>
<p>Lee et al. [42] proposed a convex optimization algorithm for simultaneously learning metapriors and feature weights from an ensemble of related prediction tasks.The metapriors can be transferred among different tasks.Jebara [43] proposed to select features for multitask learning with SVMs.Ru ¨ckert and Kramer [54] designed a kernel-based approach to inductive transfer, which aims at finding a suitable kernel for the target data.</p>
<p>Unsupervised Feature Construction</p>
<p>In [22], Raina et al. proposed to apply sparse coding [55], which is an unsupervised feature construction method, for learning higher level features for transfer learning.The basic idea of this approach consists of two steps.In the first step, higher level basis vectors b ¼ fb 1 ; b 2 ; . . .; b s g are learned on the source domain data by solving the optimization problem (2) as shown as follows:
min a;b X i x Si À X j a j S i b j 2 2 þ a Si 1 s:t: kb j k 2 1; 8j 2 1; . . . ; s:ð2Þ
In this equation, a j S i is a new representation of basis b j for input x S i and is a coefficient to balance the feature construction term and the regularization term.After learning the basis vectors b, in the second step, an optimization algorithm (3) is applied on the target-domain data to learn higher level features based on the basis vectors b.Finally, discriminative algorithms can be applied to fa Ã T i g 0 s with corresponding labels to train classification or regression models for use in the target domain.One drawback of this method is that the so-called higher level basis vectors learned on the source domain in the optimization problem (2) may not be suitable for use in the target domain.
a Ã Ti ¼ arg min a T i x T i À X j a j T i b j 2 2 þ a T i 1 :ð3Þ
Recently, manifold learning methods have been adapted for transfer learning.In [44], Wang and Mahadevan proposed a Procrustes analysis-based approach to manifold alignment without correspondences, which can be used to transfer the knowledge across domains via the aligned manifolds.</p>
<p>Transferring Knowledge of Parameters</p>
<p>Most parameter-transfer approaches to the inductive transfer learning setting assume that individual models for related tasks should share some parameters or prior distributions of hyperparameters.Most approaches described in this section, including a regularization framework and a hierarchical Bayesian framework, are designed to work under multitask learning.However, they can be easily modified for transfer learning.As mentioned above, multitask learning tries to learn both the source and target tasks simultaneously and perfectly, while transfer learning only aims at boosting the performance of the target domain by utilizing the source domain data.Thus, in multitask learning, weights of the loss functions for the source and target data are the same.In contrast, in transfer learning, weights in the loss functions for different domains can be different.Intuitively, we may assign a larger weight to the loss function of the target domain to make sure that we can achieve better performance in the target domain.</p>
<p>Lawrence and Platt [45] proposed an efficient algorithm known as MT-IVM, which is based on Gaussian Processes (GP), to handle the multitask learning case.MT-IVM tries to learn parameters of a Gaussian Process over multiple tasks by sharing the same GP prior.Bonilla et al. [46] also investigated multitask learning in the context of GP.The authors proposed to use a free-form covariance matrix over tasks to model intertask dependencies, where a GP prior is used to induce correlations between tasks.Schwaighofer et al. [47] proposed to use a hierarchical Bayesian framework (HB) together with GP for multitask learning.</p>
<p>Besides transferring the priors of the GP models, some researchers also proposed to transfer parameters of SVMs under a regularization framework.Evgeniou and Pontil [48] borrowed the idea of HB to SVMs for multitask learning.The proposed method assumed that the parameter, w, in SVMs for each task can be separated into two terms.One is a common term over tasks and the other is a task-specific term.In inductive transfer learning,
w S ¼ w 0 þ v S and w T ¼ w 0 þ v T ;
where w S and w T are parameters of the SVMs for the source task and the target learning task, respectively.w 0 is a common parameter while v S and v T are specific parameters for the source task and the target task, respectively.By assuming f t ¼ w t Á x to be a hyperplane for task t, an extension of SVMs to multitask learning case can be written as the following:
min w0;vt;t i Jðw 0 ; v t ; ti Þ ¼ X t2fS;T g X n t i¼1 ti þ 1 2 X t2fS;T g kv t k 2 þ 2 kw 0 k 2 s:t: y t i ðw 0 þ v t Þ Á x t i ! 1 À t i ; t i ! 0; i2
f1; 2; . . .; n t g and t 2 fS; T g:</p>
<p>By solving the optimization problem above, we can learn the parameters w 0 , v S , and v T simultaneously.Several researchers have pursued the parameter-transfer approach further.Gao et al. [49] proposed a locally weighted ensemble learning framework to combine multiple models for transfer learning, where the weights are dynamically assigned according to a model's predictive power on each test example in the target domain.</p>
<p>Transferring Relational Knowledge</p>
<p>Different from other three contexts, the relational-knowledge-transfer approach deals with transfer learning problems in relational domains, where the data are non-i.i.d. and can be represented by multiple relations, such as networked data and social network data.This approach does not assume that the data drawn from each domain be independent and identically distributed (i.i.d.) as traditionally assumed.It tries to transfer the relationship among data from a source domain to a target domain.In this context, statistical relational learning techniques are proposed to solve these problems.</p>
<p>Mihalkova et al. [50] proposed an algorithm TAMAR that transfers relational knowledge with Markov Logic Networks (MLNs) across relational domains.MLNs [56] is a powerful formalism, which combines the compact expressiveness of first-order logic with flexibility of probability, for statistical relational learning.In MLNs, entities in a relational domain are represented by predicates and their relationships are represented in first-order logic.TAMAR is motivated by the fact that if two domains are related to each other, there may exist mappings to connect entities and their relationships from a source domain to a target domain.For example, a professor can be considered as playing a similar role in an academic domain as a manager in an industrial management domain.In addition, the relationship between a professor and his or her students is similar to the relationship between a manager and his or her workers.Thus, there may exist a mapping from professor to manager and a mapping from the professor-student relationship to the manager-worker relationship.In this vein, TAMAR tries to use an MLN learned for a source domain to aid in the learning of an MLN for a target domain.Basically, TAMAR is a two-stage algorithm.In the first step, a mapping is constructed from a source MLN to the target domain based on weighted pseudo log-likelihood measure (WPLL).In the second step, a revision is done for the mapped structure in the target domain through the FORTE algorithm [57], which is an inductive logic programming (ILP) algorithm for revising first-order theories.The revised MLN can be used as a relational model for inference or reasoning in the target domain.</p>
<p>In the AAAI-2008 workshop on transfer learning for complex tasks, 4 Mihalkova and Mooney [51] extended TAMAR to the single-entity-centered setting of transfer learning, where only one entity in a target domain is available.Davis and Domingos [52] proposed an approach to transferring relational knowledge based on a form of second-order Markov logic.The basic idea of the algorithm is to discover structural regularities in the source domain in the form of Markov logic formulas with predicate variables, by instantiating these formulas with predicates from the target domain.</p>
<p>TRANSDUCTIVE TRANSFER LEARNING</p>
<p>The term transductive transfer learning was first proposed by Arnold et al. [58], where they required that the source and target tasks be the same, although the domains may be different.On top of these conditions, they further required that all unlabeled data in the target domain are available at training time, but we believe that this condition can be relaxed; instead, in our definition of the transductive transfer learning setting, we only require that part of the unlabeled target data be seen at training time in order to obtain the marginal probability for the target data.</p>
<p>Note that the word "transductive" is used with several meanings.In the traditional machine learning setting, transductive learning [59] refers to the situation where all test data are required to be seen at training time, and that the learned model cannot be reused for future data.Thus, when some new test data arrive, they must be classified together with all existing data.In our categorization of transfer learning, in contrast, we use the term transductive to emphasize concept that in this type of transfer learning, the tasks must be the same and there must be some unlabeled data available in the target domain.This definition covers the work of Arnold et al. [58], since the latter considered domain adaptation, where the difference lies between the marginal probability distributions of source and target data; i.e., the tasks are the same but the domains are different.</p>
<p>Similar to the traditional transductive learning setting, which aims to make the best use of the unlabeled test data for learning, in our classification scheme under transductive transfer learning, we also assume that some target-domain unlabeled data be given.In the above definition of transductive transfer learning, the source and target tasks are the same, which implies that one can adapt the predictive function learned in the source domain for use in the target domain through some unlabeled target-domain data.As mentioned in Section 2.3, this setting can be split to two cases: 1) The feature spaces between the source and target domains are different, X S 6 ¼ X T , and 2) the feature spaces between domains are the same, X S ¼ X T , but the marginal probability distributions of the input data are different, P ðX S Þ 6 ¼ P ðX T Þ.This is similar to the requirements in domain adaptation and sample selection bias.</p>
<p>Most approaches described in the following sections are related to case 2 above.</p>
<p>Transferring the Knowledge of Instances</p>
<p>Most instance-transfer approaches to the transductive transfer learning setting are motivated by importance sampling.To see how importance-sampling-based methods may help in this setting, we first review the problem of empirical risk minimization (ERM) [60].In general, we might want to learn the optimal parameters Ã of the model by minimizing the expected risk,
Ã ¼ arg min 2Â E E ðx;yÞ2P ½lðx; y; Þ;
where lðx; y; Þ is a loss function that depends on the parameter .However, since it is hard to estimate the probability distribution P , we choose to minimize the ERM instead,
Ã ¼ arg min 2Â 1 n X n i¼1 ½lðx i ; y i ; Þ;
where n is size of the training data.</p>
<p>In the transductive transfer learning setting, we want to learn an optimal model for the target domain by minimizing the expected risk, Otherwise, when P ðD S Þ 6 ¼ P ðD T Þ, we need to modify the above optimization problem to learn a model with high generalization ability for the target domain, as follows:
Ã ¼ arg min 2Â X ðx;yÞ2DS P ðD T Þ P ðD S Þ P ðD S Þlðx; y; Þ % arg min 2Â X n S i¼1 P T ðx Ti ; y Ti Þ P S ðx Si ; y Si Þ lðx S i ; y S i ; Þ:ð5Þ
Therefore, by adding different penalty values to each instance ðx Si ; y Si Þ with the corresponding weight
PT ðxT i ;yT i Þ PSðxS i ;yS i Þ ,
we can learn a precise model for the target domain.Furthermore, since P ðY T jX T Þ ¼ P ðY S jX S Þ.Thus, the difference between P ðD S Þ and P ðD T Þ is caused by P ðX S Þ and P ðX T Þ and
P T ðx Ti ; y Ti Þ P S ðx Si ; y Si Þ ¼ P ðx Si Þ P ðx Ti Þ :
If we can estimate
P ðx S i Þ P ðx T i Þ
for each instance, we can solve the transductive transfer learning problems.</p>
<p>There exist various ways to estimate
P ðx S i Þ P ðxT i Þ .
Zadrozny [24] proposed to estimate the terms P ðx Si Þ and P ðx Ti Þ independently by constructing simple classification problems.</p>
<p>Fan et al. [35] further analyzed the problems by using various classifiers to estimate the probability ratio.Huang et al. [32] proposed a kernel-mean matching (KMM) algorithm to learn P ðx S i Þ P ðx T i Þ directly by matching the means between the source domain data and the target domain data in a reproducing-kernel Hilbert space (RKHS).KMM can be rewritten as the following quadratic programming (QP) optimization problem.min 1 2</p>
<p>T K À T s:t: i 2 ½0; B and
X n S i¼1 i À n S n S ;ð6ÞP n T j¼1 kðx i ; x T j Þ, where x i 2 X S S X T , while x Tj 2 X T .
It can be proved that i ¼
P ðx S i Þ P ðx T i Þ [32]
. An advantage of using KMM is that it can avoid performing density estimation of either P ðx S i Þ or P ðx T i Þ, which is difficult when the size of the data set is small.Sugiyama et al. [34] proposed an algorithm known as Kullback-Leibler Importance Estimation Procedure (KLIEP) to estimate P ðx S i Þ P ðxT i Þ directly, based on the minimization of the Kullback-Leibler divergence.can be integrated with cross-validation to perform model selection automatically in two steps: 1) estimating the weights of the source domain data and 2) training models on the reweighted data.Bickel et al. [33] combined the two steps in a unified framework by deriving a kernel-logistic regression classifier.Besides sample reweighting techniques, Dai et al. [28] extended a traditional Naive Bayesian classifier for the transductive transfer learning problems.For more information on importance sampling and reweighting methods for covariate shift or sample selection bias, readers can refer to a recently published book [29] by Quionero-Candela et al.One can also consult a tutorial on Sample Selection Bias by Fan and Sugiyama in ICDM-08. 5</p>
<p>Transferring Knowledge of Feature Representations</p>
<p>Most feature-representation-transfer approaches to the transductive transfer learning setting are under unsupervised learning frameworks.Blitzer et al. [38] proposed a structural correspondence learning (SCL) algorithm, which extends [37], to make use of the unlabeled data from the target domain to extract some relevant features that may reduce the difference between the domains.The first step of SCL is to define a set of pivot features 6 (the number of pivot feature is denoted by m) on the unlabeled data from both domains.Then, SCL removes these pivot features from the data and treats each pivot feature as a new label vector.The m classification problems can be constructed.By assuming each problem can be solved by linear classifier, which is shown as follows:
f l ðxÞ ¼ sgn À w T l Á x Á ; l ¼ 1; . . . ; m:
SCL can learn a matrix W ¼ ½w 1 w 2 . . .w m of parameters.In the third step, singular value decomposition (SVD) is applied to matrix W ¼ ½w 1 w 2 . . .w m .Let W ¼ UDV T , then ¼ U T ½1:h;:</p>
<p>(h is the number of the shared features) is the matrix (linear mapping) whose rows are the top left singular vectors of W . Finally, standard discriminative algorithms can be applied to the augmented feature vector to build models.The augmented feature vector contains all the original feature x i appended with the new shared features x i .As mentioned in [38], if the pivot features are well designed, then the learned mapping encodes the correspondence between the features from the different domains.Although Ben-David et al. [61] showed experimentally that SCL can reduce the difference between domains; how to select the pivot features is difficult and domain dependent.In [38], Blitzer et al. used a heuristic method to select pivot features for natural language processing (NLP) problems, such as tagging of sentences.In their follow-up work, the researchers proposed to use Mutual Information (MI) to choose the pivot features instead of using more heuristic criteria [8].MI-SCL tries to find some pivot features that have high dependence on the labels in the source domain.</p>
<p>Transfer learning in the NLP domain is sometimes referred to as domain adaptation.In this area, Daume ´ [39] proposed a kernel-mapping function for NLP problems, which maps the data from both source and target domains to a high-dimensional feature space, where standard discriminative learning methods are used to train the classifiers.However, the constructed kernel-mapping function is domain knowledge driven.It is not easy to generalize the kernel mapping to other areas or applications.Blitzer et al. [62] analyzed the uniform convergence bounds for algorithms that minimized a convex combination of source and target empirical risks.</p>
<p>In [36], Dai et al. proposed a coclustering-based algorithm to propagate the label information across different domains.In [63], Xing et al. proposed a novel algorithm known as bridged refinement to correct the labels predicted by a shiftunaware classifier toward a target distribution and take the mixture distribution of the training and test data as a bridge to better transfer from the training data to the test data.In [64], Ling et al. proposed a spectral classification framework for cross-domain transfer learning problem, where the objective function is introduced to seek consistency between the in-domain supervision and the out-of-domain intrinsic structure.In [65], Xue et al. proposed a cross-domain text classification algorithm that extended the traditional probabilistic latent semantic analysis (PLSA) algorithm to integrate labeled and unlabeled data from different but related domains, into a unified probabilistic model.The new model is called Topic-bridged PLSA, or TPLSA.</p>
<p>Transfer learning via dimensionality reduction was recently proposed by Pan et al. [66].In this work, Pan et al. exploited the Maximum Mean Discrepancy Embedding 5. Tutorial slides can be found at http://www.cs.columbia.edu/~fan/PPT/ICDM08SampleBias.ppt.</p>
<p>6.The pivot features are domain specific and depend on prior knowledge.</p>
<p>(MMDE) method, originally designed for dimensionality reduction, to learn a low-dimensional space to reduce the difference of distributions between different domains for transductive transfer learning.However, MMDE may suffer from its computational burden.Thus, in [67], Pan et al. further proposed an efficient feature extraction algorithm, known as Transfer Component Analysis (TCA) to overcome the drawback of MMDE.Based on the definition of the unsupervised transfer learning setting, no labeled data are observed in the source and target domains in training.So far, there is little research work on this setting.Recently, Self-taught clustering (STC) [26] and transferred discriminative analysis (TDA) [27] algorithms are proposed to transfer clustering and transfer dimensionality reduction problems, respectively.</p>
<p>UNSUPERVISED TRANSFER LEARNING</p>
<p>Transferring Knowledge of Feature Representations</p>
<p>Dai et al. [26] studied a new case of clustering problems, known as self-taught clustering.Self-taught clustering is an instance of unsupervised transfer learning, which aims at clustering a small collection of unlabeled data in the target domain with the help of a large amount of unlabeled data in the source domain.STC tries to learn a common feature space across domains, which helps in clustering in the target domain.The objective function of STC is shown as follows:
Jð XT ; XS ; ZÞ ¼ IðX T ; ZÞ À Ið XT ; ZÞ þ IðX S ; ZÞ À Ið XS ; ZÞ Â Ã ;ð7Þ
where X S and X T are the source and target domain data, respectively.Z is a shared feature space by X S and X T , and IðÁ; ÁÞ is the mutual information between two random variables.Suppose that there exist three clustering functions C X T : X T !XT , C X S : X S !XS , and C Z : Z !Z, where XT , XS , and Z are corresponding clusters of X T , X S , and Z, respectively.The goal of STC is to learn XT by solving the optimization problem (7):</p>
<p>An iterative algorithm for solving the optimization function (8) was given in [26].</p>
<p>Similarly, Wang et al. [27] proposed a TDA algorithm to solve the transfer dimensionality reduction problem.TDA first applies clustering methods to generate pseudoclass labels for the target unlabeled data.It then applies dimensionality reduction methods to the target data and labeled source data to reduce the dimensions.These two steps run iteratively to find the best subspace for the target data.</p>
<p>TRANSFER BOUNDS AND NEGATIVE TRANSFER</p>
<p>An important issue is to recognize the limit of the power of transfer learning.In [68], Mahmud and Ray analyzed the case of transfer learning using Kolmogorov complexity, where some theoretical bounds are proved.In particular, the authors used conditional Kolmogorov complexity to measure relatedness between tasks and transfer the "right" amount of information in a sequential transfer learning task under a Bayesian framework.</p>
<p>Recently, Eaton et al. [69] proposed a novel graph-based method for knowledge transfer, where the relationships between source tasks are modeled by embedding the set of learned source models in a graph using transferability as the metric.Transferring to a new task proceeds by mapping the problem into the graph and then learning a function on this graph that automatically determines the parameters to transfer to the new learning task.</p>
<p>Negative transfer happens when the source domain data and task contribute to the reduced performance of learning in the target domain.Despite the fact that how to avoid negative transfer is a very important issue, little research work has been published on this topic.Rosenstein et al. [70] empirically showed that if two tasks are too dissimilar, then brute-force transfer may hurt the performance of the target task.Some works have been exploited to analyze relatedness among tasks and task clustering techniques, such as [71], [72], which may help provide guidance on how to avoid negative transfer automatically.Bakker and Heskes [72] adopted a Bayesian approach in which some of the model parameters are shared for all tasks and others more loosely connected through a joint prior distribution that can be learned from the data.Thus, the data are clustered based on the task parameters, where tasks in the same cluster are supposed to be related to each other.Argyriou et al. [73] considered situations in which the learning tasks can be divided into groups.Tasks within each group are related by sharing a low-dimensional representation, which differs among different groups.As a result, tasks within a group can find it easier to transfer useful knowledge.</p>
<p>APPLICATIONS OF TRANSFER LEARNING</p>
<p>Recently, transfer learning techniques have been applied successfully in many real-world applications.Raina et al. [74] and Dai et al. [36], [28] proposed to use transfer learning techniques to learn text data across domains, respectively.Blitzer et al. [38] proposed to use SCL for solving NLP problems.An extension of SCL was proposed in [8] for solving sentiment classification problems.Wu and Dietterich [53] proposed to use both inadequate target domain data and plenty of low quality source domain data for image classification problems.Arnold et al. [58] proposed to use transductive transfer learning methods to solve name-entity recognition problems.In [75], [76], [77], [78], [79], transfer learning techniques are proposed to extract knowledge from WiFi localization models across time periods, space, and mobile devices, to benefit WiFi localization tasks in other settings.Zhuo et al. [80] studied how to transfer domain knowledge to learn relational action models across domains in automated planning.</p>
<p>In [81], Raykar et al. proposed a novel Bayesian multipleinstance learning algorithm, which can automatically identify the relevant feature subset and use inductive transfer for learning multiple, but conceptually related, classifiers, for computer aided design (CAD).In [82], Ling et al. proposed an information-theoretic approach for transfer learning to address the cross-language classification problem for translating webpages from English to Chinese.The approach addressed the problem when there are plenty of labeled English text data whereas there are only a small number of labeled Chinese text documents.Transfer learning across the two feature spaces are achieved by designing a suitable mapping function as a bridge.</p>
<p>So far, there are at least two international competitions based on transfer learning, which made available some much needed public data.In the ECML/PKDD-2006 discovery challenge, 8 the task was to handle personalized spam filtering and generalization across related learning tasks.</p>
<p>For training a spam-filtering system, we need to collect a lot of e-mails from a group of users with corresponding labels: spam or not spam, and train a classifier based on these data.For a new e-mail user, we might want to adapt the learned model for the user.The challenge is that the distributions of emails for the first set of users and the new user are different.Thus, this problem can be modeled as an inductive transfer learning problem, which aims to adapt an old spam-filtering model to a new situation with fewer training data and less training time.</p>
<p>A second data set was made available through the ICDM-2007 Contest, in which a task was to estimate a WiFi client's indoor locations using the WiFi signal data obtained over different periods of time [83].Since the values of WiFi signal strength may be a function of time, space, and devices, distributions of WiFi data over different time periods may be very different.Thus, transfer learning must be designed to reduce the data relabeling effort.</p>
<p>Data sets for transfer learning.So far, several data sets have been published for transfer learning research.We denote the text mining data sets, Email spam-filtering data set, the WiFi localization over time periods data set, and the Sentiment classification data set by Text, E-mail, WiFi, and Sen, respectively.</p>
<ol>
<li>Text.Three data sets, 20 Newsgroups, SRAA, and Reuters-21578, 9 have been preprocessed for a transfer learning setting by some researchers.The data in these data sets are categorized to a hierarchical structure.Data from different subcategories under the same parent category are considered to be from different but related domains.The task is to predict the labels of the parent category.Contest. 10 The data were collected inside a building for localization around 145:5 Â 37:5 m 2 in two different time periods.4. Sen. This data set was first used in [8]. 11This data set contains product reviews downloaded from Amazon.com from four product types (domains): Kitchen, Books, DVDs, and Electronics.Each domain has several thousand reviews, but the exact number varies by domain.Reviews contain star ratings (1-5 stars).Empirical evaluation.To show how much benefit transfer learning methods can bring as compared to traditional learning methods, researchers have used some public data sets.We show a list taken from some published transfer learning papers in Table 5.In [6], [84], [49], the authors used the 20 Newsgroups data 12 as one of the evaluation data sets.Due to the differences in the preprocessing steps of the algorithms by different researchers, it is hard to compare the proposed methods directly.Thus, we denote them by 20-Newsgroups 1 , 20-Newsgroups 2 , and 20-Newsgroups 3 , respectively, and show the comparison results between the proposed transfer learning methods and nontransfer learning methods in the table.</li>
</ol>
<p>On the 20 Newsgroups 1 data, Dai et al. [6] showed the comparison experiments between standard SVM and the proposed TrAdaBoost algorithm.On 20 Newsgroups 2 , Shi et al. [84] applied an active learning algorithm to select important instances for transfer learning (AcTraK) with TrAdaBoost and standard SVM.Gao et al. [49] evaluated their proposed locally weighted ensemble learning algorithms, pLWE and LWE, on the 20 Newsgroups 3 , compared to SVM and Logistic Regression (LR).</p>
<p>In addition, in the table, we also show the comparison results on the sentiment classification data set reported in [8].On this data set, SGD denotes the stochastic gradientdescent algorithm with Huber loss, SCL represents a linear predictor on the new representations learned by Structural Correspondence Learning algorithm, and SCL-MI is an extension of SCL by applying Mutual Information to select the pivot features for the SCL algorithm.</p>
<p>Finally, on the WiFi localization data set, we show the comparison results reported in [67], where the baseline is a regularized least square regression model (RLSR), which is a standard regression model, and KPCA, which represents to apply RLSR on the new representations of the data learned by Kernel Principle Component Analysis.The compared transfer learning methods include KMM and the proposed algorithm, TCA.For more detail about the experimental results, the readers may refer to the reference papers showed in the table.From these comparison results, we can find that the transfer learning methods designed appropriately for real-world applications can indeed improve the performance significantly compared to the nontransfer learning methods.</p>
<p>Toolboxes for transfer learning.Researchers at UC Berkeley provided a MATLAB toolkit for transfer learning. 13The toolkit contains algorithms and benchmark data sets for transfer learning.In addition, it provides a standard platform for developing and testing new algorithms for transfer learning.</p>
<p>Other Applications of Transfer Learning</p>
<p>Transfer learning has found many applications in sequential machine learning as well.For example, Kuhlmann and Stone [85] proposed a graph-based method for identifying previously encountered games, and applied this technique to automate domain mapping for value function transfer and speed up reinforcement learning on variants of previously played games.A new approach to transfer between entirely different feature spaces is proposed in translated learning, which is made possible by learning a mapping function for bridging features in two entirely different domains (images and text) [86].Finally, Li et al. [87], [88] have applied transfer learning to collaborative filtering problems to solve the cold start and sparsity problems.In [87], Li et al. learned a shared rating-pattern mixture model, known as a Rating-Matrix Generative Model (RMGM), in terms of the latent user-and itemcluster variables.RMGM bridges multiple rating matrices from different domains by mapping the users and items in each rating matrix onto the shared latent user and item spaces in order to transfer useful knowledge.In [88], they applied coclustering algorithms on users and items in an auxiliary rating matrix.They then constructed a clusterlevel rating matrix known as a codebook.By assuming the target rating matrix (on movies) is related to the auxiliary one (on books), the target domain can be reconstructed by expanding the codebook, completing the knowledge transfer process.</p>
<p>CONCLUSIONS</p>
<p>In this survey paper, we have reviewed several current trends of transfer learning.Transfer learning is classified to three different settings: inductive transfer learning, transductive transfer learning, and unsupervised transfer learning.Most previous works focused on the former two settings.Unsupervised transfer learning may attract more and more attention in the future.</p>
<p>Furthermore, each of the approaches to transfer learning can be classified into four contexts based on "what to transfer" in learning.They include the instance-transfer approach, the feature-representation-transfer approach, the parameter-transfer approach, and the relational-knowledgetransfer approach, respectively.The former three contexts have an i.i.d.assumption on the data while the last context deals with transfer learning on relational data.Most of these approaches assume that the selected source domain is related to the target domain.</p>
<p>TABLE 5 Comparison between Transfer Learning and Nontransfer Learning Methods</p>
<p>In the future, several important research issues need to be addressed.First, how to avoid negative transfer is an open problem.As mentioned in Section 6, many proposed transfer learning algorithms assume that the source and target domains are related to each other in some sense.However, if the assumption does not hold, negative transfer may happen, which may cause the learner to perform worse than no transferring at all.Thus, how to make sure that no negative transfer happens is a crucial issue in transfer learning.In order to avoid negative transfer learning, we need to first study transferability between source domains or tasks and target domains or tasks.Based on suitable transferability measures, we can then select relevant source domains or tasks to extract knowledge from for learning the target tasks.To define the transferability between domains and tasks, we also need to define the criteria to measure the similarity between domains or tasks.Based on the distance measures, we can then cluster domains or tasks, which may help measure transferability.A related issue is when an entire domain cannot be used for transfer learning, whether we can still transfer part of the domain for useful learning in the target domain.</p>
<p>In addition, most existing transfer learning algorithms so far focused on improving generalization across different distributions between source and target domains or tasks.In doing so, they assumed that the feature spaces between the source and target domains are the same.However, in many applications, we may wish to transfer knowledge across domains or tasks that have different feature spaces, and transfer from multiple such source domains.We refer to this type of transfer learning as heterogeneous transfer learning.</p>
<p>Finally, so far, transfer learning techniques have been mainly applied to small scale applications with a limited variety, such as sensor-network-based localization, text classification, and image classification problems.In the future, transfer learning techniques will be widely used to solve other challenging applications, such as video classification, social network analysis, and logical inference.</p>
<p>Fig. 1 .
1
Fig. 1.Different learning processes between (a) traditional machine learning and (b) transfer learning.</p>
<p>categorize transfer learning under three subsettings, inductive transfer learning, transductive transfer learning, and unsupervised transfer learning, based on different situations between the source and target domains and tasks.</p>
<p>Definition 2 (
2
Inductive Transfer Learning).Given a source domain D S and a learning task T S , a target domain D T and a learning task T T , inductive transfer learning aims to help improve the learning of the target predictive function f T ðÁÞ in D T using the knowledge in D S and T S , where T S 6 ¼ T T .</p>
<p>Fig. 2 .
2
Fig. 2.An overview of different settings of transfer.</p>
<p>Definition 3 (
3
Transductive Transfer Learning).Given a source domain D S and a corresponding learning task T S , a target domain D T and a corresponding learning task T T , transductive transfer learning aims to improve the learning of the target predictive function f T ðÁÞ in D T using the knowledge in D S and T S , where D S 6 ¼ D T and T S ¼ T T .In addition, some unlabeled target-domain data must be available at training time.</p>
<p>P</p>
<p>ðD T Þlðx; y; Þ: However, since no labeled data in the target domain are observed in training data, we have to learn a model from the source domain data instead.If P ðD S Þ ¼ P ðD T Þ, then we may simply learn the model by solving the following optimization problem for use in the target domain, Ã ¼ arg min 2Â X ðx;yÞ2D S P ðD S Þlðx; y; Þ:</p>
<p>Definition 4 (
4
Unsupervised Transfer Learning).Given a source domain D S with a learning task T S , a target domain D T and a corresponding learning task T T , unsupervised transfer learning aims to help improve the learning of the target predictive function f T ðÁÞ 7 in D T using the knowledge in D S and T S , where T S 6 ¼ T T and Y S and Y T are not observable.</p>
<p>Z</p>
<p>Jð XT ; XS ; ZÞ:</p>
<p>TABLE 1
1
Relationship between Traditional Machine Learning and Various Transfer Learning Settings</p>
<p>TABLE 2 Different
2
Settings of Transfer Learning</p>
<p>TABLE 3 Different
3
Approaches to Transfer Learning</p>
<p>TABLE 4 Different
4
Approaches Used in Different Settings</p>
<ol>
<li>E-mail.This data set is provided by the 2006 ECML/ PKDD discovery challenge.3. WiFi.This data set is provided by the ICDM-2007</li>
</ol>
<p>. http://www.cs.utexas.edu/~mtaylor/AAAI08TL/.
. In unsupervised transfer learning, the predicted labels are latent variables, such as clusters or reduced dimensions.
PAN AND YANG: A SURVEY ON TRANSFER LEARNING
ACKNOWLEDGMENTSThe authors thank the support of Hong Kong CERG Project 621307 and a grant from NEC China Lab.
Top 10 Algorithms in Data Mining. X Wu, V Kumar, J R Quinlan, J Ghosh, Q Yang, H Motoda, G J Mclachlan, A F M Ng, B Liu, P S Yu, Z.-H Zhou, M Steinbach, D J Hand, D Steinberg, Knowledge and Information Systems. 1412008</p>
<p>10 Challenging Problems in Data Mining Research. Q Yang, X Wu, Int'l J. Information Technology and Decision Making. 542006</p>
<p>Text Classification without Negative Examples Revisit. G P C Fung, J X Yu, H Lu, P S Yu, IEEE Trans. Knowledge and Data Eng. 181Jan. 2006</p>
<p>A New Text Categorization Technique Using Distributional Clustering and Learning Logic. H , Al Mubaid, S A Umair, IEEE Trans. Knowledge and Data Eng. 189Sept. 2006</p>
<p>Combining Subclassifiers in Text Categorization: A DST-Based Solution and a Case Study. K Sarinnapakorn, M Kubat, IEEE Trans. Knowledge and Data Eng. 1912Dec. 2007</p>
<p>Boosting for Transfer Learning. W Dai, Q Yang, G Xue, Y Yu, Proc. 24th Int'l Conf. Machine Learning. 24th Int'l Conf. Machine LearningJune 2007</p>
<p>Transfer Learning for WiFi-Based Indoor Localization. S J Pan, V W Zheng, Q Yang, D H Hu, Proc. Workshop Transfer Learning for Complex Task of the 23rd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial Intelligence. Workshop Transfer Learning for Complex Task of the 23rd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial IntelligenceJuly 2008</p>
<p>Biographies, Bollywood, Boom-Boxes and Blenders: Domain Adaptation for Sentiment Classification. J Blitzer, M Dredze, F Pereira, Proc. 45th Ann. Meeting of the Assoc. Computational Linguistics. 45th Ann. Meeting of the Assoc. Computational Linguistics2007</p>
<p>Transfer Learning in Reinforcement Learning Problems through Partial Policy Recycling. J Ramon, K Driessens, T Croonenborghs, Proc. 18th European Conf. Machine Learning (ECML '07). 18th European Conf. Machine Learning (ECML '07)2007</p>
<p>Cross-Domain Transfer for Reinforcement Learning. M E Taylor, P Stone, Proc. 24th Int'l Conf. Machine Learning (ICML '07). 24th Int'l Conf. Machine Learning (ICML '07)2007</p>
<p>Efficient Classification across Multiple Database Relations: A Crossmine Approach. X Yin, J Han, J Yang, P S Yu, IEEE Trans. Knowledge and Data Eng. 186June 2006</p>
<p>Classifier Ensembles with a Random Linear Oracle. L I Kuncheva, J J Rodrłguez, IEEE Trans. Knowledge and Data Eng. 194Apr. 2007</p>
<p>A Lazy Approach to Associative Classification. E Baralis, S Chiusano, P Garza, IEEE Trans. Knowledge and Data Eng. 202Feb. 2008</p>
<p>Semi-Supervised Learning Literature Survey. X Zhu, 15302006Univ. of Wisconsin-MadisonTechnical Report</p>
<p>Text Classification from Labeled and Unlabeled Documents Using EM. K Nigam, A K Mccallum, S Thrun, T Mitchell, Machine Learning. 200039</p>
<p>Combining Labeled and Unlabeled Data with Co-Training. A Blum, T Mitchell, Proc. 11th Ann. Conf. Computational Learning Theory. 11th Ann. Conf. Computational Learning Theory1998</p>
<p>Transductive Inference for Text Classification Using Support Vector Machines. T Joachims, Proc. 16th Int'l Conf. Machine Learning. 16th Int'l Conf. Machine Learning1999</p>
<p>Class Noise Handling for Effective Cost-Sensitive Learning by Cost-Guided Iterative Classification Filtering. X Zhu, X Wu, IEEE Trans. Knowledge and Data Eng. 1810Oct. 2006</p>
<p>Test-Cost Sensitive Classification on Data with Missing Values. Q Yang, C Ling, X Chai, R Pan, IEEE Trans. Knowledge and Data Eng. 185May 2006</p>
<p>Multitask Learning. R Caruana, Machine Learning. 199728</p>
<p>Self-Taught Learning: Transfer Learning from Unlabeled Data. R Raina, A Battle, H Lee, B Packer, A Y Ng, Proc. 24th Int'l Conf. Machine Learning. 24th Int'l Conf. Machine LearningJune 2007</p>
<p>Domain Adaptation for Statistical Classifiers. H Daume, D Marcu, J. Artificial Intelligence Research. 262006</p>
<p>Learning and Evaluating Classifiers under Sample Selection Bias. B Zadrozny, Proc. 21st Int'l Conf. Machine Learning. 21st Int'l Conf. Machine LearningJuly 2004</p>
<p>Improving Predictive Inference under Covariate Shift by Weighting the Log-Likelihood Function. H Shimodaira, J. Statistical Planning and Inference. 902000</p>
<p>Self-Taught Clustering. W Dai, Q Yang, G Xue, Y Yu, Proc. 25th Int'l Conf. Machine Learning. 25th Int'l Conf. Machine LearningJuly 2008</p>
<p>Transferred Dimensionality Reduction. Z Wang, Y Song, C Zhang, Proc. European Conf. Machine Learning and Knowledge Discovery in Databases (ECML/PKDD '08). European Conf. Machine Learning and Knowledge Discovery in Databases (ECML/PKDD '08)Sept. 2008</p>
<p>Transferring Naive Bayes Classifiers for Text Classification. W Dai, G Xue, Q Yang, Y Yu, Proc. 22nd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial Intelligence. 22nd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial IntelligenceJuly 2007</p>
<p>J Quionero-Candela, M Sugiyama, A Schwaighofer, N D Lawrence, Dataset Shift in Machine Learning. MIT Press2009</p>
<p>Instance Weighting for Domain Adaptation in NLP. J Jiang, C Zhai, Proc. 45th Ann. Meeting of the Assoc. Computational Linguistics. 45th Ann. Meeting of the Assoc. Computational LinguisticsJune 2007</p>
<p>Logistic Regression with an Auxiliary Data Source. X Liao, Y Xue, L Carin, Proc. 21st Int'l Conf. Machine Learning. 21st Int'l Conf. Machine LearningAug. 2005</p>
<p>Correcting Sample Selection Bias by Unlabeled Data. J Huang, A Smola, A Gretton, K M Borgwardt, B Scho ¨lkopf, Proc. 19th Ann. Conf. Neural Information Processing Systems. 19th Ann. Conf. Neural Information essing Systems2007</p>
<p>Discriminative Learning for Differing Training and Test Distributions. S Bickel, M Bru, T Scheffer, Proc. 24th Int'l Conf. Machine Learning. 24th Int'l Conf. Machine Learning2007</p>
<p>Direct Importance Estimation with Model Selection and its Application to Covariate Shift Adaptation. M Sugiyama, S Nakajima, H Kashima, P V Buenau, M Kawanabe, Proc. 20th Ann. Conf. Neural Information Processing Systems. 20th Ann. Conf. Neural Information essing SystemsDec. 2008</p>
<p>An Improved Categorization of Classifier's Sensitivity on Sample Selection Bias. W Fan, I Davidson, B Zadrozny, P S Yu, Proc. Fifth IEEE Int'l Conf. Data Mining. Fifth IEEE Int'l Conf. Data Mining2005</p>
<p>Co-Clustering Based Classification for Out-of-Domain Documents. W Dai, G Xue, Q Yang, Y Yu, Proc. 13th ACM SIGKDD Int'l Conf. Knowledge Discovery and Data Mining. 13th ACM SIGKDD Int'l Conf. Knowledge Discovery and Data MiningAug. 2007</p>
<p>A High-Performance Semi-Supervised Learning Method for Text Chunking. R K Ando, T Zhang, Proc. 43rd Ann. Meeting on Assoc. for Computational Linguistics. 43rd Ann. Meeting on Assoc. for Computational Linguistics2005</p>
<p>Domain Adaptation with Structural Correspondence Learning. J Blitzer, R Mcdonald, F Pereira, Proc. Conf. Empirical Methods in Natural Language. Conf. Empirical Methods in Natural LanguageJuly 2006</p>
<p>Frustratingly Easy Domain Adaptation. H Daume, Proc. 45th Ann. Meeting of the Assoc. Computational Linguistics. 45th Ann. Meeting of the Assoc. Computational LinguisticsJune 2007</p>
<p>Multi-Task Feature Learning. A Argyriou, T Evgeniou, M Pontil, Proc. 19th Ann. Conf. Neural Information Processing Systems. 19th Ann. Conf. Neural Information essing SystemsDec. 2007</p>
<p>A Spectral Regularization Framework for Multi-Task Structure Learning. A Argyriou, C A Micchelli, M Pontil, Y Ying, Proc. 20th Ann. Conf. Neural Information Processing Systems. 20th Ann. Conf. Neural Information essing Systems2008</p>
<p>Learning a Meta-Level Prior for Feature Relevance from Multiple Related Tasks. S I Lee, V Chatalbashev, D Vickrey, D Koller, Proc. 24th Int'l Conf. Machine Learning. 24th Int'l Conf. Machine LearningJuly 2007</p>
<p>Multi-Task Feature and Kernel Selection for SVMs. T Jebara, Proc. 21st Int'l Conf. Machine Learning. 21st Int'l Conf. Machine LearningJuly 2004</p>
<p>Manifold Alignment Using Procrustes Analysis. C Wang, S Mahadevan, Proc. 25th Int'l Conf. Machine Learning. 25th Int'l Conf. Machine LearningJuly 2008</p>
<p>Learning to Learn with the Informative Vector Machine. N D Lawrence, J C Platt, Proc. 21st Int'l Conf. Machine Learning. 21st Int'l Conf. Machine LearningJuly 2004</p>
<p>Multi-Task Gaussian Process Prediction. E Bonilla, K M Chai, C Williams, Proc. 20th Ann. Conf. Neural Information Processing Systems. 20th Ann. Conf. Neural Information essing Systems2008</p>
<p>Learning Gaussian Process Kernels via Hierarchical Bayes. A Schwaighofer, V Tresp, K Yu, Proc. 17th Ann. Conf. Neural Information Processing Systems. 17th Ann. Conf. Neural Information essing Systems2005</p>
<p>Regularized Multi-Task Learning. T Evgeniou, M Pontil, Proc. 10th ACM SIGKDD Int'l Conf. Knowledge Discovery and Data Mining. 10th ACM SIGKDD Int'l Conf. Knowledge Discovery and Data MiningAug. 2004</p>
<p>Knowledge Transfer via Multiple Model Local Structure Mapping. J Gao, W Fan, J Jiang, J Han, Proc. 14th ACM SIGKDD Int'l Conf. Knowledge Discovery and Data Mining. 14th ACM SIGKDD Int'l Conf. Knowledge Discovery and Data MiningAug. 2008</p>
<p>Mapping and Revising Markov Logic Networks for Transfer Learning. L Mihalkova, T Huynh, R J Mooney, Proc. 22nd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial Intelligence. 22nd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial IntelligenceJuly 2007</p>
<p>Transfer Learning by Mapping with Minimal Target Data. L Mihalkova, R J Mooney, Proc. Assoc. for the Advancement of Artificial Intelligence (AAAI '08) Workshop Transfer Learning for Complex Tasks. Assoc. for the Advancement of Artificial Intelligence (AAAI '08) Workshop Transfer Learning for Complex TasksJuly 2008</p>
<p>Deep Transfer via Second-Order Markov Logic. J Davis, P Domingos, Proc. Assoc. for the Advancement of Artificial Intelligence (AAAI '08) Workshop Transfer Learning for Complex Tasks. Assoc. for the Advancement of Artificial Intelligence (AAAI '08) Workshop Transfer Learning for Complex TasksJuly 2008</p>
<p>Improving SVM Accuracy by Training on Auxiliary Data Sources. P Wu, T G Dietterich, Proc. 21st Int'l Conf. Machine Learning. 21st Int'l Conf. Machine LearningJuly 2004</p>
<p>Kernel-Based Inductive Transfer. U Ru, S Kramer, Proc. European Conf. Machine Learning and Knowledge Discovery in Databases (ECML/PKDD '08). European Conf. Machine Learning and Knowledge Discovery in Databases (ECML/PKDD '08)Sept. 2008</p>
<p>Efficient Sparse Coding Algorithms. H Lee, A Battle, R Raina, A Y Ng, Proc. 19th Ann. Conf. Neural Information Processing Systems. 19th Ann. Conf. Neural Information essing Systems2007</p>
<p>Markov Logic Networks. M Richardson, P Domingos, Machine Learning J. 621/22006</p>
<p>Theory Refinement of Bayesian Networks with Hidden Variables. S Ramachandran, R J Mooney, Proc. 14th Int'l Conf. Machine Learning. 14th Int'l Conf. Machine LearningJuly 1998</p>
<p>A Comparative Study of Methods for Transductive Transfer Learning. A Arnold, R Nallapati, W W Cohen, Proc. Seventh IEEE Int'l Conf. Data Mining Workshops. Seventh IEEE Int'l Conf. Data Mining Workshops2007</p>
<p>Transductive Inference for Text Classification Using Support Vector Machines. T Joachims, Proc. 16th Int'l Conf. Machine Learning. 16th Int'l Conf. Machine Learning1999</p>
<p>V N Vapnik, Statistical Learning Theory. Wiley InterscienceSept. 1998</p>
<p>Analysis of Representations for Domain Adaptation. S Ben-David, J Blitzer, K Crammer, F Pereira, Proc. 20th Ann. Conf. Neural Information Processing Systems. 20th Ann. Conf. Neural Information essing Systems2007</p>
<p>Learning Bounds for Domain Adaptation. J Blitzer, K Crammer, A Kulesza, F Pereira, J Wortman, Proc. 21st Ann. Conf. Neural Information Processing Systems. 21st Ann. Conf. Neural Information essing Systems2008</p>
<p>Bridged Refinement for Transfer Learning. D Xing, W Dai, G.-R Xue, Y Yu, Proc. 11th European Conf. Principles and Practice of Knowledge Discovery in Databases. 11th European Conf. Principles and Practice of Knowledge Discovery in DatabasesSept. 2007</p>
<p>Spectral Domain-Transfer Learning. X Ling, W Dai, G -R. Xue, Q Yang, Y Yu, Proc. 14th ACM SIGKDD Int'l Conf. Knowledge Discovery and Data Mining. 14th ACM SIGKDD Int'l Conf. Knowledge Discovery and Data MiningAug. 2008</p>
<p>Topic-Bridged PLSA for Cross-Domain Text Classification. G.-R Xue, W Dai, Q Yang, Y Yu, Proc. 31st Ann. Int'l ACM SIGIR Conf. Research and Development in Information Retrieval. 31st Ann. Int'l ACM SIGIR Conf. Research and Development in Information RetrievalJuly 2008</p>
<p>Transfer Learning via Dimensionality Reduction. S J Pan, J T Kwok, Q Yang, Proc. 23rd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial Intelligence. 23rd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial IntelligenceJuly 2008</p>
<p>Domain Adaptation via Transfer Component Analysis. S J Pan, I W Tsang, J T Kwok, Q Yang, Proc. 21st Int'l Joint Conf. 21st Int'l Joint Conf2009</p>
<p>Transfer Learning Using Kolmogorov Complexity: Basic Theory and Empirical Evaluations. M M H Mahmud, S R Ray, Proc. 20th Ann. Conf. Neural Information Processing Systems. 20th Ann. Conf. Neural Information essing Systems2008</p>
<p>Modeling Transfer Relationships between Learning Tasks for Improved Inductive Transfer. E Eaton, M Desjardins, T Lane, Proc. European Conf. Machine Learning and Knowledge Discovery in Databases (ECML/PKDD '08). European Conf. Machine Learning and Knowledge Discovery in Databases (ECML/PKDD '08)Sept. 2008</p>
<p>To Transfer or Not to Transfer. M T Rosenstein, Z Marx, L P Kaelbling, Proc. Conf. Neural Information Processing Systems (NIPS '05) Workshop Inductive Transfer: 10 Years Later. Conf. Neural Information essing Systems (NIPS '05) Workshop Inductive Transfer: 10 Years LaterDec. 2005</p>
<p>Exploiting Task Relatedness for Multiple Task Learning. S Ben-David, R Schuller, Proc. 16th Ann. Conf. Learning Theory. 16th Ann. Conf. Learning Theory2003</p>
<p>Task Clustering and Gating for Bayesian Multitask Learning. B Bakker, T Heskes, J. Machine Learning Research. 42003</p>
<p>An Algorithm for Transfer Learning in a Heterogeneous Environment. A Argyriou, A Maurer, M Pontil, Proc. European Conf. Machine Learning and Knowledge Discovery in Databases (ECML/PKDD '08). European Conf. Machine Learning and Knowledge Discovery in Databases (ECML/PKDD '08)Sept. 2008</p>
<p>Constructing Informative Priors Using Transfer Learning. R Raina, A Y Ng, D Koller, Proc. 23rd Int'l Conf. Machine Learning. 23rd Int'l Conf. Machine LearningJune 2006</p>
<p>Adaptive Temporal Radio Maps for Indoor Location Estimation. J Yin, Q Yang, L M Ni, Proc. Third IEEE Int'l Conf. Pervasive Computing and Comm. Third IEEE Int'l Conf. Pervasive Computing and CommMar. 2005</p>
<p>Adaptive Localization in a Dynamic WiFi Environment through Multi-View Learning. S J Pan, J T Kwok, Q Yang, J J Pan, Proc. 22nd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial Intelligence. 22nd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial IntelligenceJuly 2007</p>
<p>Transferring Localization Models over Time. V W Zheng, Q Yang, W Xiang, D Shen, Proc. 23rd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial Intelligence. 23rd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial IntelligenceJuly 2008</p>
<p>Transferring Localization Models across Space. S J Pan, D Shen, Q Yang, J T Kwok, Proc. 23rd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial Intelligence. 23rd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial IntelligenceJuly 2008</p>
<p>Transferring Multi-Device Localization Models Using Latent Multi-Task Learning. V W Zheng, S J Pan, Q Yang, J J Pan, Proc. 23rd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial Intelligence. 23rd Assoc. for the Advancement of Artificial Intelligence (AAAI) Conf. Artificial IntelligenceJuly 2008</p>
<p>Transferring Knowledge from Another Domain for Learning Action Models. H Zhuo, Q Yang, D H Hu, L Li, Proc. 10th Pacific Rim Int'l Conf. Artificial Intelligence. 10th Pacific Rim Int'l Conf. Artificial IntelligenceDec. 2008</p>
<p>Bayesian Multiple Instance Learning: Automatic Feature Selection and Inductive Transfer. V C Raykar, B Krishnapuram, J Bi, M Dundar, R B Rao, Proc. 25th Int'l Conf. Machine Learning. 25th Int'l Conf. Machine LearningJuly 2008</p>
<p>Can Chinese Web Pages be Classified with English Data Source?. X Ling, G -R. Xue, W Dai, Y Jiang, Q Yang, Y Yu, Proc. 17th Int'l Conf. World Wide Web. 17th Int'l Conf. World Wide WebApr. 2008</p>
<p>Estimating Location Using Wi-Fi. Q Yang, S J Pan, V W Zheng, IEEE Intelligent Systems. 231Jan./Feb. 2008</p>
<p>Actively Transfer Domain Knowledge. X Shi, W Fan, J Ren, Proc. European Conf. Machine Learning and Knowledge Discovery in Databases (ECML/PKDD '08). European Conf. Machine Learning and Knowledge Discovery in Databases (ECML/PKDD '08)Sept. 2008</p>
<p>Graph-Based Domain Mapping for Transfer Learning in General Games. G Kuhlmann, P Stone, Proc. 18th European Conf. Machine Learning. 18th European Conf. Machine LearningSept. 2007</p>
<p>Translated Learning. W Dai, Y Chen, G -R. Xue, Q Yang, Y Yu, Proc. 21st Ann. Conf. Neural Information Processing Systems. 21st Ann. Conf. Neural Information essing Systems2008</p>
<p>Transfer Learning for Collaborative Filtering via a Rating-Matrix Generative Model. B Li, Q Yang, X Xue, Proc. 26th Int'l Conf. Machine Learning. 26th Int'l Conf. Machine LearningJune 2009</p>
<p>He is a faculty member in Hong Kong University of Science and Technology's Department of Computer Science and Engineering. His research interests are data mining and machine learning, AI planning, and sensor-based activity recognition. He is a fellow of the IEEE. B Li, Q Yang, X Xue, Sinno Jialin Pan received the MS and BS degrees from the Applied Mathematics Department. China; College ParkJuly 2009. 2003 and 2005Sun Yat-sen University ; Computer Science and Engineering, the Hong Kong University of Science and TechnologyMore details about his research and background can be. For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8624 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8624</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8624</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278769016</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.14615v1.pdf" target="_blank">SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas</a></p>
                <p><strong>Paper Abstract:</strong> We introduce SATBench, a benchmark for evaluating the logical reasoning capabilities of large language models (LLMs) through logical puzzles derived from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on inference rule-based reasoning, which often involves deducing conclusions from a set of premises, our approach leverages the search-based nature of SAT problems, where the objec-tive is to find a solution that fulfills a specified set of logical constraints. Each instance in SAT-Bench is generated from a SAT formula, then translated into a puzzle using LLMs. The generation process is fully automated and allows for adjustable difficulty by varying the number of clauses. All 2100 puzzles are validated through both LLM-based and solver-based consistency checks, with human validation on a subset. Experimental results show that even the strongest model, o4-mini, achieves only 65.0% accuracy on hard UNSAT problems, close to the random baseline of 50%. Our error analysis reveals systematic failures such as satisfiability bias, context inconsistency, and condition omission, highlighting limitations of current LLMs in search-based logical reasoning. Our code and data are publicly available at https: //github.com/Anjiang-Wei/SATBench</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8624.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8624.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o4-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o4-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary, high-performing large language model evaluated in this paper; the best-performing model on SATBench in zero-shot prompting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o4-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary LLM (family: OpenAI / GPT-4o family referenced in the paper). The paper does not provide architecture or training-data details for this specific variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SATBench (SAT-derived natural-language logical puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Search-based logical reasoning: natural-language puzzles automatically generated from Boolean SAT formulas (CNF). Tasks require predicting satisfiable/unsatisfiable and producing a valid variable assignment (for SAT) or a reasoning trace/UNSAT rationale (for UNSAT).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting with chain-of-thought style reasoning traces required; evaluation prohibits external tools. Additionally, the paper tests error-aware prompting for failure correction (prompt engineering); reasoning traces judged by a separate LLM (GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Top accuracy on SATBench zero-shot (reported overall accuracy 89.3% in Table 3). On hard instances overall accuracy falls (reported ~78.0% on hard instances); on hard UNSAT subset accuracy reported as 65.0% (close to the 50% random baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to a random baseline of 50%; substantially above random overall but only modestly above random on hard instances (hard UNSAT ~65%). Error-aware prompting corrected 60.4% of previously failing o4-mini cases in targeted re-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Systematic failures include satisfiability bias (declaring SAT without a valid assignment), context inconsistency (contradictory assignments in a trace), condition omission (ignoring clauses), and reliance on spurious priors. Performance degrades sharply with clause count; plateaus near random on hardest problems.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Even very strong LLMs like o4-mini struggle with search-based logical reasoning when expressed in natural language: they can achieve high overall accuracy but fail on hard UNSAT and produce unreliable traces. Error-aware prompting yields substantial corrections, indicating predictable failure modes amenable to prompt engineering, but core search/backtracking weaknesses remain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8624.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8624.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-focused LLM (DeepSeek family) evaluated on SATBench that achieves high accuracy and benefits from targeted prompt corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recent DeepSeek family LLM optimized for reasoning (paper references DeepSeek technical reports); specific architecture/training details are not provided in SATBench beyond family name.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SATBench (SAT-derived natural-language logical puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Search-based satisfiability prediction and trace generation from CNF-to-natural-language puzzles; requires constructing satisfying assignments or demonstrating unsatisfiability.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting with required reasoning traces; analysis includes error-aware prompting which explicitly reminds models of common pitfalls.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy 87.8% on SATBench (stated in main results). Performance drops with difficulty; on hard problems accuracy approaches random baseline ranges reported across families.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to random baseline (50%). Error-aware prompting corrected 73.2% of DeepSeek-R1's previously failing cases in a targeted re-evaluation, indicating susceptibility to prompt fixes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Exhibits satisfiability bias, condition omission, and context inconsistency similar to other models; performance plateaus on hard cases despite model size/family improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>DeepSeek-R1 performs strongly overall but shares the same systematic weaknesses as other LLMs on hard search-based logical tasks; prompting that makes failure modes explicit can substantially recover previously incorrect predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8624.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8624.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Another DeepSeek-family model evaluated on SATBench with strong performance but degraded behavior on the hardest instances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Member of the DeepSeek family (reasoning-focused LLMs); paper does not specify architecture or training corpora for this variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SATBench (SAT-derived natural-language logical puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluate satisfiability and provide reasoning traces for natural-language puzzles generated from CNF SAT formulas; focuses on search-like reasoning (existential SAT vs universal UNSAT).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting for satisfiability prediction with required reasoning traces; evaluated with an LLM-as-judge (GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy 84.0% on SATBench (stated in main results). Performance decreases with clause count; hard instances approach random-baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to random baseline (50%); better overall but similar degradation on hard clauses. Prompt corrections yield improvements (paper reports general gains for error-aware prompts across models).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same categories of failures: satisfiability bias (invalid assignments), context inconsistency, condition omission, and spurious priors; poor scaling on hardest problems.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Performs well on easier problems but struggles with search/backtracking style reasoning required for challenging UNSAT cases; targeted prompting helps but core limitations remain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8624.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8624.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary GPT-4o family model used both as an evaluated model and as the primary LLM for dataset generation and judging reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI model (GPT-4o family). In this paper GPT-4o is used as one of the evaluated models and also as the LLM for generation/validation/judging; architecture/training specifics are not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SATBench (SAT-derived natural-language logical puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language SAT puzzles requiring satisfiability classification and trace generation; generation pipeline uses LLMs to convert CNF to stories and back-translate for solver validation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting for evaluation; GPT-4o additionally used for LLM-based validation, story generation, and as the judge for reasoning-trace correctness. External SAT solvers used only during dataset generation/validation (not allowed during model evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported among the stronger models (table entries indicate high performance; example values in text show declines with clause count). Exact overall accuracy figures for GPT-4o reported in Table 3 (example: ~85.5% overall as tabulated), with significant drops on hard instances toward random baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms random baseline (50%) overall but similar to other LLMs experiences plateau near 50–53% on the hardest problems. Using GPT-4o as judge for traces introduces dependency on the judge model but human validation shows >90% pass rates for dataset steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>When used as an evaluated model, GPT-4o shows the common failure modes (satisfiability bias, context inconsistency, condition omission). As the generator/judge it may introduce validation biases; human validation performed to mitigate but some judge errors remain.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Using a strong model both as generator and judge enabled scalable dataset creation and trace evaluation, but introduces potential circularities which the authors mitigate via human validation; GPT-4o's behavior highlights that generation/judging LLMs are not infallible and that natural-language framing increases task difficulty relative to raw SAT formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8624.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8624.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen3-235B-Int</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen3-235B-Int</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large Qwen-family model (235B parameters per the naming) evaluated on SATBench, achieving high accuracy among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen3-235B-Int</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large Qwen family LLM (naming indicates a large 235B-parameter instruction-tuned variant). The paper references Qwen family models but provides no further training or architecture details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>235B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SATBench (SAT-derived natural-language logical puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary satisfiability classification and reasoning-trace generation for natural-language puzzles converted from SAT CNF formulas; difficulty controlled by clause counts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting for label + reasoning trace; judged with a separate GPT-4o judge; no external solver access allowed during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported among top-performing models in Table 3 (table entries indicate high overall accuracy; table row shows ~89.0% for this variant). Performance declines with clause count and plateaus near random on hardest problems.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms random baseline (50%) overall; scaling shows larger Qwen models generally improve but still plateau on hardest instances (50–53%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same systematic failures: satisfiability bias, context inconsistency, omission of conditions, and spurious priors. Larger size helps on easier problems but gives limited gains on the hardest UNSAT cases.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large instruction-tuned Qwen models perform strongly on SATBench overall, but difficulty scaling exposes limitations; natural-language framing and search-based requirements (backtracking) are key sources of failure that are not fully resolved by scale alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8624.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8624.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.7-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.7 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary Claude-family model evaluated on SATBench that attains competitive accuracy among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.7 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary Anthropic model (Claude family). The paper lists it among evaluated proprietary models but does not provide parameter counts or architecture details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SATBench (SAT-derived natural-language logical puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language SAT puzzles requiring satisfiability labeling and reasoning-trace output for verification; tasks probe search-based logical reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot prompting with required reasoning traces; judged by GPT-4o for trace validity; no external solver access during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as a strong performer in Table 3 (table row indicates a high overall accuracy around the high 80s% range). Like other models, performance degrades with increasing clause count and hard instances approach random baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms the 50% random baseline overall; shares the same plateau effect on hard instances where scale and model choice provide limited improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Exhibits satisfiability bias, context inconsistency, and condition omission; reasoning traces on SAT cases are often less reliable (models predict SAT but fail to supply valid assignments).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Claude-3.7 Sonnet is competitive overall but highlights broader limitations of LLMs on search-based natural-language SAT tasks, reinforcing that improved prompting and supervised fine-tuning yield modest gains but do not eliminate core deficiencies in search/backtracking reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can transformers reason logically? a study in sat solving <em>(Rating: 2)</em></li>
                <li>SatLM: Satisfiability-aided language models using declarative prompting <em>(Rating: 2)</em></li>
                <li>ZebraLogic: On the scaling limits of llms for logical reasoning <em>(Rating: 2)</em></li>
                <li>PARAT <em>(Rating: 2)</em></li>
                <li>RuleTaker <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8624",
    "paper_id": "paper-278769016",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "o4-mini",
            "name_full": "o4-mini",
            "brief_description": "A proprietary, high-performing large language model evaluated in this paper; the best-performing model on SATBench in zero-shot prompting experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o4-mini",
            "model_description": "Proprietary LLM (family: OpenAI / GPT-4o family referenced in the paper). The paper does not provide architecture or training-data details for this specific variant.",
            "model_size": null,
            "reasoning_task_name": "SATBench (SAT-derived natural-language logical puzzles)",
            "reasoning_task_description": "Search-based logical reasoning: natural-language puzzles automatically generated from Boolean SAT formulas (CNF). Tasks require predicting satisfiable/unsatisfiable and producing a valid variable assignment (for SAT) or a reasoning trace/UNSAT rationale (for UNSAT).",
            "method_or_approach": "Zero-shot prompting with chain-of-thought style reasoning traces required; evaluation prohibits external tools. Additionally, the paper tests error-aware prompting for failure correction (prompt engineering); reasoning traces judged by a separate LLM (GPT-4o).",
            "performance": "Top accuracy on SATBench zero-shot (reported overall accuracy 89.3% in Table 3). On hard instances overall accuracy falls (reported ~78.0% on hard instances); on hard UNSAT subset accuracy reported as 65.0% (close to the 50% random baseline).",
            "baseline_comparison": "Compared to a random baseline of 50%; substantially above random overall but only modestly above random on hard instances (hard UNSAT ~65%). Error-aware prompting corrected 60.4% of previously failing o4-mini cases in targeted re-evaluation.",
            "limitations_or_failures": "Systematic failures include satisfiability bias (declaring SAT without a valid assignment), context inconsistency (contradictory assignments in a trace), condition omission (ignoring clauses), and reliance on spurious priors. Performance degrades sharply with clause count; plateaus near random on hardest problems.",
            "insights_or_conclusions": "Even very strong LLMs like o4-mini struggle with search-based logical reasoning when expressed in natural language: they can achieve high overall accuracy but fail on hard UNSAT and produce unreliable traces. Error-aware prompting yields substantial corrections, indicating predictable failure modes amenable to prompt engineering, but core search/backtracking weaknesses remain.",
            "uuid": "e8624.0",
            "source_info": {
                "paper_title": "SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1",
            "brief_description": "A reasoning-focused LLM (DeepSeek family) evaluated on SATBench that achieves high accuracy and benefits from targeted prompt corrections.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1",
            "model_description": "Recent DeepSeek family LLM optimized for reasoning (paper references DeepSeek technical reports); specific architecture/training details are not provided in SATBench beyond family name.",
            "model_size": null,
            "reasoning_task_name": "SATBench (SAT-derived natural-language logical puzzles)",
            "reasoning_task_description": "Search-based satisfiability prediction and trace generation from CNF-to-natural-language puzzles; requires constructing satisfying assignments or demonstrating unsatisfiability.",
            "method_or_approach": "Zero-shot prompting with required reasoning traces; analysis includes error-aware prompting which explicitly reminds models of common pitfalls.",
            "performance": "Reported accuracy 87.8% on SATBench (stated in main results). Performance drops with difficulty; on hard problems accuracy approaches random baseline ranges reported across families.",
            "baseline_comparison": "Compared to random baseline (50%). Error-aware prompting corrected 73.2% of DeepSeek-R1's previously failing cases in a targeted re-evaluation, indicating susceptibility to prompt fixes.",
            "limitations_or_failures": "Exhibits satisfiability bias, condition omission, and context inconsistency similar to other models; performance plateaus on hard cases despite model size/family improvements.",
            "insights_or_conclusions": "DeepSeek-R1 performs strongly overall but shares the same systematic weaknesses as other LLMs on hard search-based logical tasks; prompting that makes failure modes explicit can substantially recover previously incorrect predictions.",
            "uuid": "e8624.1",
            "source_info": {
                "paper_title": "SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek-V3",
            "name_full": "DeepSeek-V3",
            "brief_description": "Another DeepSeek-family model evaluated on SATBench with strong performance but degraded behavior on the hardest instances.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-V3",
            "model_description": "Member of the DeepSeek family (reasoning-focused LLMs); paper does not specify architecture or training corpora for this variant.",
            "model_size": null,
            "reasoning_task_name": "SATBench (SAT-derived natural-language logical puzzles)",
            "reasoning_task_description": "Evaluate satisfiability and provide reasoning traces for natural-language puzzles generated from CNF SAT formulas; focuses on search-like reasoning (existential SAT vs universal UNSAT).",
            "method_or_approach": "Zero-shot prompting for satisfiability prediction with required reasoning traces; evaluated with an LLM-as-judge (GPT-4o).",
            "performance": "Reported accuracy 84.0% on SATBench (stated in main results). Performance decreases with clause count; hard instances approach random-baseline performance.",
            "baseline_comparison": "Compared to random baseline (50%); better overall but similar degradation on hard clauses. Prompt corrections yield improvements (paper reports general gains for error-aware prompts across models).",
            "limitations_or_failures": "Same categories of failures: satisfiability bias (invalid assignments), context inconsistency, condition omission, and spurious priors; poor scaling on hardest problems.",
            "insights_or_conclusions": "Performs well on easier problems but struggles with search/backtracking style reasoning required for challenging UNSAT cases; targeted prompting helps but core limitations remain.",
            "uuid": "e8624.2",
            "source_info": {
                "paper_title": "SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A proprietary GPT-4o family model used both as an evaluated model and as the primary LLM for dataset generation and judging reasoning traces.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Proprietary OpenAI model (GPT-4o family). In this paper GPT-4o is used as one of the evaluated models and also as the LLM for generation/validation/judging; architecture/training specifics are not detailed here.",
            "model_size": null,
            "reasoning_task_name": "SATBench (SAT-derived natural-language logical puzzles)",
            "reasoning_task_description": "Natural-language SAT puzzles requiring satisfiability classification and trace generation; generation pipeline uses LLMs to convert CNF to stories and back-translate for solver validation.",
            "method_or_approach": "Zero-shot prompting for evaluation; GPT-4o additionally used for LLM-based validation, story generation, and as the judge for reasoning-trace correctness. External SAT solvers used only during dataset generation/validation (not allowed during model evaluation).",
            "performance": "Reported among the stronger models (table entries indicate high performance; example values in text show declines with clause count). Exact overall accuracy figures for GPT-4o reported in Table 3 (example: ~85.5% overall as tabulated), with significant drops on hard instances toward random baseline.",
            "baseline_comparison": "Outperforms random baseline (50%) overall but similar to other LLMs experiences plateau near 50–53% on the hardest problems. Using GPT-4o as judge for traces introduces dependency on the judge model but human validation shows &gt;90% pass rates for dataset steps.",
            "limitations_or_failures": "When used as an evaluated model, GPT-4o shows the common failure modes (satisfiability bias, context inconsistency, condition omission). As the generator/judge it may introduce validation biases; human validation performed to mitigate but some judge errors remain.",
            "insights_or_conclusions": "Using a strong model both as generator and judge enabled scalable dataset creation and trace evaluation, but introduces potential circularities which the authors mitigate via human validation; GPT-4o's behavior highlights that generation/judging LLMs are not infallible and that natural-language framing increases task difficulty relative to raw SAT formulas.",
            "uuid": "e8624.3",
            "source_info": {
                "paper_title": "SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen3-235B-Int",
            "name_full": "Qwen3-235B-Int",
            "brief_description": "A large Qwen-family model (235B parameters per the naming) evaluated on SATBench, achieving high accuracy among evaluated models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen3-235B-Int",
            "model_description": "Large Qwen family LLM (naming indicates a large 235B-parameter instruction-tuned variant). The paper references Qwen family models but provides no further training or architecture details.",
            "model_size": "235B",
            "reasoning_task_name": "SATBench (SAT-derived natural-language logical puzzles)",
            "reasoning_task_description": "Binary satisfiability classification and reasoning-trace generation for natural-language puzzles converted from SAT CNF formulas; difficulty controlled by clause counts.",
            "method_or_approach": "Zero-shot prompting for label + reasoning trace; judged with a separate GPT-4o judge; no external solver access allowed during evaluation.",
            "performance": "Reported among top-performing models in Table 3 (table entries indicate high overall accuracy; table row shows ~89.0% for this variant). Performance declines with clause count and plateaus near random on hardest problems.",
            "baseline_comparison": "Outperforms random baseline (50%) overall; scaling shows larger Qwen models generally improve but still plateau on hardest instances (50–53%).",
            "limitations_or_failures": "Same systematic failures: satisfiability bias, context inconsistency, omission of conditions, and spurious priors. Larger size helps on easier problems but gives limited gains on the hardest UNSAT cases.",
            "insights_or_conclusions": "Large instruction-tuned Qwen models perform strongly on SATBench overall, but difficulty scaling exposes limitations; natural-language framing and search-based requirements (backtracking) are key sources of failure that are not fully resolved by scale alone.",
            "uuid": "e8624.4",
            "source_info": {
                "paper_title": "SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Claude-3.7-Sonnet",
            "name_full": "Claude 3.7 Sonnet",
            "brief_description": "A proprietary Claude-family model evaluated on SATBench that attains competitive accuracy among evaluated models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude 3.7 Sonnet",
            "model_description": "Proprietary Anthropic model (Claude family). The paper lists it among evaluated proprietary models but does not provide parameter counts or architecture details.",
            "model_size": null,
            "reasoning_task_name": "SATBench (SAT-derived natural-language logical puzzles)",
            "reasoning_task_description": "Natural-language SAT puzzles requiring satisfiability labeling and reasoning-trace output for verification; tasks probe search-based logical reasoning capability.",
            "method_or_approach": "Zero-shot prompting with required reasoning traces; judged by GPT-4o for trace validity; no external solver access during evaluation.",
            "performance": "Reported as a strong performer in Table 3 (table row indicates a high overall accuracy around the high 80s% range). Like other models, performance degrades with increasing clause count and hard instances approach random baseline.",
            "baseline_comparison": "Outperforms the 50% random baseline overall; shares the same plateau effect on hard instances where scale and model choice provide limited improvements.",
            "limitations_or_failures": "Exhibits satisfiability bias, context inconsistency, and condition omission; reasoning traces on SAT cases are often less reliable (models predict SAT but fail to supply valid assignments).",
            "insights_or_conclusions": "Claude-3.7 Sonnet is competitive overall but highlights broader limitations of LLMs on search-based natural-language SAT tasks, reinforcing that improved prompting and supervised fine-tuning yield modest gains but do not eliminate core deficiencies in search/backtracking reasoning.",
            "uuid": "e8624.5",
            "source_info": {
                "paper_title": "SATBench: Benchmarking LLMs’ Logical Reasoning via Automated Puzzle Generation from SAT Formulas",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can transformers reason logically? a study in sat solving",
            "rating": 2,
            "sanitized_title": "can_transformers_reason_logically_a_study_in_sat_solving"
        },
        {
            "paper_title": "SatLM: Satisfiability-aided language models using declarative prompting",
            "rating": 2,
            "sanitized_title": "satlm_satisfiabilityaided_language_models_using_declarative_prompting"
        },
        {
            "paper_title": "ZebraLogic: On the scaling limits of llms for logical reasoning",
            "rating": 2,
            "sanitized_title": "zebralogic_on_the_scaling_limits_of_llms_for_logical_reasoning"
        },
        {
            "paper_title": "PARAT",
            "rating": 2
        },
        {
            "paper_title": "RuleTaker",
            "rating": 1
        }
    ],
    "cost": 0.0155605,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas
22 Sep 2025</p>
<p>Anjiang Wei anjiang@cs.stanford.edu 
Stanford University
2 UCLA 3 UIUC 4 CMU</p>
<p>Yuheng Wu 
Stanford University
2 UCLA 3 UIUC 4 CMU</p>
<p>Yingjia Wan 
Tarun Suresh 
Huanmi Tan 
Zhanke Zhou 
Stanford University
2 UCLA 3 UIUC 4 CMU</p>
<p>Sanmi Koyejo 
Stanford University
2 UCLA 3 UIUC 4 CMU</p>
<p>Ke Wang 
Nanjing University</p>
<p>Alex Aiken 
Stanford University
2 UCLA 3 UIUC 4 CMU</p>
<p>SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas
22 Sep 2025AC541A9466EDC200AA67EE5E3DDC4636arXiv:2505.14615v2[cs.AI]
We introduce SATBench, a benchmark for evaluating the logical reasoning capabilities of large language models (LLMs) through logical puzzles derived from Boolean satisfiability (SAT) problems.Unlike prior work that focuses on inference rule-based reasoning, which often involves deducing conclusions from a set of premises, our approach leverages the searchbased nature of SAT problems, where the objective is to find a solution that fulfills a specified set of logical constraints.Each instance in SAT-Bench is generated from a SAT formula, then translated into a puzzle using LLMs.The generation process is fully automated and allows for adjustable difficulty by varying the number of clauses.All 2100 puzzles are validated through both LLM-based and solver-based consistency checks, with human validation on a subset.Experimental results show that even the strongest model, o4-mini, achieves only 65.0% accuracy on hard UNSAT problems, close to the random baseline of 50%.Our error analysis reveals systematic failures such as satisfiability bias, context inconsistency, and condition omission, highlighting limitations of current LLMs in search-based logical reasoning.Our code and data are publicly available at https: //github.com/Anjiang-Wei/SATBench</p>
<p>Introduction</p>
<p>Logical reasoning is a fundamental component of human intelligence and continues to be a significant challenge in the field of artificial intelligence.The growing interest in the reasoning capabilities of large language models (LLMs) highlights the pressing need for robust benchmarks and evaluation methods (Luo et al., 2023).</p>
<p>While many datasets have been proposed to evaluate logical reasoning capabilities of LLMs, earlier datasets do not exclusively evaluate logical reasoning in isolation, e.g., LogiQA (Liu et al., 2020), and ReClor (Yu et al., 2020), which combine logical reasoning with commonsense reasoning.</p>
<p>Recently, new datasets have been introduced to assess logical reasoning in isolation, such as FO-LIO (Han et al., 2024a) and P-FOLIO (Han et al., 2024b).These datasets are manually curated by researchers and focus on logical problems based on inference rules, which involve deriving conclusions from a set of premises.</p>
<p>In this work, we introduce SATBench, a benchmark designed to create logical puzzles from Boolean satisfiability (SAT) problems (Cook, 1971;Pan et al., 2024) with LLMs.Unlike benchmarks based on inference rules, SAT problems are characterized as search-based logical reasoning tasks, where the objective is to determine a truth assignment that fulfills a specified set of logical constraints (Madusanka et al., 2024).This approach to logical reasoning emphasizes a search process akin to backtracking used in SAT solvers.Unlike other search-based benchmarks such as ZebraLogic (Lin et al., 2025), which presuppose the existence of a valid solution, SAT problems can result in either a satisfiable solution (SAT) or no solution (UNSAT).</p>
<p>As shown in Figure 1, starting from a SAT formula in Conjunctive Normal Form (CNF), such as (A ∨ ¬B) ∧ (¬C ∨ ¬D), our framework uses LLMs to generate a story context and define a mapping between formula variables and entities in the story.Each clause is then translated into a natural language condition based on this mapping.By sampling CNF formulas with varying numbers of clauses, we can control puzzle difficulty.To ensure the quality of resulting logical puzzles, we reverse the generation process: LLMs translate the natural language conditions back into logical formulas, which are then compared to the originals using a combination of LLM-assisted and solver-based consistency checks.In the evaluation pipeline, we check the result and employ the LLM-as-a-judge strategy to assess the reasoning trace.To validate Figure 1: Overview of the SATBench methodology.The generation pipeline begins with sampling Conjunctive Normal Form (CNF) formulas, followed by LLM-driven creation of story backgrounds and conditions.To ensure the logical puzzle's quality, both LLM-assisted and solver-based consistency validations are employed.The evaluation pipeline then examines the puzzle's prediction outcomes and checks its reasoning process.</p>
<p>the overall process of story generation and reasoning trace evaluation, we manually checked 100 examples, with passing rates above 90%, which increases confidence in the quality of the dataset and evaluation protocol.</p>
<p>The evaluation on our generated 2100 logical puzzle dataset demonstrates that reasoning models exhibit strong performance on SATBench, with the o4-mini model achieving the highest accuracy.However, as the complexity of the problems increases, with a larger number of conditions in the logical puzzles, there is a noticeable decline in model performance.Specifically, the o4-mini model achieves an average accuracy of 65.0% for the hard subset of UNSAT problems.This highlights the challenges posed by our benchmark, particularly for hard instances, where even the bestperforming model only marginally surpasses the random baseline of 50%, leaving significant room for improvement.Moreover, our analysis shows that while models often achieve higher accuracy on SAT than UNSAT problems, their reasoning traces are less reliable in the SAT setting, frequently predicting satisfiability without a valid assignment.To better understand these failures, we conduct an error analysis that identifies systematic patterns such as satisfiability bias, context inconsistency, and condition omission (Section 5.3).These findings show that SATBench exposes limitations in current LLMs' ability to perform search-based logical reasoning.We further explore prompting and fine-tuning to improve performance on SATBench (Section 6.2).In summary, our work makes the following contributions:</p>
<p>• Task: We present SATBench, a benchmark that uses large language models to generate logical puzzles from Boolean satisfiability (SAT) problems.The benchmark highlights the search-based nature of logical reasoning by focusing on finding truth assignments that satisfy given constraints.</p>
<p>• Dataset: Our generation process is fully automated and features adjustable difficulty levels by varying the number of clauses in SAT formulas.We ensure the quality of the 2100 generated logical puzzles through LLM-based and solver-based checks, with human validation showing passing rates above 90%.</p>
<p>• Analysis: We show that accuracy declines sharply on the hard UNSAT problems and that reasoning traces for SAT are often unreliable.</p>
<p>Our error analysis reveals systematic failures such as satisfiability bias, context inconsistency, and condition omission, highlighting limitations of current LLMs in search-based logical reasoning.</p>
<p>Reasoning Evaluation</p>
<p>LogiQA (Liu et al., 2020)
✗ ✗ ✗ ✗ ✓ ✓ ✗ BIG-bench (Srivastava et al., 2022) ✗ ✗ ✗ ✗ ✓ ✓ ✗ ReClor (Yu et al., 2020) ✗ ✗ ✗ ✗ ✓ ✓ ✗ RuleTaker (Clark et al., 2020) ✗ ✓ ✓ ✓ ✓ ✗ ✗ LogicNLI (Tian et al., 2021) ✗ ✓ ✓ ✗ ✓ ✓ ✗ FOLIO (Han et al., 2024a) ✗ ✓ ✗ ✗ ✓ ✓ ✗ P-FOLIO (Han et al., 2024b) ✗ ✓ ✗ ✗ ✓ ✓ ✓ LogicPro (Jiang et al., 2024) ✗ ✗ ✓ ✗ ✓ ✓ ✓ ZebraLogic (Lin et al., 2025) ✓ ✓ ✓ ✓ ✓ ✗ ✗ AutoLogi (Zhu et al., 2025) ✓ ✓ ✓ ✓ ✓ ✓ ✗ PARAT (Pan et al., 2024) ✓ ✓ ✓ ✓ ✗ ✓ ✓ LogicBench (Parmar et al., 2024) ✗ ✓ ✓ ✗ ✓ ✗ ✗ LogicAsker (Wan et al., 2024) ✗ ✓ ✓ ✗ ✓ ✗ ✗ Unigram-FOL (Sileo, 2024) ✗ ✓ ✓ ✗ ✓ ✗ ✗ Multi-LogiEval (Patel et al., 2024) ✗ ✓ ✓ ✓ ✓ ✗ ✗ SATBench (ours) ✓ ✓ ✓ ✓ ✓ ✓ ✓
Table 1: Comparison of existing logical reasoning benchmarks.An ideal evaluation framework should meet the following six criteria: (1) Logic Isolation: the benchmark exclusively evaluates logical reasoning in isolation;</p>
<p>(2) Automated Generation: the benchmark construction is automated and scalable;</p>
<p>(3) Difficulty Control: the difficulty levels of the benchmark questions are adjustable; (4) Natural Language: the questions are written in natural language rather than formal formulas; (5) Template-Free: the benchmark does not rely on expert-designed templates, enhancing diversity; (6) Reasoning Evaluation: the benchmark evaluates both the accuracy of model predictions and the correctness of their reasoning traces.</p>
<p>Related Work</p>
<p>Logical Reasoning Benchmarks for LLMs Reasoning is a longstanding focus in NLP, with many benchmarks developed to assess model performance.Early efforts targeted natural language inference (Bowman et al., 2015) and commonsense reasoning (Talmor et al., 2018), while recently there has been increasing attention to assessing logical reasoning, as seen in LogiQA (Liu et al., 2020), ReClor (Yu et al., 2020), BoardgameQA (Kazemi et al., 2023), and CLUTRR (Sinha et al., 2019).These typically involve reasoning that relies on real-world knowledge.In contrast, datasets like FOLIO (Han et al., 2024a), RuleTaker (Clark et al., 2020), and P-FOLIO (Han et al., 2024b) aim to isolate formal logical reasoning from commonsense knowledge.Logical puzzles have emerged as a compelling testbed in this area (Giadikiaroglou et al., 2024), with benchmarks including ZebraLogic (Lin et al., 2025), AutoLogi (Zhu et al., 2025), and LogicNLI (Tian et al., 2021).PARAT (Pan et al., 2024) examines LLMs directly on SAT formulas, whereas our benchmark frames SAT problems as natural language puzzles, a more realistic setting given LLMs' training on text and the availability of efficient SAT solvers for formula inputs.Unlike AutoLogi, which builds on existing corpora and risks data contamination, our dataset is generated entirely from scratch with solver and human validation to ensure correctness and diversity.A systematic comparison of these benchmarks is provided in Table 1.</p>
<p>Logical Reasoning with Language Models Recent work investigates how large language models engage in logical reasoning via prompting techniques, supervised training on reasoning datasets, and translation into formal logic.A prominent line of research focuses on prompting methods that elicit step-by-step reasoning, including chainof-thought prompting (Wei et al., 2022), tree-ofthought prompting (Yao et al., 2023), along with other methods (Zelikman et al., 2022;Kojima et al., 2022;Li et al., 2022;Tyagi et al., 2024;Chen et al., 2025).Another approach involves fine-tuning LLMs on datasets specifically designed for logical reasoning (Young et al., 2022;Morishita et al., 2023;Luo et al., 2023;Dziri et al., 2023;Ranaldi and Freitas, 2024), which has demonstrated improved performance on formal reasoning benchmarks.Complementary to these methods, some work treats LLMs as semantic parsers that convert natural language reasoning tasks into formal logical representations, which are then executed or verified by external solvers or theorem provers (Ye et al., 2023;Ryu et al., 2024).In our evaluation, we use chain-of-thought prompting and prohibit models from invoking external tools; solvers are used only during dataset generation for validation.</p>
<p>Our objective is to create logical puzzles derived from Boolean satisfiability (SAT) formulas, ensuring the quality of the dataset through both LLMbased and solver-based consistency checks.We further validate each LLM-involved process with human review.The generation method is divided into three stages: SAT formula sampling (Section 3.1), LLM-based story generation (Section 3.2), and consistency validation (Section 3.3).In the evaluation phase, we assess the correctness of the reasoning trace (Section 3.4).</p>
<p>SAT formula Sampling</p>
<p>Conjunctive Normal Form (CNF) Conjunctive Normal Form is a structured way of expressing logical formulas, where a formula is a conjunction (AND) of one or more disjunctions (OR) of literals.Each disjunction is referred to as a clause, and each clause consists of literals, which can be either a variable or its negation.For instance, the formula
(x(2, 1))∧(¬x(1, 0)∨x(0, 0))∧(¬x(0, 0))∧ (¬x(2, 1) ∨ x(1, 0)) is in CNF.
Here, x represents a two-dimensional array with boolean elements, indicating true or false values.The SAT problem expressed in CNF form involves determining whether there exists an assignment of boolean values to the variables that satisfies the entire formula, making it true.If such an assignment exists, the formula is satisfiable.Conversely, if no such assignment can be found, the formula is unsatisfiable, and an UNSAT-Core can be identified, which is a subset of clauses that are inherently unsatisfiable.This approach constructs puzzles that challenge LLMs to determine if all conditions can be satisfied.</p>
<p>Automation and Difficulty Control</p>
<p>The SAT problem can be solved using a SAT solver, which provides a soundness guarantee and allows for an automated and scalable solution.To systematically generate problems with varying levels of difficulty, we can sample formulas that differ in the number of boolean variables and clauses.Additionally, we can increase the dimensionality of the array to create more complex story contexts.By increasing the number of boolean variables, we can generate more clauses to be translated into story conditions.This approach effectively controls the difficulty level by expanding the search space and adding complexity to the constraints, making the search-based logical reasoning more challenging.</p>
<p>Puzzle Story Generation</p>
<p>Background and Variable Mapping To transform the sampled SAT formula into a narrative context, we utilize a language model, such as GPT-4o, to generate a story background and establish a mapping of variables.For example, as shown in Figure 2, given the SAT formula, the language model creates a scenario involving three musicians: Alice, Bob, and Carol.These musicians are deciding on their performances in two musical genres, jazz and rock.Each musician can independently choose whether to perform in one or both genres, or not at all.The musicians and the genres correspond to the two dimensions of the array x.This mapping is defined as:</p>
<p>x(i, j) → "musician i performs in genre j"</p>
<p>For example:
• x(0, 0): Alice performs in jazz • x(1, 0): Bob performs in jazz • x(2, 1): Carol performs in rock
Clause-to-Condition Mapping To transform each clause of the CNF formula into a narrative condition, we employ a large language model (e.g., .This transformation leverages the previously established story background and variable mapping.For example, the clause ¬x(0, 0) is translated to the condition "Alice does not perform in jazz," while the clause ¬x(2, 1) ∨ x(1, 0) is expressed as "Either Carol does not perform in rock, or Bob performs in jazz."The final puzzle integrates the story background with these translated conditions and concludes with a question like "Is there a way for all these performance choices to work?"This question serves to assess the satisfiability of the conditions in the logical puzzle.</p>
<p>Our two-phase generation strategy, which begins with the creation of the story background and variable mapping, followed by the transformation of clauses into narrative conditions, improves the tractability and reliability of the process.This structured approach facilitates easier debugging and human validation.Additional examples of generated puzzles are provided in Section A.</p>
<p>Consistency Validation</p>
<p>LLM-based Validation</p>
<p>We utilize a large language model (GPT-4o) to ensure that each condition in the generated logical puzzle precisely</p>
<p>Sample CNF Formula</p>
<p>Story Background</p>
<p>In a small town, there are three musicians who are preparing for performances in two genres: jazz and rock.Each musician can independently choose whether to perform in one or both genres, or not at all. Figure 2: Benchmark curation pipeline.The process starts with sampling SAT formulas, followed by using an LLM to generate variable mappings and a story background.Clauses in the formula are then translated into narrative conditions.Consistency between the original formula and the generated puzzle is ensured through both LLM-based and solver-based validation.</p>
<p>matches the original SAT formula, given the specified variable mapping.This process checks that no extra conditions are introduced and none are missing.If the check fails, the puzzle is removed from our dataset.The validation prompt is provided in B.1.</p>
<p>Solver-based Validation</p>
<p>In addition to LLMbased checks, we adopt solver-based validation that enforces formula-level equivalence between the reconstructed formula and the original CNF.Using the variable mappings, an LLM first converts the narrative conditions back into a SAT formula.The original formula formula A and the reconstructed formula B are checked for equivalence using bidirectional entailment:
A ≡ B iff A |= B ∧ B |= A.
This condition is encoded into a SAT query and checked by the solver.Any reconstructed formula that fails equivalence is discarded, ensuring that the generated puzzles faithfully preserve the original logical structure.</p>
<p>Human Validation</p>
<p>To ensure the quality of our dataset, we conduct human validation at two crucial stages involving LLMs, as detailed in Section 3.2.The first stage involves the generation of the puzzle's background and variable mapping, where humans assess the logical coherence and confirm that the story background accurately reflects the independence of boolean variables.The second stage focuses on the translation of clauses into narrative conditions, where humans ensure that no additional constraints or misinterpretations are introduced.</p>
<p>Reasoning Trace Evaluation</p>
<p>After generating the logical puzzles, we evaluate an LLM's performance using this dataset.Our evaluation emphasizes both the binary prediction result (SAT or UNSAT) and the validity of the model's reasoning trace.We adopt an LLM-as-ajudge methodology, where the model is instructed to produce a reasoning trace to justify its prediction.Below, we detail the approach for assessing the reasoning trace in SAT and UNSAT scenarios.</p>
<p>SAT Problems When a problem is identified as SAT, it indicates that there is at least one assignment of True or False values to the variables that satisfies the CNF formula.Multiple solutions may exist.For example, consider the CNF formula (x(0, 0) ∨ ¬x(1, 0)) ∧ (x(1, 0) ∨ x(2, 1)).One possible satisfying assignment is x(0, 0) = True, x(1, 0) = False, and x(2, 1) = True.After the model predicts a problem as SAT, it is required to generate a reasoning trace to support its prediction.We then instruct the judging LLM to translate this reasoning into a specific variable assignment using the given variable mapping.The judging LLM is further used to verify that each clause in the SAT formula evaluates to True, thereby confirming the satisfiability of the entire SAT formula.</p>
<p>UNSAT Problems Unlike SAT problems, UN-SAT problems have no variable assignment that satisfies all clauses.A SAT solver can identify an UNSAT-Core, which is a minimal subset of unsatisfiable clauses.When the model predicts UNSAT, it must provide a reasoning trace.</p>
<p>Consider the formula: (x(2, 1)) ∧ (¬x(1, 0) ∨ x(0, 0)) ∧ (¬x(0, 0)) ∧ (¬x(2, 1) ∨ x(1, 0)).We can demonstrate its unsatisfiability through a stepby-step analysis:</p>
<ol>
<li>From the first clause, x(2, 1), we must set</li>
</ol>
<p>x(2, 1) to true.</p>
<ol>
<li>
<p>From the third clause, ¬x(0, 0), we must set x(0, 0) to false.</p>
</li>
<li>
<p>Given that x(0, 0) is false, the second clause, ¬x(1, 0) ∨ x(0, 0), can only be satisfied if ¬x(1, 0) is true, suggesting x(1, 0) is false.</p>
</li>
<li>
<p>However, since x(2, 1) is true, the fourth clause, ¬x(2, 1) ∨ x(1, 0), can only be satisfied if x(1, 0) is true.</p>
</li>
</ol>
<p>This leads to an irreconcilable contradiction: x(1, 0) is required to be both true and false simultaneously to satisfy all clauses, rendering the formula unsatisfiable.The example above illustrates a valid reasoning trace for an UNSAT problem in formula format.However, since the model being evaluated lacks access to the variable mapping during its reasoning trace generation, the judging LLM must first translate the reasoning trace back into the variable format.It then compares this translated reasoning with the provided UNSAT-Core to assess the accuracy of the reasoning trace.</p>
<p>Human Validation Given our use of an LLMas-a-judge methodology for evaluating reasoning traces, we incorporate a human validation process to check the correctness of the LLM's judgments.</p>
<p>Experimental Setup</p>
<p>Dataset The SATBench dataset consists of 2100 logical puzzle instances.Table 2 provides statistics on the average number of boolean variables and clauses in the sampled SAT formulas, as well as the average number of words and sentences in the generated logical puzzles.The dataset generation process is fully automated, allowing for the creation of additional instances as required.</p>
<p>Prompts We use 0-shot prompting to evaluate various LLMs on each logical puzzle in our dataset.Each puzzle's prompt includes a story background, Metrics In our evaluation, satisfiability is treated as a binary classification task, where random guessing results in a baseline accuracy of 50%.The primary metric we use is the accuracy of the predicted satisfiability label.Besides, we also evaluate the correctness of the model's reasoning trace, but only if the satisfiability label is correct.We employ GPT-4o to determine whether the provided explanation logically supports the predicted outcome, as detailed in Section 3.4.</p>
<p>Models</p>
<p>We evaluate both proprietary and opensource language models.The proprietary models include GPT-4o (Achiam et al., 2023), GPT-4omini, o4-mini, and Claude 3.7 Sonnet.The opensource models cover a range of recent ones from the Qwen (Yang et al., 2025), Llama (Touvron et al., 2023), and DeepSeek families (Liu et al., 2024;Guo et al., 2025).For reasoning trace evaluation, we focus on the 5 top-performing models, and use GPT-4o as the judge.</p>
<p>Results</p>
<p>Main Results</p>
<p>Table 3 presents the accuracy on SATBench using zero-shot prompting for satisfiability prediction.</p>
<p>Our findings are as follows.</p>
<p>Reasoning models excel in performance.models DeepSeek-R1 and DeepSeek-V3, with accuracies of 87.8% and 84.0%, respectively.Overall, reasoning models excel in our benchmark.</p>
<p>Model performance decreases with increasing problem difficulty.We categorize difficulty levels as Easy (4-19 clauses), Medium (20-30 clauses), and Hard (31-50 clauses).Notably, even the topperforming model, o4-mini, sees its accuracy fall to 78.0% on hard instances.Across all models, the average accuracy for hard problems is 53.0%, which is nearly equivalent to the random baseline.More analysis of difficulty is provided in Section 5.2.</p>
<p>SATBench is a challenging benchmark.For the hard instances, even the state-of-the-art model o4mini only achieves 78.0% accuracy, only a moderate improvement over the 50% random baseline.</p>
<p>For the UNSAT instances, its accuracy is only 65.0%, leaving significant room for improvement.</p>
<p>Scaling Trends.Figure 3 shows that across model families such as Qwen3, Llama3.1,Mixtral, Llama4, and DeepSeek-Distill-Qwen, larger models generally achieve higher accuracy.Yet this trend does not hold uniformly across difficulty levels.On the hard instances, accuracy plateaus around 50-53% even for the largest models in these families.Thus, the observed scaling gains are 2 0 2 1 2 2 2 3 2 4 2 5 2 6 2 7 2 8 largely limited to easier problems.For the hardest cases, simply increasing model size yields little to no gain.These findings reinforce that SATBench remains a difficult and discriminative benchmark.</p>
<p>Analysis of Difficulty</p>
<p>SAT versus UNSAT The "average" row in Table 3  for models to guess.Intuitively, the primary reason for this difference is that SAT is an existential property (there exists at least one satisfying assignment) while UNSAT is a universal property (all assignments fail to satisfy the formula).</p>
<p>Impact of Clause Quantity</p>
<p>We examine the effect of the number of clauses on model accuracy.</p>
<p>As shown in Figure 4, there is a noticeable inverse relationship: model accuracy decreases as the clause count increases.For example, the GPT-4o model experiences a significant drop in performance, nearing random guess accuracy of 50% as it approaches 30 clauses.This pattern suggests that a higher number of clauses adds complexity, demonstrating that our dataset generation methodology can effectively control difficulty levels.</p>
<p>Reasoning Traces and Error Analysis</p>
<p>Trace Evaluation We evaluate the reasoning trace validity of various models with GPT-4o, and the results are shown in Table 4.A notable observation is the disparity in trace accuracy between the SAT and UNSAT subsets.Models generally exhibit a more pronounced drop in trace accuracy on SAT problems compared to UNSAT ones.This suggests that higher prediction accuracy on SAT problems does not necessarily imply a valid variable assignment.Instead, models often show a bias toward predicting SAT outcomes without a valid assignment as evidence.</p>
<p>Error Analysis To provide a deeper understanding of model failures, we conducted a qualitative error classification of incorrect predictions, defining four major error types:</p>
<p>• Satisfiability Bias: Models answer SAT but give an invalid assignment.</p>
<p>• Context Inconsistency: Models contradict their earlier reasoning, such as assigning conflicting values across steps.</p>
<p>• Condition Omission: Models ignore one or more conditions in the reasoning trace.</p>
<p>• Spurious Priors: Models introduce commonsense assumptions that are absent from the given constraints.</p>
<p>We used GPT-4o to automatically classify errors into these categories, and the distribution for two representative models is shown in Table 5.The observed patterns highlight core challenges in searchbased logical reasoning, including failures in backtracking, difficulty in maintaining context, and reliance on prior knowledge rather than provided constraints.See examples in Section C.</p>
<p>Human Validation</p>
<p>We conducted human validation on a uniformly random sample of 100 puzzles from our generated dataset to verify the correctness of LLM-involved steps and the reliability of our evaluation protocol.Each puzzle contains a CNF formula, its satisfiability label (SAT or UNSAT) from a symbolic solver, a narrative scenario with variable mappings, natural language conditions corresponding to each clause, a reasoning trace generated by an LLM, and an LLM judgment of whether the reasoning trace is logically valid.Three co-authors independently annotated the sample and resolved disagreements by majority vote.</p>
<p>Annotators performed three validation tasks:</p>
<ol>
<li>
<p>Scenario and Mapping Consistency: Ensuring that all entities in the scenario are covered in the variable mapping, and that every logical variable is correctly grounded.We observed no errors (100% accuracy).</p>
</li>
<li>
<p>Clause Translation Faithfulness: Verifying that each clause in the CNF formula is faithfully translated into its natural language condition without omissions, additions, or misinterpretations.We found minor translation errors in three cases, yielding a 97% accuracy rate.</p>
</li>
<li>
<p>LLM Judgment Correctness: Checking whether the LLM's judgment of the reasoning trace is logically correct and aligned with the ground-truth formula and satisfiability label.</p>
</li>
</ol>
<p>Here, accuracy was 93%, with occasional errors due to incomplete assignment extraction or overly strict interpretations of valid traces.</p>
<p>Overall, these results confirm the robustness of our dataset and evaluation pipeline, with errors being rare and not significantly affecting reliability.</p>
<p>A few failure cases were observed.In story generation, one error involved the clause (¬x(2, 0) ∨ x(2, 1)) being translated as "if Dr. Brown is not assigned project 0, then Dr. Brown is assigned project 1."This misuses the if-then structure.The correct phrasing is "if Dr. Brown is assigned project 0, then Dr. Brown is also assigned project 1."</p>
<p>For the LLM-as-judge setting, the main error mode involved incomplete extraction of the assignment within the trace.In some cases, the model judged that the trace was invalid, even though the trace was logically sound.These minor errors, however, were rare and did not affect the overall robustness of our pipeline.</p>
<p>Discussion</p>
<p>SAT in Natural Language</p>
<p>SATBench frames SAT problems as natural language puzzles rather than evaluating LLMs directly on SAT formulas.Testing only on symbolic inputs overlooks how reasoning arises in practice, since real-world tasks are almost always expressed in natural language.Because LLMs are trained mainly on text, narrative puzzles provide a more faithful and revealing evaluation of their reasoning ability.Additionally, our goal is not to replace SAT solvers, but to examine whether LLMs can reason about SAT structures when expressed in natural language, something classical solvers cannot address.To test this distinction, we also evaluated models directly on CNF formulas.As shown in Table 6, accuracy was consistently higher on raw SAT inputs than on narrative puzzles, showing that natural language introduces additional complexity and makes the benchmark more challenging.</p>
<p>Improving Performance on SATBench</p>
<p>Prompting Building on our error analysis in Section 5.3, we designed error-aware prompts that explicitly remind models to avoid common pitfalls.Re-evaluating the previously misclassified cases under this setting led to substantial gains: for o4mini, 60.4% of failing cases were corrected, and for DeepSeek-R1, the rate is 73.2%.These results show that making failure patterns explicit can significantly improve model performance.</p>
<p>Fine-tuning Using 1100 correct traces from o4mini, we applied LoRA fine-tuning on Qwen2.5-14B-Instruct,raising accuracy from 51.9% to 53.6%.While modest relative to prompting, this indicates that supervised fine-tuning can help, with greater gains expected from larger datasets and reinforcement learning.</p>
<p>Conclusion</p>
<p>We present SATBench, a benchmark for assessing LLMs' logical reasoning via SAT-derived puzzles.Our dataset features search-based logical reasoning tasks, with controls difficulty and correctness checked by solvers and LLMs.SATBench contains 2100 logical puzzles, and we evaluate both satisfiability prediction and reasoning trace validity.Our findings show model performance drops with increased difficulty, with o4-mini scoring 65.0% on the hard UNSAT cases, near the 50% random baseline.We also conduct an error analysis that identifies systematic patterns such as satisfiability bias, context inconsistency, and condition omission.These findings show that SATBench exposes limitations in current LLMs' ability to perform search-based logical reasoning.</p>
<p>Limitations</p>
<p>This paper utilizes LLMs, such as GPT-4o, for the generation of logical puzzles and consistency validation.While LLMs can enhance the scalability and diversity of our dataset, they could introduce potential inaccuracies that we cannot fully eliminate.To address this issue, we incorporate human validation to ensure a high-quality dataset.However, despite these efforts, the possibility of errors remains.</p>
<p>Another limitation of this work is its exclusive focus on the Boolean satisfiability problem for logical reasoning, which means that other forms of logical reasoning are not addressed by SATBench.</p>
<p>LLM Validation Prompt Template</p>
<p>You are a logic checker .</p>
<p>You are given a SAT formula , a variable explanation , and a natural language puzzle based on the formula .Your job is to check whether the natural language conditions are logically equivalent to the original SAT formula .</p>
<p>Specifically , for each clause in the SAT formula :</p>
<p>-Verify there is a corresponding natural language condition with equivalent logical meaning .-Ensure the variable usage matches the explanation format .</p>
<p>-Make sure there are no missing clauses , no added constraints , and no changes in logic .</p>
<p>Pay special attention to logical implications and how they are expressed in natural language .For example :</p>
<p>The clause (¬x (2) ∨ x (1) ) is logically equivalent to : <code>If x (2) is true , then x (1) is also true ."A common mistake is to write this as :</code>If ¬x (2) then x (1) ", which is incorrect .That corresponds to the clause (x (2) ∨ x (1) ) , and changes the meaning .</p>
<p>Here is the information :
&lt; scenario &gt; { scenario } &lt; variable explanation &gt; { variable_mapping } &lt; conditions &gt; { conditions } &lt; question &gt; { question } &lt; SAT formula &gt; { formula }</p>
<p>Trace Evaluation Prompt Template for SAT Prediction</p>
<p>You are given a logical puzzle and a reasoning trace from a language model .The puzzle is also expressed as a SAT formula .Each clause is a disjunction ( OR ) of literals formatted like x(i) , x(i ,j) , or x(i ,j ,k).These variables follow the meaning :</p>
<p>-x ( i ) means object or person i has some unnamed property .</p>
<p>-x (i , j ) means object i has property or role j.</p>
<p>-x (i ,j ,k) means object i has property j in context or slot k (e.g., time , situation , location ).</p>
<p>A positive literal like x (0 ,1) means that the property is present .</p>
<p>A negative literal like ¬x (0 ,1) means it is absent .</p>
<p>Below is the full logical puzzle and its corresponding formula : Go through the trace and determine whether each variable appearing in the SAT formula is marked as True or False .</p>
<p>Then , for each clause , evaluate the truth value of each literal using this assignment .</p>
<p>For example , if a clause in the SAT formula is (x (0) ∨ ¬ x (1) ) , and the model says x ( 0 For any variable that is not explicitly mentioned in the reasoning trace , assume its value is 0 when constructing the assignment list .</p>
<p>Do not include anything after this label .We already know this puzzle is UNSAT ( unsatisfiable ).Your task is to judge whether the reasoning trace correctly identifies or meaningfully reflects the cause of unsatisfiability -is , whether it aligns with the given ground -truth unsat reason , even if it doesn 't name it explicitly .</p>
<p>Focus on logical precision : -Does the trace show or imply a variable assignment or chain of reasoning that leads to contradiction ?-Does it avoid hallucinations or irrelevant claims ?Note : The trace may present a specific variable assignment or reasoning path that leads to a contradiction .Whether it aligns with the given groundtruth UNSAT reason means you must judge whether the contradiction is logically valid and reflective of the actual cause , even if it doesn 't explicitly name the minimal core or unsat pattern .</p>
<p>You are ** not ** evaluating whether the conclusion " UNSAT " is correct -that is already known to be correct .You are only evaluating whether the explanation substantively captures why the instance is unsatisfiable .</p>
<p>Please think step by step .First , explain whether and how the reasoning trace aligns with the unsat reason .Then , in the last line , output one of the following labels :</p>
<p>[ YES ] -the reasoning trace is logically valid and correctly captures the UNSAT cause [ NO ] -the trace is flawed , incomplete , or does not match the correct unsat reason Do not include anything after this label .</p>
<p>Figure 3 :
3
Figure 3: Scaling trend on SATBench.</p>
<p>Figure A1 :
A1
Figure A1: Puzzle Example.</p>
<p>Figure A3 :
A3
Figure A3: SAT/UNSAT Evaluation Prompt Template.</p>
<p>to extract the truth assignment implied by the model 's reasoning trace , and evaluate whether each clause in the SAT formula is satisfied .</p>
<p>) is True and x (1) is also True , then this clause becomes [1 , 0].Think step by step .Show the variable assignments and how you evaluate each clause .Finally , in the ** last line ** , output a single line in the format : Assignment : [[1 , 0] , [0 , 1, 1] , [1] , ...]</p>
<p>Figure A4 :
A4
Figure A4: Trace Evaluation Prompt Template for SAT Prediction.</p>
<p>Figure A5 :
A5
Figure A5: Trace Evaluation Prompt Template for UNSAT Prediction.</p>
<p>Alice does not perform in jazz.3. Either Bob does not perform in jazz, or Alice performs in jazz.4. Either Carol does not perform in rock, or Bob performs in jazz.Question: Is there a way for all these performance choices to work?
Clause 1Clause 2Clause 3Clause 4Variable MappingConditionsi=0 Alicei=1 Bobi=2 Carolj=0 jazzj=1 rock1. Carol decides to perform in rock. 2. Clause TranslationConsistency Validation</p>
<p>Table 2 :
2
Dataset statistics for SATBench.a set of conditions that must be satisfiable simultaneously, and a query about their satisfiability.Models are required to generate a reasoning trace: if they determine the instance is satisfiable, they must provide a satisfying assignment for the variables; if they find it unsatisfiable, they must explain why the conditions cannot all be true at once.The final output must clearly state either SAT or UNSAT.Detailed prompts for the main evaluation and reasoning trace evaluation can be found in Appendix B.2 and Appendix B.3, respectively.
MetricValueNumber of Instances2100Average Number of Variables36.0Average Number of Clauses20.6Average Number of Words546.2Average Number of Sentences55.2</p>
<p>Table 3 :
3ModelSAT Easy Medium Hard Easy Medium Hard Easy Medium Hard UNSAT OverallAvg.Random Baseline50.050.050.050.050.050.050.050.050.050.0LLaMA3.1-8B57.960.048.930.414.817.544.137.433.238.2DeepSeek-Distill-7B63.927.616.869.143.842.166.535.729.543.9Qwen3-1.7B77.165.753.253.430.542.565.348.147.953.7gpt-4o-mini82.182.490.742.312.913.262.247.652.053.9LLaMA4-Scout84.376.766.452.024.337.568.150.552.056.9LLaMA3.1-70B82.055.745.455.259.048.968.657.447.157.7gpt-4o85.583.378.654.327.118.969.955.248.858.0LLaMA3.3-70B90.789.075.739.527.130.065.158.152.958.7DeepSeek-Distill-14B 82.951.441.185.759.051.884.355.246.462.0LLaMA4-Maverick80.286.286.176.825.717.978.556.052.062.1Qwen3-4B84.178.178.680.731.922.182.455.050.462.6Qwen3-8B82.776.767.581.634.832.182.155.749.862.6DeepSeek-Distill-32B 84.553.842.190.068.158.687.261.050.466.2Qwen3-14B87.172.980.088.947.622.188.060.251.166.4Qwen3-235B-Int890.083.383.286.146.219.688.064.851.468.1Qwen-QwQ-32B92.575.759.384.151.946.488.363.852.968.3Claude-3.7-Sonnet88.477.683.693.863.342.191.170.562.974.8DeepSeek-V393.683.871.497.583.374.395.583.672.984.0DeepSeek-R194.887.173.698.289.583.696.588.378.687.8o4-mini97.096.791.198.288.165.097.692.478.089.3Average84.173.266.772.946.439.378.559.853.063.8
The o4-mini model stands out with a remarkable accuracy of 89.3%.Close behind are the open-source Model accuracy on SATBench using zero-shot prompting for satisfiability prediction.Difficulty levels are categorized as follows: Easy (4-19 clauses),.All open-source models are instruction-tuned.</p>
<p>Table 4 :
4
Accuracy in prediction and reasoning trace evaluation.
highlights a notable disparity in model accu-racy between SAT and UNSAT subsets. Modelsperform better on SAT problems, achieving an ac-curacy of 66.7% on the hard instances, while onlyreaching 39.3% on hard UNSAT problems. Thissuggests that SAT instances are generally easier</p>
<p>Think step by step about whether the SAT formula and the natural language conditions match logically , clause by clause .Consider the number of clauses , the variable usage , and the logical operators involved .
SAT/UNSAT Evaluation Prompt TemplateYou are a logical reasoning assistant . You are given a logic puzzle .&lt; scenario &gt;{ scenario }&lt; conditions &gt;{ conditions }&lt; question &gt;{ question }Guidelines :-All constraints come ** only ** from the &lt; conditions &gt; section .-The &lt; scenario &gt; provides background and intuition , but ** does not impose anyadditional rules or constraints <strong>.-All variables represent ** independent decisions </strong>; there is no mutualexclusivity or implicit linkage unless stated explicitly in &lt; conditions &gt;.-Variables not mentioned in &lt; conditions &gt; are considered unknown andirrelevant to satisfiability .Your task :-If the puzzle is satisfiable , propose one valid assignment that satisfiesall the conditions .-If the puzzle is unsatisfiable , explain why some of the conditions cannotall be true at once .Think step by step . At the end of your answer , output exactly one of thefollowing labels on a new line :[ SAT ] -if a valid assignment exists[ UNSAT ] -if the constraints cannot be satisfiedYour job is only to evaluate whether each condition correctly represents its Do not add any text or formatting after the final label .corresponding clause in the SAT formula . You should not judge whether theoverall formula or the scenario is satisfiable , solvable , or logicallyconsistent .Do not attempt to rewrite , fix , or invent any missing conditions . If anyclause is missing , mistranslated , or not clearly represented , you must markthe result as [ INCONSISTENT ].Finally , in the last line , output either [ CONSISTENT ] or [ INCONSISTENT ].Do not include anything after this label .Figure A2: LLM Validation Prompt Template.
AcknowledgmentsWe thank Yuan Liu, Xiaohan Wang, and Gabriel Poesia for their discussions.This work was partially supported by a Google Research Award.Appendix A Example of Generated PuzzlesPlease see FigureA1.B Templates B.1 LLM Validation Prompt TemplatePlease see FigureA2.B.2 SAT/UNSAT Evaluation Prompt TemplatePlease see FigureA3.B.3 Trace Evaluation Prompt TemplatePlease see FigureA4and FigureA5.C Error Analysis ExamplesWe show one representative example for each of the four error types, paraphrased for clarity.Satisfiability BiasThe model outputs an assignment such as x(1) = 1, x(2) = 1, x(3) = 0 and prematurely declares the formula satisfiable.In reality, satisfiability requires that all clauses be satisfied, yet several clauses remain violated.This suggests that the model often assumes satisfiability without exhaustively checking all constraints and fails to engage in search-based logical reasoning with backtracking.Context InconsistencyThe model produces conflicting assignments for the same variable within one trace.For example, it first sets x(0) = 1 but later assigns x(0) = 0, trying to satisfy x(0) ∧ ¬x(0).This is impossible: a variable can only take a single value.The correct resolution is either to retain one consistent assignment in a satisfiable case or to conclude UNSAT when no such assignment exists.Condition OmissionThe model may ignore or hallucinate conditions in its reasoning trace.For example, given (x(0) ∨ x(1)) ∧ ¬x(0), it incorrectly reduces the formula to x(0) ∧ ¬x(0), which is unsatisfiable.In reality, the original formula is satisfiable with x(0) = 0, x(1) = 1.Such omissions cause the model to misclassify satisfiable instances as UNSAT.Spurious PriorsThe model introduces commonsense assumptions that are absent from the formula.For example, with x(0) ∨ x(1), the model assumes that x(0) and x(1) cannot both be true (as if they were "mutually exclusive").It then treats the assignment x(0) = 1, x(1) = 1 as a contradiction and concludes UNSAT.In reality, the formula is satisfiable and permits both variables to be true.This is because models sometimes introduce commonsense assumptions that are absent from the given constraints.Puzzle ExampleFrozen conflict chain : sequential forced assignments leading to contradiction : ( x (0 , 1) ) , (¬x (0 , 1) ∨ x (1 , 1) ) , (¬x (1 , 1) ∨ x (0 , 0) ) , (¬x (0 , 0) ∨ x (1 , 0) ) , (¬x (1 , 0) ) &lt;/ UNSAT reason &gt; &lt; scenario &gt; Two wildlife researchers , Hannah and Liam , are documenting animal behavior at a sanctuary .They are independently recording whether they observe two specific behaviors : feeding (0) and social interaction (1) .Each researcher decides on their own which behavior they have observed , and they may report multiple behaviors or none at all .&lt;/ scenario &gt; &lt; variable_mapping &gt; Let x (i , j) mean researcher i observes behavior j.Here , researcher 0 is Hannah , and researcher 1 is Liam .&lt;/ variable_mapping &gt; &lt; conditions &gt; 1.Either Hannah does not observe feeding , or Liam observes feeding .2. Hannah observes social interaction .3. Liam does not observe feeding .4. Either Hannah does not observe social interaction , or Liam observes social interaction . 5. Either Liam does not observe social interaction , or Hannah observes feeding .&lt;/ conditions &gt; &lt; question &gt; Is there a way to assign observations that make this work ?&lt;/ question &gt;
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Gabor Samuel R Bowman, Christopher Angeli, Christopher D Potts, Manning, arXiv:1508.05326A large annotated corpus for learning natural language inference. 2015arXiv preprint</p>
<p>Guizhen Chen, Weiwen Xu, Hao Zhang, Hou Pong Chan, Chaoqun Liu, Lidong Bing, Deli Zhao, Anh Tuan Luu, Yu Rong, arXiv:2502.20238Finereason: Evaluating and improving llms' deliberate reasoning through reflective puzzle solving. 2025arXiv preprint</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, 10.24963/IJCAI.2020/537Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. the Twenty-Ninth International Joint Conference on Artificial Intelligence20202020</p>
<p>The complexity of theoremproving procedures. Stephen A Cook, 10.1145/800157.805047Proceedings of the Third Annual ACM Symposium on Theory of Computing, STOC '71. the Third Annual ACM Symposium on Theory of Computing, STOC '71New York, NY, USAAssociation for Computing Machinery1971</p>
<p>Ronan Le Bras, and 1 others. 2023. Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Advances in Neural Information Processing Systems. 36</p>
<p>Panagiotis Giadikiaroglou, Maria Lymperaiou, arXiv:2402.11291Giorgos Filandrianos, and Giorgos Stamou. 2024. Puzzle solving using reasoning of large language models: A survey. arXiv preprint</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, arXiv:2501.12948Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint</p>
<p>FOLIO: natural language reasoning with first-order logic. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, Lucy Sun, Alexander Wardle-Solano, Hannah Szabó, Ekaterina Zubova, Matthew Burtell, Jonathan Fan, Yixin Liu, Brian Wong, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024a. 2024. November 12-16, 2024Malcolm Sailor, and 16 others</p>
<p>P-FOLIO: evaluating and improving logical reasoning with abundant human-written reasoning chains. Simeng Han, Aaron Yu, Rui Shen, Zhenting Qi, Martin Riddell, Wenfei Zhou, Yujie Qiao, Yilun Zhao, Semih Yavuz, Ye Liu, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Dragomir Radev, Rex Ying, Arman Cohan, Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024b. November 12-16, 2024</p>
<p>Logicpro: Improving complex logical reasoning via programguided learning. Jin Jiang, Yuchen Yan, Yang Liu, Yonggang Jin, Shuai Peng, Mengdi Zhang, Xunliang Cai, Yixin Cao, Liangcai Gao, Zhi Tang, arXiv:2409.129292024arXiv preprint</p>
<p>Boardgameqa: A dataset for natural language reasoning with contradictory information. Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. 2023. December 10 -16, 2023Xin Xu, Vaiva Imbrasaite, and Deepak Ramachandran</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, arXiv:2206.02336On the advance of making language models better reasoners. 2022arXiv preprint</p>
<p>Zebralogic: On the scaling limits of llms for logical reasoning. Ronan Bill Yuchen Lin, Kyle Le Bras, Ashish Richardson, Radha Sabharwal, Peter Poovendran, Yejin Clark, Choi, 10.48550/ARXIV.2502.01100CoRR, abs/2502.011002025</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, arXiv:2412.19437Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, 10.24963/IJCAI.2020/501Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. the Twenty-Ninth International Joint Conference on Artificial Intelligence20202020</p>
<p>Towards logiglue: A brief survey and A benchmark for analyzing logical reasoning capabilities of language models. Man Luo, Shrinidhi Kumbhar, Ming Shen, 10.48550/ARXIV.2310.00836CoRR, abs/2310.008362023Mihir Parmar, Neeraj Varshney, Pratyay Banerjee, Somak Aditya, and Chitta Baral</p>
<p>Natural language satisfiability: Exploring the problem distribution and evaluating transformer-based language models. Tharindu Madusanka, Ian Pratt-Hartmann, Riza , Theresa Batista-Navarro, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Learning deductive reasoning from synthetic corpus based on formal logic. Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, Yasuhiro Sogawa, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023. July 2023202of Proceedings of Machine Learning Research</p>
<p>Can transformers reason logically? a study in sat solving. Leyan Pan, Vijay Ganesh, Jacob Abernethy, arXiv:2410.074322024arXiv preprintChris Esposo, and Wenke Lee</p>
<p>Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral, 10.18653/V1/2024.ACL-LONG.739Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, Thailand2024. August 11-16, 20241ACL 2024. Association for Computational Linguistics</p>
<p>Multi-logieval: Towards evaluating multi-step logical reasoning ability of large language models. Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024. 2024. November 12-16, 2024</p>
<p>Aligning large and small language models via chain-of-thought reasoning. Leonardo Ranaldi, Andre Freitas, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European Chapter20241Long Papers</p>
<p>Divide and translate: Compositional first-order logic translation and verification for complex logical reasoning. Hyun Ryu, Gyeongman Kim, Hyemin S Lee, Eunho Yang, arXiv:2410.080472024arXiv preprint</p>
<p>Scaling synthetic logical reasoning datasets with context-sensitive declarative grammars. Damien Sileo, 2024</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, 10.18653/v1/D19-1458Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Gupta, arXiv:2206.04615Adrià Garriga-Alonso, and 1 others. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937Commonsenseqa: A question answering challenge targeting commonsense knowledge. 2018arXiv preprint</p>
<p>Diagnosing the firstorder logical reasoning ability through logicnli. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, 10.18653/V1/2021.EMNLP-MAIN.303Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaAssociation for Computational Linguistics2021. 7-11 November, 2021</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Arindam Mitra, and Chitta Baral. Nemika Tyagi, Mihir Parmar, Mohith Kulkarni, Aswin Rrv, Nisarg Patel, Mutsumi Nakamura, arXiv:2407.14790Step-by-step reasoning to solve grid puzzles: Where do llms falter?. 2024arXiv preprint</p>
<p>Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-Tse Huang, Pinjia He, Wenxiang Jiao, Michael R Lyu, Logicasker: Evaluating and improving the logical reasoning ability of large language models. 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235and 1 others</p>
<p>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, arXiv:2505.09388Chenxu Lv, and 1 others. 2025. Qwen3 technical report. arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in neural information processing systems. 202336</p>
<p>Satlm: Satisfiability-aided language models using declarative prompting. Xi Ye, Qiaochu Chen, Advances in Neural Information Processing Systems. 202336Isil Dillig, and Greg Durrett</p>
<p>AbductionRules: Training transformers to explain unexpected inputs. Nathan Young, Qiming Bao, Joshua Bensemann, Michael Witbrock, 10.18653/v1/2022.findings-acl.19Findings of the Association for Computational Linguistics: 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Advances in Neural Information Processing Systems. 202235Jesse Mu, and Noah Goodman</p>
<p>Autologi: Automated generation of logic puzzles for evaluating reasoning abilities of large language models. Qin Zhu, Fei Huang, Runyu Peng, Keming Lu, Bowen Yu, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang, Junyang Lin, 10.48550/ARXIV.2502.16906CoRR, abs/2502.169062025</p>            </div>
        </div>

    </div>
</body>
</html>
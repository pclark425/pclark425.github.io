<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-577 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-577</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-577</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-994a1ce6677b496bd3c0c63aceafc6556005e994</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/994a1ce6677b496bd3c0c63aceafc6556005e994" target="_blank">GLIGEN: Open-Set Grounded Text-to-Image Generation</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> A novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs, and achieves open-world grounded text2img generation with caption and bounding box condition inputs.</p>
                <p><strong>Paper Abstract:</strong> Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines by a large margin.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e577.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e577.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Continual grounding adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continual learning by freezing pretrained diffusion model and injecting gated grounding layers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure that adapts large pretrained text-to-image diffusion models to accept new grounding inputs by freezing the original model weights and training new gated Transformer layers to inject grounding information, preserving pretrained concept knowledge while adding controllable grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Freeze-and-inject continual adaptation for pretrained diffusion models</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Freeze all weights of a pretrained latent diffusion model (LDM/Stable Diffusion) and add new trainable modules (gated Transformer/self-attention layers and MLPs for grounding token construction). During training only the new parameters are updated via the original denoising objective, where the model receives an extended condition composed of the caption and grounding tokens (text/image grounding, boxes, keypoints, or spatial maps). A gated residual term (learnable scalar γ, and scheduling factor β) controls how much grounding information is fused into the visual representation. During inference, β may be scheduled across sampling steps to trade off grounding adherence vs. image quality.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / continual learning adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computer vision / recognition (foundation model pretraining paradigm)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>generative models (text-to-image diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Instead of fine-tuning full model weights, the authors freeze the pretrained UNet and cross-attention blocks and add new gated self-attention layers and small MLPs to encode grounding tokens; grounding token construction uses Fourier embeddings of spatial coordinates concatenated with text/image encodings; gate parameter γ initialized to 0 to avoid immediate influence and enable stable training; classifier-free-style token dropping used during training. The inference schedule allows turning the gating on for early diffusion steps and off later (β schedule) to improve final image aesthetics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - produced open-set grounded text-to-image generation with strong zero-shot generalization and state-of-the-art layout correspondence; e.g., GLIGEN (COCO2014D) achieved FID 5.61 (comparable/better than LDM* baseline 5.91) and YOLO-score AP/AP50/AP75 = 24.0/42.2/24.1 on COCO2014 val; GLIGEN-LDM (COCO2017) achieved FID 21.04 and YOLO 22.4/36.5/24.1, outperforming prior layout2img baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Potential knowledge forgetting if original weights were finetuned; need for stable training when adding new modules; mismatch between pretrained text embedding spaces and newly created grounding tokens; trade-off between adherence to grounding and final visual aesthetic quality.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of high-quality pretrained diffusion models with rich concept knowledge (LDM/Stable Diffusion); use of shared pretrained text encoder for both captions and grounding entities enabling semantic sharing; gated residual design and initializing gate to zero for stable integration; classifier-free token-dropping regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires access to pretrained large-scale diffusion checkpoints and their text encoder internals (to produce compatible token dimensionality), compute to train the new modules, and grounding-annotated or pseudo-annotated datasets for supervision (boxes, keypoints, maps).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within generative-modeling contexts: authors demonstrate extensions across grounding modalities (boxes, keypoints, image prompts, spatial maps) and across base checkpoints (LDM, Stable Diffusion). The paradigm (freeze + inject) is likely applicable to other pretrained generative architectures but is specific to architectures that support tokenized conditioning (cross-attention/self-attention architectures).</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural/explicit (engineering method and architectural modification), instrumental/technical (how to integrate grounding layers into diffusion UNet)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLIGEN: Open-Set Grounded Text-to-Image Generation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e577.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e577.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gated self-attention (Flamingo-inspired)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gated attention mechanism applied as gated self-attention in UNet Transformer blocks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gating mechanism that fuses grounding tokens with visual tokens via an added attention layer whose contribution is controlled by a learned scalar gate, enabling the frozen pre-trained model to leverage grounding information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Flamingo: a visual language model for few-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Gated self-attention over concatenated visual and grounding tokens</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Insert an additional attention layer in each Transformer block that computes self-attention over the concatenation of visual tokens and grounding tokens, then selects the visual-token outputs and adds them as a residual scaled by β * tanh(γ), where γ is a learned scalar initialized at zero and β is a scheduling parameter used during inference. The operation is placed between the original self-attention and cross-attention layers in the pretrained UNet.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / architectural modification</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>multimodal few-shot models (Flamingo / vision-language architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>latent diffusion UNet for text-to-image generation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Rather than gated cross-attention used in Flamingo, the authors implement gated self-attention that attends jointly over visual + grounding tokens and writes back only to visual tokens (token selection). Gate γ is initialized to 0 to avoid disrupting pretrained behavior; concatenation and dimensional projections are added to map grounding token dims to visual dims; β scheduling allows turning gating on/off across diffusion steps.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - gating enabled the model to use grounding signals and improved layout correspondence; ablation shows gated self-attention yields YOLO AP 21.7 on COCO2014CD vs. gated cross-attention variant which produced YOLO AP 16.6 (with similar FID ~5.8), indicating clear empirical benefit of self-attention variant.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Cross-attention style gating (as in Flamingo) led to worse grounding performance in this architecture, indicating non-trivial architectural compatibility issues; need to align token dimensions and ensure stability when adding attention layers to a frozen network.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Shared token-based attention architecture of diffusion UNet made attention-based injection feasible; inspiration from Flamingo's gating concept gave a principled mechanism; initializing gate to zero promoted stable training by avoiding immediate large perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to UNet Transformer blocks and ability to insert and project grounding tokens to visual token dimensions; compute and training data with grounding supervision to learn gate and projections.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Likely generalizable to other attention-based generative architectures (e.g., other diffusion UNets, transformer decoders) but requires careful adaptation (self-attention vs cross-attention choice) and gating initialization; not necessarily transferable verbatim between architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural/technical (architectural engineering insight) and explicit theoretical (use of attention/gating principles)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLIGEN: Open-Set Grounded Text-to-Image Generation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e577.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e577.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fourier coordinate embedding (from NeRF)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fourier positional embedding for spatial coordinates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using sinusoidal/Fourier feature mapping to embed bounding box or keypoint coordinates into a high-dimensional space before concatenation with textual/image embeddings for grounding tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nerf: Representing scenes as neural radiance fields for view synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Fourier embedding for spatial coordinates</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Apply a Fourier feature mapping to normalized bounding box coordinates [α_min, β_min, α_max, β_max] (or keypoint x,y), producing a 64-dimensional Fourier embedding which is concatenated with the entity text/image embedding and passed through an MLP to form a grounding token compatible with the diffusion model's attention modules.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data encoding / representation technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>3D view synthesis / neural scene representations (NeRF literature)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>spatial conditioning for latent diffusion text-to-image models</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with minimal modification</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied the same Fourier feature idea to 2D bounding box and keypoint coordinates and chose an output dimension (64) appropriate for the grounding token pipeline; concatenated positional embedding with pretrained text/image embeddings and passed through a 3-layer MLP to produce a token of the text embedding dimension (e.g., 768).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - ablation shows Fourier embedding retained image quality (FID 5.82) but yielded substantially better layout correspondence (YOLO AP 21.7) compared to replacing Fourier with a learned MLP-only embedding (YOLO AP 3.2), demonstrating the method's efficacy for spatial encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Need to choose embedding dimensionality and normalization to match text embedding scale; naive learned MLP embedding underperformed, suggesting sensitivity to encoding inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Fourier embedding's ability to represent high-frequency spatial variations and relative positions enabled the model to learn spatial relationships robustly; well-understood practice from NeRF literature guided choice.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Coordinate normalization and consistent feature scaling when concatenating with pretrained text embeddings; small MLP and projection layers to map to token dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Generalizable to other tasks that require representing continuous spatial coordinates as tokens for attention-based models (keypoints, boxes, continuous maps) but parameters (dimensionality) may need retuning by task.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural and representational knowledge (feature encoding design)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLIGEN: Open-Set Grounded Text-to-Image Generation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e577.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e577.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLIP pseudo-labeling & data-format unification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using GLIP detection model to generate pseudo grounding labels and unify detection/grounding data formats for generative training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Apply a pretrained grounded language-image detector (GLIP) to generate pseudo bounding boxes for noun phrases in caption datasets and unify diverse detection and grounding datasets into a common training format for grounded generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grounded language-image pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>GLIP-based pseudo-label generation and dataset unification</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Run the GLIP detector on image-caption datasets (e.g., CC3M, SBU) to detect noun entities mentioned in captions and produce pseudo bounding boxes; combine these pseudo-labeled datasets with explicit detection datasets (e.g., Object365, GoldG) into a joint training corpus so the diffusion model can learn open-vocabulary grounding. This unification leverages GLIP's grounded language-image alignment to extend supervision to large caption datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data augmentation / annotation transfer technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>object detection / grounded language-image pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>training data creation for grounded generative models</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied GLIP inference to captioned image corpora to create pseudo box annotations; included these pseudo-labeled samples in the generative model's training mix alongside true detection datasets; used different instruction formats (detection-only with null caption, detection+caption, grounding data where noun phrases are extracted from captions) during training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - scaling training data with GLIP pseudo-labels and adding Object365/GoldG/CC3M/SBU improved zero-shot open-vocabulary grounding: GLIGEN-LDM trained on GoldG,O365,SBU,CC3M achieved GLIP-score AP 11.1 on LVIS validation (vs. AP 6.4 for COCO2014CD-trained GLIGEN-LDM), and finetuning further increased AP to 14.9 (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Pseudo-label noise from automatic detections can degrade visual quality or grounding if not handled; GLIGEN trained on GLIP pseudo-labels (COCO2014G) had slightly worse FID and YOLO scores attributed to pseudo-label quality.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of a strong grounded detection model (GLIP) that maps language phrases to image regions; abundant captioned datasets to pseudo-annotate, increasing coverage of open-vocabulary concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to pretrained GLIP and large captioned image corpora; careful mixing strategy to balance clean detection annotations with noisy pseudo-labels; possible domain adaptation or weighting to mitigate label noise.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>General approach applicable to other settings where a powerful recognition model can produce pseudo supervision for a generative model; effectiveness depends on the quality of the pseudo-labeler and similarity of domains.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical (data annotation protocol) and practical know-how (how to integrate pseudo labels into training)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLIGEN: Open-Set Grounded Text-to-Image Generation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e577.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e577.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP projection for image-grounded tokens</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Projecting CLIP image features into CLIP text feature space for unified grounding tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique to create image-based grounding tokens by projecting CLIP image encoder outputs into the CLIP text embedding space to produce conditioning tokens compatible with caption tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>CLIP image-to-text projection for grounding token construction</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Take the CLIP image encoder's CLS token embedding, apply the linear projection composition P_t^T P_i to map image features into the CLIP text embedding space used by caption tokens, and normalize to an empirically chosen norm (28.7) to match text token magnitudes. Concatenate the projected image embedding with Fourier positional encoding of the box and feed through an MLP to generate the grounding token.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>representation alignment / feature projection</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>contrastive image-text representation learning (CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>multimodal conditioning for diffusion generation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Instead of using raw CLIP image embedding, the authors project image features into the text embedding space using the transpose of the text linear head and the image linear head (P_t^T P_i) and normalize magnitudes to empirical average of text tokens; then combined with Fourier positional encoding and MLP to produce tokens compatible with frozen caption embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - enabled image-conditioned grounding tokens that integrate smoothly with caption tokens and accelerated training convergence (authors report empirical faster convergence), and supported mixed text+image grounding use-cases (style transfer, reference-image grounding). Quantitative metrics for this projection specifically are not isolated, but image-grounded examples are demonstrated qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Need to find an appropriate projection and normalization to align image and text embedding norms/spaces; mismatch could slow training or reduce ability to generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Known CLIP linear projection heads and training objective provide a principled linear-alignment approach; using EOS/CLS token semantics yields whole-text/whole-image features for grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to the CLIP encoder weights (text and image linear heads) and consistent embedding dimensionalities; normalization hyperparameter selection.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Applicable to other multi-modal systems that share or can be mapped to a common embedding space; projection approach depends on availability of linear heads or learned mappings between modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>technical/instrumental (feature alignment technique) and procedural (how to build image-grounded tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLIGEN: Open-Set Grounded Text-to-Image Generation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e577.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e577.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inference scheduled sampling for diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-stage scheduled gating (beta) during diffusion sampling to balance grounding and aesthetic quality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time scheduling technique that applies grounding conditioning strongly in early diffusion steps and reduces or removes it in later steps to improve image quality while preserving coarse grounded layout.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Two-stage beta-scheduled inference (early grounded, late standard)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Divide the T diffusion sampling steps into two phases determined by τ (0 ≤ τ ≤ 1). For t ≤ τ*T set gating factor β=1 so gated self-attention injects grounding information strongly; for t > τ*T set β=0 to disable grounding injection and let the frozen pretrained model's later denoising steps refine image quality. This effectively makes grounding influence limited to early, coarse-layout stages.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>inference algorithm modification / sampling schedule</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>sequence modeling / curriculum/scheduled sampling concepts (general ML), but introduced here for diffusion inference</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>latent diffusion model inference for grounded generation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied the concept of stage-wise conditioning to diffusion sampling by gating added attention layers with β; implemented simple binary schedule controlled by τ (authors used τ=0.2 in examples). The gate is applied to residual updates produced by gated self-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - qualitatively and quantitatively improved final visual quality while maintaining layout adherence; enabled domain extension (e.g., a model trained on human keypoints can be steered to produce non-human but humanlike shapes) as shown in Fig.7; authors report improved perceived quality in examples and reduced conflicts between grounding and learned aesthetic priors of the pretrained model.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Choosing τ requires tuning by task; disabling grounding too early can lose precise placement, leaving it on too long can degrade aesthetic quality; not a panacea for mismatched grounding domains (e.g., keypoint semantics may not transfer across very different object categories).</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Freezing original weights made it possible to rely on pretrained late-stage denoisers for quality, enabling the two-stage approach; gated residual architecture made it straightforward to scale grounding influence with β.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to gating mechanism (β) and ability to control it during inference; some validation data or perceptual evaluation to select τ for best trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Likely generalizable to other generative models that allow staged conditioning (where early steps determine coarse layout and later steps refine details), but the optimal schedule may vary by base model and task.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural/explicit (inference protocol) and practical know-how (tuning τ and gate behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLIGEN: Open-Set Grounded Text-to-Image Generation', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Flamingo: a visual language model for few-shot learning <em>(Rating: 2)</em></li>
                <li>Nerf: Representing scenes as neural radiance fields for view synthesis <em>(Rating: 2)</em></li>
                <li>Grounded language-image pre-training <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>High-resolution image synthesis with latent diffusion models <em>(Rating: 2)</em></li>
                <li>Diffusion models beat gans on image synthesis <em>(Rating: 1)</em></li>
                <li>Photorealistic text-to-image diffusion models with deep language understanding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-577",
    "paper_id": "paper-994a1ce6677b496bd3c0c63aceafc6556005e994",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "Continual grounding adaptation",
            "name_full": "Continual learning by freezing pretrained diffusion model and injecting gated grounding layers",
            "brief_description": "A procedure that adapts large pretrained text-to-image diffusion models to accept new grounding inputs by freezing the original model weights and training new gated Transformer layers to inject grounding information, preserving pretrained concept knowledge while adding controllable grounding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Freeze-and-inject continual adaptation for pretrained diffusion models",
            "procedure_description": "Freeze all weights of a pretrained latent diffusion model (LDM/Stable Diffusion) and add new trainable modules (gated Transformer/self-attention layers and MLPs for grounding token construction). During training only the new parameters are updated via the original denoising objective, where the model receives an extended condition composed of the caption and grounding tokens (text/image grounding, boxes, keypoints, or spatial maps). A gated residual term (learnable scalar γ, and scheduling factor β) controls how much grounding information is fused into the visual representation. During inference, β may be scheduled across sampling steps to trade off grounding adherence vs. image quality.",
            "procedure_type": "computational method / continual learning adaptation",
            "source_domain": "computer vision / recognition (foundation model pretraining paradigm)",
            "target_domain": "generative models (text-to-image diffusion)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Instead of fine-tuning full model weights, the authors freeze the pretrained UNet and cross-attention blocks and add new gated self-attention layers and small MLPs to encode grounding tokens; grounding token construction uses Fourier embeddings of spatial coordinates concatenated with text/image encodings; gate parameter γ initialized to 0 to avoid immediate influence and enable stable training; classifier-free-style token dropping used during training. The inference schedule allows turning the gating on for early diffusion steps and off later (β schedule) to improve final image aesthetics.",
            "transfer_success": "successful - produced open-set grounded text-to-image generation with strong zero-shot generalization and state-of-the-art layout correspondence; e.g., GLIGEN (COCO2014D) achieved FID 5.61 (comparable/better than LDM* baseline 5.91) and YOLO-score AP/AP50/AP75 = 24.0/42.2/24.1 on COCO2014 val; GLIGEN-LDM (COCO2017) achieved FID 21.04 and YOLO 22.4/36.5/24.1, outperforming prior layout2img baselines.",
            "barriers_encountered": "Potential knowledge forgetting if original weights were finetuned; need for stable training when adding new modules; mismatch between pretrained text embedding spaces and newly created grounding tokens; trade-off between adherence to grounding and final visual aesthetic quality.",
            "facilitating_factors": "Availability of high-quality pretrained diffusion models with rich concept knowledge (LDM/Stable Diffusion); use of shared pretrained text encoder for both captions and grounding entities enabling semantic sharing; gated residual design and initializing gate to zero for stable integration; classifier-free token-dropping regularization.",
            "contextual_requirements": "Requires access to pretrained large-scale diffusion checkpoints and their text encoder internals (to produce compatible token dimensionality), compute to train the new modules, and grounding-annotated or pseudo-annotated datasets for supervision (boxes, keypoints, maps).",
            "generalizability": "High within generative-modeling contexts: authors demonstrate extensions across grounding modalities (boxes, keypoints, image prompts, spatial maps) and across base checkpoints (LDM, Stable Diffusion). The paradigm (freeze + inject) is likely applicable to other pretrained generative architectures but is specific to architectures that support tokenized conditioning (cross-attention/self-attention architectures).",
            "knowledge_type": "procedural/explicit (engineering method and architectural modification), instrumental/technical (how to integrate grounding layers into diffusion UNet)",
            "uuid": "e577.0",
            "source_info": {
                "paper_title": "GLIGEN: Open-Set Grounded Text-to-Image Generation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Gated self-attention (Flamingo-inspired)",
            "name_full": "Gated attention mechanism applied as gated self-attention in UNet Transformer blocks",
            "brief_description": "A gating mechanism that fuses grounding tokens with visual tokens via an added attention layer whose contribution is controlled by a learned scalar gate, enabling the frozen pre-trained model to leverage grounding information.",
            "citation_title": "Flamingo: a visual language model for few-shot learning",
            "mention_or_use": "use",
            "procedure_name": "Gated self-attention over concatenated visual and grounding tokens",
            "procedure_description": "Insert an additional attention layer in each Transformer block that computes self-attention over the concatenation of visual tokens and grounding tokens, then selects the visual-token outputs and adds them as a residual scaled by β * tanh(γ), where γ is a learned scalar initialized at zero and β is a scheduling parameter used during inference. The operation is placed between the original self-attention and cross-attention layers in the pretrained UNet.",
            "procedure_type": "computational method / architectural modification",
            "source_domain": "multimodal few-shot models (Flamingo / vision-language architectures)",
            "target_domain": "latent diffusion UNet for text-to-image generation",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Rather than gated cross-attention used in Flamingo, the authors implement gated self-attention that attends jointly over visual + grounding tokens and writes back only to visual tokens (token selection). Gate γ is initialized to 0 to avoid disrupting pretrained behavior; concatenation and dimensional projections are added to map grounding token dims to visual dims; β scheduling allows turning gating on/off across diffusion steps.",
            "transfer_success": "successful - gating enabled the model to use grounding signals and improved layout correspondence; ablation shows gated self-attention yields YOLO AP 21.7 on COCO2014CD vs. gated cross-attention variant which produced YOLO AP 16.6 (with similar FID ~5.8), indicating clear empirical benefit of self-attention variant.",
            "barriers_encountered": "Cross-attention style gating (as in Flamingo) led to worse grounding performance in this architecture, indicating non-trivial architectural compatibility issues; need to align token dimensions and ensure stability when adding attention layers to a frozen network.",
            "facilitating_factors": "Shared token-based attention architecture of diffusion UNet made attention-based injection feasible; inspiration from Flamingo's gating concept gave a principled mechanism; initializing gate to zero promoted stable training by avoiding immediate large perturbations.",
            "contextual_requirements": "Access to UNet Transformer blocks and ability to insert and project grounding tokens to visual token dimensions; compute and training data with grounding supervision to learn gate and projections.",
            "generalizability": "Likely generalizable to other attention-based generative architectures (e.g., other diffusion UNets, transformer decoders) but requires careful adaptation (self-attention vs cross-attention choice) and gating initialization; not necessarily transferable verbatim between architectures.",
            "knowledge_type": "procedural/technical (architectural engineering insight) and explicit theoretical (use of attention/gating principles)",
            "uuid": "e577.1",
            "source_info": {
                "paper_title": "GLIGEN: Open-Set Grounded Text-to-Image Generation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Fourier coordinate embedding (from NeRF)",
            "name_full": "Fourier positional embedding for spatial coordinates",
            "brief_description": "Using sinusoidal/Fourier feature mapping to embed bounding box or keypoint coordinates into a high-dimensional space before concatenation with textual/image embeddings for grounding tokens.",
            "citation_title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "mention_or_use": "use",
            "procedure_name": "Fourier embedding for spatial coordinates",
            "procedure_description": "Apply a Fourier feature mapping to normalized bounding box coordinates [α_min, β_min, α_max, β_max] (or keypoint x,y), producing a 64-dimensional Fourier embedding which is concatenated with the entity text/image embedding and passed through an MLP to form a grounding token compatible with the diffusion model's attention modules.",
            "procedure_type": "data encoding / representation technique",
            "source_domain": "3D view synthesis / neural scene representations (NeRF literature)",
            "target_domain": "spatial conditioning for latent diffusion text-to-image models",
            "transfer_type": "direct application with minimal modification",
            "modifications_made": "Applied the same Fourier feature idea to 2D bounding box and keypoint coordinates and chose an output dimension (64) appropriate for the grounding token pipeline; concatenated positional embedding with pretrained text/image embeddings and passed through a 3-layer MLP to produce a token of the text embedding dimension (e.g., 768).",
            "transfer_success": "successful - ablation shows Fourier embedding retained image quality (FID 5.82) but yielded substantially better layout correspondence (YOLO AP 21.7) compared to replacing Fourier with a learned MLP-only embedding (YOLO AP 3.2), demonstrating the method's efficacy for spatial encoding.",
            "barriers_encountered": "Need to choose embedding dimensionality and normalization to match text embedding scale; naive learned MLP embedding underperformed, suggesting sensitivity to encoding inductive bias.",
            "facilitating_factors": "Fourier embedding's ability to represent high-frequency spatial variations and relative positions enabled the model to learn spatial relationships robustly; well-understood practice from NeRF literature guided choice.",
            "contextual_requirements": "Coordinate normalization and consistent feature scaling when concatenating with pretrained text embeddings; small MLP and projection layers to map to token dimension.",
            "generalizability": "Generalizable to other tasks that require representing continuous spatial coordinates as tokens for attention-based models (keypoints, boxes, continuous maps) but parameters (dimensionality) may need retuning by task.",
            "knowledge_type": "explicit procedural and representational knowledge (feature encoding design)",
            "uuid": "e577.2",
            "source_info": {
                "paper_title": "GLIGEN: Open-Set Grounded Text-to-Image Generation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "GLIP pseudo-labeling & data-format unification",
            "name_full": "Using GLIP detection model to generate pseudo grounding labels and unify detection/grounding data formats for generative training",
            "brief_description": "Apply a pretrained grounded language-image detector (GLIP) to generate pseudo bounding boxes for noun phrases in caption datasets and unify diverse detection and grounding datasets into a common training format for grounded generation.",
            "citation_title": "Grounded language-image pre-training",
            "mention_or_use": "use",
            "procedure_name": "GLIP-based pseudo-label generation and dataset unification",
            "procedure_description": "Run the GLIP detector on image-caption datasets (e.g., CC3M, SBU) to detect noun entities mentioned in captions and produce pseudo bounding boxes; combine these pseudo-labeled datasets with explicit detection datasets (e.g., Object365, GoldG) into a joint training corpus so the diffusion model can learn open-vocabulary grounding. This unification leverages GLIP's grounded language-image alignment to extend supervision to large caption datasets.",
            "procedure_type": "data augmentation / annotation transfer technique",
            "source_domain": "object detection / grounded language-image pretraining",
            "target_domain": "training data creation for grounded generative models",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Applied GLIP inference to captioned image corpora to create pseudo box annotations; included these pseudo-labeled samples in the generative model's training mix alongside true detection datasets; used different instruction formats (detection-only with null caption, detection+caption, grounding data where noun phrases are extracted from captions) during training.",
            "transfer_success": "successful - scaling training data with GLIP pseudo-labels and adding Object365/GoldG/CC3M/SBU improved zero-shot open-vocabulary grounding: GLIGEN-LDM trained on GoldG,O365,SBU,CC3M achieved GLIP-score AP 11.1 on LVIS validation (vs. AP 6.4 for COCO2014CD-trained GLIGEN-LDM), and finetuning further increased AP to 14.9 (Table 5).",
            "barriers_encountered": "Pseudo-label noise from automatic detections can degrade visual quality or grounding if not handled; GLIGEN trained on GLIP pseudo-labels (COCO2014G) had slightly worse FID and YOLO scores attributed to pseudo-label quality.",
            "facilitating_factors": "Availability of a strong grounded detection model (GLIP) that maps language phrases to image regions; abundant captioned datasets to pseudo-annotate, increasing coverage of open-vocabulary concepts.",
            "contextual_requirements": "Access to pretrained GLIP and large captioned image corpora; careful mixing strategy to balance clean detection annotations with noisy pseudo-labels; possible domain adaptation or weighting to mitigate label noise.",
            "generalizability": "General approach applicable to other settings where a powerful recognition model can produce pseudo supervision for a generative model; effectiveness depends on the quality of the pseudo-labeler and similarity of domains.",
            "knowledge_type": "instrumental/technical (data annotation protocol) and practical know-how (how to integrate pseudo labels into training)",
            "uuid": "e577.3",
            "source_info": {
                "paper_title": "GLIGEN: Open-Set Grounded Text-to-Image Generation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "CLIP projection for image-grounded tokens",
            "name_full": "Projecting CLIP image features into CLIP text feature space for unified grounding tokens",
            "brief_description": "A technique to create image-based grounding tokens by projecting CLIP image encoder outputs into the CLIP text embedding space to produce conditioning tokens compatible with caption tokens.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "procedure_name": "CLIP image-to-text projection for grounding token construction",
            "procedure_description": "Take the CLIP image encoder's CLS token embedding, apply the linear projection composition P_t^T P_i to map image features into the CLIP text embedding space used by caption tokens, and normalize to an empirically chosen norm (28.7) to match text token magnitudes. Concatenate the projected image embedding with Fourier positional encoding of the box and feed through an MLP to generate the grounding token.",
            "procedure_type": "representation alignment / feature projection",
            "source_domain": "contrastive image-text representation learning (CLIP)",
            "target_domain": "multimodal conditioning for diffusion generation",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Instead of using raw CLIP image embedding, the authors project image features into the text embedding space using the transpose of the text linear head and the image linear head (P_t^T P_i) and normalize magnitudes to empirical average of text tokens; then combined with Fourier positional encoding and MLP to produce tokens compatible with frozen caption embeddings.",
            "transfer_success": "successful - enabled image-conditioned grounding tokens that integrate smoothly with caption tokens and accelerated training convergence (authors report empirical faster convergence), and supported mixed text+image grounding use-cases (style transfer, reference-image grounding). Quantitative metrics for this projection specifically are not isolated, but image-grounded examples are demonstrated qualitatively.",
            "barriers_encountered": "Need to find an appropriate projection and normalization to align image and text embedding norms/spaces; mismatch could slow training or reduce ability to generalize.",
            "facilitating_factors": "Known CLIP linear projection heads and training objective provide a principled linear-alignment approach; using EOS/CLS token semantics yields whole-text/whole-image features for grounding.",
            "contextual_requirements": "Access to the CLIP encoder weights (text and image linear heads) and consistent embedding dimensionalities; normalization hyperparameter selection.",
            "generalizability": "Applicable to other multi-modal systems that share or can be mapped to a common embedding space; projection approach depends on availability of linear heads or learned mappings between modalities.",
            "knowledge_type": "technical/instrumental (feature alignment technique) and procedural (how to build image-grounded tokens)",
            "uuid": "e577.4",
            "source_info": {
                "paper_title": "GLIGEN: Open-Set Grounded Text-to-Image Generation",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Inference scheduled sampling for diffusion",
            "name_full": "Two-stage scheduled gating (beta) during diffusion sampling to balance grounding and aesthetic quality",
            "brief_description": "An inference-time scheduling technique that applies grounding conditioning strongly in early diffusion steps and reduces or removes it in later steps to improve image quality while preserving coarse grounded layout.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Two-stage beta-scheduled inference (early grounded, late standard)",
            "procedure_description": "Divide the T diffusion sampling steps into two phases determined by τ (0 ≤ τ ≤ 1). For t ≤ τ*T set gating factor β=1 so gated self-attention injects grounding information strongly; for t &gt; τ*T set β=0 to disable grounding injection and let the frozen pretrained model's later denoising steps refine image quality. This effectively makes grounding influence limited to early, coarse-layout stages.",
            "procedure_type": "inference algorithm modification / sampling schedule",
            "source_domain": "sequence modeling / curriculum/scheduled sampling concepts (general ML), but introduced here for diffusion inference",
            "target_domain": "latent diffusion model inference for grounded generation",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Applied the concept of stage-wise conditioning to diffusion sampling by gating added attention layers with β; implemented simple binary schedule controlled by τ (authors used τ=0.2 in examples). The gate is applied to residual updates produced by gated self-attention.",
            "transfer_success": "successful - qualitatively and quantitatively improved final visual quality while maintaining layout adherence; enabled domain extension (e.g., a model trained on human keypoints can be steered to produce non-human but humanlike shapes) as shown in Fig.7; authors report improved perceived quality in examples and reduced conflicts between grounding and learned aesthetic priors of the pretrained model.",
            "barriers_encountered": "Choosing τ requires tuning by task; disabling grounding too early can lose precise placement, leaving it on too long can degrade aesthetic quality; not a panacea for mismatched grounding domains (e.g., keypoint semantics may not transfer across very different object categories).",
            "facilitating_factors": "Freezing original weights made it possible to rely on pretrained late-stage denoisers for quality, enabling the two-stage approach; gated residual architecture made it straightforward to scale grounding influence with β.",
            "contextual_requirements": "Access to gating mechanism (β) and ability to control it during inference; some validation data or perceptual evaluation to select τ for best trade-off.",
            "generalizability": "Likely generalizable to other generative models that allow staged conditioning (where early steps determine coarse layout and later steps refine details), but the optimal schedule may vary by base model and task.",
            "knowledge_type": "procedural/explicit (inference protocol) and practical know-how (tuning τ and gate behavior)",
            "uuid": "e577.5",
            "source_info": {
                "paper_title": "GLIGEN: Open-Set Grounded Text-to-Image Generation",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Flamingo: a visual language model for few-shot learning",
            "rating": 2
        },
        {
            "paper_title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "rating": 2
        },
        {
            "paper_title": "Grounded language-image pre-training",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "High-resolution image synthesis with latent diffusion models",
            "rating": 2
        },
        {
            "paper_title": "Diffusion models beat gans on image synthesis",
            "rating": 1
        },
        {
            "paper_title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "rating": 1
        }
    ],
    "cost": 0.0196295,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GLIGEN: Open-Set Grounded Text-to-Image Generation</h1>
<p>Yuheng $\mathrm{Li}^{1 \S}$, Haotian $\mathrm{Liu}^{1 \S}$, Qingyang $\mathrm{Wu}^{2}$, Fangzhou $\mathrm{Mu}^{1}$, Jianwei Yang ${ }^{3}$, Jianfeng Gao ${ }^{3}$, Chunyuan $\mathrm{Li}^{3 \pi}$, Yong Jae Lee ${ }^{1 \pi}$<br>${ }^{1}$ University of Wisconsin-Madison ${ }^{2}$ Columbia University ${ }^{3}$ Microsoft https://gligen.github.io/</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. GLIGEN enables versatile grounding capabilities for a frozen text-to-image generation model, by feeding different grounding conditions. GLIGEN supports (a) text entity + box, (b) image entity + box, (c) image style and text + box, (d) keypoints, (e) depth map, (f) edge map, (g) normal map, and (h) semantic map.</p>
<h2>Abstract</h2>
<p>Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose Gligen, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of
the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zeroshot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines by a large margin.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1. Introduction</h2>
<p>Image generation research has witnessed huge advances in recent years. Over the past couple of years, GANs [14] were the state-of-the-art, with their latent space and conditional inputs being well-studied for controllable manipulation [48, 60] and generation [27, 29, 47, 82]. Text conditional autoregressive [52, 74] and diffusion [51, 56] models have demonstrated astonishing image quality and concept coverage, due to their more stable learning objectives and large-scale training on web image-text paired data. These models have gained attention even among the general public due to their practical use cases (e.g., art design and creation).</p>
<p>Despite exciting progress, existing large-scale text-toimage generation models cannot be conditioned on other input modalities apart from text, and thus lack the ability to precisely localize concepts, use reference images, or other conditional inputs to control the generation process. The current input, i.e., natural language alone, restricts the way that information can be expressed. For example, it is difficult to describe the precise location of an object using text, whereas bounding boxes / keypoints can easily achieve this, as shown in Figure 1. While conditional diffusion models [10, 53, 55] and GANs [26, 37, 48, 71] that take in input modalities other than text for inpainting, layout2img generation, etc., do exist, they rarely combine those inputs for controllable text2img generation.</p>
<p>Moreover, prior generative models-regardless of the generative model family-are usually independently trained on each task-specific dataset. In contrast, in the recognition field, the long-standing paradigm has been to build recognition models [32, 42, 84] by starting from a foundation model pretrained on large-scale image data [4, 16, 17] or image-text pairs [33, 50, 75]. Since diffusion models have been trained on billions of image-text pairs [53], a natural question is: Can we build upon existing pretrained diffusion models and endow them with new conditional input modalities? In this way, analogous to the recognition literature, we may be able to achieve better performance on other generation tasks due to the vast concept knowledge that the pretrained models have, while acquiring more controllability over existing text-to-image generation models.</p>
<p>With the above aims, we propose a method for providing new grounding conditional inputs to pretrained text-to-image diffusion models. As shown in Figure 1, we still retain the text caption as input, but also enable other input modalities such as bounding boxes for grounding concepts, grounding reference images, grounding part keypoints, etc. The key challenge is preserving the original vast concept knowledge in the pretrained model while learning to inject the new grounding information. To prevent knowledge forgetting, we propose to freeze the original model weights and add new trainable gated Transformer layers [67] that take in the new grounding input (e.g., bounding box). During training, we gradually fuse the new grounding information into the pretrained model using a gated mechanism [1]. This design enables flexibility in the sampling process during generation for improved quality and controllability; for example, we show that using the full model (all layers) in the first half of the sampling steps and only using the original layers (without the gated Transformer layers) in the latter half can lead to generation results that accurately reflect the grounding conditions while also having high image quality.</p>
<p>In our experiments, we primarily study grounded text2img generation with bounding boxes, inspired by the recent scaling success of learning grounded languageimage understanding models with boxes in GLIP [34].To enable our model to ground open-world vocabulary concepts [32,34,76,79], we use the same pre-trained text encoder (for encoding the caption) to encode each phrase associated with each grounded entity (i.e., one phrase per bounding box) and feed the encoded tokens into the newly inserted layers with their encoded location information. Due to the shared text space, we find that our model can generalize to unseen objects even when only trained on the COCO [41] dataset. Its generalization on LVIS [15] outperforms a strong fully-supervised baseline by a large margin. To further improve our model's grounding ability, we unify the object detection and grounding data formats for training, following GLIP [34]. With larger training data, our model's generalization is consistently improved.
Contributions. 1) We propose a new text2img generation method that endows new grounding controllability over existing text2img diffusion models. 2) By preserving the pretrained weights and learning to gradually integrate the new localization layers, our model achieves open-world grounded text2img generation with bounding box inputs, i.e., synthesis of novel localized concepts unobserved in training. 3) Our model's zero-shot performance on layout2img tasks significantly outperforms the prior state-of-the-art, demonstrating the power of building upon large pretrained generative models for downstream tasks.</p>
<h2>2. Related Work</h2>
<p>Large scale text-to-image generation models. State-of-the-art models in this space are either autoregressive [13, 52, 69, 74] or diffusion [45, 51, 53, 56, 81]. Among autoregressive models, DALL-E [52] is one of the breakthrough works that demonstrates zero-shot abilities, while Parti [74] demonstrates the feasibility of scaling up autoregressive models. Diffusion models have also shown very promising results. DALL-E 2 [51] generates images from the CLIP [50] image space, while Imagen [56] finds the benefit of using pretrained language models. The concurrent Muse [6] demonstrates that masked modeling can achieve SoTA-level generation performance with higher inference speed. However, all of these models usually only take a caption as the input, which</p>
<p>can be difficult for conveying other information such as the precise location of an object. Make-A-Scene [13] also incorporates semantic maps into its text-to-image generation, by training an encoder to tokenize semantic masks to condition the generation. However, it can only operate in a closed-set (of 158 categories), whereas our grounded entities can be open-world. A concurrent work eDiff-I [3] shows that by changing the attention map, one can generate objects that roughly follow a semantic map input. However, We believe our interface with boxes is simpler, and more importantly, our method allows other conditioning inputs such as keypoints, edge map, inference images, etc., which are hard to manipulate through attention.</p>
<p>Image generation from layouts. Given bounding boxes labeled with object categories, the task is to generate a corresponding image [24,39,61-63, 72, 78], which is the reverse task of object detection. Layout2Im [78] formulated the problem and combined a VAE object encoder, an LSTM [22] object fuser, and an image decoder to generate the image, using global and object-level adversarial losses [14] to enforce realism and layout correspondence. LostGAN [61,62] generates a mask representation which is used to normalize features, taking inspiration from StyleGAN [28]. LAMA [39] improves the intermediate mask quality for better image quality. Transformer [66] based methods [24, 72] have also been explored. Critically, existing layout2image methods are closed-set, i.e., they can only generate limited localized visual concepts observed in the training set such as the 80 categories in COCO. In contrast, our method represents the first work for open-set grounded image generation. A concurrent work ReCo [73] also demonstrates open-set abilities by building upon a pretraned Stable Diffusion model [53]. However, it finetunes the original model weights, which has the potential to lead to knowledge forgetting. Furthermore, it only demonstrates box grounding results whereas we show results on more modalities as shown in the Figure 1.</p>
<p>Other conditional image generation. For GANs, various conditioning information have been explored; e.g., text [65, 70, 80], box [61, 62, 78], semantic masks [36, 47], images [8,38,83]. For diffusion models, LDM [53] proposes a unified approach for conditional generation by injecting the condition via cross-attention layers. Palette [55] performs image-to-image tasks using diffusion models. These models are usually trained from scratch independently. In our work, we investigate how to build upon existing models pretrained on large-scale web data, to enable new open-set grounded image generation capabilities in a cost-effective manner.</p>
<h2>3. Preliminaries on Latent Diffusion Models</h2>
<p>Diffusion-based methods are one of the most effective model families for text2image tasks, among which latent diffusion model (LDM) [53] and its successor Stable Dif-
fusion are the most powerful models publicly available to the research community. To reduce the computational costs of vanilla diffusion model training, LDM proceeds in two stages. The first stage learns a bidirectional mapping network to obtain the latent representation $\boldsymbol{z}$ of the image $\boldsymbol{x}$. The second stage trains a diffusion model on the latent $\boldsymbol{z}$. Since the first stage model produces a fixed bidirectional mapping between $\boldsymbol{x}$ and $\boldsymbol{z}$, from hereon, we focus on the latent generation space of LDM for simplicity.</p>
<p>Training Objective. Starting from noise $\boldsymbol{z}<em T-1="T-1">{T}$, the model gradually produces less noisy samples $\boldsymbol{z}</em>}, \boldsymbol{z<em 0="0">{T-2}, \cdots, \boldsymbol{z}</em>$ :}$, conditioned on caption $\boldsymbol{c}$ at every time step $t$. To learn such a model $f_{\boldsymbol{\theta}}$ parameterized by $\boldsymbol{\theta}$, for each step, the LDM training objective solves the denoising problem on latent representations $\boldsymbol{z}$ of the image $\boldsymbol{x</p>
<p>$$
\min <em _mathrm_LDM="\mathrm{LDM">{\boldsymbol{\theta}} \mathcal{L}</em>}}=\mathbb{E<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{z}, \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), t}\left[\left|\boldsymbol{\epsilon}-f</em>}}\left(\boldsymbol{z<em 2="2">{t}, t, \boldsymbol{c}\right)\right|</em>\right]
$$}^{2</p>
<p>where $t$ is uniformly sampled from time steps ${1, \cdots, T}$, $\boldsymbol{z}<em _boldsymbol_theta="\boldsymbol{\theta">{t}$ is the step- $t$ noisy variant of input $\boldsymbol{z}$, and $f</em>)$-conditioned denoising autoencoder.}}(*, t, \boldsymbol{c})$ is the $(t, \boldsymbol{c</p>
<p>Network Architecture. The core of the network architecture is how to encode the conditions, based on which a cleaner version of $\boldsymbol{z}$ is produced. (i) Denoising Autoencoder. $f_{\boldsymbol{\theta}}(*, t, \boldsymbol{c})$ is implemented via UNet [54]. It takes in a noisy latent $\boldsymbol{z}$, as well as information from time step $t$ and condition $\boldsymbol{c}$. It consists of a series of ResNet [19] and Transformer [67] blocks. (ii) Condition Encoding. In the original LDM, a BERT-like [9] network is trained from scratch to encode each caption into a sequence of text embeddings, $f_{\text {text }}(\boldsymbol{c})$, which is fed into (1) to replace $\boldsymbol{c}$. The caption feature is encoded via a fixed CLIP [50] text encoder in Stable Diffusion. Time $t$ is first mapped to time embedding $\phi(t)$, then injected into the UNet. The caption feature is used in a cross attention layer within each Transformer block. The model learns to predict the noise, following (1).</p>
<p>With large-scale training, the model $f_{\boldsymbol{\theta}}(*, t, \boldsymbol{c})$ is well trained to denoise $\boldsymbol{z}$ based on the caption information only. Though impressive language-to-image generation results have been shown with LDM by pretraining on internet-scale data, it remains challenging to synthesize images where additional grounding input can be instructed, and is thus the focus of our paper.</p>
<h2>4. Open-set Grounded Image Generation</h2>
<h3>4.1. Grounding Instruction Input</h3>
<p>For grounded text-to-image generation, there are a variety of ways to ground the generation process via an additional condition. We denote the semantic information of the grounding entity as $e$, which can be described either through</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Illustration of grounding token construction process for the bounding box with text case.
text or an example image; and as $\boldsymbol{l}$ the grounding spatial configuration described with e.g., a bounding box, a set of keypoints, or an edge map, etc. Note that in certain cases, both semantic and spatial information can be represented with $\boldsymbol{l}$ alone (e.g., edge map), in which a single map can represent what objects may be present in the image and where. We define the instruction to a grounded text-to-image model as a composition of the caption and grounded entities:</p>
<p>$$
\begin{aligned}
\text { Instruction: } &amp; \boldsymbol{y}=(\boldsymbol{c}, \boldsymbol{e}), \quad \text { with } \
\text { Caption: } &amp; \boldsymbol{c}=\left[c_{1}, \cdots, c_{L}\right] \
\text { Grounding: } &amp; \boldsymbol{e}=\left[\left(e_{1}, \boldsymbol{l}<em N="N">{1}\right), \cdots,\left(e</em>\right)\right]
\end{aligned}
$$}, \boldsymbol{l}_{N</p>
<p>where $L$ is the caption length, and $N$ is the number of entities to ground. In this work, we primarily study using bounding box as the grounding spatial configuration $\boldsymbol{l}$, because of its large availability and easy annotation for users. For the grounded entity $e$, we mainly focus on using text as its representation due to simplicity. We process both caption and grounding entities as input tokens to the diffusion model, as described in detail below.
Caption Tokens. The caption $\boldsymbol{c}$ is processed in the same way as in LDM. Specifically, we obtain the caption feature sequence (yellow tokens in Figure 2) using $\boldsymbol{h}^{e}=$ $\left[h_{1}^{e}, \cdots, h_{L}^{e}\right]=f_{\text {text }}(\boldsymbol{c})$, where $h_{\ell}^{e}$ is the contextualized text feature for the $\ell$-th word in the caption.
Grounding Tokens. For each grounded text entity denoted with a bounding box, we represent the location information as $\boldsymbol{l}=\left[\alpha_{\min }, \beta_{\min }, \alpha_{\max }, \beta_{\max }\right]$ with its top-left and bottomright coordinates. For the text entity $e$, we use the same pretrained text encoder to obtain its text feature $f_{\text {text }}(e)$ (light green token in Figure 2), and then fuse it with its bounding box information to produce a grounding token (dark green token in Figure 2 ):</p>
<p>$$
h^{e}=\operatorname{MLP}\left(f_{\text {text }}(e), \text { Fourier }(\boldsymbol{l})\right)
$$</p>
<p>where Fourier is the Fourier embedding [44], and $\operatorname{MLP}(\cdot, \cdot)$ is a multi-layer perceptron that first concatenates the two inputs across the feature dimension. The grounding token sequence is represented as $\boldsymbol{h}^{e}=\left[h_{1}^{e}, \cdots, h_{N}^{e}\right]$
From Closed-set to Open-set. Note that existing layout2img works only deal with a closed-set setting (e.g.,</p>
<p>COCO categories), as they typically learn a vector embedding $\boldsymbol{u}$ per entity, to replace $f_{\text {text }}(e)$ in (5). For a closed-set setting with $K$ concepts, a dictionary of with $K$ embeddings are learned, $\mathbf{U}=\left[\boldsymbol{u}<em K="K">{1}, \cdots, \boldsymbol{u}</em>$ in the evaluation stage, and thus the model can only ground the observed entities in the generated images, lacking the ability to generalize to ground new entities; (2) No word/phrase is ever utilized in the model condition, and the semantic structure [23] of the underlying language instruction is missing. In contrast, in our open-set design, since the noun entities are processed by the same text encoder that is used to encode the caption, we find that even when the localization information is limited to the concepts in the grounding training datasets, our model can still generalize to other concepts as we will show in our experiments.
Extensions to Other Grounding Conditions. Note that the proposed grounding instruction in Eq (4) is in a general form, though our description thus far has focused on the case of using text as entity $e$ and bounding box as $\boldsymbol{l}$ (the major setting of this paper). To demonstrate the flexibility of the Gligen framework, we also study additional representative cases which extend the use scenario of Eq (4).}\right]$. While this non-parametric representation works well in the closed-set setting, it has two drawbacks: (1) The conditioning is implemented as a dictionary look-up over $\mathbf{U</p>
<ul>
<li>Image Prompt. While language allows users to describe a rich set of entities in an open-vocabulary manner, sometimes more abstract and fine-grained concepts can be better characterized by example images. To this end, one may describe entity $e$ using an image, instead of language. We use an image encoder to obtain feature $f_{\text {image }}(e)$ which is used in place of $f_{\text {text }}(e)$ in Eq (5) when $e$ is an image.</li>
<li>Keypoints. As a simple parameterization method to specify the spatial configuration of an entity, bounding boxes ease the user-machine interaction interface by providing the height and width of the object layout only. One may consider richer spatial configurations such as keypoints for Gligen, by parameterizing $\boldsymbol{l}$ in Eq (4) with a set of keypoint coordinates. Similar to encoding boxes, the Fourier embedding [44] can be applied to each keypoint location $\boldsymbol{l}=[x, y]$.</li>
<li>Spatially-aligned conditions. To enable more finegrained controlability, spatially-aligned condition maps can be used, such as edge map, depth map, normal map, and semantic map. In these cases, the semantic information $e$ is already contained within each spatial coordinate $\boldsymbol{l}$ of the condition map. A network (e.g. conv layers) can be used to encode $\boldsymbol{l}$ into $h \times w$ grounding tokens. We also notice that additionally feeding $\boldsymbol{l}$ into the first conv layer of the UNet can accelerate training. Specifically, the input to the UNet is $\operatorname{CONCAT}\left(f_{l}(\boldsymbol{l}), \boldsymbol{z}<em l="l">{t}\right)$ where $f</em>$ into the same}$ is a simple downsampling network to reduce $\boldsymbol{l</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. For a pretrained text2img model, the text features are fed into each cross-attention layer. A new gated self-attention layer is inserted to take in the new conditional information.</p>
<p>spatial resolution as <strong>z<sub>t</sub></strong>. In this case, the first conv layer of the UNet needs to be trainable.</p>
<p>Figure 1 shows generated examples for these other grounding conditions. Please refer to the supp for more details.</p>
<h3>4.2. Continual Learning for Grounded Generation</h3>
<p>Our goal is to endow new spatial grounding capabilities to existing large language-to-image generation models. Large diffusion models have been pre-trained on web-scale image-text to gain the required knowledge for synthesizing realistic images based on diverse and complex language instructions. Due to the high pre-training cost and excellent performance, it is important to retain such knowledge in the model weights while expanding the new capability. Hence, we consider to lock the original model weights, and gradually adapt the model by tuning new modules.</p>
<h4>Gated Self-Attention.</h4>
<p>We denote <strong>v</strong> = [v<sub>1</sub>, . . . , v<sub>M</sub>] as the visual feature tokens of an image. The original Transformer block of LDM consists of two attention layers: The self-attention over the visual tokens, followed by cross-attention from caption tokens. By considering the residual connection, the two layers can be written:</p>
<p>$$
\begin{aligned}
\boldsymbol{v} &amp;= \boldsymbol{v} + \text{SelfAttn}(\boldsymbol{v}) \
\boldsymbol{v} &amp;= \boldsymbol{v} + \text{CrossAttn}(\boldsymbol{v}, \boldsymbol{h}^{c})
\end{aligned} \tag{6}
$$</p>
<p>We freeze these two attention layers and add a new gated self-attention layer to enable the spatial grounding ability; see Figure 3. Specifically, the attention is performed over the concatenation of visual and grounding tokens [<strong>v</strong>, <strong>h<sup>c</sup></strong>]:</p>
<p>$$
\boldsymbol{v} = \boldsymbol{v} + \beta \cdot \text{tanh}(\gamma) \cdot \text{TS}(\text{SelfAttn}([\boldsymbol{v}, \boldsymbol{h}^{c}]))
$$</p>
<p>where TS(·) is a token selection operation that considers visual tokens only, and γ is a learnable scalar which is initialized as 0. β is set as 1 during the entire training process and is only varied for scheduled sampling during inference (introduced below) for improved quality and controllability. Note that (8) is injected in between (6) and (7). Intuitively, the gated self-attention in (8) allows visual features to leverage conditional information, and the resulting grounded features are treated as a residual, whose gate is initially set to 0 (due to γ being initialized as 0). This also enables more stable training. Note that a similar idea is used in Flamingo [1]; however, it uses gated cross-attention, which leads to worse performance in our ablation study.</p>
<h4>Learning Procedure.</h4>
<p>We adapt the pre-trained model such that grounding information can be injected while all the original components remain intact. By denoting all the new parameters as <strong>θ</strong>, including all gated self-attention layers in Eq (8) and MLP in Eq (5), we use the original denoising objective as in (1) for model continual learning, based on the grounding instruction input <strong>y</strong>:</p>
<p>$$
\min_{\theta} \mathcal{L}<em _boldsymbol_z="\boldsymbol{z">{\text{Grounding}} = \mathbb{E}</em>)|_2^2 \right]. \qquad (9)
$$}, \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), t} \left[ |\epsilon - f_{{\boldsymbol{\theta}, \boldsymbol{\theta}'}}(\boldsymbol{z}_t, t, \boldsymbol{y</p>
<p>Why should the model try to use the new grounding information? Intuitively, predicting the noise that was added</p>
<p>to a training image in the reverse diffusion process would be easier if the model could leverage the external knowledge (e.g., each object's location). Thus, in this way, the model learns to use the additional information while retaining the pre-trained concept knowledge.</p>
<p>Scheduled Sampling in Inference. The standard inference scheme of GLIGEN is to set $\beta=1$ in (8), and the entire diffusion process is influenced by the grounding tokens. This constant $\beta$ sampling scheme provides overall good performance in terms of both generation and grounding, but sometimes generates lower quality images compared with the original text2img models (e.g., as Stable Diffusion is finetuned on high aesthetic scored images). To strike a better trade-off between generation and grounding for GLIGEN, we propose a scheduled sampling scheme. As we freeze the original model weights and add new layers to inject new grounding information in training, there is flexibility during inference to schedule the diffusion process to either use both the grounding and language tokens or use only the language tokens of the original model at anytime, by setting different $\beta$ values in (8). Specifically, we consider a two-stage inference procedure, divided by $\tau \in[0,1]$. For a diffusion process with $T$ steps, one can set $\beta$ to 1 at the first $\tau * T$ steps, and set $\beta$ to 0 for the remaining $(1-\tau) * T$ steps:</p>
<p>$$
\beta=\left{\begin{array}{ll}
1, &amp; t \leq \tau * T \
0, &amp; t&gt;\tau * T
\end{array} \begin{array}{ll}
\text { # Grounded inference stage } \
\text { # Standard inference stage }
\end{array}\right.
$$</p>
<p>The major benefit of scheduled sampling is improved visual quality as the rough concept location and outline are decided in the early stages, followed by fine-grained details in later stages. It also allows us to extend the model trained in one domain (human keypoint) to other domains (monkey, cartoon characters) as shown in Figure 1.</p>
<h2>5. Experiments</h2>
<p>We evaluate our model's boxes grounded text2img generation in both the closed-set and open-set settings, and show extensions to other grounding modalities. We conduct our main quantitative experiments by building upon a pretrained LDM on LAION [57], unless stated otherwise.</p>
<h3>5.1. Closed-set Grounded Text2Img Generation</h3>
<p>We first evaluate the generation quality and grounding accuracy of our model in a closed-set setting. For this, we train and evaluate on the COCO2014 [41] dataset, which is a standard benchmark used in the text2img literature [51, 56, $65,70,82]$, and evaluate how the different types of grounding instructions impact our model's performance.</p>
<p>Grounding instructions. We use the following grounding instructions to train our model: 1) COCO2014D: Detection Data. There are no caption annotations so we use a</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Generation: FID ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Grounding: YOLO ( $\uparrow$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Fine-tuned</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$\mathrm{AP} / \mathrm{AP}<em 75="75">{50} / \mathrm{AP}</em>$</td>
</tr>
<tr>
<td style="text-align: left;">CogView [11]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">27.10</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">KNN-Diffusion [2]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16.66</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DALL-E 2 [51]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10.39</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Imagen [56]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.27</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Re-Imagen [7]</td>
<td style="text-align: center;">5.25</td>
<td style="text-align: center;">6.88</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Parti [74]</td>
<td style="text-align: center;">3.20</td>
<td style="text-align: center;">7.23</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LAFITE [82]</td>
<td style="text-align: center;">8.12</td>
<td style="text-align: center;">26.94</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LAFITE2 [80]</td>
<td style="text-align: center;">4.28</td>
<td style="text-align: center;">8.42</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Make-a-Scene [13]</td>
<td style="text-align: center;">7.55</td>
<td style="text-align: center;">11.84</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">NÜWA [69]</td>
<td style="text-align: center;">12.90</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Frido [12]</td>
<td style="text-align: center;">11.24</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">XMC-GAN [77]</td>
<td style="text-align: center;">9.33</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">AttnGAN [70]</td>
<td style="text-align: center;">35.49</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DF-GAN [65]</td>
<td style="text-align: center;">21.42</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Obj-GAN [35]</td>
<td style="text-align: center;">20.75</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LDM [53]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.63</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LDM*</td>
<td style="text-align: center;">5.91</td>
<td style="text-align: center;">11.73</td>
<td style="text-align: center;">$0.6 / 2.0 / 0.3$</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN (COCO2014CD)</td>
<td style="text-align: center;">5.82</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$21.7 / 39.0 / 21.7$</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN (COCO2014D)</td>
<td style="text-align: center;">5.61</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{2 4 . 0 / 4 2 . 2 / 2 4 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN (COCO2014G)</td>
<td style="text-align: center;">6.38</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$11.2 / 21.2 / 10.7$</td>
</tr>
</tbody>
</table>
<p>Table 1. Evaluation of image quality and correspondence to layout on COCO2014 val-set. All numbers are taken from corresponding papers, LDM* is our COCO fine-tuned LDM baseline. Here GLIGEN is built upon LDM.
null caption input [21]. Detection annotations are used as noun-entities. 2) COCO2014CD: Detection + Caption Data. Both caption and detection annotations are used. Note that the noun entities may not always exist in the caption. 3) COCO2014G: Grounding Data. Given the caption annotations, we use GLIP [34], which detects the caption's noun entities in the image, to get pseudo box labels. Please refer to supp for more details about these three types of data.</p>
<p>Baselines. Baseline models are listed in Table 1. Among them, we also finetune an LDM [53] pretrained on LAION 400M [57] on COCO2014 with its caption annotations, which we denote as LDM*.</p>
<p>The text2img baselines, as they cannot be conditioned on box inputs, are evaluated on COCO2014C: Caption Data.</p>
<p>Evaluation metrics. We use the captions and/or box annotations from 30K randomly sampled images to generate 30K images for evaluation. We use FID [20] to evaluate image quality. To evaluate grounding accuracy (i.e. correspondence between the input bounding box and generated entity), we use the YOLO score [40]. Specifically, we use a pretrained YOLO-v4 [5] to detect bounding boxes on the generated images and compare them with the ground truth boxes using average precision (AP). Since prior text2img methods do not support taking box annotations as input, it is not fair to compare with them on this metric. Thus, we only report numbers for the fine-tuned LDM as a reference.</p>
<p>Results. Table 1 shows the results. First, we see that the image synthesis quality of our approach, as measured by FID, is better than most of the state-of-the-art baselines due to rich visual knowledge learned in the pretraining stage. Next, we find that all three grounding instructions lead to comparable FID to that of the LDM* baseline, which is finetuned on</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Our model can generalize to open-world concepts even when only trained using localization annotation from COCO.</p>
<p>COCO2014 with caption annotations. Our model trained using detection annotation instructions (COCO2014D) has the overall best performance. However, when we evaluate this model on COCO2014CD instructions, we find that it has worse performance (FID: 8.2) – its ability to understand real captions may be limited as it is only trained with the null caption. For the model trained with GLIP grounding instructions (COCO2014G), we actually evaluate it using the COCO2014CD instructions since we need to compute the YOLO score which requires ground-truth detection annotations. Its slightly worse FID may be attributed to its learning from GLIP pseudo-labels. The same reason can explain its low YOLO score (<em>i.e</em>., the model did not see any ground-truth detection annotations during training).</p>
<p>Overall, this experiment shows that: 1) Our model can successfully take in boxes as an additional condition while maintaining image generation quality. 2) All grounding instruction types are useful, which suggests that combining their data together can lead to complementary benefits.</p>
<p>Comparison to Layout2Img generation methods. Thus far, we have seen that our model correctly learns to use the grounding condition. But how accurate is it compared to methods that are specifically designed for layout2img generation? To answer this, we train our model on COCO2017D, which only has detection annotations. We use the 2017 splits (instead of 2014 as before), as it is the standard benchmark in the layout2img literature. In this experiment, we use the exact same annotation as all layout2img baselines.</p>
<p>Table 2 shows that we achieve the state-of-the-art performance for both image quality and grounding accuracy. We believe the core reason is because previous methods train their model from scratch, whereas we build upon a large-scale pretrained generative model with rich visual semantics. Qualitative comparisons are in the supp. We also scale up our training data (discussed later) and pretrain a model on this dataset. Figure 5 left shows this model's zero-shot and finetuned results.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>FID (↓)</th>
<th>YOLO score (AP/AP50/AP75) (↑)</th>
</tr>
</thead>
<tbody>
<tr>
<td>LostGAN-V2 [62]</td>
<td>42.55</td>
<td>9.1 / 15.3 / 9.8</td>
</tr>
<tr>
<td>OCGAN [64]</td>
<td>41.65</td>
<td>-</td>
</tr>
<tr>
<td>HCSS [25]</td>
<td>33.68</td>
<td>-</td>
</tr>
<tr>
<td>LAMA [40]</td>
<td>31.12</td>
<td>13.40 / 19.70 / 14.90</td>
</tr>
<tr>
<td>TwFA [71]</td>
<td>22.15</td>
<td>- / 28.20 / 20.12</td>
</tr>
<tr>
<td>GLIGEN-LDM</td>
<td>21.04</td>
<td>22.4 / 36.5 / 24.1</td>
</tr>
</tbody>
</table>
<p>Table 2. Image quality and correspondence to layout are compared with baselines on COCO2017 val-set.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Training data</th>
<th>AP</th>
<th>AP1</th>
<th>AP2</th>
<th>AP3</th>
</tr>
</thead>
<tbody>
<tr>
<td>LAMA [40]</td>
<td>LVIS</td>
<td>2.0</td>
<td>0.9</td>
<td>1.3</td>
<td>3.2</td>
</tr>
<tr>
<td>GLIGEN-LDM</td>
<td>COCO2014CD</td>
<td>6.4</td>
<td>5.8</td>
<td>5.8</td>
<td>7.4</td>
</tr>
<tr>
<td>GLIGEN-LDM</td>
<td>COCO2014D</td>
<td>4.4</td>
<td>2.3</td>
<td>3.3</td>
<td>6.5</td>
</tr>
<tr>
<td>GLIGEN-LDM</td>
<td>COCO2014G</td>
<td>6.0</td>
<td>4.4</td>
<td>6.1</td>
<td>6.6</td>
</tr>
<tr>
<td>GLIGEN-LDM</td>
<td>GoldG,O365</td>
<td>10.6</td>
<td>5.8</td>
<td>9.6</td>
<td>13.8</td>
</tr>
<tr>
<td>GLIGEN-LDM</td>
<td>GoldG,O365,SBU,CC3M</td>
<td>11.1</td>
<td>9.0</td>
<td>9.8</td>
<td>13.4</td>
</tr>
<tr>
<td>GLIGEN-Stable</td>
<td>GoldG,O365,SBU,CC3M</td>
<td>10.8</td>
<td>8.8</td>
<td>9.9</td>
<td>12.6</td>
</tr>
<tr>
<td>Upper-bound</td>
<td></td>
<td>25.2</td>
<td>19.0</td>
<td>22.2</td>
<td>31.2</td>
</tr>
</tbody>
</table>
<p>Table 3. GLIP-score on LVIS validation set. Upper-bound is provided by running GLIP on real images scaled to 256 × 256.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Performance comparison measured by image generation and grounding quality on COCO2017 (left) and LVIS (right) datasets. GLIGEN is built upon LDM, and continually pre-trained on the joint data of GoldG, O365, SBU, and CC3M. GLIGEN (Reference) is pre-trained on COCO/LVIS only. The circle size indicates the model size.</p>
<h3>5.2. Open-set Grounded Text2Img Generation</h3>
<p><strong>COCO-training model.</strong> We first take GLIGEN trained only with the grounding annotations of COCO (COCO2014CD), and evaluate whether it can generate grounded entities beyond the COCO categories. Figure 4 shows qualitative results, where GLIGEN can ground new concepts such as "blue jay", "croissant" or ground object attributes such as "brown wooden table", beyond the training categories. We hypothesize this is because the gated self-attention of GLIGEN learns to re-position the visual features corresponding to the grounding entities in the caption for the ensuing cross-attention layer, and gains generalization ability due to the shared text spaces in these two layers.</p>
<p>We also quantitatively evaluate our model's zero-shot generation performance on LVIS [15], which contains 1203 long-tail object categories. We use GLIP to predict bounding boxes from the generated images and calculate AP, thus we name it as <em>GLIP score</em>. We compare to a state-of-the-art model designed for the layout2img task: LAMA [40]. We</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Grounded text2image generation. The baseline lacks grounding ability and can also miss objects e.g. "umbrella" in a sentence with multiple objects due to CLIP text space, and it also struggles to generate spatially counterfactual concepts.</p>
<p>train LAMA using the official code on the LVIS training set (in a fully-supervised setting), whereas we directly evaluate our model in a zero-shot task transfer manner, by running inference on the LVIS val set without seeing any LVIS labels. Table 3 (first 4 rows) shows the results. Surprisingly, even though our model is only trained on COCO annotations, it outperforms the supervised baseline by a large margin. This is because the baseline, which is trained from scratch, struggles to learn from limited annotations (many of the rare classes in LVIS have fewer than five training samples). In contrast, our model can take advantage of the pretrained model's vast concept knowledge.</p>
<p>Scaling up the training data. We next study our model's open-set capability with much larger training data. Specifically, we follow GLIP [34] and train on Object365 [58] and GoldG [34], which combines two grounding datasets: Flickr [49] and VG [31]. We also use CC3M [59] and SBU [46] with grounding pseudo-labels generated by GLIP.</p>
<p>Table 3 shows the data scaling results. As we scale up the training data, our model's zero-shot performance increases, especially for rare concepts. We also try to finetune the model pretrained on our largest dataset on LVIS and demonstrate its performance on Figure 5 right. To demonstrate the generality of our method, we also train our model based on the Stable Diffusion model checkpoint using the largest data. We show some qualitative examples in Figure 6 using this model. Our model gains the grounding ability compared to vanilla Stable Diffusion. We notice that Stable Diffusion model may overlook certain objects ("umbrella" in the second example) due to its use of the CLIP text encoder which tends to focus on global scene properties, and may ignore object-level details [3]. It also struggles to generate spatially counterfactual concepts. By explicitly injecting entity information through grounding tokens, our model can improve the grounding ability in two ways: the referred objects are more likely to appear in the generated images, and the objects reside in the specified spatial location.</p>
<h3>5.3. Beyond Text Modality Grounding</h3>
<p>Image grounded generation. One can also use a reference image to represent a grounded entity as discussed previously. Fig. 1 (b) shows qualitative results, which demonstrate that the visual feature can complement details that are hard to describe by language.</p>
<p>Text and image grounded generation. Besides using either text or image to represent a grounded entity, one can also keep both representations in one model for more creative generation. Fig. 1 (c) shows text grounded generation with style / tone transfer. For the style reference image, we find that grounding it to an image corner or its edge is sufficient. Since the model needs to generate a harmonious style for the entire image, we hypothesize the self-attention layers may broadcast this information to all pixels, thus leading to consistent style for the entire image.</p>
<p>Keypoints grounded generation. We also demonstrate GLIGEN using keypoints for articulate objects control as shown in the Fig. 1 (d). Note that this model is only trained with human keypoint annotations; but it can generalize to other humanoid object due to the scheduled sampling technique we proposed. We also quantitatively study this grounding condition in the supp.</p>
<p>Spatially-aligned condition map grounded generation. Fig. 1 (e-h) demonstrate results for depth map, edge map, normal map, and semantic map grounded generation. These types of conditions allow users to have more fine-grained generation control. See supp for more qualitative results.</p>
<h3>5.4. Scheduled Sampling</h3>
<p>As stated in Eq. (8) and Eq. (10), we can schedule inference time sampling by setting $\beta$ to 1 (use extra grounding information) or 0 (reduce to the original pretrained diffusion model). This can make our model exploit different knowledge at different stages.</p>
<p>Fig. 7 qualitatively shows the benefits of our scheduled sampling by setting $\tau$ to be 0.2 . The images in the same row share the same noise and conditional input. The first row shows that scheduled sampling can be used to improve image quality, as the original Stable Diffusion model is trained with high quality images. The second row shows a generation example by our model trained with COCO human keypoint annotations. Since this model is purely trained with human keypoints, the final result is biased towards generating a human even if a different object (i.e., robot) is specified in the caption. However, by using scheduled sampling, we can extend this model to generate other objects with a humanlike shape.</p>
<h2>6. Conclusion</h2>
<p>We proposed GLIGEN for expanding pretrained text2img diffusion models with grounding ability, and demonstrated open-world generalization using bounding boxes as the grounding condition. Our method is simple and effective, and can be easily extended to other conditions such as keypoints, reference images, spatially-aligned conditions (e.g., edge map, depth map, etc). The versatility of GLIGEN makes it a promising direction for advancing the field of text-to-
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Scheduled Samping. It can improve visual or extend a model trained in one domain (e.g., human) to the others.
image synthesis and expanding the capabilities of pretrained models in various applications.</p>
<p>Acknowledgement. This work was supported in part by NSF CAREER IIS2150012, NASA 80NSSC21K0295, and Institute of Information \&amp; communications Technology Planning \&amp; Evaluation(IITP) grants funded by the Korea government(MSIT) (No. 2022- 0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS-2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training), and Adobe Data Science Research Award.</p>
<h2>References</h2>
<p>[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. ArXiv, abs/2204.14198, 2022. 2, 5, 14
[2] Oron Ashual, Shelly Sheynin, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and Yaniv Taigman. Knndiffusion: Image generation via large-scale retrieval. arXiv preprint arXiv:2204.02849, 2022. 6
[3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. ArXiv, abs/2211.01324, 2022. 3, 8</p>
<p>[4] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. 2
[5] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. ArXiv, abs/2004.10934, 2020. 6
[6] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-toimage generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. 2
[7] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-augmented text-to-image generator. arXiv preprint arXiv:2209.14491, 2022. 6
[8] Yunjey Choi, Min-Je Choi, Mun Su Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8789-8797, 2018. 3
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. 3
[10] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. ArXiv, abs/2105.05233, 2021. 2
[11] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers, 2021. 6
[12] Wanshu Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, and Yu-Chiang Frank Wang. Frido: Feature pyramid diffusion for complex scene image synthesis. ArXiv, abs/2208.13753, 2022. 6
[13] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scenebased text-to-image generation with human priors. ArXiv, abs/2203.13131, 2022. 2, 3, 6
[14] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. 2, 3
[15] Agrim Gupta, Piotr Dollár, and Ross B. Girshick. Lvis: A dataset for large vocabulary instance segmentation. CVPR, pages 5351-5359, 2019. 2, 7, 16
[16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1600016009, 2022. 2
[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 2
[18] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. Mask r-cnn. 2017 IEEE International Conference on Computer Vision (ICCV), pages 2980-2988, 2017. 15
[19] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CVPR, pages 770778, 2016. 3
[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017. 6
[21] Jonathan Ho. Classifier-free diffusion guidance. ArXiv, abs/2207.12598, 2022. 6, 14
[22] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735-1780, 1997. 3
[23] Ray S Jackendoff. Semantic structures, volume 18. MIT press, 1992. 4
[24] Manuel Jahn, Robin Rombach, and Björn Ommer. Highresolution complex scene synthesis with transformers. ArXiv, abs/2105.06458, 2021. 3
[25] Manuel Jahn, Robin Rombach, and Björn Ommer. Highresolution complex scene synthesis with transformers. ArXiv, abs/2105.06458, 2021. 7, 16
[26] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1219-1228, 2018. 2
[27] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. CVPR, pages 4396-4405, 2019. 2
[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. CVPR, pages 4396-4405, 2019. 3
[29] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8107-8116, 2020. 2
[30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015. 14
[31] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123:32-73, 2016. 8
[32] Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, and Jianfeng Gao. ELEVATER: A benchmark and toolkit for evaluating language-augmented visual models. In NeurIPS Track on Datasets and Benchmarks, 2022. 2
[33] Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation learning with momentum distillation. arXiv preprint arXiv:2107.07651, 2021. 2</p>
<p>[34] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 10955-10965. IEEE, 2022. 2, 6, 8
[35] Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang, Xiaodong He, Siwei Lyu, and Jianfeng Gao. Object-driven text-to-image synthesis via adversarial training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12174-12182, 2019. 6
[36] Yuheng Li, Yijun Li, Jingwan Lu, Eli Shechtman, Yong Jae Lee, and Krishna Kumar Singh. Collaging class-specific gans for semantic image synthesis. $I C C V$, pages 14398-14407, 2021. 3
[37] Yuheng Li, Yijun Li, Jingwan Lu, Eli Shechtman, Yong Jae Lee, and Krishna Kumar Singh. Contrastive learning for diverse disentangled foreground generation. ArXiv, abs/2211.02707, 2022. 2
[38] Yuheng Li, Krishna Kumar Singh, Utkarsh Ojha, and Yong Jae Lee. Mixnmatch: Multifactor disentanglement and encoding for conditional image generation. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8036-8045, 2020. 3
[39] Z. Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and Lingyun Sun. Image synthesis from layout with localityaware mask adaption. $I C C V$, pages 13799-13808, 2021. 3
[40] Z. Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and Lingyun Sun. Image synthesis from layout with localityaware mask adaption. ICCV, pages 13799-13808, 2021. 6, 7, 16
[41] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 2, 6, 14, 17
[42] Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, and Chunyuan Li. Learning customized visual models with retrieval-augmented knowledge. CVPR, 2023. 2
[43] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 13
[44] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 4, 13
[45] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In ICML, 2022. 2, 15
[46] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned photographs. In NIPS, 2011. 8
[47] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. CVPR, pages 2332-2341, 2019. 2, 3
[48] Deepak Pathak, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context encoders: Feature learning by inpainting. CVPR, pages 2536-2544, 2016. 2
[49] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes, Juan C. Caicedo, J. Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. International Journal of Computer Vision, 123:74-93, 2015. 8
[50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 3
[51] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv, abs/2204.06125, 2022. 2, 6
[52] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8821-8831. PMLR, 18-24 Jul 2021. 2
[53] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. CVPR, pages 10674-10685, 2022. 2, 3, 6, 13, 14, 15
[54] O. Ronneberger, P.Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention (MICCAI), volume 9351 of LNCS, pages 234-241. Springer, 2015. (available on arXiv:1505.04597 [cs.CV]). 3, 13
[55] Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. ACM SIGGRAPH 2022 Conference Proceedings, 2022. 2, 3
[56] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. ArXiv, abs/2205.11487, 2022. 2, 6
[57] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION400M: open dataset of clip-filtered 400 million image-text pairs. CoRR, abs/2111.02114, 2021. 6
[58] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. ICCV, pages 8429-8438, 2019. 8
[59] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. 8</p>
<p>[60] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for semantic face editing. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9240-9249, 2020. 2
[61] Wei Sun and Tianfu Wu. Image synthesis from reconfigurable layout and style. ICCV, pages 10530-10539, 2019. 3
[62] Wei Sun and Tianfu Wu. Learning layout and style reconfigurable gans for controllable image synthesis. TPAMI, 44:50705087, 2022. 3, 7, 16
[63] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. Devon Hjelm, and Shikhar Sharma. Object-centric image generation from layouts. ArXiv, abs/2003.07449, 2021. 3
[64] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. Devon Hjelm, and Shikhar Sharma. Object-centric image generation from layouts. ArXiv, abs/2003.07449, 2021. 7, 16
[65] Ming Tao, Hao Tang, Songsong Wu, N. Sebe, Fei Wu, and Xiaoyuan Jing. Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis. ArXiv, abs/2008.05865, 2020. 3, 6
[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukazz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. 3
[67] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762, 2017. 2, 3
[68] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8798-8807, 2018. 15, 16
[69] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Nüwa: Visual synthesis pretraining for neural visual world creation. In European Conference on Computer Vision, 2022. 2, 6
[70] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Finegrained text to image generation with attentional generative adversarial networks. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1316-1324, 2018. 3,6
[71] Zuopeng Yang, Daqing Liu, Chaoyue Wang, J. Yang, and Dacheng Tao. Modeling image composition for complex scene generation. CVPR, pages 7754-7763, 2022. 2, 7, 15, 16
[72] Zuopeng Yang, Daqing Liu, Chaoyue Wang, J. Yang, and Dacheng Tao. Modeling image composition for complex scene generation. CVPR, pages 7754-7763, 2022. 3
[73] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Reco: Region-controlled text-toimage generation. ArXiv, abs/2211.15518, 2022. 3
[74] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei</p>
<p>Yang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. ArXiv, abs/2206.10789, 2022. 2, 6
[75] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021. 2
[76] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and ShihFu Chang. Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14393-14402, 2021. 2
[77] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive learning for text-toimage generation, 2021. 6
[78] Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image generation from layout. CVPR, pages 8576-8585, 2019. 3
[79] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. RegionCLIP: Region-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16793-16803, 2022. 2
[80] Yufan Zhou, Chunyuan Li, Changyou Chen, Jianfeng Gao, and Jinhui Xu. Lafite2: Few-shot text-to-image generation. arXiv preprint arXiv:2210.14124, 2022. 3, 6
[81] Yufan Zhou, Bingchen Liu, Yizhe Zhu, Xiao Yang, Changyou Chen, and Jinhui Xu. Shifted diffusion for text-to-image generation. arXiv preprint arXiv:2211.15388, 2022. 2
[82] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. CVPR, 2022. 2, 6
[83] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Computer Vision (ICCV), 2017 IEEE International Conference on, 2017. 3
[84] Xueyan Zou<em>, Zi-Yi Dou</em>, Jianwei Yang*, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, and Jianfeng Gao, Generalized decoding for pixel, image and language. arXiv, 2022. 2</p>
<p>Appendix</p>
<p>In this supplemental material, we provide more implementation and training details, and then present more results and discussions.</p>
<h2>A. Implementation and training details</h2>
<p>We use the Stable Diffusion model [53] as the example to illustrate our implementation details.
Box Grounding Tokens with Text. Each grounded text is first fed into the text encoder to get the text embedding (e.g., 768 dimension of the CLIP text embedding in Stable Diffusion). Since the Stable Diffusion uses features of 77 text tokens outputted from the transformer backbone, thus we choose "EOS" token feature at this layer as our grounded text embedding. This is because in the CLIP training, this "EOS" token feature is chosen and applied a linear transform (one FC layer) to compare with visual feature, thus this token feature should contain whole information about the input text description. We also tried to directly use CLIP text embedding ( after linear projection), however, we notice slow convergence empirically probably due to unaligned space between the grounded text embedding and the caption embeddings. Following NeRF [44], we encode bounding box coordinates with the Fourier embedding with output dimension 64. As stated in the Eq 5 in the main paper, we first concatenate these two features and feed them into a multi-layer perceptron. The MLP consists of three hidden layers with hidden dimension 512, the output grounding token dimension is set to be the same as the text embedding dimension (e.g., 768 in the Stable Diffusion case). We set the maximum number of grounding tokens to be 30 in the bounding box case.</p>
<p>Box Grounding Tokens with Image. We use the similar way to get the grounding token for an image. We use the CLIP image encoder (ViT-L-14 is used for the Stable Diffusion) to get an image embedding. We denote the CLIP training objective as maximizing $\left(\mathbf{P}<em t="t">{t} \boldsymbol{h}</em>}\right)^{\top}\left(\mathbf{P<em i="i">{i} \boldsymbol{h}</em>}\right)$ (we omit normalization), where $\boldsymbol{h<em i="i">{t}$ is "EOS" token embedding from the text encoder, $\boldsymbol{h}</em>}$ is "CLS" token embedding from the image encoder, and $\mathbf{P<em i="i">{t}$ and $\mathbf{P}</em>}$ are linear transformation for text and image embedding, respectively. Since $\boldsymbol{h<em t="t">{t}$ is the text feature space used for grounded text features, to ease our training, we choose to project image features into the text feature space via $\mathbf{P}</em>}^{\top} \mathbf{P<em i="i">{i} \boldsymbol{h}</em>$ we empirically found. We also set the maximum number of grounding tokens to be 30 . Thus, 60 tokens in total if one keep both image and text as representations for a grounded entity.}$, and normalized it to 28.7 , which is average norm of $\boldsymbol{h}_{t</p>
<p>Keypoint Grounding Tokens. The grounding token for keypoint annotations is processed in the same way, except that we also learn $N$ person token embedding vectors
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Additional grounding input is fed into the Unet input for spatially aligned conditions.
$\left{\boldsymbol{p}<em N="N">{1}, \ldots, \boldsymbol{p}</em>$ is a learnable vector; the dimension of each person token is set the same as keypoint embedding dimension. The grounding token is calculated by:}\right}$ to semantically link keypoints belonging to the same person. This is to deal with the situation in which there are multiple people in the same image that we want to generate, so that the model knows which keypoint corresponds to which person. Each keypoint semantic embedding $\boldsymbol{k}_{e</p>
<p>$$
\boldsymbol{h}^{e}=\operatorname{MLP}\left(\boldsymbol{k}<em j="j">{e}+\boldsymbol{p}</em>)\right)
$$}, \text { Fourier }(\boldsymbol{l</p>
<p>where $\boldsymbol{l}$ is the $x, y$ location of each keypoint and $\boldsymbol{p}_{j}$ is the person token for the $j^{\prime}$ th person. In practice, we set $N$ as 10 , which is the maximum number of persons allowed to be generated in each image. Thus, we have 170 tokens in the COCO dataset (i.e., $10 * 17 ; 17$ keypoint annotations for each person).</p>
<p>Tokens for Spatially Aligned Condition. This type of condition includes edge map, depth map, semantic map, and normal map, etc; they can be represented as $C \times H \times$ $W$ tensor. We resize spatial size into $256 \times 256$ and use the convnext-tiny [43] as the backbone to output a feature with spatial size as $8 \times 8$, which then is flattened into 64 grounding tokens. We notice that it can help training faster if we also provide the grounding condition $l$ into the Unet input. As shown in the Figure 8, in this case, the input is $\operatorname{Concat}\left(f_{l}(\boldsymbol{l}), \boldsymbol{z}<em l="l">{t}\right)$ where $f</em>$, which is the noisy latent code at the time step $t(64 \times 64$ for the Stable Diffusion). In this case, the first conv layer of Unet needs to be trainable.}$ is a simple downsampling network to reduce $\boldsymbol{l}$ into the same spatial dimension as $\boldsymbol{z}_{t</p>
<p>Gated Self-Attention Layers. Our inserted self-attention layer is the same as the original diffusion model selfattention layer at each Transformer block, except that we add one linear projection layer which converts the grounding token into the same dimension as the visual token. For example, in the first layer of the down branch of the UNet [54], the projection layer converts grounding token of dimension 768 into 320 (which is the image feature dimension at this</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Three different types of grounding data for box.
layer), and visual tokens are concatenated with the grounding tokens as the input to the gated attention layer.
Training Details. For all COCO related experiments (Sec 5.1 in the main paper), we train LDM with batch size 64 using 16 V 100 GPUs for 100k iterations. In the scaling up training data experiment (in Sec 5.2 of the main paper), we train for 400 k iterations for LDM, but 500 K iterations with batch size of 32 for the Stable diffusion modeL For all training, we use learning rate of $5 \mathrm{e}-5$ with Adam [30], and use warm-up for the first 10k iterations. We randomly drop caption and grounding tokens with $10 \%$ probability for classifier-free guidance [21].</p>
<p>Data Details. In the main paper Sec 5.1, we study three different types of data for box grounding. The training data requires both text $\boldsymbol{c}$ and grounding entity $\boldsymbol{e}$ as the full condition. In practice, we can relax the data requirement by considering a more flexible input, i.e. the three types of data shown in Figure 9. (i) Grounding data. Each image is associated with a caption describing the whole image; noun entities are extracted from the caption, and are labeled with bounding boxes. Since the noun entities are taken directly from the natural language caption, they can cover a much richer vocabulary which will be beneficial for open-world vocabulary grounded generation. (ii) Detection data. Nounentities are pre-defined closed-set categories (e.g., 80 object classes in COCO [41]). In this case, we choose to use a null caption token as introduced in classifier-free guidance [21] for the caption. The detection data is of larger quantity (millions) than the grounding data (thousands), and can therefore greatly increase overall training data. (iii) Detection and caption data. Noun entities are same as those in the detection data, and the image is described separately with a text caption. In this case, the noun entities may not exactly match those in the caption. For example, in Figure 9, the caption only gives a high-level description of the living room without mentioning the objects in the scene, whereas the detection annotation provides more fine-grained object-level details.</p>
<h2>B. Ablation Study</h2>
<p>Ablation on gated self-attention. As shown in the main paper Figure 3 and Eq 8, our approach uses gated self-
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Inpainting results. Existing text2img diffusion models may generate objects that do not tightly fit the masked box or miss an object if the same object already exists in the image.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$1 \%-3 \%$</th>
<th style="text-align: center;">$5 \%-10 \%$</th>
<th style="text-align: center;">$30 \%-50 \%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LDM [53]</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">14.6</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN-LDM</td>
<td style="text-align: center;">$\mathbf{2 9 . 7}$</td>
<td style="text-align: center;">$\mathbf{3 0 . 9}$</td>
<td style="text-align: center;">$\mathbf{2 5 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Upper-bound</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">45.0</td>
</tr>
</tbody>
</table>
<p>Table 4. Inpainting results (YOLO AP) for different size of objects.
attention to absorb the grounding instruction. We can also consider gated cross-attention [1], where the query is the visual feature, and the keys and values are produced using the grounding condition. We ablate this design on COCO2014CD data using LDM. Compare with the Table 1 the main paper, we can find that it leads to similar FID: 5.8, but worse YOLO AP: 16.6 (compared to 21.7 for selfattention in the Table). This shows the necessity of information sharing among the visual tokens, which exists in self-attention but not in cross-attention.</p>
<p>Ablation on null caption. We choose to use the null caption when we only have detection annotations (COCO2014D). An alternative scheme is to simply combine all noun entities into a sentence; e.g., if there are two cats and a dog in an image, then the pseudo caption can be: "cat, cat, dog". In this case, the FID becomes worse and increases to 7.40 from 5.61 (null caption, refer to main paper table 1). This is likely due to the pretrained text encoder never having encountered this type of unnatural caption during LDM training. A solution would be to finetune the text encoder or design a better prompt, but this is not the focus of our work.</p>
<p>Ablation on fourier embedding. In Eq 5, we replace the Fourier embedding with MLP embedding and conduct an experiment using COCO2014CD data format (Table 1) . In this case, the image quality (FID) is similar: Fourier/MLP: 5.82/5.80; however, the layout correspondence (YOLO AP) is much worse: Fourier/MLP: 21.7/3.2.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Layout2img comparison. Our model generates better quality images, especially when using stable diffusion. Baseline images are all copied from TwFA [71]</p>
<h2><strong>C. Grounded inpainting</strong></h2>
<h3><strong>C.1. Text Grounded Inpainting</strong></h3>
<p>Like other diffusion models, GLIGEN can also work for the inpainting task by replacing the known region with a sample from q(z<sub>t</sub>|z<sub>0</sub>) after each sampling step, where z<sub>0</sub> is the latent representation of an image [53]. One can ground text descriptions to missing regions, as shown in Figure 10. In this setting, however, one may wonder, can we simply use a vanilla text-to-image diffusion model such as Stable Diffusion or DALLE2 to fill the missing region by providing the object name as the caption? What are the benefits of having extra grounding inputs in such cases? To answer this, we conduct the following experiment on the COCO dataset: for each image, we randomly mask one object. We then let the model inpaint the missing region. We choose the missing object with three different size ratios with respect to the image: small (1%-3%), median (5%-10%), and large (30%-50%). 5000 images are used for each case.</p>
<p>Table 4 demonstrates that our inpainted objects more tightly occupy the missing region (box) compared to the baselines. Fig. 10 provides examples to visually compare the inpainting results (we use Stable Diffusion for better quality). The first row shows that baselines' generated objects do not</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12. <strong>Keypoint results.</strong> Our model generates higher quality images conditioned on keypoints, and it allows to use caption to specify details such as scene or gender.</p>
<p>follow the provided box. The second row shows that when the missing category is already present in the image, they may ignore the caption. This is understandable as baselines are trained to generate a <em>whole</em> image following the caption. Our method may be more favorable for editing applications, where a user might want to generate an object that fully fits the missing region or add an instance of a class that already exists in the image.</p>
<h3><strong>C.2. Image Grounded Inpainting</strong></h3>
<p>As we previously demonstrated, one can ground text to missing region for inpainting, one can also ground reference images to missing regions. Figure 13 shows inpainting results grounded on reference images. To remove boundary artifacts, we follow GLIDE [45], and modify the first conv layer by adding 5 extra channels (4 for z<sub>0</sub> and 1 for inpainting mask) and make them trainable with the new added layers.</p>
<h3><strong>D. Study for Keypoints Grounding</strong></h3>
<p>Although we have thus far demonstrated results with bounding boxes, our approach has flexibility in the grounding condition that it can use for generation. To demonstrate this, we next evaluate our model with another type of grounding condition: human keypoints. We use the COCO2017 dataset. We compare with pix2pixHD [68], a classic image-to-image translation model. Since pix2pixHD does not take captions as input, we train two variants of our model: one uses COCO captions, the other does not. In the latter case, null caption is used as input to the cross-attention layer for a fair comparison.</p>
<p>Fig. 12 shows the qualitative comparison. Clearly, our method generates much better image quality. For our model trained with captions, we can also specify other details such as the scene ("A person is skiing down a snowy hill") or person's gender ("A woman is holding a baby"). These two inputs complement each other and can enrich a user's controllability for image creation. We measure keypoint correspondence (similar to the YOLO score for boxes) by running a MaskRCNN [18] key-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Pre-training data</th>
<th style="text-align: left;">Traing data</th>
<th style="text-align: left;">FID</th>
<th style="text-align: right;">AP</th>
<th style="text-align: right;">$\mathrm{AP}_{c}$</th>
<th style="text-align: right;">$\mathrm{AP}_{c}$</th>
<th style="text-align: right;">$\mathrm{AP}_{f}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LAMA [40]</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">LVIS</td>
<td style="text-align: left;">151.96</td>
<td style="text-align: right;">2.0</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">1.3</td>
<td style="text-align: right;">3.2</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN-LDM</td>
<td style="text-align: left;">COCO2014CD</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">22.17</td>
<td style="text-align: right;">6.4</td>
<td style="text-align: right;">5.8</td>
<td style="text-align: right;">5.8</td>
<td style="text-align: right;">7.4</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN-LDM</td>
<td style="text-align: left;">COCO2014D</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">31.31</td>
<td style="text-align: right;">4.4</td>
<td style="text-align: right;">2.3</td>
<td style="text-align: right;">3.3</td>
<td style="text-align: right;">6.5</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN-LDM</td>
<td style="text-align: left;">COCO2014G</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">13.48</td>
<td style="text-align: right;">6.0</td>
<td style="text-align: right;">4.4</td>
<td style="text-align: right;">6.1</td>
<td style="text-align: right;">6.6</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN-LDM</td>
<td style="text-align: left;">GoldG,O365</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">8.45</td>
<td style="text-align: right;">10.6</td>
<td style="text-align: right;">5.8</td>
<td style="text-align: right;">9.6</td>
<td style="text-align: right;">13.8</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN-LDM</td>
<td style="text-align: left;">GoldG,O365,SBU,CC3M</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">10.28</td>
<td style="text-align: right;">11.1</td>
<td style="text-align: right;">9.0</td>
<td style="text-align: right;">9.8</td>
<td style="text-align: right;">13.4</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN-LDM</td>
<td style="text-align: left;">GoldG,O365,SBU,CC3M</td>
<td style="text-align: left;">LVIS</td>
<td style="text-align: left;">$\mathbf{6 . 2 5}$</td>
<td style="text-align: right;">$\mathbf{1 4 . 9}$</td>
<td style="text-align: right;">$\mathbf{1 0 . 1}$</td>
<td style="text-align: right;">$\mathbf{1 2 . 8}$</td>
<td style="text-align: right;">$\mathbf{1 9 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Upper-bound</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: right;">25.2</td>
<td style="text-align: right;">19.0</td>
<td style="text-align: right;">22.2</td>
<td style="text-align: right;">31.2</td>
</tr>
</tbody>
</table>
<p>Table 5. GLIP-score on LVIS validation set. Upper-bound is provided by running GLIP on real images scaled to $256 \times 256$.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13. Image grounded Inpainting. One can use reference images to ground holes they want to fill in.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">FID</th>
<th style="text-align: center;">AP</th>
<th style="text-align: center;">$\mathrm{AP}_{50}$</th>
<th style="text-align: center;">$\mathrm{AP}_{75}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">pta2pixHD [68]</td>
<td style="text-align: center;">142.4</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">13.0</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN (w/o caption)</td>
<td style="text-align: center;">31.02</td>
<td style="text-align: center;">$\mathbf{3 1 . 8}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{3 1 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN (w caption)</td>
<td style="text-align: center;">$\mathbf{2 7 . 3 4}$</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">$\mathbf{3 1 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Upper-bound</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">65.9</td>
</tr>
</tbody>
</table>
<p>Table 6. Conditioning with Human Keypoints evaluated on COCO2017 validation set. Upper-bound is calculated on real images scaled to $256 \times 256$.
point detector on the generated images. Both of our model variants produce similar results; see Table 6.</p>
<h2>E. Additional quantitative results</h2>
<p>In this section, we show more studies with our pretrained model using our largest data (GoldG, O365, CC3M, SBU). We had reported this model's zero-shot performance on LVIS [15] in the main paper Table 3. Here we finetune this model on LVIS, and report its GLIP-score in Table 5. Clearly, after finetuning, we show much more accurate generation results, surpassing the supervised baseline LAMA [40] by a large margin.</p>
<p>Similarly, we also test this model's zero-shot performance on the COCO2017 val-set, and its finetuning results are in Table 7. The results show the benefits of pretraining which can largely improve layout correspondence performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">YOLO score</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">FID</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">$\mathrm{AP}_{50}$</td>
<td style="text-align: center;">$\mathrm{AP}_{75}$</td>
</tr>
<tr>
<td style="text-align: left;">LostGAN-V2 [62]</td>
<td style="text-align: center;">42.55</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">9.8</td>
</tr>
<tr>
<td style="text-align: left;">OCGAN [64]</td>
<td style="text-align: center;">41.65</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">HCSS [25]</td>
<td style="text-align: center;">33.68</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LAMA [40]</td>
<td style="text-align: center;">31.12</td>
<td style="text-align: center;">13.40</td>
<td style="text-align: center;">19.70</td>
<td style="text-align: center;">14.90</td>
</tr>
<tr>
<td style="text-align: left;">TwFA [71]</td>
<td style="text-align: center;">22.15</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">28.20</td>
<td style="text-align: center;">20.12</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN-LDM</td>
<td style="text-align: center;">$\mathbf{2 1 . 0 4}$</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">24.1</td>
</tr>
<tr>
<td style="text-align: left;">After pretrain on GoldG,O365,SBU,CC3M</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN-LDM (zero-shot)</td>
<td style="text-align: center;">27.03</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">20.8</td>
</tr>
<tr>
<td style="text-align: left;">GLIGEN-LDM (finetuned)</td>
<td style="text-align: center;">21.58</td>
<td style="text-align: center;">$\mathbf{3 0 . 8}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 3}$</td>
<td style="text-align: center;">$\mathbf{3 5 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 7. Image quality and correspondence to layout are compared with baselines on COCO2017 val-set.</p>
<h2>F. Analysis on GLIGEN</h2>
<p>To have a better understanding of GLIGEN, we choose to study the box grounded model. Specifically, we try to visualize attention maps within gated self-attention layer and how does the learnable $\gamma$ in Eq 8 change during the training process.</p>
<p>In the Figure 14, we first show a generation result using two grounding tokens (teddy bear; bird). Next to it, we visualize the attention maps of our added layers between the</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14. Attention maps in one gated self-attention layer. The visualization results are from the sample at the first time step (i.e., Gaussian noise) in the middle layer of the Unet.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15. learnable $\gamma$ in the gated self attention layer in the middle of Unet changes during the training progress.
visual features and two grounding tokens for all 8 heads for one middle layer in the UNet. Even for the first sampling step (input is Gaussian noise), the visual feature starts to attend to the grounding tokens with correct spatial correspondence. This correspondence fades away in later sampling steps (which is aligned with our 'scheduled sampling technique' where we find rough layout is decided in the early sample steps).</p>
<p>We also find the attention maps for the beginning layers of the UNet to be less interpretable for all sample steps. We hypothesize that this is due to the lack of positional embedding for visual tokens, whereas position information can be leaked into later layers through zero padding via Conv layers. This might suggest that adding positional embedding for diffusion model pretraining (e.g., Stable Diffusion model training) could benefit downstream adaptation.</p>
<p>The Figure 15 shows how the learned $\gamma$ at this layer (Eq 8) changes during training. We empirically find the model starts to learn the correspondence around $60-70 \mathrm{k}$ iterations (around the peak in the plot). We hypothesize the model tries to focus on learning spatial correspondence at the beginning of training, then tries to finetune and dampen the new layers' contribution so that it can focus on image quality and details as the original weights are fixed.</p>
<h2>G. More qualitative results</h2>
<p>We show qualitative comparisons with layout2img baselines in Figure 11, which complements the results in Sec 5.1
of the main paper. The results show that our model has comparable image quality when built upon LDM, but has more visual appeal and details when built upon the Stable Diffusion model.</p>
<p>Lastly, we show more grounded text2img results with bounding boxes in Figure 16 and other modality grounding results in Figure 1718192021 22. Note that our keypoint model only uses keypoint annotations from COCO [41] which is not linked with person identity, but it can successfully utilize and combine the knowledge learned in the text2img training stage to control keypoints of a specific person. Out of curiosity, we also tested whether the keypoint grounding information learned on humans can be transferred to other non-humanoid categories such as cat or lamp for keypoint grounded generation, but we find that our model struggles in such cases even with scheduled sampling. Compared to bounding boxes, which only specify a coarse location and size of an object in the image and thus can be shared across all object categories, keypoints (i.e., object parts) are not always shareable across different categories. Thus, while keypoints enable more fine-grained control than boxes, they are less generalizable.</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Caption: "Space view of a planet and its sun"
Grounded text: planet, sun
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Caption: "cartoen sketch of a little girl with a smile and balloons, old style, detailed, elegant, intricate"
Grounded text: girl with a smile, balloon, balloon, balloon
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Caption: "Walter White in GTA v"
Grounded text: Walter White, car, bulldog
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Caption: "two pirate ships on the ocean in minecraft"
Grounded text: a pirate ship, a pirate ship
Figure 16. Bounding box grounded text2image generation. Our model can ground noun entities in the caption for controllable image generation</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 17. Results for keypoints grounded generation.
<img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure 18. Results for HED map grounded generation.</p>
<p><img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Figure 19. Results for canny map grounded generation.
<img alt="img-22.jpeg" src="img-22.jpeg" /></p>
<p>Figure 20. Results for depth map grounded generation.</p>
<p><img alt="img-23.jpeg" src="img-23.jpeg" /></p>
<p>Figure 21. Results for normal map grounded generation.
<img alt="img-24.jpeg" src="img-24.jpeg" /></p>
<p>Figure 22. Results for semantic map grounded generation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>$\S$ Part of the work performed at Microsoft; $\boldsymbol{\square}$ Co-senior authors&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
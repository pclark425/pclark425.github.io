<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3487 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3487</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3487</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-269302556</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.14928v2.pdf" target="_blank">Graph Machine Learning in the Era of Large Language Models (LLMs)</a></p>
                <p><strong>Paper Abstract:</strong> Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out-of-distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3487.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3487.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow: Augmenting large-language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemistry agent that integrates large language models with a collection of specialized tools to assist tasks across drug discovery, materials design, and organic synthesis; presented as a tool-augmented LLM pipeline for diverse chemistry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemcrow: Augmenting large-language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemCrow (tool-augmented LLM agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey describes ChemCrow as an agent that couples an LLM with ~18 specialized chemistry tools (no architecture/parameter counts provided in the survey); the LLM is used as an orchestrator to call external tools for chemistry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Tool-augmented generation: LLM issues calls to specialized cheminformatics/analysis tools to perform synthesis planning, design suggestions, and other chemistry tasks rather than direct SMILES-only generation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery, materials design, and organic synthesis (general chemistry applications).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this survey (the survey only summarizes the paper's scope and capabilities); specific metrics would be in the original ChemCrow paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey reports ChemCrow as a system integrating LLMs and tools to support diverse chemistry tasks; no quantitative outcomes or benchmarks are reported in the survey itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Survey highlights general LLM limitations (hallucinations, factuality, explainability) that apply to chemistry agents and the need for tool integration to reduce hallucination; no chemistry-specific failure modes or numeric limitations are reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Machine Learning in the Era of Large Language Models (LLMs)', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3487.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3487.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructMol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage framework aligning language and molecular graph modalities to build a molecular assistant for drug discovery, using a frozen LLM and graph encoder plus a learned projector and subsequent instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructMol (LLM + graph encoder with projector and instruction tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey describes InstructMol as keeping LLM and graph encoder parameters fixed while training a projector to align molecule-graph representations to the language space, followed by instruction tuning of the LLM for drug discovery tasks; specific model sizes/architectures are not detailed in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Representation alignment and instruction tuning: align graph (molecule) embeddings to LLM text space via a learned projector, then use instruction-tuned LLM to perform design/analysis tasks (not described as directly sampling novel SMILES in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery (molecular design and assistant capabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this survey; the paper is reported as focusing on alignment and instruction tuning for downstream chemistry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey states the framework supports drug discovery tasks by aligning modalities and instruction tuning LLMs, but provides no quantitative results or benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Survey notes general LLM limitations (hallucination, factuality, explainability) and the need for careful modality alignment; no chemistry-specific limitations are detailed in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Machine Learning in the Era of Large Language Models (LLMs)', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3487.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3487.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemDFM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemDFM: Dialogue foundation model for chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dialogue foundation model trained on chemistry literature and general data to support a range of chemistry tasks including molecular recognition and molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemdfm: Dialogue foundation model for chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemDFM (dialogue foundation model for chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the survey as trained on extensive chemistry literature plus general corpora to produce a dialogue-capable foundation model for chemistry; the survey does not give architecture, parameter counts, or pretraining specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Instruction/dialogue-based generation: the model is a dialogue-capable LLM trained/tuned on chemistry literature to respond, assist in recognition, and propose designs; the survey does not detail exact molecule generation procedures (e.g., SMILES sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry broadly, including molecular recognition and molecular design (drug discovery/materials).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey reports ChemDFM 'exhibits proficiency' across various chemistry tasks (molecular recognition, design), but presents no quantitative metrics or success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Survey reiterates general LLM concerns (hallucinations, factuality) that are relevant for chemistry dialogue models; specific ChemDFM failure cases are not reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Machine Learning in the Era of Large Language Models (LLMs)', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3487.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3487.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolReGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolReGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that converts molecular representations (e.g., SMILES) into textual prompts for LLMs to extract detailed chemical information used downstream (survey mentions it in the context of molecular tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolReGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey-only mention: characterized as converting molecular graphs/SMILES into textual descriptions and prompting LLMs to provide detailed information on functional groups, shapes, and chemical properties; no architecture/size or training details provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>SMILES-to-text prompting: serialize SMILES or molecular graphs into textual prompts and ask LLM to generate interpretable descriptions/attributes (used as augmented features for downstream models).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular property prediction and molecular analysis (materials/drug discovery contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey indicates MolReGPT is used to produce textual interpretations of SMILES to support downstream property prediction pipelines; no quantitative outcomes are reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>No MolReGPT-specific limitations are reported in the survey; general concerns about LLM hallucination and factual accuracy apply.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Machine Learning in the Era of Large Language Models (LLMs)', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3487.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3487.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-MolBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-MolBERTa: GPT molecular features language model for molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses GPT-like LLMs to generate molecular feature descriptions from SMILES which are then used to train downstream (smaller) models for property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-molberta: Gpt molecular features language model for molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-MolBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey describes GPT-MolBERTa as an approach that converts SMILES into textual descriptions via an LLM and then trains a (smaller) language model for molecular property prediction; exact model sizes and pretraining details are not given in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>SMILES-to-text generation: LLMs are prompted with SMILES to generate human-readable interpretations/feature descriptions that serve as augmented attributes for downstream prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular property prediction (chemistry/drug discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey reports GPT-MolBERTa is used to generate interpretations of SMILES to assist property prediction pipelines; no quantitative results or comparisons are provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Survey does not report model-specific limitations; general LLM issues (hallucination, factuality, domain alignment) are noted as concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Machine Learning in the Era of Large Language Models (LLMs)', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3487.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3487.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReLM: Leveraging language models for enhanced chemical reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid pipeline where GNNs first propose high-probability candidate products and an LLM is used to make the final selection among those candidates for reaction prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Relm: Leveraging language models for enhanced chemical reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ReLM (GNN + LLM hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey describes ReLM as using GNNs to propose candidate reaction products and then using an LLM to select the final candidate; architectural or training-size details are not provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Candidate selection pipeline: GNN enumerates or ranks candidate products; LLM evaluates/selects among candidates (a combined model selection strategy rather than direct de novo molecule generation).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemical reaction prediction (reaction outcome/product selection).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey summarizes the two-stage approach qualitatively: GNN narrows product candidates and LLM refines the final selection; no numeric performance figures are provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Survey does not detail specific failure cases for ReLM; general LLM weaknesses (hallucination, factual inconsistency) and dependence on upstream GNN candidate quality are implicit concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Machine Learning in the Era of Large Language Models (LLMs)', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3487.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3487.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIT-Mol / Git-mol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-modal LLM that aligns molecule graphs, images, and text for molecular science tasks; used as a multi-modal foundation model in chemistry contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A molecular multimodal foundation model associating molecule graphs with natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GIT-Mol (multi-modal LLM + aligner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey references a multi-modal model (GIT-Mol / Git-mol) that aligns graph, image, and textual modalities to the target text modality using self- and cross-attention (inspired by BLIP2 Q-Former style alignment); the survey does not give parameter counts or training datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Multi-modal alignment and text-conditioned generation: fuse graph/image/text embeddings via a cross-modal 'GIT-Former' to enable LLM outputs grounded in molecular graphs and images; not described as direct de novo molecule generation in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular science tasks (interpretation, property prediction, potentially design assistance).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey reports GIT-Mol as a multi-modal alignment approach enabling LLMs to handle molecular graphs and images but provides no quantitative performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Survey notes general modality-alignment challenges (fusion/alignment complexity, input length, and factuality/hallucination risks) but provides no model-specific failure analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Machine Learning in the Era of Large Language Models (LLMs)', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3487.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3487.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DrugChat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrugChat: Towards enabling chatgpt-like capabilities on drug molecule graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A project aiming to provide ChatGPT-like interaction capabilities specialized for drug molecule graphs, leveraging LLMs for molecule-centric dialog and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Drugchat: Towards enabling chatgpt-like capabilities on drug molecule graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DrugChat (domain-specialized LLM interface)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey mentions DrugChat as an effort to enable chat-like LLM capabilities over drug molecule graphs (likely combining graph encoders with LLM interfaces); the survey does not include architectural or training specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Interactive/dialogue-based molecule reasoning and assistance; the survey does not claim direct generative design of novel molecules but focuses on chat-like molecule understanding and manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug molecule analysis and interactive assistance for drug discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey lists DrugChat among efforts to bring chat-like LLM functionality to molecule graphs; no quantitative results are reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Survey reiterates common LLM drawbacks (hallucination, factuality) and the complexity of aligning graph modalities with LLMs; no DrugChat-specific limitations are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Machine Learning in the Era of Large Language Models (LLMs)', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chemcrow: Augmenting large-language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery <em>(Rating: 2)</em></li>
                <li>Chemdfm: Dialogue foundation model for chemistry <em>(Rating: 2)</em></li>
                <li>Gpt-molberta: Gpt molecular features language model for molecular property prediction <em>(Rating: 2)</em></li>
                <li>Relm: Leveraging language models for enhanced chemical reaction prediction <em>(Rating: 2)</em></li>
                <li>A molecular multimodal foundation model associating molecule graphs with natural language <em>(Rating: 2)</em></li>
                <li>MolReGPT <em>(Rating: 1)</em></li>
                <li>Drugchat: Towards enabling chatgpt-like capabilities on drug molecule graphs <em>(Rating: 2)</em></li>
                <li>Can large language models empower molecular property prediction? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3487",
    "paper_id": "paper-269302556",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow: Augmenting large-language models with chemistry tools",
            "brief_description": "A chemistry agent that integrates large language models with a collection of specialized tools to assist tasks across drug discovery, materials design, and organic synthesis; presented as a tool-augmented LLM pipeline for diverse chemistry tasks.",
            "citation_title": "Chemcrow: Augmenting large-language models with chemistry tools",
            "mention_or_use": "mention",
            "model_name": "ChemCrow (tool-augmented LLM agent)",
            "model_description": "Survey describes ChemCrow as an agent that couples an LLM with ~18 specialized chemistry tools (no architecture/parameter counts provided in the survey); the LLM is used as an orchestrator to call external tools for chemistry tasks.",
            "generation_method": "Tool-augmented generation: LLM issues calls to specialized cheminformatics/analysis tools to perform synthesis planning, design suggestions, and other chemistry tasks rather than direct SMILES-only generation.",
            "application_domain": "Drug discovery, materials design, and organic synthesis (general chemistry applications).",
            "evaluation_metrics": "Not specified in this survey (the survey only summarizes the paper's scope and capabilities); specific metrics would be in the original ChemCrow paper.",
            "results_summary": "Survey reports ChemCrow as a system integrating LLMs and tools to support diverse chemistry tasks; no quantitative outcomes or benchmarks are reported in the survey itself.",
            "comparison_to_baselines": "Not specified in this survey.",
            "limitations_challenges": "Survey highlights general LLM limitations (hallucinations, factuality, explainability) that apply to chemistry agents and the need for tool integration to reduce hallucination; no chemistry-specific failure modes or numeric limitations are reported in the survey.",
            "uuid": "e3487.0",
            "source_info": {
                "paper_title": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "InstructMol",
            "name_full": "InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
            "brief_description": "A two-stage framework aligning language and molecular graph modalities to build a molecular assistant for drug discovery, using a frozen LLM and graph encoder plus a learned projector and subsequent instruction tuning.",
            "citation_title": "InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
            "mention_or_use": "mention",
            "model_name": "InstructMol (LLM + graph encoder with projector and instruction tuning)",
            "model_description": "Survey describes InstructMol as keeping LLM and graph encoder parameters fixed while training a projector to align molecule-graph representations to the language space, followed by instruction tuning of the LLM for drug discovery tasks; specific model sizes/architectures are not detailed in the survey.",
            "generation_method": "Representation alignment and instruction tuning: align graph (molecule) embeddings to LLM text space via a learned projector, then use instruction-tuned LLM to perform design/analysis tasks (not described as directly sampling novel SMILES in the survey).",
            "application_domain": "Drug discovery (molecular design and assistant capabilities).",
            "evaluation_metrics": "Not specified in this survey; the paper is reported as focusing on alignment and instruction tuning for downstream chemistry tasks.",
            "results_summary": "Survey states the framework supports drug discovery tasks by aligning modalities and instruction tuning LLMs, but provides no quantitative results or benchmarks.",
            "comparison_to_baselines": "Not specified in this survey.",
            "limitations_challenges": "Survey notes general LLM limitations (hallucination, factuality, explainability) and the need for careful modality alignment; no chemistry-specific limitations are detailed in the survey text.",
            "uuid": "e3487.1",
            "source_info": {
                "paper_title": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ChemDFM",
            "name_full": "ChemDFM: Dialogue foundation model for chemistry",
            "brief_description": "A dialogue foundation model trained on chemistry literature and general data to support a range of chemistry tasks including molecular recognition and molecular design.",
            "citation_title": "Chemdfm: Dialogue foundation model for chemistry",
            "mention_or_use": "mention",
            "model_name": "ChemDFM (dialogue foundation model for chemistry)",
            "model_description": "Described in the survey as trained on extensive chemistry literature plus general corpora to produce a dialogue-capable foundation model for chemistry; the survey does not give architecture, parameter counts, or pretraining specifics.",
            "generation_method": "Instruction/dialogue-based generation: the model is a dialogue-capable LLM trained/tuned on chemistry literature to respond, assist in recognition, and propose designs; the survey does not detail exact molecule generation procedures (e.g., SMILES sampling).",
            "application_domain": "Chemistry broadly, including molecular recognition and molecular design (drug discovery/materials).",
            "evaluation_metrics": "Not specified in this survey.",
            "results_summary": "Survey reports ChemDFM 'exhibits proficiency' across various chemistry tasks (molecular recognition, design), but presents no quantitative metrics or success rates.",
            "comparison_to_baselines": "Not specified in this survey.",
            "limitations_challenges": "Survey reiterates general LLM concerns (hallucinations, factuality) that are relevant for chemistry dialogue models; specific ChemDFM failure cases are not reported in the survey.",
            "uuid": "e3487.2",
            "source_info": {
                "paper_title": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MolReGPT",
            "name_full": "MolReGPT",
            "brief_description": "A method that converts molecular representations (e.g., SMILES) into textual prompts for LLMs to extract detailed chemical information used downstream (survey mentions it in the context of molecular tasks).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MolReGPT",
            "model_description": "Survey-only mention: characterized as converting molecular graphs/SMILES into textual descriptions and prompting LLMs to provide detailed information on functional groups, shapes, and chemical properties; no architecture/size or training details provided in the survey.",
            "generation_method": "SMILES-to-text prompting: serialize SMILES or molecular graphs into textual prompts and ask LLM to generate interpretable descriptions/attributes (used as augmented features for downstream models).",
            "application_domain": "Molecular property prediction and molecular analysis (materials/drug discovery contexts).",
            "evaluation_metrics": "Not specified in this survey.",
            "results_summary": "Survey indicates MolReGPT is used to produce textual interpretations of SMILES to support downstream property prediction pipelines; no quantitative outcomes are reported in the survey.",
            "comparison_to_baselines": "Not specified in this survey.",
            "limitations_challenges": "No MolReGPT-specific limitations are reported in the survey; general concerns about LLM hallucination and factual accuracy apply.",
            "uuid": "e3487.3",
            "source_info": {
                "paper_title": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-MolBERTa",
            "name_full": "GPT-MolBERTa: GPT molecular features language model for molecular property prediction",
            "brief_description": "A method that uses GPT-like LLMs to generate molecular feature descriptions from SMILES which are then used to train downstream (smaller) models for property prediction.",
            "citation_title": "Gpt-molberta: Gpt molecular features language model for molecular property prediction",
            "mention_or_use": "mention",
            "model_name": "GPT-MolBERTa",
            "model_description": "Survey describes GPT-MolBERTa as an approach that converts SMILES into textual descriptions via an LLM and then trains a (smaller) language model for molecular property prediction; exact model sizes and pretraining details are not given in the survey.",
            "generation_method": "SMILES-to-text generation: LLMs are prompted with SMILES to generate human-readable interpretations/feature descriptions that serve as augmented attributes for downstream prediction.",
            "application_domain": "Molecular property prediction (chemistry/drug discovery).",
            "evaluation_metrics": "Not specified in this survey.",
            "results_summary": "Survey reports GPT-MolBERTa is used to generate interpretations of SMILES to assist property prediction pipelines; no quantitative results or comparisons are provided in the survey.",
            "comparison_to_baselines": "Not specified in this survey.",
            "limitations_challenges": "Survey does not report model-specific limitations; general LLM issues (hallucination, factuality, domain alignment) are noted as concerns.",
            "uuid": "e3487.4",
            "source_info": {
                "paper_title": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ReLM",
            "name_full": "ReLM: Leveraging language models for enhanced chemical reaction prediction",
            "brief_description": "A hybrid pipeline where GNNs first propose high-probability candidate products and an LLM is used to make the final selection among those candidates for reaction prediction.",
            "citation_title": "Relm: Leveraging language models for enhanced chemical reaction prediction",
            "mention_or_use": "mention",
            "model_name": "ReLM (GNN + LLM hybrid)",
            "model_description": "Survey describes ReLM as using GNNs to propose candidate reaction products and then using an LLM to select the final candidate; architectural or training-size details are not provided in the survey.",
            "generation_method": "Candidate selection pipeline: GNN enumerates or ranks candidate products; LLM evaluates/selects among candidates (a combined model selection strategy rather than direct de novo molecule generation).",
            "application_domain": "Chemical reaction prediction (reaction outcome/product selection).",
            "evaluation_metrics": "Not specified in this survey.",
            "results_summary": "Survey summarizes the two-stage approach qualitatively: GNN narrows product candidates and LLM refines the final selection; no numeric performance figures are provided in the survey.",
            "comparison_to_baselines": "Not specified in this survey.",
            "limitations_challenges": "Survey does not detail specific failure cases for ReLM; general LLM weaknesses (hallucination, factual inconsistency) and dependence on upstream GNN candidate quality are implicit concerns.",
            "uuid": "e3487.5",
            "source_info": {
                "paper_title": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GIT-Mol / Git-mol",
            "name_full": "GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text",
            "brief_description": "A multi-modal LLM that aligns molecule graphs, images, and text for molecular science tasks; used as a multi-modal foundation model in chemistry contexts.",
            "citation_title": "A molecular multimodal foundation model associating molecule graphs with natural language",
            "mention_or_use": "mention",
            "model_name": "GIT-Mol (multi-modal LLM + aligner)",
            "model_description": "Survey references a multi-modal model (GIT-Mol / Git-mol) that aligns graph, image, and textual modalities to the target text modality using self- and cross-attention (inspired by BLIP2 Q-Former style alignment); the survey does not give parameter counts or training datasets.",
            "generation_method": "Multi-modal alignment and text-conditioned generation: fuse graph/image/text embeddings via a cross-modal 'GIT-Former' to enable LLM outputs grounded in molecular graphs and images; not described as direct de novo molecule generation in the survey.",
            "application_domain": "Molecular science tasks (interpretation, property prediction, potentially design assistance).",
            "evaluation_metrics": "Not specified in this survey.",
            "results_summary": "Survey reports GIT-Mol as a multi-modal alignment approach enabling LLMs to handle molecular graphs and images but provides no quantitative performance metrics.",
            "comparison_to_baselines": "Not specified in this survey.",
            "limitations_challenges": "Survey notes general modality-alignment challenges (fusion/alignment complexity, input length, and factuality/hallucination risks) but provides no model-specific failure analyses.",
            "uuid": "e3487.6",
            "source_info": {
                "paper_title": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DrugChat",
            "name_full": "DrugChat: Towards enabling chatgpt-like capabilities on drug molecule graphs",
            "brief_description": "A project aiming to provide ChatGPT-like interaction capabilities specialized for drug molecule graphs, leveraging LLMs for molecule-centric dialog and reasoning.",
            "citation_title": "Drugchat: Towards enabling chatgpt-like capabilities on drug molecule graphs",
            "mention_or_use": "mention",
            "model_name": "DrugChat (domain-specialized LLM interface)",
            "model_description": "Survey mentions DrugChat as an effort to enable chat-like LLM capabilities over drug molecule graphs (likely combining graph encoders with LLM interfaces); the survey does not include architectural or training specifics.",
            "generation_method": "Interactive/dialogue-based molecule reasoning and assistance; the survey does not claim direct generative design of novel molecules but focuses on chat-like molecule understanding and manipulation.",
            "application_domain": "Drug molecule analysis and interactive assistance for drug discovery workflows.",
            "evaluation_metrics": "Not specified in this survey.",
            "results_summary": "Survey lists DrugChat among efforts to bring chat-like LLM functionality to molecule graphs; no quantitative results are reported in the survey.",
            "comparison_to_baselines": "Not specified in this survey.",
            "limitations_challenges": "Survey reiterates common LLM drawbacks (hallucination, factuality) and the complexity of aligning graph modalities with LLMs; no DrugChat-specific limitations are provided.",
            "uuid": "e3487.7",
            "source_info": {
                "paper_title": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chemcrow: Augmenting large-language models with chemistry tools",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
            "rating": 2,
            "sanitized_title": "instructmol_multimodal_integration_for_building_a_versatile_and_reliable_molecular_assistant_in_drug_discovery"
        },
        {
            "paper_title": "Chemdfm: Dialogue foundation model for chemistry",
            "rating": 2,
            "sanitized_title": "chemdfm_dialogue_foundation_model_for_chemistry"
        },
        {
            "paper_title": "Gpt-molberta: Gpt molecular features language model for molecular property prediction",
            "rating": 2,
            "sanitized_title": "gptmolberta_gpt_molecular_features_language_model_for_molecular_property_prediction"
        },
        {
            "paper_title": "Relm: Leveraging language models for enhanced chemical reaction prediction",
            "rating": 2,
            "sanitized_title": "relm_leveraging_language_models_for_enhanced_chemical_reaction_prediction"
        },
        {
            "paper_title": "A molecular multimodal foundation model associating molecule graphs with natural language",
            "rating": 2,
            "sanitized_title": "a_molecular_multimodal_foundation_model_associating_molecule_graphs_with_natural_language"
        },
        {
            "paper_title": "MolReGPT",
            "rating": 1
        },
        {
            "paper_title": "Drugchat: Towards enabling chatgpt-like capabilities on drug molecule graphs",
            "rating": 2,
            "sanitized_title": "drugchat_towards_enabling_chatgptlike_capabilities_on_drug_molecule_graphs"
        },
        {
            "paper_title": "Can large language models empower molecular property prediction?",
            "rating": 1,
            "sanitized_title": "can_large_language_models_empower_molecular_property_prediction"
        }
    ],
    "cost": 0.0184155,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Graph Machine Learning in the Era of Large Language Models (LLMs)
4 Jun 2024</p>
<p>Wenqi Fan 
Shijie Wang 
Jiani Huang 
Zhikai Chen 
Yu Song 
Wenzhuo Tang 
Haitao Mao 
Hui Liu 
Xiaorui Liu 
Dawei Yin 
Qing Li 
Graph Machine Learning in the Era of Large Language Models (LLMs)
4 Jun 202401C6DA907BE782485C0C5DD598F1C3A6arXiv:2404.14928v2[cs.LG]Graph Machine LearningGraph Foundation ModelsGraph LearningLarge Language Models (LLMs)Pre-training and Fine-tuningPromptingRepresentation Learning
Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery.With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures.Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems.This remarkable success has also attracted interest in applying LLMs to the graph domain.Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability.Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability.Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners.Therefore, in this survey, we first review the recent developments in Graph ML.We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out-of-distribution (OOD) generalization.Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference.Furthermore, we investigate various applications and discuss the potential future directions in this promising field.</p>
<p>INTRODUCTION</p>
<p>G Raph data are widespread in many real-world appli- cations [1], [2], including social graphs, knowledge graphs, and recommender systems [3]- [5].Typically, graphs consist of nodes and edges, e.g., in a social graph, nodes represent users and edges represent relationships [6], [7].In addition to the topological structure, graphs tend to possess various features of nodes, such as textual description, which provide valuable context and semantic information about nodes.To effectively model the graph, Graph Machine Learning (Graph ML) has garnered significant interest.With the advent of deep learning (DL), Graph Neural Networks (GNNs) have become a critical technique in Graph ML due to their message-passing mechanism.This mechanism allows each node to obtain its representation by recursively receiving and aggregating messages from neighboring nodes [8], [9], thereby capturing the high-order relationships and dependencies within the graph structure.To mitigate the Figure 1: Illustration of the application of Large Language Models (LLMs) in graph machine learning.The integration of LLMs with Graph Neural Networks (GNNs) is utilized to model an extensive range of graph data across various downstream tasks.reliance on supervised data, many research focused on developing self-supervised Graph ML methods to advance GNNs to capture transferable graph patterns, enhancing their generalization capabilities across various tasks [10]- [13].Given the exponential growth of applications of graph data, researchers are actively working to develop more powerful Graph ML methods.</p>
<p>Recently, Large Language Models (LLMs) have started a new trend of AI and have shown remarkable capabilities in natural language processing (NLP) [14], [15].With the evolution of these models, LLMs are not only being applied   to language tasks but also showcasing great potentials in various applications such as CV [16], and Recommender System [17].The effectiveness of LLMs in complex tasks is attributed to their extensive scale in both architecture and dataset size.For example, GPT-3 with 175 billion parameters demonstrates exciting capabilities by generating human-like text, answering complex questions, and coding.Furthermore, LLMs are able to grasp extensive general knowledge and sophisticated reasoning due to their vast training datasets.Therefore, their abilities in linguistic semantics and knowledge reasoning enable them to learn semantic information.Additionally, LLMs exhibit emergence abilities, excelling in new tasks and domains with limited or no specific training.This attribute is expected to provide high generalisability across different downstream datasets and tasks even in few-shot or zero-shot situations.Therefore, leveraging the capabilities of LLMs in Graph Machine Learning (Graph ML) has gained increasing interest and is expected to enhance Graph ML towards Graph Foundation Models (GFMs) [18], [19].</p>
<p>GFMs are generally trained on extensive data and can be adapted for a wide range of downstream tasks [20].By exploiting the ability of LLMs, it is expected to enhance the ability of Graph ML to generalize a variety of tasks, thus facilitating GFMs.Currently, researchers have made several initial efforts to explore the potential of LLMs in advancing Graph ML towards GFMs. Figure 1 demonstrates an example of integrating LLMs and GNNs for various graph tasks.Firstly, some methods leverage LLMs to alleviate the reliance of vanilla Graph ML on labeled data, where they make inferences based on implicit and explicit graph structure information [21]- [23].For instance, InstructGLM [21] finetunes models like LlaMA [24] and T5 [25] by serializing graph data as tokens and encoding structural information about the graph to solve graph tasks.Secondly, to overcome the challenge of feature quality, some methods further employ LLMs to enhance the quality of graph features [26]- [28].For example, SimTeG [26] fine-tunes LLMs on textual graphs datasets to obtain textual attribute embeddings, which are then utilized to augment the GNN for various downstream tasks.Additionally, some studies explore using LLMs to address challenges such as heterogeneity [29] and OOD [27] of graphs.</p>
<p>On the other hand, although LLM achieves great success in various fields, it still faces several challenges, including hallucinations, actuality awareness, and lacking explainability [30]- [33].Graphs, especially knowledge graphs, capture extensive high-quality and reliable factual knowledge in a structured format [5].Therefore, incorporating graph structure into LLMs could improve the reasoning ability of LLMs and mitigate these limitations [34].To this end, efforts have been made to explore the potential of graphs in augmenting LLMs' explainability [35], [36] and mitigating hallucination [37], [38].Given the rapid evolution and significant potential of this field, a thorough review of recent advancements in graph applications and Graph ML in the era of LLMs is imperative.</p>
<p>Therefore, in this survey, we aim to provide a com-prehensive review of Graph Machine Learning in the era of LLMs.The outline of the survey is shown in Figure 2: Section 2 reviews work related to graph machine learning and foundation models.Section 3 introduces the deep learning methods on graphs, which focus on various GNN models and self-supervised methods.Subsequently, the survey delves into how LLMs can be used to enhance Graph ML in Section 4 and how graphs can be adopted for augmenting LLMs in Section 5. Finally, some applications and potential future directions for Graph ML in the era of LLMs are discussed in Section 6 and Section 7, respectively.Our main contributions can be summarized as follows:</p>
<p> We detail the evolution from early graph learning methods to the latest GFMs in the era of LLMs;  We provide a comprehensive analysis of current LLMs enhanced Graph ML methods, highlighting their advantages and limitations, and offering a systematic categorization;</p>
<p> We thoroughly investigate the potential of graph structures to address the limitations of LLMs;  We explore the applications and prospective future directions of Graph ML in the era of LLMs, and discuss both research and practical applications in various fields.Concurrent to our survey, Wei et al. [39] review the development of graph learning.Zhang et al. [40] provide a prospective review of large graph models.Jin et al. [41] and Li et al. [42] review different techniques for pretraining language models (in particular LLMs) on graphs and applications to different types of graphs, respectively.Liu et al. [43] review the Graph Foundation Models according to the pipelines.Mao et al. [19] focus on the fundamental principles and discuss the potential of GFMs.Different from these concurrent surveys, our survey provides a more comprehensive review with the following differences: (1) we present a more systematic review of the development of Graph Machine Learning and further exploration of LLMs for Graph ML towards GFMs; (2) we present a more comprehensive and fine-grained taxonomy of recent advancements of Graph ML in the era of LLMs; (3) we delve into the limitations of recent Graph ML, and provide insights into how to overcome these limitations from LLM's perspective; (4) we further explore how graphs can be used to augment LLMs; and (5) we thoroughly summarize a broad range of applications and present a more forward-looking discussion on the challenges and future directions.</p>
<p>RELATED WORK</p>
<p>In this section, we briefly review some related works in the fields of graph machine learning and foundation model techniques.</p>
<p>Graph Machine Learning</p>
<p>As one of the most active fields in artificial intelligence, graph learning has attracted considerable attention to its capability to model complex relationships and structures in data represented as graphs [44].Nowadays, it has been widely adopted in various applications, including social network analysis [45], protein detection [46], recommender systems [47], [48], etc.</p>
<p>The initial phases of graph learning typically use Random Walks, which is a foundational method for exploring graph structures.This technique involves a stochastic process of moving from one node to another within a graph, which is instrumental in understanding node connectivity and influence within networks.Building upon Random Walks, Graph Embedding methods aim to represent nodes (or edges) as low-dimensional vectors while preserving graph topology and node relationships.Representative methods such as LINE [49], DeepWalk [50], and Node2Vec [51] leverage Random Walks to learn node representations, capturing local structures and community information effectively.</p>
<p>Due to the exceptional representation learning and modeling capabilities, GNNs bolstered by deep learning have brought significant advances in graph learning.For example, GCNs [52] introduce convolutional operations to graph data, enabling effective aggregation of neighborhood information for each node, thus enhancing node representation learning.GraphSAGE [53] learns a function to aggregate information from a node's local neighborhood in an inductive setting, allowing efficient embedding generation for unseen nodes.GAT [54] further advances GNNs by integrating attention mechanisms, assigning varying weights to nodes in a neighborhood, thereby sharpening the model's ability to focus on significant nodes.Inspired by the success of transformers [55] in NLP and CV, several studies [56]- [60] adopt self-attention mechanisms to graph data, providing a more global perspective of graph structures and interactions.Recent works [61]- [65] further leverage transformer architectures to enhance graph data modeling.For example, GraphFormer [61] integrates GNN within each layer in the transformer, enabling simultaneous consideration of textual and graph information.</p>
<p>The advancements in LLMs have given rise to graph learning.Recent works [21], [22], [27], [66], [67] apply techniques from these advanced language models like LLaMA [24] or ChatGPT to graph data, resulting in models capable of understanding and handling graph structures in a manner similar to natural language processing.A typical approach, GraphGPT [23], tokenizes graph data for insertion into LLMs (i.e.Vicuna [68] and LLaMA [24]) thus providing a powerful generalization capability.GLEM [69] further integrates the graph models and LLMs, specifically DeBERTa [70], within a variational Expectation-Maximization (EM) framework.It alternates between updating LLM and GNN in the E-step and M-step, thereby scaling efficiently and improving effectiveness in downstream tasks.</p>
<p>Foundation Models (FMs)</p>
<p>Foundation Models (FMs) represent a significant breakthrough in the field of artificial intelligence, characterized by their ability to be extensively pre-trained on large-scale datasets and adapted to a variety of downstream tasks.These models are distinguished by their extensive pre-training on large-scale datasets and their adaptability to a wide range of downstream tasks.It is worth noting that FMs are not limited to a single field and can be found in natural language [14], [15], vision [71], [72], and graph domains [19], [43], serving as a promising research direction.</p>
<p>In the realm of vision, Visual Foundation Models (VFMs) have gained significant success, making substantial impacts on areas such as image recognition, object detection, and scene understanding.Specifically, VFMs benefit from pretraining on extensive and diverse image datasets, allowing them to learn intricate patterns and features.For instance, models such as DALL-E [73], and CLIP [71] leverage selfsupervised learning to understand and generate images based on textual descriptions, demonstrating remarkable cross-modal understanding capabilities.Recent Visual Chat-GPT [72] integrates ChatGPT with a series of Visual Foundation Models (VFMs), making it perform a variety of complex visual tasks.These VFMs allow models to learn from a broader range of visual data, thereby improving their generalizability and robustness.</p>
<p>In the sphere of Natural Language Processing (NLP), Large Language Models (LLMs) such as ChatGPT and LLaMA have also revolutionized the field [74].Characterized by their extensive scale, LLMs are trained on billions of parameters using extensive textual datasets, which enable them to excel in comprehending and generating natural language.The landscape of pre-trained language models is diverse, such as GPT (Generative Pre-trained Transformer) [14], BERT (Bidirectional Encoder Representations from Transformers) [15] and T5 (Text-To-Text Transfer Transformer) [25].These models can broadly fall into three categories: encoder-only, decoder-only, and encoder-decoder models.Encoder-only models, such as BERT, specialize in understanding and interpreting language.In contrast, decoder-only models like GPT excel in generating coherent and contextually relevant text.Encoder-decoder models, like T5, combine both abilities, efficiently performing various NLP tasks from translation to summarization.</p>
<p>As an encoder-only model, BERT introduces a paradigm in NLP with its innovative bi-directional attention mechanism, which analyzes text from both directions simultaneously, unlike its predecessors like transformer which processed text in a single direction (either left-to-right or right-to-left).This feature allows BERT to attain a comprehensive context understanding, significantly improving its language nuance comprehension.On the other hand, decoder-only models such as GPT, including variants like ChatGPT, utilize a unidirectional self-attention mechanism.This design makes them particularly effective in predicting subsequent words in a sequence, thus excelling in tasks like text completion, creative writing, language translation, and code generation [75].Additionally, as an encoder-decoder model, T5 uniquely transforms a variety of NLP tasks as text generation problems.For example, it reframes sentiment analysis from a classification task to a text generation task, where input like "Sentiment: Today is sunny" would prompt T5 to generate an output such as "Positive".This text-to-text approach underscores T5's versatility and adaptability across diverse language tasks.</p>
<p>The evolution of LLMs has seen the emergence of advanced models like GPT-3 [97], LaMDA [98], PaLM [99], and Vicuna [68].These models represent significant advances in NLP, distinguished by their enhanced capabilities in comprehending and generating complex, fine-grained language.Their training methods are usually more sophisticated, involving larger datasets and more powerful computational resources.This scaling up has led to unprecedented language understanding and generation capabilities, exhibiting emergent properties such as in-context learning (ICL), adaptability, and flexibility.Furthermore, recent advancements demonstrate the successful integration of LLMs with other models, like recommender system [17], reinforcement learning (RL) [100], GNNs [26], [101]- [103].This integration enables LLMs to tackle both traditional and novel challenges, proposing prospective avenues for applications.</p>
<p>LLMs have found applications in diverse sectors like chemistry [104], [105], education [106], [107], and finance [108], [109].In these fields, they contribute to various tasks from data analysis to personalized learning.Particularly, LLMs exhibit great potential in graph tasks such as graph classification and link prediction, demonstrating their versatility and broad applicability.Specifically, several studies like Simteg [26], GraD [102], Graph-Toolformer [101], Graph CoT [110], and Graphologue [103] have notably advanced graph learning.These models utilize LLMs for textual graph learning, graph-aware distillation, and graph reasoning, illustrating the potential of LLMs in enhancing the understanding of and interaction with complex graph structures.</p>
<p>Although FMs have revolutionized Vision and NLP domains, the development of Graph Foundation Models (GFMs) is still in the nascent stages.With the rapid evolution and significant potential of this field, it is imperative to continue exploring and developing advanced techniques that can further enhance Graph ML towards GFMs.</p>
<p>DEEP LEARNING ON GRAPHS</p>
<p>With the rapid development of deep neural networks (DNNs), GNN techniques modeling graph structure and node attributes for representation learning have been widely explored and have become one key technology in Graph ML.While vanilla GNNs demonstrate proficiency in various graph tasks, they still encounter several challenges such as scalability, generalization to unseen data, and limited capability in capturing complex graph structures.To overcome these limitations, many efforts have been made to improve GNN with the self-supervised paradigm.Therefore, to provide a comprehensive review of these methods, in this section, we first introduce the backbone architecture, including GNN-based models and graph transformer-based models.After that, we explore two important aspects of self-supervised graph ML models: graph pretext tasks and downstream adaptation.Note that a comprehensive summary of these methods is presented in Table 1.</p>
<p>Backbone Architecture</p>
<p>As one of the most active fields in the artificial intelligence (AI) community, various GNN methods have been proposed to solve various tasks.The powerful capability of these models is largely dependent on the development of their backbone architectures.Therefore, in this subsection, we focus on two broadly used architectures: neighborhood aggregation-based models and graph transformer-based models.</p>
<p>Neighborhood Aggregation-based Model</p>
<p>Neighborhood aggregation-based models are the most popular graph learning architectures that have been extensively studied and applied in various downstream  [11] GNN Contrastive Learning Fine-tuning Graph GCC [13] GNN Contrastive Learning URL&amp;Fine-tuning Node, Graph G-BERT [80] BERT Graph Generation Fine-tuning Recommendation AdapterGNN [81] GNN Multi-task Fine-tuning Graph GROVER [82] Graph Transformer Property Prediction Fine-tuning Graph Graph-Bert [60] Graph Transformer Graph Generation URL&amp;Fine-tuning Node G-Adapter [83] Graph Transformer Multi-task Fine-tuning Graph GraphGPT [84] Graph Transformer Graph Generation Fine-tuning Node, Edge, Graph MoMu [85] BERT, GNN Contrastive Learning Fine-tuning Graph TOUCHUP-G [86] BERT, ViT, GNN tasks.These models operate based on the message-passing mechanism [111], which updates a node's representation by aggregating the features of its neighboring nodes along with its own features.Formally, this process can be represented as:
m u = Aggregate(f v , v  N u ),(1)f  u = U pdate(m u , f u ),(2)
where, for each node u, a message m u is generated through the aggregation function from its neighboring nodes.Subsequently, the graph signal f is updated with the message.GCN is a typical method designed to leverage both the graph structure and the node attributes.This architecture updates node representations by aggregating neighboring features with the node's own.As the number of network layers increases, each captures an increasingly larger neighborhood.Owing to the efficiency and performance, GCN [52] has been widely applied by several methods such as CSSL [11] and PRODIGY [94].GraphSAGE [53] is another notable neighborhood aggregation-based model.Due to its inductive paradigms, GraphSAGE can easily generalize to unseen nodes or graphs, making it widely employed by many studies such as PinSage [112] for inductive learning.Additionally, several studies [78], [91], [94] incorporate Graph Attention Networks (GATs) [54] as the backbone architecture.GATs integrate attention mechanisms into GNNs, assigning variable weights to neighboring nodes, thereby focusing on the most relevant parts of the input graph for improved node representations.As another important model in the family of GNNs, Graph Isomorphism Network (GIN) [113] has also been widely used [10], [13], [87], [95], due to its powerful representation ability.Its unique architecture guarantees the expressiveness equivalent to the Weisfeiler-Lehman isomorphism test, making it widely chosen as the backbone model for a lot of structure-intensive tasks.</p>
<p>Although these models are widely adopted to solve graph tasks, they still suffer from some inherent limitations, such as over-smoothing and lack of generalization.In addition, the lower amount of parameters also limits the modeling capacity as the backbone model to serve multiple datasets and tasks.</p>
<p>Graph Transformer-based Model</p>
<p>While neighborhood aggregation-based GNN models have shown remarkable performance in processing graphstructured data, they suffer from some limitations.A significant challenge for these models is their difficulty in handling large graphs due to their reliance on local neighborhood information and their limited capacity in capturing long-range dependencies within the graph [64], [114], [115].To overcome these problems, inspired by the success of the transformer model in various NLP tasks, graph transformer-based models have been proposed [57], [62], [64].These models leverage the self-attention mechanism to adaptly capture both local and global graph structures, which allows the model to stack multiple layers without over-smoothing.Due to the lower inductive bias, graph transformer-based models can learn the structural patterns from data rather than solely relying on the graph structure.Additionally, transformers have demonstrated great scaling behavior in CV and NLP, suggesting that their performance can keep improving with more data and parameters.</p>
<p>Graph transformer-based models have been widely applied as a backbone architecture in various tasks [60], [82], [83], [96], [116].For example, Heterformer [117] introduces a graph-empowered Transformers architecture by adding neighbor tokens into each language Transformer layer.Edgeformers [118] propose to encode text and structure inside each Transformer layer jointly.Graph-Bert [60] employs a transformer to pre-train on the graph dataset with feature and edge reconstruction tasks and then finetunes for various downstream tasks.Similarly, GROVER [82] introduces a self-supervised graph transformer-based model designed specifically for large-scale molecular data.It pretrains on extensive molecular datasets and then fine-tunes for specific downstream tasks.GraphGPT [84] employs a (semi-)Eulerian path to transform the graph into a sequence of tokens, and then feeds the sequence into the transformer.Specifically, it constructs a dataset-specific vocabulary such that each node can correspond to a unique node ID.</p>
<p>Despite graph transformer-based models that can somehow address the limitations of traditional GNNs, they also face several challenges.One of the challenges is the quadratic complexity caused by self-attention, which becomes particularly problematic for large-scale graphs.In addition, there is a risk of losing some information about the original graph structure when serializing the graph.</p>
<p>Self-Supervised Learning on Graphs</p>
<p>To adapt GNNs to various graph tasks, many self-supervised learning methods have been proposed and extensively studied.These approaches enable GNNs to learn graph representations from the pre-training task and transfer them to various downstream tasks, such as node classification, graph classification, and link prediction.Therefore, in this subsection, we will introduce graph self-supervised learning methods from pretext tasks and downstream adaptation, respectively.</p>
<p>Graph Pretext Tasks</p>
<p>Graph Contrastive Learning aims to learn augmentation representations by contrasting similar and dissimilar graph data pairs, effectively identifying nuanced relationships and structural patterns.We can review graph contrastive learning from two perspectives: graph augmentations and the scale of contrast.</p>
<p>Generally, graph augmentations can be broadly categorized into two types: 1) feature perturbation and 2) topology perturbation.They assume that tiny changes in the feature or structural space do not change the semantics of the Node/Edge/(sub)graph. Feature perturbation involves perturbing the features of the nodes in the graph.For example, GRACE [77] randomly masks the node features to learn more robust representations.On the other hand, topology perturbation mainly involves modifying the structure of the graph.A typical example is CSSL [11] which employs strategies like edge perturbation or node dropping to adopt graph-graph level contrast, thereby enhancing the robustness of representations.</p>
<p>Regarding the scale of contrast, the approaches can be divided into node-level and graph-level.For example, GRACE [77] computes the similarities between node-level embeddings to learn discriminative node representations.GCC [13] also works at the node level but learns local structural patterns by sampling a node's neighbors to obtain subgraphs (positive pairs) and contrasting them with randomly selected non-contextual subgraphs (negative pairs).In contrast, DGI [76] contrasts node-level embeddings with graph-level embedding to capture global graph structures.GraphCL [10] takes a different approach by implementing graph-to-graph level contrast, thereby learning robust representations.The scale used for pre-training has a huge impact on the downstream performance.When adopting contrastive learning as the pre-training task, one key challenge is how to design the objective such that the embeddings learned can account for downstream tasks of different scales.Graph Generation methods aim to learn the distribution of graph data to enable graph generation or reconstruction.In contrast to models in CV that predict masked image patches, or in NLP that predict the next token in a sequence, graph data presents a unique challenge due to its interconnected nature.Consequently, graph generation methods typically work on the feature or structural space.Feature generation methods focus on masking the features of one or a subset of nodes and then training the model to recover the masked features.For instance, GraphMAE [78] utilizes a masked autoencoder framework to reconstruct masked graph portions based on their context, effectively capturing the underlying node semantics and their connection patterns.Alternatively, structure generation methods concentrate on training the model to recover the graph structure.The method GraphGPT [84] encodes the graph into sequences of tokens and then employs a transformer decoder to predict the next token of the sequence to recover the connectivity of the graph.In addition, Graph-Bert [60] is trained on both node attribute recovery and graph structure recovery tasks to ensure that the model captures local node attribute information while maintaining a global view of the graph structure.Graph Property Prediction methods gain guidance from the node-, edge-, and graph-level properties, which are inherently present in the graph data.These methods follow a training approach similar to supervised learning, as both utilize "sample-label" pairs for training.The key distinction lies in the origin of the labels: in supervised learning, labels are manually annotated by human experts which can be costly in real scenarios, whereas in property-based learning, the labels are automatically generated from the graph using some heuristics or algorithms.For example, GROVER [82] utilizes professional software to extract the information on graph motifs as labels for classification.Similarly, [119] leverage statistical properties of the graph for graph selfsupervised learning.</p>
<p>Downstream Adaptation</p>
<p>Unsupervised Representation Learning (URL) is a common method due to the scarcity of labeled data in the real world [76]- [79].In URL, the pre-trained graph encoder is frozen and only a task-specific layer is learned during downstream tuning.The learned representations are then directly fed into decoders.This pattern allows URLs to be efficiently applied to downstream tasks.For example, DGI [76] trains an encoder model to learn node representations within graph-structured.Node representations can then be used for downstream tasks.However, due to the gap between the pretext task and downstream tasks, URL can also lead to suboptimal performance.Fine-tuning is the default method to adapt a pre-trained model to a certain downstream task.As shown in Figure 3, it adds a randomly initialized task header (e.g., a classifier) on top of the pre-trained model, and during fine-tuning, both the backbone model and the header are jointly trained [10], [11], [60].Compared with URL, fine-tuning provides more flexibility as it allows changes in the backbone parameters, and one can choose the layers to be tuned while keeping others fixed.Additionally, recent studies [10], [81], [83] further explore advanced graph fine-tuning methods that go beyond naive fine-tuning.For instance, AdapterGNN [81] introduces two trainable adapters in parallel before and after the message passing.It freezes the GNN model during fine-tuning while only tuning the adapters, enabling parameter-efficient fine-tuning with minimal influence on the downstream performance.Prompt-tuning: "Pre-training &amp; fine-tuning" is prevalent in adapting pre-trained models to specific downstream tasks, but it overlooks the gap between pre-training and downstream tasks, potentially limiting generalization capabilities.Moreover, fine-tuning for different tasks also leads to significant time and computational costs.Inspired by recent advancements in NLP, several methods [87]- [93], [95], [96] have presented the potential of introducing prompts to adapt pre-trained models to specific tasks as illustrated in Figure 3. Specifically, Prompt-tuning first unifies the downstream task with the pre-trained task into the same paradigm, followed by the introduction of learnable prompts for tuning.For example, GPPT [88] first reframe node classification as link predictions.GraphPrompt [87] further extends graph classification into link prediction.On the other hand, Prog [91] unifies all the downstream tasks into subgraph classification.The inserting prompt including vectors [87], [88], [90], node [95] and sub-graph [91].By inserting these prompts, the pre-trained parameters can be utilized in a way that aligns more closely with the requirements of the downstream tasks.</p>
<p>LLMS FOR GRAPH MODELS</p>
<p>Despite great potential, Graph ML based on the GNNs has its inherent limitations.Firstly, vanilla GNN models commonly demand labeled data for supervision, and obtaining these annotations can be resource-intensive in terms of time and cost.Secondly, real-world graphs often contain abundant textual information, which is crucial for downstream tasks.However, GNNs typically rely on shallow text embeddings for semantic extraction, thereby limiting their capacity to capture intricate semantics and text features.Moreover, the diversity of graphs presents challenges for GNN models in terms of generalization across diverse domains and tasks.</p>
<p>Recently, LLMs have achieved remarkable success in handling natural language, with exciting features like (1) conducting zero/few-shot predictions and (2) providing a unified feature space.These capabilities present a potential solution to address the above challenges faced by Graph ML and GFMs.Therefore, this section aims to investigate the contributions that current LLMs can make to enhance Graph ML's progress towards GFMs, while also examining their current limitations, as Figure 4 shows.</p>
<p>Enhancing Feature Quality</p>
<p>Graphs encompass diverse attribute information, spanning text, images, audio, and other multi-modal modes.The semantics of these attributes play a crucial role in a range of downstream tasks.In comparison with earlier pre-trained models, LLMs stand out due to their substantial parameter volume and training on extensive datasets, endowing them with rich open-world knowledge.Consequently, researchers are exploring the potential of LLMs to improve feature quality and align feature space.This section delves into research endeavors aimed at leveraging LLMs to accomplish these goals.</p>
<p>Enhancing Feature Representation</p>
<p>Researchers utilize the powerful language understanding capabilities of LLMs to generate better representations for text attributes compared to traditional shallow text embeddings [27], [120], [121].For example, Patton [154] proposes to pre-train a language model on the target graph to obtain high-quality feature representation.METERN [155] introduces a soft prompt-based method to learn node multiplex embeddings for different edge types with one language model encoder.Chen et al. [27] utilize LLMs as text encoders and the GNN model as a predictor, validating the effectiveness of LLMs as an enhancer in node classification tasks.In LKPNR [120], an LK-Aug news encoder enhances the news recommender system by concatenating LLM embeddings with entity embeddings within the news text to obtain an enriched news representation.Several researchers explore fine-tuning LLMs to obtain text representations better suited for downstream graph tasks.SimTeG [26] treats node classification and link prediction tasks as text classification and text similarity tasks, fine-tuning PLMs using LoRA [156] on the TAG dataset.The fine-tuned PLMs are then used to generate embeddings for text attributes, followed by GNN training for downstream tasks.</p>
<p>Generating Augmented Information</p>
<p>Several studies investigate leveraging the generation capabilities and general knowledge of LLMs to generate augmented information from original textual attributes.TAPE [122] Heterophily and Generalization Instruction Who is the key influencer in the network?</p>
<p>The key influencer in the network is Annie.first leverages LLM to generate potential node labels and explanations, utilizing text attributes (such as title and abstract) as input.These labels and explanations generated by the LLM are regarded as augmented attributes.Subsequently, these augmented attributes are encoded by a fine-tuned language model (LM) and processed by a GNN model, which integrates the graph structure for making final predictions.</p>
<p>Solving Vanilla GNN Training Limitations
Enhancing Feature Quality Graph Attributes Social Network Answer Ignoring Structure
In contrast to TAPE, KEA [27] does not directly predict node labels with LLM.Instead, LLM extracts terms mentioned in textual attributes and provides detailed descriptions of these terms.</p>
<p>In the domain of molecular property prediction, both LLM4Mol [66] and GPT-MolBERTa [126] adopt a similar approach, where LLMs generate interpretations for input Simplified Molecular-Input Line-Entry System (SMILES) notations as augmented attributes.</p>
<p>In the realm of recommender systems, several methods leverage LLMs to enhance the textual attributes of both users and items.LLM-Rec [125] enables LLMs to produce more detailed item descriptions by explicitly stating the recommendation intent within the prompt.RLMRec [123] explores using LLM to enhance user preference.Specifically, the LLM receives user and item information as input, generates user preferences, potential types of users that the item may attract, and the reasoning process.LLMRec [124] employs a similar approach to enhance item and user attributes in recommender systems.For instance, based on historical behavior information, LLM outputs user profiles like age, gender, country, language, and preferred or disliked genres.For item attributes, taking movie information such as title as input, LLM generates outputs such as movie director, country, and language.</p>
<p>In addition to generating augmented text attributes, researchers also employ LLMs to enhance graph topological structures by generating or refining nodes and edges.In ENG [127], LLM is employed to generate new nodes and their corresponding text attributes for each node category.To integrate the generated nodes into the original graph, the authors train an edge predictor using relations in the original dataset as supervised signals.Sun et al. [128] leverage LLMs to refine graph structures.Specifically, they let LLMs remove unreliable edges by predicting the semantic similarity between node attributes.Additionally, they utilize pseudolabels generated by LLMs to aid the GNN in learning proper edge weights.</p>
<p>Aligning Feature Space</p>
<p>In real-world scenarios, the text attributes of graphs across different domains exhibit considerable diversity.Additionally, beyond text modal attributes, the graph may Table 2: A summary of LLM for Graph ML research.We present the GNN model, LLM model, predictor, domain, task, datasets, and project link.FT is Fine-tuning, refers to whether modifications are made to the parameters of the LLM model while PR is Prompting, involves inputting textual prompts to the LLM to obtain responses.In the context of task, "node" denotes node-level tasks such as node classification, "edge" signifies edge-level tasks like link prediction, "graph" represents graph-level tasks such as graph classification, and "structure" pertains to structure understanding tasks, such as node degree counting.</p>
<p>Role</p>
<p>Solving Vanilla GNN Training Limitations</p>
<p>The training of vanilla GNNs relies on labeled data.However, obtaining high-quality labeled data has long been associated with substantial time and costs.In contrast to GNNs, LLMs showcase robust zero/few-shot capabilities and possess expansive open-world knowledge.This unique characteristic empowers LLMs to directly leverage node information for prediction, without relying on extensive annotated data.Therefore, researchers have explored employing LLMs to generate annotations or predictions, alleviating dependence on human supervision signals in Graph ML.According to how structural information in graph data is processed, we categorize the methods into the following three categories:</p>
<p> Ignoring structural information: utilize node attributes exclusively for constructing textual prompts, disregarding neighboring labels and relations.</p>
<p> Implicit Structural information: describe neighbor information and graph topology structure in natural language;  Explicit Structural information: employ GNN models to encode graph structure.</p>
<p>Ignoring Structural Information</p>
<p>The fundamental distinction between graphs and text lies in the structural information inherent in graphs.Given that the LLM processes text as its input, an intuitive approach involves leveraging the textual attributes of the target node, disregarding the structural information within the graph, and making predictions directly.For instance, the work of [130] explores the effectiveness of LLMs in solving graph tasks without using structure information.In the citation network, they employ the article's title and abstract to construct a prompt and instruct the LLM to predict the article's category.Since this kind of paradigm does not incorporate the structural information of the graph, the actual task performed by the LLM is text classification rather than a graph-related task.</p>
<p>Implicit Structural Information</p>
<p>Researchers implicitly leverage structural information to solve graph tasks by describing graph structure in natural language.For example, Hu et al. [130] propose two kinds of methods for utilizing structural information.The first method involves directly inputting the data of all neighboring nodes into LLM, while the second method employs a retrievalbased prompt to guide the LLM to focus solely on relevant neighbor data.Similarly, Huang et al. [137] employ an LLM to assign scores to neighboring nodes and subsequently choose high-scoring nodes as structural information.NLGraph [131] introduces a Build-a-Graph prompting strategy to improve the LLM's understanding of graph structure.This strategy entails appending "Let's construct a graph with the nodes and edges first."after providing the graph data description.The work of [21] introduces InstructGLM, which utilizes natural language for graph description and fine-tunes Flan-T5 through instruction tuning.They generate a set of 31 prompts by combining four configuration parameters: task type, inclusion of node features, maximum hop order, and utilization of node connections.Notably, maximum hop order and node connections implicitly convey graph structure information to the LLM.GraphEdit [141] leverages LLMs to understand graph structure and refine it by removing noisy edges and uncovering implicit node connections.Specifically, it employs an edge predictor to identify the top k candidate edges for each node, and these candidate edges, along with the original edges of the graph, are then fed into the LLM.The LLM is prompted to determine which edges should be integrated into the final graph structure.</p>
<p>In addition to employing natural language expression, several researchers leverage structured languages for graph description.GPT4Graph [22], for instance, utilizes Graph Modelling Language [157] and Graph Markup Language [158] to represent graph structure in XML format.GraphText [29] constructs a graph syntax tree for each graph, containing node attributes and relations information.By traversing this tree, structural graph-text sequences can be generated.The advantage of GraphText lies in the ability to integrate the typical inductive bias of GNNs through the construction of various graph syntax trees.</p>
<p>Explicit Structural Information</p>
<p>While implicitly describing structure in natural language has achieved preliminary success, these methods still face certain limitations.Firstly, due to the constraint of input length, LLMs can only get local structural information, and lengthy contexts might diminish their reasoning [159] and instruction-following abilities [27].Secondly, for different tasks and datasets, substantial effort is often required for prompt engineering.A prompt that performs well on one dataset may not generalize effectively to others, resulting in a lack of robustness.Consequently, researchers investigate representing graph structure explicitly, typically comprising three essential modules: encoding module, fusion module, and LLM module.More specifically, the encoding module aims to process the graph-structured and textual information, generating graph embeddings and text embeddings, respectively.Afterward, the fusion module takes these two embeddings as input, producing a modality fusion embedding.At last, the modality fusion embedding, which contains both graph information and instruction information, is fed into the LLM to obtain the final answer.Given the research focus is on how LLMs explicitly utilize graph structure information, we will delve into the encoding and fusion modules of various studies in detail, without primarily focusing on the LLM model itself.Encoding Module.The encoding module is responsible for both graph and text encoding, and we will provide separate summaries for each.</p>
<p> Graph Encoding.Pre-trained GNN models are commonly used for graph encoding.For instance, GIT-Mol [147] employs the GIN model from the pre-trained MoMu model [85] to encode molecular graphs.KoPA [145] utilizes the pre-trained RotateE model to obtain embeddings for entities and relations in the knowledge graph.In addition, GIMLET [146] presents a unified graph-text model without the need for additional graph encoding modules.Particularly, GIMLET proposes a distance-based joint position embedding method, where the shortest graph distance is utilized to represent the relative positions between graph nodes, enabling the Transformer encoder to encode both graph and text.GraphToken [152] evaluates a series of GNN models as graph encoders, including GCN, MPNN [111], GIN, Graph Transformer, HGT [59], etc.  Text Encoding.Due to the tremendous capability of LLMs in understanding textual information, most existing methods, such as ProteinChat [149] and DrugChat [144], directly employ LLMs as text encoders.In GraphLLM [142], the tokenizer and frozen embedding table of LLM are leveraged to obtain the representation of node text attributes, aligning with the downstream frozen LLM.</p>
<p>Fusion Module.The goal of the fusion module is to align the graph and text modalities, generating a fusion embedding as input for the LLM.To achieve the goal, a straightforward solution is to design a linear projection layer to directly transform the graph representation generated by GNN into an LLM-compatible soft prompt vector [144], [145], [148].</p>
<p>Additionally, inspired by BLIP2's Q-Former [160], [147]  propose a GIT-Former, which aligns graph, image, and text with the target text modality using self-attention and crossattention mechanisms.</p>
<p>In addition to the above methods, G-Retriever is proposed to integrate both explicit and implicit structural information [151].To be specific, GAT is employed to encode the graph structure, while representing node and relationship details through textual prompts.To accommodate real-world graphs with larger scales, G-Retriever introduces a RAG module specifically designed for retrieving subgraphs relevant to user queries.</p>
<p>Heterophily and Generalization</p>
<p>Despite achieving promising performance in graph tasks, GNNs exhibit several shortcomings.A notable drawback involves the inadequacy of the neighbor information aggregation mechanism, especially when dealing with heterogeneous graphs.GNN performance notably diminishes when faced with instances where adjacent nodes lack similarity.Additionally, GNN encounters challenges in out-of-distribution (OOD) generalization, leading to a degradation in model performance on distributions beyond the training data.This challenge is particularly prevalent in practical applications, primarily due to the inherent difficulty of encompassing all possible graph structures within limited training data.Consequently, when GNNs infer on unseen graph structures, their performance may experience a substantial decline.This reduced generalization capability renders GNNs relatively fragile when confronted with evolving graph data in real-world scenarios.For example, GNNs may encounter difficulties handling newly emergent social relationships in social networks.</p>
<p>LLMs have been utilized to mitigate the above limitations.In particular, GraphText [29] effectively decouples depth and scope by encapsulating node attributes and relationships in the graph syntax tree.This approach yields superior results compared to the GNN baseline, particularly on heterogeneous graphs.Chen et al. [27] investigate the LLM's ability to handle OOD generalization scenarios.They utilize the GOOD [161] benchmark as the criterion and results demonstrate that LLMs exhibit promising performances in addressing OOD generalization issues.OpenGraph [153] aims at solving zero-shot graph tasks across different domains.In this model, LLMs are leveraged to generate synthetic graphs of data scarcity scenarios, thereby enhancing the pre-training process of OpenGraph.</p>
<p>GRAPHS FOR LLMS</p>
<p>LLMs have demonstrated impressive language generation and understanding capabilities across various domains.Nevertheless, they still face several pressing challenges, including factuality awareness, hallucinations, limited explainability in the reasoning process, and beyond.To alleviate these issues, one potential approach is to take advantage of the Knowledge Graphs (KGs), which store high-quality, humancurated factual knowledge in a structured format [5].Recent reviews [30], [162]- [164] have summarized the research on using KGs to enhance LMs.Hu et al. [162] present a review on knowledge-enhanced pre-training language models for natural language understanding and natural language generation.Agrawal et al. [163] systematically review research on mitigating hallucination in LLMs by leveraging KGs across three dimensions: inference process, learning algorithm, and answer validation.Pan et al. [164] provides a comprehensive summary of the integration of KGs and LLMs from three distinct perspectives: KG-enhanced LLMs, LLM-augmented KGs, and the synergized LLMs and KGs, where LLMs and KGs mutually reinforce each other.In this section, we will delve into relevant research that explores the usage of KGs to achieve knowledge-enhanced language model pre-training, mitigate hallucinations, and improve inference explainability.</p>
<p>KG-enhanced LLM Pre-training</p>
<p>While LLMs excel in text understanding and generation, they may still produce grammatically accurate yet factually incorrect information.Explicitly incorporating knowledge from KGs during LLM pre-training holds promise for augmenting LLM's learning capacity and factual awareness [165]- [167].</p>
<p>In this subsection, we will outline the research advancements in KG-enhanced pre-trained language models (PLMs).While there is limited work on KG-enhanced pre-training for LLMs, research on KG-enhanced PLMs can offer insights for LLM pretraining.Existing KG-enhanced pre-training methods can be classified into three main categories: modifying input data, modifying model structures, and modifying pre-training tasks.</p>
<p>Modifying Input Data</p>
<p>Several researchers investigate integrating KG knowledge by modifying input data while keeping the model architecture unchanged.For instance, Moiseev et al. [168] directly train PLMs on mixed corpora consisting of factual triples from KGs and natural language texts.E-BERT [169] aligns entity vectors with BERT's wordpiece vector space, preserving the structure and refraining from additional pre-training tasks.KALM [170] utilizes an entity-name dictionary to identify entities within sentences and employs an entity tokenizer to tokenize them.The input of the Transformer consists of the original word embeddings and entity embeddings.Moreover, K-BERT [171] integrates the original sentence with relevant triples by constructing a sentence tree, where the trunk represents the original sentence and the branches represent the triples.To convert the sentence tree into model input, K-BERT introduces both a hard-position index and a softposition index within the embedding layer to differentiate between original tokens and triple tokens.</p>
<p>Modifying Model Structures</p>
<p>Some research designs knowledge-specific encoders or fusion modules to better inject knowledge into PLMs.ERNIE [172] introduces a K-Encoder to inject knowledge into representations.This involves feeding token embeddings and the concatenation of token embeddings and entity embeddings into a fusion layer for generating new token embeddings and entity embeddings.In contrast, CokeBERT [173] extends this approach by incorporating relation information from KGs during pre-training.It introduces a semantic-driven GNN model to assign relevant scores to relations and entities based on the given text.Finally, it fuses the selected relations and entities with text using a K-Encoder similar to ERNIE.KLMO [174] propose Knowledge Aggregator to fuse text modality and KG modality during pre-training.To incorporate the structural information in KG embeddings, KLMO utilizes KG attention, which integrates a visibility matrix with a conventional attention mechanism, facilitating interaction among adjacent entities and relations within the KG.Subsequently, the token embeddings and contextual KG embeddings are aggregated with entity-level cross-KG attention.</p>
<p>Several studies refrain from modifying the overall structures of the language model but introduce additional adapters to inject knowledge.To preserve the original knowledge within PLMs, Wang et al. [175] propose K-Adapter as a pluggable module to leverage KG knowledge.During pre-training, the parameters of the K-Adapter are updated while the parameters of the PLMs remain frozen.KALA [176] introduces a Knowledge-conditioned Feature Modulation layer, which functions similarly to an adapter module, by scaling and shifting the intermediate hidden representations of PLMs with retrieved knowledge representations.To further control the activation levels of adapters, DAKI [177] incorporates an attention-based knowledge controller module, which is an adapter module with additional linear layers.</p>
<p>Modifying Pre-training Tasks</p>
<p>To explicitly model the interactions between text and KG knowledge, various pre-training tasks are proposed.Three major lines of work in this direction include entity-centric tasks [172], [178]- [181], relation-centric tasks [165], and beyond.</p>
<p>For entity-centric tasks, ERNIE [172] randomly masks some token-entity alignments and then requires the model to predict all corresponding entities based on aligned tokens.LUKE [178] uses Wikipedia articles as training corpora and treats hyperlinks within them as entity annotations, training the model to predict randomly masked entities.KILM [179] also utilizes hyperlinks in Wikipedia articles as entities.However, it inserts entity descriptions after corresponding entities, tasking the model with reconstructing the masked description tokens rather than directly masking entities.In addition to predicting masked entities, GLM [180] further introduces a distractor-suppressed ranking task.This task leverages negative entity samples from KGs as distractors, enhancing the model's ability to distinguish various entities.</p>
<p>Relation-centric tasks are also commonly utilized in KGenhanced PLMs.For instance, JAKET [182] proposes relation prediction and entity category prediction tasks for enhancing knowledge modeling.Dragon [183] is pre-trained in a KG link prediction task.Given a text-KG pair, the model needs to predict the masked relations in KG and the masked tokens in the sentence.ERICA [184] introduces a relation discrimination task aiming at semantically distinguishing the proximity between two relations.Specifically, it adopts a contrastive learning manner, wherein the relation representations of entity pairs belonging to the same relations are encouraged to be closer.</p>
<p>Additionally, there are several innovative pre-training tasks for KG-enhanced pre-training.KEPLER [185] proposes a knowledge embedding task to enhance knowledgeawareness of PLMs.Specifically, it uses PLMs to encode entity descriptions as entity embeddings and jointly train the knowledge embedding and masked language modeling tasks on the same PLM.ERNIE 2.0 [186] constructs a series of continuous pre-training tasks from word, structure, and semantic perspectives.</p>
<p>KG-enhanced LLM Inference</p>
<p>Knowledge within KGs can be dynamically updated, whereas updating the knowledge in LLMs often necessitates adjustment of model parameters, which demands substantial computational resources and time.Therefore, many studies opt to utilize KGs during LLMs inference stage.The "black-box" nature of LLMs poses a significant challenge in understanding how the model made a specific prediction or generated a specific text.Additionally, LLMs have often been criticized for generating false, erroneous, or misleading content, typically referred to as hallucination [31], [32], [187].Given the structured and fact-based nature of KGs, integrating them during the inference stage can enhance the explainability of LLM answers and consequently mitigate hallucinations.</p>
<p>While several methods extract relevant triples from KGs based on user queries and describe these triples in natural language within prompts [188], [189], these approaches overlook the structured information inherent in KGs, and still fail to elucidate how LLMs arrive at their answers.Consequently, extensive studies utilize KGs to aid LLMs in reasoning and generate intermediary information like relation paths, evidence subgraphs, and rationales, forming the basis for explaining the LLM's decision-making process and checking for hallucinations [35], [37], [38], [190]- [192].</p>
<p>Several researchers investigate enabling LLMs to directly reason on KGs and generate relation paths to interpret LLM's answers.The reasoning path at each step helps to enhance the explainability of the answer and the transparency of the reasoning process.Through observing the reasoning decisions made at each step, it becomes possible to identify and address hallucinations arising from LLMs' reasoning.Both RoG [35], Knowledge Solver [191], and Keqing [36] employ relation paths as explanations for LLM's responses.Specifically, given the KG schema and user query, RoG [35] guides LLMs to predict multiple relation paths using textual prompts like "Please generate helpful relation paths for answering the question".Subsequently, LLMs generate the final answer based on the retrieving results of the valid relation path.Conversely, the Knowledge Solver method [191] differs in that it enables LLMs to generate the relation path step by step.Keqing [36] initially decomposes complex questions into several sub-questions, each of which can be addressed by pre-defined logical chains on KGs, and then LLMs will generate final answers with relation paths based on the answers of sub-questions.Mindmap [190] uses evident subgraphs to explain the answers generated by LLMs, where path-based and neighbor-based methods are introduced to obtain several evident subgraphs.The LLM in Mindmap is prompted to merge these evident subgraphs, utilizing the merged graph to generate the final answer.In contrast to previous methods which involve gradually retrieving knowledge and obtaining answers, KGR [37] takes a different approach.Initially, the LLM directly generates a draft answer.Subsequently, it extracts the claims requiring verification from this answer and retrieves KG's information to correct claims with hallucinations.Based on the corrected claims, the LLM adjusts the draft answer to get the final answer.</p>
<p>The above research employs relation paths or evident graphs as the basis for explaining the LLM's decision-making process and checking hallucinations.In contrast, several research explore using inherently interpretable models rather than LLMs to make final predictions.ChatGraph [193] presents an innovative approach to enhance both the text classification capabilities and explainability of ChatGPT.It utilizes ChatGPT to extract triples from unstructured text and subsequently constructs KGs based on these triples.To ensure the explainability of the classified results, ChatGraph avoids employing LLMs directly for predictions.Instead, it leverages a graph model without non-linear activation functions and trains the model on text graphs to get predictions.Given a question and a list of possible answers, XplainLLM [194] propose an explainer model to explain why LLMs choose a particular answer while rejecting others.Specifically, the approach involves constructing an element graph based on the entities present in the question and the candidate answers.Subsequently, a GCN model is employed to assign attention scores to each node within the element graph.Nodes exhibiting high attention scores are identified as reason elements, and LLMs are then prompted to provide explanations based on these selected reason elements.</p>
<p>To assess the transparency and interpretability of LLMs, various benchmarks have been proposed.For example, Li et al. [38] introduce a novel task named Knowledgeaware Language Model Attribution (KaLMA) and develop a corresponding benchmark dataset.This benchmark evaluates the LLM's capability to derive citation information from KG to support its answers.KaLMA also provides an automatic evaluation covering aspects such as text quality, citation quality, and text-citation alignment of the answers.In addition, XplainLLM [194] introduces a dataset for better understanding LLMs' decision-making from the perspectives of "why-choose" and "why-not-choose".</p>
<p>APPLICATIONS</p>
<p>In this section, we will present practical applications that demonstrate the potential and value of GFMs and LLMs.As shown in Table 2, recommender systems, knowledge graphs, AI for science, and robot task planning emerge as the most prevalent domains.We will provide a comprehensive summary of each of these applications.</p>
<p>Recommender Systems</p>
<p>Recommender systems leverage user historical behaviors to predict items that users are likely to appreciate [195]- [197].Graphs play a crucial role in recommender systems, wherein items can be regarded as nodes and collaborative behaviors such as clicks and purchases can be viewed as edges.Recently, an increasing amount of research is exploring the use of LLMs for direct recommendation [198]- [201] or leveraging LLMs to enhance graph models or datasets for recommendation tasks [120], [123], [124], [202], [203].</p>
<p>For directly using LLMs as recommendation models, liu et al. [204] construct task-specific prompts to evaluate ChatGPT's performance on five common recommendation tasks, encompassing rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization.Bao et al. [205] employ prompt templates to guide LLM to decide whether the user will like the target item based on their historical interactions and perform instruction tuning on the LLM to improve its recommendation capability.</p>
<p>For using LLMs to enhance traditional recommendation methods or datasets, KAR [202] leverages LLMs to generate factual knowledge of items and reasoning basis of user preferences; these knowledge texts are then encoded into vectors and integrated into existing recommendation models.Methods like LLM-Rec [125], RLMRec [123], and LLM-Rec [124] enrich recommendation datasets by incorporating LLM-generated descriptions.In contrast, Wu et al. [203] utilize LLMs to condense recommendation datasets, in which LLMs are employed to synthesize a condensed dataset for the content-based recommendation, aiming at addressing the challenge of resource-intensive training on large datasets.</p>
<p>While the previously discussed methods have explored utilizing LLMs for certain recommendation tasks or domains, an emerging research direction aims to develop foundation models for recommendation.Tang et al. [199] propose an LLM-based domain-agnostic framework for sequential recommendation.Their approach integrates user behavior across domains, and leverages LLMs to model user behaviors based on multi-domain historical interactions and item titles.Hua et al. [206] attempt to address the potential unfairness of recommender systems introduced by LLM bias.They propose a Counterfactually Fair Prompting method to develop an unbiased foundation model for recommendation.To summarize the progress in the area of recommendation foundation model, Huang et al. [207] provide a systematic overview of the existing approaches, categorizing them into three main types: language foundation models, personalized agent foundation models, and multi-modal foundation models.</p>
<p>Knowledge Graphs</p>
<p>LLMs with robust text generation and language understanding capabilities have found extensive applications in KGrelated tasks, including KG completion [145], [208], [209], KG question answering [189], [191], [210]- [212], KG reasoning [213] and beyond.Meyer et al. [214] introduce LLM-KG-Bench, a framework that automatically evaluates the model's proficiency in KG engineering tasks such as fixing errors in Turtle files, facts extraction, and dataset generation.KG-LLM [209] is proposed to evaluate LLMs' performance on KG completion, including triple classification, relation prediction, and link prediction tasks.Kim et al. [210] propose KG-GPT, using LLMs for complex reasoning tasks on knowledge graphs.ChatKBQA [211] introduces a generate-then-retrieve framework for LLMs on knowledge base question answering.Wu et al. [189] present a KG-enhanced LLM framework for KG question answering, which involves fine-tuning an LLM to convert structured triples into free-form text, enhancing LLMs' understanding of KG data.The successful application of LLMs in tasks such as KG construction, completion, and question answering offers robust support for advancing the understanding and exploration of KGs.</p>
<p>Drawing inspiration from foundation models in language and vision, researchers are delving into the development of foundation models tailored for KGs.These GFMs aim to generalize to any unseen relations and entities within KGs.Galkin et al. [215] propose Ultra, which learns universal graph representations by leveraging interactions between relations.This study is based on the insight that those interactions remain similar and transferable across different datasets.</p>
<p>AI for Science</p>
<p>The rapid advancement of AI has led to an increasing number of studies leveraging AI to assist scientific research [216], [217].Recent research has applied LLMs and GFMs for scientific purposes, such as drug discovery, molecular property prediction, and material design.Notably, these applications encompass scenarios involving graph-structured data.</p>
<p>The molecular graph is a way of representing molecules, where the nodes represent atoms, and the edges represent the bonds between the atoms.With the emergence of LLMs, researchers have explored their performance in tasks related to molecular graphs.Methods like MolReGPT [139] and GPT-MolBERTa [126] adopt a similar approach, converting molecular graphs into textual descriptions using SMILES language.They create prompts based on SMILES data, asking the LLM to provide detailed information about functional groups, shapes, chemical properties, etc.This information is then used to train a smaller LM for molecular property prediction.In contrast to methods directly using LLMs for prediction, ReLM [136] first uses GNNs to predict highprobability candidate products, and then leverages LLMs to make the final selection from these candidates.</p>
<p>In addition to the above research, LLMs are further utilized in drug discovery and materials design.Bran et al. [105] present ChemCrow, a chemistry agent integrating LLMs and 18 specialized tools for diverse tasks across drug discovery, materials design, and organic synthesis.InstructMol [218] presents a two-stage framework for aligning language and molecule graph modalities in drug discovery.Initially, the framework maintains the LLM and graph encoder parameters fixed, focusing on training the projector to align molecule-graph representations.Subsequently, instruction tuning is conducted on the LLM to address drug discovery tasks.Zhao et al. [219] propose ChemDFM, the first dialogue foundation model for chemistry.Trained on extensive chemistry literature and general data, ChemDFM exhibits proficiency in various chemistry tasks such as molecular recognition, molecular design, and beyond.</p>
<p>Robot Task Planning</p>
<p>Robot task planning aims to decompose tasks into a series of high-level operations for the step-by-step completion by a robot [220].During task execution, the robot needs to perceive information about the surrounding environment, typically represented using scene graphs.In a scene graph, nodes represent scene objects like people and tables, while edges describe the spatial or functional relationships between objects.Enabling LLMs for robot task planning crucially depends on how to represent the environmental information in the scene graph.</p>
<p>Many studies have explored using textual descriptions of scene information and constructing prompts for LLMs to generate task plans.Chalvatzaki et al. [221] introduce the Graph2NL mapping table, representing attributes with different numerical ranges using corresponding textual expressions.For instance, a distance value greater than 5 is represented as "distant", and smaller than 3 is represented as "reachable".SayPlan [222] describes the scene graph in JSON as a text sequence, iteratively invoking LLM to generate plans and allowing for self-correction.Zhen et al. [223] propose an effective prompt template, Think Net Prompt, to enhance LLM performance in task planning.In contrast to methods that rely on language to describe scene graph information, GRID [121] employs the graph transformer to encode scene graphs.It utilizes cross-modal attention to align the graph modality with user instruction, ultimately outputting action tokens through a decoder layer.The powerful understanding and reasoning capabilities of LLMs showcase significant potential in robot task planning.However, as task complexity increases, the search space explosively expands, posing a challenge in efficiently generating viable task plans with LLMs.</p>
<p>FUTURE DIRECTIONS</p>
<p>In this survey, we have thoroughly reviewed the latest developments of Graph Machine Learning in the era of LLMs, revealing significant advancements and potential in this field.By harnessing the power of LLMs, it is potential to enhance Graph ML to enable GFMs.As this research direction is still in the exploratory stage, future directions in this field can be diverse and innovative.Therefore, in this section, we delve into several potential future directions of this promising field.</p>
<p>Generalization and Transferability</p>
<p>While Graph ML has deployed for various graph tasks, a notable problem is their limited capacity for generalization and transferability across different graph domains [40].Different from fields such as NLP and CV, where data often adhere to a uniform format (e.g., a sequence of tokens or a grid of pixels), graphs can be highly heterogeneous in nature.This heterogeneity manifests in varying graph sizes, densities, and types of nodes and edges, which presents a significant challenge in developing a universal model capable of performing optimally across various graph structure data.Currently, LLMs have demonstrated great potentials in improving the generalization ability of the graph model.For example, OFA [129] provides a solution for classification tasks across several certain domains.Nevertheless, there is still scarce exploration of the generalizability of GFMs compared to LLMs.Therefore, future research should aim to develop more adaptable and flexible models that can effectively apply learned patterns from one graph type, such as social networks, to another, like molecular structures, without extensive retraining.</p>
<p>Multi-modal Graph Learning</p>
<p>Recent LLMs have shown significant potential in advancing GFMs.Many efforts have been made to transform graph data into formats suitable for LLM input, such as tokens or text [27], [84], [131].However, many nodes in graphs are enriched with diverse modalities of information, including text, images, and videos.Understanding this multi-modal data can potentially benefit graph learning.For example, on social media platforms, a user's post could include textual content, images, and videos, all of which are valuable for comprehensive user modeling.Given the importance of multi-modal data, a promising direction for future research is to empower LLMs to process and integrate graph structure with multi-modal data.Currently, TOUCHUP-G [86] makes an initial exploration of multi-modal ( i.e., texts, images) in graph learning.In the future, we expect the development of a unified model capable of modeling universal modalities for more advanced GFMs.</p>
<p>Trustworthiness</p>
<p>The recent applications of LLMs for Graph ML have significantly enhanced graph modeling capabilities and broadened their utility in various fields.Despite these advancements, with the growing reliance on these models, it is important to ensure their trustworthiness, particularly in critical areas like healthcare, finance, and social network analysis [224], [225].Robustness is fundamental in safeguarding the models against adversarial attacks, ensuring consistent reliability.Explainability is essential for users to understand and trust the decisions made by these models.Fairness is crucial for the model's ethical and effective use in various applications.Privacy is important for legal compliance and key to maintaining user trust.Therefore, the development of trustworthy LLMs on graphs must be equipped with Robustness&amp;Safety, Explainability, Fairness, and Privacy, ensuring their safe and effective use in various applications.</p>
<p>Robustness&amp;Safety</p>
<p>Recently, integrating LLMs into Graph ML has shown promising performance in various downstream tasks, but they are also highly vulnerable to adversarial perturbations, raising significant concerns about their robustness and safety.To enhance the resilience of these models, some studies add adversarial perturbations to GNNs [226], [227] or LLMs [228], [229] for adversarial training.However, these methods may not be effective for the new paradigm of Graph ML integrating LLMs, as vulnerabilities can arise from both graphs, such as graph poisoning attacks [230], [231] and graph modification attacks [232], [233], and from the language model, like prompt attacks [234] and misleading text data [235].To address these issues, more sophisticated detection and defense mechanisms need to be developed by considering both the intricacies of LLMs and graphs to ensure the comprehensive safety and robustness of Graph ML.</p>
<p>Explainability</p>
<p>Nowadays, LLMs are increasingly employed in Graph ML across various applications, such as recommender systems [15], [236] and molecular discovery [85], [139].However, due to privacy and security concerns, an application provider may prefer to provide an API version without revealing the architecture and parameters of the LLM, such as with ChatGPT.This lack of transparency can make it challenging for users to understand the model's results, leading to confusion and dissatisfaction.Therefore, it's important to enhance the explainability of Graph ML, especially with LLMs.Owing to their reasoning and interpretive capabilities, LLMs are promising to provide better explainability in graphrelated tasks.For example, P5 [236] can provide reasons for its recommendations in recommendation tasks.Future efforts should be directed toward making the inner workings of these models more transparent and explainable to better comprehend their decision-making processes.</p>
<p>Fairness</p>
<p>As LLMs become prevalent in enhancing Graph ML towards GFMs, concerns about their fairness are growing.</p>
<p>Fairness is crucial to ensure these models operate without biases or discrimination, especially when dealing with complex, interconnected graph data [225].Recent studies demonstrate that both language models [237], [238] and GNN models [239] can potentially be discriminatory and unfair [42].Therefore, it is necessary to maintain fairness in both textual and graph contexts.To enhance the fairness of LLMs, recent studies include retraining strategies that adjust model parameters for unbiased outputs [240], implementing alignment constraints [241], and adopting contrastive learning to diminish bias in model training [242].Concurrently, studies like FairNeg [239] also explore improving the fairness of recommendation data.Despite these efforts, achieving fairness in GFMs is still a significant challenge that needs further exploration.</p>
<p>Privacy</p>
<p>Privacy is a critical issue in Graph ML, particularly given the risk of these models inadvertently leaking sensitive information contained in graph data [243]- [245].For example, Graph ML integrated with LLMs could potentially expose private user data, like browsing histories or social connections when generating outputs.This concern is especially pressing in highly data-sensitive areas such as healthcare or finance.To mitigate these privacy risks, [246] introduces Privacy-Preserving Prompt Tuning (RAPT) to protect user privacy through local differential privacy.Future exploration in LLM-enhanced Graph ML should also focus on integrating privacy-preserving technologies like differential privacy and federated learning to strengthen data security and user privacy.</p>
<p>Efficiency</p>
<p>While LLMs have proven effective in constructing GFMs, their operational efficiency, particularly in processing large and complex graphs, is still a significant challenge [247].For example, the use of APIs like GPT4 for large-scale graph tasks can lead to high costs under current billing models.Additionally, deploying open-source large models (e.g., LLaMa) for parameter updates or just inference in local environments demands substantial computational resources and storage.Therefore, enhancing the efficiency of LLMs for graph tasks remains a critical issue.Recent studies introduce techniques like LoRA [156] and QLoRA [248] to fine-tune LLM parameters more efficiently.Furthermore, model pruning [249], [250] is also a promising method to increase efficiency by removing redundant parameters or structures from LLMs, thereby simplifying their application in graph machine learning.</p>
<p>CONCLUSION</p>
<p>In this survey, we have thoroughly reviewed the recent progress of graph applications and Graph ML in the era of LLMs, an emerging field in graph learning.We first review the evolution of Graph ML, and then delve into various methods of LLMs enhancing Graph ML.Due to the remarkable capabilities in various fields, LLMs have great potential to enhance Graph ML towards GFMs.We further explore the augmenting of LLMs with graphs, highlighting their ability to enhance LLM pre-training and inference.</p>
<p>Additionally, we demonstrate their potential in diverse applications such as molecule discovery, knowledge graphs, and recommender systems.Despite their success, this field is still evolving and presents numerous opportunities for further advancements.Therefore, we further discuss several challenges and potential future directions.Overall, our survey aims to provide a systematic and comprehensive review to researchers and practitioners, inspiring future explorations in this promising field.</p>
<p>Figure 2 :
2
Figure 2: The outline of our survey.Section 3 Deep Learning on Graphs explores the development of DNN-based methods, focusing on the Backbone Architecture, Graph Pretext Tasks, and Downstream Adaption three aspects.Section 4 LLMs for Graph Models explore how current LLMs help the current Graph ML towards GFMs from Enhancing Feature Quality, Solving Vanilla GNN Training Limitations, and Heterophily and Generalization three aspects.Section 5 Graph for LLMs focuses on Knowledge Graph(KG)-enhanced LLM Pre-training and KG-enhanced LLM Inference.Section 6 Applications presents various applications, including Recommender System, Knowledge Graph, AI for Science, and Robot Task Planning.Section 7 Future Directions discusses potential future directions for LLMs in graph machine learning from the Generalization and Transferability, Multi-modal Graph Learning, Trustworthiness and Efficiency.</p>
<p>Figure 3 :
3
Figure 3: A comparison of pre-training, fine-tuning, and prompt tuning.(a) Pre-training involves training the GNN model based on specific pre-training tasks.(b) Fine-tuning updates the parameters of the pre-trained GNN model according to the downstream tasks.(c) Prompt tuning generates and updates the features of the prompt according to the downstream tasks, while keeping the pre-trained GNN model fixed and without any modification.</p>
<p>Figure 4 :
4
Figure 4: Illustration of LLMs for Graph ML. (1) Methods using LLMs for Enhancing Feature Quality by enhancing feature representation, generating augmented information, and aligning feature space.(2) Explorations for solving Vanilla GNN Training Limitations are categorized based on how structural information in the graph is processed: ignoring structural information, implicit structural information, and explicit structural information.(3) Research about employing LLMs to alleviate the limitations of Heterophily and Generalization.</p>
<p>Figure 5 :
5
Figure 5: The illustration of employing LLMs with implicit and explicit structural information.(1) Methods leveraging implicit structural information describe nodes and graph structure information in natural language and combine task-specific instructions to form a textual prompt, which is then input into the LLM to generate prediction results.(2) Methods employing explicit structural information use GNNs and LLMs to encode graph and instruction information separately.Then, fusion layers are added to align the graph and text modalities, and the fused embedding is input into the LLM for prediction.</p>
<p>Table 1 :
1
A comparison of various DNN-based models.We present Models and their Architecture, Pretext Task, Adaptation Method, and Downstream Tasks.URL in Adaptation Method indicates Unsupervised Representation Learning.
ModelArchitecturePretext TaskAdaptation MethodDownstream TasksDGI [76]GNNContrastive LearningURLNodeGRACE [77]GNNContrastive LearningURLNodeGraphMAE [78]GNNGraph GenerationURLNode, GraphMVGRL [79]GNNContrastive LearningURLNode, GraphGraphCL [10]GNNContrastive LearningFine-tuningNode, GraphCSSL</p>
<p>Revisiting link prediction: A data perspective. H Mao, J Li, H Shomer, B Li, W Fan, Y Ma, T Zhao, N Shah, J Tang, arXiv:2310.007932023arXiv preprint</p>
<p>M Hashemi, S Gong, J Ni, W Fan, B A Prakash, W Jin, arXiv:2402.03358A comprehensive survey on graph reduction: Sparsification, coarsening, and condensation. 2024arXiv preprint</p>
<p>Adversarial attacks for black-box recommender systems via copying transferable cross-domain user profiles. W Fan, X Zhao, Q Li, T Derr, Y Ma, H Liu, J Wang, J Tang, IEEE Transactions on Knowledge and Data Engineering. 2023</p>
<p>Disentangled contrastive learning for social recommendation. J Wu, W Fan, J Chen, S Liu, Q Li, K Tang, Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management. the 31st ACM International Conference on Information &amp; Knowledge Management2022</p>
<p>Knowledge-enhanced black-box attacks for recommendations. J Chen, W Fan, G Zhu, X Zhao, C Yuan, Q Li, Y Huang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Epidemic graph convolutional network. T Derr, Y Ma, W Fan, X Liu, C Aggarwal, J Tang, Proceedings of the 13th International Conference on Web Search and Data Mining (WSDM). the 13th International Conference on Web Search and Data Mining (WSDM)2020</p>
<p>Deep learning on graphs. Y Ma, J Tang, 2021Cambridge University Press</p>
<p>A comprehensive survey on graph neural networks. Z Wu, S Pan, F Chen, G Long, C Zhang, S Y Philip, IEEE transactions on neural networks and learning systems. 202032</p>
<p>Lineartime graph neural networks for scalable recommendations. J Zhang, R Xue, W Fan, X Xu, Q Li, J Pei, X Liu, arXiv:2402.139732024arXiv preprint</p>
<p>Graph contrastive learning with augmentations. Y You, T Chen, Y Sui, T Chen, Z Wang, Y Shen, Advances in neural information processing systems. 202033</p>
<p>Contrastive self-supervised learning for graph classification. J Zeng, P Xie, Proceedings of the AAAI conference on Artificial Intelligence. the AAAI conference on Artificial Intelligence202135832</p>
<p>Ccgl: Contrastive cascade graph learning. X Xu, F Zhou, K Zhang, S Liu, IEEE Transactions on Knowledge and Data Engineering. 3552022</p>
<p>Gcc: Graph contrastive coding for graph neural network pre-training. J Qiu, Q Chen, Y Dong, J Zhang, H Yang, M Ding, K Wang, J Tang, Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining2020</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Unified vision-language pre-training for image captioning and vqa. L Zhou, H Palangi, L Zhang, H Hu, J Corso, J Gao, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence20203449</p>
<p>Recommender systems in the era of large language models (llms). W Fan, Z Zhao, J Li, Y Liu, X Mei, Y Wang, Z Wen, F Wang, X Zhao, J Tang, Q Li, Aug. 2023</p>
<p>Zerog: Investigating cross-dataset zero-shot transferability in graphs. Y Li, P Wang, Z Li, J X Yu, J Li, arXiv:2402.112352024arXiv preprint</p>
<p>Graph foundation models. H Mao, Z Chen, W Tang, J Zhao, Y Ma, T Zhao, N Shah, M Galkin, J Tang, arXiv:2402.022162024arXiv preprint</p>
<p>On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Natural language is all a graph needs. R Ye, C Zhang, R Wang, S Xu, Y Zhang, Aug. 2023</p>
<p>Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, M Zhou, X He, S Han, Jul. 2023</p>
<p>Graphgpt: Graph instruction tuning for large language models. J Tang, Y Yang, W Wei, L Shi, L Su, S Cheng, D Yin, C Huang, Oct. 2023</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozire, N Goyal, E Hambro, F Azhar, arXiv:2302.139712023arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Simteg: A frustratingly simple approach improves textual graph learning. K Duan, Q Liu, T.-S Chua, S Yan, W T Ooi, Q Xie, J He, arXiv:2308.025652023arXiv preprint</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Z Chen, H Mao, H Li, W Jin, H Wen, X Wei, S Wang, D Yin, W Fan, H Liu, J Tang, Aug. 2023</p>
<p>Node feature extraction by selfsupervised multi-scale neighborhood prediction. E Chien, W.-C Chang, C.-J Hsieh, H.-F Yu, J Zhang, O Milenkovic, I S Dhillon, arXiv:2111.000642021arXiv preprint</p>
<p>Graphtext: Graph reasoning in text space. J Zhao, L Zhuo, Y Shen, M Qu, K Liu, M Bronstein, Z Zhu, J Tang, Oct. 2023</p>
<p>A survey on rag meets llms: Towards retrieval-augmented large language models. Y Ding, W Fan, L Ning, S Wang, H Li, D Yin, T.-S Chua, Q Li, arXiv:2405.062112024arXiv preprint</p>
<p>Barack's wife hillary: Using knowledge-graphs for fact-aware language modeling. R L Logan, I V , N F Liu, M E Peters, M Gardner, S Singh, arXiv:1906.072412019arXiv preprint</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Explainability for large language models: A survey. H Zhao, H Chen, F Yang, N Liu, H Deng, H Cai, S Wang, D Yin, M Du, ACM Transactions on Intelligent Systems and Technology. 1522024</p>
<p>Large language models can learn temporal reasoning. S Xiong, A Payani, R Kompella, F Fekri, arXiv:2401.068532024arXiv preprint</p>
<p>Reasoning on graphs: Faithful and interpretable large language model reasoning. L Luo, Y.-F Li, G Haffari, S Pan, Oct. 2023</p>
<p>keqing: knowledge-based question answering is a nature chain-of-thought mentor of llm. C Wang, Y Xu, Z Peng, C Zhang, B Chen, X Wang, L Feng, B An, arXiv:2401.004262023arXiv preprint</p>
<p>Mitigating large language model hallucinations via autonomous knowledge graph-based retrofitting. X Guan, Y Liu, H Lin, Y Lu, B He, X Han, L Sun, arXiv:2311.133142023arXiv preprint</p>
<p>Towards verifiable generation: A benchmark for knowledge-aware language model attribution. X Li, Y Cao2, L Pan, Y Ma, A Sun, Oct. 2023</p>
<p>Graph learning and its advancements on large language models: A holistic survey. S Wei, Y Zhao, X Chen, Q Li, F Zhuang, J Liu, F Ren, G Kou, arXiv:2212.089662022arXiv preprint</p>
<p>Large graph models: A perspective. Z Zhang, H Li, Z Zhang, Y Qin, X Wang, W Zhu, Aug. 2023</p>
<p>Large language models on graphs: A comprehensive survey. B Jin, G Liu, C Han, M Jiang, H Ji, J Han, arXiv:2312.027832023arXiv preprint</p>
<p>Y Li, Z Li, P Wang, J Li, X Sun, H Cheng, J X Yu, arXiv:2311.12399A survey of graph meets large language model: Progress and future directions. 2023arXiv preprint</p>
<p>Towards graph foundation models: A survey and beyond. J Liu, C Yang, Z Lu, J Chen, Y Li, M Zhang, T Bai, Y Fang, L Sun, P S Yu, arXiv:2310.118292023arXiv preprint</p>
<p>Fast graph condensation with structure-based neural tangent kernel. L Wang, W Fan, J Li, Y Ma, Q Li, arXiv:2310.110462023arXiv preprint</p>
<p>Untargeted black-box attacks for social recommendations. W Fan, S Wang, X -Y. Wei, X Mei, Q Li, arXiv:2311.071272023arXiv preprint</p>
<p>Compound-protein interaction prediction with end-to-end learning of neural networks for graphs and sequences. M Tsubaki, K Tomii, J Sese, Bioinformatics. 3522019</p>
<p>Graph neural networks for social recommendation. W Fan, Y Ma, Q Li, Y He, E Zhao, J Tang, D Yin, The world wide web conference. 2019</p>
<p>A graph neural network framework for social recommendations. W Fan, Y Ma, Q Li, J Wang, G Cai, J Tang, D Yin, IEEE Transactions on Knowledge and Data Engineering. 2020</p>
<p>Line: Large-scale information network embedding. J Tang, M Qu, M Wang, M Zhang, J Yan, Q Mei, Proceedings of the 24th international conference on world wide web. the 24th international conference on world wide web2015</p>
<p>Deepwalk: Online learning of social representations. B Perozzi, R Al-Rfou, S Skiena, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data mining2014</p>
<p>node2vec: Scalable feature learning for networks. A Grover, J Leskovec, Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining2016</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Inductive representation learning on large graphs. W Hamilton, Z Ying, J Leskovec, Advances in neural information processing systems. 201730</p>
<p>Graph attention networks. P Velikovi, G Cucurull, A Casanova, A Romero, P Lio, Y Bengio, arXiv:1710.109032017arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez,  Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Graph transformer. Y Li, X Liang, Z Hu, Y Chen, E P Xing, 2018</p>
<p>Graph transformer networks. S Yun, M Jeong, R Kim, J Kang, H J Kim, Advances in neural information processing systems. 201932</p>
<p>Accurate learning of graph representations with graph multiset pooling. J Baek, M Kang, S J Hwang, ICLR2021</p>
<p>Heterogeneous graph transformer. Z Hu, Y Dong, K Wang, Y Sun, Proceedings of the web conference 2020. the web conference 20202020</p>
<p>Graph-bert: Only attention is needed for learning graph representations. J Zhang, H Zhang, C Xia, L Sun, arXiv:2001.051402020arXiv preprint</p>
<p>Graphformers: Gnn-nested transformers for representation learning on textual graph. J Yang, Z Liu, S Xiao, C Li, D Lian, S Agrawal, A Singh, G Sun, X Xie, Advances in Neural Information Processing Systems. 202134810</p>
<p>Do transformers really perform badly for graph representation?. C Ying, T Cai, S Luo, S Zheng, G Ke, D He, Y Shen, T.-Y Liu, Advances in Neural Information Processing Systems. 202134</p>
<p>Introducing self-attention to target attentive graph neural networks. S Mitheran, A Java, S K Sahu, A Shaikh, arXiv:2107.015162021arXiv preprint</p>
<p>Rethinking graph transformers with spectral attention. D Kreuzer, D Beaini, W Hamilton, V Ltourneau, P Tossou, Advances in Neural Information Processing Systems. 202134</p>
<p>A generalization of transformer networks to graphs. V P Dwivedi, X Bresson, arXiv:2012.096992020arXiv preprint</p>
<p>Can large language models empower molecular property prediction?. C Qian, H Tang, Z Yang, H Liang, Y Liu, Jul. 2023</p>
<p>Graphwiz: An instructionfollowing language model for graph problems. N Chen, Y Li, J Tang, J Li, arXiv:2402.160292024arXiv preprint</p>
<p>Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, April 2023142023</p>
<p>Learning on large-scale text-attributed graphs via variational inference. J Zhao, M Qu, C Li, H Yan, Q Liu, R Li, X Xie, J Tang, arXiv:2210.147092022arXiv preprint</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. P He, X Liu, J Gao, W Chen, arXiv:2006.036542020arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, ICML. 2021</p>
<p>Visual chatgpt: Talking, drawing and editing with visual foundation models. C Wu, S Yin, W Qi, X Wang, Z Tang, N Duan, arXiv:2303.046712023arXiv preprint</p>
<p>Zero-shot text-to-image generation. A Ramesh, M Pavlov, G Goh, S Gray, C Voss, A Radford, M Chen, I Sutskever, ICML. 2021</p>
<p>Fashionregen: Llm-empowered fashion report generation. Y Ding, Y Ma, W Fan, Y Yao, T.-S Chua, Q Li, arXiv:2403.066602024arXiv preprint</p>
<p>Harnessing the power of large language models for natural language to firstorder logic translation. Y Yang, S Xiong, A Payani, E Shareghi, F Fekri, arXiv:2305.155412023arXiv preprint</p>
<p>Deep graph infomax. P Velikovi, W Fedus, W L Hamilton, P Li , Y Bengio, R D Hjelm, arXiv:1809.103412018arXiv preprint</p>
<p>Deep graph contrastive representation learning. Y Zhu, Y Xu, F Yu, Q Liu, S Wu, L Wang, arXiv:2006.041312020arXiv preprint</p>
<p>Graphmae: Self-supervised masked graph autoencoders. Z Hou, X Liu, Y Cen, Y Dong, H Yang, C Wang, J Tang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Contrastive multi-view representation learning on graphs. K Hassani, A H Khasahmadi, International conference on machine learning. PMLR2020</p>
<p>Pre-training of graph augmented transformers for medication recommendation. J Shang, T Ma, C Xiao, J Sun, arXiv:1906.003462019arXiv preprint</p>
<p>Adaptergnn: Efficient delta tuning improves generalization ability in graph neural networks. S Li, X Han, J Bai, arXiv:2304.095952023arXiv preprint</p>
<p>Self-supervised graph transformer on large-scale molecular data. Y Rong, Y Bian, T Xu, W Xie, Y Wei, W Huang, J Huang, Advances in Neural Information Processing Systems. 202033</p>
<p>G-adapter: Towards structureaware parameter-efficient transfer learning for graph transformer networks. A Gui, J Ye, H Xiao, arXiv:2305.103292023arXiv preprint</p>
<p>Graphgpt: Graph learning with generative pre-trained transformers. Q Zhao, W Ren, T Li, X Xu, H Liu, arXiv:2401.005292023arXiv preprint</p>
<p>A molecular multimodal foundation model associating molecule graphs with natural language. B Su, D Du, Z Yang, Y Zhou, J Li, A Rao, H Sun, Z Lu, J.-R Wen, arXiv:2209.054812022arXiv preprint</p>
<p>Touchup-g: Improving feature representation through graphcentric finetuning. J Zhu, X Song, V N Ioannidis, D Koutra, C Faloutsos, arXiv:2309.138852023arXiv preprint</p>
<p>Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. Z Liu, X Yu, Y Fang, X Zhang, Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023</p>
<p>Gppt: Graph pretraining and prompt tuning to generalize graph neural networks. M Sun, K Zhou, X He, Y Wang, X Wang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Prompt tuning for multi-view graph contrastive learning. C Gong, X Li, J Yu, C Yao, J Tan, C Yu, D Yin, arXiv:2310.103622023arXiv preprint</p>
<p>Universal prompt tuning for graph neural networks. T Fang, Y M Zhang, Y Yang, C Wang, C Lei, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>All in one: Multi-task prompting for graph neural networks. X Sun, H Cheng, J Li, B Liu, J Guan, 2023</p>
<p>Ultra-dp: Unifying graph pre-training with multi-task graph dual prompt. M Chen, Z Liu, C Liu, J Li, Q Mao, J Sun, arXiv:2310.148452023arXiv preprint</p>
<p>Enhancing graph neural networks with structure-based prompt. Q Ge, Z Zhao, Y Liu, A Cheng, X Li, S Wang, D Yin, arXiv:2310.173942023arXiv preprint</p>
<p>Prodigy: Enabling in-context learning over graphs. Q Huang, H Ren, P Chen, G Krmanc, D Zeng, P Liang, J Leskovec, arXiv:2305.126002023arXiv preprint</p>
<p>Sgl-pt: A strong graph learner with graph prompt tuning. Y Zhu, J Guo, S Tang, arXiv:2302.124492023arXiv preprint</p>
<p>Deep prompt tuning for graph transformers. R Shirkavand, H Huang, arXiv:2309.101312023arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, 2020NeurIPS</p>
<p>Lamda: Language models for dialog applications. R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, arXiv:2201.082392022arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.023112022arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K R Narasimhan, S Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. J Zhang, arXiv:2304.111162023arXiv preprint</p>
<p>Train your own gnn teacher: Graph-aware distillation on textual graphs. C Mavromatis, V N Ioannidis, S Wang, D Zheng, S Adeshina, J Ma, H Zhao, C Faloutsos, G Karypis, arXiv:2304.106682023arXiv preprint</p>
<p>Graphologue: Exploring large language model responses with interactive diagrams. P Jiang, J Rayan, S P Dow, H Xia, arXiv:2305.114732023arXiv preprint</p>
<p>Do large language models understand chemistry? a conversation with chatgpt. C M Castro Nascimento, A S Pimentel, Journal of Chemical Information and Modeling. 6362023</p>
<p>Chemcrow: Augmenting large-language models with chemistry tools. A M Bran, S Cox, A D White, P Schwaller, arXiv:2304.053762023arXiv preprint</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. E Kasneci, K Seler, S chemann, M Bannert, D Dementieva, F Fischer, U Gasser, G Groh, S nnemann, E llermeier, Learning and Individual Differences. 1031022742023</p>
<p>How does chatgpt perform on the united states medical licensing examination? the implications of large language models for medical education and knowledge assessment. A Gilson, C W Safranek, T Huang, V Socrates, L Chi, R A Taylor, D Chartash, JMIR Medical Education. 91e453122023</p>
<p>S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023arXiv preprint</p>
<p>Finbert: A pretrained language model for financial communications. Y Yang, M C S Uy, A Huang, arXiv:2006.080972020arXiv preprint</p>
<p>Graph chain-of-thought: Augmenting large language models by reasoning on graphs. B Jin, C Xie, J Zhang, K K Roy, Y Zhang, S Wang, Y Meng, J Han, arXiv:2404.071032024arXiv preprint</p>
<p>Neural message passing for quantum chemistry. J Gilmer, S S Schoenholz, P F Riley, O Vinyals, G E Dahl, International conference on machine learning. 2017</p>
<p>Graph convolutional neural networks for web-scale recommender systems. R Ying, R He, K Chen, P Eksombatchai, W L Hamilton, J Leskovec, Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining2018</p>
<p>How powerful are graph neural networks?. K Xu, W Hu, J Leskovec, S Jegelka, arXiv:1810.008262018arXiv preprint</p>
<p>Provably powerful graph networks. H Maron, H Ben-Hamu, H Serviansky, Y Lipman, Advances in neural information processing systems. 201932</p>
<p>Weisfeiler and leman go neural: Higherorder graph neural networks. C Morris, M Ritzert, M Fey, W L Hamilton, J E Lenssen, G Rattan, M Grohe, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>An effective self-supervised framework for learning expressive molecular global representations to drug discovery. P Li, J Wang, Y Qiao, H Chen, Y Yu, X Yao, P Gao, G Xie, S Song, Briefings in Bioinformatics. 2261092021</p>
<p>Heterformer: Transformerbased deep node representation learning on heterogeneous textrich networks. B Jin, Y Zhang, Q Zhu, J Han, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Edgeformers: Graphempowered transformers for representation learning on textualedge networks. B Jin, Y Zhang, Y Meng, J Han, arXiv:2302.110502023arXiv preprint</p>
<p>Pre-training graph neural networks for generic structural feature extraction. Z Hu, C Fan, T Chen, K.-W Chang, Y Sun, arXiv:1905.137282019arXiv preprint</p>
<p>Lkpnr: Llm and kg for personalized news recommendation framework. C Hao, X Runfeng, C Xiangyang, Y Zhou, W Xin, X Zhanwei, Z Kai, Aug. 2023</p>
<p>Grid: Scene-graph-based instruction-driven robotic task planning. Z Ni, X.-X Deng, C Tai, X.-Y Zhu, X Wu, Y.-J Liu, L Zeng, Sep. 2023</p>
<p>Harnessing explanations: Llm-to-lm interpreter for enhanced textattributed graph representation learning. X He, X Bresson, T Laurent, A Perold, Y Lecun, B Hooi, Oct. 2023</p>
<p>Representation learning with large language models for recommendation. X Ren, W Wei, L Xia, L Su, S Cheng, J Wang, D Yin, C Huang, Oct. 2023</p>
<p>Llmrec: Large language models with graph augmentation for recommendation. W Wei, X Ren, J Tang, Q Wang, L Su, S Cheng, J Wang, D Yin, C Huang, </p>
<p>Llm-rec: Personalized recommendation via prompting large language models. H Lyu, S Jiang, H Zeng, Y Xia, J Luo, arXiv:2307.157802023arXiv preprint</p>
<p>Gpt-molberta: Gpt molecular features language model for molecular property prediction. S Balaji, R Magar, Y Jadhav, A B Farimani, Oct. 2023</p>
<p>Empower text-attributed graphs learning with large language models (llms). J Yu, Y Ren, C Gong, J Tan, X Li, X Zhang, Oct. 2023</p>
<p>Large language models as topological structure enhancers for text-attributed graphs. S Sun, Y Ren, C Ma, X Zhang, arXiv:2311.143242023arXiv preprint</p>
<p>One for all: Towards training one graph model for all classification tasks. H Liu, J Feng, L Kong, N Liang, D Tao, Y Chen, M Zhang, arXiv:2310.001492023arXiv preprint</p>
<p>Beyond text: A deep dive into large language models' ability on understanding graph data. Y Hu, Z Zhang, L Zhao, Oct. 2023</p>
<p>Can language models solve graph problems in natural language. H Wang, S Feng, T He, Z Tan, X Han, Y Tsvetkov, arXiv:2305.100372023arXiv preprint</p>
<p>Evaluating large language models on graphs: Performance insights and comparative analysis. C Liu, B Wu, Sep. 2023</p>
<p>Graph agent: Explicit reasoning agent for graphs. Q Wang, Z Gao, R Xu, arXiv:2310.164212023arXiv preprint</p>
<p>Llm-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions. A N Rubungo, C Arnold, B P Rand, A B Dieng, Oct. 2023</p>
<p>Exploring large language model for graph data understanding in online job recommendations. L Wu, Z Qiu, Z Zheng, H Zhu, E Chen, arXiv:2307.057222023arXiv preprint</p>
<p>Relm: Leveraging language models for enhanced chemical reaction prediction. Y Shi, A Zhang, E Zhang, Z Liu, X Wang, arXiv:2310.135902023arXiv preprint</p>
<p>Can llms effectively leverage graph structural information: When and why. J Huang, X Zhang, Q Mei, J Ma, Sep. 2023</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J Halcrow, B Perozzi, Oct. 2023</p>
<p>Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective. J Li, Y Liu, W Fan, X.-Y Wei, H Liu, J Tang, Q Li, arXiv:2306.066152023arXiv preprint</p>
<p>R Chen, T Zhao, A Jaiswal, N Shah, Z Wang, arXiv:2402.08170Llaga: Large language and graph assistant. 2024arXiv preprint</p>
<p>Graphedit: Large language models for graph structure learning. Z Guo, L Xia, Y Yu, Y Wang, Z Yang, W Wei, L Pang, T.-S Chua, C Huang, arXiv:2402.151832024arXiv preprint</p>
<p>Graphllm: Boosting graph reasoning ability of large language model. Z Chai, T Zhang, L Wu, K Han, X Hu, X Huang, Y Yang, Oct. 2023</p>
<p>Graph neural prompting with large language models. Y Tian, H Song, Z Wang, H Wang, Z Hu, F Wang, N V Chawla, P Xu, Sep. 2023</p>
<p>Drugchat: Towards enabling chatgpt-like capabilities on drug molecule graphs. Y Liang, R Zhang, L Zhang, P Xie, May 2023</p>
<p>Making large language models perform better in knowledge graph completion. Y Zhang, Z Chen, W Zhang, H Chen, Oct. 2023</p>
<p>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning. H Zhao, S Liu, C Ma, H Xu, J Fu, Z.-H Deng, L Kong, Q Liu, Bioinformatics. Jun. 2023Preprint</p>
<p>Git-mol: A multi-modal large language model for molecular science with graph, image, and text. P Liu, Y Ren, Z Ren, Aug. 2023</p>
<p>Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine. Y Luo, J Zhang, S Fan, K Yang, Y Wu, M Qiao, Z Nie, arXiv:2308.094422023arXiv preprint</p>
<p>Proteinchat: Towards achieving chatgpt-like functionalities on protein 3d structures. H Guo, M Huo, R Zhang, P Xie, 2023</p>
<p>Disentangled representation learning with large language models for textattributed graphs. Y Qin, X Wang, Z Zhang, W Zhu, arXiv:2310.181522023arXiv preprint</p>
<p>G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. X He, Y Tian, Y Sun, N V Chawla, T Laurent, Y Lecun, X Bresson, B Hooi, arXiv:2402.076302024arXiv preprint</p>
<p>Let your graph do the talking: Encoding structured data for llms. B Perozzi, B Fatemi, D Zelle, A Tsitsulin, M Kazemi, R Al-Rfou, J Halcrow, arXiv:2402.058622024arXiv preprint</p>
<p>Opengraph: Towards open graph foundation models. L Xia, B Kao, C Huang, arXiv:2403.011212024arXiv preprint</p>
<p>B Jin, W Zhang, Y Zhang, Y Meng, X Zhang, Q Zhu, J Han, arXiv:2305.12268Patton: Language model pretraining on text-rich networks. 2023arXiv preprint</p>
<p>Learning multiplex embeddings on text-rich networks with one text encoder. B Jin, W Zhang, Y Zhang, Y Meng, H Zhao, J Han, arXiv:2310.066842023arXiv preprint</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.096852021arXiv preprint</p>
<p>Gml: Graph modelling language. M Himsolt, 1997University of Passau</p>
<p>Graph markup language (graphml). U Brandes, M Eiglsperger, J Lerner, C Pich, 2013</p>
<p>Lost in the middle: How language models use long contexts. N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, arXiv:2307.031722023arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, arXiv:2301.125972023arXiv preprint</p>
<p>Good: A graph out-of-distribution benchmark. S Gui, X Li, L Wang, S Ji, Advances in Neural Information Processing Systems. 202235</p>
<p>A survey of knowledge enhanced pre-trained language models. L Hu, Z Liu, Z Zhao, L Hou, L Nie, J Li, IEEE Transactions on Knowledge and Data Engineering. 2023</p>
<p>Can knowledge graphs reduce hallucinations in llms?: A survey. G Agrawal, T Kumarage, Z Alghami, H Liu, arXiv:2311.079142023arXiv preprint</p>
<p>Unifying large language models and knowledge graphs: A roadmap. S Pan, L Luo, Y Wang, C Chen, J Wang, X Wu, Jun. 2023</p>
<p>Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. Y Sun, S Wang, S Feng, S Ding, C Pang, J Shang, J Liu, X Chen, Y Zhao, Y Lu, 20212107arXiv e-prints</p>
<p>A knowledgeenriched ensemble method for word embedding and multi-sense embedding. L Fang, Y Luo, K Feng, K Zhao, A Hu, IEEE Transactions on Knowledge and Data Engineering. 2022</p>
<p>Oerl: Enhanced representation learning via open knowledge graphs. Q Li, D Wang, S F K Song, Y Zhang, G Yu, IEEE Transactions on Knowledge and Data Engineering. 2022</p>
<p>Skill: Structured knowledge infusion for large language models. F Moiseev, Z Dong, E Alfonseca, M Jaggi, May 2022</p>
<p>E-bert: Efficientyet-effective entity embeddings for bert. N Poerner, U Waltinger, H Sch tze, arXiv:1911.036812019arXiv preprint</p>
<p>Knowledge-aware language model pretraining. C Rosset, C Xiong, M Phan, X Song, P Bennett, S Tiwary, arXiv:2007.006552020arXiv preprint</p>
<p>K-bert: Enabling language representation with knowledge graph. W Liu, P Zhou, Z Zhao, Z Wang, Q Ju, H Deng, P Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceApr. 202034</p>
<p>Ernie: Enhanced language representation with informative entities. Z Zhang, X Han, Z Liu, X Jiang, M Sun, Q Liu, arXiv:1905.071292019arXiv preprint</p>
<p>Cokebert: Contextual knowledge selection and embedding towards enhanced pre-trained language models. Y Su, X Han, Z Zhang, Y Lin, P Li, Z Liu, J Zhou, M Sun, AI Open. 2Jan. 2021</p>
<p>Klmo: Knowledge graph enhanced pretrained language model with fine-grained relationships. L He, S Zheng, T Yang, F Zhang, Findings of the Association for Computational Linguistics: EMNLP 2021. M.-F Moens, X Huang, L Specia, S W -T, Yih, Punta Cana, Dominican RepublicAssociation for Computational LinguisticsNov. 2021</p>
<p>K-adapter: Infusing knowledge into pretrained models with adapters. R Wang, D Tang, N Duan, Z Wei, X Huang, J Ji, G Cao, D Jiang, M Zhou, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. C Zong, F Xia, W Li, R Navigli, Association for Computational LinguisticsAug. 2021</p>
<p>Kala: knowledge-augmented language model adaptation. M Kang, J Baek, S J Hwang, arXiv:2204.105552022arXiv preprint</p>
<p>Parameter-efficient domain knowledge integration from multiple sources for biomedical pre-trained language models. Q Lu, D Dou, T H Nguyen, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Luke: Deep contextualized entity representations with entityaware self-attention. I Yamada, A Asai, H Shindo, H Takeda, Y Matsumoto, arXiv:2010.010572020arXiv preprint</p>
<p>Kilm: Knowledge injection into encoder-decoder language models. Y Xu, M Namazifar, D Hazarika, A Padmakumar, Y Liu, D Hakkani-T r, Feb. 2023</p>
<p>Exploiting structured knowledge in text via graph-guided representation learning. T Shen, Y Mao, P He, G Long, A Trischler, W Chen, arXiv:2004.142242020arXiv preprint</p>
<p>Spanbert: Improving pre-training by representing and predicting spans. M Joshi, D Chen, Y Liu, D S Weld, L Zettlemoyer, O Levy, Transactions of the association for computational linguistics. 82020</p>
<p>Jaket: Joint pre-training of knowledge graph and language understanding. D Yu, C Zhu, Y Yang, M Zeng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceJun. 202236</p>
<p>Deep bidirectional language-knowledge graph pretraining. M Yasunaga, A Bosselut, H Ren, X Zhang, C D Manning, P S Liang, J Leskovec, Advances in Neural Information Processing Systems. 202235323</p>
<p>Erica: Improving entity and relation understanding for pre-trained language models via contrastive learning. Y Qin, Y Lin, R Takanobu, Z Liu, P Li, H Ji, M Huang, M Sun, J Zhou, arXiv:2012.150222020arXiv preprint</p>
<p>Kepler: A unified model for knowledge embedding and pretrained language representation. X Wang, T Gao, Z Zhu, Z Zhang, Z Liu, J Li, J Tang, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Ernie 2.0: A continual pre-training framework for language understanding. Y Sun, S Wang, Y Li, S Feng, H Tian, H Wu, H Wang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. J Baek, A F Aji, A Saffari, arXiv:2306.041362023arXiv preprint</p>
<p>Retrieve-rewrite-answer: A kg-to-text enhanced llms framework for knowledge graph question answering. Y Wu, N Hu, S Bi, G Qi, J Ren, A Xie, W Song, Sep. 2023</p>
<p>Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. Y Wen, Z Wang, J Sun, Sep. 2023</p>
<p>Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs. C Feng, X Zhang, Z Fei, Sep. 2023</p>
<p>Minimizing factual inconsistency and hallucination in large language models. S Saxena, S Prasad, M Prakash, A Shankar, V Vaddina, S Gopalakrishnan, arXiv:2311.138782023arXiv preprint</p>
<p>Chatgraph: Interpretable text classification by converting chatgpt knowledge to graphs. Y Shi, H Ma, W Zhong, G Mai, X Li, T Liu, J Huang, arXiv:2305.035132023arXiv preprint</p>
<p>Xplainllm: A qa explanation dataset for understanding llm decision-making. Z Chen, J Chen, M Gaidhani, A Singh, M Sra, arXiv:2311.086142023arXiv preprint</p>
<p>Deep modeling of social relations for recommendation. W Fan, Q Li, M Cheng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>Deep adversarial social recommendation. W Fan, T Derr, Y Ma, J Wang, J Tang, Q Li, 28th International Joint Conference on Artificial Intelligence. 2019IJCAI-19</p>
<p>Deep social collaborative filtering. W Fan, Y Ma, D Yin, J Wang, J Tang, Q Li, Proceedings of the 13th ACM Conference on Recommender Systems. the 13th ACM Conference on Recommender Systems2019</p>
<p>Heterogeneous knowledge fusion: A novel approach for personalized recommendation via llm. B Yin, J Xie, Y Qin, Z Ding, Z Feng, X Li, W Lin, Proceedings of the 17th ACM Conference on Recommender Systems. the 17th ACM Conference on Recommender Systems2023</p>
<p>One model for all: Large language models are domain-agnostic recommendation systems. Z Tang, Z Huan, Z Li, X Zhang, J Hu, C Fu, J Zhou, C Li, Oct. 2023</p>
<p>Uncovering chatgpt's capabilities in recommender systems. S Dai, N Shao, H Zhao, W Yu, Z Si, C Xu, Z Sun, X Zhang, J Xu, arXiv:2305.021822023arXiv preprint</p>
<p>Rethinking large language model architectures for sequential recommendations. H Wang, X Liu, W Fan, X Zhao, V Kini, D Yadav, F Wang, Z Wen, J Tang, H Liu, arXiv:2402.095432024arXiv preprint</p>
<p>Towards open-world recommendation with knowledge augmentation from large language models. Y Xi, W Liu, J Lin, J Zhu, B Chen, R Tang, W Zhang, R Zhang, Y Yu, arXiv:2306.109332023arXiv preprint</p>
<p>Leveraging large language models (llms) to empower trainingfree dataset condensation for content-based recommendation. J Wu, Q Liu, H Hu, W Fan, S Liu, Q Li, X.-M Wu, K Tang, arXiv:2310.098742023arXiv preprint</p>
<p>Is chatgpt a good recommender? a preliminary study. J Liu, C Liu, R Lv, K Zhou, Y Zhang, arXiv:2304.101492023arXiv preprint</p>
<p>Tallrec: An effective and efficient tuning framework to align large language model with recommendation. K Bao, J Zhang, Y Zhang, W Wang, F Feng, X He, arXiv:2305.004472023arXiv preprint</p>
<p>Up5: Unbiased foundation model for fairness-aware recommendation. W Hua, Y Ge, S Xu, J Ji, Y Zhang, arXiv:2305.120902023arXiv preprint</p>
<p>Foundation models for recommender systems: A survey and new perspectives. C Huang, T Yu, K Xie, S Zhang, L Yao, J Mcauley, arXiv:2402.111432024arXiv preprint</p>
<p>Cp-kgc: Constrained-prompt knowledge graph completion with large language models. R Yang, L Fang, Y Zhou, Oct. 2023</p>
<p>Exploring large language models for knowledge graph completion. L Yao, J Peng, C Mao, Y Luo, Sep. 2023</p>
<p>Kg-gpt: A general framework for reasoning on knowledge graphs using large language models. J Kim, Y Kwon, Y Jo, E Choi, Oct. 2023</p>
<p>Chatkbqa: A generate-thenretrieve framework for knowledge base question answering with fine-tuned large language models. H Luo, H E , Z Tang, S Peng, Y Guo, W Zhang, C Ma, G Dong, M Song, W Lin, Oct. 2023</p>
<p>J Jiang, K Zhou, Z Dong, K Ye, W X Zhao, J.-R Wen, arXiv:2305.09645Structgpt: A general framework for large language model to reason over structured data. 2023arXiv preprint</p>
<p>Chatrule: Mining logical rules with large language models for knowledge graph reasoning. L Luo, J Ju, B Xiong, Y.-F Li, G Haffari, S Pan, Sep. 2023</p>
<p>Developing a scalable benchmark for assessing large language models in knowledge graph engineering. L.-P Meyer, J Frey, K Junghanns, F Brei, K Bulert, S Gr nder-Fahrer, M Martin, Aug. 2023</p>
<p>Towards foundation models for knowledge graph reasoning. M Galkin, X Yuan, H Mostafa, J Tang, Z Zhu, arXiv:2310.045622023arXiv preprint</p>
<p>A general single-cell analysis framework via conditional diffusion generative models. W Tang, R Liu, H Wen, X Dai, J Ding, H Li, W Fan, Y Xie, J Tang, bioRxiv. 2023</p>
<p>Identifying the kind behind smiles-anatomical therapeutic chemical classification using structure-only representations. Y Cao, Z.-Q Yang, X.-L Zhang, W Fan, Y Wang, J Shen, D.-Q Wei, Q Li, X.-Y Wei, Briefings in Bioinformatics. 2353462022</p>
<p>Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery. H Cao, Z Liu, X Lu, Y Yao, Y Li, arXiv:2311.162082023arXiv preprint</p>
<p>Chemdfm: Dialogue foundation model for chemistry. Z Zhao, D Ma, L Chen, L Sun, Z Li, H Xu, Z Zhu, S Zhu, S Fan, G Shen, arXiv:2401.148182024arXiv preprint</p>
<p>Robot task planning using semantic maps. C Galindo, J.-A Fernndez-Madrigal, J Gonzlez, A Saffiotti, Robotics and autonomous systems. 56112008</p>
<p>Learning to reason over scene graphs: A case study of finetuning gpt-2 into a robot language model for grounded task planning. G Chalvatzaki, A Younes, D Nandha, A T Le, L F R Ribeiro, I Gurevych, Frontiers in Robotics and AI. 101221739Aug. 2023</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. K Rana, J Haviland, S Garg, J Abou-Chakra, I Reid, N Suenderhauf, Sep. 2023</p>
<p>Y Zhen, S Bi, L Xing-Tong, P Wei-Qin, S Hai-Peng, C Zi-Rui, F Yi-Shu, Robot task planning based on large language model representing knowledge with directed graph structures. Jun. 2023</p>
<p>Trustworthy ai: A computational perspective. H Liu, Y Wang, W Fan, X Liu, Y Li, S Jain, Y Liu, A K Jain, J Tang, arXiv:2107.066412021arXiv preprint</p>
<p>A comprehensive survey on trustworthy recommender systems. W Fan, X Zhao, X Chen, J Su, J Gao, L Wang, Q Liu, Y Wang, H Xu, L Chen, arXiv:2209.101172022arXiv preprint</p>
<p>Jointly attacking graph neural network and its explanations. W Fan, W Jin, X Liu, H Xu, X Tang, S Wang, Q Li, J Tang, J Wang, C Aggarwal, 2023 IEEE 39th International Conference on Data Engineering (ICDE). IEEE2023</p>
<p>Attacking black-box recommendations via copying cross-domain user profiles. W Fan, T Derr, X Zhao, Y Ma, H Liu, J Wang, J Tang, Q Li, 2021 IEEE 37th International Conference on Data Engineering (ICDE). IEEE2021</p>
<p>Baseline defenses for adversarial attacks against aligned language models. N Jain, A Schwarzschild, Y Wen, G Somepalli, J Kirchenbauer, P -Y. Chiang, M Goldblum, A Saha, J Geiping, T Goldstein, arXiv:2309.006142023arXiv preprint</p>
<p>Towards building a robust toxicity predictor. D Bespalov, S Bhabesh, Y Xiang, L Zhou, Y Qi, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics20235Industry Track</p>
<p>Understanding and improving graph injection attack by promoting unnoticeability. Y Chen, H Yang, Y Zhang, K Ma, T Liu, B Han, J Cheng, arXiv:2202.080572022arXiv preprint</p>
<p>Tdgia: Effective injection attacks on graph neural networks. X Zou, Q Zheng, Y Dong, X Guan, E Kharlamov, J Lu, J Tang, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining. the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining2021</p>
<p>Adversarial attack on graph structured data. H Dai, H Li, T Tian, X Huang, L Wang, J Zhu, L Song, International conference on machine learning. PMLR2018</p>
<p>Adversarial attacks on neural networks for graph data. D gner, A Akbarnejad, S nnemann, Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining2018</p>
<p>Jailbroken: How does llm safety training fail. A Wei, N Haghtalab, J Steinhardt, arXiv:2307.024832023arXiv preprint</p>
<p>Certified robustness for large language models with self-denoising. Z Zhang, G Zhang, B Hou, W Fan, Q Li, S Liu, Y Zhang, S Chang, arXiv:2307.071712023arXiv preprint</p>
<p>Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5). S Geng, S Liu, Z Fu, Y Ge, Y Zhang, Proceedings of the 16th ACM Conference on Recommender Systems. the 16th ACM Conference on Recommender Systems2022</p>
<p>Does gender matter? towards fairness in dialogue systems. H Liu, J Dacon, W Fan, H Liu, Z Liu, J Tang, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational Linguistics2020</p>
<p>Fairness reprogramming. G Zhang, Y Zhang, Y Zhang, W Fan, Q Li, S Liu, S Chang, Thirty-sixth Conference on Neural Information Processing Systems. 2022</p>
<p>Fairly adaptive negative sampling for recommendations. X Chen, W Fan, J Chen, H Liu, Z Liu, Z Zhang, Q Li, Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023</p>
<p>Measuring and reducing gendered correlations in pre-trained models. K Webster, X Wang, I Tenney, A Beutel, E Pitler, E Pavlick, J Chen, E Chi, S Petrov, arXiv:2010.060322020arXiv preprint</p>
<p>Auto-debias: Debiasing masked language models with automated biased prompts. Y Guo, Y Yang, A Abbasi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Mabel: Attenuating gender bias using textual entailment data. J He, M Xia, C Fellbaum, D Chen, arXiv:2210.149752022arXiv preprint</p>
<p>Extracting training data from large language models. N Carlini, F Tramer, E Wallace, M Jagielski, A Herbert-Voss, K Lee, A Roberts, T B Brown, D Song, U Erlingsson, USENIX Security Symposium. 20216</p>
<p>Are large pre-trained language models leaking your personal information. J Huang, H Shao, K C , -C Chang, arXiv:2205.126282022arXiv preprint</p>
<p>Graph unlearning with efficient partial retraining. J Zhang, L Wang, S Wang, W Fan, arXiv:2403.073532024arXiv preprint</p>
<p>Privacy-preserving prompt tuning for large language model services. Y Li, Z Tan, Y Liu, arXiv:2305.062122023arXiv preprint</p>
<p>Efficient large language models fine-tuning on graphs. R Xue, X Shen, R Yu, X Liu, arXiv:2312.047372023arXiv preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, arXiv:2305.143142023arXiv preprint</p>
<p>Llm-pruner: On the structural pruning of large language models. X Ma, G Fang, X Wang, arXiv:2305.116272023arXiv preprint</p>
<p>Sheared llama: Accelerating language model pre-training via structured pruning. M Xia, T Gao, Z Zeng, D Chen, arXiv:2310.066942023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
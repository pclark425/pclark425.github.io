<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6741 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6741</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6741</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-763eb8d43e2f8a5d9da26269a4985efd1c099a5b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/763eb8d43e2f8a5d9da26269a4985efd1c099a5b" target="_blank">ExpertPrompting: Instructing Large Language Models to be Distinguished Experts</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts, and produces a competitive open-source chat assistant called ExpertLLaMA, which outperforms existing open-source opponents and achieves 96\% of the original ChatGPT's capability.</p>
                <p><strong>Paper Abstract:</strong> The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at https://github.com/OFA-Sys/ExpertLLaMA.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6741.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6741.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExpertPrompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ExpertPrompting (agent-identity augmented prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic prompting strategy that uses in-context learning to generate a customized 'expert identity' for each instruction and conditions an LLM's answer on that identity to elicit more expert-like, detailed responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Expert identity prompting (ExpertPrompting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>agent‑based / in‑context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Alpaca (52k) data; GPT-4 500-sample pairwise preference eval; Vicuna80 (for model eval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General instruction-following across diverse categories (knowledge, math, Fermi, counterfactual, roleplay, coding, writing, common-sense); data-quality evaluation via GPT-4 pairwise preference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>GPT-4 pairwise preference rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>48.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Vanilla Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>25.5</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ExpertPrompting automatically generates a specialized, detailed identity per instruction (k=3 exemplars used) and conditions gpt-3.5 on it. Answers are substantially longer on average (138.30 words vs 108.44 vanilla) and were preferred by GPT-4 (48.5% for expert answers vs 23% for vanilla). Authors attribute improved quality to customization and informational richness of the identity; no formal significance tests reported. No ablation isolating identity-generation components was presented.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpertPrompting: Instructing Large Language Models to be Distinguished Experts', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6741.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6741.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExpertLLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ExpertLLaMA (chat assistant trained on ExpertPrompting data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chat assistant based on LLaMA 7B fine-tuned on instruction-following data generated using ExpertPrompting (GPT-3.5 outputs), intended to produce more expert-like responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ExpertLLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Instruction‑tuning on ExpertPrompting‑generated data</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>fine‑tuned instruction‑following (agent‑augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Vicuna80</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Unseen instruction-following evaluation across diverse categories (Vicuna80 synthesized by GPT‑4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relative score normalized to ChatGPT = 100%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>96.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-4.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ExpertLLaMA, trained on ExpertPrompting data, outperforms the authors' reproductions of other open-source assistants (Alpaca, Vicuna, LLaMA-GPT3.5, etc.) in GPT‑4-based evaluation and achieves roughly 96% of ChatGPT's score per their normalization; authors note this as promising but state it needs more rigorous validation. No statistical tests or ablations isolating training-data effects reported.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpertPrompting: Instructing Large Language Models to be Distinguished Experts', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6741.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6741.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static DESC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static Description Prompting (fixed expert description)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline prompting strategy that prepends a fixed, generic 'you are an expert' description to all prompts to encourage professional answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Static expert description prompt</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>prompt‑augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Alpaca (52k) data (compared on average answer length and used as baseline in GPT-4 eval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Instruction-following with a fixed expert-like preface added to all prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average answer length (words)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>108.67</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Vanilla Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>0.23</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Static DESCRIPTION produced negligible change in average answer length compared to vanilla prompting (108.67 vs 108.44 words), whereas ExpertPrompting produced substantially longer answers; authors use this baseline to argue that simple generic expert phrasing is insufficient compared to tailored expert identities.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpertPrompting: Instructing Large Language Models to be Distinguished Experts', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6741.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6741.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-Context Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Learning (few-shot exemplar-based prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing a small number (k) of exemplar instruction–expert‑identity pairs in the prompt so the LLM generates tailored expert identity descriptions for new instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>In‑Context Learning (few‑shot exemplar generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>few‑shot / in‑context</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a customized expert identity description per instruction using k=3 exemplars; these identities are then used to condition answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors manually wrote k=3 exemplar expert identities and found gpt-3.5 capable of producing satisfying, instruction‑specific identities automatically; this step is central to ExpertPrompting but no isolated quantitative ablation of k or exemplar choice is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpertPrompting: Instructing Large Language Models to be Distinguished Experts', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6741.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6741.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-based automatic evaluation (pairwise preference scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated evaluation protocol using GPT-4 to rate two assistant responses per instruction on helpfulness, relevance, accuracy, and detail; outputs scores and explanations and was used to compare data quality and model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Automated pairwise preference evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>automated evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>500-sample subset of the 52k Alpaca instructions (for data eval); Vicuna80 (for model eval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Pairwise comparison of two assistant answers to the same instruction, producing preference rates and rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Preference percentage; normalized scores to ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>The authors use this GPT-4 based template (appendix) for both data and model evaluation, randomizing answer order to reduce bias. The key reported numbers (e.g., ExpertPrompting preferred 48.5% vs vanilla 23%) come from this evaluation protocol; no formal significance analysis accompanies the preference percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpertPrompting: Instructing Large Language Models to be Distinguished Experts', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6741.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6741.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed method that allows models to interleave chain-of-thought style reasoning ('thoughts') with external actions (e.g., tool calls), enabling iterative externalization of internal reasoning before final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / action‑augmented</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Complex reasoning tasks where iterative thought/action improves performance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Cited in related work as an approach that externalizes thoughts and iterates, improving downstream performance on certain complex reasoning tasks; authors contrast these iterative/thought-externalization methods as being more specialized to complex tasks, whereas ExpertPrompting aims to be general-purpose and automatic.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpertPrompting: Instructing Large Language Models to be Distinguished Experts', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6741.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6741.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna80</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna80 (GPT-4-synthesized test set)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unseen testset synthesized by GPT-4 consisting of 80 questions spanning multiple categories (knowledge, math, Fermi, counterfactual, roleplay, generic, coding, writing, common-sense), used for model evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>benchmark dataset (Vicuna80)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Vicuna80</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diverse instruction-following across multiple categories synthesized by GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized score relative to ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Used as the unseen test set to compare ExpertLLaMA and other chat assistants; authors present normalized comparisons (ChatGPT=100%) and win/tie/loss counts but no detailed per-task breakdown in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExpertPrompting: Instructing Large Language Models to be Distinguished Experts', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Vicuna: An opensource chatbot impressing gpt-4 with 90% * chatgpt quality <em>(Rating: 2)</em></li>
                <li>Stanford alpaca: An instruction-following llama model <em>(Rating: 2)</em></li>
                <li>Self-instruct: Aligning language model with self generated instructions <em>(Rating: 2)</em></li>
                <li>Llama: Open and efficient foundation language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6741",
    "paper_id": "paper-763eb8d43e2f8a5d9da26269a4985efd1c099a5b",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "ExpertPrompting",
            "name_full": "ExpertPrompting (agent-identity augmented prompting)",
            "brief_description": "An automatic prompting strategy that uses in-context learning to generate a customized 'expert identity' for each instruction and conditions an LLM's answer on that identity to elicit more expert-like, detailed responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "reasoning_method_name": "Expert identity prompting (ExpertPrompting)",
            "reasoning_method_type": "agent‑based / in‑context prompting",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Alpaca (52k) data; GPT-4 500-sample pairwise preference eval; Vicuna80 (for model eval)",
            "task_description": "General instruction-following across diverse categories (knowledge, math, Fermi, counterfactual, roleplay, coding, writing, common-sense); data-quality evaluation via GPT-4 pairwise preference.",
            "performance_metric": "GPT-4 pairwise preference rate (%)",
            "performance_value": 48.5,
            "comparison_target_method": "Vanilla Prompting",
            "performance_difference": 25.5,
            "statistical_significance": false,
            "analysis_notes": "ExpertPrompting automatically generates a specialized, detailed identity per instruction (k=3 exemplars used) and conditions gpt-3.5 on it. Answers are substantially longer on average (138.30 words vs 108.44 vanilla) and were preferred by GPT-4 (48.5% for expert answers vs 23% for vanilla). Authors attribute improved quality to customization and informational richness of the identity; no formal significance tests reported. No ablation isolating identity-generation components was presented.",
            "ablation_study_present": false,
            "uuid": "e6741.0",
            "source_info": {
                "paper_title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ExpertLLaMA",
            "name_full": "ExpertLLaMA (chat assistant trained on ExpertPrompting data)",
            "brief_description": "A chat assistant based on LLaMA 7B fine-tuned on instruction-following data generated using ExpertPrompting (GPT-3.5 outputs), intended to produce more expert-like responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ExpertLLaMA",
            "model_size": "7B",
            "reasoning_method_name": "Instruction‑tuning on ExpertPrompting‑generated data",
            "reasoning_method_type": "fine‑tuned instruction‑following (agent‑augmented)",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Vicuna80",
            "task_description": "Unseen instruction-following evaluation across diverse categories (Vicuna80 synthesized by GPT‑4).",
            "performance_metric": "Relative score normalized to ChatGPT = 100%",
            "performance_value": 96.0,
            "comparison_target_method": "ChatGPT",
            "performance_difference": -4.0,
            "statistical_significance": false,
            "analysis_notes": "ExpertLLaMA, trained on ExpertPrompting data, outperforms the authors' reproductions of other open-source assistants (Alpaca, Vicuna, LLaMA-GPT3.5, etc.) in GPT‑4-based evaluation and achieves roughly 96% of ChatGPT's score per their normalization; authors note this as promising but state it needs more rigorous validation. No statistical tests or ablations isolating training-data effects reported.",
            "ablation_study_present": false,
            "uuid": "e6741.1",
            "source_info": {
                "paper_title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Static DESC",
            "name_full": "Static Description Prompting (fixed expert description)",
            "brief_description": "A baseline prompting strategy that prepends a fixed, generic 'you are an expert' description to all prompts to encourage professional answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "reasoning_method_name": "Static expert description prompt",
            "reasoning_method_type": "prompt‑augmentation",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "Alpaca (52k) data (compared on average answer length and used as baseline in GPT-4 eval)",
            "task_description": "Instruction-following with a fixed expert-like preface added to all prompts.",
            "performance_metric": "Average answer length (words)",
            "performance_value": 108.67,
            "comparison_target_method": "Vanilla Prompting",
            "performance_difference": 0.23,
            "statistical_significance": false,
            "analysis_notes": "Static DESCRIPTION produced negligible change in average answer length compared to vanilla prompting (108.67 vs 108.44 words), whereas ExpertPrompting produced substantially longer answers; authors use this baseline to argue that simple generic expert phrasing is insufficient compared to tailored expert identities.",
            "ablation_study_present": false,
            "uuid": "e6741.2",
            "source_info": {
                "paper_title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "In-Context Learning",
            "name_full": "In-Context Learning (few-shot exemplar-based prompting)",
            "brief_description": "Providing a small number (k) of exemplar instruction–expert‑identity pairs in the prompt so the LLM generates tailored expert identity descriptions for new instructions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "reasoning_method_name": "In‑Context Learning (few‑shot exemplar generation)",
            "reasoning_method_type": "few‑shot / in‑context",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": null,
            "task_description": "Generate a customized expert identity description per instruction using k=3 exemplars; these identities are then used to condition answers.",
            "performance_metric": null,
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Authors manually wrote k=3 exemplar expert identities and found gpt-3.5 capable of producing satisfying, instruction‑specific identities automatically; this step is central to ExpertPrompting but no isolated quantitative ablation of k or exemplar choice is reported.",
            "ablation_study_present": false,
            "uuid": "e6741.3",
            "source_info": {
                "paper_title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 eval",
            "name_full": "GPT-4-based automatic evaluation (pairwise preference scoring)",
            "brief_description": "An automated evaluation protocol using GPT-4 to rate two assistant responses per instruction on helpfulness, relevance, accuracy, and detail; outputs scores and explanations and was used to compare data quality and model outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4",
            "model_size": null,
            "reasoning_method_name": "Automated pairwise preference evaluation",
            "reasoning_method_type": "automated evaluation",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "500-sample subset of the 52k Alpaca instructions (for data eval); Vicuna80 (for model eval)",
            "task_description": "Pairwise comparison of two assistant answers to the same instruction, producing preference rates and rationales.",
            "performance_metric": "Preference percentage; normalized scores to ChatGPT",
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": false,
            "analysis_notes": "The authors use this GPT-4 based template (appendix) for both data and model evaluation, randomizing answer order to reduce bias. The key reported numbers (e.g., ExpertPrompting preferred 48.5% vs vanilla 23%) come from this evaluation protocol; no formal significance analysis accompanies the preference percentages.",
            "ablation_study_present": false,
            "uuid": "e6741.4",
            "source_info": {
                "paper_title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "A recently proposed method that allows models to interleave chain-of-thought style reasoning ('thoughts') with external actions (e.g., tool calls), enabling iterative externalization of internal reasoning before final answers.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "reasoning_method_name": "ReAct",
            "reasoning_method_type": "sequential / action‑augmented",
            "reasoning_style_diversity": "single style",
            "benchmark_name": null,
            "task_description": "Complex reasoning tasks where iterative thought/action improves performance",
            "performance_metric": null,
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Cited in related work as an approach that externalizes thoughts and iterates, improving downstream performance on certain complex reasoning tasks; authors contrast these iterative/thought-externalization methods as being more specialized to complex tasks, whereas ExpertPrompting aims to be general-purpose and automatic.",
            "ablation_study_present": null,
            "uuid": "e6741.5",
            "source_info": {
                "paper_title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Vicuna80",
            "name_full": "Vicuna80 (GPT-4-synthesized test set)",
            "brief_description": "An unseen testset synthesized by GPT-4 consisting of 80 questions spanning multiple categories (knowledge, math, Fermi, counterfactual, roleplay, generic, coding, writing, common-sense), used for model evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "reasoning_method_name": "benchmark dataset (Vicuna80)",
            "reasoning_method_type": "benchmark",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Vicuna80",
            "task_description": "Diverse instruction-following across multiple categories synthesized by GPT-4.",
            "performance_metric": "Normalized score relative to ChatGPT",
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Used as the unseen test set to compare ExpertLLaMA and other chat assistants; authors present normalized comparisons (ChatGPT=100%) and win/tie/loss counts but no detailed per-task breakdown in the main text.",
            "ablation_study_present": false,
            "uuid": "e6741.6",
            "source_info": {
                "paper_title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Vicuna: An opensource chatbot impressing gpt-4 with 90% * chatgpt quality",
            "rating": 2
        },
        {
            "paper_title": "Stanford alpaca: An instruction-following llama model",
            "rating": 2
        },
        {
            "paper_title": "Self-instruct: Aligning language model with self generated instructions",
            "rating": 2
        },
        {
            "paper_title": "Llama: Open and efficient foundation language models",
            "rating": 1
        }
    ],
    "cost": 0.0160785,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ExpertPrompting: Instructing Large Language Models to be Distinguished Experts</h1>
<p>Benfeng Xu ${ }^{1}$, An Yang ${ }^{2}$, Junyang Lin ${ }^{2}$, Quan Wang ${ }^{3}$, Chang Zhou ${ }^{2}$, Yongdong Zhang ${ }^{1}$ and Zhendong Mao ${ }^{1 *}$<br>${ }^{1}$ University of Science and Technology of China<br>${ }^{2}$ Alibaba DAMO Academy, ${ }^{3}$ Beijing University of Posts and Telecommunications benfeng@mail.ustc.edu.cn, zdmao@ustc.edu.cn</p>
<h4>Abstract</h4>
<p>The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves $96 \%$ of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at https:// github.com/OFA-Sys/ExpertLLaMA.</p>
<h2>1 Introduction</h2>
<p>Large language models, when trained on highquality instructing-following data, can be effectively aligned with human intents and serve as potent tools in a wide range of language tasks (Ouyang et al., 2022; Bai et al., 2022). Many successful models have demonstrated impressive ability to respond to a diverse array of generalized instructions and are still evolving rapidly. Nevertheless, the quality of the output as well as the satisfaction of users are sometimes subjected to the art of prompting. The same communicative intent could receive either a comprehensive, detailed response or a less helpful one, depending solely on the way of crafting the prompt.</p>
<p>Many recent works have put great efforts to pursue an improved solution for interacting with LLMs</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ExpertPrompting framework.
like ChatGPT. One line of work (Yao et al., 2023; Shinn et al., 2023) proposes sophisticated formulation to allow the model to iterate and externalize thoughts before giving the final answer, and observes improved performance in a series of downstream language tasks. However, they are mostly suited for only a handful of tasks that require complex reasoning. Fulford and Ng (2023) initiate an online course that provides several general principles for crafting prompts, such as writing clear and specific instructions or giving the model time to "think". There are also resources of case-based prompts (Akın and Contributors, 2023) that are already proven useful and are shared as an opensource prompt collection. However, these solutions are either not directly executable or restricted by their use case, thus requiring further development and adaptation in actual practice.</p>
<p>In the meantime, very recent explorations (Park et al., 2023; Li et al., 2023) have found LLMs entail the potential to act like an expected agent if given sufficient and detailed descriptions. Drawing inspiration from such agent-acting capability of LLMs, we propose ExpertPrompting as an augmented strategy for instructing LLMs. For each specific instruction, ExpertPrompting first envisions a distinguished expert agent that is best suited for the instruction, and then asks the LLMs to answer the instruction conditioned on such expert identity. The</p>
<p>framework is illustrated in Figure 2.</p>
<p>ExpertPrompting is an automatic prompting method. The expert identity, although specialized for each instruction, is produced with In-Context Learning <em>Brown et al. (2020); Xu et al. (2023)</em>, we only need to write several instruction-expert pair exemplars. We empirically find the generated identity satisfying. ExpertPrompting is a generalized prompting method. Each expert identity is defined at very delicate granularity using a detailed and elaborate description. It can readily match instructions in almost any domain or genre, e.g., a nutritionist to provide the advice of keeping healthy, or a physicist to explain the structure of an atom. Besides, ExpertPrompting is also simple to implement, requiring no sophisticated crafting of prompt templates or iterative processes.</p>
<p>We apply ExpertPrompting on GPT-3.5 using the prevailing 52k Alpaca instructions <em>Taori et al. (2023)</em>, which is a diverse collection of instructions produced using Self-Instruct <em>Wang et al. (2022)</em>. This procedure produces a new set of expert data where we observe improved answering quality using GPT-based evaluation <em>Chiang et al. (2023)</em>. With these high-quality instruction-following data, we also train a chat-based assistant, ExpertLLaMA, using an open LLM LLaMA <em>Touvron et al. (2023)</em>, and compare it against other assistants. ExpertLLaMA demonstrates clear advantage over Alpaca <em>Taori et al. (2023)</em> that is trained on the same set of instructions but with different answers. It also outperforms more competitive opponents including Vicuna <em>Chiang et al. (2023)</em> or LLaMA-GPT4 <em>Peng et al. (2023)</em>, albeit the latter utilizes much more powerful GPT4 as LLM. According to the detailed score, ExpertLLaMA approximately achieves $\mathbf{9 6 \%}$ of the original ChatGPT's capability.</p>
<h2>2 Method</h2>
<p>Given instruction $q$, an aligned LLM (e.g., ChatGPT, Claude) would produce an answer $a$.</p>
<p>$$
a=L L M(q)
$$</p>
<p>And ExpertPrompting first adaptively produces an identity description of a distinguished expert $e_{q}$, and then conditioned on such identity to instruct the LLM for a possibly improved response $\widetilde{a}$. We explain the procedure as follows.</p>
<h3>2.1 Expert Identity</h3>
<p>Writing expert identity is the essential component underpinning the proposed method. Generally we summarize three concerning aspects: distinguished, informative and automatic. Firstly, the description should be cutomized to each specific instruction, and the imagined expert should be specialized in the exact area with the most fitted background or experience. Secondly, the description should be detailed and comprehensive to cover all necessary information of the expert agent, so the LLM would behave as expected. And finally, the creation of all descriptions must be automatic as manually annotating is neither efficient nor practical.</p>
<p>We employ In-Context Learning <em>Brown et al. (2020); Xu et al. (2023)</em> that well satisfy these requirements. We manually revise $k$ expert identity description for sampled instructions, and simply prepend them into the prompt to ask for new descriptions:</p>
<p>$$
e_{q}=L L M\left(\left{q_{1}, e_{q_{1}}\right} \oplus \ldots \oplus\left{q_{k}, e_{q_{k}}\right} \oplus q\right)
$$</p>
<p>The descriptions are generally about the identity, profession, expertise as well as experience related to the specific instruction. Figure 2 provides a specific case to illustrate what an expert identity roughly looks like. We simply set $k=3$, and the prompt is illustrated in Appendix A, Table 6. We empirically observe that conditioned on the exemplars, LLMs like GPT-3.5 are capable of producing satisfying expert identity by themselves.</p>
<h3>2.2 Expert Prompting</h3>
<p>We now pair each expert identity $e_{q}$ with the original instruction $q$, and ask for an augmented answer:</p>
<p>$$
\hat{a}=L L M\left(\left{e_{q}, q\right}\right)
$$</p>
<p>and $\hat{a}$ is expected to be provable better than $a$. In practice, we find that LLM occasionally generates mixed content that involves the given identity, mostly at the beginning of the answer, such as:</p>
<p>As a physicist specializing in atomic structure, I can give you a description of the structure of an atom. ...</p>
<p>Such behavior, although makes sense, is not what we intended. We simply remove them in a postprocessing procedure as the patterns are recognizable. Figure 2 illustrates the effect after we augment an instruction with appropriate expert identity.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Instruction: Describe the structure of an atom.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Expert Identity: <br> You are a physicist specialized in atomic structure and the properties of matter at the atomic level. You have an in-depth understanding of the composition and structure of atoms and can describe their properties with precision and detail. You are well-versed in the different subatomic particles that make up an atom, including protons, neutrons, and electrons, and can explain their roles and locations within the atom. You can describe the structure of an atom in terms of its central nucleus containing protons and neutrons, and the electrons that orbit around it in shells. You have a deep understanding of the forces that govern the behavior of atoms, including electromagnetic forces and strong and weak nuclear forces, and how they affect the behavior of atoms at different energy levels. Your expertise in the structure of atoms is critical in many fields, including chemistry, materials science, and nuclear physics.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5 answer: <br> An atom consists of a nucleus, which is made up of positively charged protons and neutral neutrons, surrounded by negatively charged electrons that orbit the nucleus. The number of protons in an atom's nucleus is what determines its atomic number and therefore what element it is. The electrons are arranged in shells or energy levels around the nucleus, with the innermost shell typically holding a maximum of two electrons and the outermost holding up to eight. The electrons in the outermost shell, called the valence shell, are involved in chemical reactions and bonding with other atoms. The size of an atom is determined by the distance between the nucleus and the outermost electrons.</td>
<td style="text-align: center;">gpt-3.5 answer w/ expert identity: <br> The structure of an atom can be thought of as comprising of three main components: a central nucleus containing protons and neutrons, and electrons that orbit around it in distinct energy levels or shells. The nucleus is the dense, positively charged center of the atom, while the electrons are negatively charged particles that orbit around the nucleus at varying distances. The number of protons present in the nucleus determines the atomic number of the atom, which in turn defines its unique chemical properties and place in the periodic table of elements. The number of neutrons present in the nucleus can vary, giving rise to isotopes of the atom. The electrons occupy distinct energy levels or shells around the nucleus, with those closest to the nucleus having lower energy levels than those further away. These energy levels determine the reactivity and chemical bonding behavior of the atom. The overall structure of an atom is governed by the electromagnetic force that holds the negatively charged electrons in orbit around the positively charged nucleus, and the strong nuclear force that binds the protons and neutrons together in the nucleus.</td>
</tr>
</tbody>
</table>
<p>Figure 2: Effects of expert identity augmentation when prompting gpt-3.5-turbo.</p>
<p>For all prompt templates involved in the entire process, we refer to Appendix A.</p>
<h3>2.3 ExpertLLaMA</h3>
<p>We apply both standard prompting and ExpertPrompting strategy to the same instructions set adopted from Alpaca (Taori et al., 2023), where gpt-3.5-turbo is selected as LLM due to affordable expenses and state-of-the-art capability. Using the latter expert answers, we follow Alpaca and similarly trained a new chat-based assistant using the open-sourced LLM LLaMA 7B (Touvron et al., 2023). We name the resulting chat assistant ExpertLLaMA. We release the model along with the expert answers to facilitate future research.</p>
<h2>3 Evaluation</h2>
<h3>3.1 Experimental Setting</h3>
<p>Reliably evaluating the quality of instructionfollowing data is a challenging task. In our experiments, we resort to the recently proposed GPT4based automatic evaluation (Chiang et al., 2023). The template is provided in Appendix A. Besides, we randomly permute the order of two answers at each evaluation to avoid bias.</p>
<p>We evaluate both the data and the trained chat assistant. For data evaluation, we randomly sample 500 instances out of the 52 k data, and ask GPT4 to rate the expert answer ${\tilde{a}}$ against the vanilla answer ${a}$ (See Appendix A for prompt illustration). For model evaluation, we compare ExpertLLaMA trained on ${\tilde{a}}$ and LLaMA-GPT-3.5 trained on ${a}$. So the evaluation results not only conclude the model capability but also can also be recognized as a reflection of the training data quality. We also included several popular assistants known by the community as introduced later. We use Vicuna80 (Chiang et al., 2023) as unseen test set, which is synthesized by GPT4 and consists of various categories of questions including knowledge, math, Fermi, counterfactual, roleplay, generic, coding, writing, common-sense.</p>
<h3>3.2 Baselines</h3>
<p>To better analyze the effectiveness of ExpertPrompting, we introduce a baseline that augments the prompts with a fixed description:</p>
<p>Imaging you are an expert in the regarding field, try to answer the following instruction as professional as possible.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Num. of Words</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vanilla Prompting</td>
<td>108.44</td>
</tr>
<tr>
<td>Vanilla Prompting + Static DESC</td>
<td>108.67</td>
</tr>
<tr>
<td>Expert Prompting</td>
<td>138.30</td>
</tr>
</tbody>
</table>
<p>Table 1: Average answer length of different prompting strategies. Calculated with answers from GPT-3.5-Turbo for the 52k Alpaca instructions.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Comparison of answer quality (ExpertPrompting VS Vanilla Prompting). Evaluated by GPT4.</p>
<h4>{Instruction}</h4>
<p>For latter convenience, we refer to this prompting strategy as + Static DESC, the resulting answers as {a^{+}}, and the chat assistant trained with it as LLaMA-GPT3.5+.</p>
<p>To sum up, our baselines are: 1) Alpaca: Trained with answers produced using Self-Instruct with text-davinci-003. They also produced and released the 52k instructions. 2) LLaMA-GPT4: Trained with answers produced by GPT4, the instructions are the same with Alpaca. 3) LLaMA-GPT-3.5: Our implemented baseline, trained with answers produced by GPT-3.5-Turbo^{1}, i.e., {a}, using the same 52k instructions. 4) LLaMA-GPT-3.5+: Our implemented baseline, trained with answers produced by GPT-3.5-Turbo, i.e., {a^{+}}, using the same 52k instructions and Static DESC prompting strategy. 5) Vicuna: Trained from LLaMA 13B with user-shared conversations collected from ShareGPT^{2}.</p>
<p>Besides, we also include 6) ChatGPT and 7) Bard for comparison. To achieve comparable conclusion, we use the same answers released by Chi-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Comparison of popular chat assistants. Scores are aligned to ChatGPT as 100%.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Comparison of popular chat assistants. Number of win, tie and losses are counted.</p>
<p>ang et al. (2023) for Vicuna, ChatGPT and Bard^{3}. While for other models, we reproduce the model using identical training recipe following Alpaca. All answers will also be released for reproducing results in this paper.</p>
<h3>3.3 Data Eval</h3>
<p>To demonstrate the effectiveness of the proposed prompting strategy, we evaluate the generated data {a} against vanilla answer {a} as well as the other baseline Static DESC {a^{+}}.</p>
<p>We first examine the length of these answers in Table 1. We find that expert answers are significantly more lengthy than vanilla answers, which potentially implies comprehensiveness and thoroughness considering that we did not explicitly ask for a longer answer or mention any word number restrictions.</p>
<p>We then randomly sample 500 instructions, and compare these answers using GPT4-based evalua-</p>
<p>^{1}Accessed at 05, May, 2023</p>
<p>^{2}https://sharegpt.com/</p>
<p>tion. Results in Figure 3 show that ExpertPrompting answers are preferred at $48.5 \%$ by the reviewer model, compare to $23 \%$ of the vanilla answer, which demonstrates clear superiority.</p>
<h3>3.4 Model Eval</h3>
<p>We evaluate the capability of ExpertLLaMA as a chat assistant on Vicuna80. We first compare all models against ChatGPT in Figure 4, then compare ExpertLLaMA to all other assistants in Figure 5. Both experiments exhibit consistent conclusions that ExpertLLaMA outperforms existing open-source chat assistants including Vicuna, LLaMA-GPT4, Alpaca, etc, while only inferior to ChatGPT. It achieves approximately $96 \%$ of the original ChatGPT capability although this conclusion needs more rigorous validation.</p>
<h2>4 Conclusion</h2>
<p>We propose ExpertPrompting and ExpertLLaMA in this paper. ExpertPrompting is an augmented prompting strategy for instructing LLMs to answer like distinguished experts. It is automatic, generalized, while still being simple to implement. We apply such prompting strategy on GPT-3.5 to produce a new set of instruction-following data, and based on it train a new open-source chat assistant ExpertLLaMA. According to GPT4-based evaluation, ExpertPrompting produces higher quality answers, and ExpertLLaMA outperforms existing open-source chat assistants, achieving $96 \%$ of the original ChatGPT's capability. In the future, we will enlarge the scale of instruction data beyond 52k Alpaca to further improve ExpertLLaMA.</p>
<h2>References</h2>
<p>Fatih Kadir Akın and Contributors. 2023. Awesome chatgpt prompts. https://github.com/f/ awesome-chatgpt-prompts.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens</p>
<p>Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \%$ * chatgpt quality.</p>
<p>Isa Fulford and Andrew Ng. 2023. Chatgpt prompt engineering for developers. Accessed on 17 May 2023.</p>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. Camel: Communicative agents for" mind" exploration of large scale language model society. arXiv preprint arXiv:2303.17760.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems.</p>
<p>Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions.</p>
<p>Benfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu, Qiaoqiao She, and Yongdong Zhang. 2023. \$k\$NN prompting: Beyond-context learning with calibrationfree nearest neighbor inference. In The Eleventh International Conference on Learning Representations.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR).</p>
<h2>A Prompt Template</h2>
<p>We list all used prompting templates in Figure 6, 7 , and 8 .</p>
<p>For each instruction, write a high-quality description about the most capable and suitable agent to answer the instruction. In second person perspective.
<a href="\{question\}">Instruction</a>: Make a list of 5 possible effects of deforestation.
[Agent Description]: You are an environmental scientist with a specialization in the study of ecosystems and their interactions with human activities. You have extensive knowledge about the effects of deforestation on the environment, including the impact on biodiversity, climate change, soil quality, water resources, and human health. Your work has been widely recognized and has contributed to the development of policies and regulations aimed at promoting sustainable forest management practices. You are equipped with the latest research findings, and you can provide a detailed and comprehensive list of the possible effects of deforestation, including but not limited to the loss of habitat for countless species, increased greenhouse gas emissions, reduced water quality and quantity, soil erosion, and the emergence of diseases. Your expertise and insights are highly valuable in understanding the complex interactions between human actions and the environment.
<a href="\{question\}">Instruction</a>: Identify a descriptive phrase for an eclipse.
[Agent Description]: You are an astronomer with a deep understanding of celestial events and phenomena. Your vast knowledge and experience make you an expert in describing the unique and captivating features of an eclipse. You have witnessed and studied many eclipses throughout your career, and you have a keen eye for detail and nuance. Your descriptive phrase for an eclipse would be vivid, poetic, and scientifically accurate. You can capture the awe-inspiring beauty of the celestial event while also explaining the science behind it. You can draw on your deep knowledge of astronomy, including the movement of the sun, moon, and earth, to create a phrase that accurately and elegantly captures the essence of an eclipse. Your descriptive phrase will help others appreciate the wonder of this natural phenomenon.
<a href="\{question\}">Instruction</a>: Identify the parts of speech in this sentence: \"The dog barked at the postman\".
[Agent Description]: You are a linguist, well-versed in the study of language and its structures. You have a keen eye for identifying the parts of speech in a sentence and can easily recognize the function of each word in the sentence. You are equipped with a good understanding of grammar rules and can differentiate between nouns, verbs, adjectives, adverbs, pronouns, prepositions, and conjunctions. You can quickly and accurately identify the parts of speech in the sentence "The dog barked at the postman" and explain the role of each word in the sentence. Your expertise in language and grammar is highly valuable in analyzing and understanding the nuances of communication.</p>
<p>[Agent Description]:</p>
<p>Figure 6: Template of In-Context Learning used for producing expert identity.</p>
<h1>{expert_identity}</h1>
<p>Now given the above identity background, please answer the following instruction:
{question}</p>
<p>Figure 7: Template of ExpertPromtping.</p>
<h2>[Question]</h2>
<p>{instruction}
[The Start of Assistant 1's Answer]
{answer_bot1}
[The End of Assistant 1's Answer]
[The Start of Assistant 2's Answer]
{answer_bot2}
[The End of Assistant 2's Answer]
[System]
We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.
Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.
Please first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.</p>
<p>Figure 8: Template for GPT4-based automatic evaluation, adapted from Chiang et al. (2023).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Corresponding author.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
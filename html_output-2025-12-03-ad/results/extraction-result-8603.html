<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8603 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8603</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8603</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-276574798</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.17017v1.pdf" target="_blank">Quantifying Logical Consistency in Transformers via Query-Key Alignment</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated impressive performance in various natural language processing tasks, yet their ability to perform multi-step logical reasoning remains an open challenge. Although Chain-of-Thought prompting has improved logical reasoning by enabling models to generate intermediate steps, it lacks mechanisms to assess the coherence of these logical transitions. In this paper, we propose a novel, lightweight evaluation strategy for logical reasoning that uses query-key alignments inside transformer attention heads. By computing a single forward pass and extracting a"QK-score"from carefully chosen heads, our method reveals latent representations that reliably separate valid from invalid inferences, offering a scalable alternative to traditional ablation-based techniques. We also provide an empirical validation on multiple logical reasoning benchmarks, demonstrating improved robustness of our evaluation method against distractors and increased reasoning depth. The experiments were conducted on a diverse set of models, ranging from 1.5B to 70B parameters.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8603.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8603.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen 2.5 (as reported in paper Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frozen, pretrained instruct-style transformer LLM evaluated in this work; used to test QK-score head selection on multiple logical reasoning benchmarks (PrOntoQA-OOD, PARARULE Plus, Extended-Multi-LogiEval).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based, frozen pretrained instruct/chat-tuned language model used by the authors; evaluated without fine-tuning (instruct-style usage). The paper only reports it among a set of models ranging from 1.5B to 70B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈2.5B (reported as Qwen2.5 in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>PrOntoQA-OOD; PARARULE Plus; Extended-Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>PrOntoQA-OOD: synthetic first-order logic chain-of-thought QA over abstract categories, evaluated by true/false; PARARULE Plus: true/false questions with fixed-depth deductive reasoning; Extended-Multi-LogiEval: expanded Multi-LogiEval first-order logical schemes (balanced yes/no). Tasks require strict rule-based deductive reasoning (Modus Ponens, MT, HS, DS, dilemmas, quantifier rules, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>QK-score: compute per-head dot product between the query vector for the candidate answer token (true/false) and the key vector of the statement token to score logical validity; select high-scoring heads on a calibration set and use a single forward pass to evaluate examples. Baseline comparison: model's final token probability (select higher-probability answer).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to outperform baseline in-domain: Table 1 and text show selected heads for Qwen-2.5 achieve accuracies often in the 0.60–0.80 range on PARARULE Plus (examples in Table 1), while baseline is ≈0.62 in comparable columns; in-domain PrOntoQA-OOD results (BEST HEAD) consistently exceed baseline across reasoning depths and distractor counts. Exact per-depth numbers are in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>QK-score (BEST HEAD) consistently outperforms the Baseline (final-layer probability selection) across reasoning depths and under distractors for in-domain PrOntoQA-OOD; cross-dataset: heads selected on PrOntoQA-OOD yield >10% improvement over baseline on PARARULE Plus for several heads (paper reports three of five selected heads exceed baseline by >10%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires calibration dataset (~600 samples used, ~300/300 true/false calibration in setups); head selection may produce reversed sign (some heads distinguish correctness in a reversed manner on other datasets); generalization to all logical rules is not guaranteed (head scope may be limited).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>QK-scores reveal latent heads that act as stable verification anchors, robust to distractors and reasoning depth; internal QK alignment sometimes yields clearer logical signals than final-layer probabilities, enabling lightweight single-pass evaluation of logical consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Logical Consistency in Transformers via Query-Key Alignment', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8603.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8603.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen 1.5B (instruct variant mentioned in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller (1.5B) frozen pretrained instruct-style transformer evaluated in experiments comparing QK-score head selection to baseline probability-based answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-1.5B (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based, frozen pretrained instruct-style LLM used in experiments; evaluated zero-shot with single-word true/false answers. Listed among models ranging 1.5B–70B.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>PrOntoQA-OOD; PARARULE Plus; Extended-Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same set of strict deductive reasoning benchmarks as above, requiring first-order and propositional inference across multiple hops and rules.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>QK-score head extraction and selection on calibration set; Baseline selects highest-probability output token.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that BEST HEAD selected on calibration outperforms baseline across depths and distractors for in-domain PrOntoQA-OOD; cross-dataset transfer shows mixed improvement — some heads exceed baseline on PARARULE Plus and fewer on Extended-Multi-LogiEval (paper reports dataset-level comparisons in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>BEST HEAD vs Baseline: consistent in-domain improvements; cross-dataset improvements for several selected heads (three of five heads >10% on PARARULE Plus as aggregated result in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same limitations: calibration data required; head scope may be limited to certain deduction-rule subsets; some heads invert signal on other datasets (reversed correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Even smaller instruct models (1.5B) contain attention heads with latent alignment signals useful for verifying logical transitions; QK-score is effective in identifying such heads without model modification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Logical Consistency in Transformers via Query-Key Alignment', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8603.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8603.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distill-Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distill-Qwen-7B (reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distilled 7B Qwen-model variant evaluated with QK-score head selection and baseline comparisons on logical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Distill-Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Distilled transformer LLM (7B) used frozen; evaluated zero-shot on true/false logical tasks. Appears in cross-dataset Table 1 comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>PrOntoQA-OOD; PARARULE Plus; Extended-Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Structured logical deduction tasks spanning single- and multi-step inference, including Modus Ponens and composed deduction rules; binary true/false outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Same QK-score per-head evaluation and head selection on calibration; Baseline: final token probability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 1 includes Distill-Qwen-7B rows showing per-head accuracies; selected heads sometimes substantially outperform baseline (example values in table include head accuracies in 0.60–0.75 on PARARULE Plus vs baseline ≈0.50–0.51 in some columns). Overall, some selected heads generalize across datasets while others do not.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Selected QK-score heads frequently beat the baseline for in-domain tasks; cross-dataset gains reported on PARARULE Plus and Mixed results on Extended-Multi-LogiEval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Observed head-specific failure modes (some heads produce reversed signals); dependence on balanced calibration and potential dataset-format mismatch reduces transfer success.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Distilled mid-sized models retain heads that capture logical alignment; QK-scoring can reveal and exploit these for improved logical-consistency judgments compared to final-layer probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Logical Consistency in Transformers via Query-Key Alignment', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8603.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8603.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1 (model variant reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model variant evaluated in the paper; used to demonstrate variability in head behavior across models and datasets (including strongly reversed-behavior head in one case).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frozen pretrained transformer LLM (exact architecture/size not explicitly specified in main text); used in experiments for PrOntoQA-OOD, PARARULE Plus and Extended-Multi-LogiEval evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified (within the paper's reported 1.5B–70B range)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>PrOntoQA-OOD; PARARULE Plus; Extended-Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Strict logical deduction benchmarks requiring application of formal inference rules and multi-step proofs with distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>QK-score per-head evaluation and selection; baseline final-token probability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper notes that some heads from DeepSeek-R1 show strong performance when selected via QK-score; however, a specific head (22,16) consistently yields accuracy below 0.35 on PARARULE Plus, indicating reversed classification (QK-score distinguishes but with inverted sign). Other DeepSeek-R1 heads in Table 1 show accuracies >0.60 in some columns.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mixed: while some heads outperform baseline substantially on PARARULE Plus, the reversed-sign failure for certain heads highlights potential mismatch when transferring head selection across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Exhibited a concrete failure case where a selected head’s QK-score correlates with correctness but with inverted sign (head (22,16) on PARARULE Plus). This underscores dataset-format dependence and the need for careful calibration and possibly per-dataset sign correction.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Model-specific head behaviors can vary widely; QK-score can identify discriminative heads but sign/interpretation must be validated per dataset and per head to avoid misclassification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Logical Consistency in Transformers via Query-Key Alignment', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8603.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8603.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA3 (70B instruct variant reported in tables)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large (70B) transformer LLM variant evaluated with QK-score and baseline; reported in aggregate performance tables comparing QK head scoring vs baseline across many inference rules and depths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3 (70B, instruct-like evaluations reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based language model (70B parameters) evaluated in frozen instruct-style setting; reported in paper's comparative tables showing near-perfect QK-head performance on many rule types/depths.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>PrOntoQA-OOD; Extended-Multi-LogiEval (first-order logic schemes)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step strict deductive reasoning across a comprehensive set of first-order inference rules (MP, MT, HS, DS, CD, DD, BD, CT, DMT, CO, IM, EG, UI) and varying proof depths.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>QK-score head selection on calibration, used to pick BEST HEAD and evaluate on held-out sets; Baseline is final-layer probability answer selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Tables in the paper show very high QK accuracies for LLaMA3 QK heads (many entries at/near 0.99–1.00 for low depths and still high for larger depths) and strong baseline scores (but generally lower than QK in many entries). The paper claims QK-heads can substantially exceed baseline on multi-step inferences and are robust to distractors; exact per-rule numbers provided in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>QK-head scoring often matches or exceeds baseline for LLaMA3 70B, especially at shallow-to-moderate reasoning depths; reported QK accuracies frequently higher than the baseline across many rule types in presented tables.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper notes that head selection was done only for two sets of deduction rules and on instruct/chat-tuned variants; it does not claim these heads are solely responsible for reasoning. Calibration requirements and potential overfitting to calibration distribution remain limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large models like LLaMA3 appear to contain very clear internal QK alignment signals corresponding to logical validity; QK-scoring can surface near-perfect verification heads in such large models, showing that essential reasoning signals can exist prior to final output computations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Logical Consistency in Transformers via Query-Key Alignment', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8603.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8603.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA3.1 (70B variant reported in tables)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Another large 70B variant (LLaMA3.1) evaluated with QK-score vs baseline and included in comparative tables for many inference rules/depths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3.1 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based 70B frozen model (reported as LLaMA3.1 in tables). Evaluations mirror those for LLaMA3; QK-head accuracies reported as high across many rule types and depths.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>PrOntoQA-OOD; Extended-Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>First-order and propositional multi-step logical inference tasks described in the paper and supplemental datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Per-head QK-score selection on calibration and single-pass evaluation; baseline final-token probability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper tables report QK accuracies very high for LLaMA3.1 (many entries ≈0.99 or above for many rule/depth settings) and baseline somewhat lower on some rule types; demonstrates that QK-heads generalize strongly in the largest models shown.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>QK approach frequently outperforms baseline for LLaMA3.1 across many rule types and depths per tabulated results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same experimental caveats: selection limited to certain deduction-rule sets and instruct/chat-tuned variants; method requires calibration and may not identify all reasoning-contributing heads.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Very large models show the clearest QK alignment signals for logical validity; QK-scoring is especially effective at surfacing these in models at this scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Logical Consistency in Transformers via Query-Key Alignment', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8603.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8603.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>8B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>8B Instruct (unnamed 8B instruct-style model in tables)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B parameter instruct-style frozen model included in comparative tables showing QK vs baseline performance across rules/depths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>8B Instruct (unnamed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruct-style, frozen transformer LLM of approximately 8B parameters used in the paper's tabulated comparisons; exact family not specified in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>PrOntoQA-OOD; Extended-Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Structured deductive reasoning requiring application of first-order and propositional inference rules across varying depths.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>QK-score per-head evaluation and baseline probability comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Tabulated results show high QK accuracies for this 8B instruct model on many rule/depth cells (examples in Table 4 and 5), with QK often matching or exceeding baseline at many depths; exact numbers are in the paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>QK-head selection provides improved or comparable performance to baseline in many reported cells; aggregated claims in text state BEST HEAD outperforms baseline across depths/distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Calibration and dataset-dependence limitations apply; the paper notes it only studied instruct-and chat-tuned variants and did not explore base-to-finetuned transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Mid-sized instruct models can contain heads with useful QK alignment signals; QK-score provides a light-weight probe across scales (8B included).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Logical Consistency in Transformers via Query-Key Alignment', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8603.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8603.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>70B Instruct (unnamed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>70B Instruct (unnamed 70B instruct-style model reported in tables)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B instruct-style frozen LLM variant included in comparisons showing high performance both by baseline and QK-head measures on many logical rules and depths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>70B Instruct (unnamed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based instruct/chat-tuned model with approximately 70B parameters; evaluated frozen in zero-shot single-word answer setting across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>PrOntoQA-OOD; Extended-Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step strict logical reasoning across various first-order and propositional inference rules and depths.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>QK-score per-head selection (BEST HEAD) vs Baseline final-probability answer selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Presented tables show very strong performance for this 70B instruct variant: high QK accuracies across many rule/depth combinations and strong baseline performance as well; QK often matches or marginally exceeds baseline in many cells (exact numbers in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Both QK and baseline are strong at this scale; QK-head selection can still provide gains or clearer internal verification signals in multi-step or distractor-rich scenarios as reported in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same limitations apply: calibration needed, head selection limited to a subset of deduction rules, and experiments limited to instruct/chat-tuned variants.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>At the largest scales shown, internal QK alignment for logical validity is particularly clear; QK-scoring can be used as a lightweight diagnostic even when baseline performance is already high.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Logical Consistency in Transformers via Query-Key Alignment', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8603.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8603.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QK-score</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Query-Key Score (QK-score) evaluation method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight single-pass evaluation technique that computes per-head alignment between the query vector of the answer token and key vector of the statement token to score logical validity and select 'reasoning' heads.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>method (applied across multiple transformer LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model but a method applied to frozen transformer LLMs: for each head (l,h), compute S_QK(l,h)(c,s,a_i) = q^{(l,h)}_{a_i}^T k^{(l,h)}_s and use head(s) with best calibration performance to classify true/false logical transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>method applies to models across sizes 1.5B–70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>PrOntoQA-OOD; PARARULE Plus; Extended-Multi-LogiEval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Strict logical reasoning benchmarks requiring formal deductive steps and chain-of-thought style multi-hop inference; QK-score evaluates validity of single-step logical transitions within these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Compute QK-score per head on a calibration set (balanced true/false), select BEST HEAD(s), then evaluate classification accuracy on held-out sets; single forward pass processes all heads; contrasts with ablation or intervention methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>BEST HEAD selected via QK-score consistently outperforms baseline (final probabilities) on PrOntoQA-OOD across depths and distractor counts; cross-dataset transfer: 3/5 selected heads exceed baseline by >10% on PARARULE Plus, 2/5 beat baseline on Extended-Multi-LogiEval. Large models sometimes show near-perfect QK accuracies in tabulated results.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>QK-score often yields higher accuracy than baseline across models and datasets; the paper provides multiple table entries where QK-heads exceed baseline and reports consistent in-domain superiority of BEST HEAD.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires a sufficiently large, balanced, and debiased calibration dataset (~400+ reasoning questions; authors used 600 with 300/300 split). Head selection was only performed for two sets of deduction rules in the study; some heads may only apply to subsets of logical principles. Reversed-sign behavior of some heads in cross-dataset transfer observed (e.g., head (22,16)). Selection restricted to instruct/chat-tuned models in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>QK-score provides an efficient, scalable probe for internal logical-consistency signals in transformer heads, surfacing heads that act as 'verification anchors' robust to distractors and reasoning depth; it complements CoT and ablation studies by offering single-pass, interpretable internal metrics and can reveal reasoning signals that final outputs obscure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Logical Consistency in Transformers via Query-Key Alignment', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PrOntoQA <em>(Rating: 2)</em></li>
                <li>Testing the general deductive reasoning capacity of large language models using OOD examples <em>(Rating: 2)</em></li>
                <li>PARARULE Plus <em>(Rating: 2)</em></li>
                <li>Multi-LogiEval: Towards evaluating multi-step logical reasoning ability of large language models <em>(Rating: 2)</em></li>
                <li>LogicBench: Towards systematic evaluation of logical reasoning ability of large language models <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of syllogistic reasoning in auto-regressive language models <em>(Rating: 1)</em></li>
                <li>Chain-of-Thought and related prompting work (Wei et al., Kojima et al.) <em>(Rating: 1)</em></li>
                <li>FOLIO: Natural language reasoning with first-order logic <em>(Rating: 1)</em></li>
                <li>Enhancing reasoning capabilities of LLMs via principled synthetic logic corpus (Morishita et al.) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8603",
    "paper_id": "paper-276574798",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "Qwen-2.5",
            "name_full": "Qwen 2.5 (as reported in paper Table 1)",
            "brief_description": "A frozen, pretrained instruct-style transformer LLM evaluated in this work; used to test QK-score head selection on multiple logical reasoning benchmarks (PrOntoQA-OOD, PARARULE Plus, Extended-Multi-LogiEval).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5",
            "model_description": "Transformer-based, frozen pretrained instruct/chat-tuned language model used by the authors; evaluated without fine-tuning (instruct-style usage). The paper only reports it among a set of models ranging from 1.5B to 70B parameters.",
            "model_size": "≈2.5B (reported as Qwen2.5 in Table 1)",
            "reasoning_task_name": "PrOntoQA-OOD; PARARULE Plus; Extended-Multi-LogiEval",
            "reasoning_task_description": "PrOntoQA-OOD: synthetic first-order logic chain-of-thought QA over abstract categories, evaluated by true/false; PARARULE Plus: true/false questions with fixed-depth deductive reasoning; Extended-Multi-LogiEval: expanded Multi-LogiEval first-order logical schemes (balanced yes/no). Tasks require strict rule-based deductive reasoning (Modus Ponens, MT, HS, DS, dilemmas, quantifier rules, etc.).",
            "method_or_approach": "QK-score: compute per-head dot product between the query vector for the candidate answer token (true/false) and the key vector of the statement token to score logical validity; select high-scoring heads on a calibration set and use a single forward pass to evaluate examples. Baseline comparison: model's final token probability (select higher-probability answer).",
            "performance": "Reported to outperform baseline in-domain: Table 1 and text show selected heads for Qwen-2.5 achieve accuracies often in the 0.60–0.80 range on PARARULE Plus (examples in Table 1), while baseline is ≈0.62 in comparable columns; in-domain PrOntoQA-OOD results (BEST HEAD) consistently exceed baseline across reasoning depths and distractor counts. Exact per-depth numbers are in paper tables.",
            "baseline_comparison": "QK-score (BEST HEAD) consistently outperforms the Baseline (final-layer probability selection) across reasoning depths and under distractors for in-domain PrOntoQA-OOD; cross-dataset: heads selected on PrOntoQA-OOD yield &gt;10% improvement over baseline on PARARULE Plus for several heads (paper reports three of five selected heads exceed baseline by &gt;10%).",
            "limitations_or_failures": "Requires calibration dataset (~600 samples used, ~300/300 true/false calibration in setups); head selection may produce reversed sign (some heads distinguish correctness in a reversed manner on other datasets); generalization to all logical rules is not guaranteed (head scope may be limited).",
            "insights_or_conclusions": "QK-scores reveal latent heads that act as stable verification anchors, robust to distractors and reasoning depth; internal QK alignment sometimes yields clearer logical signals than final-layer probabilities, enabling lightweight single-pass evaluation of logical consistency.",
            "uuid": "e8603.0",
            "source_info": {
                "paper_title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Qwen-1.5B",
            "name_full": "Qwen 1.5B (instruct variant mentioned in Table 1)",
            "brief_description": "A smaller (1.5B) frozen pretrained instruct-style transformer evaluated in experiments comparing QK-score head selection to baseline probability-based answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-1.5B (instruct)",
            "model_description": "Transformer-based, frozen pretrained instruct-style LLM used in experiments; evaluated zero-shot with single-word true/false answers. Listed among models ranging 1.5B–70B.",
            "model_size": "1.5B",
            "reasoning_task_name": "PrOntoQA-OOD; PARARULE Plus; Extended-Multi-LogiEval",
            "reasoning_task_description": "Same set of strict deductive reasoning benchmarks as above, requiring first-order and propositional inference across multiple hops and rules.",
            "method_or_approach": "QK-score head extraction and selection on calibration set; Baseline selects highest-probability output token.",
            "performance": "Paper reports that BEST HEAD selected on calibration outperforms baseline across depths and distractors for in-domain PrOntoQA-OOD; cross-dataset transfer shows mixed improvement — some heads exceed baseline on PARARULE Plus and fewer on Extended-Multi-LogiEval (paper reports dataset-level comparisons in Table 1).",
            "baseline_comparison": "BEST HEAD vs Baseline: consistent in-domain improvements; cross-dataset improvements for several selected heads (three of five heads &gt;10% on PARARULE Plus as aggregated result in paper).",
            "limitations_or_failures": "Same limitations: calibration data required; head scope may be limited to certain deduction-rule subsets; some heads invert signal on other datasets (reversed correctness).",
            "insights_or_conclusions": "Even smaller instruct models (1.5B) contain attention heads with latent alignment signals useful for verifying logical transitions; QK-score is effective in identifying such heads without model modification.",
            "uuid": "e8603.1",
            "source_info": {
                "paper_title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Distill-Qwen-7B",
            "name_full": "Distill-Qwen-7B (reported in Table 1)",
            "brief_description": "A distilled 7B Qwen-model variant evaluated with QK-score head selection and baseline comparisons on logical benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Distill-Qwen-7B",
            "model_description": "Distilled transformer LLM (7B) used frozen; evaluated zero-shot on true/false logical tasks. Appears in cross-dataset Table 1 comparisons.",
            "model_size": "7B",
            "reasoning_task_name": "PrOntoQA-OOD; PARARULE Plus; Extended-Multi-LogiEval",
            "reasoning_task_description": "Structured logical deduction tasks spanning single- and multi-step inference, including Modus Ponens and composed deduction rules; binary true/false outputs.",
            "method_or_approach": "Same QK-score per-head evaluation and head selection on calibration; Baseline: final token probability.",
            "performance": "Table 1 includes Distill-Qwen-7B rows showing per-head accuracies; selected heads sometimes substantially outperform baseline (example values in table include head accuracies in 0.60–0.75 on PARARULE Plus vs baseline ≈0.50–0.51 in some columns). Overall, some selected heads generalize across datasets while others do not.",
            "baseline_comparison": "Selected QK-score heads frequently beat the baseline for in-domain tasks; cross-dataset gains reported on PARARULE Plus and Mixed results on Extended-Multi-LogiEval.",
            "limitations_or_failures": "Observed head-specific failure modes (some heads produce reversed signals); dependence on balanced calibration and potential dataset-format mismatch reduces transfer success.",
            "insights_or_conclusions": "Distilled mid-sized models retain heads that capture logical alignment; QK-scoring can reveal and exploit these for improved logical-consistency judgments compared to final-layer probabilities.",
            "uuid": "e8603.2",
            "source_info": {
                "paper_title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1 (model variant reported in Table 1)",
            "brief_description": "A model variant evaluated in the paper; used to demonstrate variability in head behavior across models and datasets (including strongly reversed-behavior head in one case).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1",
            "model_description": "Frozen pretrained transformer LLM (exact architecture/size not explicitly specified in main text); used in experiments for PrOntoQA-OOD, PARARULE Plus and Extended-Multi-LogiEval evaluation.",
            "model_size": "unspecified (within the paper's reported 1.5B–70B range)",
            "reasoning_task_name": "PrOntoQA-OOD; PARARULE Plus; Extended-Multi-LogiEval",
            "reasoning_task_description": "Strict logical deduction benchmarks requiring application of formal inference rules and multi-step proofs with distractors.",
            "method_or_approach": "QK-score per-head evaluation and selection; baseline final-token probability.",
            "performance": "Paper notes that some heads from DeepSeek-R1 show strong performance when selected via QK-score; however, a specific head (22,16) consistently yields accuracy below 0.35 on PARARULE Plus, indicating reversed classification (QK-score distinguishes but with inverted sign). Other DeepSeek-R1 heads in Table 1 show accuracies &gt;0.60 in some columns.",
            "baseline_comparison": "Mixed: while some heads outperform baseline substantially on PARARULE Plus, the reversed-sign failure for certain heads highlights potential mismatch when transferring head selection across datasets.",
            "limitations_or_failures": "Exhibited a concrete failure case where a selected head’s QK-score correlates with correctness but with inverted sign (head (22,16) on PARARULE Plus). This underscores dataset-format dependence and the need for careful calibration and possibly per-dataset sign correction.",
            "insights_or_conclusions": "Model-specific head behaviors can vary widely; QK-score can identify discriminative heads but sign/interpretation must be validated per dataset and per head to avoid misclassification.",
            "uuid": "e8603.3",
            "source_info": {
                "paper_title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "LLaMA3 (70B)",
            "name_full": "LLaMA3 (70B instruct variant reported in tables)",
            "brief_description": "A large (70B) transformer LLM variant evaluated with QK-score and baseline; reported in aggregate performance tables comparing QK head scoring vs baseline across many inference rules and depths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA3 (70B, instruct-like evaluations reported)",
            "model_description": "Large transformer-based language model (70B parameters) evaluated in frozen instruct-style setting; reported in paper's comparative tables showing near-perfect QK-head performance on many rule types/depths.",
            "model_size": "70B",
            "reasoning_task_name": "PrOntoQA-OOD; Extended-Multi-LogiEval (first-order logic schemes)",
            "reasoning_task_description": "Multi-step strict deductive reasoning across a comprehensive set of first-order inference rules (MP, MT, HS, DS, CD, DD, BD, CT, DMT, CO, IM, EG, UI) and varying proof depths.",
            "method_or_approach": "QK-score head selection on calibration, used to pick BEST HEAD and evaluate on held-out sets; Baseline is final-layer probability answer selection.",
            "performance": "Tables in the paper show very high QK accuracies for LLaMA3 QK heads (many entries at/near 0.99–1.00 for low depths and still high for larger depths) and strong baseline scores (but generally lower than QK in many entries). The paper claims QK-heads can substantially exceed baseline on multi-step inferences and are robust to distractors; exact per-rule numbers provided in tables.",
            "baseline_comparison": "QK-head scoring often matches or exceeds baseline for LLaMA3 70B, especially at shallow-to-moderate reasoning depths; reported QK accuracies frequently higher than the baseline across many rule types in presented tables.",
            "limitations_or_failures": "Paper notes that head selection was done only for two sets of deduction rules and on instruct/chat-tuned variants; it does not claim these heads are solely responsible for reasoning. Calibration requirements and potential overfitting to calibration distribution remain limitations.",
            "insights_or_conclusions": "Large models like LLaMA3 appear to contain very clear internal QK alignment signals corresponding to logical validity; QK-scoring can surface near-perfect verification heads in such large models, showing that essential reasoning signals can exist prior to final output computations.",
            "uuid": "e8603.4",
            "source_info": {
                "paper_title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "LLaMA3.1",
            "name_full": "LLaMA3.1 (70B variant reported in tables)",
            "brief_description": "Another large 70B variant (LLaMA3.1) evaluated with QK-score vs baseline and included in comparative tables for many inference rules/depths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA3.1 (70B)",
            "model_description": "Transformer-based 70B frozen model (reported as LLaMA3.1 in tables). Evaluations mirror those for LLaMA3; QK-head accuracies reported as high across many rule types and depths.",
            "model_size": "70B",
            "reasoning_task_name": "PrOntoQA-OOD; Extended-Multi-LogiEval",
            "reasoning_task_description": "First-order and propositional multi-step logical inference tasks described in the paper and supplemental datasets.",
            "method_or_approach": "Per-head QK-score selection on calibration and single-pass evaluation; baseline final-token probability.",
            "performance": "Paper tables report QK accuracies very high for LLaMA3.1 (many entries ≈0.99 or above for many rule/depth settings) and baseline somewhat lower on some rule types; demonstrates that QK-heads generalize strongly in the largest models shown.",
            "baseline_comparison": "QK approach frequently outperforms baseline for LLaMA3.1 across many rule types and depths per tabulated results.",
            "limitations_or_failures": "Same experimental caveats: selection limited to certain deduction-rule sets and instruct/chat-tuned variants; method requires calibration and may not identify all reasoning-contributing heads.",
            "insights_or_conclusions": "Very large models show the clearest QK alignment signals for logical validity; QK-scoring is especially effective at surfacing these in models at this scale.",
            "uuid": "e8603.5",
            "source_info": {
                "paper_title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "8B Instruct",
            "name_full": "8B Instruct (unnamed 8B instruct-style model in tables)",
            "brief_description": "An 8B parameter instruct-style frozen model included in comparative tables showing QK vs baseline performance across rules/depths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "8B Instruct (unnamed)",
            "model_description": "An instruct-style, frozen transformer LLM of approximately 8B parameters used in the paper's tabulated comparisons; exact family not specified in main text.",
            "model_size": "8B",
            "reasoning_task_name": "PrOntoQA-OOD; Extended-Multi-LogiEval",
            "reasoning_task_description": "Structured deductive reasoning requiring application of first-order and propositional inference rules across varying depths.",
            "method_or_approach": "QK-score per-head evaluation and baseline probability comparison.",
            "performance": "Tabulated results show high QK accuracies for this 8B instruct model on many rule/depth cells (examples in Table 4 and 5), with QK often matching or exceeding baseline at many depths; exact numbers are in the paper tables.",
            "baseline_comparison": "QK-head selection provides improved or comparable performance to baseline in many reported cells; aggregated claims in text state BEST HEAD outperforms baseline across depths/distractors.",
            "limitations_or_failures": "Calibration and dataset-dependence limitations apply; the paper notes it only studied instruct-and chat-tuned variants and did not explore base-to-finetuned transitions.",
            "insights_or_conclusions": "Mid-sized instruct models can contain heads with useful QK alignment signals; QK-score provides a light-weight probe across scales (8B included).",
            "uuid": "e8603.6",
            "source_info": {
                "paper_title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "70B Instruct (unnamed)",
            "name_full": "70B Instruct (unnamed 70B instruct-style model reported in tables)",
            "brief_description": "A 70B instruct-style frozen LLM variant included in comparisons showing high performance both by baseline and QK-head measures on many logical rules and depths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "70B Instruct (unnamed)",
            "model_description": "Transformer-based instruct/chat-tuned model with approximately 70B parameters; evaluated frozen in zero-shot single-word answer setting across benchmarks.",
            "model_size": "70B",
            "reasoning_task_name": "PrOntoQA-OOD; Extended-Multi-LogiEval",
            "reasoning_task_description": "Multi-step strict logical reasoning across various first-order and propositional inference rules and depths.",
            "method_or_approach": "QK-score per-head selection (BEST HEAD) vs Baseline final-probability answer selection.",
            "performance": "Presented tables show very strong performance for this 70B instruct variant: high QK accuracies across many rule/depth combinations and strong baseline performance as well; QK often matches or marginally exceeds baseline in many cells (exact numbers in paper tables).",
            "baseline_comparison": "Both QK and baseline are strong at this scale; QK-head selection can still provide gains or clearer internal verification signals in multi-step or distractor-rich scenarios as reported in the main text.",
            "limitations_or_failures": "Same limitations apply: calibration needed, head selection limited to a subset of deduction rules, and experiments limited to instruct/chat-tuned variants.",
            "insights_or_conclusions": "At the largest scales shown, internal QK alignment for logical validity is particularly clear; QK-scoring can be used as a lightweight diagnostic even when baseline performance is already high.",
            "uuid": "e8603.7",
            "source_info": {
                "paper_title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "QK-score",
            "name_full": "Query-Key Score (QK-score) evaluation method",
            "brief_description": "A lightweight single-pass evaluation technique that computes per-head alignment between the query vector of the answer token and key vector of the statement token to score logical validity and select 'reasoning' heads.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "method (applied across multiple transformer LLMs)",
            "model_description": "Not a model but a method applied to frozen transformer LLMs: for each head (l,h), compute S_QK(l,h)(c,s,a_i) = q^{(l,h)}_{a_i}^T k^{(l,h)}_s and use head(s) with best calibration performance to classify true/false logical transitions.",
            "model_size": "method applies to models across sizes 1.5B–70B",
            "reasoning_task_name": "PrOntoQA-OOD; PARARULE Plus; Extended-Multi-LogiEval",
            "reasoning_task_description": "Strict logical reasoning benchmarks requiring formal deductive steps and chain-of-thought style multi-hop inference; QK-score evaluates validity of single-step logical transitions within these tasks.",
            "method_or_approach": "Compute QK-score per head on a calibration set (balanced true/false), select BEST HEAD(s), then evaluate classification accuracy on held-out sets; single forward pass processes all heads; contrasts with ablation or intervention methods.",
            "performance": "BEST HEAD selected via QK-score consistently outperforms baseline (final probabilities) on PrOntoQA-OOD across depths and distractor counts; cross-dataset transfer: 3/5 selected heads exceed baseline by &gt;10% on PARARULE Plus, 2/5 beat baseline on Extended-Multi-LogiEval. Large models sometimes show near-perfect QK accuracies in tabulated results.",
            "baseline_comparison": "QK-score often yields higher accuracy than baseline across models and datasets; the paper provides multiple table entries where QK-heads exceed baseline and reports consistent in-domain superiority of BEST HEAD.",
            "limitations_or_failures": "Requires a sufficiently large, balanced, and debiased calibration dataset (~400+ reasoning questions; authors used 600 with 300/300 split). Head selection was only performed for two sets of deduction rules in the study; some heads may only apply to subsets of logical principles. Reversed-sign behavior of some heads in cross-dataset transfer observed (e.g., head (22,16)). Selection restricted to instruct/chat-tuned models in this study.",
            "insights_or_conclusions": "QK-score provides an efficient, scalable probe for internal logical-consistency signals in transformer heads, surfacing heads that act as 'verification anchors' robust to distractors and reasoning depth; it complements CoT and ablation studies by offering single-pass, interpretable internal metrics and can reveal reasoning signals that final outputs obscure.",
            "uuid": "e8603.8",
            "source_info": {
                "paper_title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PrOntoQA",
            "rating": 2
        },
        {
            "paper_title": "Testing the general deductive reasoning capacity of large language models using OOD examples",
            "rating": 2,
            "sanitized_title": "testing_the_general_deductive_reasoning_capacity_of_large_language_models_using_ood_examples"
        },
        {
            "paper_title": "PARARULE Plus",
            "rating": 2,
            "sanitized_title": "pararule_plus"
        },
        {
            "paper_title": "Multi-LogiEval: Towards evaluating multi-step logical reasoning ability of large language models",
            "rating": 2,
            "sanitized_title": "multilogieval_towards_evaluating_multistep_logical_reasoning_ability_of_large_language_models"
        },
        {
            "paper_title": "LogicBench: Towards systematic evaluation of logical reasoning ability of large language models",
            "rating": 2,
            "sanitized_title": "logicbench_towards_systematic_evaluation_of_logical_reasoning_ability_of_large_language_models"
        },
        {
            "paper_title": "A mechanistic interpretation of syllogistic reasoning in auto-regressive language models",
            "rating": 1,
            "sanitized_title": "a_mechanistic_interpretation_of_syllogistic_reasoning_in_autoregressive_language_models"
        },
        {
            "paper_title": "Chain-of-Thought and related prompting work (Wei et al., Kojima et al.)",
            "rating": 1,
            "sanitized_title": "chainofthought_and_related_prompting_work_wei_et_al_kojima_et_al"
        },
        {
            "paper_title": "FOLIO: Natural language reasoning with first-order logic",
            "rating": 1,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "Enhancing reasoning capabilities of LLMs via principled synthetic logic corpus (Morishita et al.)",
            "rating": 1,
            "sanitized_title": "enhancing_reasoning_capabilities_of_llms_via_principled_synthetic_logic_corpus_morishita_et_al"
        }
    ],
    "cost": 0.015380999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Quantifying Logical Consistency in Transformers via Query-Key Alignment
24 Feb 2025</p>
<p>Eduard Tulchinskii 
Skolkovo Institute of Science and Technology</p>
<p>AI Foundation and Algorithm Lab</p>
<p>Anastasia Voznyuk 
Moscow Institute of Physics and Technology</p>
<p>Laida Kushnareva 
AI Foundation and Algorithm Lab</p>
<p>Andrei Andriiainen 
Skolkovo Institute of Science and Technology</p>
<p>Moscow Institute of Physics and Technology</p>
<p>Irina Piontkovskaya 
AI Foundation and Algorithm Lab</p>
<p>Evgeny Burnaev 
Skolkovo Institute of Science and Technology</p>
<p>Artificial Intelligence Research Institute (AIRI)</p>
<p>Serguei Barannikov 
Skolkovo Institute of Science and Technology</p>
<p>CNRS
Université Paris Cité
France</p>
<p>Quantifying Logical Consistency in Transformers via Query-Key Alignment
24 Feb 20253D8210B300E1802876BC1A58248F6C18arXiv:2502.17017v1[cs.CL]
Large language models (LLMs) have demonstrated impressive performance in various natural language processing tasks, yet their ability to perform multi-step logical reasoning remains an open challenge.Although Chain-of-Thought prompting has improved logical reasoning by enabling models to generate intermediate steps, it lacks mechanisms to assess the coherence of these logical transitions.In this paper, we propose a novel, lightweight evaluation strategy for logical reasoning that uses query-key alignments inside transformer attention heads.By computing a single forward pass and extracting a "QK-score" from carefully chosen heads, our method reveals latent representations that reliably separate valid from invalid inferences, offering a scalable alternative to traditional ablation-based techniques.We also provide an empirical validation on multiple logical reasoning benchmarks, demonstrating improved robustness of our evaluation method against distractors and increased reasoning depth.The experiments were conducted on a diverse set of models, ranging from 1.5B to 70B parameters.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have achieved remarkable success in a range of NLP tasks, yet they still struggle with reliable multi-step logical reasoning (Wei et al., 2022;Kojima et al., 2022;Yang et al., 2024;Seals and Shalin, 2023;Wan et al., 2024).While chain-of-thought prompting has improved performance by allowing models to generate intermediate reasoning steps, it lacks a mechanism to assess the coherence of these transitions.</p>
<p>In this work, we propose a novel evaluation method that uses certain internal Query-Key (QK) interactions within transformer heads as a proxy for logical consistency.Specifically, given a triplet-context c, statement s, and candidate answer a i , a 0 = true or a 1 = false -we define the QK-score as:
S (l,h) QK (c, s, a i ) = q (l,h)⊤ a i k (l,h) s ,
where q (l,h) a i is the query vector for the answer token and k (l,h) s is the key vector corresponding to the statement, see Section 3. Our method efficiently identifies transformer heads capable of accurately evaluating the validity of logical transitions.It processes all heads in a single run, making it scalable to large models.</p>
<p>The contributions of this paper are as follows:</p>
<p>• We propose a novel QK-score mechanism that uses the interactions between certain queryand key-vectors to assess the correctness of logical transitions in the corresponding tasks.</p>
<p>• We conduct extensive experiments on multiple logical benchmarks to demonstrate that our method improves logical consistency, especially in multi-step reasoning scenarios or when context contain reasoning distractors.</p>
<p>Related Work</p>
<p>A significant line of research has focused on improving multi-step logical reasoning in LLMs via chain-of-thought (CoT) prompting (Wei et al., 2022;Kojima et al., 2022;Yang et al., 2024;Seals and Shalin, 2023;Wan et al., 2024).Although CoT methods allow models to generate intermediate reasoning steps, they lack a mechanism to assess the coherence of these logical transitions.Mechanistic interpretability studies have examined the roles of transformer attention heads.Recent work revealed that attention layers operate in phases-knowledge recalling, latent reasoning, and expression preparation (Elhage et al., 2021;Olah et al., 2020;Zheng et al., 2024).Subsequent studies have shown that some attention heads introduce biases by evenly splitting probabilities between Figure 1: Our method calculates the Query-Key score between the end-of-line token immediately after the statement and the "true"/"false" tokens, for the designated head, from which we derive the answer.</p>
<p>answer options (Lieberum et al., 2023;Yu et al., 2024), while others suppress such behaviors during the final expression phase (Kim et al., 2024).In addition, recent investigations have attempted to analyze model behavior by disabling specific components (Zhang and Nanda, 2024;Todd et al., 2024;Yao et al., 2024), though these approaches are often computationally expensive or limited to simpler tasks (Wang et al., 2023;Yao et al., 2024).</p>
<p>For an expanded discussion on related works, see Appendix A.</p>
<p>Approach</p>
<p>Our method evaluates logical consistency by examining certain internal Query-Key (QK) interactions within transformer heads.In our setup, each input consists of a context c (which provides the premises), a statement s (a candidate conclusion), and a candidate answer a i (with a 0 = true and a 1 = false), see Figures 2, 4, 5 for examples.</p>
<p>For every attention head, indexed by h in layer l, we compute a QK-score that quantifies the alignment between the representation of the statement and that of the candidate answer.Given the triplet (c, s, a i ), the QK-score is the dot-product
S (l,h) QK (c, s, a i ) = q (l,h)⊤ a i k (l,h) s
, where q (l,h) a i is the query vector associated with the token representing a i (either "true" or "false") and k (l,h) s is the key vector corresponding to the token marking the end of the statement (see Figure 1).This score reflects how well a head can distinguish valid logical transitions.</p>
<p>By evaluating all heads in a single forward pass, our approach identifies those that reliably assess logical consistency.This efficient procedure avoids the need for extensive model modifications or headby-head ablation studies that are common in prior work.</p>
<p>Use provided context and answer whether the statement is true or false.CONTEXT: Each rompus is a wumpus.Every rompus is not opaque.Every jompus is a rompus.Every jompus is not sour.Vumpuses are jompuses [...] Polly is an impus.STATEMENT: Polly is opaque.ANSWER: true.We evaluate our method on three diverse benchmarks for logical reasoning:</p>
<p>ProntoQA-OOD (Saparov et al., 2023) is a synthetic dataset for chain-of-thought first-order logic question-answering over abstract categories (see Figure 2).We consider two deduction settings:</p>
<p>(1) tasks requiring only repeated applications of Modus Ponens; and (2) tasks involving all six supported deduction rules.For each setup, we partition the data by reasoning depth, selecting 600 examples (300 "true" and 300 "false") for calibration and using the remaining 1,000+ examples for evaluation.To further test robustness, we adapt the scripts to vary the number of distractors (irrelevant context statements).</p>
<p>PARARULE Plus (Bao et al., 2022) is a dataset of true/false questions with fixed-depth reasoning.We use its test subset without modifications.</p>
<p>Extended-Multi-LogiEval builds on Multi-LogiEval (Patel et al., 2024).The original dataset is constrained by only 10 samples per logical scheme and an imbalanced answer distribution.We address these limitations by generating additional samples (100 per scheme) and enforcing a balanced 50/50 split for "yes" and "no" responses (treated as a 0 and a 1 , respectively, for QK-score computation).</p>
<p>All experiments are performed in a zero-shot setup.Further details are provided in Appendices B and D.</p>
<p>Experimental Setup</p>
<p>All experiments were performed with frozen pretrained LLMs of size from 1.5B to 70B (for models larger than 7B, see Appendix E).</p>
<p>Our questions assume single-word answers; the standard approach in such setup is to select from a 0 and a 1 the option that was assigned the highest output probability by LLM, when the models are given the samples and asked to provide an answer.We refer to this method as the Baseline.</p>
<p>Our experiments were performed in two steps.First, for each explored setup from ProntoQA-OOD, we chose the best head in terms of QK-score on calibration set.Then we report the accuracy of QK-Scoring via the chosen head and the accuracy of Baseline on the evaluation subset.</p>
<p>Second, we selected five heads from those that achieved the top-10 performance in various ProntoQA-OOD setups (we aimed to choose heads that cover the higher number of setups).Then, we evaluated their performance on the PARARULE Plus and Extended-Multi-LogiEval datasets to asses the generalization capabilities of the QK-score informed head selection.</p>
<p>Results</p>
<p>In-Domain Evaluation</p>
<p>For each deduction setup in ProntoQA-OOD, we select the best-performing head on the calibration set (referred to as BEST HEAD) and evaluate its performance on the held-out evaluation set. Figure 3 shows that the BEST HEAD consistently outperforms the baseline across varying reasoning depths and under different numbers of distractors.In addition, we report the performance of five individual heads drawn from the top-five performers in each setup.These results indicate that the selected heads maintain stable performance regardless of the number of reasoning steps, thereby confirming that our QK-score reliably identifies transformer components that contribute to logical consistency.Results for three LLMs are presented here; further details are provided in Appendix E.)</p>
<p>Transfer Learning Evaluation</p>
<p>We further assessed the cross-dataset generalization of our approach by evaluating the QK-scoring accuracy of the heads selected in the above experiments on two additional datasets: PARARULE Plus and Extended-Multi-LogiEval.As reported in Table 1, three out of five selected heads achieve an accuracy exceeding the baseline by more than 10% on PARARULE Plus.In contrast, only two heads outperform the baseline on Extended-Multi-LogiEval, which may be attributed to differences in question format (binary "yes/no" vs. true/false evaluations).</p>
<p>Overall, these results demonstrate that our QKscore method identifies transformer heads capable of reliably assessing logical transitions, with robust performance observed both within the ProntoQA-OOD domain and in cross-dataset setups.</p>
<p>Analysis</p>
<p>Our findings suggest that QK-scores offer a distinctive lens on how LLMs process logical structure.Unlike raw attention weights, QK-scores are independent of positional encodings, thereby focusing purely on semantic alignment between the statement and candidate answers.Moreover, heads selected via QK-scores often outperform the model's final probabilities, confirming that essential reasoning signals can be sometimes obscured by laterstage processing (Kim et al., 2024;Lieberum et al., 2023).</p>
<p>We also observe that high QK-scoring heads consistently identify valid inferences even with distractors in the input.This stability indicates that such heads act as "verification anchors," largely unaffected by irrelevant context.Consequently, QK-scores may bolster both interpretability and robustness: by highlighting the heads that preserve logical consistency, they help clarify how multistep reasoning unfolds within large language models.</p>
<p>Conclusion</p>
<p>We introduced a QK-score framework for uncovering attention heads that consistently capture logical validity.Our experiments show that in multi-step inferences with distractors, certain heads outperform the model's own final-layer predictions.Crucially, this single-pass procedure sidesteps the computational overhead of ablation-based methods and generalizes relatively well across datasets.By identifying attention heads that act as "reasoning checkpoints," our approach offers a tractable window into how these models process complex logical relationships.Future work may refine QK-scoring for specialized tasks, explore synergy with chainof-thought prompting, and extend this analysis to other interpretability settings.Ultimately, bridging internal alignment signals with logical consistency represents a key step toward transparent and reliable language models.</p>
<p>Limitations</p>
<p>Application of our method requires a sufficiently large calibration dataset (at least ≈ 400 reasoning questions).This dataset must cover logical rules with more-or-less balanced number of examples and must be carefully debiased to prevent selection of attention heads that guess the answer from the form of the question.</p>
<p>We do not state that heads identified by our method are the only ones responsible for the logical inference within the model.</p>
<p>During the research for this short paper, we performed head selection only on two sets of deduction rules ("Modus Ponens" vs. "Modus Ponens, conjunction/disjunction introduction/elimination, and proof by contradiction").It is quite possible, that the 'scope' of certain attention heads is limited to different sets of logical principles, and a more extensve experiments may be needed to explore those cases.</p>
<p>Finally, we considered only Instruct-and Chattuned models.Question of if and how capabilities for logic in attention heads change during the transition from base to fine-tuned version we leave for the future work.</p>
<p>A Related Work</p>
<p>Here we discuss in more details the works related to logical reasoning and internal model interpretability.</p>
<p>Chain-of-Thought (CoT) and Logical Reasoning.CoT prompting has been widely adopted to improve reasoning in LLMs (Wei et al., 2022;Kojima et al., 2022;Yang et al., 2024;Seals and Shalin, 2023;Wan et al., 2024).These methods prompt models to produce intermediate steps in the reasoning process.However, despite the success in various tasks, they do not offer a measure to quantify the coherence of the reasoning transitions, leaving a gap that our QK-score method aims to fill.</p>
<p>Mechanistic Interpretability.A complementary line of research has focused on understanding transformer models through mechanistic interpretability.Work by Elhage et al. (2021) and Olah et al. (2020) has shown that transformer attention heads can be categorized into different functional phases, including knowledge recalling, latent reasoning, and expression preparation (Zheng et al., 2024).More recent studies have explored specific reasoning tasks: for example, Kim et al. (2024) examined syllogistic reasoning, showing that models learn content-independent reasoning mechanisms transferable across different logical schemes.Other investigations (Lieberum et al., 2023;Yu et al., 2024) have highlighted that certain heads can adversely affect the final decision by introducing latent biases.While ablation-based techniques (Zhang and Nanda, 2024;Todd et al., 2024;Yao et al., 2024) have been used to study these phenomena, they are often resource-intensive or limited to smaller models (Wang et al., 2023).</p>
<p>Benchmark Datasets.Several benchmarks have been designed to evaluate logical reasoning in LLMs.For example, LogicBench (Parmar et al., 2024) and Multi-LogiEval (Patel et al., 2024) test models on tasks such as truth table reasoning, logical entailment, and satisfiability.Datasets specifically tailored for chain-of-thought evaluation, such as PrOntoQA (Saparov and He, 2022), FOLIO (Han et al., 2022), and FLD (Morishita et al., 2024), demonstrate that unstructured intermediate reasoning steps can enhance performance.</p>
<p>Alternative Evaluation Strategies.Other lines of work have explored neuro-symbolic and datadriven methods to assess reasoning quality.Some approaches reformulate reasoning tasks into structured formats, while others propose direct evaluation metrics based on model outputs.</p>
<p>Summary.While previous work has substantially advanced our understanding of how LLMs reason and how their internal representations operate, there remains a need for efficient, scalable methods to assess logical consistency.Our method, which relies on the natural alignment between specific query and the last statement token key vectors, complements existing techniques and offers an efficient alternative to ablation-based analysis.</p>
<p>B ProntoQA-OOD: additional details</p>
<p>Questions in PrOntoQA-OOD are organized in a following way: given a set of axioms (context) it is required to prove a theorem (statement).For our study we reformulate them into answering if the statement is true or false given the context.</p>
<p>We used scripts publsihed by the authors of the  dataset to generate the data.We used following command line flags:</p>
<p>• For Modus Ponense only:</p>
<p>--ordering random --distractors none --deduction-rule ModusPonens --proofs-only --ontology fictional --min-hops 1 --max-hops 5</p>
<p>• For composition of deduction rules:</p>
<p>--ordering random --deduction-rule Composed --proofs-only --distractors none --ontology fictional --min-hops 1 -max-hops 5</p>
<p>We also modified the original scripts to make possible variating the number of distractors in prompts, and generated data for 1−hop questions on Modus Ponense with 1, 5 distractors respectively.</p>
<p>In our experiments we used training data from in context examples.For each setup (deduction rule+number of hops + number of distractors) scripts yielded 4, 000 samples.To exclude possible biases, in each case we select equal number of questions where the statement is given in positive (X is Y.) and negative (X is not Y.) forms and for each sample we also generate its counterpart where its statement and ground-truth answer are negated (thus balancing the classes).Depending on the setup, this resulted in 2, 600 − 7, 000 questions, out of which 600 (300/300 with answers "true" and "false" respectively) were taken for calibration and the rest were used as evaluation set (see Table 2).We ensured that no pair axioms+theorem is included in both subsets.On PARARULE Plus, three out of five heads, selected on the ProntoQA-OOD dataset, reach accuracy higher than the baseline, in most cases by more than 10%.Interestingly, head (22, 16) from DeepSeek-R1 consistently yields an accuracy below 0.35 on PARARULE Plus, suggesting that its QK-score distinguishes correct from incorrect logical implications but in a reversed manner.Similar effect also occurs in some setups on other heads.While the original dataset Multi-Logi-Eval consisted of three types of logic, namely First-Order Logic, Nonmonotonic Logic and Propositional Logic, we extended only a part with first-order logic.We used GPT-4o to generate more samples for each scheme.We refer to original paper (Patel et al., 2024) for the detailed description of logical schemes, and we did not add any new schemes or changed them in any way on all levels of reasoning.Every scheme of reasoning depth k consists of k atomic rules from reasoning depth 1, see them in Table 4.</p>
<p>C Pararule-plus: additional details</p>
<p>E Extended results of in-domain evaluation on ProntoQA-OOD</p>
<p>Tables 5 and 6 provide the in-domain numerical evaluation results on ProntoQA-OOD for BEST HEAD and BASELINE methods calculated for various LLMs.</p>
<p>Rule</p>
<p>First-order Logic
MP (∀x(p(x) → q(x)) ∧ p(a)) ⊢ q(a) MT (∀x(p(x) → q(x)) ∧ ¬q(a)) ⊢ ¬p(a) HS (∀x((p(x) → q(x)) ∧ (q(x) → r(x))) ⊢ (p(a) → r(a)) DS (∀x(p(x) ∨ q(x)) ∧ ¬p(a)) ⊢ q(a) CD (∀x((p(x) → q(x)) ∧ (r(x) → s(x))) ∧ (p(a) ∨ r(a))) ⊢ (q(a) ∨ s(a)) DD (∀x((p(x) → q(x)) ∧ (r(x) → s(x))) ∧ (¬q(a) ∨ ¬s(a))) ⊢ (¬p(a) ∨ ¬r(a)) BD (∀x((p(x) → q(x)) ∧ (r(x) → s(x))) ∧ (p(a) ∨ ¬s(a))) ⊢ (q(a) ∨ ¬r(a)) CT ∀x(p(x) ∨ q(x)) ⊣⊢ ∀x(q(x) ∨ p(x)) DMT ¬∀x(p(x) ∧ q(x)) ⊣⊢ ∃x(¬p(x) ∨ ¬q(x)) CO ∀x((p(x) → q(x)) ∧ (p(x) → r(x))) ⊢ ∀x(p(x) → (q(x) ∧ r(x))) IM ∀x(p(x) → (q(x) → r(x))) ⊣⊢ ∀x((p(x) ∧ q(x)) → r(x)) EG p(a) ⊢ ∃x(p(x)) UI ∀x(p(x)) ⊢ p(a)
Figure</p>
<p>Figure 2: PrOntoQA-OOD Example</p>
<p>Figure 3 :
3
Figure 3: In-domain performamce on ProntoQA-OOD dataset.Best head was selected on calibration data for each case individually.</p>
<p>Figure 4 :
4
Figure 4: PARARULE-PLUS prompt example of reasoning, depth 2</p>
<p>Figure 5 :
5
Figure 5: Multi-LogiEval Modus Ponens Example of reasoning depth 1</p>
<p>Table 1 :
1
Performance of QK-score on heads selected on ProntoQA-OOD in cross-domain evaluation.PARARULE Plus and Extended-Multi-LogiEval datasets are used.Best results are highlighted in bold.Those results that are better than baseline are underlined
PARARULE PlusExtended-Multi-LogiEvalReasoning depthReasoning depthHead23451234Qwen2.5 (26, 7) 0.6043 0.6772 0.6483 0.7095 0.4730 0.4663 0.5163 0.44131.5B, instruct (20, 6) 0.5310 0.5844 0.5436 0.6097 0.6069 0.6063 0.5804 0.5516(23, 5) 0.5999 0.6488 0.6468 0.6694 0.5026 0.5587 0.4859 0.3840(17, 10) 0.4552 0.5569 0.4800 0.4549 0.5003 0.5165 0.5413 0.4943(22, 9) 0.7095 0.7529 0.7719 0.8003 0.2445 0.4531 0.4141 0.4527Baseline-0.6240 0.6263 0.6521 0.6423 0.4996 0.4834 0.5011 0.5057DeepSeek-R1 (22, 26) 0.6736 0.7418 0.5495 0.6145 0.5236 0.5112 0.5402 0.5115Distill-Qwen-7B (22, 16) 0.3206 0.3422 0.2763 0.2388 0.4689 0.5271 0.5185 0.5143(21, 12) 0.4982 0.5203 0.4452 0.4740 0.7682 0.5733 0.5989 0.6218(22, 24) 0.6523 0.6454 0.6149 0.6557 0.6572 0.5495 0.5771 0.5888(25, 22) 0.6227 0.6233 0.6145 0.5357 0.5093 0.4768 0.4902 0.5186Baseline-0.4969 0.5212 0.4523 0.4717 0.5153 0.4835 0.5010 0.5057</p>
<p>Table 2 :
2
Size of ProntoQA-OOD evaluation set that was used in different setups</p>
<p>Here we demonstrate the example from this dataset: CONTEXT: Harry is strong.Harry is big.Harry is high.Anne is thin.Anne is little.Gary is smart.Gary is quiet.Gary is kind.Fiona is poor.Fiona is rough.Fiona is sad.Strong people are smart.If someone is thin and little then they are short.If someone is poor and rough then they are bad.If someone is smart and quiet then they are nice.All short people are small.All smart people are quiet.All nice people are wealthy.
All bad people are dull.STATEMENT: Harry is quiet.ANSWER: true</p>
<p>Table 3 compares the statistics of generated dataset with the statistics of the original (Multi-LogiEval dataset):
DepthDataset1234Multi-LogiEval (FOL)130105 135 120Extended Multi-LogiEval1300 700 900 700</p>
<p>Table 3
3:Statistics of generated Extended-MultiLogiEval dataset.</p>
<p>Table 4 :
4
Inference rules that establish the relationship between premises and their corresponding conclusions in Extended Multi-LogiEval.The schemes are MP: Modus Ponens, MT: Modus Tollens, HS: Hypothetical Syllogism, DS: Disjunctive Syllogism, CD: Constructive Dilemma, DD: Destructive Dilemma, BD: Bidirectional Dilemma, CT: QK 0.9881 0.9577 0.9192 0.8799 0.8676 0.8084 0.7184 0.7188 0.7239 0.7052 8B Instruct Baseline 0.9843 0.9785 0.9473 0.9035 0.8775 0.6848 0.6206 0.5847 0.5888 0.5731 LLaMA3 QK 1.0000 0.9999 0.9963 0.9981 0.9960 0.9890 0.9214 0.8799 0.8617 0.8353 70B Instruct Baseline 0.9993 0.9999 0.9981 0.9868 0.9739 0.9805 0.8767 0.8324 0.8171 0.7948 LLaMA3.1 QK 0.9987 0.9993 0.9993 0.9968 0.9942 0.9890 0.9144 0.9073 0.8762 0.8631 70B Instruct Baseline 1.0000 0.9999 0.9967 0.9904 0.9762 0.9922 0.8912 0.8685 0.8348 0.8246 Qwen-1.5BBaseline 0.5361 0.5103 0.5053 0.5011 0.5020 0.4873 0.4980 0.4959 0.4971 0.4985
Commutation, DMT: De Morgan's Theorem, CO: Composition, IM: Importation, EG: Existential Generalization,UI: Universal Instantiation</p>
<p>Table 5 :
5
Comparison of models with different reasoning depths on ProntoQA-OOD.Best results are highlighted in bold.</p>
<p>Multi-step deductive reasoning over natural language: An empirical study on out-of-distribution generalisation. Qiming Bao, Alex Yuxuan Peng, Tim Hartill, Neset Tan, Zhenyun Deng, Michael Witbrock, Jiamou Liu, Proceedings of the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning &amp; Reasoning (IJCLR 2022). the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning &amp; Reasoning (IJCLR 2022)Cumberland Lodge, Windsor Great Park, United Kingdom2022</p>
<p>Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A mathematical framework for transformer circuits. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack ClarkTransformer Circuits Thread</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq Joty, Alexander R Fabbri, Wojciech Kryscinski, arXiv:2209.00840Xi Victoria Lin, Caiming Xiong, and Dragomir Radev. 2022. Folio: Natural language reasoning with first-order logic. arXiv preprint</p>
<p>A mechanistic interpretation of syllogistic reasoning in auto-regressive language models. Geonhee Kim, Marco Valentino, André Freitas, arXiv:2408.085902024arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, Vladimir Mikulik, arXiv:2307.09458Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. 2023arXiv preprint</p>
<p>Enhancing reasoning capabilities of llms via principled synthetic logic corpus. Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, Yasuhiro Sogawa, Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Zoom in: An introduction to circuits. Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, Shan Carter, Distill. 532020</p>
<p>LogicBench: Towards systematic evaluation of logical reasoning ability of large language models. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral, 10.18653/v1/2024.acl-long.739Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, Thailand20241Association for Computational Linguistics</p>
<p>Multi-LogiEval: Towards evaluating multi-step logical reasoning ability of large language models. Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral, 10.18653/v1/2024.emnlp-main.1160Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, arXiv:2210.012402022arXiv preprint</p>
<p>Testing the general deductive reasoning capacity of large language models using OOD examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, He He, 10.48550/arXiv.2305.15269CoRR, abs/2305.152692023</p>
<p>Evaluating the deductive competence of large language models. S M Seals, Valerie L Shalin, North American Chapter. the Association for Computational Linguistics2023</p>
<p>Function vectors in large language models. Eric Todd, Millicent Li, Arnab Sharma, Aaron Mueller, Byron C Wallace, David Bau, International Conference on Learning Representations. ICLR2024</p>
<p>LogicAsker: Evaluating and improving the logical reasoning ability of large language models. Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-Tse Huang, Pinjia He, Wenxiang Jiao, Michael Lyu, 10.18653/v1/2024.emnlp-main.128Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>            </div>
        </div>

    </div>
</body>
</html>
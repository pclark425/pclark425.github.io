<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2993 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2993</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2993</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-258887799</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.16130v3.pdf" target="_blank">Language Models Implement Simple Word2Vec-style Vector Arithmetic</a></p>
                <p><strong>Paper Abstract:</strong> A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple vector arithmetic style mechanism to solve some relational tasks using regularities encoded in the hidden space of the model (e.g., Poland:Warsaw::China:Beijing). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, uppercasing, and past-tensing) a key part of the mechanism reduces to a simple additive update typically applied by the feedforward (FFN) networks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a growing body of work on the interpretability of LMs, and offer reason to be optimistic that, despite the massive and non-linear nature of the models, the strategies they ultimately use to solve tasks can sometimes reduce to familiar and even intuitive algorithms.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2993.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2993.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VectorArithmetic_FFNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word2Vec-style Vector Arithmetic Implemented by FFN Output Vectors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Language models (primarily GPT2-Medium) implement simple vector-addition style transformations for one-to-one relational tasks by adding feedforward-network (FFN) output vectors (o vectors) into the residual stream, producing the target token analogous to classic word2vec analogies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-Medium (primary); evaluated also across GPT-2 family, GPT-J-6B, BLOOM-176B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer LMs studied include GPT2 variants (small/medium/large/xl), GPT-J (6B), and BLOOM (176B). Experiments focus on GPT2-Medium for manual analysis; architectures are standard autoregressive transformers with multi-head attention and per-layer FFN sublayers; sizes range from 124M to 176B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>One-to-one relational mapping tasks (not numeric): world capitals (country→capital), token uppercasing (word→CapitalizedWord), past-tense verb mapping (present→past).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>A content-independent additive update: FFN outputs (o vectors) implement a context-independent transformation (function) that when added to an argument token's residual-stream representation produces the corresponding target token; this is analogous to vector arithmetic (embedding + offset → related embedding).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Early-decoding shows argument token promotion followed by abrupt replacement by answer at a mid/late layer; isolated o vectors extracted from FFN outputs, when patched into other contexts, promote the intended outputs; mean reciprocal-rank improvements (e.g., ~10th→~3rd for world capitals), and sizable increases in top-token rate (world capitals top token in 21.3% of cases after intervention; uppercasing 53.5%; past-tensing 7.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Mechanism appears specific to one-to-one (injective) relations and fails on many-to-one/many-to-many relations; effectiveness varies by model (weaker effects on GPT-J/BLOOM for some tasks); tokenization splitting and low-pretraining-frequency pairs reduce effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Activation-space intervention: extract FFN output vectors (o vectors) from a natural example and replace/insert these FFN outputs in later layers of a forward pass for new inputs; also subtractive interventions (e.g., -o_case) and replacing multiple FFN layers with precomputed o vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Patching o vectors consistently promotes the target token across contexts; e.g., zero-shot world-capital output improved from 5.6% to 33.0% when replacing certain FFNs with o_city; adding +o_case recovered accuracy from 4.5% to 29.5% in an abstractive uppercasing setup. Interventions can both enable desired outputs and be used to force or suppress transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate improvements reported: mean reciprocal rank for world/state capitals improved from ~10th/17th to ~3rd/4th rank by last layer after o-vector intervention; top-token rates after intervention — world capitals 21.3%, uppercasing 53.5%, past-tensing 7.8%; intervention no-effect rates (in-domain / out-of-domain): capitals ~37%/20%, uppercasing 4%/5%, past tensing 19%/22%; zero-shot capital accuracy increased 5.6%→33.0%; replacing FFNs 16-24 with +o_case increased abstractive accuracy from 4.5% to 29.5% (approximated recovery to 72% of unablated model performance of 41%).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>No effect in a substantial fraction of cases (notably for less-frequent country-capital pairs); failure when mapping is non-injective; reduced effectiveness when tokenization splits words into subtokens; intervention sensitivity to which layers are patched and number of layers patched; variability across models and hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors compare mechanism qualitatively to vector arithmetic in static word embeddings (Mikolov et al., 2013) — similar successes/failures (works for injective relations, fails for many-to-one). No direct quantitative comparison to human arithmetic or symbolic calculators (mechanism is not algorithmic numeric computation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Implement Simple Word2Vec-style Vector Arithmetic', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2993.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2993.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Argument-Function_Processing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Argument–Function Processing Signature (Argument Formation → Function Application → Saturation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A characteristic forward-pass processing signature where the model first promotes the argument token (x) in intermediate layers, then abruptly switches to the function output (y) typically via an FFN update, and finally saturates with little further change.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed across multiple decoder-only LMs (GPT2 variants, GPT-J-6B, BLOOM-176B) but analyzed in detail in GPT2-Medium</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer families; phenomenon more pronounced in deeper models (more layers give more measurement points).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>One-to-one relational mapping tasks (capitals, uppercasing, past-tensing) — describes internal processing rather than an external arithmetic task.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Processing stages: (1) Argument Formation — earlier layers surface the argument token in residual stream; (2) Function Application — mid/late-layer FFN adds an o vector to convert argument→answer; (3) Saturation — remaining layers cease meaningful updates.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Layerwise early-decoding/logit-lens traces show an X-shaped pattern of reciprocal ranks (argument rises then falls as answer rises), consistent across tasks and models; stage locations are proportionally similar across architectures; interventions changing FFN outputs alter the function application stage.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Pattern disappears for extractive tasks where answer is present in context (attention heads suffice); also absent on non-injective relations on average (Appendix G).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Early-decoding (logit lens) instrumentation plus FFN output replacement/patching during the layers corresponding to function application.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Intervening during the function-application layers (replacing FFNs with o vectors) can force the model to apply the function to new arguments and thereby alter predictions; intervening outside these layers has lesser effect.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative stage detection across correct-answer examples; quantitative metrics tied to o-vector patching (see VectorArithmetic_FFNs entry).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>If the task is extractive, argument-function stages may not be used; stage detection depends on examples filtered for correct model predictions; when the argument formation and function application occur within a single layer, measurement resolution is lower.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human comparison; provides an operational decomposition of LLM inference into stages analogous to applying functions to arguments (informal analogy to symbolic function application).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Implement Simple Word2Vec-style Vector Arithmetic', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2993.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2993.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o_vector_interventions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FFN Output Vector ('o vector') Patching Interventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique of extracting the FFN output vector (o) from a layer when it implements a function (e.g., get_capital) and injecting that vector into the same or later FFN updates in other contexts to induce the same function behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-Medium (primary), also tested on GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (GPT2-Medium) where FFN outputs at layers ~18-23 were used for interventions; extraction required manual selection of layer and example.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to one-to-one relational mapping tasks (capitals, uppercasing, past tense); not numeric arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Assumes FFN output vectors encode context-independent functional updates; inserting these vectors adds the corresponding transformation to the residual stream for novel arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Direct intervention experiments: inserting o_city promotes capitals in nonsense contexts (e.g., repeated random tokens + 'China'), increasing reciprocal rank and often making target top token; aggregate improvements shown across datasets; subtractive interventions (−o_case) can force lowercasing.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Intervention sometimes has no effect (rates vary by task), and effect size is variable across models; necessity to replace multiple consecutive FFNs (single-layer patch less effective) suggests distributed/gradual implementation rather than a perfectly isolated single-vector operation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Activation-space patching: replace FFN outputs in selected later layers with precomputed o vectors from exemplar runs; can add or subtract these vectors (+o, −o); also used as a partial replacement of FFN sublayers to steer zero-shot outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Promoted target tokens (quantified in VectorArithmetic_FFNs entry); improved zero-shot capital output from 5.6%→33.0%; replacing top third of FFNs with +o_case in one experiment increased abstractive accuracy from 4.5%→29.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Top-token rates after interventions (see VectorArithmetic_FFNs): world capitals top in 21.3% of examples after intervention; mean reciprocal-rank improvements as described above; single-layer interventions produce smaller boosts than replacing multiple layers.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Depends on selecting correct layer range (hyperparameter-sensitive); subtokens/tokenization mismatch reduces effectiveness; sometimes produces wrong city (e.g., predicting Vienna for Armenia) when pretraining associations are weak.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Intervention is analogous to inserting a fixed additive rule into a symbolic pipeline, but authors emphasize it's an activation-level empirical method rather than a symbolic edit; compared qualitatively to task arithmetic/editing work (Ilharco et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Implement Simple Word2Vec-style Vector Arithmetic', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2993.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2993.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FFN_Ablation_Results</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation of Mid/Late FFN Sublayers and Role Distinction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablating (removing) mid-to-late FFN sublayers drastically reduces performance on abstractive (out-of-context) retrieval tasks while leaving extractive (in-context) tasks comparatively intact, indicating a specialization of FFNs for out-of-context factual recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models tested: GPT2 variants, GPT-J-6B, BLOOM-176B (results reported across models; detailed figures for GPT2-Medium and Bloom highlighted).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer models across sizes (124M–176B). Ablations performed by removing sets of FFN sublayers from the top down (e.g., removing layers 20-24, then 15-24, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Abstractive vs. extractive relational mapping tasks (capitals, colored-objects uppercasing), used to assess mechanism role rather than numeric arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>FFNs in mid-to-late layers implement out-of-context transformations/memory retrieval; attention heads specialized for copying/pasting from local context (extractive retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation curves show abstractive accuracy collapses much earlier than extractive accuracy as FFNs are removed (e.g., for BLOOM removing 24 FFN sublayers drops abstractive accuracy by 73% down to 1%, while extractive accuracy drops only 17%); consistent trend across models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Some residual extractive performance remains after heavy FFN ablation, and attention sometimes suffices for extractive tasks; degree of modularity and exact layer roles vary by model.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Surgical ablations: sequential removal of FFN sublayers from the top of the model; combined with replacement experiments (replacing removed FFNs with +o vectors) to measure recoverability.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Ablating FFNs severely degrades abstractive retrieval but leaves extractive tasks relatively robust; replacing ablated FFNs with appropriate o vectors can recover substantial performance (e.g., +o_case recovery described above).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Example: Bloom (39B? paper mentions 39B in text for some experiment) losing 24 FFN sublayers: extractive accuracy drop 17% from full-model; abstractive drop 73% down to 1%. Trend consistent across architectures (plots in Appendix B).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Ablation hyperparameters (which layers removed) matter; some tasks may be solved via alternative model pathways; removing FFNs can also harm extractive tasks if those layers contribute in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No human/symbolic comparison; authors relate the modular division of labor (FFN memory vs attention copying) to prior mechanistic work on transformer roles but do not equate to explicit symbolic memory systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Implement Simple Word2Vec-style Vector Arithmetic', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2993.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2993.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EarlyDecoding_LogitLens</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Early Decoding / Logit Lens Instrumentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding the model's next-token prediction at intermediate layers by applying the pretrained language-model head to residual-stream representations to reveal intermediate 'predictions' (the logit-lens technique).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to the same decoder-only transformer models (GPT2 variants, GPT-J, BLOOM); used as measurement tool rather than a model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Technique: apply existing unembedding matrix E and softmax to intermediate residual-stream vectors at various layers to obtain predicted-token distributions throughout the forward pass.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to monitor internal progression on relational mapping tasks (capitals, uppercasing, past tense) and to identify argument-function stages; not used to compute numeric arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Reveals the progressive construction of vocabulary distribution and the timing of argument formation and function application; supports the view that the model incrementally builds predictions through additive updates in the residual stream.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Layerwise decoding shows argument token prominence then target token takeover; saturation layers with little change are observed; used to choose layers for extracting o vectors and for observing intervention effects.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Logit-lens measurements are coarse when function application and attention updates happen within the same layer (only measuring after FFN for simplicity); early decoding is diagnostic but cannot alone prove causal mechanisms without interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Measurement/instrumentation (not an intervention), but used in conjunction with FFN patching and ablation to inform interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>N/A (instrumentation used to time interventions and to show their effect).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used to compute reciprocal-rank/time-series traces and to detect saturation; supports quantitative claims elsewhere (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Limited measurement resolution depending on where in-layer updates occur; reliant on the pretrained LM head being a faithful probe of internal activations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No human comparison; offers an interpretability analogue to 'print statements' in program debugging to observe intermediate computations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Implement Simple Word2Vec-style Vector Arithmetic', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Linguistic regularities in continuous space word representations <em>(Rating: 2)</em></li>
                <li>Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space <em>(Rating: 2)</em></li>
                <li>Transformer feed-forward layers are key-value memories <em>(Rating: 2)</em></li>
                <li>Locating and editing factual associations in gpt <em>(Rating: 2)</em></li>
                <li>Editing models with task arithmetic <em>(Rating: 2)</em></li>
                <li>A mathematical framework for transformer circuits <em>(Rating: 1)</em></li>
                <li>Interpreting GPT: the logit lens <em>(Rating: 1)</em></li>
                <li>Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2993",
    "paper_id": "paper-258887799",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "VectorArithmetic_FFNs",
            "name_full": "Word2Vec-style Vector Arithmetic Implemented by FFN Output Vectors",
            "brief_description": "Language models (primarily GPT2-Medium) implement simple vector-addition style transformations for one-to-one relational tasks by adding feedforward-network (FFN) output vectors (o vectors) into the residual stream, producing the target token analogous to classic word2vec analogies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-Medium (primary); evaluated also across GPT-2 family, GPT-J-6B, BLOOM-176B",
            "model_description": "Decoder-only transformer LMs studied include GPT2 variants (small/medium/large/xl), GPT-J (6B), and BLOOM (176B). Experiments focus on GPT2-Medium for manual analysis; architectures are standard autoregressive transformers with multi-head attention and per-layer FFN sublayers; sizes range from 124M to 176B parameters.",
            "arithmetic_task_type": "One-to-one relational mapping tasks (not numeric): world capitals (country→capital), token uppercasing (word→CapitalizedWord), past-tense verb mapping (present→past).",
            "reported_mechanism": "A content-independent additive update: FFN outputs (o vectors) implement a context-independent transformation (function) that when added to an argument token's residual-stream representation produces the corresponding target token; this is analogous to vector arithmetic (embedding + offset → related embedding).",
            "evidence_for_mechanism": "Early-decoding shows argument token promotion followed by abrupt replacement by answer at a mid/late layer; isolated o vectors extracted from FFN outputs, when patched into other contexts, promote the intended outputs; mean reciprocal-rank improvements (e.g., ~10th→~3rd for world capitals), and sizable increases in top-token rate (world capitals top token in 21.3% of cases after intervention; uppercasing 53.5%; past-tensing 7.8%).",
            "evidence_against_mechanism": "Mechanism appears specific to one-to-one (injective) relations and fails on many-to-one/many-to-many relations; effectiveness varies by model (weaker effects on GPT-J/BLOOM for some tasks); tokenization splitting and low-pretraining-frequency pairs reduce effectiveness.",
            "intervention_type": "Activation-space intervention: extract FFN output vectors (o vectors) from a natural example and replace/insert these FFN outputs in later layers of a forward pass for new inputs; also subtractive interventions (e.g., -o_case) and replacing multiple FFN layers with precomputed o vectors.",
            "effect_of_intervention": "Patching o vectors consistently promotes the target token across contexts; e.g., zero-shot world-capital output improved from 5.6% to 33.0% when replacing certain FFNs with o_city; adding +o_case recovered accuracy from 4.5% to 29.5% in an abstractive uppercasing setup. Interventions can both enable desired outputs and be used to force or suppress transformations.",
            "performance_metrics": "Aggregate improvements reported: mean reciprocal rank for world/state capitals improved from ~10th/17th to ~3rd/4th rank by last layer after o-vector intervention; top-token rates after intervention — world capitals 21.3%, uppercasing 53.5%, past-tensing 7.8%; intervention no-effect rates (in-domain / out-of-domain): capitals ~37%/20%, uppercasing 4%/5%, past tensing 19%/22%; zero-shot capital accuracy increased 5.6%→33.0%; replacing FFNs 16-24 with +o_case increased abstractive accuracy from 4.5% to 29.5% (approximated recovery to 72% of unablated model performance of 41%).",
            "notable_failure_modes": "No effect in a substantial fraction of cases (notably for less-frequent country-capital pairs); failure when mapping is non-injective; reduced effectiveness when tokenization splits words into subtokens; intervention sensitivity to which layers are patched and number of layers patched; variability across models and hyperparameters.",
            "comparison_to_humans_or_symbolic": "Authors compare mechanism qualitatively to vector arithmetic in static word embeddings (Mikolov et al., 2013) — similar successes/failures (works for injective relations, fails for many-to-one). No direct quantitative comparison to human arithmetic or symbolic calculators (mechanism is not algorithmic numeric computation).",
            "uuid": "e2993.0",
            "source_info": {
                "paper_title": "Language Models Implement Simple Word2Vec-style Vector Arithmetic",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Argument-Function_Processing",
            "name_full": "Argument–Function Processing Signature (Argument Formation → Function Application → Saturation)",
            "brief_description": "A characteristic forward-pass processing signature where the model first promotes the argument token (x) in intermediate layers, then abruptly switches to the function output (y) typically via an FFN update, and finally saturates with little further change.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed across multiple decoder-only LMs (GPT2 variants, GPT-J-6B, BLOOM-176B) but analyzed in detail in GPT2-Medium",
            "model_description": "Decoder-only transformer families; phenomenon more pronounced in deeper models (more layers give more measurement points).",
            "arithmetic_task_type": "One-to-one relational mapping tasks (capitals, uppercasing, past-tensing) — describes internal processing rather than an external arithmetic task.",
            "reported_mechanism": "Processing stages: (1) Argument Formation — earlier layers surface the argument token in residual stream; (2) Function Application — mid/late-layer FFN adds an o vector to convert argument→answer; (3) Saturation — remaining layers cease meaningful updates.",
            "evidence_for_mechanism": "Layerwise early-decoding/logit-lens traces show an X-shaped pattern of reciprocal ranks (argument rises then falls as answer rises), consistent across tasks and models; stage locations are proportionally similar across architectures; interventions changing FFN outputs alter the function application stage.",
            "evidence_against_mechanism": "Pattern disappears for extractive tasks where answer is present in context (attention heads suffice); also absent on non-injective relations on average (Appendix G).",
            "intervention_type": "Early-decoding (logit lens) instrumentation plus FFN output replacement/patching during the layers corresponding to function application.",
            "effect_of_intervention": "Intervening during the function-application layers (replacing FFNs with o vectors) can force the model to apply the function to new arguments and thereby alter predictions; intervening outside these layers has lesser effect.",
            "performance_metrics": "Qualitative stage detection across correct-answer examples; quantitative metrics tied to o-vector patching (see VectorArithmetic_FFNs entry).",
            "notable_failure_modes": "If the task is extractive, argument-function stages may not be used; stage detection depends on examples filtered for correct model predictions; when the argument formation and function application occur within a single layer, measurement resolution is lower.",
            "comparison_to_humans_or_symbolic": "No direct human comparison; provides an operational decomposition of LLM inference into stages analogous to applying functions to arguments (informal analogy to symbolic function application).",
            "uuid": "e2993.1",
            "source_info": {
                "paper_title": "Language Models Implement Simple Word2Vec-style Vector Arithmetic",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "o_vector_interventions",
            "name_full": "FFN Output Vector ('o vector') Patching Interventions",
            "brief_description": "A technique of extracting the FFN output vector (o) from a layer when it implements a function (e.g., get_capital) and injecting that vector into the same or later FFN updates in other contexts to induce the same function behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-Medium (primary), also tested on GPT-J",
            "model_description": "Decoder-only transformer (GPT2-Medium) where FFN outputs at layers ~18-23 were used for interventions; extraction required manual selection of layer and example.",
            "arithmetic_task_type": "Applied to one-to-one relational mapping tasks (capitals, uppercasing, past tense); not numeric arithmetic.",
            "reported_mechanism": "Assumes FFN output vectors encode context-independent functional updates; inserting these vectors adds the corresponding transformation to the residual stream for novel arguments.",
            "evidence_for_mechanism": "Direct intervention experiments: inserting o_city promotes capitals in nonsense contexts (e.g., repeated random tokens + 'China'), increasing reciprocal rank and often making target top token; aggregate improvements shown across datasets; subtractive interventions (−o_case) can force lowercasing.",
            "evidence_against_mechanism": "Intervention sometimes has no effect (rates vary by task), and effect size is variable across models; necessity to replace multiple consecutive FFNs (single-layer patch less effective) suggests distributed/gradual implementation rather than a perfectly isolated single-vector operation.",
            "intervention_type": "Activation-space patching: replace FFN outputs in selected later layers with precomputed o vectors from exemplar runs; can add or subtract these vectors (+o, −o); also used as a partial replacement of FFN sublayers to steer zero-shot outputs.",
            "effect_of_intervention": "Promoted target tokens (quantified in VectorArithmetic_FFNs entry); improved zero-shot capital output from 5.6%→33.0%; replacing top third of FFNs with +o_case in one experiment increased abstractive accuracy from 4.5%→29.5%.",
            "performance_metrics": "Top-token rates after interventions (see VectorArithmetic_FFNs): world capitals top in 21.3% of examples after intervention; mean reciprocal-rank improvements as described above; single-layer interventions produce smaller boosts than replacing multiple layers.",
            "notable_failure_modes": "Depends on selecting correct layer range (hyperparameter-sensitive); subtokens/tokenization mismatch reduces effectiveness; sometimes produces wrong city (e.g., predicting Vienna for Armenia) when pretraining associations are weak.",
            "comparison_to_humans_or_symbolic": "Intervention is analogous to inserting a fixed additive rule into a symbolic pipeline, but authors emphasize it's an activation-level empirical method rather than a symbolic edit; compared qualitatively to task arithmetic/editing work (Ilharco et al., 2023).",
            "uuid": "e2993.2",
            "source_info": {
                "paper_title": "Language Models Implement Simple Word2Vec-style Vector Arithmetic",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "FFN_Ablation_Results",
            "name_full": "Ablation of Mid/Late FFN Sublayers and Role Distinction",
            "brief_description": "Ablating (removing) mid-to-late FFN sublayers drastically reduces performance on abstractive (out-of-context) retrieval tasks while leaving extractive (in-context) tasks comparatively intact, indicating a specialization of FFNs for out-of-context factual recall.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple models tested: GPT2 variants, GPT-J-6B, BLOOM-176B (results reported across models; detailed figures for GPT2-Medium and Bloom highlighted).",
            "model_description": "Decoder-only transformer models across sizes (124M–176B). Ablations performed by removing sets of FFN sublayers from the top down (e.g., removing layers 20-24, then 15-24, etc.).",
            "arithmetic_task_type": "Abstractive vs. extractive relational mapping tasks (capitals, colored-objects uppercasing), used to assess mechanism role rather than numeric arithmetic.",
            "reported_mechanism": "FFNs in mid-to-late layers implement out-of-context transformations/memory retrieval; attention heads specialized for copying/pasting from local context (extractive retrieval).",
            "evidence_for_mechanism": "Ablation curves show abstractive accuracy collapses much earlier than extractive accuracy as FFNs are removed (e.g., for BLOOM removing 24 FFN sublayers drops abstractive accuracy by 73% down to 1%, while extractive accuracy drops only 17%); consistent trend across models.",
            "evidence_against_mechanism": "Some residual extractive performance remains after heavy FFN ablation, and attention sometimes suffices for extractive tasks; degree of modularity and exact layer roles vary by model.",
            "intervention_type": "Surgical ablations: sequential removal of FFN sublayers from the top of the model; combined with replacement experiments (replacing removed FFNs with +o vectors) to measure recoverability.",
            "effect_of_intervention": "Ablating FFNs severely degrades abstractive retrieval but leaves extractive tasks relatively robust; replacing ablated FFNs with appropriate o vectors can recover substantial performance (e.g., +o_case recovery described above).",
            "performance_metrics": "Example: Bloom (39B? paper mentions 39B in text for some experiment) losing 24 FFN sublayers: extractive accuracy drop 17% from full-model; abstractive drop 73% down to 1%. Trend consistent across architectures (plots in Appendix B).",
            "notable_failure_modes": "Ablation hyperparameters (which layers removed) matter; some tasks may be solved via alternative model pathways; removing FFNs can also harm extractive tasks if those layers contribute in some cases.",
            "comparison_to_humans_or_symbolic": "No human/symbolic comparison; authors relate the modular division of labor (FFN memory vs attention copying) to prior mechanistic work on transformer roles but do not equate to explicit symbolic memory systems.",
            "uuid": "e2993.3",
            "source_info": {
                "paper_title": "Language Models Implement Simple Word2Vec-style Vector Arithmetic",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "EarlyDecoding_LogitLens",
            "name_full": "Early Decoding / Logit Lens Instrumentation",
            "brief_description": "Decoding the model's next-token prediction at intermediate layers by applying the pretrained language-model head to residual-stream representations to reveal intermediate 'predictions' (the logit-lens technique).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to the same decoder-only transformer models (GPT2 variants, GPT-J, BLOOM); used as measurement tool rather than a model.",
            "model_description": "Technique: apply existing unembedding matrix E and softmax to intermediate residual-stream vectors at various layers to obtain predicted-token distributions throughout the forward pass.",
            "arithmetic_task_type": "Used to monitor internal progression on relational mapping tasks (capitals, uppercasing, past tense) and to identify argument-function stages; not used to compute numeric arithmetic.",
            "reported_mechanism": "Reveals the progressive construction of vocabulary distribution and the timing of argument formation and function application; supports the view that the model incrementally builds predictions through additive updates in the residual stream.",
            "evidence_for_mechanism": "Layerwise decoding shows argument token prominence then target token takeover; saturation layers with little change are observed; used to choose layers for extracting o vectors and for observing intervention effects.",
            "evidence_against_mechanism": "Logit-lens measurements are coarse when function application and attention updates happen within the same layer (only measuring after FFN for simplicity); early decoding is diagnostic but cannot alone prove causal mechanisms without interventions.",
            "intervention_type": "Measurement/instrumentation (not an intervention), but used in conjunction with FFN patching and ablation to inform interventions.",
            "effect_of_intervention": "N/A (instrumentation used to time interventions and to show their effect).",
            "performance_metrics": "Used to compute reciprocal-rank/time-series traces and to detect saturation; supports quantitative claims elsewhere (see other entries).",
            "notable_failure_modes": "Limited measurement resolution depending on where in-layer updates occur; reliant on the pretrained LM head being a faithful probe of internal activations.",
            "comparison_to_humans_or_symbolic": "No human comparison; offers an interpretability analogue to 'print statements' in program debugging to observe intermediate computations.",
            "uuid": "e2993.4",
            "source_info": {
                "paper_title": "Language Models Implement Simple Word2Vec-style Vector Arithmetic",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Linguistic regularities in continuous space word representations",
            "rating": 2,
            "sanitized_title": "linguistic_regularities_in_continuous_space_word_representations"
        },
        {
            "paper_title": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space",
            "rating": 2,
            "sanitized_title": "transformer_feedforward_layers_build_predictions_by_promoting_concepts_in_the_vocabulary_space"
        },
        {
            "paper_title": "Transformer feed-forward layers are key-value memories",
            "rating": 2,
            "sanitized_title": "transformer_feedforward_layers_are_keyvalue_memories"
        },
        {
            "paper_title": "Locating and editing factual associations in gpt",
            "rating": 2,
            "sanitized_title": "locating_and_editing_factual_associations_in_gpt"
        },
        {
            "paper_title": "Editing models with task arithmetic",
            "rating": 2,
            "sanitized_title": "editing_models_with_task_arithmetic"
        },
        {
            "paper_title": "A mathematical framework for transformer circuits",
            "rating": 1,
            "sanitized_title": "a_mathematical_framework_for_transformer_circuits"
        },
        {
            "paper_title": "Interpreting GPT: the logit lens",
            "rating": 1,
            "sanitized_title": "interpreting_gpt_the_logit_lens"
        },
        {
            "paper_title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't",
            "rating": 1,
            "sanitized_title": "analogybased_detection_of_morphological_and_semantic_relations_with_word_embeddings_what_works_and_what_doesnt"
        }
    ],
    "cost": 0.0137345,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models Implement Simple Word2Vec-style Vector Arithmetic</p>
<p>Jack Merullo jack_merullo@brown.edu 
Department of Computer Science
Brown University</p>
<p>Carsten Eickhoff carsten.eickhoff@uni-tuebingen.de 
School of Medicine
University of Tübingen</p>
<p>Ellie Pavlick ellie_pavlick@brown.edu 
Department of Computer Science
Brown University</p>
<p>Language Models Implement Simple Word2Vec-style Vector Arithmetic
9CFE618707D4D3CBDF00A0208A4D3E3F
A primary criticism towards language models (LMs) is their inscrutability.This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple vector arithmetic style mechanism to solve some relational tasks using regularities encoded in the hidden space of the model (e.g., Poland:Warsaw::China:Beijing).We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, uppercasing, and past-tensing) a key part of the mechanism reduces to a simple additive update typically applied by the feedforward (FFN) networks.We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context.Our results contribute to a growing body of work on the interpretability of LMs, and offer reason to be optimistic that, despite the massive and non-linear nature of the models, the strategies they ultimately use to solve tasks can sometimes reduce to familiar and even intuitive algorithms. 1</p>
<p>Introduction</p>
<p>The growing capabilities of large language models (LLMs) have led to an equally growing interest in understanding how such models work under the hood.Such understanding is critical for ensuring that LLMs are reliable and trustworthy once deployed.Recent work in interpretability has contributed to this understanding by reverseengineering the data structures and algorithms that are implicitly encoded in the model's weights, e.g., by identifying detailed circuits (Wang et al., 2022;Elhage et al., 2021;Olsson et al., 2022) or by identifying mechanisms for factual storage and retrieval which support intervention and editing (Geva et al., 1 Code available at: https://github.com/jmerullo/lm_vector_arithmetic 2021a; Li et al., 2022;Meng et al., 2022a,c;Dai et al., 2022).</p>
<p>Here, we contribute to this growing body of work by analyzing how LLMs recall information during in-context learning.Modern LLMs are based on a complex transformer architecture (Vaswani et al., 2017) which produces contextualized word embeddings (Peters et al., 2018;Devlin et al., 2019) connected via multiple non-linearities.Despite this, we find that LLMs implement a basic vector-addition mechanism qualitatively similar to relational information encoded in their static word embeddings predecessors Mikolov et al. (2013).We also find that for non-injective relations that static embeddings typically fail to encode (Gladkova et al., 2016), LMs do not use the identified mechanism (Appendix G).</p>
<p>We study this phenomenon across nine tasks, but focus on three in the main paper: recalling capital cities, uppercasing tokens, and past-tensing verbs.</p>
<p>Our key findings are:</p>
<p>• We find evidence of a distinct processing signature in the forward pass which characterizes argument-function processing ( §3).That is, if models need to perform the get_capital(x) function, which takes an argument x and yields an answer y, they first surface the argument x in earlier layers which enables them to apply the function and yield y as the final output (Figure 2).This signature generalizes across models and tasks, but appears to become sharper as models increase in size.</p>
<p>• We take a closer look at GPT2-Medium, and find that the vector arithmetic mechanism is often implemented by mid-to-late layer feedforward networks (FFNs) in a way that is modular and supports intervention ( §4).E.g., an FFN outputs a content-independent update which produces Warsaw given Poland and can arXiv:2305.16130v3[cs.CL] 3 Apr 2024 be patched into an unrelated context to produce Beijing given China.We don't find this evidence of this mechanism being used for tasks in which word embedding vector arithmetic classically fails (Appendix G).</p>
<p>• We demonstrate that this mechanism is specific to recalling information from pretraining memory ( §5).For settings in which the correct answer can be retrieved from the prompt, this mechanism does not appear to play any role, and FFNs can be ablated entirely with relatively minimal performance degradation.Thus, we present new evidence supporting the claim that FFNs and attention specialize for different roles, with FFNs supporting factual recall and attention copying and pasting from local context.</p>
<p>Taken together, our results offer new insights about one component of the complex algorithms that underlie in-context learning.The mechanism's simplicity raises the possibility that other apparently complicated behaviors may be supported by a sequence of simple operations under the hood.Moreover, our results suggest a distinct processing signature and hint at a method for intervention.These ideas could support future work on detecting and preventing unwanted behavior by LLMs at runtime.</p>
<p>Methods</p>
<p>In decoder-only transformer language models (Vaswani et al., 2017), a sentence is processed one word at a time, from left to right.In this paper, we focus on the transformations that the next-token prediction undergoes in order to predict the answer to some task.At each layer, an attention module and feed-forward network (FFN) module apply subsequent additive updates to this representation.Consider the FFN update at layer i, where x i is the current next-token representation.The update applied by the FFN here is calculated as
FFN( ⃗ x i ) = ⃗ o i , ⃗ x i+1 = ⃗ x i + ⃗ o i where ⃗ x i+1
is the updated token for the next layer.Due to the residual connection, the output vector ⃗ o i is added to the input.⃗ x is updated this way by the attention and FFNs until the end of the model, where the token is decoded into the vocab space with the language modeling head E: softmax(E⃗ x).From start to end, x is only updated by additive updates, forming a residual stream (Elhage et al., 2021).Thus, the token representation x i represents all of the additions made into the residual stream up to layer i.</p>
<p>Early Decoding</p>
<p>A key insight from the residual stream perspective is that we can decode the next token prediction with the LM head before it reaches the final layer.This effectively allows for "print statements" throughout the model's processing.The intuition behind this technique is that LMs incrementally update the token representation ⃗ x to build and refine an encoding of the vocabulary distribution.This technique was initially introduced in nostalgebraist (2020) as the logit lens, and Geva et al. (2022b) show that LMs do in fact refine the output distribution over the course of the model.Figure 1 illustrates the process we use to decode hidden states into the vocabulary space using the pre-trained language modeling head E. After decoding, we apply a softmax to get a probability distribution over all tokens.When we decode at some layer, we say that the most likely token in the resulting vocab distribution is currently being represented in the residual stream.We examine the evolution of these predictions over the course of the forward pass for several tasks.</p>
<p>Tasks</p>
<p>We apply early decoding to suite of in-context learning tasks to explore the transformations the next token prediction undergoes in order to predict the answer.</p>
<p>World Capitals</p>
<p>The World Capitals task requires the model to retrieve the capital city for various states and countries in a few-shot setting.The dataset we use contains 248 countries and territories.A one-shot example is shown below: "Q: What is the capital of France?A: Paris Q: What is the capital of Poland?A:___" Expected Answer: " Warsaw" Reasoning about Colored Objects We focus on a subset of 200 of the reasoning about colored objects dataset prompts (i.e., the colored objects dataset) from BIG-Bench (Srivastava et al., 2022).A list of colored common objects is given to the model before being asked about one object's color.For the purposes of this paper, we focus only on one aspect of this task-the model's ability to output the final answer in the correct format. 2he unembedding matrix projects into the vocabulary space.</p>
<p>…</p>
<p>Selected by example</p>
<p>Figure 1: When decoding the next word, additive updates are made through the residual connections of each attention/FFN sub-layer.To decode the running prediction at every layer, the pre-trained language modeling head is applied at various points in each layer as in Geva et al. (2022a); nostalgebraist (2020).The ⃗ o vector interventions we make ( §4.1) are illustrated by patching one or more FFN outputs with one from another example "Q: On the floor, I see a silver keychain, [...] and a blue cat toy.What color is the keychain?A: Silver Q: On the table, you see a brown sheet of paper, a red fidget spinner, a blue pair of sunglasses, a teal dog leash, and a gold cup.What color is the sheet of paper?A:<strong><em>" Expected answer: " Brown" Past Tense Verb Mapping Lastly, we examine whether an LM can accurately predict the past tense form of a verb given a pattern of its present tense.The dataset used is the combination of the regular and irregular partitions of the past tense linguistic mapping task in BIG-Bench (Srivastava et al., 2022).After filtering verbs in which the present and past tense forms start with the same token, we have a total of 1,567 verbs.An example one-shot example is given below: "Today I abandon.Yesterday I abandoned.Today I abolish.Yesterday I</em></strong>" Expected answer: " abolished" The above tasks could all be described as oneto-one (e.g., each country has one capital, each word only has one uppercase/past tense form).In Appendix G we explore six additional tasks, three of which are either many-to-many or many-to-one.We find that the observed mechanism only applies to one-to-one relations, indicating that the model learns some sensitivity to this type of relation in order for it to represent the structure required for the mechanism described here, similar to static embeddings (Gladkova et al., 2016)/</p>
<p>Models</p>
<p>We experiment on decoder-only transformer LMs across various sizes and pre-training corpora.When not specified, results in figures are from GPT2-medium.We also include results portraying the stages of processing signatures in the residual streams of the small, large, and extra large variants (Radford et al.), the 6B parameter GPT-J model (Wang and Komatsuzaki, 2021), and the 176B BLOOM model (Scao et al., 2022), either in the main paper or in the Appendix.</p>
<p>Stages of Processing in Predicting the Next Token</p>
<p>First, we use the early decoding method in order to investigate how the processing proceeds over the course of a forward pass to the model.Each task requires the model to infer some relation to recall some fact, e.g., retrieving the capital of Poland.In these experiments, we see several discrete stages of processing that the next token undergoes before</p>
<p>reaching the final answer.These states together provide evidence that the models "apply" the relevant functions (e.g., get_capital) abruptly at some mid-late layer to retrieve the answer.Moreover, in these cases, the model prepares the argument to this function in the layers prior to that in which the function is applied.</p>
<p>In Figure 2 we illustrate an example of the stages we observe across models.For the first several layers, we see no movement on the words of interest.Then, during Argument Formation, the model first represents the argument to the desired relation in the residual stream.This means that the top token in the vocabulary distribution at some intermediate layer(s) is the subject the question inquires about (e.g., the x, in get_capital(x)).</p>
<p>During Function Application we find that the model abruptly switches from the argument to the output of the function (the y, in get_capital(x) = y).We find that function application is typically applied by the FFN update at that layer to the residual stream.This is done by adding the output vector ⃗ o of the FFN to the residual stream representation, thus transforming it with an additive update.We study these ⃗ o vectors in detail in Section 4. Finally, the model enters Saturation3 , where the model recognizes it has solved the next token, and ceases updating the token representation for the remaining layers.</p>
<p>The trend can be characterized by an X-shaped pattern of the argument and final output tokens when plotting the ranks of the argument(x) and output (y) tokens.We refer to this behavior as argument-function processing.Figure 3 shows that this same processing signature can be observed consistently across tasks and models.Moreover, it appears to become more prominent as the models increase in size.Interestingly, despite large differences in number of layers and overall size, models tend to undergo this process at similar points proportionally in the model.</p>
<p>Implementation of Context-Independent Functions in FFN Updates</p>
<p>The above results on processing signature suggest that the models "apply" a function about 2/3rds of the way through the network with the addition of an FFN update.Here, we investigate the mechanism via which that function is applied more closely.Specifically, focusing on GPT2-Medium4 , we show that we can force the encoded function to be applied to new arguments in new contexts by isolating the responsible FFN output vector and then dropping into a forward pass on a new input.</p>
<p>⃗ o Vector Interventions</p>
<p>Consider the example in Figure 2. At layer 18, the residual stream ( ⃗ x 18 ) is in argument formation, and represents the " Poland" token.At the end of layer 19, a function is applied, transforming ⃗</p>
<p>x 19 into the answer token " Warsaw.</p>
<p>As discussed in the previous section, we can isolate the function application in this case to FFN  to some x which represents the " China" token, it will transform into " Beijing".Thus we refer to ⃗ o 19 as ⃗ o city since it retrieves the capital cities of locations stored in the residual stream.We locate such ⃗ o vectors in the uppercasing and past tense mapping tasks in the examples given in Section 2.2, which we refer to as ⃗ o upper and ⃗ o past , respectively.5We test whether these updates have the same effect, and thus implement the same function, as they do in the original contexts from which they were extracted.To do so, we replace entire FFN layers with these vectors and run new inputs through the intervened model. 6ata: We are interested in whether the captured o vectors can be applied in a novel context, in particular, to a context that is otherwise devoid of cues as to the function of interest.Thus, we synthesize a new dataset where each entry is a string of three random tokens (with leading spaces) followed by a token x which represents a potential argument to the function of interest.For example, in experiments involving o city , we might include a sequence such as table mug free China table mug free China table mug free.This input primes the model to produce "China" at the top of the residual stream, but provides no cues that the capital city is relevant, and thus allows us to isolate the effect of o city in promoting "Beijing" in the residual stream.In addition to the original categories, we also include an "out-of-domain" dataset for each task: US states and capitals, 100 non-color words, and 128 irregular verbs.These additional data test the sensitivity of the ⃗ o vectors to different types of arguments.</p>
<p>Results: Figure 4 shows results for a single example.Here, we see that "Beijing" is promoted all the way to the top of the distribution solely due to the injection of ⃗ o city into the forward pass.Figure 5 shows that this pattern holds in aggregate.In all settings, we see that the outputs of the intended functions are strongly promoted by adding the corresponding ⃗ o vectors.By the last layer, for world and state capitals, the mean reciprocal rank of the target city name across all examples improves from roughly the 10th to the 3rd-highest ranked word and 17th and 4th-ranked words respectively.The target output token becomes the top token in 21.3%, 53.5%, and 7.8% of the time in the last layer in the world capitals, uppercasing, and past tensing tasks, respectively.</p>
<p>We also see the promotion of the proper past tense verbs by ⃗ o past .The reciprocal ranks improve similarly for both regular (approx.7th to 3rd rank) and irregular verbs (approx.6th to 3rd), indicating that the relationship between tenses is encoded similarly by the model for these two types.⃗ o upper promotes the capitalized version of the test token almost every time, although the target word starts at a higher rank (on average, rank 5).These results together show that regardless of the surrounding context and the argument to which it is applied, ⃗ o vectors consistently apply the expected functions.Since each vector was originally extracted from the model's processing of a single naturalistic in-  put, this generalizability suggests cross-context abstraction within the learned embedding space.</p>
<p>Common Errors: While the above trend clearly holds on the aggregate, the intervention is not perfect for individual cases.The most common error is that the intervention has no real effect.In the in-domain (out-domain) settings, this occurred in about 37% (20%) of capital cities, 4% (5%) on uppercasing, and 19% (22%) for past tensing.We believe the rate is so much higher for world capitals because the model did not have a strong association between certain country-capital pairs from pretraining, e.g, for less frequently mentioned countries.Typically, in these cases, the top token remains the argument, but sometimes becomes some random other city, for example, predicting the capital of Armenia is Vienna.We also find that the way tokenization splits the argument and target words affects the ability of the ⃗ o vector to work and is another source of errors.This is discussed further in Appendix F.</p>
<p>The Role of FFNs in Out-of-Context Retrieval</p>
<p>So far, we have shown that FFN output vectors can encode functions that transfer across contexts.</p>
<p>Here, we investigate the role of this mechanism when we control whether the answer occurs in context.The tasks we study previously require recalling a token that does not appear in the given context (abstractive tasks).In this section we show that mid-higher layer FFNs are crucial for this process.When the answer to the question does appear in context (extractive tasks), we find that ablating a subset of FFNs has a comparatively minor effect on performance, indicating that they are relatively modular and there is a learned division of labor within the model.This observation holds across the decoder-only LMs tested in this paper.This breakdown is consistent with previous work finding that FFNs store facts learned from pre-training (Geva et al., 2021b;Meng et al., 2022b,c) and attention heads copy from the previous context (Wang et al.;Olsson et al., 2022).</p>
<p>Abstractive vs. Extractive Tasks</p>
<p>Extractive Tasks: Extractive tasks are those in which the exact tokens required to answer a prompt can be found in the input context.These tasks can thus be solved by parsing the local context alone, and thus do not necessarily require the model to apply a function of the type we have focused on in this paper (e.g., a function like get_capital).</p>
<p>Abstractive Tasks: Are those in which the answer to a prompt is not given in the input context and must be retrieved from pretraining memory.Our results suggest this is done primarily through argument-function processing, requiring function application through (typically) FFN updates as described in Section 3.</p>
<p>We provide examples with their associated GPT2-Medium layerwise decodings in Figure 7.We expect that the argument formation and function application stages of processing occur primarily in abstractive tasks.Indeed, in Appendix A, we show that the characteristic argument-answer X pattern disappears on extractive inputs.We hypothesize that applying out-of-context transformations to the predicted token representation is one of the primary functions of FFNs in the mid-to-late layers, and that removing them should only have a major effect on tasks that require out-of-context retrieval.</p>
<p>Effect of Ablating FFNs</p>
<p>Data: Consider the example shown in Section 2.2 demonstrating the ⃗ o upper function.By providing the answer to the in-context example as " Silver", the task is abstractive by requiring the in-context token " brown" to be transformed to " Brown" in the test example.However, if we provide the in-context label as " silver", the task becomes extractive, as the expected answer becomes " brown".We create an extractive version of this dataset by lowercasing the example answer.All data is presented to the model with a single example (one-shot).We repeat this experiment on the world capitals (see Figure 7), thought note that since the answer is provided explicitly, this task is much easier for the models in the extractive case.</p>
<p>Results:</p>
<p>We run the one-shot extractive and abstractive datasets on the full models, and then repeatedly remove an additional set of FFNs from the top down (e.g., in 24 layer GPT2-Medium: removing the 20-24th FFNs, then the 15-24th, etc.).Our results are shown in Figure 6.Despite the fact that the inputs in the abstractive and extractive datasets only slightly differ (by a single character in  the colored objects case) we find that performance plummets on the abstractive task as FFNs are ablated, while accuracy on the extractive task drops much more slowly.For example, even after 24 FFN sublayers are removed from Bloom (totaling 39B parameters) extractive task accuracy for the colored objects dataset drops 17% from the full model's performance, while abstractive accuracy drops 73% (down to 1% accuracy).The case is similar across model sizes and pretraining corpora; we include results on additional models in Appendix B. This indicates that we can isolate the effect of locating and retrieving out of context tokens in this setting to the FFNs.Additionally, because the model retains reasonably strong performance compared to using the full model, we do not find convincing evidence that the later layer FFNs are contributing to the extractive task performance, supporting the idea of modularity within the network.</p>
<p>Related Work</p>
<p>Attributing roles to components in pretrained LMs is a widely studied topic.In particular, the attention layers (Olsson et al., 2022;Kobayashi et al., 2020;Wang et al.) and in the FFN modules, which are frequently associated with factual recall and knowledge storage (Geva et al., 2021b;Meng et al., 2022a,c).How language models store and use knowledge has been studied more generally as well (Petroni et al., 2019;Cao et al., 2021;Dai et al., 2022;Bouraoui et al., 2019;Burns et al., 2022;Dalvi et al., 2022;Da et al., 2021) as well as in static embeddings (Dufter et al., 2021).Recent work in mechanistic interpretability aims to fully reverse engineer how LMs perform some behaviors (Elhage et al., 2021).Our work builds on the finding that FFN layers promote concepts in the vocabulary space (Geva et al., 2022a) by breaking down the process the model uses to do this in context; Bansal et al. (2022) perform ablation studies to test the importance of attention and FFN layers on in-context learning tasks.Other work analyze information flow within an LM to study how representations are built through the layers, finding discrete processing stages (Voita et al., 2019;Tenney et al., 2019).We also follow this approach, but our analysis focuses on interpreting how models use individual updates within the forward pass, rather than probing for information encoded within some representation.Ilharco et al. (2023) show that vector arithmetic can be performed with the weights of finetuned models to compose tasks, similar to how ⃗ o vectors can induce functions in the activation space of the model.</p>
<p>Discussion &amp; Conclusion</p>
<p>A core challenge in interpreting neural networks is determining whether the information attributed to certain model components is actually used for that purpose during inference (Hase and Bansal, 2022;Leavitt and Morcos, 2020).While previous work has implicated FFNs in recalling factual associations (Geva et al., 2022a;Meng et al., 2022a), we show through intervention experiments that we can manipulate the information flowing through the model according to these stages.This process provides a simple explanation for the internal subprocesses used by LMs and our findings invite future work aimed at understanding why, and under what conditions, LMs learn to use this mechanism when they are capable of solving such tasks using, e.g., adhoc memorization.</p>
<p>The mechanism we identify bears similarities to linguistic regularities that allow for vector arithmetic analogies in static word embeddings (Mikolov et al., 2013) suggesting at least a qualitative similarity between large complex contextual models and these simpler static models.Gladkova et al. (2016) show that not all relations can be encoded with vector arithmetic analogies, specifically, relations that are not one-to-one (e.g., mapping a country to its official language).In Appendix G we find evidence that LMs exhibit similar success and failure cases by analyzing six additional tasks.We provide our most detailed investigation on GPT2-Medium, which clearly illustrates the phenomenon.Our experiments on stages of processing with GPT-J suggest that the same phenomena is in play, although (as discussed in Section 4 and Appendix A), the procedures we derive for interventions on GPT2-Medium do not transfer perfectly.Specifically, we can strongly reproduce the intervention results on uppercasing for GPT-J; results on the other two tasks are positive but with overall weaker effects.As we understand these processes more deeply, a priority in future work must be to generalize specific findings to model-agnostic phenomena.That said, in this work and other similar efforts, a single positive example as a proof of concept is often sufficient to advance understanding and spur future work that improves robustness across models.</p>
<p>Contemporaneous work (Geva et al., 2023) has studied a different mechanism for factual recall in LMs, but it is unclear how and when these mechanisms interact.Eventually, if we can understand how models break down complex problems into simple and predictable subprocesses, we can help more readily audit their behavior.Interpreting the processing signatures of model behaviors might offer an avenue via which to evaluate and intervene at runtime in order to prevent unwanted behavior.</p>
<p>A Argument-Function Processing in Other Models</p>
<p>In Section 3 we show that GPT2-Medium and Bloom promote the in-context 'argument' token to some function before promoting the answer to that function.In figure 8we show that this effect is present across other models as well in the three tasks we test.Qualitatively, we find that the pattern is more prominent in models that have more layers, likely because we are able to get more measurements after the FFN updates, so it is less likely that entire argument formation stage happens within a single layer (i.e., after the attention module update -we only take measurements after the FFN update for simplicity).In the extractive task setting, we would not expect the model to go through argument-function processing in order to reach the prediction, since it already appears in context (although this does not preclude it from doing so -it is still a valid way to retrieve the required information).We see that this X shaped pattern disappears when we plot the argument-answer curves for the extractive world capitals data, as shown next to the abstractive setting in Figure 9.We repeat the random tokens task on GPT-J using the same stimuli as in the main paper to select ⃗ o vectors.We find that we can locate ⃗ o vectors occurring in other models, however the success rate varies for the tasks that we evaluate in this work.Results are shown in Figure 10.Although the uppercasing function works very well, we get weaker responses for the past tense and world capitals mappings.One explanation could be that these tasks are not solved with an as-general solution as in GPT2, but the process for carrying out this intervention depends on hyperparameters which are often model-specific (i.e., the exact layer at which to perform the intervention), so future work is needed to understand where differences between these models lie.</p>
<p>B Additional Results on Ablating FFNs</p>
<p>We include the results for all six models we test for the FFN ablation study for both the colored objects task (Figure 11) and the world capitals task (Figure 12).We find that the trend of abstractive performance dropping off far before extractive performance is reflected across all models.</p>
<p>B.1 +/-o case Intervention on Colors</p>
<p>As described in the main paper, adding o case to the residual stream (x 19 + o case ) has the effect of capitalizing the first letter in the word 'brown'.Similar to the results in Sections 2.2 and 2.2, we find that adding o case to the residual stream has the effect of uppercasing the token prediction on arbitrary contextualized representations in the mid layers of GPT2-Medium.However, we also find that lowercasing the first letter can be accomplished by subtracting it.Qualitatively, this works much the same way as adding the ⃗ o vectors previously discussed.We show this effect empirically, by showing the difference between replacing the FFN updates in GPT2-Medium with either positive or negative o c ase (having the effect of adding or subtracting from the residual stream).</p>
<p>We progressively remove FFNs from the top of the model, and show the effect of adding or subtracting o case in Figure 13.In the abstractive case, we find that accuracy is greatly boosted when adding o case which we identify as implementing an uppercasing function, and reflects the results in Sections 2.2 and 2.2.We find that we can replace the top third of GPT2-Medium FFN layers (FFNs in layers 16-24, around 20% of all parameters) with +o case to gain 25% in total accuracy (from 4.5% to 29.5%) and recovering to 72% of the performance of the un-ablated model (41%).Conversely, if we subtract o case in the abstractive setting to encourage lowercasing (i.e., encouraging the model to output a lowercased answer when the answer it should have a capital first letter), the model immediately hits 0% performance.We see the opposite effect in the extractive setting, where adding ocase hurts performance to a greater degree than subtracting it.According to our results presented so far, we would expect FFNs to be unnecessary for solving the extractive dataset examples, which is possibly why performance is degraded in both cases we intervene, but we don't test this idea in this work.</p>
<p>C What are the Attention Heads Doing?</p>
<p>We focus on the outputs of the FFN layers in this work, but that is not to say that the attention heads are not contributing to the final answer.As shown in Section 5, the attention layers are able to get the final answer when it already appears explicitly in context (when it's extractive).This leads to a possible explanation for why LMs learn to implement argument-function processing.We speculate that this process may be the result of a natural progression in training.When the argument token needs to be transformed ("brown" to "Brown"), the model notices that it is the subject of the next token, and uses attention heads to copy the value of that token into the next token prediction.This operation could be done using mover heads (Wang et al., 2022;Merullo et al., 2023) or induction heads (Olsson et al., 2022).In the following layers, the model Figure 9: The 'X' pattern of argument and answer tokens crossing in the course of the forward pass is the characteristic pattern in argument-function processing.In the main text, we show how the models we test use this type of processing to recall the capital cities of locations.When we make the task extractive (by including the correct capital in the given context), the model does not have to setup an argument and function in order to get the answer, and the pattern disappears.This highlights the differences we describe in processing extractive and abstractive tasks.Both datasets are filtered for examples where the models were correct.transforms this representation into the final output.When subject enrichment (Geva et al., 2023) is not possible, these same pseudo mover heads would then copy the unenriched subjects (i.e., the regular argument tokens).In these cases, the model would have to apply the function after already copying it over, creating the argument-processing signature.Future work is needed to see if it is possible to unify these different interpretations and perspectives.</p>
<p>D Effect on Zero-shot Performance</p>
<p>We find that intervening on the model with ⃗ o vectors has applications in controllable generation, that is, guiding the generation process towards some relevant text.We showed this was the case in Section 4, but we can also apply this idea to the context of zero-shot learning.When we provide in-context examples, we are also providing the output format of the prompt.Consider the example "Q: What is the capital of Poland?A:".unlike the one shot example given in Figure 2, there is no indication that the next word should be " Warsaw" over continuing the generation as a complete sentence "The capital of Poland is Warsaw", which is what GPT2-Medium actually generates.If we decode at every layer, as is shown in to guide the generation to the expected response of immediately generating the capital.We can perform this experiment on the past tensing task as well.Results on the zero-shot tasks are shown in Figure 14.We find that on the world capitals task, we can greatly improve the propensity of the model to output the expected answer by performing an ⃗ o vector intervention, improving zero-shot performance from 5.6% to 33.0%.On the past tense mapping task, where perhaps the output format is more obvious from the prompt, the zero and one shot performances are about equal, but we still see a modest improvement over the one shot results of about 4.2%.Although the tasks are very simple, we achieve this by effectively ablating FFN layers (layers 19-23) and precomputing their activations, suggesting it might be possible to edit models extensively to limit their expressiveness to one type of output while also making them more efficient.</p>
<p>We are optimistic about future work in this area.</p>
<p>E Effect of Layer Choice on Intervention Results</p>
<p>In the main text, we replace FFNs starting at either layer 18 or 19 GPT2-Medium to the end (indexed at 0).We find that intervening on only one layer promotes the output token, but not to the top of the distribution.One possibility is that the model makes gradual updates that are pushing the token representation in generally the same direction (Jastrzebski et al., 2017).In Figure 15, we show that adding any of the ⃗ o vector interventions at any single layer at 18 or afterwards, there is a roughly equivalent increase to the average reciprocal rank of the target word.The logit difference between the argument and answer token (in the logits of each early-decoded layer) shows this as well as a gradual increase.This is exemplified in Figure 2 in the main paper.</p>
<p>F Effect of Tokenization on the Effectiveness of ⃗ o Vectors</p>
<p>The tokenizer can split one word into multiple subtokens, such as "Purple" into the tokens "Pur" and "ple".This occurs with words that were less frequent in the training data.We find that this process has a generally negative effect on the performance of the intervention we perform.Intuitively, if we are trying to use ⃗ o upper to capitalize the "purple" token into "Purple", it must map from "purple" (one token) to "Pur".It seems less obvious, then,</p>
<p>Layer Reciprocal Rank</p>
<p>Figure 15: Replacing any individual FFN update is worse than replacing all of them.This supports the idea that networks made gradual updates to their representations, and that the ⃗ o vectors we extract behave this way as well: multiple similar updates are made k layers in a row.Interestingly, the average boost to the reciprocal rank is about the same regardless of which single layer we apply the update at, suggesting that this range of FFNs are operating in same space.that the embeddings would encode a linear relationship between these two, since "Pur" is a subtoken in many other words.We explore this specific phenomenon on the random tokens task from Section 4 with the ⃗ o upper intervention.We take 100 single token words that capitalize to a single token, and 100 others that capitalize to words that break down into multiple tokens.Our results can be seen in Figure 16.We find that tokens that get broken up into multiple tokens are less probable than for tokens that capitalize to single token forms.</p>
<p>G Additional Tasks: One-to-One, Many-to-One, and Many-to-Many Relations</p>
<p>In the main paper, we show study three one-to-one relations that exhibit the argument/output pattern, but it remains unclear how well this generalizes to other relations.Using six additional tasks, three many-to-X and three new one-to-one, we provide evidence that suggests that the observed mechanism is specific to one-to-one relations, and does not work when multiple inputs map to one output.This suggests that the model is sensitive to this distinction of relations during pretraining, and the vector arithmetic mechanism structure we observe only presents for the most explicit relations.In Table 2, we give examples of the six new tasks, following the same prompt format as the one used in the main paper.In Table 3, we break down the relation type of each task and provide the GPT2-Medium accuracy for each one.Figure 17 shows the early decoding patterns for the argument and answer tokens.While the three one-to-one tasks exhibit the initial promotion of the argument token, followed by the answer token on average, the argument token does not become highly promoted on any of the non one-to-one relations.</p>
<p>H Compute</p>
<p>All models were run on NVidia RTX 3090s; Bloom was run locally on 3090s in float16 with CPU offloading.Figure 18: For the first two tasks, the average argument-answer spike pattern is similar to the other one-to-one tasks in which the vector arithmetic analogy held.The results for noun plurals are mostly negative as it appears the model uses argument-function processing only some of the time.We will expand on this in the camera ready paper.</p>
<p>residual connection, Attention and FFN blocks can be viewed as reading from the residual stream and adding their</p>
<p>Figure 3 :
3
Figure3: Argument formation and function application is characterized by a promotion of the argument (red) followed by it being replaced with the answer token (blue), forming an X when plotting reciprocal ranks.Across the three tasks we evaluate, we see that most of the models exhibit these traces, and despite the major differences in model depths, the stages occur at similar points in the models.Data shown is filtered by examples in which the models got the correct answer.</p>
<p>Figure 4 :
4
Figure 4: The gray area indicates layers with the FFN intervention.Even if the input context is nonsense (repeating pattern), when "China" is represented in the residual stream, the ⃗ o city vector promotes the correct capital city.</p>
<p>Figure 5 :
5
Figure5: We intervene on GPT2-Medium's forward pass while it is predicting the completion of a pattern.The control indicates normal model execution, while the gray boxes indicate which FFNs are replaced with our selected ⃗ o vectors.We can see a significant increase in the reciprocal rank of the output of the function implemented by the ⃗ o vector used even though the context is completely absent of any indication of the original task.</p>
<p>Figure 6 :
6
Figure6: Removing FFNs negatively affects performance when the task is abstractive: the in-context label is an out-of-context transformation of the in-context prompt (e.g., " silver" in context, answer given as " Silver").In comparison, on the extractive dataset, performance is robust to a large proportion of FFNs being removed.Other models tested are shown in Appendix B</p>
<p>Figure 7 :
7
Figure7: The abstractive task undergoes argument formation and function application, while the extractive task immediately saturates (yellow).Layers 0-11 decode as nonsense and are omitted for brevity.</p>
<p>Figure 10 :Figure 11 :
1011
Figure 10: We use the same stimuli to extract ⃗ o vectors on GPT-J.Results are similar for the uppercasing function, but only very weakly positive on the world capitals task.</p>
<p>Figure 12 :
12
Figure 12: Results of removing FFN sublayers for the world capitals task for all models.</p>
<p>Figure 13 :
13
Figure13: Replacing FFN updates with +o case helps recover accuracy in abstractive tasks where the answer is expected to be uppercase compared to subtracting it or ablating the FFNs.In extractive tasks, the task is primarily solved by attention modules and adding or subtracting o case only hurts performance.</p>
<p>Figure 14 :
14
Figure 14: By replacing FFN networks with the corresponding ⃗ o vectors, we show that we can improve zero-shot performance by taking advantage of the model going through argument formation in the zero-shot setting.</p>
<p>Figure16: When the uppercase version of a word gets broken down into multiple subtokens, mapping to that token becomes much less probable and is generally harder of an association for the model to make.</p>
<p>Figure 17 :
17
Figure17: Non-injective tasks show no evidence of argument-function processing on average.In sharp contrast to the past tense, colored objects, capital cities, and un-adj.tasks where this is observed, here, the argument token experiences virtually no spike in reciprocal rank in the intermediate layers.</p>
<p>19; let x19 represent the residual stream after the attention update, but before the FFN update at layer 19 (which still represents Poland).Recall that the update made by FFN 19 is written FFN 19 ( x19 ) = ⃗ o 19 and ⃗ x 19 = x19 + ⃗ o 19 .We find that ⃗ o 19 will apply the get_capital function regardless of the content of x19 .For example, if we add ⃗ o 19</p>
<p>Table 1 :
1
Table 1 we can see that the model still goes through argument formation despite preferring to generate the full sentence.We can take advantage of this behavior by replacing the FFN layers in the later layers with ⃗ o city in order These are the top tokens per layer in GPT2-Medium on the example zero-shot Poland example.
Layer Top Token(AAAAAAAATheTheTheTheTheTheTheTheThePolandPolandPolandPolandPolandThe</p>
<p>The anaconda is a kind of what?\nA: (snake/reptile/boa/...) Name to Nationality ...Q: What is the nationality of Balzac?\nA: (French) Country to Language ...Q: What is the official language of Argentina?\nA:(Spanish) Adj. to un-Adj....Q: What is the opposite of able?\nA: (unable) 3rd Person Verbs ...Q: What is the third person singular of become?\nA: (becomes) Noun Plurals ...Q: What is the plural of album?\nA: albums
TaskExampleAnimal Hypernyms...Q:</p>
<p>Table 2 :
2
Examples from three non-injective and one injective relation.A given animal (anaconda) is a type of snake and reptile, and other snakes/reptiles also exist (many-to-many).Balzac is only French and other people map to French (many-to-one), etc.
TaskAccuracy (%)Task TypeAnimal Hypernyms30.4±1.7Many-to-ManyName to Nationality73.2±2.0Many-to-OneCountry to Language71.2±2.4Many-to-ManyAdj. to un-Adj.12.0±1.1One-to-One3rd Person Verbs22.4±0.7One-to-OneNoun Plurals51.6±1.7One-to-One</p>
<p>Table 3 :
3
One-shot accuracies for each task across 5 random seeds for GPT2-Medium.</p>
<p>The reason for this is that most of the results in this paper were originally observed as incidental findings while studying the Colored Objects task more generally. We thus zoom in on this one component for the purposes of the mechanism studied here, acknowledging that the full task involves many other steps that will no doubt involve other types of mechanisms.
Saturation events are described inGeva et al. (2022a) where detection of such events is used to "early-exit" out of the forward pass
We focus on one model because manual analysis was required in order to determine how to perform the intervention. See Appendix for results on GPT-J and Section 7 for discussion.
  5  In Appendix A, we extend these results to GPT-J, for which the same procedure leads to strong effects on uppercasing, but smaller overall positive effects on capital cities and past tensing (see Section
7).6  Which FFNs to replace is a hyperparameter; we find that replacing layers 18-23 in GPT2-Medium leads to good results. It also appears necessary to replace multiple FFNs at a time. See additional experiments in Appendix E. It is likely that the ⃗ o vectors are added over the course of several layers, consistent with the idea gradual updates fromJastrzebski et al. (2017).
World Capitals TaskMapping to UppercaseMapping Verbs to Past TenseFigure8: Across several model architectures and tasks, we find evidence that on average, the argument (which appears in context) rises to the top of the vocab distribution before crossing with the answer to the task.We describe this as argument-function processing where the argument to some function is represented in the residual stream before some update from the model is added to it to produce the output of that function.Qualitatively, we observe that models with more layers display this pattern more prominently.
Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale. Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati, Katrin Kirchhoff, Dan Roth, 10.48550/arXiv.2212.09095ArXiv:2212.090952022</p>
<p>. Zied Bouraoui, Jose Camacho-Collados, Steven Schockaert, 2019Inducing Relational Knowledge from BERT</p>
<p>Discovering Latent Knowledge in Language Models Without Supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, ArXiv:2212.038272022</p>
<p>Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases. Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, Jin Xu, 10.18653/v1/2021.acl-long.146Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut, Analyzing Commonsense Emergence in Few-shot Knowledge Models. 2021</p>
<p>Knowledge neurons in pretrained transformers. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Discovering Latent Concepts Learned in BERT. Fahim Dalvi, Abdul Rafae Khan, Firoj Alam, Nadir Durrani, Jia Xu, Hassan Sajjad, ArXiv:2205.072372022</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Static Embeddings as Efficient Knowledge Bases?. Philipp Dufter, Nora Kassner, Hinrich Schütze, 10.18653/v1/2021.naacl-main.186Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>A mathematical framework for transformer circuits. Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, 2021Transformer Circuits Thread</p>
<p>Dissecting recall of factual associations in auto-regressive language models. Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson, 2023</p>
<p>Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. Mor Geva, Avi Caciularu, Kevin Ro Wang, Yoav Goldberg, arXiv:2203.146802022aarXiv preprint</p>
<p>Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space. Mor Geva, Avi Caciularu, Kevin Ro Wang, Yoav Goldberg, 10.48550/arXiv.2203.14680ArXiv:2203.146802022b</p>
<p>Transformer Feed-Forward Layers Are Key-Value Memories. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy, 10.18653/v1/2021.emnlp-main.446Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021aOnline and Punta Cana</p>
<p>Transformer feed-forward layers are key-value memories. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021b</p>
<p>Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't. Anna Gladkova, Aleksandr Drozd, Satoshi Matsuoka, 10.18653/v1/N16-2002Proceedings of the NAACL Student Research Workshop. the NAACL Student Research WorkshopSan Diego, CaliforniaAssociation for Computational Linguistics2016</p>
<p>When can models learn from explanations? a formal framework for understanding the roles of explanation data. Peter Hase, Mohit Bansal, 10.18653/v1/2022.lnls-1.4Proceedings of the First Workshop on Learning with Natural Language Supervision. the First Workshop on Learning with Natural Language SupervisionDublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Editing models with task arithmetic. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, Ali Farhadi, 2023ICLR</p>
<p>Residual connections encourage iterative inference. Stanisław Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Che Tong, Yoshua Bengio, International Conference on Learning Representations. 2017</p>
<p>Attention is not only a weight: Analyzing transformers with vector norms. Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui, 10.18653/v1/2020.emnlp-main.574Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>L Matthew, Ari Leavitt, Morcos, arXiv:2010.12016Towards falsifiable interpretability research. 2020arXiv preprint</p>
<p>Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, arXiv:2210.13382Emergent world representations: Exploring a sequence model trained on a synthetic task. 2022arXiv preprint</p>
<p>Locating and Editing Factual Associations in GPT. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, 10.48550/arXiv.2202.05262ArXiv:2202.05262 [cs] version: 42022a</p>
<p>Locating and editing factual associations in gpt. Kevin Meng, David Bau, Alex J Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. 2022b</p>
<p>Kevin Meng, Sen Arnab, Alex Sharma, Yonatan Andonian, David Belinkov, Bau, arXiv:2210.07229Mass editing memory in a transformer. 2022carXiv preprint</p>
<p>Circuit component reuse across tasks in transformer language models. Jack Merullo, Carsten Eickhoff, Ellie Pavlick, 2023</p>
<p>Linguistic regularities in continuous space word representations. Tomáš Mikolov, Wen-Tau Yih, Geoffrey Zweig, Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies. the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies2013</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, interpreting GPT: the logit lens. Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam Mccandlish, Chris Olah, 2020. 2022context learning and induction heads. Transformer Circuits Thread</p>
<p>Deep contextualized word representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, 10.18653/v1/N18-1202Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>Language Models as Knowledge Bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, </p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, arXiv:2211.05100Bloom: A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Bert rediscovers the classical nlp pipeline. Ian Tenney, Dipanjan Das, Ellie Pavlick, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives. Elena Voita, Rico Sennrich, Ivan Titov, 10.18653/v1/D19-1448Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Ben Wang, Aran Komatsuzaki, GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. 2021</p>
<p>Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, 10.48550/arXiv.2211.00593ArXiv:2211.005932022</p>
<p>Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. Kevin Ro, Wang , Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, NeurIPS ML Safety Workshop. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2242 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2242</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2242</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-c0fe02247120f6cc8931fb4bb2727c5be7a8790d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c0fe02247120f6cc8931fb4bb2727c5be7a8790d" target="_blank">Self-Controlled Dynamic Expansion Model for Continual Learning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> An innovative Self-Controlled Dynamic Expansion Model (SCDEM) is introduced, which orchestrates multiple distinct trainable pre-trained ViT backbones to furnish diverse and semantically enriched representations and presents a novel Dynamic Layer-Wise Feature Attention Mechanism (DLWFAM) to autonomously determine the penalization intensity on each trainable representation layer.</p>
                <p><strong>Paper Abstract:</strong> Continual Learning (CL) epitomizes an advanced training paradigm wherein prior data samples remain inaccessible during the acquisition of new tasks. Numerous investigations have delved into leveraging a pre-trained Vision Transformer (ViT) to enhance model efficacy in continual learning. Nonetheless, these approaches typically utilize a singular, static backbone, which inadequately adapts to novel tasks, particularly when engaging with diverse data domains, due to a substantial number of inactive parameters. This paper addresses this limitation by introducing an innovative Self-Controlled Dynamic Expansion Model (SCDEM), which orchestrates multiple distinct trainable pre-trained ViT backbones to furnish diverse and semantically enriched representations. Specifically, by employing the multi-backbone architecture as a shared module, the proposed SCDEM dynamically generates a new expert with minimal parameters to accommodate a new task. A novel Collaborative Optimization Mechanism (COM) is introduced to synergistically optimize multiple backbones by harnessing prediction signals from historical experts, thereby facilitating new task learning without erasing previously acquired knowledge. Additionally, a novel Feature Distribution Consistency (FDC) approach is proposed to align semantic similarity between previously and currently learned representations through an optimal transport distance-based mechanism, effectively mitigating negative knowledge transfer effects. Furthermore, to alleviate over-regularization challenges, this paper presents a novel Dynamic Layer-Wise Feature Attention Mechanism (DLWFAM) to autonomously determine the penalization intensity on each trainable representation layer. An extensive series of experiments have been conducted to evaluate the proposed methodology's efficacy, with empirical results corroborating that the approach attains state-of-the-art performance.</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2242.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2242.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCDEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Controlled Dynamic Expansion Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continual-learning architecture that maintains multiple pretrained ViT backbones and dynamically instantiates small task-specific experts; it adaptively fine-tunes only the last L layers of each backbone and uses layer-wise attention, KL distillation (COM) and Wasserstein feature-alignment (FDC) to preserve prior knowledge while enabling plasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SCDEM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-pretrained-ViT backbone mixture + dynamic expert creation per task: concatenates class-token outputs from t' pretrained ViTs to form an augmented representation, creates a small adaptive module f_xi_j and linear classifier f_omega_j per task, selectively fine-tunes last L layers of each backbone; includes Collaborative Optimization Mechanism (KL distillation against frozen snapshots), Feature Distribution Consistency (Wasserstein distance between fused features), and Dynamic Layer-Wise Feature Attention (selector g_phi) to weight layers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>42.27M (SCDEM^2 reported in paper Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Dynamic expansion: multiple pretrained backbones whose class-token outputs are concatenated into an augmented feature; dynamically-created task-specific expert modules (adaptive projector + classifier); selector network (DLWFAM) provides adaptive layer-wise weighting; COM (KL) and FDC (Wasserstein) regularize representations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continual image classification across multiple domains (TinyImageNet, CIFAR-10, CIFAR-100, Birds525 species) in class-IL / task-IL and multi-domain sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Multiple reported metrics: e.g. Tiny-Cifar10-Birds (Table 2) SCDEM^2 Average 97.16 ± 0.06, Last 99.81 ± 0.09; SCDEM^3 Average 97.83 ± 0.38, Last 99.50 ± 0.44. TinyImageNet class-IL (Table 3) SCDEM^2: 5-step Avg 92.48, Last 90.20; 10-step Avg 94.02, Last 92.00. Ablation Table 5: Tiny-ImageNet Avg 94.94, CIFAR100 Avg 94.23.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>ViT_1 + ViT_2 concatenation baseline (Table 5): Tiny-ImageNet Avg 93.91, CIFAR100 Avg 92.48; DER++ (replay baseline, Table 2) e.g. Tiny-Cifar10-Birds Avg 94.77 ± 0.20 — generally lower average retention than SCDEM in many reported multi-domain configs.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Reported for SCDEM^2 (Table 6): Params 42.27M, GPU Avg 5330 MiB, Iteration speed 4.71 it/s, Task Time 71.05 s (Tiny-Birds scenario, RTX 4090, averaged over runs).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>StarPrompt (Table 6): Params 86.41M, GPU Avg 10112 MiB, 2.49 it/s, Task Time 424.19 s; DER++ (Table 6): Params 42.27M, GPU 3490 MiB, 3.22 it/s, Task Time 110.5 s. SCDEM reports ~51% fewer parameters, ~47% less GPU usage and much faster training time vs StarPrompt (paper claims these reductions).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Paper reports strong generalization across heterogeneous domain sequences and that adding a third suitable backbone (SCDEM^3) consistently improves performance over dual-backbone SCDEM^2 (examples: SCDEM^2 vs SCDEM^3 in Table 2 show small but consistent gains); overall SCDEM attains SOTA average accuracy across the evaluated multi-domain task sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated across many multi-domain sequences; SCDEM attains higher 'Average' (mean over tasks) and competitive 'Last' (final-task) accuracy compared with baselines — e.g. Table 2/1/3 show SCDEM often top in Average across 2/3/4-domain setups and class-IL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Table 6 shows SCDEM^2 achieves modest parameter count (42.27M) and moderate GPU usage (5330 MiB) with high iteration throughput (4.71 it/s) and much lower task training time (71.05 s) vs heavy prompt-based StarPrompt (86.41M params, 10112 MiB, 424.19 s).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Dynamically allocating small task-specific experts on top of multiple pretrained backbones and using adaptive layer-wise fusion plus distributional alignment yields higher multi-task (average) continual-learning performance and faster, more parameter-efficient training than several single-backbone or prompt-based baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Empirical results show task-specific dynamic experts and adaptive layer-wise weighting improve average continual-learning performance and efficiency relative to uniform single-backbone baselines, supporting the value of task-aligned abstractions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2242.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2242.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoE-2E/1R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture-of-Experts (as evaluated: MoE-2E/1R)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mixture-of-experts style model evaluated by the paper that dynamically activates a subset of experts per input/task via routing/gating; included as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Boosting continual learning of vision-language models via mixture-ofexperts adapters</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoE-2E/1R</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture-of-Experts architecture where a gating/router activates certain expert modules for inputs/tasks; intended to allocate computation adaptively across experts rather than using a uniform representation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>64.05M (MoE-22E reported in Table 6; the paper's MoE baseline is the evaluated variant)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Mixture-of-experts with dynamic expert activation (routing/gating) to allocate experts per input/task.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continual image classification across multi-domain sequences (same benchmarks as other comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Reported poor mean retention (Average) across multi-domain sequences: e.g. Table 1 TinyImage-Birds Average 26.7 ± 0.85, Last 76.0 ± 0.31; Table 2 Tiny-Cifar10-Birds Average 31.22 ± 0.36, Last 92.00 ± 0.41. Generally high Last but low Average indicates good current-task performance but poor retention across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Uniform/replay baselines (DER++, StarPrompt) report much higher Average scores (e.g., DER++ Avg ~94.7 in some configs; StarPrompt Avg ~97.7 in Table 2), showing MoE's average retention was substantially lower in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Table 6: MoE-22E Params 64.05M, GPU Avg 21362 MiB, Iteration speed 1.93 it/s, Task Time 266.65 s (much larger GPU footprint and slower iteration rate than SCDEM).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Compared to SCDEM^2 (42.27M params, 5330 MiB, 4.71 it/s, 71.05 s), MoE uses more parameters and much more GPU memory and is slower.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Paper notes MoE-style baselines tend to exhibit high performance on the current task (Last) but poor Average performance across tasks, indicating limited transfer/retention in these multi-domain continual-learning setups.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Poor average (multi-task) performance despite strong single-task (Last) numbers; dynamic expert allocation did not translate into improved continual retention in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>High GPU memory use (21.3 GiB in reported experiment) and slower throughput compared to SCDEM and several baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Although MoE provides dynamic allocation of experts, in the evaluated multi-domain continual-learning settings MoE baselines yielded strong current-task accuracy but very poor average retention and had larger resource costs.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>MoE implements dynamic task-aligned allocation (theory-supporting), but in these experiments it failed to improve multi-task retention and was resource-intensive, producing mixed evidence for the practical benefits of dynamic expert allocation in continual learning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2242.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2242.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StarPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Residual Prompts (StarPrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based continual-learning method that injects learned prompts to adapt pretrained models for new tasks while aiming to preserve previous knowledge; evaluated as a strong baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Semantic Residual Prompts for Continual Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>StarPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-based, parameter-efficient continual-learning approach that maintains prompt vectors (task-specific) to steer a pretrained backbone for each task, reducing the need to modify backbone weights.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>86.41M (reported in Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Prompt injection / prompt tuning: task-specific prompt vectors appended/inserted to the model to induce task-aligned representations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continual image classification across multiple datasets/domains (TinyImageNet, CIFAR variants, Birds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>High performance reported in multiple configs: e.g., Table 2 Tiny-Cifar10-Birds Avg 97.70 ± 0.63, Last 99.12 ± 0.77; Table 3 TinyImageNet class-IL 5-step Avg 87.99, Last 86.10.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>SCDEM variants sometimes match or exceed StarPrompt on Average (e.g., SCDEM^2 94.94 Tiny-ImageNet avg vs StarPrompt lower in some class-IL configs), but direct numbers depend on configuration; StarPrompt is often among the top baselines for Last and sometimes Average metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Table 6: Params 86.41M, GPU Avg 10112 MiB, Iteration 2.49 it/s, Task Time 424.19 s — higher parameter and GPU footprint and much slower training time vs SCDEM^2.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>SCDEM^2: 42.27M params, 5330 MiB GPU, 4.71 it/s, 71.05 s; StarPrompt uses roughly double params and GPU and much longer runtime in the reported scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Strong performance in many benchmarks indicates good within-domain adaptation; comparative multi-domain generalization is task-dependent — SCDEM shows improved average retention in some multi-domain settings while StarPrompt performs competitively.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>High Last scores and competitive Average across many tasks; prompt-based task-specific representations help retain performance but come at higher resource cost.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Relatively heavy in both parameters and GPU usage (see Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Prompt-based, task-specific representations (StarPrompt) are effective and often achieve strong accuracy, but they tend to be more resource-heavy than SCDEM and do not uniformly dominate multi-domain average retention in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>StarPrompt's strong performance supports that task-aligned prompt representations can be beneficial; the paper shows alternative task-aligned mechanisms (SCDEM) can match or improve performance with lower resource cost.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2242.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2242.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generating instance-level prompts for rehearsal-free continual learning (Dap)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instance-level prompt generation method that creates prompts per instance to adapt a pretrained model for continual learning without rehearsal memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating instance-level prompts for rehearsal-free continual learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dap</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Rehearsal-free continual-learning approach that generates instance-level prompts enabling parameter-efficient adaptation of pretrained backbones to new tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.68M (reported in Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Instance-level prompt generation (per-example/task prompt vectors) to produce task- or instance-aligned representations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continual image classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Table 2 examples: Tiny-Cifar10-Birds Avg 94.48 ± 0.51, Last 92.65 ± 0.45; other configs vary but generally lower Average than top-performing SCDEM variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>SCDEM^2 and StarPrompt report higher Average scores in many configs (SCDEM^2 e.g. 97.16 in related config), indicating Dap trades some accuracy for high parameter efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Table 6: Params 0.68M, GPU Avg 4420 MiB, Iteration 2.33 it/s, Task Time 147.08 s — very small parameter count but slower iteration rate and longer time than SCDEM in the reported scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>SCDEM^2 larger params but faster iteration and much lower task time (71.05 s).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Parameter-efficient but tends to underperform SCDEM and top prompt-based methods in Average multi-task retention in the paper's multi-domain benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Extremely low parameter count, moderate GPU usage; presents a favorable tradeoff for memory-constrained deployment at some accuracy cost.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Instance-level prompt adaptation is highly parameter-efficient and rehearsal-free but delivers lower average continual-learning accuracy than SCDEM and some other baselines in multi-domain experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Dap demonstrates that task/instance-aligned prompt representations can be an effective parameter-efficient route to continual adaptation, supporting the utility of task-aligned abstractions even when resource-limited.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2242.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2242.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RanPac</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RanPac</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that combines random projections with pretrained models for continual learning; emphasizes simplicity and leverage of pretrained representations rather than dynamic task-specific allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ranpac: Random projections and pre-trained models for continual learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RanPac</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses random projections coupled with pretrained backbones to produce features used for continual learning, relying less on dynamically allocated task-specific components.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.49M (reported in Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Random projections of pretrained features (non-adaptive, uniform feature processing rather than explicit task-specific allocation).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continual image classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Table 2 example: Tiny-Cifar10-Birds Avg 93.92 ± 0.48, Last 91.10 ± 0.35; class-IL Table 3 shows RanPac competitive on some class-IL configs (e.g., TinyImageNet 5-step Avg 72.81).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Comparable to some uniform baselines; generally lower Average than SCDEM but competitive in certain class-IL settings.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Table 6: Params 1.49M, GPU Avg 3566 MiB, Iteration 3.44 it/s, Task Time 250.82 s.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Very low parameter count but slower overall task time than SCDEM^2 in the reported scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Moderate multi-task performance; simpler uniform/random-projection approach sometimes competitive but usually below best task-aligned methods.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Low parameter footprint and modest GPU usage; tradeoff between simplicity and peak average accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Random projection plus pretrained backbones is parameter-efficient and can be competitive, but it lacks explicit task-aligned allocation and typically underperforms SCDEM on multi-domain average retention.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>RanPac shows that simpler, more uniform architectures can be competitive in some settings, offering a neutral datapoint about when task-aligned allocation is necessary.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2242.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2242.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DER family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dark Experience Replay (DER, DER++, DER+++refresh)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experience-replay based continual-learning baselines that store exemplars and use replay (plus variants with distillation/refresh) to mitigate forgetting using largely uniform backbone representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dark experience for general continual learning: a strong, simple baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DER / DER++ / DER+++refresh</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Rehearsal-based methods that store a buffer of past examples and replay them during training; combine replay with distillation and other tweaks to preserve previous-task performance while updating a shared backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>42.27M (DER++ / DER+++ reported in Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Experience replay with a uniform backbone and distillation; no dynamic per-task experts or per-layer adaptive fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continual image classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Examples: Table 2 DER++ Tiny-Cifar10-Birds Avg 94.77 ± 0.20, Last 99.50 ± 0.19; Table 3 TinyImageNet class-IL DER avg values lower than SCDEM in many multi-domain configs (see tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>These methods are used as uniform baselines; DER variants frequently show high Last (current-task) performance but lower Average than SCDEM in multi-domain experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Table 6 DER++: Params 42.27M, GPU Avg 3490 MiB, Iteration 3.22 it/s, Task Time 110.5 s; DER+++re: same params but reported higher GPU usage in table.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Comparable parameter count to SCDEM but slower iteration and longer task time in some reported configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Uses a uniform replay-buffer size of 5120 in experiments; no claim of improved sample efficiency beyond replay.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Tends to maintain strong performance on recent tasks (high Last) but limited average retention across highly heterogeneous multi-domain streams compared to SCDEM.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Strong on current task, sometimes weaker average retention across domain shifts; serves as a strong practical uniform baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Moderate GPU/memory footprint relative to other baselines (see Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Replay-based uniform adaptation is a strong baseline for continual learning (high Last) but in this paper SCDEM's task-aligned dynamic experts achieve better average retention across heterogeneous domain sequences while remaining efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>DER shows uniform replay can be highly effective on recent tasks, indicating that uniform approaches remain competitive; however, SCDEM's results suggest task-aligned dynamic allocation can further improve multi-domain average retention.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2242.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2242.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic expansion frameworks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic expansion / progressive architectures (e.g., Progressive Neural Networks, AdaNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of continual-learning architectures that grow the model by adding new modules/experts for new tasks while freezing previous modules to preserve prior knowledge; discussed in related work and as conceptual predecessors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dynamic expansion frameworks (general class)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frameworks that allocate new subnetworks/experts or expand layers/nodes when new tasks arrive, aiming to avoid catastrophic forgetting by isolating new-task parameters from historical parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Architectural expansion: add new layers/experts/modules per task and optionally freeze older modules (e.g., Progressive Neural Networks, AdaNet).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continual learning (general; often image classification and other supervised tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Dynamic expansion is a widely-studied strategy for preventing forgetting by allocating new parameters per task; SCDEM follows this design philosophy but augments it with multi-pretrained-backbone fusion and adaptive layer-wise attention.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>The paper builds on and extends dynamic expansion ideas, presenting empirical evidence that carefully managed expansion + adaptive fusion improves multi-domain continual performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Boosting continual learning of vision-language models via mixture-ofexperts adapters <em>(Rating: 2)</em></li>
                <li>Semantic Residual Prompts for Continual Learning <em>(Rating: 2)</em></li>
                <li>Ranpac: Random projections and pre-trained models for continual learning <em>(Rating: 2)</em></li>
                <li>Generating instance-level prompts for rehearsal-free continual learning <em>(Rating: 2)</em></li>
                <li>Dark experience for general continual learning: a strong, simple baseline. <em>(Rating: 2)</em></li>
                <li>Progressive neural networks <em>(Rating: 2)</em></li>
                <li>Adanet: Adaptive structural learning of artificial neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2242",
    "paper_id": "paper-c0fe02247120f6cc8931fb4bb2727c5be7a8790d",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "SCDEM",
            "name_full": "Self-Controlled Dynamic Expansion Model",
            "brief_description": "A continual-learning architecture that maintains multiple pretrained ViT backbones and dynamically instantiates small task-specific experts; it adaptively fine-tunes only the last L layers of each backbone and uses layer-wise attention, KL distillation (COM) and Wasserstein feature-alignment (FDC) to preserve prior knowledge while enabling plasticity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SCDEM",
            "model_description": "Multi-pretrained-ViT backbone mixture + dynamic expert creation per task: concatenates class-token outputs from t' pretrained ViTs to form an augmented representation, creates a small adaptive module f_xi_j and linear classifier f_omega_j per task, selectively fine-tunes last L layers of each backbone; includes Collaborative Optimization Mechanism (KL distillation against frozen snapshots), Feature Distribution Consistency (Wasserstein distance between fused features), and Dynamic Layer-Wise Feature Attention (selector g_phi) to weight layers.",
            "model_size": "42.27M (SCDEM^2 reported in paper Table 6)",
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Dynamic expansion: multiple pretrained backbones whose class-token outputs are concatenated into an augmented feature; dynamically-created task-specific expert modules (adaptive projector + classifier); selector network (DLWFAM) provides adaptive layer-wise weighting; COM (KL) and FDC (Wasserstein) regularize representations.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Continual image classification across multiple domains (TinyImageNet, CIFAR-10, CIFAR-100, Birds525 species) in class-IL / task-IL and multi-domain sequences.",
            "performance_task_aligned": "Multiple reported metrics: e.g. Tiny-Cifar10-Birds (Table 2) SCDEM^2 Average 97.16 ± 0.06, Last 99.81 ± 0.09; SCDEM^3 Average 97.83 ± 0.38, Last 99.50 ± 0.44. TinyImageNet class-IL (Table 3) SCDEM^2: 5-step Avg 92.48, Last 90.20; 10-step Avg 94.02, Last 92.00. Ablation Table 5: Tiny-ImageNet Avg 94.94, CIFAR100 Avg 94.23.",
            "performance_uniform_baseline": "ViT_1 + ViT_2 concatenation baseline (Table 5): Tiny-ImageNet Avg 93.91, CIFAR100 Avg 92.48; DER++ (replay baseline, Table 2) e.g. Tiny-Cifar10-Birds Avg 94.77 ± 0.20 — generally lower average retention than SCDEM in many reported multi-domain configs.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Reported for SCDEM^2 (Table 6): Params 42.27M, GPU Avg 5330 MiB, Iteration speed 4.71 it/s, Task Time 71.05 s (Tiny-Birds scenario, RTX 4090, averaged over runs).",
            "computational_efficiency_baseline": "StarPrompt (Table 6): Params 86.41M, GPU Avg 10112 MiB, 2.49 it/s, Task Time 424.19 s; DER++ (Table 6): Params 42.27M, GPU 3490 MiB, 3.22 it/s, Task Time 110.5 s. SCDEM reports ~51% fewer parameters, ~47% less GPU usage and much faster training time vs StarPrompt (paper claims these reductions).",
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Paper reports strong generalization across heterogeneous domain sequences and that adding a third suitable backbone (SCDEM^3) consistently improves performance over dual-backbone SCDEM^2 (examples: SCDEM^2 vs SCDEM^3 in Table 2 show small but consistent gains); overall SCDEM attains SOTA average accuracy across the evaluated multi-domain task sequences.",
            "interpretability_results": null,
            "multi_task_performance": "Evaluated across many multi-domain sequences; SCDEM attains higher 'Average' (mean over tasks) and competitive 'Last' (final-task) accuracy compared with baselines — e.g. Table 2/1/3 show SCDEM often top in Average across 2/3/4-domain setups and class-IL experiments.",
            "resource_constrained_results": "Table 6 shows SCDEM^2 achieves modest parameter count (42.27M) and moderate GPU usage (5330 MiB) with high iteration throughput (4.71 it/s) and much lower task training time (71.05 s) vs heavy prompt-based StarPrompt (86.41M params, 10112 MiB, 424.19 s).",
            "key_finding_summary": "Dynamically allocating small task-specific experts on top of multiple pretrained backbones and using adaptive layer-wise fusion plus distributional alignment yields higher multi-task (average) continual-learning performance and faster, more parameter-efficient training than several single-backbone or prompt-based baselines.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Empirical results show task-specific dynamic experts and adaptive layer-wise weighting improve average continual-learning performance and efficiency relative to uniform single-backbone baselines, supporting the value of task-aligned abstractions.",
            "uuid": "e2242.0"
        },
        {
            "name_short": "MoE-2E/1R",
            "name_full": "Mixture-of-Experts (as evaluated: MoE-2E/1R)",
            "brief_description": "A mixture-of-experts style model evaluated by the paper that dynamically activates a subset of experts per input/task via routing/gating; included as a baseline in experiments.",
            "citation_title": "Boosting continual learning of vision-language models via mixture-ofexperts adapters",
            "mention_or_use": "use",
            "model_name": "MoE-2E/1R",
            "model_description": "Mixture-of-Experts architecture where a gating/router activates certain expert modules for inputs/tasks; intended to allocate computation adaptively across experts rather than using a uniform representation.",
            "model_size": "64.05M (MoE-22E reported in Table 6; the paper's MoE baseline is the evaluated variant)",
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Mixture-of-experts with dynamic expert activation (routing/gating) to allocate experts per input/task.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Continual image classification across multi-domain sequences (same benchmarks as other comparisons).",
            "performance_task_aligned": "Reported poor mean retention (Average) across multi-domain sequences: e.g. Table 1 TinyImage-Birds Average 26.7 ± 0.85, Last 76.0 ± 0.31; Table 2 Tiny-Cifar10-Birds Average 31.22 ± 0.36, Last 92.00 ± 0.41. Generally high Last but low Average indicates good current-task performance but poor retention across tasks.",
            "performance_uniform_baseline": "Uniform/replay baselines (DER++, StarPrompt) report much higher Average scores (e.g., DER++ Avg ~94.7 in some configs; StarPrompt Avg ~97.7 in Table 2), showing MoE's average retention was substantially lower in these experiments.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Table 6: MoE-22E Params 64.05M, GPU Avg 21362 MiB, Iteration speed 1.93 it/s, Task Time 266.65 s (much larger GPU footprint and slower iteration rate than SCDEM).",
            "computational_efficiency_baseline": "Compared to SCDEM^2 (42.27M params, 5330 MiB, 4.71 it/s, 71.05 s), MoE uses more parameters and much more GPU memory and is slower.",
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Paper notes MoE-style baselines tend to exhibit high performance on the current task (Last) but poor Average performance across tasks, indicating limited transfer/retention in these multi-domain continual-learning setups.",
            "interpretability_results": null,
            "multi_task_performance": "Poor average (multi-task) performance despite strong single-task (Last) numbers; dynamic expert allocation did not translate into improved continual retention in these experiments.",
            "resource_constrained_results": "High GPU memory use (21.3 GiB in reported experiment) and slower throughput compared to SCDEM and several baselines.",
            "key_finding_summary": "Although MoE provides dynamic allocation of experts, in the evaluated multi-domain continual-learning settings MoE baselines yielded strong current-task accuracy but very poor average retention and had larger resource costs.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "MoE implements dynamic task-aligned allocation (theory-supporting), but in these experiments it failed to improve multi-task retention and was resource-intensive, producing mixed evidence for the practical benefits of dynamic expert allocation in continual learning.",
            "uuid": "e2242.1"
        },
        {
            "name_short": "StarPrompt",
            "name_full": "Semantic Residual Prompts (StarPrompt)",
            "brief_description": "A prompt-based continual-learning method that injects learned prompts to adapt pretrained models for new tasks while aiming to preserve previous knowledge; evaluated as a strong baseline in the paper.",
            "citation_title": "Semantic Residual Prompts for Continual Learning",
            "mention_or_use": "use",
            "model_name": "StarPrompt",
            "model_description": "Prompt-based, parameter-efficient continual-learning approach that maintains prompt vectors (task-specific) to steer a pretrained backbone for each task, reducing the need to modify backbone weights.",
            "model_size": "86.41M (reported in Table 6)",
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Prompt injection / prompt tuning: task-specific prompt vectors appended/inserted to the model to induce task-aligned representations.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Continual image classification across multiple datasets/domains (TinyImageNet, CIFAR variants, Birds).",
            "performance_task_aligned": "High performance reported in multiple configs: e.g., Table 2 Tiny-Cifar10-Birds Avg 97.70 ± 0.63, Last 99.12 ± 0.77; Table 3 TinyImageNet class-IL 5-step Avg 87.99, Last 86.10.",
            "performance_uniform_baseline": "SCDEM variants sometimes match or exceed StarPrompt on Average (e.g., SCDEM^2 94.94 Tiny-ImageNet avg vs StarPrompt lower in some class-IL configs), but direct numbers depend on configuration; StarPrompt is often among the top baselines for Last and sometimes Average metrics.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Table 6: Params 86.41M, GPU Avg 10112 MiB, Iteration 2.49 it/s, Task Time 424.19 s — higher parameter and GPU footprint and much slower training time vs SCDEM^2.",
            "computational_efficiency_baseline": "SCDEM^2: 42.27M params, 5330 MiB GPU, 4.71 it/s, 71.05 s; StarPrompt uses roughly double params and GPU and much longer runtime in the reported scenario.",
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Strong performance in many benchmarks indicates good within-domain adaptation; comparative multi-domain generalization is task-dependent — SCDEM shows improved average retention in some multi-domain settings while StarPrompt performs competitively.",
            "interpretability_results": null,
            "multi_task_performance": "High Last scores and competitive Average across many tasks; prompt-based task-specific representations help retain performance but come at higher resource cost.",
            "resource_constrained_results": "Relatively heavy in both parameters and GPU usage (see Table 6).",
            "key_finding_summary": "Prompt-based, task-specific representations (StarPrompt) are effective and often achieve strong accuracy, but they tend to be more resource-heavy than SCDEM and do not uniformly dominate multi-domain average retention in the paper's experiments.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "StarPrompt's strong performance supports that task-aligned prompt representations can be beneficial; the paper shows alternative task-aligned mechanisms (SCDEM) can match or improve performance with lower resource cost.",
            "uuid": "e2242.2"
        },
        {
            "name_short": "Dap",
            "name_full": "Generating instance-level prompts for rehearsal-free continual learning (Dap)",
            "brief_description": "An instance-level prompt generation method that creates prompts per instance to adapt a pretrained model for continual learning without rehearsal memory.",
            "citation_title": "Generating instance-level prompts for rehearsal-free continual learning",
            "mention_or_use": "use",
            "model_name": "Dap",
            "model_description": "Rehearsal-free continual-learning approach that generates instance-level prompts enabling parameter-efficient adaptation of pretrained backbones to new tasks.",
            "model_size": "0.68M (reported in Table 6)",
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Instance-level prompt generation (per-example/task prompt vectors) to produce task- or instance-aligned representations.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Continual image classification",
            "performance_task_aligned": "Table 2 examples: Tiny-Cifar10-Birds Avg 94.48 ± 0.51, Last 92.65 ± 0.45; other configs vary but generally lower Average than top-performing SCDEM variants.",
            "performance_uniform_baseline": "SCDEM^2 and StarPrompt report higher Average scores in many configs (SCDEM^2 e.g. 97.16 in related config), indicating Dap trades some accuracy for high parameter efficiency.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Table 6: Params 0.68M, GPU Avg 4420 MiB, Iteration 2.33 it/s, Task Time 147.08 s — very small parameter count but slower iteration rate and longer time than SCDEM in the reported scenario.",
            "computational_efficiency_baseline": "SCDEM^2 larger params but faster iteration and much lower task time (71.05 s).",
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Parameter-efficient but tends to underperform SCDEM and top prompt-based methods in Average multi-task retention in the paper's multi-domain benchmarks.",
            "resource_constrained_results": "Extremely low parameter count, moderate GPU usage; presents a favorable tradeoff for memory-constrained deployment at some accuracy cost.",
            "key_finding_summary": "Instance-level prompt adaptation is highly parameter-efficient and rehearsal-free but delivers lower average continual-learning accuracy than SCDEM and some other baselines in multi-domain experiments.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Dap demonstrates that task/instance-aligned prompt representations can be an effective parameter-efficient route to continual adaptation, supporting the utility of task-aligned abstractions even when resource-limited.",
            "uuid": "e2242.3"
        },
        {
            "name_short": "RanPac",
            "name_full": "RanPac",
            "brief_description": "A method that combines random projections with pretrained models for continual learning; emphasizes simplicity and leverage of pretrained representations rather than dynamic task-specific allocation.",
            "citation_title": "Ranpac: Random projections and pre-trained models for continual learning",
            "mention_or_use": "use",
            "model_name": "RanPac",
            "model_description": "Uses random projections coupled with pretrained backbones to produce features used for continual learning, relying less on dynamically allocated task-specific components.",
            "model_size": "1.49M (reported in Table 6)",
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Random projections of pretrained features (non-adaptive, uniform feature processing rather than explicit task-specific allocation).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Continual image classification",
            "performance_task_aligned": "Table 2 example: Tiny-Cifar10-Birds Avg 93.92 ± 0.48, Last 91.10 ± 0.35; class-IL Table 3 shows RanPac competitive on some class-IL configs (e.g., TinyImageNet 5-step Avg 72.81).",
            "performance_uniform_baseline": "Comparable to some uniform baselines; generally lower Average than SCDEM but competitive in certain class-IL settings.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Table 6: Params 1.49M, GPU Avg 3566 MiB, Iteration 3.44 it/s, Task Time 250.82 s.",
            "computational_efficiency_baseline": "Very low parameter count but slower overall task time than SCDEM^2 in the reported scenario.",
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Moderate multi-task performance; simpler uniform/random-projection approach sometimes competitive but usually below best task-aligned methods.",
            "resource_constrained_results": "Low parameter footprint and modest GPU usage; tradeoff between simplicity and peak average accuracy.",
            "key_finding_summary": "Random projection plus pretrained backbones is parameter-efficient and can be competitive, but it lacks explicit task-aligned allocation and typically underperforms SCDEM on multi-domain average retention.",
            "supports_or_challenges_theory": "neutral",
            "supports_or_challenges_theory_explanation": "RanPac shows that simpler, more uniform architectures can be competitive in some settings, offering a neutral datapoint about when task-aligned allocation is necessary.",
            "uuid": "e2242.4"
        },
        {
            "name_short": "DER family",
            "name_full": "Dark Experience Replay (DER, DER++, DER+++refresh)",
            "brief_description": "Experience-replay based continual-learning baselines that store exemplars and use replay (plus variants with distillation/refresh) to mitigate forgetting using largely uniform backbone representations.",
            "citation_title": "Dark experience for general continual learning: a strong, simple baseline.",
            "mention_or_use": "use",
            "model_name": "DER / DER++ / DER+++refresh",
            "model_description": "Rehearsal-based methods that store a buffer of past examples and replay them during training; combine replay with distillation and other tweaks to preserve previous-task performance while updating a shared backbone.",
            "model_size": "42.27M (DER++ / DER+++ reported in Table 6)",
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Experience replay with a uniform backbone and distillation; no dynamic per-task experts or per-layer adaptive fusion.",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Continual image classification",
            "performance_task_aligned": "Examples: Table 2 DER++ Tiny-Cifar10-Birds Avg 94.77 ± 0.20, Last 99.50 ± 0.19; Table 3 TinyImageNet class-IL DER avg values lower than SCDEM in many multi-domain configs (see tables).",
            "performance_uniform_baseline": "These methods are used as uniform baselines; DER variants frequently show high Last (current-task) performance but lower Average than SCDEM in multi-domain experiments.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Table 6 DER++: Params 42.27M, GPU Avg 3490 MiB, Iteration 3.22 it/s, Task Time 110.5 s; DER+++re: same params but reported higher GPU usage in table.",
            "computational_efficiency_baseline": "Comparable parameter count to SCDEM but slower iteration and longer task time in some reported configurations.",
            "sample_efficiency_results": "Uses a uniform replay-buffer size of 5120 in experiments; no claim of improved sample efficiency beyond replay.",
            "transfer_generalization_results": "Tends to maintain strong performance on recent tasks (high Last) but limited average retention across highly heterogeneous multi-domain streams compared to SCDEM.",
            "interpretability_results": null,
            "multi_task_performance": "Strong on current task, sometimes weaker average retention across domain shifts; serves as a strong practical uniform baseline in the paper.",
            "resource_constrained_results": "Moderate GPU/memory footprint relative to other baselines (see Table 6).",
            "key_finding_summary": "Replay-based uniform adaptation is a strong baseline for continual learning (high Last) but in this paper SCDEM's task-aligned dynamic experts achieve better average retention across heterogeneous domain sequences while remaining efficient.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "DER shows uniform replay can be highly effective on recent tasks, indicating that uniform approaches remain competitive; however, SCDEM's results suggest task-aligned dynamic allocation can further improve multi-domain average retention.",
            "uuid": "e2242.5"
        },
        {
            "name_short": "Dynamic expansion frameworks (general)",
            "name_full": "Dynamic expansion / progressive architectures (e.g., Progressive Neural Networks, AdaNet)",
            "brief_description": "A class of continual-learning architectures that grow the model by adding new modules/experts for new tasks while freezing previous modules to preserve prior knowledge; discussed in related work and as conceptual predecessors.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Dynamic expansion frameworks (general class)",
            "model_description": "Frameworks that allocate new subnetworks/experts or expand layers/nodes when new tasks arrive, aiming to avoid catastrophic forgetting by isolating new-task parameters from historical parameters.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Architectural expansion: add new layers/experts/modules per task and optionally freeze older modules (e.g., Progressive Neural Networks, AdaNet).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Continual learning (general; often image classification and other supervised tasks).",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": null,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Dynamic expansion is a widely-studied strategy for preventing forgetting by allocating new parameters per task; SCDEM follows this design philosophy but augments it with multi-pretrained-backbone fusion and adaptive layer-wise attention.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "The paper builds on and extends dynamic expansion ideas, presenting empirical evidence that carefully managed expansion + adaptive fusion improves multi-domain continual performance.",
            "uuid": "e2242.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Boosting continual learning of vision-language models via mixture-ofexperts adapters",
            "rating": 2
        },
        {
            "paper_title": "Semantic Residual Prompts for Continual Learning",
            "rating": 2
        },
        {
            "paper_title": "Ranpac: Random projections and pre-trained models for continual learning",
            "rating": 2
        },
        {
            "paper_title": "Generating instance-level prompts for rehearsal-free continual learning",
            "rating": 2
        },
        {
            "paper_title": "Dark experience for general continual learning: a strong, simple baseline.",
            "rating": 2
        },
        {
            "paper_title": "Progressive neural networks",
            "rating": 2
        },
        {
            "paper_title": "Adanet: Adaptive structural learning of artificial neural networks",
            "rating": 1
        }
    ],
    "cost": 0.0278325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Controlled Dynamic Expansion Model for Continual Learning</h1>
<p>Runqing Wu<br>Huazhong University of Science and Technology Wuhan, China<br>Hanyi Zhang<br>Technische Universität München München, Germany</p>
<h2>Abstract</h2>
<p>Continual Learning (CL) epitomizes an advanced training paradigm wherein prior data samples remain inaccessible during the acquisition of new tasks. Numerous investigations have delved into leveraging a pre-trained Vision Transformer (ViT) to enhance model efficacy in continual learning. Nonetheless, these approaches typically utilize a singular, static backbone, which inadequately adapts to novel tasks, particularly when engaging with diverse data domains, due to a substantial number of inactive parameters. This paper addresses this limitation by introducing an innovative Self-Controlled Dynamic Expansion Model (SCDEM), which orchestrates multiple distinct trainable pre-trained ViT backbones to furnish diverse and semantically enriched representations. Specifically, by employing the multi-backbone architecture as a shared module, the proposed SCDEM dynamically generates a new expert with minimal parameters to accommodate a new task. A novel Collaborative Optimization Mechanism (COM) is introduced to synergistically optimize multiple backbones by harnessing prediction signals from historical experts, thereby facilitating new task learning without erasing previously acquired knowledge. Additionally, a novel Feature Distribution Consistency (FDC) approach is proposed to align semantic similarity between previously and currently learned representations through an optimal transport distance-based mechanism, effectively mitigating negative knowledge transfer effects. Furthermore, to alleviate over-regularization challenges, this paper presents a novel Dynamic Layer-Wise Feature Attention Mechanism (DLWFAM) to autonomously determine the penalization intensity on each trainable representation layer. An extensive series of experiments have been conducted to evaluate the proposed methodology's efficacy, with empirical results corroborating that the approach attains state-of-the-art performance.</p>
<h2>Keywords</h2>
<p>Continual Learning, Cross-Domain Continual Learning, Mixture Model</p>
<h2>1 Introduction</h2>
<p>The goal of continual learning (CL), also known as lifelong learning, is to create a model that can continuously learn new information while remembering what has already been learned [40]. However, current deep learning models often suffer significant performance</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Kaihui Huang<br>University of Electronic Science and Technology of China Shenzhen, China</p>
<p>Fei $\mathrm{Ye}^{*}$
University of Electronic Science and Technology Chengdu, China
degradation in continual learning, mainly from catastrophic forgetting [40], as these models do not have the mechanisms to prevent information loss when adjusting to new tasks. Because of these benefits, continual learning has been applied to real-world applications in a variety of domains, such as autonomous driving, robotic navigation, and medical diagnostics.</p>
<p>Numerous methods have been developed to solve the problem of network forgetting in the continual learning scenario. These fall into three main categories: the rehearsal-based methods, which optimize a small memory buffer to preserve many important examples $[3,9]$, the dynamic expansion frameworks, which allow for the automatic construction and integration of new hidden layers and nodes into an existing backbone to capture new information [10, 21]; and the regularization-based methods, which add a regularization term to the primary objective function to minimize significant changes to many previously important network parameters [27, 35]. These methods, however, are primarily focused on addressing catastrophic forgetting while ignoring plasticity which is the ability of learning new tasks.</p>
<p>In continual learning, achieving an equilibrium between network forgetting and plasticity is paramount to ensuring optimal performance across both historical and current tasks (refer to [25]). Numerous investigations have advocated for the utilization of the pre-trained Vision Transformer (ViT) [14] as a means to mitigate network forgetting while enhancing plasticity [14, 34, 36]. The semantically enriched representations generated by the pre-trained ViT backbone facilitate rapid adaptation to novel task learning. Nevertheless, these approaches typically rely on a singular pre-trained ViT as the backbone, which may exhibit constrained learning capabilities when confronted with tasks containing information divergent from the pre-trained ViT's stored knowledge. Furthermore, these methodologies often immobilize the parameters of the pretrained backbone to prevent forgetting, thereby impacting plasticity. This paper introduces a novel framework, the Self-Controlled Dynamic Expansion Model (SCDEM), which concurrently addresses network forgetting and plasticity by managing and optimizing a series of diverse pre-trained ViT backbones to deliver semantically rich representations. By utilizing these backbones as the shared module, a new expert network is dynamically constructed with minimal parameters, aiming to capture information from new task learning. In contrast to existing pre-trained methodologies that employ a single backbone and consequently fail to achieve optimal performance across various specific tasks [14, 34, 36], the proposed</p>
<p>SCDEM demonstrates robust generalization across diverse data domains.</p>
<p>To augment plasticity within the realm of continual learning, we propose an innovative Collaborative Optimization Mechanism (COM) designed to iteratively refine the backbones, thereby yielding adaptive and resilient representations. In addition, the proposed COM targets the optimization of the last few representation layers of each backbone, thereby mitigating substantial computational demands. To circumvent the issue of negative knowledge transfer, it is imperative that optimizing the backbones should not alter the pre-established prediction patterns of historical experts. To achieve this, the proposed COM freezes and copies the trainable parameters of each backbone as the frozen backbone, aiming to preserve the previously learned representation information on the most recent task. Subsequently, the proposed COM endeavors to minimize the Kullback-Leibler (KL) divergence between predictions derived from both previously and currently acquired backbones, facilitating the incremental assimilation of new information while retaining all previously acquired knowledge.</p>
<p>To further mitigate the adverse effects of negative knowledge transfer, we introduce an innovative Feature Distribution Consistency (FDC) method designed to stabilize the trainable representation layers within neural network backbones during the optimization process. The proposed FDC method conceptualizes the representations derived from multi-level feature layers as feature distributions and seeks to minimize the optimal transport distance between previously acquired and newly learned feature distributions. This strategy ensures the retention of robust, previously acquired representations while facilitating the learning of new tasks. Additionally, to address over-regularization challenges that impede model plasticity, we propose a novel Dynamic Layer-Wise Feature Attention Mechanism (DLFAM). This mechanism manages and optimizes a parametric function to autonomously assess the significance of each representation layer during the regularization process. The proposed DLFAM synthesizes weighted layer-wise features from each backbone into a cohesive representation, forming an augmented feature distribution. An optimal transport distance metric is applied to the augmented feature distributions to guide the model's optimization process, thereby selectively penalizing alterations in each trainable representation layer and circumventing over-regularization issues. A thorough array of experiments centred on continual learning has been executed, illustrating that our proposed methodology markedly exceeds current baselines across all experimental setups. The principal contributions of this research are delineated as follows :</p>
<ul>
<li>This paper proposes a novel Self-Controlled Dynamic Expansion Model (SCDEM) that optimizes and manages several different pre-trained ViT backbones to provide semantically rich representations, enhancing the model's performance in cross-domain continual learning.</li>
<li>We propose a novel COM to collaboratively optimize each backbone to adapt to new tasks without forgetting all previously learnt knowledge.</li>
<li>We propose a novel FDC approach to align the semantic similarity between the previously and currently learnt representations, which can minimize the negative knowledge transfer effects.</li>
<li>We propose a novel DLWFAM to automatically determine the importance of each trainable representation layer during the model's regularization process, which can effectively avoid overregularization issues.</li>
</ul>
<h2>2 Relate Work</h2>
<p>Rehearsal-based methods remain one of the most fundamental and widely used strategies in continual learning to address the problem of catastrophic forgetting [4]. These methods mitigate forgetting by storing a representative subset of previously seen samples and replaying them during the training of new tasks [4, $7,18,19,22,41,44,45,49]$. The effectiveness of such methods is highly dependent on the quality of the sample selection. To further enhance performance, rehearsal strategies are often combined with regularization-based approaches through the use of memory buffers [2, 9, 11-13, 23, 33, 35, 39, 47, 51]. As an alternative to storing raw data, generative replay methods employ models such as Variational Autoencoders (VAEs) [29] and Generative Adversarial Networks (GANs) [16] to synthesize previous data distributions [1, 28, 42, 48, 57], thereby addressing privacy concerns associated with direct data storage.
Knowledge distillation (KD) has also been widely adopted in continual learning, originally developed to transfer knowledge from a larger teacher model to a more compact student model [17, 20]. In the continual learning setting, KD is adapted by treating the model trained on previous tasks as the teacher and the current model as the student. By minimizing the discrepancy between their outputs, the student is guided to retain knowledge from past tasks [32]. Several approaches integrate KD with rehearsal mechanisms into unified frameworks to further improve performance. A notable example is iCaRL [43], which combines rehearsal with a nearest-mean-ofexemplars classifier, enhancing robustness to representation drift. Additionally, self-distillation techniques have been proposed to preserve learned features without relying on external teacher models, effectively alleviating forgetting [7].
Dynamic expansion architectures offers a complementary strategy to fixed-capacity models. While rehearsal and KD-based methods have shown promising results, they often struggle with long task sequences or highly heterogeneous domains. To address this, dynamic and expandable architectures have been proposed, which progressively allocate new sub-networks or hidden layers for incoming tasks, while keeping previously learned parameters frozen to preserve prior knowledge [10, 21, 26, 41, 46, 50, 53, 58]. Such approaches allow continual models to scale with task complexity and maintain performance across all learned tasks. More recently, Vision Transformers (ViT) [14] have been adopted as modular backbones in dynamic architectures, demonstrating improved scalability and adaptability compared to CNN-based variants [15, 54].
For a more comprehensive overview of related techniques and comparisons, please refer to the extended discussion in Appendix A from Supplementary Material (SM).</p>
<h2>3 Methodology</h2>
<h3>3.1 Problem Statement</h3>
<p>In continual learning, a model is trained in a dynamic and nonstationary environment where data arrives sequentially in the</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the SCDEM training framework. (a) Initial task stage: (i) Each backbone $f_{\theta_{j}}$ is partially fine-tuned to extract multi-source features $\mathrm{z}^{f}$, which are used to train a task-specific expert $\mathcal{E}<em _xi__t="\xi_{t">{t}=\left{f</em>}}, f_{\omega_{t}}\right}$. (ii) Backbone copies $\hat{f<em j="j">{\theta</em>}}$ are frozen to retain prior knowledge. (b) Continual learning stage: (iii) A selector $g_{\theta_{j}}$ assigns layer-wise weights to compute $Z_{j}^{\text {used }}$, aligned with its frozen counterpart via Wasserstein distance. (iv) Knowledge consistency is enforced through KL divergence between expert outputs ( $\mathcal{L<em _CE="{CE" _text="\text">{\text {COM }}$ ), and task-specific supervision is applied via cross-entropy loss ( $\mathcal{L}</em>$ ).
form of tasks. At each stage, the model is only allowed to access the training data from the current task, and data from previous tasks is no longer accessible. Let the $i$-th training task be denoted as $D_{i}^{s}=\left(\mathbf{x}}<em j="j">{j}^{t}, \mathbf{y}</em>\right)}^{t<em i="i">{j=1}^{n^{t}}$, and the corresponding test set be $D</em>}^{t}=\left(\mathbf{x<em j="1">{j}^{t, i}, \mathbf{y}^{t, i}\right)</em>}^{n^{t, t}}$, where $n^{t}$ and $n^{t, i}$ represent the number of training and testing samples, respectively. Here, $\mathbf{x<em x="x">{j}^{t, i} \in \mathcal{X} \subseteq \mathbb{R}^{d</em>}}$ is the input feature and $\mathbf{y<em y="y">{j}^{t, i} \in \mathcal{Y} \subseteq \mathbb{R}^{d</em>(j-1)\right}$ remain unavailable.}}$ is the corresponding label, with $\mathcal{X}$ and $\mathcal{Y}$ denoting the input and label spaces. In a classincremental setting, each training dataset $D_{i}^{s}$ is partitioned into $C_{i}$ disjoint subsets: $\left{D_{i}^{s}(1), \cdots, D_{i}^{s}\left(C_{i}\right)\right}$, where each subset contains samples belonging to a single or a small group of consecutive classes. Let $\left{T_{1}, \ldots, T_{C_{i}}\right}$ denote the sequence of tasks, with task $T_{j}$ corresponding to subset $D_{i}^{s}(j)$. During training on task $T_{j}$, the model is restricted to accessing only $D_{i}^{s}(j)$, and all previous subsets $\left{D_{i}^{s}(1), \ldots, D_{i}^{s</p>
<p>While most existing continual learning approaches focus on learning new categories within a single domain, real-world applications often involve domain heterogeneity. Suppose we are given $t$ domains $\left{D_{1}^{s}, \ldots, D_{t}^{s}\right}$, where each $D_{i}^{s}$ is further divided into $C_{i}$ subsets as described above. A sequential data stream $S$ can be defined as:</p>
<p>$$
S=D_{1}^{s}(1), \ldots, D_{1}^{s}\left(C_{1}\right), \ldots, D_{t}^{s}\left(C_{t}\right)
$$</p>
<p>This scenario introduces challenges from both class-incremental learning and domain shift. After the model finishes training over the entire stream, it is evaluated on the corresponding test sets $\left{D_{1}^{t}, \ldots, D_{t}^{t}\right}$ to assess its ability to retain knowledge and generalize across tasks and domains.</p>
<h3>3.2 Framework Overview</h3>
<p>In continual learning scenarios, existing research often introduces a new, independent expert module in mixture systems to begin training with minimal parameters. This approach can employ a single pre-trained ViT as the backbone network that contains only a small subset of the semantic knowledge from one or a few data domains. As a result, the model exhibits significant limitations when dealing with data from domains that have large distributional shifts. Additionally, the parameters of the backbone in these dynamic expansion models are usually frozen during training, which reduces the model's generalization ability and adaptability to the newly seen data domain. To address these issues, we propose a novel dynamic expansion model, which manages and optimizes several different backbone networks that were trained on different data sources. Such a dynamic expansion model demonstrates strong generalization across various domains while mitigating catastrophic forgetting of previous knowledge. The overall architecture of the proposed framework is shown in fig. 1, and the individual network components will be discussed in detail in the following sections.
The multi-source backbones. Utilizing multiple different backbones, each trained on distinct datasets and domains, can produce richer, more versatile feature representations that significantly enhance the model's capacity in continual learning scenarios. Let $\left{f_{\theta_{1}}, \ldots, f_{\theta_{t^{\prime}}}\right}$ represent a collection of $t^{\prime}$ distinct backbones, where each backbone $f_{\theta_{j}}: \mathcal{X} \rightarrow \mathbb{Z}$ is implemented using a pre-trained ViT [14], where an input image $\mathbf{x} \in \mathcal{X}$ is mapped to a feature vector $\mathbf{z} \in \mathbb{Z}$, with $\mathbb{Z} \subseteq \mathbb{R}^{d_{z}}$ representing the feature space of dimension $d_{z}$, and $\theta_{j}$ denoting the parameters of the $j$-th backbone. To</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) Selector-weighted fusion(DLWFAM): layer-wise features from $f_{\theta_{j}}$ are aggregated via attention weights $\left{a_{k}\right}$ to form $Z_{j}^{\text {fused }}$, aligned with the frozen $\hat{Z}_{j}^{\text {fused }}$ via Wasserstein distance. (b) Task-free expert selection: each expert is scored by combining prediction entropy and KL divergence between its log-likelihood and a global softmax distribution, enabling class-IL inference without task labels.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">The</span><span class="w"> </span><span class="nt">training</span><span class="w"> </span><span class="nt">process</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">SCDEM</span>
<span class="nt">1</span><span class="w"> </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">Total</span><span class="w"> </span><span class="nt">tasks</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">);</span><span class="w"> </span><span class="nt">Backbones</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">f_{\theta_{1</span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">cdots</span><span class="o">,</span><span class="w"> </span><span class="nt">f_</span><span class="p">{</span><span class="err">\theta_{p^{\prime</span><span class="p">}</span><span class="err">}}\</span><span class="nt">right</span><span class="err">\}\</span><span class="o">);</span><span class="w"> </span><span class="nt">Depth</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">L</span><span class="err">\</span><span class="o">);</span>
<span class="nt">2</span><span class="w"> </span><span class="nt">Output</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\mathcal{E</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\mathrm{f</span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="err">\}</span><span class="nt">_</span><span class="p">{</span><span class="err">f=1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">N</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">Updated</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">f_{\theta_{j</span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="err">\}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="nt">3</span><span class="w"> </span><span class="nt">Init</span><span class="o">:</span><span class="w"> </span><span class="nt">Freeze</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">f_{\theta_{j</span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="err">\}</span><span class="nt">_</span><span class="p">{</span><span class="err">f=1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">except</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">last</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">L</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">layers</span><span class="o">;</span>
<span class="nt">4</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">=</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="nt">5</span><span class="w"> </span><span class="nt">Create</span><span class="w"> </span><span class="nt">new</span><span class="w"> </span><span class="nt">expert</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">E</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\mathrm{f</span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">f_{\xi_{f</span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="nt">f_</span><span class="p">{</span><span class="err">\omega_{f</span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="err">\}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="nt">6</span><span class="w"> </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">&gt;</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span><span class="w"> </span><span class="nt">Create</span><span class="w"> </span><span class="nt">selectors</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">g_{\phi_{1</span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">cdots</span><span class="o">,</span><span class="w"> </span><span class="nt">g_</span><span class="p">{</span><span class="err">\phi_{t^{\prime</span><span class="p">}</span><span class="err">}}\</span><span class="nt">right</span><span class="err">\}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="nt">7</span><span class="w"> </span><span class="nt">Training</span><span class="o">:</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="p">{</span><span class="err">\mathbf{x</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">y</span><span class="p">}</span><span class="err">\}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="nt">D_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="err">\</span><span class="o">(</span><span class="nt">z</span><span class="o">^</span><span class="p">{</span><span class="err">f</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">bigotimes_</span><span class="p">{</span><span class="err">j=1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="w"> </span><span class="nt">f_</span><span class="p">{</span><span class="err">\theta_{j</span><span class="p">}</span><span class="err">}</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="o">),</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\mathbf{y</span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="nt">f_</span><span class="p">{</span><span class="err">\omega_{t</span><span class="p">}</span><span class="err">}\</span><span class="nt">left</span><span class="o">(</span><span class="nt">f_</span><span class="p">{</span><span class="err">\xi_{f</span><span class="p">}</span><span class="err">}\</span><span class="nt">left</span><span class="o">(</span><span class="nt">z</span><span class="o">^</span><span class="p">{</span><span class="err">f</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="nt">Step</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\mathrm{cls</span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\mathrm{CE</span><span class="p">}</span><span class="err">}</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\mathbf{y</span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">y</span><span class="p">}</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="nt">18</span><span class="w"> </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">&gt;</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="nt">11</span>
<span class="nt">12</span>
<span class="nt">13</span>
<span class="nt">14</span>
<span class="nt">15</span>
<span class="nt">16</span>
<span class="nt">17</span>
<span class="nt">18</span>
<span class="nt">19</span>
<span class="nt">20</span>
</code></pre></div>

<p>Step 2: $\hat{\mathbf{z}}^{f}=\bigotimes_{j=1}^{t^{\prime}} f_{\theta_{j}}(\mathbf{x})$
$\mathcal{L}<em t="1">{\mathrm{COM}}=\sum</em>\right)\right)\right] ;$
Step 3: Get features $\mathcal{Z}}^{t-1} D_{\mathrm{KL}}\left[f_{\omega_{t}}\left(f_{\xi_{f}}\left(\mathbf{z}^{f}\right)\right) | f_{\omega_{t}}\left(f_{\xi_{f}}\left(\hat{\mathbf{z}}^{f<em j="j">{j}, \hat{\mathcal{Z}}</em>$
$\left.\boldsymbol{\alpha}<em _phi__j="\phi_{j">{j}=\operatorname{Softmax}\left(g</em>\right}\right)\right)\right.$;
$\hat{\boldsymbol{\alpha}}}}\left(\left{\mathbf{z}^{j, 1}, \mathbf{z}^{j, 2}, \ldots, \mathbf{z}^{j, L<em _phi__j="\phi_{j">{j}=\operatorname{Softmax}\left(g</em>\right}\right)\right)$ ；
$\mathbf{Z}}}\left(\left{\hat{\mathbf{z}}^{j, 1}, \hat{\mathbf{z}}^{j, 2}, \ldots, \hat{\mathbf{z}}^{j, L<em j="j">{j}^{\text {fused }}=\sum \alpha</em>}[k] \cdot \mathbf{z}^{j, k}, \hat{\mathbf{Z}<em j="j">{j}^{\text {fused }}=\sum \hat{\alpha}</em> ;$
$\mathcal{L}}[k] \cdot \hat{\mathbf{z}}^{j, k<em j="1">{\text {Fused }}=\sum</em>}^{t^{\prime}} \mathcal{W}\left(P_{\mathbf{Z<em _mathbf_Z="\mathbf{Z">{j}^{\text {fused }}}, P</em><em _text="\text" _total="{total">{j}^{\text {fused }}}\right) ;$
$\mathcal{L}</em>}}=\mathcal{L<em _COM="{COM" _text="\text">{\text {cls }}+\mathcal{L}</em>}}+\mathcal{L<em _xi__f="\xi_{f">{\text {Fused }} ;$
Step 4: Update $\left{f</em>}}, f_{\omega_{f}}, \theta_{j}^{(L)}, \phi_{t}\right}$ by $\nabla \mathcal{L<em _theta__j="\theta_{j">{\text {total }} ;$
Snapshot: $\left{f</em>$;
minimize computational cost while retaining key information, we extract and use only the class token from each backbone's output as representations. Given an input $\mathbf{x}$, we can leverage all $t^{\prime}$ pretrained backbones to generate a robust feature representation by concatenating their outputs as follows :}}\right} \leftarrow \operatorname{copy}\left{f_{\theta_{j}}\right} ; \quad$ Freeze $\left{f_{\theta_{j}}\right}, \mathcal{E}_{\mathrm{f}</p>
<p>$$
\mathbf{z}^{f}=\mathbf{z}^{1} \otimes \mathbf{z}^{2} \otimes \cdots \otimes \mathbf{z}^{t^{\prime}}
$$</p>
<p>where $\mathbf{z}^{j}$ represents the feature vector produced by the $j$-th backbone $f_{\theta_{j}}$, and $\otimes$ indicates the concatenation of these vectors. The resulting feature vector $\mathbf{z}^{f}$ lies in an augmented feature space $\mathcal{Z}^{f} \in \mathbb{R}^{d_{e} \times t^{\prime}}$.</p>
<p>The expert module. Although pre-trained backbones are effective at producing rich feature representations, they cannot be directly used for making predictions on new tasks. To address this issue, we propose a new creation approach to dynamically construct and integrate an expert module within a flexible expansion framework to learn the decision boundary for a new task. Specifically, for a given task $T_{j}$, we design a new expert module $\mathcal{E}<em _xi__j="\xi_{j">{j}$, which consists of an adaptive module $f</em>$ is expressed as :}}: \mathcal{Z}^{f} \rightarrow \mathcal{Z}^{e}$ that learns a task-specific representation, and a linear classifier $f_{\omega_{j}}: \mathcal{Z}^{e} \rightarrow \mathcal{Y}$ that identifies the decision-making pattern for the task. The adaptive module $f_{\xi_{j}}$ processes the augmented feature vector $\mathbf{z}^{f}$ and generates a new feature vector $\hat{\mathbf{z}}^{j}$ in the feature space $\mathcal{Z}^{e} \subseteq \mathbb{R}^{d_{e}}$, where $d_{e}$ represents the dimensionality of the learned task-specific features. The prediction process using the $j$-th expert for a given data sample $\mathbf{x</p>
<p>$$
y^{\prime}=\arg \max \left(\operatorname{Softmax}\left(\mathbf{W}^{\mathrm{T}} \omega_{j} \mathbf{z}^{j}\right)\right)
$$</p>
<p>where $\mathbf{W} \omega_{j}$ is the weight matrix of the classifier $f_{\omega_{j}}$, and Softmax $(\cdot)$ denotes the Softmax activation function. $\mathbf{W}<em j="j">{\omega</em>$ is the predicted class label.}}^{\mathrm{T}}$ represents the transpose of the weight matrix and $y^{\prime</p>
<h3>3.3 Collaborative Optimization Mechanism</h3>
<p>Freezing all backbone networks can mitigate catastrophic forgetting; however, it constrains adaptability in acquiring new tasks due to the limited activation of parameters. To address this challenge and improve the model's generalization capabilities in new task learning, we propose optimizing only a select few of the final $L$ trainable representation layers of each backbone $f_{\theta_{j}}$, where $j=1, \cdots, t^{\prime}$. Notably, optimizing these trainable representation layers during new task learning may induce catastrophic forgetting in each historical expert. To counteract this, we introduce an innovative Collaborative Optimization Mechanism (COM) designed to incrementally optimize each backbone while minimizing significant forgetting.</p>
<p>Specifically, before training on a new task $\left(T_{J}\right)$, we preserve and freeze the trainable parameters of all backbone networks, denoted as $\left{f_{\theta_{1}}, \ldots, f_{\theta_{t^{\prime}}}\right}$, forming a static historical knowledge framework. Given an input sample $\mathbf{x} \in \mathcal{X}$, we can get augmented features $\mathbf{z}^{f}$ and $\hat{\mathbf{z}}^{f}$ extracted from the activated backbones $\left{f_{\theta_{1}}, \cdots, f_{\theta_{t^{\prime}}}\right}$ and the</p>
<p>Table 1: Performance comparison of SCDEM and SOTA models in a dual-domain task configuration. "Average" denotes mean performance across all tasks, while "Last" shows the performance on the final task. All results are averaged over 10 runs. SCDEM ${ }^{2}$ or SCDEM ${ }^{3}$ indicates the use of 2 or 3 backbones respectively.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>TinyImage-Birds</th>
<th></th>
<th></th>
<th>Bird</th>
<th></th>
<th></th>
<th>Bird</th>
<th></th>
<th></th>
<th>100-Bird</th>
<th></th>
<th>Bird</th>
<th>100</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Average</td>
<td>Last</td>
<td>Average</td>
<td>Last</td>
<td>Average</td>
<td>Last</td>
<td>Average</td>
<td>Last</td>
<td>Average</td>
<td>Last</td>
<td>Average</td>
<td>Last</td>
<td></td>
</tr>
<tr>
<td>DER [6]</td>
<td>83.7±2.92</td>
<td>99.8±0.44</td>
<td>90.1±1.54</td>
<td>93.3±0.43</td>
<td>85.2±3.60</td>
<td>99.8±0.44</td>
<td>98.3±0.27</td>
<td>96.3±0.31</td>
<td>86.9±2.90</td>
<td>99.6±0.55</td>
<td>94.0±0.76</td>
<td>95.3±0.49</td>
<td></td>
</tr>
<tr>
<td>DER++ [6]</td>
<td>95.6±0.41</td>
<td>99.5±0.23</td>
<td>95.2±0.24</td>
<td>93.1±1.00</td>
<td>98.7±0.34</td>
<td>99.2±1.30</td>
<td>99.3±0.06</td>
<td>96.1±0.41</td>
<td>97.5±0.11</td>
<td>99.6±0.89</td>
<td>97.1±0.07</td>
<td>95.1±0.19</td>
<td></td>
</tr>
<tr>
<td>DER+++refresh [52]</td>
<td>95.7±0.36</td>
<td>99.5±0.19</td>
<td>95.1±0.24</td>
<td>93.5±0.47</td>
<td>98.7±0.33</td>
<td>99.5±0.24</td>
<td>99.4±0.09</td>
<td>96.2±0.28</td>
<td>97.5±0.19</td>
<td>99.5±0.23</td>
<td>97.3±0.18</td>
<td>95.6±0.47</td>
<td></td>
</tr>
<tr>
<td>MoE-2E/1R [56]</td>
<td>26.7±0.85</td>
<td>76.0±0.31</td>
<td>20.0±3.63</td>
<td>92.1±1.21</td>
<td>26.5±3.52</td>
<td>70.0±0.54</td>
<td>33.9±0.43</td>
<td>96.8±0.35</td>
<td>33.4±0.52</td>
<td>96.2±0.32</td>
<td>37.4±0.22</td>
<td>93.5±0.82</td>
<td></td>
</tr>
<tr>
<td>FDR [5]</td>
<td>21.9±3.34</td>
<td>98.6±1.51</td>
<td>35.3±7.61</td>
<td>92.7±0.47</td>
<td>65.9±8.78</td>
<td>99.0±1.73</td>
<td>70.9±4.21</td>
<td>95.9±0.06</td>
<td>48.9±12.6</td>
<td>99.0±0.43</td>
<td>68.2±7.29</td>
<td>94.9±0.68</td>
<td></td>
</tr>
<tr>
<td>AGEM-R [8]</td>
<td>39.6±14.7</td>
<td>77.8±40.7</td>
<td>64.9±10.2</td>
<td>92.3±0.75</td>
<td>46.6±18.1</td>
<td>99.2±0.84</td>
<td>95.4±1.58</td>
<td>96.0±0.21</td>
<td>40.6±8.29</td>
<td>98.8±1.30</td>
<td>83.7±4.07</td>
<td>95.3±0.30</td>
<td></td>
</tr>
<tr>
<td>iCaRL [43]</td>
<td>64.8±1.21</td>
<td>32.2±6.06</td>
<td>66.2±0.62</td>
<td>91.8±0.45</td>
<td>12.3±0.76</td>
<td>2.00±2.54</td>
<td>35.2±2.69</td>
<td>96.3±0.17</td>
<td>58.1±1.49</td>
<td>42.8±5.22</td>
<td>55.4±1.53</td>
<td>94.3±0.14</td>
<td></td>
</tr>
<tr>
<td>StarPrompt [38]</td>
<td>97.8±0.48</td>
<td>98.9±0.82</td>
<td>97.8±0.79</td>
<td>96.3±0.91</td>
<td>99.2±0.37</td>
<td>99.0±0.71</td>
<td>99.2±0.46</td>
<td>98.0±1.01</td>
<td>98.3±0.59</td>
<td>96.8±1.32</td>
<td>98.2±0.52</td>
<td>97.6±0.92</td>
<td></td>
</tr>
<tr>
<td>RanPac [37]</td>
<td>93.8±0.88</td>
<td>91.2±0.31</td>
<td>94.1±0.65</td>
<td>95.9±0.43</td>
<td>98.9±0.74</td>
<td>93.1±0.53</td>
<td>98.7±0.92</td>
<td>98.7±0.42</td>
<td>95.4±0.95</td>
<td>88.6±0.32</td>
<td>95.4±0.32</td>
<td>98.6±0.32</td>
<td></td>
</tr>
<tr>
<td>Dap [24]</td>
<td>92.9±0.72</td>
<td>95.0±0.89</td>
<td>92.4±0.52</td>
<td>93.4±0.41</td>
<td>83.4±0.67</td>
<td>97.9±0.88</td>
<td>90.7±0.42</td>
<td>99.0±0.32</td>
<td>90.4±0.52</td>
<td>94.8±0.42</td>
<td>90.6±0.68</td>
<td>98.0±0.72</td>
<td></td>
</tr>
<tr>
<td>SCDEM ${ }^{2}$ (Ours)</td>
<td>97.2±0.08</td>
<td>99.6±0.18</td>
<td>97.0±0.15</td>
<td>93.9±0.44</td>
<td>99.3±0.11</td>
<td>99.7±0.19</td>
<td>99.2±0.18</td>
<td>96.4±0.25</td>
<td>97.8±0.14</td>
<td>99.7±0.12</td>
<td>97.6±0.12</td>
<td>95.4±0.33</td>
<td></td>
</tr>
<tr>
<td>Rel.ER vs DER+++re</td>
<td>$\downarrow 34.88 \%$</td>
<td>$\downarrow 21.56 \%$</td>
<td>$\downarrow 38.77 \%$</td>
<td>$\downarrow 6.15 \%$</td>
<td>$\downarrow 46.92 \%$</td>
<td>$\downarrow 41.17 \%$</td>
<td>$\uparrow 35.11 \%$</td>
<td>$\downarrow 5.26 \%$</td>
<td>$\downarrow 12.4 \%$</td>
<td>$\downarrow 41.27 \%$</td>
<td>$\downarrow 11.44 \%$</td>
<td>$\uparrow 51.22 \%$</td>
<td></td>
</tr>
<tr>
<td>SCDEM ${ }^{3}$ (Ours)</td>
<td>97.9±0.74</td>
<td>99.6±0.27</td>
<td>98.0±0.42</td>
<td>97.9±0.41</td>
<td>99.4±0.83</td>
<td>99.2±0.56</td>
<td>99.6±0.56</td>
<td>98.0±1.36</td>
<td>98.4±0.83</td>
<td>99.2±0.55</td>
<td>98.3±1.11</td>
<td>97.2±0.76</td>
<td></td>
</tr>
<tr>
<td>Rel.ER vs StarPrompt</td>
<td>$\downarrow 4.97 \%$</td>
<td>$\downarrow 63.96 \%$</td>
<td>$\downarrow 9.50 \%$</td>
<td>$\downarrow 43.39 \%$</td>
<td>$\downarrow 25.92 \%$</td>
<td>$\downarrow 20.79 \%$</td>
<td>$\downarrow 50.62 \%$</td>
<td>$\downarrow 0 \%$</td>
<td>$\downarrow 6.43 \%$</td>
<td>$\downarrow 12.77 \%$</td>
<td>$\downarrow 6.07 \%$</td>
<td>$\uparrow 17.01 \%$</td>
<td></td>
</tr>
</tbody>
</table>
<p>frozen historical backbones $\left{\hat{f}<em 1="1">{\theta</em>}}, \ldots, \hat{f<em>{\theta_{T}}\right}$, respectively. By using $\mathbf{z}^{f}$ and $\hat{\mathbf{z}}^{f}$, each expert $\mathcal{E}</em>f$ can give the task-specific representations, expressed as :</p>
<p>$$
\hat{\mathbf{z}}^{f}=f_{\xi_{f}}\left(\mathbf{z}^{f}\right), \hat{\mathbf{z}}^{f}=f_{\xi_{f}}\left(\hat{\mathbf{z}}^{f}\right)
$$</p>
<p>By utilizing the extracted features, we can create two predictive distributions $p\left(\mathbf{y} \mid \hat{\mathbf{z}}^{i}, \mathcal{E}<em>f\right)$ and $p\left(\mathbf{y} \mid \hat{\mathbf{z}}^{i}, \mathcal{E}</em>f\right)$ in which the variable $\mathbf{y}$ relies on the feature $\hat{\mathbf{z}}^{i}$ extracted from the activated and frozen backbones, respectively. As a result, the proposed COM minimizes the probability distance between two predictive distributions, expressed as :</p>
<p>$$
\mathcal{L}<em _mathrm_KL="\mathrm{KL">{\mathrm{COM}}=\sum_{i=1}^{j-1} D</em>}}\left(p\left(\mathbf{y} \mid \hat{\mathbf{z}}^{i}, \mathcal{E<em>f\right) | p\left(\mathbf{y} \mid \hat{\mathbf{z}}^{i}, \mathcal{E}</em>f\right)\right)
$$</p>
<p>where $D_{\mathrm{KL}}(\cdot)$ is the Kullback-Leibler (KL) divergence. In practice, each $p\left(\mathbf{y} \mid \hat{\mathbf{z}}^{i}, \mathcal{E}*f\right)$ is implemented using the softmax activate function of the classifier, expressed as $f_{o z_{f}}\left(\hat{\mathbf{z}}^{i}\right)$. As a result, Eq. (5) can be rewritten as :</p>
<p>$$
\mathcal{L}*{\mathrm{COM}}^{\prime}=\sum_{i=1}^{j-1}\left{\sum_{c=1}^{U}\left{f_{o z_{f}}\left(\hat{\mathbf{z}}^{i}\right)\left[c \mid \frac{f_{o z_{f}}\left(\hat{\mathbf{z}}^{i}\right)[c]}{f_{o z_{f}}\left(\hat{\mathbf{z}}^{i}\right)[c]}\right}\right}
$$</p>
<p>where $f_{o z_{f}}\left(\hat{\mathbf{z}}^{i}\right)[c]$ denotes the $c$-th dimension of the prediction $f_{o z_{f}}\left(\hat{\mathbf{z}}^{i}\right)$ and $U$ is the total number of classes. Eq. (6) can ensure that optimizing the parameters of these backbones does not influence the previously learnt prediction ability of each history expert.</p>
<h3>3.4 Feature Distribution Consistency via Wasserstein Distance</h3>
<p>In addition to ensure that the outputs of the expert modules within the activated backbones $\left{\hat{f}<em 1="1">{\theta</em>}}, \ldots, \hat{f<em T="T">{\theta</em>}}\right}$ are consistent with those of the historical backbones $\left{\hat{f<em 1="1">{\theta</em>}}, \ldots, \hat{f<em T="T">{\theta</em>$ across all previously encountered tasks, it is imperative to preserve the semantic congruence of the representations derived from both the activated and frozen backbones. This strategy effectively mitigates the adverse effects of negative knowledge transfer. To achieve this objective, we introduce an innovative Feature Distribution Consistency (FDC) method, which quantifies the feature distribution divergence between corresponding layers through the application of the Wasserstein distance [55]. The Wasserstein distance is based on the transport distance theory and has several advantages : (1) It provides meaningful gradients even when two target distributions are disjoint; (2) It encourages the generator to cover the entire support of the real data distribution, compared to other distance measures such as KL and JS divergence. Specifically, we define a feature extraction function to derive a layer-specific representation, denoted as:}}\right</p>
<p>$$
F_{t}\left(f_{\theta_{f}}, \mathbf{x}, k\right)= \begin{cases}f_{\theta_{f}^{i}}(\mathbf{x}) &amp; k=1 \ f_{\theta_{f}^{i}}\left(f_{\theta_{f}^{i}}\left(\hat{f}<em f="f">{\theta</em>
$$}^{i}}(\mathbf{x})\right) &amp; k=2 \ f_{\theta_{f}^{k}}\left(\cdots f_{\theta_{f}^{i}}\left(f_{\theta_{f}^{k}}(\mathbf{x})\right)\right) &amp; 3 \leq k \leq L\end{cases</p>
<p>where $f_{\theta_{f}^{k}}$ denotes the $k$-th trainable layer of the backbone $f_{\theta_{f}}$, which receives the feature vector from the $(k-1)$-th trainable layer and returns a representation. By using Eq. (7), a set of feature vectors extracted by a backbone can be expressed as :</p>
<p>$$
\mathbf{Z}^{j, k}=\left{\mathbf{z}<em c="c">{c} \mid \mathbf{z}</em>
$$}=F\left(f_{\theta_{f}}, \mathbf{x}_{c}, k\right), c=1, \cdots, b\right</p>
<p>where $j=1, \cdots, t^{\prime}$ and $k=1, \cdots, L$ denote the index of the expert and trainable representation layer, respectively. Let $P_{\mathbf{Z}^{j, k}}$ denote the probability distribution of $\mathbf{Z}^{j, k}$. The proposed FDC approach minimizes the Wasserstein distance between distributions :</p>
<p>$$
\mathcal{L}<em j="1">{\mathrm{FDC}}=\sum</em>
$$}^{t^{\prime}}\left{\sum_{k=1}^{L}\left{W\left(P_{\mathbf{Z}^{j, k}}, P_{\hat{\mathbf{Z}}^{j, k}}\right)\right}\right</p>
<p>where $P_{\hat{\mathbf{Z}}^{j, k}}$ is the distribution of the representations returned using $F_{t}\left(\hat{f}<em j="j">{\theta</em> W(\cdot, \cdot)$ denotes the Wasserstein distance.}}, \mathbf{x}, k\right)$ and ${ }^{\prime</p>
<h3>3.5 Dynamic Layer-Wise Feature Attention Mechanism</h3>
<p>Different layers within backbone networks capture features at varying semantic granularities. Shallow layers generally encode low-level visual information, whereas deeper layers provide taskspecific semantic abstractions. Consequently, each layer contributes differently when adapting to new tasks. To dynamically balance these multi-layer representations, we propose an adaptive feature fusion mechanism using a learnable attention network.</p>
<p>Formally, given the last $L$ trainable representation layers from the backbone $f_{\theta_{j}}$, we construct the layer-wise feature set as $\mathcal{Z}<em z="z">{j}=$ $\left[\mathbf{z}^{j, 1}, \mathbf{z}^{j, 2}, \ldots, \mathbf{z}^{j, L}\right] \in \mathbb{R}^{L \times d</em>$, which jointly processes the entire feature set and outputs a vector of layer-specific logits:}}$, where $\mathbf{z}^{j, k}$ denotes the feature vector extracted using the $k$-th feature layer of the $j$-th backbone. To dynamically determine each layer's contribution, we introduce a learnable attention network $g_{\phi_{t}}(\cdot)$ named selector parameterized by $\phi_{t</p>
<p>$$
\boldsymbol{\alpha}<em k="k">{j}=\left{\alpha</em>, k=1, \ldots, L\right}
$$} \mid \alpha_{k}=\frac{\exp \left(g_{\phi_{t}}\left(\mathbf{z}^{j, k}\right)\right)}{\sum_{l=1}^{L} \exp \left(g_{\phi_{t}}\left(\mathbf{z}^{j, l}\right)\right)</p>
<p>where $\boldsymbol{\alpha}_{j}$ denotes the adaptive weight for the trainable representation layers of the $j$-th backbone. By using Eq. (10), we can extend the layer-wise features into a single unified representation as :</p>
<p>$$
Z_{j}^{\text {fused }}=\sum_{k=1}^{L}\left{\boldsymbol{\alpha}<em j="j">{j}[k] \cdot \mathbf{z}^{j, k}\right}, \quad \hat{Z}</em>\right}
$$}^{\text {fused }}=\sum_{k=1}^{L}\left{\hat{\boldsymbol{\alpha}}_{j}[k] \cdot \hat{\mathbf{z}}^{j, k</p>
<p>where $\hat{\boldsymbol{\alpha}}<em _theta__j="\theta_{j">{j}[k]$ denotes the adaptive weight of the $k$-th representation layer of the $j$-th frozen backbone $f</em>}}$. To enforce semantic consistency and prevent forgetting during incremental learning, we minimize the Wasserstein distance between the distributions of current fused features $\mathbf{z<em j="j">{j}^{\text {fused }}$ and historical fused features $\hat{\mathbf{z}}</em>$, resulting in :}^{\text {fused }</p>
<p>$$
\mathcal{L}<em j="1">{\text {Fused }}=\sum</em>}^{t^{\prime}} \mathcal{W}\left(P_{\hat{\mathbf{z}<em _hat_mathbf_z="\hat{\mathbf{z">{j}^{\text {fused }}}, P</em>\right)
$$}}_{j}^{\text {fused }}</p>
<p>where $P_{\hat{\mathbf{z}}<em _hat_mathbf_z="\hat{\mathbf{z">{j}^{\text {fused }}}$ and $P</em>}<em _phi__t="\phi_{t">{j}^{\text {fused }}}$ represent the distributions of fused features from the current and historical backbones, respectively. The parameters of $g</em>(\cdot)$ are optimized jointly with backbone parameters during the new task learning, allowing the model to dynamically prioritize informative layers according to task-specific demands. Compared to the regularization loss term defined in Eq. (9), Eq. (12) can adaptively penalize the changes on each trainable representation layer of the backbones, which avoids over-regularization issues and reduces computational costs.}</p>
<h3>3.6 Algorithm Implementation</h3>
<p>The training procedure of SCDEM, summarized in Algorithm 1, consists of four main stages :
Step 1: Supervised classification. For each task $T_{t}$, a task-specific expert $\mathcal{E}<em _hat_xi="\hat{\xi">{t}=\left{f</em><em _mathcal_O="\mathcal{O">{t}}, f</em><em _theta__j="\theta_{j">{t}}\right}$ is instantiated. It takes as input the concatenated multi-domain representation $\mathbf{z}^{T}$, produced by applying all
active backbones $\left{f</em>}}\right}$. The prediction $\hat{\mathbf{y}}$ is optimized using the cross-entropy loss $\mathcal{L<em _theta__j="\theta_{j">{\mathrm{cls}}$.
Step 2: Collaborative optimization. To mitigate forgetting, frozen versions of backbones $\left{f</em>}}\right}$ are preserved before each task. During training, we compute $\hat{\mathbf{z}}^{T}$ using the frozen backbones, and constrain the predictive behaviour of all past experts $\left{\mathcal{E<em i_t="i&lt;t">{i}\right}</em>}$ by minimizing the divergence between outputs based on $\mathbf{z}^{T}$ and $\hat{\mathbf{z}}^{T}$, leading to the distillation loss $\mathcal{L<em _phi__t="\phi_{t">{\text {COM }}$ by Eq. (6).
Step 3: Fused feature consistency. Instead of constraining each layer individually, we adopt a selector network $g</em>}}$ to assign soft attention weights $\boldsymbol{\alpha<em j="j">{j}$ over the $L$ trainable layers of each backbone as shown in fig. 2(a). These weights are used to generate fused task-aware features $\mathbf{Z}</em>}^{\text {fused }}$ and their frozen references $\hat{\mathbf{Z}<em _Fused="{Fused" _text="\text">{j}^{\text {fused }}$. A Wasserstein-based regularization term $\mathcal{L}</em>$ is introduced to maintain distributional consistency by Eq. (12), which avoids overregularization while improving efficiency and robustness.
Step 4: Parameter update. The final loss $\mathcal{L}}<em t="t">{\text {total }}$ combines all components above and is used to jointly update the expert $\mathcal{E}</em>$. After task completion, all backbones are snapshotted and frozen to serve as reference models for future tasks.}$, the last $L$ layers of each $f_{\theta_{j}}$, and the selector $g_{\phi_{t}</p>
<h2>4 Experiment</h2>
<h3>4.1 Experimental Setup</h3>
<p>Datasets: The model's performance is evaluated in a continual learning framework across several domains, including CIFAR-10 [30], TinylmageNet [31], CIFAR-100 [30], and Birds 525 Species.
Evaluation Metrics: To evaluate and compare the performance of the model in multi-task scenarios, we employ two key metrics: "Average" and "Last." The "Average" metric computes the mean accuracy across all tasks within a given scenario over all the testing samples, while the "Last" metric focuses on the accuracy achieved on the final task. We provide additional experimental configurations in Appendix-B from SM.</p>
<h3>4.2 Comparison with State-of-the-Art Methods</h3>
<p>In this section, we compare our method with several SOTA continual learning approaches, including experience replay-based methods, dynamic expansion models, and other incremental strategies. For experience replay, we evaluate DER [6] and its variants DER++ [6] and DER+++refresh [52], which address catastrophic forgetting by storing and replaying past samples. We also include three feature distillation-based methods: FDR [5], which applies feature regularization; AGEM-R [8], which adjusts gradients using historical task information; and iCaRL [43], which employs memory and nearest-neighbor classification. All methods are implemented with a dual-ViT backbone, unfreezing the last three layers of each ViT for fine-tuning, and sharing a uniform replay buffer size of 5120. We further compare against Mixture-of-Experts (MoE) models [56], which dynamically activate subsets of experts per task, and incremental learning methods that do not use replay, such as Random Packing (RanPac) [37] and Data Augmentation Prompt (Dap) [24]. Additionally, we also consider employing the prompt-based learning models such as the StarPrompt [38] as another baseline in our comparison, which maintains a balance between new and previous tasks through prompt injection and generated replay.</p>
<p>Table 2: Performance comparison of SCDEM and SOTA in 3-domain and 4-domain configurations, summarizing average performance across all tasks and performance on the final task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Tiny-Cifar10-Birds</th>
<th style="text-align: center;">Tiny-Cifar100-Birds</th>
<th style="text-align: center;">Tiny-C100-Birds-C10</th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">Last</td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">Last</td>
<td style="text-align: center;">Average</td>
</tr>
<tr>
<td style="text-align: center;">DER [6]</td>
<td style="text-align: center;">78.83±1.07</td>
<td style="text-align: center;">99.65±0.39</td>
<td style="text-align: center;">66.97±4.14</td>
<td style="text-align: center;">99.49±0.22</td>
<td style="text-align: center;">75.42±3.07</td>
</tr>
<tr>
<td style="text-align: center;">DER++ [6]</td>
<td style="text-align: center;">94.77±0.20</td>
<td style="text-align: center;">99.50±0.19</td>
<td style="text-align: center;">93.93±0.19</td>
<td style="text-align: center;">99.60±0.89</td>
<td style="text-align: center;">93.73±0.32</td>
</tr>
<tr>
<td style="text-align: center;">DER+++refresh [52]</td>
<td style="text-align: center;">94.89±0.27</td>
<td style="text-align: center;">99.50±0.12</td>
<td style="text-align: center;">94.26±0.28</td>
<td style="text-align: center;">99.80±0.09</td>
<td style="text-align: center;">93.83±0.30</td>
</tr>
<tr>
<td style="text-align: center;">MoE-2E/1R [56]</td>
<td style="text-align: center;">31.22±0.36</td>
<td style="text-align: center;">92.00±0.41</td>
<td style="text-align: center;">28.83±0.43</td>
<td style="text-align: center;">91.36±0.40</td>
<td style="text-align: center;">27.55±0.51</td>
</tr>
<tr>
<td style="text-align: center;">FDR [5]</td>
<td style="text-align: center;">24.25±2.39</td>
<td style="text-align: center;">98.20±2.05</td>
<td style="text-align: center;">17.28±1.68</td>
<td style="text-align: center;">98.4±0.89</td>
<td style="text-align: center;">17.09±1.75</td>
</tr>
<tr>
<td style="text-align: center;">AGEM-R [8]</td>
<td style="text-align: center;">32.71±2.88</td>
<td style="text-align: center;">74.83±39.3</td>
<td style="text-align: center;">24.95±7.86</td>
<td style="text-align: center;">37.87±45.6</td>
<td style="text-align: center;">47.89±3.87</td>
</tr>
<tr>
<td style="text-align: center;">iCaRL [43]</td>
<td style="text-align: center;">49.84±0.49</td>
<td style="text-align: center;">4.6±2.61</td>
<td style="text-align: center;">70.59±1.12</td>
<td style="text-align: center;">41.20±3.76</td>
<td style="text-align: center;">70.29±1.64</td>
</tr>
<tr>
<td style="text-align: center;">StarPrompt [38]</td>
<td style="text-align: center;">97.70±0.63</td>
<td style="text-align: center;">99.12±0.77</td>
<td style="text-align: center;">97.01±0.15</td>
<td style="text-align: center;">98.01±1.00</td>
<td style="text-align: center;">97.39±0.33</td>
</tr>
<tr>
<td style="text-align: center;">RanPac [37]</td>
<td style="text-align: center;">93.92±0.48</td>
<td style="text-align: center;">91.10±0.35</td>
<td style="text-align: center;">94.15±0.38</td>
<td style="text-align: center;">92.32±0.76</td>
<td style="text-align: center;">94.14±0.39</td>
</tr>
<tr>
<td style="text-align: center;">Dap [24]</td>
<td style="text-align: center;">94.48±0.51</td>
<td style="text-align: center;">92.65±0.45</td>
<td style="text-align: center;">92.77±0.39</td>
<td style="text-align: center;">95.51±0.40</td>
<td style="text-align: center;">91.62±0.47</td>
</tr>
<tr>
<td style="text-align: center;">SCDEM ${ }^{2}$ (Ours)</td>
<td style="text-align: center;">97.16±0.06</td>
<td style="text-align: center;">99.81±0.09</td>
<td style="text-align: center;">96.43±0.05</td>
<td style="text-align: center;">99.72±0.13</td>
<td style="text-align: center;">96.51±0.08</td>
</tr>
<tr>
<td style="text-align: center;">ReLER vs DER+++re</td>
<td style="text-align: center;">$\downarrow 44.42 \%$</td>
<td style="text-align: center;">$\downarrow 64.78 \%$</td>
<td style="text-align: center;">$\downarrow 37.80 \%$</td>
<td style="text-align: center;">$\uparrow 38.09 \%$</td>
<td style="text-align: center;">$\downarrow 43.44 \%$</td>
</tr>
<tr>
<td style="text-align: center;">SCDEM ${ }^{3}$ (Ours)</td>
<td style="text-align: center;">97.83±0.38</td>
<td style="text-align: center;">99.50±0.44</td>
<td style="text-align: center;">97.22±0.41</td>
<td style="text-align: center;">99.42±0.50</td>
<td style="text-align: center;">97.32±0.34</td>
</tr>
<tr>
<td style="text-align: center;">ReLER vs StarPrompt</td>
<td style="text-align: center;">$\downarrow 5.65 \%$</td>
<td style="text-align: center;">$\downarrow 4.32 \%$</td>
<td style="text-align: center;">$\downarrow 7.02 \%$</td>
<td style="text-align: center;">$\downarrow 70.85 \%$</td>
<td style="text-align: center;">$\uparrow 2.68 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of Class-IL accuracy in TinyImageNet.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">5 step</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">10 step</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">20 step</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Last</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Last</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Last</td>
</tr>
<tr>
<td style="text-align: center;">DER [6]</td>
<td style="text-align: center;">53.89</td>
<td style="text-align: center;">89.25</td>
<td style="text-align: center;">44.41</td>
<td style="text-align: center;">94.40</td>
<td style="text-align: center;">33.26</td>
<td style="text-align: center;">94.81</td>
</tr>
<tr>
<td style="text-align: center;">DER++ [6]</td>
<td style="text-align: center;">70.12</td>
<td style="text-align: center;">90.20</td>
<td style="text-align: center;">70.61</td>
<td style="text-align: center;">93.45</td>
<td style="text-align: center;">70.92</td>
<td style="text-align: center;">96.20</td>
</tr>
<tr>
<td style="text-align: center;">DER+++refresh[52]</td>
<td style="text-align: center;">70.13</td>
<td style="text-align: center;">90.26</td>
<td style="text-align: center;">69.77</td>
<td style="text-align: center;">93.20</td>
<td style="text-align: center;">72.11</td>
<td style="text-align: center;">94.88</td>
</tr>
<tr>
<td style="text-align: center;">MoE-2E/1R [56]</td>
<td style="text-align: center;">22.91</td>
<td style="text-align: center;">84.55</td>
<td style="text-align: center;">13.80</td>
<td style="text-align: center;">89.45</td>
<td style="text-align: center;">6.38</td>
<td style="text-align: center;">69.80</td>
</tr>
<tr>
<td style="text-align: center;">iCaRL [43]</td>
<td style="text-align: center;">75.08</td>
<td style="text-align: center;">63.75</td>
<td style="text-align: center;">69.56</td>
<td style="text-align: center;">53.55</td>
<td style="text-align: center;">63.03</td>
<td style="text-align: center;">38.80</td>
</tr>
<tr>
<td style="text-align: center;">FDR [5]</td>
<td style="text-align: center;">21.01</td>
<td style="text-align: center;">67.02</td>
<td style="text-align: center;">9.56</td>
<td style="text-align: center;">92.90</td>
<td style="text-align: center;">5.36</td>
<td style="text-align: center;">95.60</td>
</tr>
<tr>
<td style="text-align: center;">AGEM-R [8]</td>
<td style="text-align: center;">24.82</td>
<td style="text-align: center;">89.85</td>
<td style="text-align: center;">10.25</td>
<td style="text-align: center;">93.41</td>
<td style="text-align: center;">5.17</td>
<td style="text-align: center;">95.00</td>
</tr>
<tr>
<td style="text-align: center;">RanPac [37]</td>
<td style="text-align: center;">72.81</td>
<td style="text-align: center;">69.00</td>
<td style="text-align: center;">72.89</td>
<td style="text-align: center;">70.70</td>
<td style="text-align: center;">73.99</td>
<td style="text-align: center;">74.45</td>
</tr>
<tr>
<td style="text-align: center;">Dap [24]</td>
<td style="text-align: center;">76.42</td>
<td style="text-align: center;">72.89</td>
<td style="text-align: center;">65.98</td>
<td style="text-align: center;">66.30</td>
<td style="text-align: center;">47.26</td>
<td style="text-align: center;">49.40</td>
</tr>
<tr>
<td style="text-align: center;">StarPrompt [38]</td>
<td style="text-align: center;">87.99</td>
<td style="text-align: center;">86.10</td>
<td style="text-align: center;">86.92</td>
<td style="text-align: center;">85.39</td>
<td style="text-align: center;">86.31</td>
<td style="text-align: center;">85.60</td>
</tr>
<tr>
<td style="text-align: center;">SCDEM ${ }^{2}$ (Ours)</td>
<td style="text-align: center;">92.48</td>
<td style="text-align: center;">90.20</td>
<td style="text-align: center;">94.02</td>
<td style="text-align: center;">92.00</td>
<td style="text-align: center;">92.73</td>
<td style="text-align: center;">95.39</td>
</tr>
</tbody>
</table>
<p>Multi-domain Task Incremental Learning. We examine a variety of domain combinations, including six two-domain configurations, two three-domain setups, and one four-domain scenario. The performance is evaluated using two key metrics: "Average" and "Last." For the two-domain scenarios, we test different orderings of domains to assess how the models generalize under various configurations. Additionally, we investigate both dual-ViT and triple-ViT approaches to determine whether the inclusion of multiple pretrained backbones can enhance generalization performance.
Results Analysis. The classification performance of our approach, compared with several SOTA methods, is shown in table 1 and table 2. The results clearly indicate that our method, referred to as "Ours," achieves superior average performance in nearly all task configurations when using the dual-ViT setup. In addition, memory replay-based approaches, such as DER, and mixture-of-experts models like MoE tend to show weaker performance in the multidomain task settings. These methods exhibit strong performance on the current task, reflected in their relatively high "Last" scores, demonstrating limited ability to prevent catastrophic forgetting.</p>
<p>In the dual-domain setting, our method (SCDEM ${ }^{3}$ ) outperforms StarPrompt by $17.25 \%$ on the Average metric and $20.65 \%$ on the Last metric. For the three-domain and four-domain configurations,</p>
<p>Table 4: Comparison of Class-IL accuracy in CIFAR100.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">5 step</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">10 step</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">20 step</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Last</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Last</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Last</td>
</tr>
<tr>
<td style="text-align: center;">DER [6]</td>
<td style="text-align: center;">55.54</td>
<td style="text-align: center;">95.32</td>
<td style="text-align: center;">37.45</td>
<td style="text-align: center;">97.30</td>
<td style="text-align: center;">10.02</td>
<td style="text-align: center;">99.40</td>
</tr>
<tr>
<td style="text-align: center;">DER++ [6]</td>
<td style="text-align: center;">77.80</td>
<td style="text-align: center;">93.95</td>
<td style="text-align: center;">75.48</td>
<td style="text-align: center;">97.70</td>
<td style="text-align: center;">74.83</td>
<td style="text-align: center;">97.65</td>
</tr>
<tr>
<td style="text-align: center;">DER+++refresh[52]</td>
<td style="text-align: center;">77.54</td>
<td style="text-align: center;">95.15</td>
<td style="text-align: center;">76.11</td>
<td style="text-align: center;">96.40</td>
<td style="text-align: center;">76.35</td>
<td style="text-align: center;">97.82</td>
</tr>
<tr>
<td style="text-align: center;">MoE-2E/1R [56]</td>
<td style="text-align: center;">85.83</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">84.52</td>
<td style="text-align: center;">87.20</td>
<td style="text-align: center;">84.30</td>
<td style="text-align: center;">83.60</td>
</tr>
<tr>
<td style="text-align: center;">iCaRL [43]</td>
<td style="text-align: center;">78.89</td>
<td style="text-align: center;">81.85</td>
<td style="text-align: center;">79.33</td>
<td style="text-align: center;">76.13</td>
<td style="text-align: center;">79.64</td>
<td style="text-align: center;">76.25</td>
</tr>
<tr>
<td style="text-align: center;">FDR [5]</td>
<td style="text-align: center;">22.03</td>
<td style="text-align: center;">95.85</td>
<td style="text-align: center;">11.79</td>
<td style="text-align: center;">97.65</td>
<td style="text-align: center;">6.96</td>
<td style="text-align: center;">99.45</td>
</tr>
<tr>
<td style="text-align: center;">AGEM-R [8]</td>
<td style="text-align: center;">22.41</td>
<td style="text-align: center;">95.30</td>
<td style="text-align: center;">14.21</td>
<td style="text-align: center;">98.30</td>
<td style="text-align: center;">7.76</td>
<td style="text-align: center;">98.20</td>
</tr>
<tr>
<td style="text-align: center;">RanPac [37]</td>
<td style="text-align: center;">76.85</td>
<td style="text-align: center;">78.65</td>
<td style="text-align: center;">77.03</td>
<td style="text-align: center;">77.20</td>
<td style="text-align: center;">77.12</td>
<td style="text-align: center;">74.20</td>
</tr>
<tr>
<td style="text-align: center;">Dap [24]</td>
<td style="text-align: center;">40.03</td>
<td style="text-align: center;">38.55</td>
<td style="text-align: center;">24.68</td>
<td style="text-align: center;">5.60</td>
<td style="text-align: center;">13.21</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;">StarPrompt [38]</td>
<td style="text-align: center;">88.32</td>
<td style="text-align: center;">90.45</td>
<td style="text-align: center;">93.62</td>
<td style="text-align: center;">99.00</td>
<td style="text-align: center;">86.16</td>
<td style="text-align: center;">83.80</td>
</tr>
<tr>
<td style="text-align: center;">SCDEM ${ }^{2}$ (Ours)</td>
<td style="text-align: center;">94.61</td>
<td style="text-align: center;">96.39</td>
<td style="text-align: center;">96.61</td>
<td style="text-align: center;">98.20</td>
<td style="text-align: center;">98.00</td>
<td style="text-align: center;">98.40</td>
</tr>
</tbody>
</table>
<p>our method (SCDEM ${ }^{3}$ ) shows improvements of $3.33 \%$ and $28.93 \%$, respectively. It is noteworthy that the performance of the threebackbone network model consistently surpasses that of the dualbackbone network across all task configurations. This suggests that incorporating an additional suitable backbone can further enhance the model's performance.
Class Incremental Learning. To accommodate the expert mechanism, our model requires the task identifier during inference, which is typical for the Task-IL scenario. To extend the model's applicability to the Class-IL scenario, we propose a novel approach. Specifically, when a new task is introduced, the fused feature representation from the backbone network is input into all experts, each generating their respective log-likelihoods. By computing the entropy of each expert's distribution and the Kullback-Leibler (KL) divergence between their distributions and the overall fused distribution, we derive a "confidence score" for each expert. The expert with the highest confidence score is selected as the output head. This procedure, illustrated in fig. 2(b), does not require the task identifier and involves minimal computational overhead, making it a lightweight expert selection mechanism. Experimental results, summarized in table 3 and table 4 , show that our model consistently outperforms all other methods across all task configurations,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (a) and (b) illustrate the feature distributions of the final layer from the dual-backbone network using t-SNE and UMAP, respectively. (c) and (d) compare the cosine distance statistics between the output features and the baseline.
demonstrating its effectiveness and stability in continual learning. We provide additional results in Appendix-B from SM.
Computational Cost. We evaluate the computational efficiency of our method in comparison with other baseline approaches by analyzing both computational costs and the number of parameters. A detailed comparison of various models is presented in table 6 , including metrics such as training parameters (M), average GPU memory usage (MiB), and runtime efficiency (it/s). Our proposed framework, which leverages a dual-backbone architecture, demonstrates a clear advantage over existing state-of-the-art (SOTA) methods. When compared to the prominent SOTA method StarPrompt, our approach achieves a reduction in training parameters by $51.08 \%$, GPU memory usage by $47.29 \%$, and training time by $83.21 \%$. These results highlight the efficiency of our method in enhancing continual learning for ViT-based models.</p>
<h3>4.3 Ablation study</h3>
<p>Analysis of Modules. Table 5 reports an ablation study evaluating the contribution of each module in SCDEM ${ }^{2}$ on Tiny-ImageNet and CIFAR100. Removing the COM module ( $\sim$ No Collaboration) leads to a performance drop of $1.01 \%$ and $0.42 \%$, respectively, confirming that dual-backbone representations are not merely additive but synergistic-supporting the learning of richer and more disentangled abstractions. Excluding the Wasserstein Distance constraint (FDC) ( $\sim$ No W-D Constraint) yields a drop of $0.45 \%$ and $0.20 \%$, suggesting that aligning feature distributions across tasks serves as an implicit regularizer, enhancing temporal consistency without explicit memory replay. Removing feature attention (DLWFAM)( $\sim$ No Attention) further reduces accuracy by $0.37 \%$ and $0.14 \%$, demonstrating its effectiveness in amplifying transferable knowledge while suppressing task-specific noise. Overall, these results underscore that SCDEM ${ }^{2}$ is not a collection of heuristics but a purposefully</p>
<p>Table 5: Performance comparison from ablation studies on Tiny-ImagNet and Cifar100 by divided into 10 tasks, all results are averaged over 5 runs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method / Backbone(s)</th>
<th style="text-align: center;">Tiny-ImageNet</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Cifar100</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">$\Delta$</td>
</tr>
<tr>
<td style="text-align: center;">In21k-ft-In1k (ViT_1)</td>
<td style="text-align: center;">94.55</td>
<td style="text-align: center;">0.39 $\downarrow$</td>
<td style="text-align: center;">92.62</td>
<td style="text-align: center;">0.39 $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">In21k (ViT_2)</td>
<td style="text-align: center;">89.03</td>
<td style="text-align: center;">5.91 $\downarrow$</td>
<td style="text-align: center;">87.82</td>
<td style="text-align: center;">6.41 $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">ViT_1 + ViT_2</td>
<td style="text-align: center;">93.91</td>
<td style="text-align: center;">1.03 $\downarrow$</td>
<td style="text-align: center;">92.48</td>
<td style="text-align: center;">1.75 $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">SCDEM $^{2}$</td>
<td style="text-align: center;">94.94</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">94.23</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">-No Collaboratio (COM)</td>
<td style="text-align: center;">93.93</td>
<td style="text-align: center;">1.01 $\downarrow$</td>
<td style="text-align: center;">93.81</td>
<td style="text-align: center;">0.42 $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">-No W-D Constraint (FDC)</td>
<td style="text-align: center;">94.49</td>
<td style="text-align: center;">0.45 $\downarrow$</td>
<td style="text-align: center;">94.03</td>
<td style="text-align: center;">0.20 $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">-No Attention (DLWFAM)</td>
<td style="text-align: center;">94.57</td>
<td style="text-align: center;">0.37 $\downarrow$</td>
<td style="text-align: center;">94.09</td>
<td style="text-align: center;">0.14 $\downarrow$</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of our method with other SOTA methods in terms of training parameters, GPU usage, and training time. All results are from the "Tiny-Birds" task scenario on RTX 4090 (24GB) and averaged over 5 runs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Params $\downarrow$</th>
<th style="text-align: center;">GPU Avg $\downarrow$</th>
<th style="text-align: center;">Iteration $\uparrow$</th>
<th style="text-align: center;">Task Time $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DER++ [6]</td>
<td style="text-align: center;">42.27 M</td>
<td style="text-align: center;">3490 MiB</td>
<td style="text-align: center;">$3.22 \mathrm{it} / \mathrm{s}$</td>
<td style="text-align: center;">110.5 s</td>
</tr>
<tr>
<td style="text-align: left;">DER+++re [52]</td>
<td style="text-align: center;">42.27 M</td>
<td style="text-align: center;">9914 MiB</td>
<td style="text-align: center;">$2.27 \mathrm{it} / \mathrm{s}$</td>
<td style="text-align: center;">357.74 s</td>
</tr>
<tr>
<td style="text-align: left;">MoE-22E [56]</td>
<td style="text-align: center;">64.05 M</td>
<td style="text-align: center;">21362 MiB</td>
<td style="text-align: center;">$1.93 \mathrm{it} / \mathrm{s}$</td>
<td style="text-align: center;">266.65 s</td>
</tr>
<tr>
<td style="text-align: left;">StarPrompt [38]</td>
<td style="text-align: center;">86.41 M</td>
<td style="text-align: center;">10112 MiB</td>
<td style="text-align: center;">$2.49 \mathrm{it} / \mathrm{s}$</td>
<td style="text-align: center;">424.19 s</td>
</tr>
<tr>
<td style="text-align: left;">RanPac [37]</td>
<td style="text-align: center;">1.49 M</td>
<td style="text-align: center;">3566 MiB</td>
<td style="text-align: center;">$3.44 \mathrm{it} / \mathrm{s}$</td>
<td style="text-align: center;">250.82 s</td>
</tr>
<tr>
<td style="text-align: left;">Dap [24]</td>
<td style="text-align: center;">0.68 M</td>
<td style="text-align: center;">4420 MiB</td>
<td style="text-align: center;">$2.33 \mathrm{it} / \mathrm{s}$</td>
<td style="text-align: center;">147.08 s</td>
</tr>
<tr>
<td style="text-align: left;">SCDEM $^{2}$ (Ours)</td>
<td style="text-align: center;">42.27 M</td>
<td style="text-align: center;">5330 MiB</td>
<td style="text-align: center;">$4.71 \mathrm{it} / \mathrm{s}$</td>
<td style="text-align: center;">71.05 s</td>
</tr>
<tr>
<td style="text-align: left;">vs StarPrompt</td>
<td style="text-align: center;">$-51.08 \% \downarrow$</td>
<td style="text-align: center;">$-47.29 \% \downarrow$</td>
<td style="text-align: center;">$+89.16 \% \uparrow$</td>
<td style="text-align: center;">$-83.21 \% \downarrow$</td>
</tr>
</tbody>
</table>
<p>structured system that balances stability and plasticity through architectural alignment and semantic selection.
Analysis of W-D Constraint (FDC). To evaluate the impact of feature alignment using Wasserstein Distance (W-D), we randomly selected 20 classes from TinyImageNet and visualized feature distributions via t-SNE and UMAP. As illustrated in fig. 3.(a) and (b), features constrained by W-D remain significantly closer to the Baseline distribution-i.e., the output of the frozen pretrained backbone-compared to the unconstrained fine-tuned version. This suggests that W-D helps preserve the semantic geometry of the original feature space while enabling adaptation to new tasks.</p>
<p>The histograms in (c) and (d) further quantify this effect: SCDEM achieves notably lower cosine distances to the Baseline ( 0.23 vs. 0.36 and 0.19 vs. 0.46 ), reflecting a smaller deviation from the pretrained representations. From a modeling perspective, the fused features are softly regularized toward their historical counterparts, encouraging geometric alignment in both global and local structure. This alignment acts as a structural prior that supports stable yet flexible representation learning across tasks. Additional results are provided in Appendix-C from SM.</p>
<h2>5 Conclusion</h2>
<p>This paper proposes the SCDEM to deal with multiple data domains over time, which can balance adaptability and stability without relying on replay buffers. The three mechanisms, including COM, FDC and DLWFAM are introduced to enhance the adaptability while preventing network forgetting. The empirical results demonstrate that the proposed approach achieves state-of-the-art performance.</p>
<h2>References</h2>
<p>[1] A. Achille, T. Eccles, L. Matthey, C. Burgess, N. Watters, A. Lerchner, and I. Higgins. 2018. Life-long disentangled representation learning with cross-domain latent homologies. In Advances in Neural Information Processing Systems (NeurIPS). $9873-9883$.
[2] Hongjoon Ahn, Sungmin Cha, Donggyu Lee, and Taesup Moon. 2019. Uncertainty-based Continual Learning with Adaptive Regularization. In Advances in Neural Information Processing Systems. 4394-4404.
[3] Jihwan Bang, Heesu Kim, Youngloon Yoo, Jung-Woo Ha, and Jonghyun Choi. 2021. Rainbow Memory: Continual Learning with a Memory of Diverse Samples. In Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 8218-8227.
[4] Jihwan Bang, Hyunseo Koh, Seulki Park, Hwanjun Song, Jung-Woo Ha, and Jonghyun Choi. 2022. Online Continual Learning on a Contaminated Data Stream With Blurry Task Boundaries. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 9275-9284.
[5] Ari S Benjamin, David Rolnick, and Konrad Kording. 2018. Measuring and regularizing networks in function space. arXiv preprint arXiv:1805.08289 (2018).
[6] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. 2020. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems 33 (2020), 1592015930.
[7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. 2021. Co2l: Contrastive continual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 9516-9525.
[8] Anlan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. 2018. Efficient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420 (2018).
[9] A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. Dokania, P. H. S. Torr, and M.A. Ranzato. 2019. On Tiny Episodic Memories in Continual Learning. arXiv preprint arXiv:1902.10486 (2019).
[10] C. Cortes, X. Gonzalvo, V. Kuznetsov, M. Mohri, and S. Yang. 2017. Adanet: Adaptive structural learning of artificial neural networks. In Proc. of Int. Conf. on Machine Learning (ICML), vol. PMLR 70. 874-885.
[11] Danruo Deng, Guangyong Chen, Jianye Hao, Qiong Wang, and Pheng-Ann Heng. 2021. Flattening Sharpness for Dynamic Gradient Projection Memory Benefits Continual Learning. Advances in Neural Information Processing Systems 34 (2021), $18710-18721$.
[12] Mohammad Mahdi Derakhshani, Xiantong Zhen, Ling Shao, and Cees Snoek. 2021. Kernel Continual Learning. In International Conference on Machine Learning. PMLR, 2621-2631.
[13] Shibbamti Dohare, J Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, A Rupam Mahmood, and Richard S Sutton. 2024. Loss of plasticity in deep continual learning. Nature 632, 8026 (2024), 768-774.
[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).
[15] Arthur Douillard, Alexandre Ramé, Guillaume Couairon, and Matthieu Cord. 2022. Dytos: Transformers for continual learning with dynamic token expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9285-9295.
[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. 2014. Generative adversarial nets. In Proc. Advances in Neural Inf. Proc. Systems (NIPS). 2672-2680.
[17] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey. International Journal of Computer Vision 129 (2021), $1789-1819$.
[18] Yanan Gu, Xu Yang, Kun Wei, and Cheng Deng. 2022. Not Just Selection, but Exploration: Online Class-Incremental Continual Learning via Dual View Consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 7442-7451.
[19] Yiduo Guo, Bing Liu, and Dongyan Zhao. 2022. Online continual learning through mutual information maximization. In International Conference on Machine Learning. PMLR, 8109-8126.
[20] G. Hinton, O. Vinyals, and J. Dean. 2014. Distilling the knowledge in a neural network. In Proc. NIPS Deep Learning Workshop. arXiv preprint arXiv:1503.02531.
[21] Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-Song Chen. 2019. Compacting, Picking and Growing for Unforgetting Continual Learning. In Advances in Neural Information Processing Systems. 1364713657.
[22] Fushuo Huo, Wenchao Xu, Jingcai Guo, Haozhao Wang, and Yunfeng Fan. 2024. Non-exemplar Online Class-Incremental Continual Learning via Dual-Prototype Self-Augment and Refinement. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 12698-12707.
[23] Sauran Jha, Dong Gong, He Zhao, and Lina Yao. 2024. NPCL: Neural processes for uncertainty-aware continual learning. Advances in Neural Information Processing</p>
<p>Systems 36 (2024).
[24] Dabuin Jung, Dongyoon Han, Jihwan Bang, and Hwanjun Song. 2023. Generating instance-level prompts for rehearsal-free continual learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 11847-11857.
[25] Dabuin Jung, Dongjin Lee, Sunwon Hong, Hyemi Jang, Ho Bae, and Sungroh Yoon. 2023. New insights for the stability-plasticity dilemma in online continual learning. arXiv preprint arXiv:2302.08741 (2023).
[26] Hanyang Kang, Rusty John Lloyd Mina, Sultan Rizky Hikmawan Madjid, Jaehong Yoon, Mark Hasegawa-Johnson, Sung Ju Hwang, and Chang D Yoo. 2022. Forgetfree Continual Learning with Winning Subnetworks. In International Conference on Machine Learning. PMLR, 10734-10750.
[27] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. 2018. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.
[28] Junsu Kim, Hoseong Cho, Jihyeon Kim, Yihalem Yimolal Tiruneh, and Seungryul Baek. 2024. Sddgr: Stable diffusion-based deep generative replay for class incremental object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 28772-28781.
[29] D. P. Kingma and M. Welling. 2013. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114 (2013).
[30] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning multiple layers of features from tiny images. Technical Report. Univ. of Toronto.
[31] Ya Le and Xuan Yang. 2015. Tiny imageNet visual recognition challenge. Technical Report. Univ. of Stanford. 1-6 pages.
[32] Z. Li and D. Hoiem. 2017. Learning without forgetting. IEEE Trans. on Pattern Analysis and Machine Intelligence 40, 12 (2017), 2935-2947.
[33] David Lopez-Paz and Marc'Aurelio Ranzato. 2017. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems. $6467-6476$.
[34] Daniel Marczak, Sebastian Cygert, Tomasz Trzciński, and Bartłomiej Twardowski. 2024. Revisiting Supervision for Continual Representation Learning. In European Conference on Computer Vision. Springer, 181-197.
[35] James Martens and Roger B. Grosse. 2015. Optimizing Neural Networks with Kronecker-factored Approximate Curvature. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 (JMLR Workshop and Conference Proceedings, Vol. 37), Francis R. Bach and David M. Blei (Eds.). JMLR.org, 2408-2417. http://proceedings.mlr.press/v37/ martens15.html
[36] Mark D. McDonnell, Dong Gong, Amin Parvaneh, Ehsan Abbasnejad, and Anton van den Hengel. 2023. RanPAC: Random Projections and Pre-trained Models for Continual Learning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/ 2793dc35e14003dd367684d93d236847-Abstract-Conference.html
[37] Mark D McDonnell, Dong Gong, Amin Parvaneh, Ehsan Abbasnejad, and Anton Van den Hengel. 2023. Ranpac: Random projections and pre-trained models for continual learning. Advances in Neural Information Processing Systems 36 (2023), 12022-12053.
[38] Martin Menabue, Emanuele Frascaroli, Matteo Boschini, Enver Sangineto, Lorenzo Bonicelli, Angelo Porrello, and Simone Calderara. 2024. Semantic Residual Prompts for Continual Learning. arXiv preprint arXiv:2403.06870 (2024).
[39] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. 2018. Variational continual learning. In Proc. of Int. Conf. on Learning Representations (ICLR), arXiv preprint arXiv:1710.10628.
[40] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter. 2019. Continual lifelong learning with neural networks: A review. Neural Networks 113 (2019), $54-71$.
[41] R. Polikar, L. Upda, S. S. Upda, and Vasant Honavar. 2001. Learn++: An incremental learning algorithm for supervised neural networks. IEEE Trans. on Systems Man and Cybernetics, Part C 31, 4 (2001), 497-508.
[42] J. Ramapuram, M. Gregorova, and A. Kalousis. 2017. Lifelong generative modeling. In Proc. Int. Conf. on Learning Representations (ICLR), arXiv preprint arXiv:1705.09847.
[43] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. 2017. iCalRL: Incremental classifier and representation learning. In Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2001-2010.
[44] B. Ren, H. Wang, J. Li, and H. Gao. 2017. Life-long learning based on dynamic combination model. Applied Soft Computing 56 (2017), 390-404.
[45] Hippolyt Ritter, Aleksandar Botev, and David Barber. 2018. Online Structured Laplace Approximations for Overcoming Catastrophic Forgetting. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 31. 3742-3752.
[46] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raja Hahsell. 2016. Progressive neural networks. arXiv preprint arXiv:1606.04671 (2016).
[47] Yujun Shi, Li Yuan, Yunpeng Chen, and Jiashi Feng. 2021. Continual learning via bit-level information preserving. In Proceedings of the IEEE/CVF Conference on</p>
<p>Computer Vision and Pattern Recognition. 16674-16683.
[48] H. Shin, J. K. Lee, J. Kim, and J. Kim. 2017. Continual learning with deep generative replay. In Advances in Neural Inf. Proc. Systems (NIPS). 2990-2999.
[49] Rishabh Tiwari, Krishnateja Killamsetty, Rishabh Iyer, and Pradeep Shenoy. 2022. GCR: Gradient Coreset Based Replay Buffer Selection for Continual Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 99-108.
[50] Vinay Kumar Verma, Kevin J Liang, Nikhil Mehta, Piyush Rai, and Lawrence Carin. 2021. Efficient feature transformations for discriminative and generative continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 13865-13875.
[51] Shipeng Wang, Xiaorong Li, Jian Sun, and Zongben Xu. 2021. Training networks in null space of feature covariance for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 184-193.
[52] Zhenyi Wang, Yan Li, Li Shen, and Heng Huang. 2024. A unified and general framework for continual learning. arXiv preprint arXiv:2403.13249 (2024).
[53] Yeming Wen, Dustin Tran, and Jimmy Ba. 2020. BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning. In Proc. Int. Conf. on</p>
<p>Learning Representations (ICLR), arXiv preprint arXiv:2002.06715.
[54] Mengqi Xue, Haofei Zhang, Jie Song, and Mingli Song. 2022. Meta-attention for vit-backed continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 150-159.
[55] Fei Ye and Adrian G Bors. 2023. Wasserstein Expansible Variational Autoencoder for Discriminative and Generative Continual Learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 18665-18675.
[56] Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang, Hachuan Lu, and You He. 2024. Boosting continual learning of vision-language models via mixture-ofexperts adapters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 23219-23230.
[57] M. Zhai, L. Chen, F. Tung, J He, M. Nawhal, and G. Mori. 2019. Lifelong GAN: Continual Learning for Conditional Image Generation. In Proc. of the IEEE/CVF Int. Conf. on Computer Vision (ICCV). 2759-2768.
[58] Guanyu Zhou, Kihyuk Sohn, and Honglak Lee. 2012. Online incremental feature learning with denoising autoencoders. In Proc. Int. Conf. on Artificial Intelligence and Statistics (AISTATS), vol. PMLR 22. 1453-1461.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
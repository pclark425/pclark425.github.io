<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8084 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8084</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8084</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-607d8df2c6c5f103f10a2d631d7364cc952cb489</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/607d8df2c6c5f103f10a2d631d7364cc952cb489" target="_blank">BatchEval: Towards Human-like Text Evaluation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Inspired by the fact that humans treat both criterion definition and inter sample comparison as references for evaluation, BatchEval is proposed, a paradigm that conducts batch-wise evaluation iteratively to alleviate the above problems.</p>
                <p><strong>Paper Abstract:</strong> Significant progress has been made in automatic text evaluation with the introduction of large language models (LLMs) as evaluators. However, current sample-wise evaluation paradigm suffers from the following issues: (1) Sensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble performance with static reference. Inspired by the fact that humans treat both criterion definition and inter sample comparison as references for evaluation, we propose BatchEval, a paradigm that conducts batch-wise evaluation iteratively to alleviate the above problems. We explore variants under this paradigm and confirm the optimal settings are two stage procedure with heterogeneous batch composition strategy and decimal scoring format. Comprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate that BatchEval outperforms state-of-the-art methods by 10.5% on Pearson correlations with only 64% API cost on average. Further analyses have been conducted to verify the robustness, generalization, and working mechanism of BatchEval.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8084.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8084.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BatchEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BATCHEval (Batch-wise Evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch-wise LLM-based text evaluation paradigm that iteratively groups samples into batches so the LLM can compare samples in-batch as an additional reference to the criterion; uses heterogeneous batch composition, two-stage analyze-then-score procedure, and decimal scoring by default.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>BatchEval: Towards Human-like Text Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>turn-level response evaluation; dialogue-level evaluation; story generation; summarization factual consistency</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Topical-Chat; FED-dialog; HANNA; QAGS</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 (primary); also GPT-3.5-turbo; Llama-2-70b-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 (0613) used as main judge; experiments also run with GPT-3.5-turbo (0613) and Llama-2-70b-chat-hf. Default BATCHEval hyperparameters: two-stage procedure, heterogeneous batch composition, decimal scoring, iterations N=5, batch size B=10, decoding temperature 0.2; each iteration shares one prompt for the batch.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>benchmark dataset human raters (dataset-provided annotators); inter-annotator agreement reported as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r (r_p) and Spearman rho (r_s) vs human ratings</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.752</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>None reported for BATCHEval itself; contrasts with prior LLM judges which: lag behind humans in alignment; are sensitive to prompt design; have poor resistance to noise; lack inter-sample comparison leading to non-uniform score distributions; exhibit low ensemble diversity under static-reference generation; limited by LLM context length</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>BATCHEval produces more uniform (less peaked) score distributions, higher discrimination between samples, greater robustness to prompt rewriting and to input noise, and higher variance across generations (improving ensemble error). Attention analysis on Llama-2-70b-chat-hf suggests deeper layers use in-batch comparisons and criterion to finalize scores.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Improves alignment with human judgments compared to prior LLM-based metrics (example: +~10.5% Pearson relative improvement vs best baselines across four benchmarks); lower API cost in experiments (reported ~64% of baseline API cost) because batch samples share prompts and fewer generations are averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Two-stage (analyze all samples, then score), heterogeneous batch redrawing across iterations, decimal scoring format; 5 iterations, batchsize 10, temperature 0.2. Ensemble scores are averages of per-iteration scores. Compared against baselines that used 20 generations per sample.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BatchEval: Towards Human-like Text Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8084.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8084.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CloserLook</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CloserLook (sample-wise LLM evaluator / analyze-first variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sample-wise LLM evaluation approach (analyze-then-score) used as a competitive baseline; operates on each sample independently and often uses chain-of-thought style analyses before scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A closer look into using large language models for automatic evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>BatchEval: Towards Human-like Text Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>turn-level response evaluation; dialogue-level evaluation; story generation; summarization factual consistency (used as baseline across same benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Topical-Chat; FED-dialog; HANNA; QAGS</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Implemented with multiple LLMs in paper: GPT-3.5-turbo, GPT-4, Llama-2-70b-chat-hf (reproduced variants)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Reproduced according to Chiang & Lee (2023) defaults; baseline ensemble produced by averaging 20 independent generations per sample (static reference/ prompt unchanged across generations).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>benchmark dataset human raters (dataset-provided annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r and Spearman rho vs human ratings</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.682</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>sensitivity to prompt formulation (higher variance across prompt rewrites); uneven, peaked score distributions that reduce robustness to input noise; low inter-generation diversity when ensembling from static reference; limited discrimination between similar-quality samples due to lack of in-batch comparison</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Per-sample evaluation without inter-sample comparison leads to non-uniform scoring (concentration around discrete scores) and greater susceptibility to small perturbations in prompts or input text; demonstrated substantial drop under artificial noise (e.g., a reported drop of ~0.109 Pearson on HANNA when tokens were perturbed).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Strong baseline — significantly better than traditional rule/embedding metrics and learning-based metrics on many criteria; supports chain-of-thought style prompting (analyze-first) which can improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Sample-wise evaluation, typically analyze-then-score (chain-of-thought); baselines in paper averaged 20 independent generations per sample for ensemble; tested sensitivity to prompt rewrites (human-written vs GPT-4 rewritten prompts) and to input noise (5% token synonym replacement + 5% deletion).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BatchEval: Towards Human-like Text Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8084.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8084.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based NLG evaluation method that uses a model (GPT-4 in original) to evaluate generated text according to self-generated procedures/prompts; used here as a competitive baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>G-eval: NLG evaluation using gpi-4 with better human alignment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>BatchEval: Towards Human-like Text Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>turn-level response evaluation; dialogue-level evaluation; story generation; summarization factual consistency (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Topical-Chat; FED-dialog; HANNA; QAGS</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 (as used in original G-Eval; reproduced here with GPT-4 (0613))</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>G-Eval uses GPT-4 and prompts/procedures produced by the model itself per original; in this paper G-Eval was reproduced under a unified API; baseline ensembles used 20 generations per sample.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>benchmark dataset human raters (dataset-provided annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r and Spearman rho vs human ratings</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.633</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Although better than traditional metrics, G-Eval still lags behind BATCHEval in correlation with humans; can be sensitive to prompt/procedure and lacks inter-sample comparison which can reduce robustness to noise and limit ensemble gains.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Per-paper reproduction shows G-Eval performs well across criteria but achieves lower Pearson/Spearman correlation than BATCHEval on the evaluated benchmarks; less uniform scoring distribution than batch-wise method.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Strong alignment with human judgments compared to classic automatic metrics; flexible because it uses model-generated evaluation procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Followed original G-Eval procedure as reproduced by authors; typically single-sample prompts and averaging over multiple independent generations (20) to produce ensemble score.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BatchEval: Towards Human-like Text Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8084.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8084.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Inter-annotator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human inter-annotator agreement (benchmark human raters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human annotators' ratings on the benchmarks used as the ground-truth reference and reported inter-annotator agreement as an upper-bound baseline for automatic evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>BatchEval: Towards Human-like Text Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>turn-level response evaluation; dialogue-level evaluation; story generation; summarization factual consistency</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Topical-Chat; FED-dialog; HANNA; QAGS (human ratings provided by each benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>benchmark dataset human annotators (the original datasets' raters); inter-annotator agreement reported</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r and Spearman rho (inter-annotator correlations reported in tables as reference)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.568</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Human inter-annotator agreement is imperfect (non-1.0), indicating inherent noise/variance in human labels that automatic evaluators must match; used as ground truth for correlation calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Human inter-annotator agreement provides a baseline/upper bound for automatic evaluator correlations; automatic methods aim to approach this consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Not applicable (this entry is the human baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Human ratings provided by the public benchmarks; inter-annotator correlations reported in tables (e.g., Topical-Chat inter-annotator average Pearson r_p ~0.568).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BatchEval: Towards Human-like Text Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A closer look into using large language models for automatic evaluation <em>(Rating: 2)</em></li>
                <li>G-eval: NLG evaluation using gpi-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 1)</em></li>
                <li>Evaluating large language models: A comprehensive survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8084",
    "paper_id": "paper-607d8df2c6c5f103f10a2d631d7364cc952cb489",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "BatchEval",
            "name_full": "BATCHEval (Batch-wise Evaluation)",
            "brief_description": "A batch-wise LLM-based text evaluation paradigm that iteratively groups samples into batches so the LLM can compare samples in-batch as an additional reference to the criterion; uses heterogeneous batch composition, two-stage analyze-then-score procedure, and decimal scoring by default.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "BatchEval: Towards Human-like Text Evaluation",
            "evaluation_task": "turn-level response evaluation; dialogue-level evaluation; story generation; summarization factual consistency",
            "dataset_name": "Topical-Chat; FED-dialog; HANNA; QAGS",
            "judge_model_name": "GPT-4 (primary); also GPT-3.5-turbo; Llama-2-70b-chat-hf",
            "judge_model_details": "GPT-4 (0613) used as main judge; experiments also run with GPT-3.5-turbo (0613) and Llama-2-70b-chat-hf. Default BATCHEval hyperparameters: two-stage procedure, heterogeneous batch composition, decimal scoring, iterations N=5, batch size B=10, decoding temperature 0.2; each iteration shares one prompt for the batch.",
            "human_evaluator_type": "benchmark dataset human raters (dataset-provided annotators); inter-annotator agreement reported as baseline",
            "agreement_metric": "Pearson r (r_p) and Spearman rho (r_s) vs human ratings",
            "agreement_score": 0.752,
            "reported_loss_aspects": "None reported for BATCHEval itself; contrasts with prior LLM judges which: lag behind humans in alignment; are sensitive to prompt design; have poor resistance to noise; lack inter-sample comparison leading to non-uniform score distributions; exhibit low ensemble diversity under static-reference generation; limited by LLM context length",
            "qualitative_findings": "BATCHEval produces more uniform (less peaked) score distributions, higher discrimination between samples, greater robustness to prompt rewriting and to input noise, and higher variance across generations (improving ensemble error). Attention analysis on Llama-2-70b-chat-hf suggests deeper layers use in-batch comparisons and criterion to finalize scores.",
            "advantages_of_llm_judge": "Improves alignment with human judgments compared to prior LLM-based metrics (example: +~10.5% Pearson relative improvement vs best baselines across four benchmarks); lower API cost in experiments (reported ~64% of baseline API cost) because batch samples share prompts and fewer generations are averaged.",
            "experimental_setting": "Two-stage (analyze all samples, then score), heterogeneous batch redrawing across iterations, decimal scoring format; 5 iterations, batchsize 10, temperature 0.2. Ensemble scores are averages of per-iteration scores. Compared against baselines that used 20 generations per sample.",
            "uuid": "e8084.0",
            "source_info": {
                "paper_title": "BatchEval: Towards Human-like Text Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "CloserLook",
            "name_full": "CloserLook (sample-wise LLM evaluator / analyze-first variant)",
            "brief_description": "A sample-wise LLM evaluation approach (analyze-then-score) used as a competitive baseline; operates on each sample independently and often uses chain-of-thought style analyses before scoring.",
            "citation_title": "A closer look into using large language models for automatic evaluation",
            "mention_or_use": "use",
            "paper_title": "BatchEval: Towards Human-like Text Evaluation",
            "evaluation_task": "turn-level response evaluation; dialogue-level evaluation; story generation; summarization factual consistency (used as baseline across same benchmarks)",
            "dataset_name": "Topical-Chat; FED-dialog; HANNA; QAGS",
            "judge_model_name": "Implemented with multiple LLMs in paper: GPT-3.5-turbo, GPT-4, Llama-2-70b-chat-hf (reproduced variants)",
            "judge_model_details": "Reproduced according to Chiang & Lee (2023) defaults; baseline ensemble produced by averaging 20 independent generations per sample (static reference/ prompt unchanged across generations).",
            "human_evaluator_type": "benchmark dataset human raters (dataset-provided annotators)",
            "agreement_metric": "Pearson r and Spearman rho vs human ratings",
            "agreement_score": 0.682,
            "reported_loss_aspects": "sensitivity to prompt formulation (higher variance across prompt rewrites); uneven, peaked score distributions that reduce robustness to input noise; low inter-generation diversity when ensembling from static reference; limited discrimination between similar-quality samples due to lack of in-batch comparison",
            "qualitative_findings": "Per-sample evaluation without inter-sample comparison leads to non-uniform scoring (concentration around discrete scores) and greater susceptibility to small perturbations in prompts or input text; demonstrated substantial drop under artificial noise (e.g., a reported drop of ~0.109 Pearson on HANNA when tokens were perturbed).",
            "advantages_of_llm_judge": "Strong baseline — significantly better than traditional rule/embedding metrics and learning-based metrics on many criteria; supports chain-of-thought style prompting (analyze-first) which can improve performance.",
            "experimental_setting": "Sample-wise evaluation, typically analyze-then-score (chain-of-thought); baselines in paper averaged 20 independent generations per sample for ensemble; tested sensitivity to prompt rewrites (human-written vs GPT-4 rewritten prompts) and to input noise (5% token synonym replacement + 5% deletion).",
            "uuid": "e8084.1",
            "source_info": {
                "paper_title": "BatchEval: Towards Human-like Text Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "G-Eval",
            "name_full": "G-Eval",
            "brief_description": "An LLM-based NLG evaluation method that uses a model (GPT-4 in original) to evaluate generated text according to self-generated procedures/prompts; used here as a competitive baseline.",
            "citation_title": "G-eval: NLG evaluation using gpi-4 with better human alignment",
            "mention_or_use": "use",
            "paper_title": "BatchEval: Towards Human-like Text Evaluation",
            "evaluation_task": "turn-level response evaluation; dialogue-level evaluation; story generation; summarization factual consistency (used as baseline)",
            "dataset_name": "Topical-Chat; FED-dialog; HANNA; QAGS",
            "judge_model_name": "GPT-4 (as used in original G-Eval; reproduced here with GPT-4 (0613))",
            "judge_model_details": "G-Eval uses GPT-4 and prompts/procedures produced by the model itself per original; in this paper G-Eval was reproduced under a unified API; baseline ensembles used 20 generations per sample.",
            "human_evaluator_type": "benchmark dataset human raters (dataset-provided annotators)",
            "agreement_metric": "Pearson r and Spearman rho vs human ratings",
            "agreement_score": 0.633,
            "reported_loss_aspects": "Although better than traditional metrics, G-Eval still lags behind BATCHEval in correlation with humans; can be sensitive to prompt/procedure and lacks inter-sample comparison which can reduce robustness to noise and limit ensemble gains.",
            "qualitative_findings": "Per-paper reproduction shows G-Eval performs well across criteria but achieves lower Pearson/Spearman correlation than BATCHEval on the evaluated benchmarks; less uniform scoring distribution than batch-wise method.",
            "advantages_of_llm_judge": "Strong alignment with human judgments compared to classic automatic metrics; flexible because it uses model-generated evaluation procedures.",
            "experimental_setting": "Followed original G-Eval procedure as reproduced by authors; typically single-sample prompts and averaging over multiple independent generations (20) to produce ensemble score.",
            "uuid": "e8084.2",
            "source_info": {
                "paper_title": "BatchEval: Towards Human-like Text Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Human Inter-annotator",
            "name_full": "Human inter-annotator agreement (benchmark human raters)",
            "brief_description": "Human annotators' ratings on the benchmarks used as the ground-truth reference and reported inter-annotator agreement as an upper-bound baseline for automatic evaluators.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "BatchEval: Towards Human-like Text Evaluation",
            "evaluation_task": "turn-level response evaluation; dialogue-level evaluation; story generation; summarization factual consistency",
            "dataset_name": "Topical-Chat; FED-dialog; HANNA; QAGS (human ratings provided by each benchmark)",
            "judge_model_name": null,
            "judge_model_details": null,
            "human_evaluator_type": "benchmark dataset human annotators (the original datasets' raters); inter-annotator agreement reported",
            "agreement_metric": "Pearson r and Spearman rho (inter-annotator correlations reported in tables as reference)",
            "agreement_score": 0.568,
            "reported_loss_aspects": "Human inter-annotator agreement is imperfect (non-1.0), indicating inherent noise/variance in human labels that automatic evaluators must match; used as ground truth for correlation calculation.",
            "qualitative_findings": "Human inter-annotator agreement provides a baseline/upper bound for automatic evaluator correlations; automatic methods aim to approach this consistency.",
            "advantages_of_llm_judge": "Not applicable (this entry is the human baseline).",
            "experimental_setting": "Human ratings provided by the public benchmarks; inter-annotator correlations reported in tables (e.g., Topical-Chat inter-annotator average Pearson r_p ~0.568).",
            "uuid": "e8084.3",
            "source_info": {
                "paper_title": "BatchEval: Towards Human-like Text Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A closer look into using large language models for automatic evaluation",
            "rating": 2
        },
        {
            "paper_title": "G-eval: NLG evaluation using gpi-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 1
        },
        {
            "paper_title": "Evaluating large language models: A comprehensive survey",
            "rating": 1
        }
    ],
    "cost": 0.0162175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>BatchEval: Towards Human-like Text Evaluation</h1>
<p>Peiwen Yuan^{1}, Shaoxiong Feng^{2}, Yiwei Li^{1}, Xinglin Wang^{1}, Boyuan Pan^{2}
Heda Wang^{2}, Kan Li^{1*}
^{1}School of Computer Science and Technology, Beijing Institute of Technology
^{2}Xiaohongshu Inc
{peiwenyuan, liyiwei,wangxinglin, likan}@bit.edu.cn
{shaoxiongfeng2023,whd.thu}@gmail.com {panby}@zju.edu.cn</p>
<h6>Abstract</h6>
<p>Significant progress has been made in automatic text evaluation with the introduction of large language models (LLMs) as evaluators. However, current sample-wise evaluation paradigm suffers from the following issues: (1) Sensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble performance with static reference. Inspired by the fact that humans treat both criterion definition and inter sample comparison as references for evaluation, we propose BATCHEval, a paradigm that conducts batch-wise evaluation iteratively to alleviate the above problems. We explore variants under this paradigm and confirm the optimal settings are two stage procedure with heterogeneous batch composition strategy and decimal scoring format. Comprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate that BATCHEval outperforms state-of-the-art methods by 10.5% on Pearson correlations with only 64% API cost on average. Further analyses have been conducted to verify the robustness, generalization, and working mechanism of BATCHEval.</p>
<h2>1 Introduction</h2>
<p>Accurately evaluating the text quality of specific criterion (e.g., coherence) can facilitate better understanding, application, and development of large language models (LLMs), which becomes more crucial with their recent rapid progress in text generation capabilities <em>OpenAI (2023)</em>. Due to the labor-intensive and time-consuming nature of human evaluation, early works have explored automatic evaluation methods, which can be categorized into rule-based <em>Papineni et al. (2002); Lavie and Denkowski (2009)</em>, embedding-based <em>Forgues et al. (2014); Zhang et al. (2020)</em>, and learning-based <em>Mehri and Eskénazi (2020); Zhang et al. (2022)</em> approaches. Continuous progress has been achieved through these methods, but there remains a significant gap in their consistency with human judgments <em>Sai et al. (2023)</em>.</p>
<p>Recently, the revolutionary power of LLMs has been applied across various fields, demonstrating performance that is even on par with humans <em>OpenAI (2023); Guo et al. (2023a)</em>. In text evaluation filed, LLM-based evaluators <em>Chiang and Lee (2023a); Liu et al. (2023); Guo et al. (2023b); Chiang and Lee (2023b)</em> have also made significant progress compared to traditional methods, but they still lag behind human evaluators. We carefully compared their working procedures and found that the difference in evaluation references might be the reason for the performance disparity (Figure 1). Human evaluators analyze samples based on the criterion definition and provide discriminative scores through comparison between samples. However, LLM-based evaluators assess each sample individually, thus only having criterion as a reference.</p>
<p>We analyze that such a sample-wise evaluation paradigm will face problems on three aspects: (1) <em>Robustness against prompt design.</em> Since criterion is the sole reference for evaluation, minor changes to the prompt may lead to significant variations in the evaluation results (See §4.4 for empirical validation). (2) <em>Robustness against noise.</em> Due to</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Human evaluators evaluate text quality based on criterion definition and sample comparison, while current LLM-based evaluators only rely on criterion.</p>
<p>the absence of comparison between samples, the evaluation scores lack discrimination and exhibit a non-uniform distribution (See Figure 3), which can lead to reduced robustness against noise ${ }^{2}$ (See Theorem 1). (3) Performance under ensemble. Current LLM-based evaluators average scores from multiple generations as the final rating for given sample. However, generating multiple times from the static reference (criterion) induces a lack of diversity among scores (Figure 4), which can weaken the effect of ensemble according to Theorem 2.</p>
<p>To address the aforementioned problems, we propose BATCHEval, a new LLM-based text evaluation paradigm that assesses samples batch-wise, akin to the way of humans. Overall, BATCHEval iterates through a process where all samples are split into several batches, with each batch then being compiled into a prompt for input to the LLM. By introducing in-batch samples as an additional reference apart from criterion, the orthogonal and complementary references can not only reduce the dependency on prompt design but also enhance the discrimination of scores between samples through in-batch comparison, leading to improved robustness. Furthermore, the iteratively changing batch composition can provide LLMs with varying evaluation references, thereby enhancing diversity and the ensemble performance.</p>
<p>While the idea of BATCHEval is simple, there are many ways it can be realized. We explored variants in evaluation procedure, format of scoring and composition of batch. Some of them work surprisingly well while some do not meet expectations. Experiments and analyses confirm that separate analyzing and scoring evaluation procedure, decimal scoring format, and quality-heterogeneous batch composition strategy yield the optimal results.</p>
<p>We conduct extensive experiments on 4 text evaluation tasks primarily with GPT-4: turn-level response, dialogue, text summarization, and story generation. By allowing in-batch samples to share single prompt and applying a small iteration rounds, BATCHEval outperforms best performing LLMbased evaluators by a significant margin (10.5\%) in terms of correlation with human evaluations, while incurring only $64 \%$ of API costs. We also validate the generalization of BATCHEval on more LLMs, robustness to prompt design and noise, and analyze the choice of hyperparameters through further</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>experiments. Finally, we probe into the working mechanism of BATCHEval through attention analysis on Llama-2-70b-chat-hf. Our contributions are summarized as follows:</p>
<ol>
<li>We analyzed how the sample-wise evaluation paradigm of LLM-based evaluators, differing from human evaluators, limited their robustness and consistency with human judgment.</li>
<li>We proposed BATCHEval, a new paradigm that evaluates texts batch-wise, and experimentally validated its optimal settings.</li>
<li>We validated through experiments on 4 tasks that BATCHEval outperforms public state-of-the-art methods by $10.5 \%$ while incurring only $64 \%$ of the API cost.</li>
<li>We analyzed the generalization, robustness, hyperparameter selection, and probed into the working mechanism of BATCHEval.</li>
</ol>
<h2>2 Background</h2>
<h3>2.1 Automatic Text Evaluation</h3>
<p>Automatic text evaluation method has been extensively studied as a supplement to labor-intensive and time-consuming human evaluation, with its correlation to human judgment as the criterion for assessment. Both rule-based (Papineni et al., 2002; Lavie and Denkowski, 2009) and embedding-based (Zhang et al., 2020; Forgues et al., 2014) evaluation methods rely on the assumption that high-quality generated texts should have a significant word overlap with reference texts. However, this assumption conflicts with the high entropy nature of text generation, restricting its consistency with humans. Learning-based methods consider directly assessing text quality through supervised (Lowe et al., 2017; Goyal and Durrett, 2021) and self-supervised (Mehri and Eskénazi, 2020; Zhang et al., 2022) approaches and achieve significant progress. Recently, LLM-based evaluators (Guo et al., 2023b; Chiang and Lee, 2023b; Liu et al., 2023) have demonstrated advanced consistency with humans leveraging their incredible knowledge and capabilities. However, typical sample-wise evaluation paradigm of the above methods leads to a lack of inter-sample comparison during scoring process, which serves as an important reference for human evaluators. Therefore, we propose BATCHEval to fill this gap for better alignment with humans.</p>
<h3>2.2 The Theorems Involved</h3>
<p>Theorem 1 The robustness against noise correlates positively with the uniformity of evaluator scoring distribution. (See Appendix A for derivation in details)</p>
<p>Yuan et al. (2023) proposed this theorem and verified that learning-based evaluators, by adjusting the training loss function to uniformize the score distribution, can achieve better robustness against noise. We have experimentally proven that samplewise LLM-based evaluators also exhibit an uneven score distribution (Figure 3), which can weaken their robustness against noise (Appendix C). Thus, we propose BATCHEval for a more uniform score distribution and better robustness against noise.</p>
<p>Theorem 2 Given scores from multiple generations of certain $L L M \mathcal{S}=\left{s_{i} \mid i=1, . ., N\right}$ and human evaluation score $y$ for sample $x, \bar{s}$ is the average of $\mathcal{S}$, the following equation holds:</p>
<p>$$
\operatorname{Err}(\bar{s}, y)=\operatorname{Err}(\mathcal{S}, y)-\operatorname{Var}(\mathcal{S})
$$</p>
<p>where:</p>
<p>$$
\begin{array}{r}
\operatorname{Err}(\bar{s}, y)=(\bar{s}-y)^{2} \
\operatorname{Err}(\mathcal{S}, y)=\frac{1}{N} \sum_{i=1}^{N}\left(s_{i}-y\right)^{2} \
\operatorname{Var}(\mathcal{S})=\frac{1}{N} \sum_{i=1}^{N}\left(s_{i}-\bar{s}\right)^{2}
\end{array}
$$</p>
<p>Eq. (1) (Zhou, 2012) implies that smaller average error in single prediction scores $(\operatorname{Err}(\mathcal{S}, y))$ and larger variance among multiple prediction scores $(\operatorname{Var}(\mathcal{S}))$ induce smaller error in ensemble score $(\operatorname{Err}(\bar{s}, y))$. However, current sample-wise LLMbased evaluators score multiple times based solely on static reference (criterion), resulting in smaller $\operatorname{Var}(\mathcal{S})$ (Figure 5). To address this, we propose iterative quality-heterogenized batch composition strategy for LLMs to score with unbiased varying references, thus increasing $\operatorname{Var}(\mathcal{S})$ for lower $\operatorname{Err}(\bar{s}, y)$.</p>
<h2>3 Methodology</h2>
<p>The core idea behind BATCHEval is to fully use in-batch sample comparison to enhance evaluation accuracy and robustness. Algorithm 1 illustrates the working process of BATCHEval, which involves $N$ rounds of iteration: (1) $B$ samples of each batch are compiled with pre-defined (task, criterion, evaluation procedure) into a single prompt for input to the LLM; (2) Based on the LLM's assessment of the samples' quality, we optimize batch allocation according to certain batch composition strategy. The core designs throughout the process are how to evaluate (evaluation procedure), what to input (batch composition strategy), and what to output (scoring format). Below we discuss their potential variants in detail.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Workflow</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">BATCHEVAL</span><span class="o">.</span>
<span class="n">Require</span><span class="p">:</span><span class="w"> </span><span class="n">Samples</span><span class="w"> </span>\<span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="o">|</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="o">|</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">LLM</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">M</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">Evaluation</span><span class="w"> </span><span class="n">procedure</span><span class="w"> </span>\<span class="p">(</span><span class="n">P</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Task</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">criterion</span><span class="w"> </span>\<span class="p">(</span><span class="n">T</span>\<span class="p">),</span><span class="w"> </span><span class="n">Iteration</span><span class="w"> </span><span class="n">rounds</span><span class="w"> </span>\<span class="p">(</span><span class="n">N</span>\<span class="p">),</span><span class="w"> </span><span class="n">Batchsize</span><span class="w"> </span>\<span class="p">(</span><span class="n">B</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Batch</span><span class="w"> </span><span class="n">composition</span><span class="w"> </span><span class="n">strategy</span><span class="w"> </span><span class="n">BATCHSTRATEGY</span>
<span class="n">Ensure</span><span class="p">:</span><span class="w"> </span><span class="n">Ensemble</span><span class="w"> </span><span class="n">evaluation</span><span class="w"> </span><span class="n">scores</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">bar</span><span class="p">{</span><span class="n">s</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="o">|</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="o">|</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="p">:</span><span class="w"> </span><span class="n">Randomly</span><span class="w"> </span><span class="n">divide</span><span class="w"> </span>\<span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="o">|</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="o">|</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">batches</span><span class="w"> </span>\<span class="p">(</span><span class="n">b</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="n">L</span><span class="p">},</span><span class="w"> </span><span class="n">L</span><span class="o">=</span>\<span class="n">left</span>\<span class="n">lceil</span>\<span class="n">frac</span><span class="p">{</span><span class="o">|</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="o">|</span><span class="p">}{</span><span class="n">B</span><span class="p">}</span>\<span class="n">right</span>\<span class="n">rceil</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">S_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">all</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">{</span><span class="n">i</span><span class="p">:[]</span>\<span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">i</span><span class="w"> </span>\<span class="ow">in</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">|</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="o">|</span><span class="p">]</span>\<span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">i</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">S_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">current</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">varnothing</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">j</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">L</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span><span class="p">:</span>
<span class="w">            </span>\<span class="p">(</span><span class="n">S_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">current</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">S_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">current</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="o">.</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">Append</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">M</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="n">P</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="o">^</span><span class="p">{</span><span class="n">j</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span><span class="n">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">S_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">all</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">S_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">all</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="o">.</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">Merge</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">S_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">current</span><span class="w"> </span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">b</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="n">L</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">BATCHSTRATEGY</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="w"> </span>\<span class="n">mid</span><span class="p">},</span><span class="w"> </span><span class="n">S_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">all</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span><span class="n">B</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">s</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="o">|</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="o">|</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">Average</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">S_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">all</span><span class="w"> </span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
</code></pre></div>

<h3>3.1 How to Evaluate</h3>
<p>LLM-evaluators conduct sample-wise evaluation through a process of either analyzing followed by scoring (Guo et al., 2023b) or scoring followed by analyzing (Liu et al., 2023), where the former typically performs better (Chiang and Lee, 2023b) possibly due to the effect of chain-ofthought (Wei et al., 2022). Following this insight, we explored three possible evaluation procedures for BATCHEval (See Appendix I for prompts):</p>
<p>One stage As the most intuitive extension of sample-wise evaluation, LLM analyzes and scores each sample of the batch in order. This procedure enables adequate comparison between samples, but insufficient comparison between analyses (the analyses of subsequent samples cannot be referenced by the earlier samples for scoring).</p>
<p>Two stage To enhance the comparison among analyses, the LLM first analyzes all the samples. Based on the full comparisons among samples and analyses, the LLM further scores for each sample.</p>
<p>Three stage From human experience, it can be easier to first rank and then score the samples, as compared to directly scoring them. Therefore, we consider a procedure that sequentially performs analyzing, ranking, and scoring for all samples.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overall illustration of BATCHEval.</p>
<h3>3.2 What to Input</h3>
<p>The composition of the batch largely determines the efficacy of in-batch comparison as evaluation reference. One basic method is to fix a random batch division, attain multiple scores from the LLM and then average them for each sample. However, based on preliminary experiments and Theorem 2, we have found that this method does not yield good results due to the lack of diversity. Therefore, we consider redrawing the batch divisions after each round of evaluation to provide the LLM with varying references when assessing a certain sample. ${ }^{3}$</p>
<p>Random Batch One naive way is to reallocate batches randomly after each round of evaluation.</p>
<p>Homogeneous Batch Based on the idea of coarse-to-fine evaluation, we consider forming homogeneous batches in which samples have similar scores from the previous round of evaluation, in the hope that these samples can be further compared by LLM and ultimately attain discriminative scores.</p>
<p>Heterogeneous Batch A contrary idea is to select samples with diversified scores based on the previous round of evaluation results to form a new batch. In this way, LLM develops an unbiased perception of samples with different qualities through batch optimization, thus scoring more accurately.</p>
<h3>3.3 What to Output</h3>
<p>Sample-wise evaluation methods typically apply integers as the format for LLM scoring (Liu et al., 2023; Chiang and Lee, 2023b), and Lin and Chen (2023) proved that using more refined scoring format can not bring additional gains. Will this trend</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>be similar in BATCHEval? Let us consider a concrete example: there are two samples with close but different quality, with human ratings of 2.2 and 1.8 , respectively. Due to having only the criterion as reference, sample-wise evaluators may consider them to be close to the 2-point standard and consequently assign a score of 2 regardless of whether decimal is allowed. However, if they appear in the same batch, on the basis of judging that they are all close to 2 points, LLM can further compare their quality directly. Thus, it is possible for LLM to give them differentiated decimal scores if it is allowed, thereby achieving more consistent judgments with humans. Based on the analysis above, we consider trying out two different scoring formats: integer and decimal.</p>
<p>Our default settings of BATCHEval include two stage evaluation procedure, heterogeneous batch composition strategy and decimal scoring format, as shown in Figure 2.</p>
<h2>4 Experiments</h2>
<p>Centered around BATCHEval, we will empirically explore the optimal variants in $\S 4.2$, demonstrate its performance on different LLMs and tasks in $\S 4.3$, validate the robustness in $\S 4.4$, and delve into its working mechanism in $\S 4.5$. We also investigate the choice of hyperparameters in Appendix §B.</p>
<h3>4.1 Experimental settings</h3>
<p>Benchmarks A brief introduction of benchmarks involved are listed as follows:</p>
<ul>
<li>Topical-Chat (Mehri and Eskénazi, 2020) is a benchmark for evaluating dialogue response generation. To save costs, we exclude knowledge as input to LLM and therefore choose</li>
</ul>
<p>| Type | Method | Scheme | Engaging | Understand | Naturalness | Coherence | Overall | Average |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| | | | $r_{p}$ | $r_{s}$ | $r_{p}$ | $r_{s}$ | $r_{p}$ | $r_{s}$ | $r_{p}$ | $r_{s}$ | $r_{p}$ | $r_{s}$ | $r_{p}$ | $r_{s}$ | $\$ / i t e m$ |
| Human | Inter-annotator |  | .575 | .581 | .510 | .510 | .486 | .487 | .558 | .560 | .710 | .718 | .568 | .571 | - |
| Rule | BLEU-4" | - | .232 | .316 | .201 | .218 | .180 | .175 | .131 | .235 | .216 | .296 | .192 | .248 | - |
|  | METEOR ${ }^{+}$ | - | .367 | .439 | .245 | .225 | .212 | .191 | .250 | .302 | .337 | .391 | .282 | .310 | - |
| Embedding | V-Extrema ${ }^{+}$ | - | .210 | .205 | .156 | .132 | .101 | .076 | .184 | .184 | .203 | .209 | .171 | .161 | - |
|  | BERTScore ${ }^{+}$ | - | .317 | .335 | .256 | .226 | .226 | .209 | .214 | .233 | .298 | .325 | .262 | .266 | - |
| Learning | USR $^{+}$ | - | .456 | .465 | .293 | .315 | .276 | .304 | .416 | .377 | .422 | .419 | .373 | .376 | - |
|  | BCR | - | .460 | .463 | .297 | .325 | .260 | .298 | .425 | .391 | .437 | .421 | .376 | .380 | - |
| LLM | G-Eval | - | .710 | .719 | .568 | .593 | .595 | .605 | .576 | .584 | .717 | .705 | .633 | .641 | .0614 |
|  | CloserLook | - | .651 | .688 | .649 | .699 | .656 | .665 | .675 | .687 | .778 | .772 | .682 | .702 | .0686 |
|  | CloserLook | + ICL | .714 | .743 | .603 | .685 | .679 | .693 | .720 | .733 | .786 | .783 | .700 | .727 | .0856 |
|  | $\begin{gathered} \text { BATCHEVAL } \ \text { (Ours) } \end{gathered}$ | one stage | .780 | .783 | .642 | .680 | .706 | .710 | .727 | .729 | .785 | .793 | .728 | .739 | .0525 |
|  |  | three stage | .782 | .778 | .667 | .725 | .712 | .704 | .712 | .714 | .797 | .798 | .734 | .744 | .0541 |
|  |  | random | .746 | .743 | .685 | .724 | .711 | .700 | .716 | .720 | .798 | .799 | .731 | .737 | .0528 |
|  |  | homogeneous | .654 | .663 | .639 | .607 | .671 | .674 | .669 | .631 | .722 | .703 | .671 | .656 | .0537 |
|  |  | integer | .771 | .778 | .686 | .732 | .726 | .727 | .722 | .727 | .790 | .783 | .739 | .749 | .0526 |
|  |  | default | .792 | .790 | .694 | .727 | .730 | .735 | .740 | .744 | .805 | .800 | .752 | .759 | .0529 |</p>
<p>Table 1: Turn-level Pearson $\left(r_{p}\right) /$ Spearman $\left(r_{s}\right)$ correlations and average API cost per sample $(\$ / i t e m)$ of different metrics on Topical-Chat benchmark. The results of methods with ${ }^{+}$come from USR. We reproduced other methods with a unified API (the results were generally better than those reported in the original paper). All results of our replication are statistically significant (p-value $&lt;0.05$ ).
criteria where knowledge is not necessary: Naturalness, Coherence, Engaging, Naturalness and Overall.</p>
<ul>
<li>FED (Mehri and Eskenazi, 2020) includes human ratings on 11 criteria to evaluate the quality of dialogue. We choose the top 4 important criteria as claimed in the original paper for evaluation: Coherent, Understanding, Likeable and Overall.</li>
<li>HANNA (Chhun et al., 2022) serves as a benchmark for meta-evaluating evaluation methods on story generation, with criteria including: Coherence, Relevance, Empathy, Surprise, Engagement and Complexity.</li>
<li>QAGS (Chhun et al., 2022) is a benchmark for evaluating the Factual Consistency of summaries on CNN (Hermann et al., 2015) and XSUM (Narayan et al., 2018).</li>
</ul>
<p>Baselines We introduce four types of baseline methods in the experiments. Among them, both rule-based and embedding-based methods need reference text, which is unavailable in FED and QAGS. Learning-based methods are typically taskspecific. Below we briefly list their categories and snapshots of LLM-based methods. Refer to Appendix G for detailed introductions.</p>
<ul>
<li>Rule-based: BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009).</li>
<li>Embedding-based: Vector Extrema (Forgues et al., 2014), BERTScore (Zhang et al., 2020)</li>
<li>Learning-based: USR (Mehri and Eskénazi, 2020), BCR (Yuan et al., 2023), FED (Mehri and Eskenazi, 2020), DynaEval (Zhang et al., 2021), QAGS (Wang et al., 2020).</li>
<li>LLM-based ${ }^{4}$ : G-Eval (Liu et al., 2023) recommended using LLM to evaluate according to the procedures generated by itself. Chiang and Lee (2023b) tried various evaluation schemes and proved through experiments that analyze rate led to the best performance, which we denote as CloserLook.</li>
</ul>
<p>Details We explore variants of BATCHEval on Topical-Chat for its wide recognition. If not specified, FED serves as our default dataset for exploratory experiments as it only has 125 samples, thus can save API expenses. The other two benchmarks are used to confirm the generalization across tasks of BATCHEval. We primarily conduct experiments with GPT-4 (0613) and validate the generalization across models of BATCHEVAL with GPT-3.5-turbo (0613) and Llama-2-70b-chat-hf. We set</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Score distribution and corresponding entropy ( $-\sum_{s} p(s) \log _{2} p(s)$ ) of different methods.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparisons between BATCHEval and CloserLook from the perspective of Theorem 2.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Average batch bias of different strategies.
iteration rounds as 5 , batchsize as 10 , decoding temperature as 0.2 for all the experiment. For other LLM-based evaluators, we reproduced them according to their default settings ( 20 generations per sample) with the same API for a fair comparison. We choose Pearson and Spearman correlations to measure consistency with humans and also report API expenses for adequate comparison. We follow Chiang and Lee, 2023b) to design prompts (See prompts in Appendix I).</p>
<h1>4.2 Variants Exploration</h1>
<p>As shown in Table 1, based on the default settings shown in Figure 2, we validate the effects of different variants (replacing the default setting with specific scheme) of BATCHEval.</p>
<p>Evaluation Procedure Compared to one stage procedure, the two stage procedure (default) achieves higher correlations by enhancing the comparison among analyses during scoring. Surprisingly, however, the three stage procedure does not perform well as expected. We speculate this may be due to the LLM's over-reliance on ranking results while neglecting the analyses and samples during scoring, and valide this in Appendix D.</p>
<p>Batch Composition Strategy As shown in Table 1, the performance of batch composition strategies ranks as follows: heterogeneous (default) $&gt;$ random $&gt;$ homogeneous. To investigate the reasons, we introduce batch bias as follows:</p>
<p>$$
\operatorname{Bias}(\mathcal{B})=\operatorname{abs}\left(\sum_{i \in \mathcal{B}} s_{i}^{\mathcal{B}}-\sum_{i \in \mathcal{B}} \hat{s}_{i}\right) /|\mathcal{B}|
$$</p>
<p>where $\mathcal{B}$ denotes the set of sample indexes of certain batch, $s_{i}^{\mathcal{B}}$ denotes score of sample $x_{i}$ generated with batch $\mathcal{B}, \hat{s}<em i="i">{i}$ denotes average score of sample $x</em>}$ across all the iterations. Ideally, we aspire for the batch bias to approach zero. This implies that LLM should not have the overall scores in a batch skewed either high or low compared to the ensemble scores. We evaluate the average $\operatorname{Bias}(\mathcal{B})$ of different strategies and find that $\operatorname{Bias}(\mathcal{B})$ correlates negatively with correlations $\boldsymbol{r<em _boldsymbol_p="\boldsymbol{p">{\boldsymbol{s}}$ and $\boldsymbol{r}</em>$ (Figure 5). This indicates that the more varied the quality of samples in a batch, the better they can simulate a real distribution as an unbiased reference to bring smaller batch bias for better correlations.}</p>
<p>Scoring Format We observe from Table 1 that decimal scoring format brings around 1 point correlations improvement upon integer. As shown in Figure 3, the decimal scheme brings a more uniform scoring distribution. This implies that LLM indeed assigns more discriminative scores to different samples through in-batch comparison if decimal score is allowed, which verifies our hypothesis in $\S 3.3$ and accounts for the progress.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Method</th>
<th>Model</th>
<th>Likeable</th>
<th>Understand</th>
<th>Coherent</th>
<th>Overall</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>Human</td>
<td>Inter-annotator</td>
<td>-</td>
<td>- 838</td>
<td>- 809</td>
<td>- 809</td>
<td>- 830</td>
<td>- 822</td>
</tr>
<tr>
<td>Learning</td>
<td>USR</td>
<td>-</td>
<td>.245 .226</td>
<td>.182 .178</td>
<td>.170 .185</td>
<td>.284 .302</td>
<td>.220 .223</td>
</tr>
<tr>
<td></td>
<td>FED</td>
<td>-</td>
<td>.248 .262</td>
<td>.295 .306</td>
<td>.262 .253</td>
<td>.460 .449</td>
<td>.316 .318</td>
</tr>
<tr>
<td></td>
<td>DynalEval</td>
<td>-</td>
<td>.389 .393</td>
<td>.379 .368</td>
<td>.399 .409</td>
<td>.484 .490</td>
<td>.413 .415</td>
</tr>
<tr>
<td>LLM</td>
<td>CloserLook</td>
<td>Llama-2-70b</td>
<td>.525 .550</td>
<td>.574 .611</td>
<td>.640 .563</td>
<td>.634 .639</td>
<td>.593 .591</td>
</tr>
<tr>
<td></td>
<td>BatchEval</td>
<td>Llama-2-70b</td>
<td>.537 .563</td>
<td>.619 .597</td>
<td>.627 .648</td>
<td>.722 .732</td>
<td>.626 .635</td>
</tr>
<tr>
<td></td>
<td>CloserLook</td>
<td>GPT-3.5-turbo</td>
<td>.681 .666</td>
<td>.691 .605</td>
<td>.726 .724</td>
<td>.687 .709</td>
<td>.696 .676</td>
</tr>
<tr>
<td></td>
<td>BatchEval</td>
<td>GPT-3.5-turbo</td>
<td>.682 .674</td>
<td>.704 .708</td>
<td>.733 .730</td>
<td>.705 .699</td>
<td>.706 .703</td>
</tr>
<tr>
<td></td>
<td>G-Eval</td>
<td>GPT-4</td>
<td>.638 .692</td>
<td>.670 .625</td>
<td>.707 .721</td>
<td>.689 .652</td>
<td>.676 .673</td>
</tr>
<tr>
<td></td>
<td>CloserLook $w$ human prompt</td>
<td>GPT-4</td>
<td>.658 .680</td>
<td>.701 .614</td>
<td>.739 .751</td>
<td>.715 .684</td>
<td>.703 .682</td>
</tr>
<tr>
<td></td>
<td>CloserLook $w$ GPT-4 prompt</td>
<td>GPT-4</td>
<td>.632 .660</td>
<td>.678 .639</td>
<td>.725 .749</td>
<td>.723 .678</td>
<td>.690 .682</td>
</tr>
<tr>
<td></td>
<td>BatchEval $w$ human prompt</td>
<td>GPT-4</td>
<td>.731 .741</td>
<td>.778 .696</td>
<td>.753 .753</td>
<td>.738 .729</td>
<td>.750 .730</td>
</tr>
<tr>
<td></td>
<td>BatchEval $w$ GPT-4 prompt</td>
<td>GPT-4</td>
<td>.736 .741</td>
<td>.780 .700</td>
<td>.784 .749</td>
<td>.748 .727</td>
<td>.762 .729</td>
</tr>
</tbody>
</table>
<p>Table 2: Dialog-level Pearson $\left(\boldsymbol{r}<em _boldsymbol_s="\boldsymbol{s">{\boldsymbol{p}}\right)$ / Spearman $\left(\boldsymbol{r}</em>\right)$ correlations and average API cost per sample (\$/item) on FED-dialog benchmark. We implemented and tested all the methods with p-value $&lt;0.05$.}</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Coherence</th>
<th></th>
<th>Relevance</th>
<th></th>
<th>Empathy</th>
<th></th>
<th>Surprise</th>
<th></th>
<th>Engagement</th>
<th></th>
<th>Complexity</th>
<th></th>
<th>Average</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$r_{p}$</td>
<td>$r_{s}$</td>
<td>$r_{p}$</td>
<td>$r_{s}$</td>
<td>$r_{p}$</td>
<td>$r_{s}$</td>
<td>$r_{p}$</td>
<td>$r_{s}$</td>
<td>$r_{p}$</td>
<td>$r_{s}$</td>
<td>$r_{p}$</td>
<td>$r_{s}$</td>
<td>$r_{p}$</td>
<td>$r_{s}$</td>
</tr>
<tr>
<td>BLEU-4</td>
<td>.220</td>
<td>.218</td>
<td>.135</td>
<td>.175</td>
<td>.242</td>
<td>.216</td>
<td>.178</td>
<td>.224</td>
<td>.242</td>
<td>.270</td>
<td>.362</td>
<td>.273</td>
<td>.230</td>
<td>.229</td>
</tr>
<tr>
<td>METEOR</td>
<td>.335</td>
<td>.273</td>
<td>.202</td>
<td>.190</td>
<td>.304</td>
<td>.282</td>
<td>.285</td>
<td>.283</td>
<td>.316</td>
<td>.338</td>
<td>.520</td>
<td>.482</td>
<td>.307</td>
<td>.307</td>
</tr>
<tr>
<td>BERTScore</td>
<td>.358</td>
<td>.293</td>
<td>.201</td>
<td>.188</td>
<td>.308</td>
<td>.303</td>
<td>.302</td>
<td>.290</td>
<td>.308</td>
<td>.331</td>
<td>.501</td>
<td>.472</td>
<td>.330</td>
<td>.313</td>
</tr>
<tr>
<td>G-Eval</td>
<td>.572</td>
<td>.578</td>
<td>.582</td>
<td>.584</td>
<td>.453</td>
<td>.461</td>
<td>.311</td>
<td>.347</td>
<td>.562</td>
<td>.591</td>
<td>.602</td>
<td>.557</td>
<td>.514</td>
<td>.520</td>
</tr>
<tr>
<td>CloserLook</td>
<td>.595</td>
<td>.591</td>
<td>.579</td>
<td>.597</td>
<td>.498</td>
<td>.478</td>
<td>.280</td>
<td>.339</td>
<td>.605</td>
<td>.607</td>
<td>.619</td>
<td>.568</td>
<td>.529</td>
<td>.530</td>
</tr>
<tr>
<td>BatchEval</td>
<td>.678</td>
<td>.625</td>
<td>.702</td>
<td>.679</td>
<td>.546</td>
<td>.543</td>
<td>.368</td>
<td>.381</td>
<td>.617</td>
<td>.605</td>
<td>.625</td>
<td>.575</td>
<td>.589</td>
<td>.568</td>
</tr>
</tbody>
</table>
<p>Table 3: Story-level Pearson $\left(\boldsymbol{r}<em _boldsymbol_s="\boldsymbol{s">{\boldsymbol{p}}\right)$ / Spearman $\left(\boldsymbol{r}</em>\right)$ correlations and average API cost per sample (\$/item) of on HANNA benchmark. We implemented and tested all the methods with p-value $&lt;0.05$.}</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>QAGS-C</th>
<th>QAGS-X</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$r_{p}$</td>
<td>$r_{s}$</td>
<td>$r_{p}$</td>
</tr>
<tr>
<td>BERTScore</td>
<td>.576 .505</td>
<td>.024 .008</td>
<td>.300 .256</td>
</tr>
<tr>
<td>QAGS</td>
<td>.545 -</td>
<td>.175 -</td>
<td>.375 -</td>
</tr>
<tr>
<td>G-Eval</td>
<td>.631 .685</td>
<td>.558 .537</td>
<td>.599 .611</td>
</tr>
<tr>
<td>CloserLook</td>
<td>.581 .602</td>
<td>.549 .573</td>
<td>.498 .478</td>
</tr>
<tr>
<td>BatchEval</td>
<td>.785 .643</td>
<td>.618 .634</td>
<td>.682 .639</td>
</tr>
</tbody>
</table>
<p>Table 4: Results on QAGS benchmark (QAGS with -C and -X denote subset CNN and XSUM respectively). The results of methods with * come from G-EVAL. Our replication results of G-Eval are lower than those reported in the original paper, so we present the original results here to avoid potential replication errors.</p>
<h3>4.3 Overall Performance of BatchEval</h3>
<p>As shown in Table 1, 2, 3, 4, BatchEval achieves an average of 6.5 points ( $10.5 \%$ ) Pearson and 4.5 points ( $7.1 \%$ ) Spearman correlations improvements with humans across four benchmarks compared to the best performing methods. From the perspective of Theorem 2, as shown in Figure 4, we found that the reason BatchEval outperforms CloserLook under score ensemble $(E r r(\bar{s}, y))$ is twofold. First, BatchEval attains more accurate single predictions $(E r r(\mathcal{S}, y))$ through thorough in-batch comparison. Second, the scoring diversity $(V a r(\mathcal{S}))$ of BatchEval is significantly improved. This validates that iterative heterogeneous batch composition strategy can provide LLM with unbiased varying evaluation references, thus stably enhancing diversity and ensemble performance.</p>
<p>In terms of cost, BatchEval only consumes $64 \%$ API expenses of the best performing baselines. This is because we only use the average scores from 5 iterations and allow in-batch samples to share a single prompt, while the LLM-based baselines average scores from 20 generations. ${ }^{5}$ Considering that baselines reach ensemble saturation at about</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>definition</th>
<th>criterion</th>
<th>sample1</th>
<th>sample2</th>
<th>sample3</th>
<th>presubset</th>
<th>analysis1</th>
<th>analysis2</th>
<th>analysis3</th>
<th>score1</th>
<th>score2</th>
<th>score3</th>
</tr>
</thead>
<tbody>
<tr>
<td>definition</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>criterion</td>
<td>0.01</td>
<td>0.01</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>sample1</td>
<td>0.00</td>
<td>1.00</td>
<td>0.01</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>sample2</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>sample3</td>
<td>0.04</td>
<td>1.01</td>
<td>0.04</td>
<td>1.00</td>
<td>0.01</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>presubset</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>1.00</td>
<td>1.11</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>analysis1</td>
<td>0.02</td>
<td>0.93</td>
<td>0.06</td>
<td>0.90</td>
<td>1.03</td>
<td>1.20</td>
<td>0.70</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>analysis2</td>
<td>0.04</td>
<td>0.97</td>
<td>0.05</td>
<td>0.88</td>
<td>0.98</td>
<td>1.13</td>
<td>1.08</td>
<td>0.91</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>analysis3</td>
<td>1.00</td>
<td>0.98</td>
<td>0.93</td>
<td>0.96</td>
<td>0.99</td>
<td>1.08</td>
<td>1.10</td>
<td>0.92</td>
<td>0.70</td>
<td>0.90</td>
<td>0.90</td>
</tr>
<tr>
<td>score1</td>
<td>1.33</td>
<td>1.12</td>
<td>0.89</td>
<td>0.91</td>
<td>0.96</td>
<td>0.99</td>
<td>1.06</td>
<td>1.12</td>
<td>1.28</td>
<td>1.00</td>
<td>1.00</td>
</tr>
<tr>
<td>score2</td>
<td>1.38</td>
<td>1.12</td>
<td>0.89</td>
<td>0.90</td>
<td>0.96</td>
<td>0.99</td>
<td>1.06</td>
<td>1.16</td>
<td>1.18</td>
<td>0.99</td>
<td>0.99</td>
</tr>
<tr>
<td>score3</td>
<td>1.38</td>
<td>1.12</td>
<td>0.89</td>
<td>0.89</td>
<td>0.96</td>
<td>0.99</td>
<td>1.06</td>
<td>1.09</td>
<td>1.16</td>
<td>0.99</td>
<td>0.99</td>
</tr>
<tr>
<td>definition</td>
<td>criterion</td>
<td>sample1</td>
<td>sample2</td>
<td>sample3</td>
<td>presubset</td>
<td>analysis1</td>
<td>analysis2</td>
<td>analysis3</td>
<td>score1</td>
<td>score2</td>
<td>score3</td>
</tr>
<tr>
<td>definition</td>
<td>1.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>criterion</td>
<td>0.02</td>
<td>0.01</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>sample1</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>sample2</td>
<td>1.00</td>
<td>0.93</td>
<td>0.98</td>
<td>0.14</td>
<td>0.93</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>sample3</td>
<td>1.33</td>
<td>0.82</td>
<td>0.88</td>
<td>0.64</td>
<td>0.15</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>presubset</td>
<td>1.05</td>
<td>1.18</td>
<td>0.17</td>
<td>1.18</td>
<td>0.42</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>analysis1</td>
<td>0.06</td>
<td>0.00</td>
<td>0.02</td>
<td>0.08</td>
<td>0.07</td>
<td>0.01</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>analysis2</td>
<td>1.13</td>
<td>1.15</td>
<td>1.08</td>
<td>1.15</td>
<td>1.13</td>
<td>0.97</td>
<td>0.07</td>
<td>0.01</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>analysis3</td>
<td>1.13</td>
<td>1.20</td>
<td>1.11</td>
<td>1.12</td>
<td>1.21</td>
<td>1.05</td>
<td>0.10</td>
<td>0.11</td>
<td>0.01</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>score1</td>
<td>1.18</td>
<td>1.19</td>
<td>1.08</td>
<td>1.17</td>
<td>1.19</td>
<td>1.17</td>
<td>0.56</td>
<td>0.02</td>
<td>0.07</td>
<td>0.01</td>
<td>0.00</td>
</tr>
<tr>
<td>score2</td>
<td>1.23</td>
<td>1.18</td>
<td>1.09</td>
<td>1.19</td>
<td>1.20</td>
<td>1.09</td>
<td>0.55</td>
<td>0.03</td>
<td>0.14</td>
<td>0.00</td>
<td>0.01</td>
</tr>
<tr>
<td>score3</td>
<td>1.35</td>
<td>1.17</td>
<td>1.03</td>
<td>1.21</td>
<td>1.31</td>
<td>1.08</td>
<td>0.53</td>
<td>0.06</td>
<td>0.11</td>
<td>0.00</td>
<td>0.00</td>
</tr>
</tbody>
</table>
<p>Figure 6: Normalized attention matrices of the first (top figure) and last (bottom figure) transformer layer with Llama-2-70b-chat-hf. We set batchsize as 3 for clear demonstration. See Appendix H for the normalizing process.</p>
<p>20 generations, BATCHEval has broad potential for performance improvement by further increasing the number of iterations.</p>
<h3>4.4 Robustness of BATCHEval</h3>
<p>Robustness against Prompt Design We test BATCHEval and CloserLook respectively on prompts written by human and rewritten by GPT-4, with results as shown in Table 2. We calculate the average difference in correlations across metrics under two types of prompts. The standard deviation of $\boldsymbol{r}<em _boldsymbol_s="\boldsymbol{s">{\boldsymbol{p}}$ and $\boldsymbol{r}</em>$ are 0.009 and 0.007 for CloserLook, while only 0.006 and 0.002 for BATCHEval. This verifies that BATCHEval attains better robustness against prompt design by introducing in-batch samples as additional references.}</p>
<p>Robustness against Noise As shown in Figure 3, the score distribution of BATCHEval is more uniform and has lower entropy compared with CloserLook due to in-batch comparison with decimal scoring format, which can theoretically enhance robustness against noise according to Theorem 1. We further experimentally validate this in Appendix C.</p>
<h3>4.5 Further Discussion and Analysis</h3>
<p>Relationship with In-context-learning ICL (Brown et al., 2020) can also provide sample-side references by incorporating samples and corresponding answers into the prompt. The main differences between ICL and BATCHEval are: (1) BATCHEval can provide LLM with varying and comprehensive references through iterative heterogeneous batch, while the references provided by ICL are relatively fixed and may bring bias (sensitive to prompt design). (2) BATCHEval uses in-batch samples as references to each other, thus saving the costs of demonstrations in ICL prompts. Thanks to the aforementioned advancements, BATCHEval outperforms CloserLook with ICL by more than 5 points Pearson correlations while only incurs $61.8 \%$ expense (Table 1).</p>
<p>Working Mechanism of BATCHEval To further understand how BATCHEval benefits from in-batch comparison, we visualized the normalized attention matrices of the first and last layers of Llama-2-70b-chat-hf (Figure 6). The value at $(\mathrm{X}, \mathrm{Y})$ represents the average normalized attention of tokens corresponding to X towards tokens corresponding to Y. We observe that in the final scoring phase (red box), LLM first perceives samples with varied qualities based on the already generated scores and analyses at the shallower layers. Afterwards, LLM completes scoring based on criterion and comparison between samples at the deeper layers. This process demonstrates the in-batch comparison mechanism of BATCHEval, which we hope can inspire future research.</p>
<h2>5 Conclusions</h2>
<p>In this paper, we propose BATCHEval, a new text evaluation paradigm that evaluate samples batch-wise to alleviate the limitations of sample-</p>
<p>wise evaluation paradigm. We explore variants of BATCHEval on multiple dimensions and figure out the optimal settings. Following the human evaluation method, BATCHEval treats in-batch samples and criterion as complementary references and optimizes the batch composition through iteration to eliminate batch bias. Comprehensive experiments have confirmed that BATCHEval can achieve higher consistency with humans at a lower cost, while also demonstrating better robustness to prompt design and noise. We further analyze and reveal the working mechanism of BATCHEval, shedding lights on future work.</p>
<h2>Limitations</h2>
<p>From an objective perspective, we think there are two main limitations of this paper:</p>
<ol>
<li>BATCHEval requires LLMs to have a certain capability to handle longer contexts. From Appendix B, we found that as the batchsize increases, LLMs struggle to handle too many samples, leading to a performance decline. We also attempted to test BATCHEval's performance on Llama-2-13b-chat-hf and found that the batchsize must be set to 2 or 3 to see any benefits. Therefore, when setting the batchsize, we cannot exceed the limit of how many samples an LLM can process in a single context. Fortunately, we discovered that a batchsize of 10 is suitable for current mainstream LLMs. Additionally, as LLMs continue to advance, they can handle increasingly larger contexts. Thus, from this perspective, BATCHEval is a scalable method that improves alongside the capabilities of LLMs (increasing the batchsize within the capabilities of the LLM can enhance the evaluation effectiveness of the LLM).</li>
<li>We only explored a limited number of schemes of BATCHEval. We leave exploring possible schemes of BATCHEval for future research.</li>
</ol>
<h2>Ethics Statement</h2>
<p>All of the datasets used in this study were publicly available, and no annotators were employed for our data collection. We confirm that the datasets we used did not contain any harmful content and was consistent with their intended use (research). We
have cited the datasets and relevant works used in this study.</p>
<h2>References</h2>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Cyril Chhun, Pierre Colombo, Fabian M Suchanek, and Chloé Clavel. 2022. Of human criteria and automatic metrics: A benchmark of the evaluation of story generation. In 29th International Conference on Computational Linguistics (COLING 2022).</p>
<p>David Cheng-Han Chiang and Hung-yi Lee. 2023a. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 15607-15631. Association for Computational Linguistics.</p>
<p>David Cheng-Han Chiang and Hung-yi Lee. 2023b. A closer look into using large language models for automatic evaluation. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 8928-8942. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387.</p>
<p>Gabriel Forgues, Joelle Pineau, Jean-Marie Larchevêque, and Réal Tremblay. 2014. Bootstrapping dialog systems with word embeddings. In Nips,</p>
<p>modern machine learning and natural language processing workshop, volume 2, page 168.</p>
<p>Tanya Goyal and Greg Durrett. 2021. Annotating and modeling fine-grained factuality in summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 1449-1462. Association for Computational Linguistics.</p>
<p>Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, and Deyi Xiong. 2023a. Evaluating large language models: A comprehensive survey. CoRR, abs/2310.19736.</p>
<p>Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, and Deyi Xiong. 2023b. Evaluating large language models: A comprehensive survey. CoRR, abs/2310.19736.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28.</p>
<p>Alon Lavie and Michael J. Denkowski. 2009. The meteor metric for automatic evaluation of machine translation. Mach. Transl., 23(2-3):105-115.</p>
<p>Yen-Ting Lin and Yun-Nung Chen. 2023. Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. CoRR, abs/2305.13711.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpi-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 2511-2522. Association for Computational Linguistics.</p>
<p>Ryan Lowe, Michael D. Noseworthy, Iulian Vlad Serban, Nicolas Angelard-Gontier, Yoshua Bengio, and Joelle Pineau. 2017. Towards an automatic turing test: Learning to evaluate dialogue responses. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 1116-1126. Association for Computational Linguistics.</p>
<p>Shikib Mehri and Maxine Eskenazi. 2020. Unsupervised evaluation of interactive dialog with dialogpt. In 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page 225.</p>
<p>Shikib Mehri and Maxine Eskénazi. 2020. USR: an unsupervised and reference free evaluation metric for dialog generation. In Proceedings of the 58th Annual</p>
<p>Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 681-707. Association for Computational Linguistics.</p>
<p>Shashi Narayan, Shay Cohen, and Maria Lapata. 2018. Don't give me the details, just the summary! topicaware convolutional neural networks for extreme summarization. In 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311-318. ACL.</p>
<p>Ananya B. Sai, Akash Kumar Mohankumar, and Mitesh M. Khapra. 2023. A survey of evaluation metrics used for NLG systems. ACM Comput. Surv., 55(2):26:1-26:39.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS.</p>
<p>Peiwen Yuan, Xinglin Wang, Jiayi Shi, Bin Sun, Yiwei Li, and Kan Li. 2023. Better correlation and robustness: A distribution-balanced self-supervised learning framework for automatic dialogue evaluation. In Thirty-seventh Conference on Neural Information Processing Systems.</p>
<p>Chen Zhang, Yiming Chen, Luis Fernando D'Haro, Yan Zhang, Thomas Friedrichs, Grandee Lee, and Haizhou Li. 2021. Dynaeval: Unifying turn and dialogue level evaluation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 5676-5689. Association for Computational Linguistics.</p>
<p>Pengfei Zhang, Xiaohui Hu, Kaidong Yu, Jian Wang, Song Han, Cao Liu, and Chunyang Yuan. 2022. MME-CRS: multi-metric evaluation based on correlation re-scaling for evaluating open-domain dialogue. CoRR, abs/2206.09403.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International</p>
<p>Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv e-prints, pages arXiv-2306.</p>
<p>Zhi-Hua Zhou. 2012. Ensemble methods: foundations and algorithms. CRC press.</p>
<h2>A Proof of Theorem 1</h2>
<p>For any $f(x)$, the probability density function of score distribution, the Spearman correlation $\mathbb{E}\left(r_{s}\right)$ between the original scores and scores adding a small disturbance has an upper bound:</p>
<p>$$
\mathbb{E}\left(r_{s}\right) \leq 1-\frac{6 \mathbb{E}(\lambda)^{2}}{n^{2}-1}
$$</p>
<p>and the equality condition is $f(x) \equiv 1, \forall x \in[0,1]$.
Proof 1 The ranking difference $d(x)$ before and after disturbance is :</p>
<p>$$
d(x)=\int_{x}^{x+\lambda} f(x) d x
$$</p>
<p>According to the definition of Spearman correlations, $E\left(r_{s}\right)$ can be written as:</p>
<p>$$
\mathbb{E}\left(r_{s}\right)=\mathbb{E}\left(1-\frac{6 \sum_{i=1}^{n} d\left(x_{i}\right)^{2}}{n\left(n^{2}-1\right)}\right)
$$</p>
<p>we derive the lower bound of $\mathbb{E}\left(d(x)^{2}\right)$ as follows:</p>
<p>$$
\begin{aligned}
\mathbb{E}\left(d(x)^{2}\right)= &amp; \int_{0}^{1}\left(\int_{x}^{x+\mathbb{E}(\lambda)} f(u) d u\right)^{2} f(x) d x \
= &amp; \int_{0}^{1}\left(\int_{x}^{x+\mathbb{E}(\lambda)} f(u) d u \sqrt{f(x)}\right)^{2} d x \
= &amp; \int_{0}^{1}\left(\int_{x}^{x+\mathbb{E}(\lambda)} f(u) d u \sqrt{f(x)}\right)^{2} d x \
&amp; \cdot \int_{0}^{1} f(x) d x \
\geq &amp; \left(\int_{0}^{1} \int_{x}^{x+\mathbb{E}(\lambda)} f(u) d u f(x) d x\right)^{2} \
&amp; \text { (Cauchy's Inequality) } \
= &amp; \left(\int_{0}^{1} \mathbb{E}(\lambda) \cdot f(x) \cdot f(x) d x\right)^{2}(\mathbb{E}(\lambda) \rightarrow 0) \
= &amp; \mathbb{E}(\lambda)^{2}\left(\int_{0}^{1} f(x) \cdot f(x) d x\right)^{2} \
= &amp; \mathbb{E}(\lambda)^{2}\left(\int_{0}^{1} f(x)^{2} d x \cdot \int_{0}^{1} 1^{2} d x\right)^{2} \
\geq &amp; \mathbb{E}(\lambda)^{2}\left(\left(\int_{0}^{1} f(x) d x\right)^{2}\right)^{2} \
&amp; \text { (Cauchy's Inequality) } \
= &amp; \mathbb{E}(\lambda)^{2}
\end{aligned}
$$</p>
<p>The equality condition is $f(x) \equiv 1$ for $x \in$ $[0,1]$. Taking the lower bound of $\mathbb{E}\left(d(x)^{2}\right)$ into Eq. (6), we conclude the proof. Note that higher $\mathbb{E}\left(r_{s}\right)$ denotes better robustness against noise. Hence, we can derive that the robustness against noise correlates positively with the uniformity of score distribution.</p>
<h2>B Hyperparameter Analysis</h2>
<p>In the experiments of the main text, we set the batch size to 10 and the temperature to 0.2 . In this section, we explore the impact of different hyperparameter choices on performance.</p>
<h2>B. 1 Effect of Batchsize</h2>
<p>On FED dataset, we test BATCHEval with batchsize among [1, 2, 5, 10]. As shown in Figure 7, we found that as the batch size increases, the performance generally undergoes a process of initial improvement followed by a decline. Similar observations were made on other datasets as well. We further discovered that the performance turning point of the ensemble results from five iterations is slightly delayed compared to a single prediction. Considering that increasing the batchsize will make the combination of in-batch samples more diverse, thereby increasing scoring diversity, we have the following conjecture about Figure 7: When the batchsize starts to increase from 1, due to the effect of in-batch comparison and the increase in diversity, the performance of both 1-round score and ensemble score increase a lot. However, as the batchsize continues to increase, LLM finds it difficult to handle too many samples simultaneously, resulting in a decrease in 1-round score performance. When the rate of decrease in 1-round score performance gets greater than the rate of increase in diversity, ensemble score performance also begins to decrease according to Theorem 2. Therefore, the batchsize should not be too large or too small. We found that setting the batchsize to 10 can achieve superior performance on different tasks. We also believe that for LLMs with weaker ability to handle longer context, the batchsize should be set to be smaller. Fortunately, we have noticed that current LLMs are continually improving in processing long contextual texts, which illuminates further development prospects for BATCHEval in the future.</p>
<h2>B. 2 Effect of Temperature</h2>
<p>We also test BATCHEval with temperature among $[0,0.2,0.5,1]$. We found that as the temperature rises in Figure 8, the performance of BATCHEval does not exhibit a uniform trend of change. Overall, the performance of 5 iterations is relatively stable along the temperature dimension, suggesting that BATCHEval is quite robust to temperature variations.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Dialog-level Pearson correlations on FED-dialog dataset of BATCHEVAL with different batchsize.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Dialog-level Pearson correlations on FED-dialog dataset of BATCHEVAL with different temperature.</p>
<h2><strong>C Robustness against Noise</strong></h2>
<p>To test the robustness against noise of BATCHEVAL, we use an external tool<sup>6</sup> to add noise to the input and calculate the changes in performance before and after the noise is added. For the sake of noise balance, we randomly replace 5% of tokens with synonyms and randomly delete 5% of tokens. As shown in Table 5, CloserLook experiences a decrease of 0.109 in Pearson correlation and 0.081 in Spearman correlation, respectively. In contrast, BATCHEVAL only shows a decrease of 0.003 and 0.009, respectively. This indicates that BATCHEVAL has much better robustness to noise.</p>
<h2><strong>D Inferior Performance of Three Stage Procedure</strong></h2>
<p>As shown in Table 1, we observe a performance drop of BATCHEVAL with three stage procedure, though it may be closer to human evaluation procedure. We speculate this may be due to the LLM's over-reliance on ranking results while neglecting the analyses and samples during scoring. To validate this, we delete the ranking and scoring contents of LLM's three stage procedure response and ask LLM to score based on the remaining contents (samples and analyses). If the new scoring results perform similarly to BATCHEVAL with two stage procedure, the inferior performance of BATCHEVAL with three stage procedure can be attributed to its excessive focus on ranking results. Otherwise, the reason lies in the decrease in the quality of analyses. As shown in Table 6, the performance of three stage w/o rank results is on par with that of two stage procedure. This validates our conjecture that the over-reliance on ranking results causes the performance drop of BATCHEVAL with three stage procedure.</p>
<h2><strong>E Relationship with Pair-wise Evaluation</strong></h2>
<p>The current mainstream text evaluation approach adopts sample-wise assessment. Alternatively, an LLM evaluator is presented with a question and two answers, and is tasked with determining which one is better or declaring a tie (Zheng et al., 2023; Dubois et al., 2023). However, as the number of models to be evaluated grows, the scalability of pairwise comparison becomes a challenge, due to the quadratic increase in the potential number of pairs. Therefore, this pair-wise paradigm has not been as extensively studied as sample-wise evaluation. Zheng et al. (2023) validates that this method performs slightly better than a sample-wise evaluator, potentially due to its ability to discern subtle differences between specific pairs.</p>
<p>Similarly, we have enhanced the evaluation capabilities of the LLM evaluator through in-batch sample comparison. The main difference lies in the</p>
<p><sup>6</sup>nlpaug(https://github.com/makcedward/nlpaug)</p>
<p>| Method | Likeable | | Understand | | Coherent | | Overall | | Average | | |
| | $\boldsymbol{r}<em _boldsymbol_s="\boldsymbol{s">{\boldsymbol{p}}$ | $\boldsymbol{r}</em>}}$ | $\boldsymbol{r<em _boldsymbol_s="\boldsymbol{s">{\boldsymbol{p}}$ | $\boldsymbol{r}</em>}}$ | $\boldsymbol{r<em _boldsymbol_s="\boldsymbol{s">{\boldsymbol{p}}$ | $\boldsymbol{r}</em>}}$ | $\boldsymbol{r<em _boldsymbol_s="\boldsymbol{s">{\boldsymbol{p}}$ | $\boldsymbol{r}</em>}}$ | $\boldsymbol{r<em _boldsymbol_s="\boldsymbol{s">{\boldsymbol{p}}$ | $\boldsymbol{r}</em>$ | $\$ / i t e m$ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| CloserLook w/o noise | .658 | .680 | .701 | .614 | .739 | .755 | .715 | .684 | .703 | .683 | .0785 |
| CloserLook w noise | .509 | .580 | .626 | .606 | .608 | .605 | .632 | .616 | .594 (-109) | .602 (-081) | .0866 |
| BATCHEVAL w/o noise | .731 | .741 | .778 | .696 | .753 | .757 | .738 | .729 | .750 | .731 | .0314 |
| BATCHEVAL w noise | .729 | 718 | .775 | .700 | .764 | .754 | .720 | .724 | .747 (-083) | .724 (-007) | .0344 |}</p>
<p>Table 5: Story-level Pearson $\left(\boldsymbol{r}<em _boldsymbol_s="\boldsymbol{s">{\boldsymbol{p}}\right) /$ Spearman $\left(\boldsymbol{r}</em>\right)$ correlations and average API cost per sample ( $\$ / i t e m$ ) of on HANNA benchmark. We tested all the methods for a fair comparison with p-value $&lt;0.05$.}</p>
<p>| Method | Scheme | Engaging |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | </p>
<p>distribution.
FED (Mehri and Eskenazi, 2020) is a unified dialogue evaluation method that uses pretrained language models to calculate scores based on the difference in the probability of generating positive and negative evaluation words for a certain criterion.</p>
<p>DynaEval (Zhang et al., 2021) is also a unified dialogue evaluation method that leverages graph convolutional network to model the sentences among a dialogue for accurate evaluation.</p>
<p>QAGS (Wang et al., 2020) is a method that based on question-answering, which creates questions from a summary and then verifies whether their answers are present in the original source document.</p>
<h2>H Details of Normalizing Process</h2>
<p>We will introduce how to normalize the attention matrix to make it more visually appealing like in Figure 6. Due to the autoregressive generation mode of mainstream LLMs, the expected values of attention between token pairs at different positions vary. If we use $\operatorname{Att}(x, y)$ to represent the attention of the $x^{t h}$ token to the $y^{t h}$ token, then its expected value is $\frac{1}{2}$. Since tokens at different positions will be visualized into the same graph, we first multiply each $\operatorname{Att}(x, y)$ by $x$ to make its expected value 1. On this basis, we determine the token intervals corresponding to different strings through word matching, and calculate $\operatorname{Att}($ string 1 , string 2$)$ as follows:</p>
<p>$$
\operatorname{Att}(s 1, s 2)=\operatorname{Avg}({\operatorname{Att}(x, y) \mid x \in s 1, y \in s 2})
$$</p>
<p>according to which we plot our attention matrices.</p>
<h2>I Example Prompts</h2>
<h2>I. 1 Evaluate Coherence for Topical-Chat default prompt</h2>
<p>You will be given a batch of {{number}} samples. Each sample contains a conversation between Speaker $A$ and Speaker $B$ and one potential response for the next turn.</p>
<p>Your task is to assign a float score to the response on one metric.</p>
<p>You should carefully horizontally compare the given samples in order to assign a suitable float score to each sample.</p>
<p>Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.</p>
<h2>Evaluation Criteria:</h2>
<p>Coherence (floating point numbers within the interval [1,3]): Does the response serve as a valid continuation of the conversation history?</p>
<ul>
<li>A float score near 1 (no) means that the response drastically changes topic or ignores the conversation history.</li>
<li>A float score near 2 (somewhat) means the response refers to the conversation history in a limited capacity (e.g., in a generic way) and shifts the conversation topic.</li>
<li>A float score near 3 (yes) means the response is on topic and strongly acknowledges the conversation history.</li>
</ul>
<p>Conversations and corresponding potential response to be evaluated:
{{Data}}
Evaluation Form (Answer by starting with "I will do my best to provide individual analysis for each sample. Analysis:" to analyze the given samples regarding the evaluation criteria as concise as possible (Attention: Don't give your scores during this step). After analysing all the samples, please give all the float scores in order following the template "Float Scores: {Sample1:score of Sample1,.., Sample{{number}}:score of Sample{{number}}}".</p>
<ul>
<li>Coherence:</li>
</ul>
<h2>one stage prompt</h2>
<p>You will be given a batch of {{number}} samples. Each sample contains a conversation between Speaker $A$ and Speaker $B$ and one potential response for the next turn.</p>
<p>Your task is to assign a float score to the response on one metric.</p>
<p>You should carefully horizontally compare the given samples in order to assign a suitable float score to the given samples one by one.</p>
<p>Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.</p>
<h2>Evaluation Criteria:</h2>
<p>Coherence (floating point numbers within the interval [1,3]): Does the response serve as a valid continuation of the conversation history?</p>
<ul>
<li>A float score near 1 (no) means that the response drastically changes topic or ignores the conversation history.</li>
<li>A float score near 2 (somewhat) means the response refers to the conversation history in a limited capacity (e.g., in a generic way) and shifts the conversation topic.</li>
<li>A float score near 3 (yes) means the response is on topic and strongly acknowledges the conversation history.</li>
</ul>
<p>Conversations and corresponding potential response to be evaluated:
{{Data}}
Evaluation Form (Answer by starting with "I will do my best to provide individual analysis and give a suitable float score for each sample in order". When rating for each sample, please follow the template "Score of SampleX:{float score}").</p>
<ul>
<li>Coherence:</li>
</ul>
<h2>three stage prompt</h2>
<p>You will be given a batch of {{number}} samples. Each sample contains a conversation between Speaker A and Speaker B and one potential response for the next turn.</p>
<p>You will be introduced to a metric to be evaluated.</p>
<p>You should carefully horizontally compare the given samples in order to assign a suitable float score to each sample.</p>
<p>Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.</p>
<h2>Evaluation Criteria:</h2>
<p>Coherence (floating point numbers within the interval [1,3]): Does the response serve as a valid continuation of the conversation history?</p>
<ul>
<li>A float score near 1 (no) means that the response drastically changes topic or ignores the conversation history.</li>
<li>A float score near 2 (somewhat) means the response refers to the conversation history in a limited capacity (e.g., in a generic way) and shifts the conversation topic.</li>
<li>A float score near 3 (yes) means the response is on topic and strongly acknowledges the conversation history.</li>
</ul>
<p>Conversations and corresponding potential response to be evaluated:
{{Data}}
Answer by starting with "I will do my best to provide individual analysis for each sample. Analysis:" to analyze the given samples regarding the evaluation criteria as concise as possible (Attention: Don't give your scores during this step). After analysing all the samples, please horizontally compare the given samples, rank all the samples according to the analysis of the response and give the corresponding reasons. After ranking, according to the analysis and rank, please give all the float scores in order following the template "Float Scores: {Sample1:score of Sample1,.., Sample{{number}}:score of Sample{{number}}]".</p>
<ul>
<li>Coherence:</li>
</ul>
<h2>Integer prompt</h2>
<p>You will be given a batch of {{number}} samples. Each sample contains a conversation between Speaker A and Speaker B and one potential response for the next turn.</p>
<p>Your task is to rate the responses on one metric.</p>
<p>You should carefully horizontally compare the given samples in order to assign a score to each sample.</p>
<p>Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.</p>
<h2>Evaluation Crieteria:</h2>
<p>Coherence (1-3): Does the response serve as a valid continuation of the conversation history?</p>
<ul>
<li>A score of 1 (no) means that the response drastically changes topic or ignores the conversation history.</li>
<li>A score of 2 (somewhat) means the response refers to the conversation history in a limited capacity (e.g., in a generic way) and shifts the conversation topic.</li>
<li>A score of 3 (yes) means the response is on topic and strongly acknowledges the conversation history.</li>
</ul>
<p>Conversations and corresponding potential response to be evaluated:
{{Data}}
Evaluation Form (Answer by starting with "I will do my best to provide individual analysis for each sample. Analysis:" to analyze the given samples regarding the evaluation criteria as concise as possible (Attention: Don't give your scores during this step). After analysing all the samples, please give all the scores in order following the template "Scores: {Sample1:score of Sample1,....Sample{{number}}:score of Sample{{number}}}".</p>
<ul>
<li>Coherence:</li>
</ul>
<h2>I. 2 Evaluate Coherent for FED-Dialogue default prompt</h2>
<p>You will be given a batch of {{number}} samples. Each sample contains a conversation between User and a dialogue System.</p>
<p>Your task is to assign a float score to the sample on one metric.</p>
<p>You should carefully horizontally compare the given samples in order to assign a suitable float score to each sample.</p>
<p>Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.</p>
<h2>Evaluation Criteria:</h2>
<p>Coherent (floating point numbers within the interval [1,3]): Does System maintain coherence and a good flow of conversation throughout the dialogue?</p>
<ul>
<li>A float score near 1 (not coherent) means that System's responses are unrelated to the conversation topic and may disrupt or confuse the flow of the dialogue.</li>
<li>A float score near 2 (somewhat coherent) means that System's responses are partially related to the conversation topic but may not be clear or direct.</li>
<li>A float score near 3 (very coherent) means that System's responses are closely related to the conversation topic and contribute to maintaining a smooth dialogue.</li>
</ul>
<p>Conversations to be evaluated:
{{Data}}
Evaluation Form (Answer by starting with "I will do my best to provide individual analysis for each sample. Analysis:" to analyze the given samples regarding the evaluation criteria as concise as possible (Attention: Don't give your scores during this step). After analysing all the samples, please give all the float scores in order following the</p>
<p>template "Float Scores: {Sample1:score of Sample1,...,Sample[{number}}:score of Sample[{number}}]".</p>
<ul>
<li>Coherent:</li>
</ul>
<h2>L3 Evaluate Coherence for HANNA default prompt</h2>
<p>You will be given a batch of {{number}} samples. Each sample contains a prompt and a story generated following the prompt.
Your task is to assign a float score to the story according to the prompt on one metric.
You should carefully horizontally compare the given samples in order to assign a suitable float score to each sample.
Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.</p>
<h2>Evaluation Criteria:</h2>
<p>Coherence (floating point numbers within the interval ${1,5}$ ) Measures whether the story makes sense?</p>
<ul>
<li>A float score near 1 means the story does not make sense at all. For instance, the setting and/or characters keep changing, and/or there is no understandable plot.</li>
<li>A float score near 2 means most of the story does not make sense.</li>
<li>A float score near 3 means the story mostly makes sense but has some incoherences.</li>
<li>A float score near 4 means the story almost makes sense overall, except for one or two small incoherences.</li>
<li>A float score near 5 means the story makes sense from beginning to end.</li>
</ul>
<p>Prompts and corresponding stories to be evaluated:
{{Data}}
Evaluation Form (Answer by starting with "I will do my best to provide
individual analysis for each sample. Analysis:" to analyze the given samples regarding the evaluation criteria as concise as possible (Attention: Don't give your scores during this step). After analysing all the samples, please give all the float scores in order following the template "Float Scores: {Sample1:score of Sample1,...,Sample[{number}}:score of Sample ${{n u m b e r}}]$ ".</p>
<ul>
<li>Coherence:</li>
</ul>
<h2>L4 Evaluate Factual Consistency for QAGS</h2>
<p>default prompt</p>
<p>You will be given a batch of {{number}} samples. Each sample contains an article and a sentence.</p>
<p>Your task is to determine if the sentence is factually correct given the contents of the article.</p>
<p>You should carefully horizontally compare the given samples in order to assign a suitable float score to each sample.
Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.</p>
<h2>Evaluation Criteria:</h2>
<p>Consistency ({1,3}) - Is the sentence supported by the article? (consistent with the article)</p>
<ul>
<li>A float score near 1 (not) means that the sentence is totally not supported by the article.</li>
<li>A float score near 2 (somewhat) means that the sentence is partially supported by the article.</li>
<li>A float score near 3 (very) means that the sentence is completely supported by the article.</li>
</ul>
<p>Articles and corresponding sentences to be evaluated:
{{Data}}
Evaluation Form (Answer by starting with "I will do my best to provide</p>
<p>individual analysis for each sample. Analysis:" to analyze the given samples regarding the evaluation criteria as concise as possible (Attention: Don't give your scores during this step). After analysing all the samples, please give all the float scores in order following the template "Float Scores: {Sample1:score of Sample1,...,Sample{{number}}:score of Sample{{number}}]".</p>
<ul>
<li>Consistency:</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Due to changes in the prompt during iteration, the prompt expense needs to be billed 5 times for our method, whereas baselines require only once. Therefore the expenditure ratio $(64 \%)$ is higher than the proportion of generations (5:20).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
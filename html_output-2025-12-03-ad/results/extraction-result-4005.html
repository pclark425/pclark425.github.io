<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4005 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4005</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4005</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-93.html">extraction-schema-93</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-213d79556c2d50f6b4113a2d85309ebb5e3909f5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/213d79556c2d50f6b4113a2d85309ebb5e3909f5" target="_blank">Quality-Diversity through AI Feedback</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text, is introduced, a step towards AI systems that can independently search, diversify, evaluate, and improve.</p>
                <p><strong>Paper Abstract:</strong> In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-QD controls. Further, human evaluation of QDAIF-generated creative texts validates reasonable agreement between AI and human evaluation. Our results thus highlight the potential of AI feedback to guide open-ended search for creative and original solutions, providing a recipe that seemingly generalizes to many domains and modalities. In this way, QDAIF is a step towards AI systems that can independently search, diversify, evaluate, and improve, which are among the core skills underlying human society's capacity for innovation.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4005.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4005.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI feedback (LLM-as-a-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI feedback: instruction-tuned language models used as evaluators for quality and diversity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using instruction-tuned LMs (a finetuned AI feedback model in most experiments, and GPT-4 in some) to score generated texts for quality (binary or scalar) and to label or score diversity attributes, replacing hand-crafted heuristics or human-only evaluation in QD search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>QUALITY-DIVERSITY THROUGH AI FEEDBACK</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-a-judge evaluating quality and categorical/axis-based diversity in a MAP-Elites QD pipeline across creative-writing domains (Opinions, Stories; used to compute fitness/diversity and drive search).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Creative text generation: opinion pieces, short stories (genre/ending), and poetry (genre/tone).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Finetuned AI feedback model (instruction-tuned LMs; GPT-4 used as evaluator for Poetry in some experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Multiple reported statistics: Table 1 (Human-AI agreement on diversity label across methods) — Fixed-Few-Shot: 0.800; Shuffling-Few-Shot: 0.700; Random-Search: 0.733; LMX, Quality-Only: 0.633; QDAIF: 0.833. Appendix A.1 aggregated statistics — AI feedback agrees with a single human annotator on category label 73% of the time; when both annotators agree the Human-AI agreement increases to 82%; when both annotators give a non-neutral label agreement increases to 95%. (All values reported in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Reported differences vs humans include: (1) LMs can be well aligned on many labels but show misalignment at high-confidence ranges (top fitness), where LM scores stop correlating with human quality judgments; (2) LMs sometimes place samples differently around neutral/ambiguous cases (humans may differ or label neutral while LM picks one extreme); (3) calibration is non-linear (LM logits do not map uniformly to perceived qualitative change), requiring non-uniform binning to better match human perceptions; (4) few-shot prompt design and ordering affect LM judgments and can change agreement patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Documented failure modes: (a) reward hacking/adversarial exploitation — generations that exploit LM evaluation semantics to receive very high LM scores despite being low quality to humans (correlation drops in fitness range ~0.995–1); (b) decreased reliability on the top end of LM confidence (LM high-confidence scores not reliably matching human preference); (c) sensitivity to prompt exemplars and ordering (few-shot prompting can reduce Human-AI agreement); (d) categorical/axis mis-specification: when diversity axes are poorly defined, LM evaluation can miss important aspects (e.g., neglect interactions between characters); (e) some bins remain empty (LM judge + generator failing to discover certain genres/forms, e.g., limericks in poetry experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Reported strengths where LLM-as-judge matches or aids humans: (1) overall strong alignment with humans in many settings (e.g., QDAIF Human-AI agreement up to 0.833); (2) reliable categorical labeling in many cases (73% agreement overall, 82% when annotators agree); (3) GPT-4 showed high internal consistency (quality ratings fluctuated by ≤1 point across repeated calls); (4) AI feedback outperformed semantic-embedding feedback in guiding search to text preferred by humans (see embedding comparison); (5) LMs enable scalable, consistent, and cheap evaluation compared to humans and can evaluate attributes that are hard to hand-craft.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Paper-recommended mitigations and best practices: (a) validate LM judgments with human evaluation (especially for high-confidence LM scores); (b) use RLHF finetuning to make evaluators more robust to adversarial/reward-hacking generations; (c) consider ensembles of different AI evaluators to reduce correlated blind spots; (d) use seed examples and constrained initialization to reduce pathological search behavior; (e) use non-uniform binning to account for LM calibration behavior when discretizing diversity axes; (f) combine LLM and human evaluations for final assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>QUALITY-DIVERSITY THROUGH AI FEEDBACK</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Diversity through AI Feedback', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4005.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4005.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 as evaluator (Poetry)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used to determine genre/tone labels and to rate poem quality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>In the Poetry domain experiments GPT-4 was used as the evaluator for both categorical diversity (closest genre/tone labels) and scalar quality ratings (1–10), showing high internal consistency across repeated calls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>QUALITY-DIVERSITY THROUGH AI FEEDBACK</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-a-judge (GPT-4) assigning genre/tone categories and numeric quality ratings for generated poems within MAP-Elites (Poetry domain).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Poetry generation with categorical diversity axes (genre: haiku/sonnet/ballad/limerick/hymn; tone: happy/dark/mysterious/romantic/reflective).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>No single summary Human-AI agreement rate for poetry with GPT-4 is reported; authors report high internal consistency of GPT-4 (quality ratings for the same poem fluctuated by no more than one point across multiple LM calls).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>GPT-4 provided highly consistent categorical labels and scalar ratings across calls, but the paper notes that even with GPT-4 as judge the evolutionary process failed to populate some bins (e.g., limericks), indicating evaluator + generator limitations in exploring certain formal categories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limitations observed: inability of the generation+rewrite pipeline to discover certain categorical forms in practice (some bins empty); need for targeted prompting/guidance (LMX-guided or targeted generation) to reliably hit all discrete genres/tone combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Strengths: GPT-4 exhibited stable and consistent judgments (≤1 point variability), enabling reliable numeric scores for MAP-Elites; GPT-4-based evaluation appeared effective enough to drive higher QD scores for QDAIF in poetry compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When using GPT-4 as evaluator, add explicit diversity guidance for generation (e.g., instruct rewrite to a target genre/tone or use LMX-guided prompts) to better cover discrete bins; validate with humans for bins that remain empty or where diversity is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>QUALITY-DIVERSITY THROUGH AI FEEDBACK</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Diversity through AI Feedback', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4005.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4005.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic embedding feedback (QDEF)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic embedding-based feedback using a 13B embedding model as evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative evaluator tested: compute cosine similarity between a generated text embedding and query attribute embeddings to derive quality and an axis-based diversity measure; compared against AI feedback (LM evaluators).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>QUALITY-DIVERSITY THROUGH AI FEEDBACK</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Embedding-based LLM surrogate judge (13B embedding model) that measures similarity to attribute query embeddings to produce continuous diversity/quality scores in MAP-Elites.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Creative text generation (Opinions, Stories) — used as an alternative to instruction-tuned LM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>13B semantic embedding model (architecture described per Muennighoff 2022; exact name not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Reported to have lower human-alignment in human study: QDEF Human-AI agreement values (per Table 2) were lower than corresponding QDAIF runs (examples: QDEF runs had Human-AI agreement 0.5–0.8 while QDAIF runs showed 0.6–1.0 depending on setup). The paper summarizes AI feedback (LM) agreed slightly more often with humans than embedding feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Embedding feedback tended to reward outputs that closely matched the query text/attribute embedding, producing many texts that were semantically near the query but lacked the subjective qualities humans preferred; embeddings capture proximity to query but miss richer, nuanced human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Documented failures: (a) stronger propensity for reward-hacking where high scoring texts become overly similar to the attribute query (reduced subjective quality/diversity); (b) lower human-perceived quality of discovered elites compared to AI feedback; (c) less reliable as an evaluator for nuanced subjective attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>No cases reported where embedding feedback clearly outperformed instruction-tuned LM feedback in human-preference metrics; embedding feedback can be simpler and cheaper computationally but at cost of human-alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Paper recommends preferring instruction-tuned LM AI feedback over simple embedding-similarity feedback when the goal is human-preferred subjective quality/diversity; if using embeddings, validate heavily with human judgments and watch for reward-hacking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>QUALITY-DIVERSITY THROUGH AI FEEDBACK</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Diversity through AI Feedback', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Constitutional AI: Harmlessness from AI feedback <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Discovering language model behaviors with model-written evaluations <em>(Rating: 2)</em></li>
                <li>Self-critiquing models for assisting human evaluators <em>(Rating: 2)</em></li>
                <li>Defining and characterizing reward gaming <em>(Rating: 2)</em></li>
                <li>Deep neural networks are easily fooled: High confidence predictions for unrecognizable images <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4005",
    "paper_id": "paper-213d79556c2d50f6b4113a2d85309ebb5e3909f5",
    "extraction_schema_id": "extraction-schema-93",
    "extracted_data": [
        {
            "name_short": "AI feedback (LLM-as-a-judge)",
            "name_full": "AI feedback: instruction-tuned language models used as evaluators for quality and diversity",
            "brief_description": "Using instruction-tuned LMs (a finetuned AI feedback model in most experiments, and GPT-4 in some) to score generated texts for quality (binary or scalar) and to label or score diversity attributes, replacing hand-crafted heuristics or human-only evaluation in QD search.",
            "citation_title": "QUALITY-DIVERSITY THROUGH AI FEEDBACK",
            "mention_or_use": "use",
            "evaluation_setting": "LLM-as-a-judge evaluating quality and categorical/axis-based diversity in a MAP-Elites QD pipeline across creative-writing domains (Opinions, Stories; used to compute fitness/diversity and drive search).",
            "task_or_domain": "Creative text generation: opinion pieces, short stories (genre/ending), and poetry (genre/tone).",
            "llm_model_name": "Finetuned AI feedback model (instruction-tuned LMs; GPT-4 used as evaluator for Poetry in some experiments)",
            "agreement_rate": "Multiple reported statistics: Table 1 (Human-AI agreement on diversity label across methods) — Fixed-Few-Shot: 0.800; Shuffling-Few-Shot: 0.700; Random-Search: 0.733; LMX, Quality-Only: 0.633; QDAIF: 0.833. Appendix A.1 aggregated statistics — AI feedback agrees with a single human annotator on category label 73% of the time; when both annotators agree the Human-AI agreement increases to 82%; when both annotators give a non-neutral label agreement increases to 95%. (All values reported in paper.)",
            "qualitative_differences": "Reported differences vs humans include: (1) LMs can be well aligned on many labels but show misalignment at high-confidence ranges (top fitness), where LM scores stop correlating with human quality judgments; (2) LMs sometimes place samples differently around neutral/ambiguous cases (humans may differ or label neutral while LM picks one extreme); (3) calibration is non-linear (LM logits do not map uniformly to perceived qualitative change), requiring non-uniform binning to better match human perceptions; (4) few-shot prompt design and ordering affect LM judgments and can change agreement patterns.",
            "limitations_or_failure_cases": "Documented failure modes: (a) reward hacking/adversarial exploitation — generations that exploit LM evaluation semantics to receive very high LM scores despite being low quality to humans (correlation drops in fitness range ~0.995–1); (b) decreased reliability on the top end of LM confidence (LM high-confidence scores not reliably matching human preference); (c) sensitivity to prompt exemplars and ordering (few-shot prompting can reduce Human-AI agreement); (d) categorical/axis mis-specification: when diversity axes are poorly defined, LM evaluation can miss important aspects (e.g., neglect interactions between characters); (e) some bins remain empty (LM judge + generator failing to discover certain genres/forms, e.g., limericks in poetry experiments).",
            "counterexamples_or_strengths": "Reported strengths where LLM-as-judge matches or aids humans: (1) overall strong alignment with humans in many settings (e.g., QDAIF Human-AI agreement up to 0.833); (2) reliable categorical labeling in many cases (73% agreement overall, 82% when annotators agree); (3) GPT-4 showed high internal consistency (quality ratings fluctuated by ≤1 point across repeated calls); (4) AI feedback outperformed semantic-embedding feedback in guiding search to text preferred by humans (see embedding comparison); (5) LMs enable scalable, consistent, and cheap evaluation compared to humans and can evaluate attributes that are hard to hand-craft.",
            "recommendations_or_best_practices": "Paper-recommended mitigations and best practices: (a) validate LM judgments with human evaluation (especially for high-confidence LM scores); (b) use RLHF finetuning to make evaluators more robust to adversarial/reward-hacking generations; (c) consider ensembles of different AI evaluators to reduce correlated blind spots; (d) use seed examples and constrained initialization to reduce pathological search behavior; (e) use non-uniform binning to account for LM calibration behavior when discretizing diversity axes; (f) combine LLM and human evaluations for final assessments.",
            "citation": "QUALITY-DIVERSITY THROUGH AI FEEDBACK",
            "uuid": "e4005.0",
            "source_info": {
                "paper_title": "Quality-Diversity through AI Feedback",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 as evaluator (Poetry)",
            "name_full": "GPT-4 used to determine genre/tone labels and to rate poem quality",
            "brief_description": "In the Poetry domain experiments GPT-4 was used as the evaluator for both categorical diversity (closest genre/tone labels) and scalar quality ratings (1–10), showing high internal consistency across repeated calls.",
            "citation_title": "QUALITY-DIVERSITY THROUGH AI FEEDBACK",
            "mention_or_use": "use",
            "evaluation_setting": "LLM-as-a-judge (GPT-4) assigning genre/tone categories and numeric quality ratings for generated poems within MAP-Elites (Poetry domain).",
            "task_or_domain": "Poetry generation with categorical diversity axes (genre: haiku/sonnet/ballad/limerick/hymn; tone: happy/dark/mysterious/romantic/reflective).",
            "llm_model_name": "GPT-4",
            "agreement_rate": "No single summary Human-AI agreement rate for poetry with GPT-4 is reported; authors report high internal consistency of GPT-4 (quality ratings for the same poem fluctuated by no more than one point across multiple LM calls).",
            "qualitative_differences": "GPT-4 provided highly consistent categorical labels and scalar ratings across calls, but the paper notes that even with GPT-4 as judge the evolutionary process failed to populate some bins (e.g., limericks), indicating evaluator + generator limitations in exploring certain formal categories.",
            "limitations_or_failure_cases": "Limitations observed: inability of the generation+rewrite pipeline to discover certain categorical forms in practice (some bins empty); need for targeted prompting/guidance (LMX-guided or targeted generation) to reliably hit all discrete genres/tone combinations.",
            "counterexamples_or_strengths": "Strengths: GPT-4 exhibited stable and consistent judgments (≤1 point variability), enabling reliable numeric scores for MAP-Elites; GPT-4-based evaluation appeared effective enough to drive higher QD scores for QDAIF in poetry compared to baselines.",
            "recommendations_or_best_practices": "When using GPT-4 as evaluator, add explicit diversity guidance for generation (e.g., instruct rewrite to a target genre/tone or use LMX-guided prompts) to better cover discrete bins; validate with humans for bins that remain empty or where diversity is critical.",
            "citation": "QUALITY-DIVERSITY THROUGH AI FEEDBACK",
            "uuid": "e4005.1",
            "source_info": {
                "paper_title": "Quality-Diversity through AI Feedback",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Semantic embedding feedback (QDEF)",
            "name_full": "Semantic embedding-based feedback using a 13B embedding model as evaluator",
            "brief_description": "An alternative evaluator tested: compute cosine similarity between a generated text embedding and query attribute embeddings to derive quality and an axis-based diversity measure; compared against AI feedback (LM evaluators).",
            "citation_title": "QUALITY-DIVERSITY THROUGH AI FEEDBACK",
            "mention_or_use": "use",
            "evaluation_setting": "Embedding-based LLM surrogate judge (13B embedding model) that measures similarity to attribute query embeddings to produce continuous diversity/quality scores in MAP-Elites.",
            "task_or_domain": "Creative text generation (Opinions, Stories) — used as an alternative to instruction-tuned LM evaluators.",
            "llm_model_name": "13B semantic embedding model (architecture described per Muennighoff 2022; exact name not specified)",
            "agreement_rate": "Reported to have lower human-alignment in human study: QDEF Human-AI agreement values (per Table 2) were lower than corresponding QDAIF runs (examples: QDEF runs had Human-AI agreement 0.5–0.8 while QDAIF runs showed 0.6–1.0 depending on setup). The paper summarizes AI feedback (LM) agreed slightly more often with humans than embedding feedback.",
            "qualitative_differences": "Embedding feedback tended to reward outputs that closely matched the query text/attribute embedding, producing many texts that were semantically near the query but lacked the subjective qualities humans preferred; embeddings capture proximity to query but miss richer, nuanced human judgments.",
            "limitations_or_failure_cases": "Documented failures: (a) stronger propensity for reward-hacking where high scoring texts become overly similar to the attribute query (reduced subjective quality/diversity); (b) lower human-perceived quality of discovered elites compared to AI feedback; (c) less reliable as an evaluator for nuanced subjective attributes.",
            "counterexamples_or_strengths": "No cases reported where embedding feedback clearly outperformed instruction-tuned LM feedback in human-preference metrics; embedding feedback can be simpler and cheaper computationally but at cost of human-alignment.",
            "recommendations_or_best_practices": "Paper recommends preferring instruction-tuned LM AI feedback over simple embedding-similarity feedback when the goal is human-preferred subjective quality/diversity; if using embeddings, validate heavily with human judgments and watch for reward-hacking.",
            "citation": "QUALITY-DIVERSITY THROUGH AI FEEDBACK",
            "uuid": "e4005.2",
            "source_info": {
                "paper_title": "Quality-Diversity through AI Feedback",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Constitutional AI: Harmlessness from AI feedback",
            "rating": 2
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Discovering language model behaviors with model-written evaluations",
            "rating": 2
        },
        {
            "paper_title": "Self-critiquing models for assisting human evaluators",
            "rating": 2
        },
        {
            "paper_title": "Defining and characterizing reward gaming",
            "rating": 2
        },
        {
            "paper_title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
            "rating": 1
        }
    ],
    "cost": 0.014176499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>QUALITY-DIVERSITY THROUGH AI FEEDBACK</h1>
<p>Herbie Bradley ${ }^{1,2,3 <em>}$ Andrew Dai ${ }^{4 </em>}$ Hannah Teufel ${ }^{4}$ Jenny Zhang ${ }^{5,6}$<br>Koen Oostermeijer ${ }^{4}$ Marco Bellagente ${ }^{7 \dagger}$ Jeff Clune ${ }^{5,6,8}$ Kenneth Stanley ${ }^{9}$<br>Grégory Schott ${ }^{4}$ Joel Lehman ${ }^{10}$<br>${ }^{1}$ CarperAI ${ }^{2}$ CAML Lab, University of Cambridge ${ }^{3}$ EleutherAI ${ }^{4}$ Aleph Alpha<br>${ }^{5}$ Department of Computer Science, University of British Columbia<br>${ }^{6}$ Vector Institute ${ }^{7}$ Stability AI ${ }^{8}$ Canada CIFAR AI Chair ${ }^{9}$ Maven ${ }^{10}$ Stochastic Labs</p>
<h4>Abstract</h4>
<p>In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-QD controls. Further, human evaluation of QDAIF-generated creative texts validates reasonable agreement between AI and human evaluation. Our results thus highlight the potential of AI feedback to guide open-ended search for creative and original solutions, providing a recipe that seemingly generalizes to many domains and modalities. In this way, QDAIF is a step towards AI systems that can independently search, diversify, evaluate, and improve, which are among the core skills underlying human society's capacity for innovation. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Human innovation is not only a generative capacity for creativity, but also includes the ability to evaluate the subjective quality of new ideas and artifacts. Great ideas are rarely generated all at once out of whole cloth, but rather gradually emerge through divergent chains of elaboration and revision (Stanley \&amp; Lehman, 2015). To successfully navigate such a tree of ideas, the creator must evaluate which steps in a chain are worth pursuing further, a question that can be highly subjective, especially in domains with artistic or literary dimensions.</p>
<p>Until now, even if AI could provide candidates, the hope for such subjectively tinged evaluation lay firmly with humans. However, the emerging foundation model technology of recent years (Bommasani et al., 2021) now means that the model can also play the role of evaluator, even when the evaluation is in part subjective (Madaan et al., 2023). In this way, for the first time, an entire ideation process that returns a diverse set of interesting artifacts can in principle be automated. This process cannot be run by LMs entirely on their own, but requires chaining together a search algorithm with model calls in a nuanced way. This paper highlights one way to achieve this potential: to combine LMs with the field of quality-diversity (QD) (Mouret \&amp; Clune, 2015), which centers on how to design search processes that produce high-quality solutions that span a design space.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: QDAIF (left) covers more the search space with diverse, high-quality stories compared to the baseline (right). The baseline is LMX, Quality-Only (Meyerson et al., 2023), which optimizes only for the quality of solutions. QDAIF discovered more interesting stories about a spy and a politician, covering examples such as romance stories with a happy-ending, to horror stories with a tragic-ending. The baseline produced a story (right-middle position, starting with "Jason") with a lower quality score due to the lack of a desired spy character (denoted by the red-colored bin, for a story with a neutral ending, and leaning to horror). QDAIF discovered a better, more-relevant story (bottom-middle position, starting with "a wealthy politician") for this same neutral bin.</p>
<p>The main insight in QD algorithms is to explicitly maintain and seek high-quality diverse responses. Typically such search algorithms require hand-designed measures of diversity and quality, as well as a way to generate meaningful variation. Yet the most interesting and complex domains nearly always involve notions of performance, diversity, and variation that are subjective or difficult to specify algorithmically. Extending work that generates variation through LMs (Lehman et al., 2022; Meyerson et al., 2023) and evaluates the quality of potential solutions through LMs (Ahn et al., 2022), we show that LMs can also be used to evaluate qualitative aspects of diversity. In this way, LMs can instantiate the three main ingredients of QD search, thereby enabling powerful new QD algorithms that can ride the coattails of continual LM advances, which we name Quality-Diversity through AI Feedback (QDAIF). Such QDAIF can explore and return diverse, high-quality responses to an LM prompt through more-intuitive diversity measures, without the need for model fine-tuning (although, it could also be used for LMs to self-improve by generating fine-tuning data (Lehman et al., 2022; Chen et al., 2023)), an interesting direction for self-curated effective learning environments via generated data, towards AI-generating algorithms (Clune, 2019)).</p>
<p>We evaluate QDAIF across three creative writing domains: opinion writing, short stories, and poetry. The idea is that in such creative domains, users often enjoy seeing a wide range of possible stories or poems from which to choose or be inspired by. Quantitative results indicate that QDAIF significantly outperforms existing baselines. Additionally, through human evaluation, we observe a strong alignment between human and AI-generated feedback, providing empirical evidence that AI feedback is grounded and that the method can work in practice (i.e., it yields improved quality and diversity as measured by humans). Overall, QDAIF brings us a step closer to AI models that can independently search and innovate, one of the keystone abilities of humans that allow them to create culture and science (Stanley et al., 2017).</p>
<h2>2 Background &amp; Related Work</h2>
<h3>2.1 Evolution Through Large Models</h3>
<p>Advancements in language models have enabled new kinds of powerful search algorithms that apply LMs as search operators, e.g., to create variation or evaluate solutions. While other search algorithms could also be used, this paper creates a QDAIF algorithm by extending upon Evolution through Large Models (ELM) (Lehman et al., 2022), a framework for evolutionary search for code or text that uses LMs to generate intelligent variation (for example through specialized language</p>
<p>models trained on code diffs (Bradley et al., 2023b), or through simple few-shot prompting (Meyerson et al., 2023; Chen et al., 2023)). Most QDAIF results in this paper generate new search candidates through Language Model Crossover (LMX) (Meyerson et al., 2023), a recent and general few-shot prompting approach that can evolve e.g. mathematical expressions, sentences, Python programs, and prompts for text-to-image models, by leveraging in-context learning capabilities of LMs (Brown et al., 2020). The approach is simple: A few existing search candidates are concatenated into a prompt, predisposing the LM to generate new, similar candidates. In this way, LMX enables creating intelligent variation without requiring any specially-trained models. Our experimental implementation builds on OpenELM (Bradley et al., 2023a), a versatile open-source Python library designed for research into LM-based evolutionary algorithms.</p>
<h1>2.2 Quality Diversity Algorithms</h1>
<p>Traditional optimization algorithms aim to discover a single high-quality solution, which while appropriate for many situations, can fail to illuminate the full range of possible high-quality solutions. For creative and design problems in particular, a user may want to choose what they think is most appropriate from a diversity of such candidates. In contrast, Quality Diversity (QD) algorithms aim to optimize not just for a single optimal solution, but for a diverse set of high-quality solutions (Lehman \&amp; Stanley, 2011b; Mouret \&amp; Clune, 2015; Pugh et al., 2016; Fontaine \&amp; Nikolaidis, 2021). QD algorithms can thus provide a richer landscape of solutions, enabling adaptability and flexibility in addressing multifaceted challenges (Cully et al., 2015). In addition to a quality measure (i.e. an objective function), QD requires a metric such that it can encourage desired axes of diversity. For instance, Lehman et al. (2022) evolved Python programs to design varied locomoting robots, where the diversity dimensions are the robot's height, width, and mass.</p>
<p>A significant limitation of existing QD algorithms lies in their reliance on low-level quality and diversity measures (Mouret \&amp; Clune, 2015). This requirement confounds applying QD algorithms to complex and creative domains, such as the creative writing ones explored in this paper. Intuitively, such measures (e.g. sensor readings (Cully et al., 2015), feature engineering (Manning, 2009)) lack the subtlety and depth needed to capture the complexities of human creativity and intuition, e.g. nuances, moods, or cultural references that resonate in human experience. Interestingly, from having trained on vast amounts of human-generated data, LMs can begin to emulate such human-nuanced judgments (cf. Section 2.3). Thus, by employing an LM to evaluate both quality and diversity, QDAIF significantly simplifies and enlarges the range of domains QD can be applied to.</p>
<p>Feedback from learned ML models has been used in prior work to reduce the need for hand-crafted heuristics or expensive ground-truth evaluations. In model-based QD, learned feedback is supplied by surrogate models. Gaier et al. (2017) introduced the use of surrogates (via a Gaussian process) to predict fitness (quality). Subsequently, Keller et al. (2020) introduced a learned model to predict both fitness and behavior characteristics (diversity), becoming a standard approach (Lim et al., 2021; 2022; Zhang et al., 2022; Bhatt et al., 2022). Surrogate models require domain-specific training data to update their predictions on a limited domain, whereas AI feedback leverages off-the-shelf instruction-tuned LMs (Chung et al., 2022; Ouyang et al., 2022) to automate expensive human feedback for a variety of evaluation tasks. More recently, Fontaine \&amp; Nikolaidis (2021) utilized CLIP embeddings (Radford et al., 2021) as both quality and diversity measures to navigate the search space of StyleGAN (Karras et al., 2019), producing a range of faces with the desired characteristic (e.g. "A person with red hair"). We show that using pre-trained surrogate models is more prone to reward hacking in the natural language case (Skalse et al., 2022; Lehman et al., 2019) (cf. Appendix A.2). Hence, QDAIF capitalizes on the strengths of general-purpose LMs for evaluating generated solutions.</p>
<h3>2.3 AI Feedback</h3>
<p>Recent months have seen a surge in research that leverages LMs to provide feedback on the training, evaluation, or problem-solving capabilities of other LMs (Bai et al., 2022; Perez et al., 2022; Shinn et al., 2023; Wang et al., 2023a; Colas et al., 2023; Zhang et al., 2023; Lee et al., 2023). Bai et al. (2022) show that using LM-generated critiques and refinements has been instrumental in enhancing performance on metrics like helpfulness and harmlessness. One particularly promising direction for AI feedback is self-refinement, where LMs evaluate and score their own generations, and then itera-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of Quality-Diversity through AI Feedback (QDAIF). Dark components are where Language Models (LM) are employed. QDAIF randomly selects a solution from the QD archive. This chosen solution (parent) forms part of the prompt that is fed into an LM, undergoing LMX mutation to produce a new solution. An LM then evaluates the quality and diversity attributes of the new solution. We compare the newly evaluated solution with its existing solutions in the QD archive, and update it.</p>
<p>tively improve their output (Bai et al., 2022; Madaan et al., 2023). Self-refinement has demonstrated significant improvement in output quality as gauged by human evaluators (Madaan et al., 2023), underscoring generation-discrimination discrepancy (Saunders et al., 2022, p.12), meaning that it is often easier for a model to evaluate the quality of a generation than to generate the same high-quality text. Complementary to single-objective optimization with self-refine, QDAIF utilizes AI feedback to assess diversity in addition to quality, facilitating more varied and improved text generation over multiple iterations of refinement through evolution.</p>
<h2>3 Approach</h2>
<p>Figure 2 provides an overview of the approach, which is to extend a common QD algorithm (MAP-Elites) with LM operators that generate variation, as well as evaluate both the quality and diversity of candidate solutions. The result is a search algorithm capable of iterative discovery and refinement, applicable to subjective text-based domains.</p>
<p><strong>MAP-Elites.</strong> Our QDAIF implementation builds upon MAP-Elites (Mouret &amp; Clune, 2015), a widely used QD algorithm (Lehman et al., 2022; Cully et al., 2015; Nilsson &amp; Cully, 2021; Vassiliades et al., 2016). MAP-Elites discretizes the diversity space (i.e., dimensions of relevant diversity) into a grid, called the archive. The overarching objective is to populate each grid bin (or cell) within the archive with as high-quality a solution as possible. An iteration in MAP-Elites follows these steps: (1) randomly select an existing solution from the archive, (2) mutate the chosen solution to generate new solutions, (3) evaluate the new solution's quality and diversity characteristics, and (4) if the new solution is higher quality than the current occupant at the cell corresponding to its diversity characteristics, replace the previous cell occupant solution with the new solution. For a new solution to be added to the archive, it has to improve either the quality or the diversity of the grid, meaning that it has to either fill an empty bin or perform better than the solution already in its bin. QDAIF distinguishes itself from standard MAP-Elites in four key areas: archive initialization, solution mutation, solution evaluation, and grid discretization (cf. Figure 2). We provide details on each of these differences below.</p>
<p><strong>Initialization and Mutation.</strong> For archive initialization, QDAIF employs few-shot prompting, generating solutions based on a hand-chosen set of seed examples. We list in Appendix A.21 the three few-shot examples utilized in each domain, each chosen to span a breadth of diversity characteristics. For example, in a domain where you want diversity of sentiments (like the Opinions domain described in Section 4.1), the few-shot examples demonstrate positive, neutral, and negative sentiments. For solution mutation, QDAIF employs LMX-Near (referred to as "LMX" for brevity in the rest of this manuscript), as detailed in Meyerson et al. (2023). LMX evolves varied text representations (e.g., mathematical expressions, sentences, Python code) by leveraging effective in-context learning (Brown et al., 2020). LMX prompts are kept simple, typically starting with <em>"Here is a random example of"</em>. Appendix A.22 shows the full LMX prompts. We also introduce a novel mutation method with instruction-following prompts for poetry in Section 4.4.</p>
<p><strong>Archive Measures.</strong> While it is sometimes feasible to devise hand-crafted heuristics to evaluate the quality of a solution (e.g., efficiency in locomotion) or diversity characteristics (e.g., a robot's size and mass), this approach falters as domains become more complex and nuanced, as in creative writing. For example, hand-crafting robust heuristics for qualitative aspects of a story, such as its genre (e.g., romance vs. horror), is very difficult. QDAIF circumvents the need for hand-coded</p>
<p>measures through prompting LMs with easily-written natural language queries to generate feedback. In particular, capable LMs trained on expansive text corpora can begin to mirror human intuition across a range of potentially subtle diversity characteristics.</p>
<p>Quantifying Performance and Diversity. For quality assessment, we prompt the LM to discern whether the input text contains a high-quality solution or pertains to the requested topic, requesting a "yes" or "no" response. The solution's quality estimate is derived from the logarithm of the probability of the LM predicting one response versus the other response. Similarly, for diversity evaluation, we guide the LM to identify a particular diversity trait. For instance, in an opinion generating domain, the LM is prompted to gauge a solution's sentiment, with a requested response of "positive" or "negative". The log probability of these responses serves as our measure of solution diversity. Appendix A. 22 shows the full prompts used in each domain to evaluate the solutions. We also introduce a novel categorical approach to evaluate solution attributes based on raw predictions of discrete labels in Section 4.4.</p>
<p>Discretization. MAP-Elites typically partitions the grid into equally-sized bins, from the intuition that all parts of the behavior space are equally interesting. However, we observe that when assigning a bin along the diversity axis - which is in our approach based on logits of an LM AI feedback that qualitative changes in behavior do not uniformly correspond to changes in the logits (cf. Appendix A.31). This is likely due to the (non-linear) calibration behavior of instruction-tuned models in predicting the labels (as output tokens) of text passages (Jiang et al., 2021). Hence, we use custom non-uniform bins, which are denser towards range ends. Qualitative analysis of the generated text showed that the non-uniform bins yielded better alignment with typical human perceptions of diversity changes, influenced by both the AI model's calibration and the domain-specific goals.</p>
<p>Models and Setup. Details on the LMX generation model (Appendix A.24) and finetuned AI feedback model (Appendix A.25) are given, with details on the training of these LMs. Additional default hyperparameters are described in Appendix A. 27.</p>
<h1>4 EXPERIMENTS ON CREATIVE WRITING DOMAIN</h1>
<h3>4.1 Setup: Opinion Writing, Short Stories</h3>
<p>To demonstrate the versatility of QDAIF in different applications of creative text evolution, we evaluated QDAIF on these domains: Opinions, and Stories. The Opinions domain is focused on generating diverse, realistic pieces about one's opinions on eating vegetables and plant-based foods - the diversity measure is based on the sentiment of opinions on this topic (e.g. shown in example texts in the Figure 2 overview). For the Stories domain, the topic is about a short story, containing two characters: a spy, and a politician. The diversity of stories is evaluated using a variety of measures based on AI Feedback, with the main ones being: Stories - Genre (Romance vs Horror) (1D archive), Stories - Ending (Happy vs Tragic) (1D archive), and Stories - Genre and Ending (2D archive). These domains capture the strengths and limitations of all methods, ranging from simple (Opinions) to challenging (Stories - Genre and Ending). We show in Figure 1 that the 2D domain is challenging, yet QDAIF still outperforms the baseline in filling the archive with diverse, high-quality stories. The AI feedback prompts are outlined in Appendix A. 23.
Evaluation. To assess the performance of methods in creative writing generation, we compute QD scores (Pugh et al., 2016), a standard metric used to measure the quality-diversity of the discovered corpus of texts. A QD score is defined as the sum of the highest quality values found in each bin. To understand the alignment between AI and human feedback for practical applications in QDAIF, we conducted a human evaluation study on selected elite samples from each method (chosen from the median QD score run out of 5 random seed runs). Using a Likert scale (Allen \&amp; Seaman, 2007) for quality assessment, we evaluate the capability of each method to produce a collection of diverse, high-quality texts. To do so we calculate a "human" QD score, defined as the sum of quality scores given for all diversity categories identified by the annotator within the set. Furthermore, to understand how closely AI feedback aligns with human perspectives on subjective evaluation, we measured the agreement rates between human annotators and AI and between two human annotators. Details of the human study are specified in Appendix A.1, demonstrating the validity and advantages of AI feedback in generating human-like feedback on subjective quality and diversity measures.</p>
<h1>4.2 COMPARISONS BETWEEN QDAIF AND BASELINES</h1>
<p>To evaluate the strengths and limitations of QDAIF in generating high-quality and diverse creative writing texts, we compared our method against the following baseline methods:</p>
<ul>
<li>Fixed-Few-Shot: Use a fixed few-shot prompt (cf. Appendix A.21) to sample many completions for creative domain texts (i.e. no iterative search).</li>
<li>Shuffling-Few-Shot: Shuffle the in-context examples of Fixed-Few-Shot prompt, before sampling the completion from this prompt.</li>
<li>Random-Search: Create a prompt pool of examples (initialized from examples in Appendix A.21), add all completions from few-shot prompting to the pool, and choose fewshot examples from the growing pool (without pool size limit).</li>
<li>LMX, Quality-Only: Maintain a pool as in Random-Search, but only up to 100 highestquality completions (as evaluated by AI Feedback) are kept in the pool (i.e. iterative search focused only on quality). Single-objective LMX as in Meyerson et al. (2023).</li>
</ul>
<p>We choose a variety of baselines, some highlighting representative alternative approaches (e.g. fewshot prompting), and ablations of QDAIF, to validate our algorithmic choices. For example, Fixed-Few-Shot and Shuffling-Few-Shot enable the generation of different texts (relying on stochastic sampling), while being constrained to the output distribution of the fixed set of in-context examples. Random-Search and LMX, Quality-Only are methods where the (prompt) pool of examples that we can sample from grows in size, starting from an initial pool. In contrast to QDAIF, RandomSearch is limited by the lack of constraints in the prompt pool, especially in maintaining the quality of the growing pool through evaluation. LMX, Quality-Only adds a quality-based evaluation step with AI feedback that optimizes the quality of texts in the pool over time to contain only texts with high quality scores, but is not designed to encourage diversity in texts in comparison to QDAIF.
For each domain and baseline described above, we recorded runs for 2000 iterations, repeated with 5 different random seeds. To enable comparisons with baselines, AI feedback is used to compute the quality and diversity measures for all iteration outputs. Similar to QDAIF, the baseline methods use hand-written examples in Appendix A.21, either in a fixed prompt or as the initial prompt pool.</p>
<p>Performance Comparison. We report results comparing the QD score performance for the different methods of generating creative writing texts in Figure 3. We computed the mean, and the bootstrapped $95 \%$ confidence intervals (CI) from 100k resamples, across 5 random seeds of runs. We noticed that QDAIF achieves significantly better QD scores than all baseline methods in Opinions and Stories. The broader range of texts generated by QDAIF is also evident qualitatively. For example, in the Stories - Genre and Ending domain, while the baseline methods deliver straightforward and more predictable stories of how the spy "pulled out a knife and stabbed [the politician] to death", QDAIF generates a more dramatic story of how the spy "transformed into the monster and killed everyone". Random-Search is the worst-performing method overall, with significantly lower QD score performance in Opinions and Stories - Genre compared to the best-performing baselines. Interestingly, LMX, Quality-Only does not significantly outperform the methods using a fixed population prompt pool (Fixed-Few-Shot and Shuffling-Few-Shot). On Stories - Genre, LMX, Quality-Only is often weaker than Fixed-Few-Shot and Shuffling-Few-Shot. The results show that single-objective optimization cannot guide the search for diverse, high-quality texts alone. Furthermore, we found that QDAIF outperforms other (diversity-seeking baselines), as well as extensions of those baselines with quality filters, becoming more similar to QDAIF (cf. Appendix A.8).</p>
<p>Human Feedback Evaluation. We report the results of the human study comparing QDAIF and baseline samples in Table 1. We observe that compared to baselines, QDAIF is competitive with or better at discovering diverse, high-quality texts in Opinions and Stories according to human feedback. QDAIF sets also showed high agreement between humans and AI feedback on the diversity categories of presented texts, as well as between two annotators, competitive with Fixed-Few-Shot. Although the average perceived quality of texts is better from Fixed-Few-Shot, this is not enough for bringing high-quality examples for different niches of outputs (i.e. higher QD score). Furthermore, Shuffling-Few-Shot demonstrates even lower human evaluation scores, despite the use of the same fixed set of hand-written seeds, indicating lower robustness due to the use of different ordering of few-shot examples. Prior work hints at the sensitivity of LMs to few-shot prompt ordering, with task-solving capabilities varying significantly due to this ordering (Lu et al., 2021). The gap in</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: QDAIF significantly outperforms baselines in QD score performance in all domains. Performance stats with mean bootstrapped $95 \%$ CI, across 5 random seed runs. The maximum possible QD score is 20 ( 100 for 2D archive (4th plot)). See Appendix A. 7 for additional stats.</p>
<p>Table 1: QDAIF is competitive/better in terms of Human QD score against baseline methods from the human evaluation study. The stats are averaged across three domains: Opinions, Stories - Genre, and Stories - Ending. The Human QD score quantifies the perceived quality-diversity of a set of solutions returned by a method (i.e. high-quality texts for all the diversity categories of texts that the evaluator could identify). Quality rating is from humans. See Appendix A. 1 for study setup.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Human <br> QD score</th>
<th style="text-align: center;">Quality <br> rating</th>
<th style="text-align: center;">Human-AI <br> agreement</th>
<th style="text-align: center;">Human <br> agreement</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Fixed-Few-Shot</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">4.133</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.867</td>
</tr>
<tr>
<td style="text-align: left;">Shuffling-Few-Shot</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">3.500</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.667</td>
</tr>
<tr>
<td style="text-align: left;">Random-Search</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">3.300</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.600</td>
</tr>
<tr>
<td style="text-align: left;">LMX, Quality-Only</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;">3.533</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.733</td>
</tr>
<tr>
<td style="text-align: left;">QDAIF (ours)</td>
<td style="text-align: center;">0.772</td>
<td style="text-align: center;">3.900</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.800</td>
</tr>
</tbody>
</table>
<p>human-evaluated performance between Fixed-Few-Shot and Shuffling-Few-Shot indicates that reliance on fixed prompts is less likely to enable reliable search, in contrast to the robustness shown by QDAIF. Additionally, Random-Search and LMX, Quality-Only obtained even lower human evaluation scores, even though the methods either explore different prompts or optimize for the quality of texts. We provide a detailed discussion (for baseline methods) on findings from the subjective study of the discovered texts in Appendix A.15, as well as the qualitative behavior of the text search over time in Appendix A.16. Through guided evolutionary search, QDAIF surpasses all baseline methods in terms of computed QD score performance, and is competitive (or better) compared to baselines, according to human evaluation.</p>
<h1>4.3 Extensions to AI Feedback and Mutation Model</h1>
<p>In addition to experiments with QDAIF described in previous sections, we investigated the effects on the performance due to variations of the method.</p>
<p>LMX Model Size. We used larger versions of the LMX models (30B and 70B) for mutation, and compared it to the performance of the 13B model (default). While no relationship was found between model size and QD score, quality ratings from human feedback improved with outputs from larger models (described in detail in Appendix A.3).</p>
<p>Few-Shot AI Feedback. We compared the performance of QDAIF on the Stories - Genre domain when we prompted our AI feedback model for diversity measures given 2-shot, 4-shot, and 8-shot prompts. Using a higher number of few-shots led to improvements in human quality ratings of texts. Further discussion and results are highlighted in Appendix A.4.</p>
<p>Varying Initialization and Mutation Method. Ideally, QDAIF would be simpler if it could be run without seed examples (e.g. requesting a story from an instruction-following LM). We investigated the potential of QDAIF when the solution population is initialized from zero-shot prompted generations, and evolved using LMX. Initial results on Opinions and Stories along with prior work discussion are shown in Appendix A.5, highlighting comparable performance in terms of QD score, with divergence observed in alignment with human preferences. We also find that QDAIF, in these domains, is robust to the mechanism of generating variation. Appendix A. 6 describes an alternative mutation method (based on a more gradual few-shot replacement operation) that is more effective in some circumstances, although in general offers comparable performance. We provide a detailed discussion (for QDAIF methods) on findings from the subjective study of the discovered texts in Appendix A.13, as well as the qualitative behavior of the text search over time in Appendix A.14.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: QDAIF (LMX-guided) (left) covers the space of poetry with high-quality solutions (on a rating scale), with poems matching the closest bins. QDAIF solutions take qualitative inspiration from the seed poem's imagery of "fields of green waves" in Appendix A. 30 while giving meaningfully diverse kinds of poems across the search space. QDAIF (LMX-rewrite) (not shown) also covers more the space of diverse, high-quality poems compared to Random-Poems (right).</p>
<p>There may be many ways of leveraging LMs to generate variation (including finetuning models; see Appendix A.11), and this is an exciting avenue of future research.</p>
<h1>4.4 Evolving Solutions through Instruction Guidance</h1>
<p>This experiment explores scaling up QDAIF to a more capable model, GPT-4 (OpenAI, 2023), in the challenging task of generating poetry, highlighting how QDAIF will benefit from advances in model capabilities. The aim of the Poetry domain is to produce high-quality poems with varying genres and emotional tones, unrestricted by topic. Here, the MAP-Elites archive has two axes of diversity: genre and tone, and they are delineated using categorical labels. The genre axis has the labels "haiku", "sonnet", "ballad", "limerick", and "hymn", while the tone axis has the labels of "happy", "dark", "mysterious", "romantic", and "reflective". We created a new mutation operator, LMX-rewrite, for this domain, that leverages instruction-following to tell a model to translate a parent poem into an offspring with different diversity characteristics. To generate a new solution, we prompt GPT-4 to rewrite the selected poem into an inspired, different poem: Inspired by this poem, write a new poem of very high, award winning quality with a different poetic genre (format and form) and tone compared to the poem above.
We used GPT-4 to determine the genre and tone of a poem and rate its quality. For quality, we prompt GPT-4 to Rate the quality of the above poem on a scale from 1 to 10. To determine the diversity attributes, we prompt GPT-4 to determine which genre or tone is the poem closest to. For example, to determine genre, we ask GPT-4: What genre is this poem closest to from the following list: ["haiku", "sonnet", "ballad", "limerick", "hymn"]? Appendix A. 30 shows the full prompts and setup. We observed high consistency in GPT-4's responses; across multiple LM calls, quality ratings for the same poem fluctuated by no more than a single point. We show qualitative examples of poems with their AI feedback evaluations in Appendix A.19. In this setup, all methods are run for 200 iterations, and QDAIF is benchmarked against the baseline, Random-Poems, as well as an ablation method, Fixed Seed Rewrite. Random-Poems simply generates 200 random poems. Fixed Seed Rewrite rewrites only the seed poem in Appendix A.30, without evolutionary search.
We found that QDAIF achieves a higher QD score of 130 (CI: 118 - 145) in comparison to RandomPoems with 76 (CI: 67 - 85) and Fixed Seed Rewrite with 99 (CI: 72 - 117). We observed a similar trend (with a wider performance gap between QDAIF and other methods) when we used GPT-3.5-Turbo for the generation step instead of GPT-4 while keeping GPT-4 as the evaluator (cf. Appendix A.17). QDAIF was shown to have greater QD score performance than the other methods according to the Mann-Whitney U Test ( $p \leq 0.05$ ). Still, QDAIF with LMX-rewrite fails to discover solutions in some bins (e.g. limericks). For this, we can adapt QDAIF by adding guidance on the desired (randomly chosen) genre and tone for rewriting (LMX-guided). The performance of</p>
<p>this method of QDAIF is on par with a Targeted-Poems approach (generating high-quality poems of randomly chosen genre and tone per step) in terms of QD score, and even better when using an older version of GPT-4 (cf. Appendix A.17). Furthermore, we found that the rewriting step is useful for generating poems that can meaningfully preserve inspiration from parent poems, enabling users to control with more nuance the outcomes of search, through approximating the iterative refinement typical of human creative processes (Stanley et al., 2017) (see Appendix A.18). Figure 4 highlights the potential of QDAIF with GPT-4, especially in controlling a set of solutions subjectively aligned with AI feedback labels. QDAIF is even demonstrated for practical applicability to a domain outside of creative writing, for solving coding problems (cf. Appendix A.20). Overall, results highlight that diversity suffers when prompting models without explicit diversity guidance (Renda et al., 2023; Friedrich et al., 2023; Kirk et al., 2023), and that evolution with a rewriting mutation operator can lead to more human-influenceable, diverse, and high-quality solutions.</p>
<h1>5 Discussion and ConCLUSION</h1>
<p>This paper introduces QDAIF, a quality-diversity method that aims to discover diverse and highquality solutions in qualitative domains, by leveraging advances in foundation models to evaluate the quality and diversity of generated individuals. QDAIF outperforms baseline methods in returning more diverse, high-quality solutions in creative writing domains (Opinions, Stories, Poetry), that benefit greatly from accurate AI feedback measures. The paper's results highlight that QDAIF can succeed at its aims, generating solutions that align with human perception of quality and diversity.
We note limitations with QDAIF that motivate future work. Firstly, we suspect reward hacking happening when using LMs to generate feedback. Our human evaluation investigation shows that while the LM's evaluation of quality mostly aligns with human perception, the correlation drops when the evaluated quality is in the range 0.995 to 1 (cf. Figure 5). The text generation might have exploited certain attributes or phrasings that allow an LM to give a high-quality estimate, but not what humans would agree is good. This is a common issue highlighted by other works when using AI models as classifiers or evaluators (Nguyen et al., 2015a), highlighting risks of open-ended search to be tackled (Ecoffet et al., 2020). One method to address this limitation could be to use RLHF finetuning (Ouyang et al., 2022) to produce LMs that can detect and mitigate adversarially generated texts. Another possible approach could be to use an ensemble of different AI models to evaluate solutions, rather than relying only on one; the hope would be that robustness would result from models having uncorrelated blind spots.
Furthermore, although QDAIF makes it easy to specify qualitative aspects of diversity through natural language prompts, it still requires specified definitions of diversity axes. For example, if we applied QDAIF to generate short stories of different genres (e.g. comparing horror vs. romance), it would not autonomously explore other important attributes that a writer might care about (e.g. firstperson vs. third-person perspective) unless explicitly specified. When we tested different diversity measures in the Stories domain, such pathologies were observed (Appendix A.32). For example, when using "hero spy vs. hero politician" as the diversity measure, many of the solutions generated tend to neglect the interaction between the spy and the politician, focusing solely on the character that is meant to be the hero. However, someone writing a short story about a spy and a politician would naturally care about how the characters interact with one another. One possible way to automatically determine interesting diversity measures is to utilize the human notions of interestingness distilled into foundation models (Zhang et al., 2023). That is, we could ask LMs to suggest interesting diversity measures that a human would typically care about in the domain, thereby enabling a more autonomous creative search (see Appendix A. 10 for findings on the potential of this method).
In conclusion, we show that QDAIF is a promising approach to open-ended search that can reveal unexplored creative writing spaces, surpassing alternative text generation methods in generating diverse high-quality natural language text. AI feedback, Evolution through Large Models (ELM), and quality-diversity search (QD) were found to be essential ingredients for enhanced AI systems that can innovate in subjective spaces, similar to past research on Innovation Engines (Nguyen et al., 2016; 2015b). In fact, we see AI feedback as a general ingredient for open-ended search for solutions in multimodal domains, capable of following instructions beyond text (Liu et al., 2023). QDAIF can be easily extended to multi-modal domains (e.g. vision-language) for synthetic data generation and evaluation, building on top of recent advances in the field (Eichenberg et al., 2021; Alayrac</p>
<p>et al., 2022; Bellagente et al., 2023; Driess et al., 2023; Bhatt et al., 2023; Sudhakaran et al., 2023; Todd et al., 2023). We see many possibilities from QDAIF to build creative search systems with evaluation, diversification, and improvement capabilities, bringing us closer to AI that can support and extend human innovation.</p>
<h1>ETHICS STATEMENT</h1>
<p>Human evaluations were performed by the co-authors of this paper and select colleagues. All human evaluators provided informed consent, and their feedback and assessments were obtained without coercion or bias. We took action to prevent bias by presenting evaluators with texts to evaluate in a blind setting, with only the instructions for the study annotation task presented (to carefully read through the presented texts, then give a quality score and a label of the characteristic that best matches the texts). We show a detailed setup for the human study in Appendix A.1.</p>
<p>For transparency, we provide the full set of results with caption descriptions from our human evaluation. In the Opinions domain, Tables 13-16 contain the human evaluation results for sets from baseline methods, Tables 29-32 contain the human evaluation results for sets from QDAIF methods, and Tables 25-28 contain the human evaluation results for sets from embedding feedback QD methods. In the Stories - Genre domain, Tables 17-20 contain the human evaluation results for sets from baseline methods, and Tables 33-36 contain the human evaluation results for sets from QDAIF methods. For the Stories - Ending domain, Tables 21-24 contain the human evaluation results for sets from baseline methods, and Tables 37-40 contain the human evaluation results for sets from QDAIF methods.</p>
<h2>AUTHOR CONTRIBUTIONS</h2>
<p>Herbie developed the setup and framework for the Poetry domain experiments and base library for research. Andrew developed the setup and experiments for the Opinions and Stories domains, and contributed to extended studies, visualization, and analysis across experiments in the paper. Hannah contributed additional experimentation in the Stories domain, in addition to coordinating part of human evaluation studies. Jenny contributed qualitative analysis across studied domains. Koen developed visualization scripts used in Opinions and Stories domain experiments. Marco contributed to part of the technical implementation and ideation. Andrew conducted the blind human evaluation study, and Grégory advised on the conduct and analysis of the human study. Joel, Jeff, and Ken initiated early ideation for this work. Joel, Grégory, Jeff, and Ken advised and guided. Andrew, Jenny, Herbie, and Joel wrote the manuscript with edits and feedback from all authors.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>We thank Robert Baldock, Samuel Weinbach, Souradeep Nanda, Jan Zierstek, and Andres Felipe Cruz Salinas for insightful discussions and feedback within the lab at Aleph Alpha. We also thank Katherine Hardgrave, David Nugent, Daniel Flood, and Formula Trinity Autonomous for the inspiration that seeded the momentum leading up to this work.</p>
<h2>REFERENCES</h2>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as I can, not as I say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: A visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022.
I. Elaine Allen and Christopher A. Seaman. Likert scales and data analyses. Quality progress, 40 (7):64-65, 2007.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022.</p>
<p>Marco Bellagente, Manuel Brack, Hannah Teufel, Felix Friedrich, Björn Deiseroth, Constantin Eichenberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Koen Oostermeijer, et al. Multifusion: Fusing pre-trained models for multi-lingual, multi-modal image generation. arXiv preprint arXiv:2305.15296, 2023.</p>
<p>Varun Bhatt, Bryon Tjanaka, Matthew Fontaine, and Stefanos Nikolaidis. Deep surrogate assisted generation of environments. Advances in Neural Information Processing Systems, 35:3776237777, 2022.</p>
<p>Varun Bhatt, Heramb Nemlekar, Matthew Fontaine, Bryon Tjanaka, Hejia Zhang, Ya-Chuan Hsu, and Stefanos Nikolaidis. Surrogate assisted generation of human-robot interaction scenarios. arXiv preprint arXiv:2304.13787, 2023.</p>
<p>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Herbie Bradley, Honglu Fan, Francisco Carvalho, Matthew Fisher, Louis Castricato, reciprocated, dmayhem93, Shivanshu Purohit, and Joel Lehman. OpenELM, January 2023a. URL https: //github.com/CarperAI/OpenELM.</p>
<p>Herbie Bradley, Honglu Fan, Harry Saini, Reshinth Adithyan, Shivanshu Purohit, and Joel Lehman. Diff models - A new way to edit code. CarperAI Blog, Jan 2023b. URL https://carper. ai/diff-model/.</p>
<p>Jonathan C. Brant and Kenneth O. Stanley. Minimal criterion coevolution: A new approach to open-ended search. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 67-74, 2017.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Angelica Chen, David M. Dohan, and David R. So. Evoprompting: Language models for code-level neural architecture search. arXiv preprint arXiv:2302.14838, 2023.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Jeff Clune. AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence. arXiv preprint arXiv:1905.10985, 2019.</p>
<p>Cédric Colas, Laetitia Teodorescu, Pierre-Yves Oudeyer, Xingdi Yuan, and Marc-Alexandre Côté. Augmenting autotelic agents with large language models. arXiv preprint arXiv:2305.12487, 2023.</p>
<p>Antoine Cully. Autonomous skill discovery with quality-diversity and unsupervised descriptors. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 81-89, 2019.</p>
<p>Antoine Cully and Yiannis Demiris. Hierarchical behavioral repertoires with unsupervised descriptors. Proceedings of the Genetic and Evolutionary Computation Conference, 2018.</p>
<p>Antoine Cully, Jeff Clune, Danesh Tarapore, and Jean-Baptiste Mouret. Robots that can adapt like animals. Nature, 521(7553):503-507, 2015.</p>
<p>Li Ding, Jenny Zhang, Jeff Clune, Lee Spector, and Joel Lehman. Quality diversity through human feedback. arXiv preprint arXiv:2310.12103, 2023.</p>
<p>Danny Driess, Fei Xia, Mehdi S.M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.</p>
<p>Adrien Ecoffet, Jeff Clune, and Joel Lehman. Open questions in creating safe open-ended ai: tensions between control and creativity. In Artificial Life Conference Proceedings 32, pp. 27-35. MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info ..., 2020.</p>
<p>Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and Anette Frank. MAGMA - Multimodal augmentation of generative models through adapter-based finetuning. arXiv preprint arXiv:2112.05253, 2021.</p>
<p>Manon Flageat and Antoine Cully. Fast and stable map-elites in noisy domains using deep grids. In Artificial Life Conference Proceedings 32, pp. 273-282. MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info ..., 2020.</p>
<p>Manon Flageat and Antoine Cully. Uncertain quality-diversity: Evaluation methodology and new methods for quality-diversity in uncertain domains. IEEE Transactions on Evolutionary Computation, 2023.</p>
<p>Matthew Fontaine and Stefanos Nikolaidis. Differentiable quality diversity. Advances in Neural Information Processing Systems, 34:10040-10052, 2021.</p>
<p>Felix Friedrich, Patrick Schramowski, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Sasha Luccioni, and Kristian Kersting. Fair diffusion: Instructing text-to-image generation models on fairness. arXiv preprint arXiv:2302.10893, 2023.</p>
<p>Adam Gaier. Accelerating Evolutionary Design Exploration with Predictive and Generative Models. PhD thesis, Université de Lorraine, 2020.</p>
<p>Adam Gaier, Alexander Asteroth, and Jean-Baptiste Mouret. Data-efficient exploration, optimization, and modeling of diverse designs through surrogate-assisted illumination. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 99-106, 2017.</p>
<p>Adam Gaier, Alexander Asteroth, and Jean-Baptiste Mouret. Are quality diversity algorithms better at generating stepping stones than objective-based search? In Proceedings of the Genetic and Evolutionary Computation Conference Companion, pp. 115-116, 2019.</p>
<p>Luca Grillotti and Antoine Cully. Unsupervised behaviour discovery with quality-diversity optimisation. arXiv preprint arXiv:2106.05648, 2021.</p>
<p>Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning, pp. 2790-2799. PMLR, 2019.</p>
<p>Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language models know? On the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962-977, 2021.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.</p>
<p>Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401-4410, 2019.</p>
<p>Leon Keller, Daniel Tanneberg, Svenja Stark, and Jan Peters. Model-based quality-diversity search for efficient robot learning. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 9675-9680. IEEE, 2020.</p>
<p>Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity. arXiv preprint arXiv:2310.06452, 2023.</p>
<p>Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: Scaling reinforcement learning from human feedback with AI feedback. arXiv preprint arXiv:2309.00267, 2023.</p>
<p>Joel Lehman and Kenneth O. Stanley. Revising the evolutionary computation abstraction: Minimal criteria novelty search. In Proceedings of the 12th annual conference on Genetic and evolutionary computation, pp. 103-110, 2010.</p>
<p>Joel Lehman and Kenneth O Stanley. Abandoning objectives: Evolution through the search for novelty alone. Evolutionary computation, 19(2):189-223, 2011a.</p>
<p>Joel Lehman and Kenneth O. Stanley. Evolving a diversity of virtual creatures through novelty search and local competition. In Proceedings of the 13th annual conference on Genetic and evolutionary computation, pp. 211-218, 2011b.</p>
<p>Joel Lehman, Kenneth O Stanley, et al. Exploiting open-endedness to solve problems through the search for novelty. In ALIFE, pp. 329-336, 2008.</p>
<p>Joel Lehman, Bryan Wilder, and Kenneth O. Stanley. On the critical role of divergent selection in evolvability. Frontiers in Robotics and AI, 3:45, 2016.</p>
<p>Joel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Lee Altenberg, Julie Beaulieu, Peter J. Bentley, Samuel Bernard, Guillaume Beslon, David M. Bryson, Patryk Chrabaszcz, Nick Cheney, Antoine Cully, Stephane Doncieux, Fred C. Dyer, Kai Olav Ellefsen, Robert Feldt, Stephan Fischer, Stephanie Forrest, Antoine Frénoy, Christian Gagné, Leni Le Goff, Laura M. Grabowski, Babak Hodjat, Frank Hutter, Laurent Keller, Carole Knibbe, Peter Krcah, Richard E. Lenski, Hod Lipson, Robert MacCurdy, Carlos Maestre, Risto Miikkulainen, Sara Mitri, David E. Moriarty, Jean-Baptiste Mouret, Anh Nguyen, Charles Ofria, Marc Parizeau, David Parsons, Robert T. Pennock, William F. Punch, Thomas S. Ray, Marc Schoenauer, Eric Shulte, Karl Sims, Kenneth O. Stanley, François Taddei, Danesh Tarapore, Simon Thibault, Westley Weimer, Richard Watson, and Jason Yosinski. The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities, 2019.</p>
<p>Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O. Stanley. Evolution through large models. arXiv preprint arXiv:2206.08896, 2022.</p>
<p>Bryan Lim, Luca Grillotti, Lorenzo Bernasconi, and Antoine Cully. Dynamics-aware qualitydiversity for efficient learning of skill repertoires. arXiv preprint arXiv:2109.08522, 2021.</p>
<p>Bryan Lim, Alexander Reichenbach, and Antoine Cully. Learning to walk autonomously via resetfree quality-diversity. arXiv preprint arXiv:2204.03655, 2022.</p>
<p>Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.</p>
<p>Christopher D Manning. An introduction to information retrieval. Cambridge university press, 2009.
Elliot Meyerson, Mark J. Nelson, Herbie Bradley, Arash Moradi, Amy K. Hoover, and Joel Lehman. Language model crossover: Variation through few-shot prompting. arXiv preprint arXiv:2302.12170, 2023.</p>
<p>Jean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015.</p>
<p>Niklas Muennighoff. SGPT: GPT sentence embeddings for semantic search. arXiv:2202.08904, 2022.</p>
<p>Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 427-436, 2015a.</p>
<p>Anh Nguyen, Jason Yosinski, and Jeff Clune. Innovation engines: Automated creativity and improved stochastic optimization via deep learning. In Proceedings of the Genetic and Evolutionary Computation Conference, 2015b.</p>
<p>Anh Nguyen, Jason Yosinski, and Jeff Clune. Understanding innovation engines: Automated creativity and improved stochastic optimization via deep learning. Evolutionary computation, 24(3): $545-572,2016$.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019.</p>
<p>Olle Nilsson and Antoine Cully. Policy gradient assisted MAP-Elites. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 866-875, 2021.</p>
<p>OpenAI. GPT-4 technical report, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations. arXiv preprint arXiv:2212.09251, 2022.</p>
<p>Julien Pourcel, Cédric Colas, Pierre-Yves Oudeyer, and Laetitia Teodorescu. Aces: generating diverse programming puzzles with autotelic language models and semantic descriptors. arXiv preprint arXiv:2310.10692, 2023.</p>
<p>Justin K. Pugh, Lisa B. Soros, and Kenneth O. Stanley. Quality diversity: A new frontier for evolutionary computation. Frontiers in Robotics and AI, 3:40, 2016.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using siamese BERTnetworks. arXiv preprint arXiv:1908.10084, 2019.</p>
<p>Alex Renda, Aspen K. Hopkins, and Michael Carbin. Can LLMs generate random numbers? evaluating LLM sampling in controlled domains. In ICML 2023 Workshop: Sampling and Optimization in Discrete Space, 2023. URL http://people.csail.mit.edu/renda/ llm-sampling-paper.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.</p>
<p>William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.</p>
<p>Jimmy Secretan, Nicholas Beato, David B. D'Ambrosio, Adelein Rodriguez, Adam Campbell, Jeremiah T. Folsom-Kovarik, and Kenneth O. Stanley. Picbreeder: A case study in collaborative evolutionary exploration of design space. Evol. Comput., 19(3):373-403, 2011. doi: 10.1162/EVCO $\backslash a \backslash 00030$. URL https://doi.org/10.1162/EVCO_a_00030.</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: An autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460-9471, 2022.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631-1642, 2013.</p>
<p>Kenneth O Stanley. Compositional pattern producing networks: A novel abstraction of development. Genetic programming and evolvable machines, 8:131-162, 2007.</p>
<p>Kenneth O. Stanley and Joel Lehman. Why greatness cannot be planned: The myth of the objective. Springer, 2015.</p>
<p>Kenneth O. Stanley, Joel Lehman, and Lisa Soros. Open-endedness: The last grand challenge you've never heard of. O'Reilly Radar, 2017.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.</p>
<p>Shyam Sudhakaran, Miguel González-Duque, Claire Glanois, Matthias Freiberger, Elias Najarro, and Sebastian Risi. Mariogpt: Open-ended text2level generation through large language models, 2023.</p>
<p>Graham Todd, Sam Earle, Muhammad Umair Nasir, Michael Cerny Green, and Julian Togelius. Level generation through large language models. In Proceedings of the 18th International Conference on the Foundations of Digital Games, pp. 1-8, 2023.</p>
<p>Vassilis Vassiliades, Konstantinos Chatzilygeroudis, and Jean-Baptiste Mouret. Scaling up MAPElites using centroidal Voronoi tessellations. arXiv preprint arXiv:1610.05729, 2016.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.</p>
<p>Ren-Jian Wang, Ke Xue, Yutong Wang, Peng Yang, Haobo Fu, Qiang Fu, and Chao Qian. Diversity from human feedback. arXiv preprint arXiv:2310.06648, 2023b.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022a.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. arXiv preprint arXiv:2204.07705, 2022b.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Jenny Zhang, Joel Lehman, Kenneth Stanley, and Jeff Clune. OMNI: Open-endedness via models of human notions of interestingness. arXiv preprint arXiv:2306.01711, 2023.</p>
<p>Yulun Zhang, Matthew C Fontaine, Amy K Hoover, and Stefanos Nikolaidis. Deep surrogate assisted MAP-Elites for automated hearthstone deckbuilding. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 158-167, 2022.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 Human Study on Quality-Diversity of Text Samples</h2>
<p>A study through human feedback lets us understand how well QD score performance through AI feedback translates to creating a high-quality, diverse set of creative texts from a subjective angle. We compare sets of human feedback evaluations for samples of diverse elites at the end of each run to measure translations from AI-assessed performance to human-assessed performance of methods in generating high-quality, diverse texts. In addition, the Human QD Score (sum of mean quality score for each category/label that is found in the set according to human feedback) gives us a rough understanding of how aligned quality-diversity improvement during the search is with the more subjective notion of quality-diversity; this score is low if the set deemed to cover a wide space of diverse texts from AI feedback fails to subjectively cover the space of desired diversity according to human evaluations. To distinguish between quality ratings from humans vs. AI feedback in this section, we refer to quality scores as those from human evaluators, and fitness scores as those from AI feedback.</p>
<p>To assess the robustness of quality and diversity measures in AI feedback, we carried out a study involving diverse elite samples selected from different bins of the QD archive from our tested runs. Over a total of 28 experiments (of which 4 are from embedding feedback experiment runs), five distinct stories per experiment are reviewed by six annotators. Each (generated) text was independently reviewed by two persons, resulting in a total of 280 annotations.</p>
<p>During the annotation process, we collected a subjective assessment of the quality of the generation using a 5-point Likert scale based on the text quality in terms of flow, plot, presence of repetition, and correspondence to the study's topic. In addition, we assign each text to one of three categories. Two categories were specific to the study performed (such as positive/negative sentiment, romance/horror genre, or tragic/happy ending) and a third category was used when no element of the other two classes was identified.</p>
<p>We took action to prevent bias by presenting evaluators with texts to evaluate in a blind setting, with only the instructions for the study annotation task presented (to carefully read through the presented texts, then give a quality score and a label of the characteristic that best matches the texts). We provide the full set of results with caption descriptions from our human evaluation. In the Opinions domain, Tables 13-16 contain the human evaluation results for sets from baseline methods, Tables 29-32 contain the human evaluation results for sets from QDAIF methods, and Tables 25-28 contain the human evaluation results for sets from embedding feedback QD methods. In the Stories - Genre domain, Tables 17-20 contain the human evaluation results for sets from baseline methods, and Tables 33-36 contain the human evaluation results for sets from QDAIF methods. For the Stories - Ending domain, Tables 21-24 contain the human evaluation results for sets from baseline methods, and Tables 37-40 contain the human evaluation results for sets from QDAIF methods.</p>
<p>We compiled the results of the full study in Appendix A.12, and summarized the stats across the study in the paragraphs below.</p>
<p>Comparison of quality scores We observed that both annotators generally have a close agreement in their ratings with an average difference of 0.9 Likert points but there were occasional instances where they may differ by 2 or 3 units. These deviations occurred in $15 \%$ and $5 \%$ of the cases, respectively. This reflects that assessing the quality is somewhat subjective. To get a final estimation of the quality of the generated texts, the scores from both annotators were averaged.</p>
<p>Figure 5 shows the average quality ratings from annotators for various ranges of fitness (here, the quality score obtained from AI feedback). The ranges were chosen in a way that ensured a similar amount of samples in each range. We observe clear evidence of a correlation between human and AI quality measures, indicating the usefulness of using AI feedback to assess quality. However, we also observed that fitness for the texts with the highest scores becomes uncorrelated to human-assessed quality. This implies that above a certain threshold, fitness is not a reliable measure of quality in some cases (and maybe slightly lower fitness solutions were preferred by humans). Therefore, we suggest that in future work, more research is done in studying the relationship between (highconfidence) evaluations from AI feedback, and reward hacking of solutions (Nguyen et al., 2015a;</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Correlation plot between quality rating from human annotators, and fitness range (quality computed from AI feedback). Mean human-annotated quality and statistical error for different ranges of AI feedback fitness scores indicate more frequent instances of reward hacking (Skalse et al., 2022; Lehman et al., 2019) from the outputs of some search methods evaluated in this study.</p>
<p>Skalse et al., 2022; Lehman et al., 2019) under certain conditions and controls during the search (e.g. through the use of seed texts for human-preferred outputs, as shown in Table 1, or additional constraints on the generated solutions during search (Lehman \&amp; Stanley, 2010; Brant \&amp; Stanley, 2017)).</p>
<p>Comparison of diversity measurements For diversity, we observed that the two annotators agreed on the classification of a generated text in the same category 73 percent of the time. On the AI feedback side, diversity is collected on a 20 -bin axis, measuring different degrees of correspondence to two experiment-specific categories mentioned earlier. For the purpose of comparison with human feedback, these bins are clustered into a first category (bins 0 to 8), a second category (bins 11 to 19) and a neutral category (bins 9 and 10). Additionally, the 5 samples for each set were collected from a relatively uniform spread of bins from one end of the diversity axis to another (specifically, bins in set $[0,6,9,13,19]$, except for when the method fails to find a solution for the bin, then the next closest solution near the bin is chosen). This arbitrary arrangement allows for a relatively uniform distribution among the three categories.</p>
<p>On average, AI feedback agrees with a human annotator on the text category label $73 \%$ of the time. On samples where both annotators agree on the label of the text, this agreement rate increases to $82 \%$. Within this set, the agreement increases to $95 \%$ on samples when we look at samples where both annotators give a label that is not the neutral category. For some samples from bins 6 and 13, one human annotator gave a neutral label and the other annotator one of the other two labels (closer to the AI feedback diversity measure for these bins), indicating that some samples lie between neutral and extreme in the given measure. These findings suggest that diversity classification from AI feedback is relatively reliable, especially in texts where humans agree on the label.</p>
<p>Baseline quality rating. The average quality score given by annotators for a given sample was 3.18 , close to the middle rating of 3 . This gives us an indication of what could be considered the threshold for subjectively good or bad outputs.</p>
<p>The average human (subjective) QD score of all sets in the study is 0.606 . This is another indication for the threshold for determining which set from a given run/method had high-quality, diverse texts.</p>
<p>Table 2: QDAIF outperforms QD with embedding feedback (QDEF) according to human evaluation in human QD score and quality, when the difference is in feedback type for each base method.. The mean of Human QD score and quality are computed for the set of each method/run. The differences between the two evaluators for QD score and Quality are shown. Agreement between human and AI feedback on diversity labels given is slightly higher across QDAIF results compared to QDEF results, as well as on agreement between two annotators.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Human <br> QD score</th>
<th style="text-align: center;">QD score <br> range</th>
<th style="text-align: center;">Quality <br> rating</th>
<th style="text-align: center;">Quality <br> range</th>
<th style="text-align: center;">Human-AI <br> agreement</th>
<th style="text-align: center;">Human <br> agreement</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">QDEF, LMX, Zero-Shot Init</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.050</td>
<td style="text-align: center;">1.300</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.400</td>
</tr>
<tr>
<td style="text-align: left;">QDEF, LMX, Seeded Init</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">0.050</td>
<td style="text-align: center;">1.700</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.600</td>
</tr>
<tr>
<td style="text-align: left;">QDEF, LMX-Replace, Zero-Shot Init</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">2.100</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.600</td>
</tr>
<tr>
<td style="text-align: left;">QDEF, LMX-Replace, Seeded Init</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">2.300</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">QDAIF, LMX, Zero-Shot Init</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">1.700</td>
<td style="text-align: center;">1.400</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">QDAIF, LMX, Seeded Init</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">3.200</td>
<td style="text-align: center;">1.200</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: left;">QDAIF, LMX-Replace, Zero-Shot Init</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">4.300</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.600</td>
</tr>
<tr>
<td style="text-align: left;">QDAIF, LMX-Replace, Seeded Init</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.078</td>
<td style="text-align: center;">3.700</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.800</td>
</tr>
</tbody>
</table>
<h1>A. 2 Comparing AI Feedback Against Alternative Measures of Diversity</h1>
<p>To understand the effect of fuzzy evaluation tools as a component of our QD setup, we tested the use of an alternative method of feedback compared to our default method (AI feedback) in the MAPElites pipeline: semantic embedding feedback (Reimers \&amp; Gurevych, 2019). For this method, we used a 13B embedding model ${ }^{1}$, based on the architecture described in Muennighoff (2022), with an asymmetric search setup to measure the distance between a generated text (document embedding) and a query embedding for a desired measure (e.g. "This is a positive opinion"). To compute a diversity measure that can be defined on an axis, we first get the cosine distances between the document embedding and each of two opposing attribute query embeddings. From this, we can measure how close the document embedding is to one attribute compared to the other, and obtain a single diversity measure normalized in the range $[0,1]$. We use the same method for quality feedback, with a query that aims to measure the relevance of generated texts to a specific domain. The cosine similarity is used here as a quality score, with negative values being clipped to 0 . Additional setup details are shown in Appendix A.28. Since the subjective quality of the resulting elites (of creative texts) is more informative of the method's potential in further applications for practical synthetic data generation, We conducted a human study in addition to the reporting of QD score stats as part of our results.</p>
<p>We display performance statistics from our runs (QD Score) as well as human evaluation scores on elite samples in Table 2. We observe from our human study that using AI feedback as the evaluator instead of semantic embedding feedback for every variation of the QD run setup potentially leads to subjectively better generations. This is likely due to more prominent reward hacking (Skalse et al., 2022; Lehman et al., 2019) occurring in runs using embedding feedback, where the highest quality score texts end up being very similar to the query "An opinion piece about eating vegetables and plant-based foods", while not optimizing for the subjective quality of texts in different bins. Qualitative analysis of human-evaluated sets of texts from QD with embedding feedback is shown in Tables 25-28. Furthermore, agreement between human and AI feedback on text diversity labels was slightly higher across QDAIF sets compared to QD with embedding feedback (QDEF) sets. Overall, AI feedback outperforms the alternative measure of semantic embedding feedback, by guiding the generation of texts that are more preferred by humans, and by serving as a better evaluator for quality and diversity measures than embedding feedback.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: QD score plots for different LMX model sizes on the Stories - Genre domain. There is no clear trend in scaling model size with QD score.</p>
<h1>A. 3 On Scaling LMs for Mutation</h1>
<p>Previous work has consistently shown that LMs demonstrate improved capabilities in various taskbased benchmarks at larger scales (Kaplan et al., 2020; Chowdhery et al., 2022; Chung et al., 2022). This applies to performance in solving tasks through in-context learning, which LMX is based on. Prior work in LMX (Meyerson et al., 2023) has found that a relationship between model scaling and performance of mutations can be observed when evolving binary strings in a search domain. Interestingly, experiments in LMX as well as ELM (Lehman et al., 2022) observed that in some cases, emerging capabilities in mutation ability appear for reasonably small LMs, but may not scale with a clear trend.</p>
<p>Firstly, we show the QD score performance between runs in Figure 6. The standard error in the score across 5 seeds is also shown. We observed that the QD score from the 70B runs converged to a lower point compared to the 13B and 30B runs, which have comparable scores. This highlights a trend between model size and QD score that is not directly proportional. Although suggestive trends are not seen here, a study based on subjective feedback is still necessary for a deeper understanding of the performance of each experiment.</p>
<p>We observed from the human feedback evaluation a trend in the quality ratings, with average quality scores from each experiment set of: 3.43 (13B), 3.60 (30B), and 4.03 (70B). The quality score increases with an increase in model size used, with a higher jump in score for generated stories from the 70B runs.</p>
<p>In terms of the agreement on the genres of evaluated stories between AI feedback and human feedback, we observed the following percentage rates in the following sets: $80.0 \%$ (13B), $73.3 \%$ (30B), and $73.3 \%(70 \mathrm{~B})$. There is a slight decrease in agreement for texts from the 30B and 70B runs. We found that evaluators differed more frequently in their labels on stories deemed as "neutral" or "romance" according to AI feedback, suggesting that the use of the prompt pool in combination with larger models might lead to generated texts with misaligned AI evaluations on the genre. Still, these agreement rates indicate a good alignment between AI and human feedback.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: QD score Performance between Seeded Init and Zero-Shot. Performance stats with mean, interquartile mean (IQM) and bootstrapped $95 \%$ CI, across 5 random seed runs. Seeded Init is potentially better, but within the CI of Zero-Shot Init. In the Stories - Genre domain, the CI of Zero-Shot Init runs is much wider, indicating significant variation in performance for different random seeds.</p>
<h1>A. 4 On Few-Shot AI Feedback Prompting</h1>
<p>Instruction-following LMs are typically trained to align the model towards generating better answers to zero-shot prompts (Wei et al., 2021; Ouyang et al., 2022). However, few-shot prompting with exemplars was shown to be effective in some aspects with instruction-tuned LMs, especially towards understanding task structure and improving robustness to prompting variations (Wei et al., 2021).</p>
<p>In terms of the average human-evaluated quality, we see a drop in subjective quality for the set of stories from 2-shot AI feedback runs (3.10) in comparison to zero-shot AI feedback runs (3.43). We observed for the other sets that this score increases for the 4 -shot set (3.93) and the 8 -shot set (4.03). Furthermore, we see that this trend is mostly consistent when we consider the scores for each bin category. This suggests an improvement in QDAIF's ability to discover texts that are perceived to be of higher quality according to human feedback when we use a higher number of in-context examples during AI feedback evaluation.</p>
<p>In terms of the agreement between AI feedback and human feedback, we see a drop in average agreement for ratings that were given to stories in the few-shot feedback experiment sets, with $50.0 \%$ (2-shot), $66.7 \%$ (4-shot), and $56.7 \%$ (8-shot) agreement on sets, compared to $80.0 \%$ for zero-shot (default). The level of disagreement occurs more frequently on stories evaluated to have the romance genre. Additionally, evaluators labeled samples from the few-shot sets as "horror" or "neutral" more frequently than "romance", while the proportion of labels given to samples from the zero-shot set was more uniform.</p>
<p>The performance may vary due to the ordering of in-context examples in our few-shot prompts (in Appendix A.29, as shown in Lu et al. (2021). Furthermore, the nature and wording of input-output exemplars/tasks could also influence the performance, in addition to some variation due to the nature of subjective evaluation.</p>
<h2>A. 5 On the Initialization Method for QDAIF</h2>
<p>Recent results from LMX demonstrated successful optimization when the search was initialized by evolving a set of pre-existing seed examples (e.g. examples of equations, quotes, text-to-image prompts, code) (Meyerson et al., 2023). At the same time, previous applications of QD methods demonstrated successful search outcomes when using random initialization in different domains, such as in robotics, and latent space illumination of generative models (Cully et al., 2015; Fontaine \&amp; Nikolaidis, 2021; Bhatt et al., 2022).</p>
<p>We compared the performance between QDAIF with Seeded Init against Zero-Shot Init and a baseline method LMX, Quality-Only initialized with Zero-Shot Init. From Figure 7, we can see that there is potential improvement in QD score when using Seeded Init compared to Zero-Shot Init, but the difference may not be significant. However, the difference is clear on the lower performance of LMX, Quality-Only with Zero-Shot Init. This suggests that Zero-Shot Init is viable for QDAIF. Still, we need to analyze the effect of the initialization on qualitative samples of elite texts. We also compared the effects of initialization on subjective quality-diversity of texts (see Table 3). Sets of texts discovered by Seeded Init within single runs were found to be subjectively higher-quality and more diverse compared to sets from Zero-Shot Init ( 0.772 vs. 0.383 subjective QD score). This suggests potential reward hacking (described for RL problems in Skalse et al. (2022); Lehman et al. (2019) during the search with Zero-Shot Init runs, where potentially out-of-distribution texts can</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://aleph-alpha.com/luminous\%2Dexplore\%2Da\%2Dmodel\%2Dfor\%2Dworld\% 2Dclass\%2Dsemantic\%2Drepresentation/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
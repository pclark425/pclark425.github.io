<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8829 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8829</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8829</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-253244506</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2211.00053v1.pdf" target="_blank">Generating Sequences by Learning to Self-Correct</a></p>
                <p><strong>Paper Abstract:</strong> Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8829.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8829.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-CORRECT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction / Self-Corrector</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decomposition of generation into a base generator that produces an initial hypothesis and a learned text-to-text corrector that iteratively refines that hypothesis using a training procedure that pairs lower-value generations with nearby higher-value corrections; the corrector can optionally condition on explicit feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (GPT-Neo 1.3B, GPT-2 variants, GPT-3 engines used as generators or feedback providers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The framework is model-agnostic: experiments use GPT-Neo 1.3B as a corrector (and sometimes as a generator), GPT-2 (medium/large/XL) as generator/corrector for some tasks, and GPT-3 (davinci / text-davinci-002 / Instruct variants) as large generators or as feedback providers; model sizes explicitly mentioned include GPT-Neo 1.3B and GPT-2 Large/XL and GPT-3 (~175B, referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correction (learned corrector / iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train a separate corrector p_θ(y | y_curr, x, f(y_curr)) with an online self-corrective learning algorithm that (1) collects a pool of generated outputs from a base generator, (2) forms value-improving pairs (hypothesis, higher-valued correction) where 'value' is a scalar quality metric v(y), (3) samples pairs proportionally to improvement and proximity and updates the corrector via cross-entropy, and (4) explores by adding corrections sampled from the current corrector. At inference, decode y0 from generator p0, then apply the corrector iteratively y_{t+1} ∼ q(p_θ(· | y_t, x, f(y_t))) for T steps (T is task-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>general / multiple tasks (see per-experiment entries)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Self-CORRECT is evaluated across tasks requiring iterative improvement: mathematical program synthesis (programs that must execute to correct answers), lexically-constrained generation (cover specified constraint words), and toxicity reduction (reduce toxic continuations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Across tasks, the learned corrector consistently improves generator outputs (see task-specific entries).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline generator performance varies by task and model (see task-specific entries); SELF-CORRECT improves upon these baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Separate learned corrector module (text-to-text) trained with self-corrective learning using scalar value function v(y) (and optionally natural-language feedback f(y)); iterative decoding pipeline that applies the corrector multiple times to generator outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative improvements reported on three diverse tasks: large increases in program-synthesis correctness, substantial increases in lexical-constraint coverage, and large reductions in measured toxicity; ablations show pairing/proportional sampling and exploration improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires access to a value function v(y) during training (and optionally at inference for feedback). Training relies on the generator producing diverse partial solutions to form improvement pairs. The corrector's effectiveness can depend on number of correction iterations and quality of feedback; when using a feedback model with access to gold solutions there is risk of solution leakage. No catastrophic failure cases are reported, but in math more than 2–3 corrections may require extra guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared favorably to constrained decoding (NeuroLogic-A*), decoding-time control methods (PPLM, GeDi, DExperts), and RL-based methods for detoxification (PPO, Quark); SELF-CORRECT often attains better constraint coverage / lower toxicity while maintaining fluency and being computationally efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations show (1) sampling pairs proportional to improvement & proximity improves performance vs. uniform sampling; (2) exploration (adding corrections sampled from the current corrector to the datapool) improves performance; (3) multiple corrections increase benefit (esp. with feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8829.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8829.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-CORRECT (Math)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction applied to Mathematical Program Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A corrector (GPT-Neo 1.3B) is trained to improve program generations (Python programs converted from word problems) produced by a base generator (GPT-Neo 1.3B or few-shot/prompted GPT-3), using correctness (execution equals ground-truth answer) as the scalar value function.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo 1.3B (corrector); generators: GPT-Neo 1.3B fine-tuned and GPT-3 (davinci/text-davinci-002 / Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-Neo 1.3B used as pretrained code-and-language model for both generator and as trained corrector; GPT-3 engines (≈175B) used as large generators or as feedback-providing LM in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correction corrector (1-step or multiple-step iterative corrections)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train corrector to map an incorrect or partially-correct program y to a higher-valued program y' (value = binary correctness after execution). Inference: decode y0 from generator (greedy), then apply corrector for k corrections (k=1 in main experiments; also experiments with up to 5 when using feedback). An 'oracle' variant only applies correction if draft is incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical program synthesis (MultiArith, Multitask, GSM8k)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a math word problem, generate a Python program that executes to the correct numeric answer. Datasets: MultiArith, AddSub, SingleOp, SVAMP, GSM8k.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>MultiArith: after correction 98–99% correctness (a ≈38 point improvement over generator). Multitask: large improvement (exact table values in paper); GSM8k: generator 8.57% → SELF-CORRECT 21% (always correct) and 24% (only correct incorrect solutions). For GPT-3 Instruct as generator: Multitask 84.90% → 90.90% after adding corrector; further training with GPT-3 generator yields Multitask 92.75%. GSM8k for GPT-3 Instruct: 36.80% → 45.00% with corrector; trained with GPT-3 gives 45.92%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>MultiArith/Multitask/GSM baselines vary by generator: one explicit baseline reported is generator correctness 8.57% on GSM8k (GPT-Neo baseline) and GPT-3 Instruct 84.90% / 36.80% on Multitask/GSM respectively (prior to correction).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>A separate corrector model trained via self-corrective learning using the execution-correctness scalar reward; pairing selects nearby higher-valued program corrections for supervised learning; inference applies the corrector to generator outputs (greedy decoding), optionally multiple times.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative gains: very high MultiArith accuracy (98–99%) after correction; +~38 percentage points over generator; GSM8k improvement from 8.57% → 21%/24%; corrector also transfers to improve larger generators (GPT-3) as shown by swap-in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Correctness requires executable programs and an automated executor to compute v(y). The corrector sometimes needs multiple corrections and additional guidance for complex problems; when using feedback provided by a feedback LM that has access to gold solutions there is risk of leakage. Performance on very challenging datasets (GSM8k) remains modest in absolute terms (e.g., 21–24% for GPT-Neo-based correctors).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms baselines based on similarly sized models reported in prior work (e.g., tuned GPT-Neo 2.7B variants), and shows transferability to larger generators; compared qualitatively to methods that sample self-sampled correct programs (Ni et al., 2022) and to RL-based approaches in other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Exploration (adding corrections sampled from the corrector to the datapool) improves program-synthesis performance; pairing and proportional sampling for pair selection also improve results versus uniform pairing (ablation summarized in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8829.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8829.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-CORRECT (Lexical)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction applied to Lexically-Constrained Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train a corrector to improve constraint coverage (percentage of required constraint words included) for sentence generation tasks; base generator is GPT-2 (medium/large) fine-tuned for the task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (medium for E2E; large for CommonGen) as generator and corrector</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 variants fine-tuned per task serve as base generators; a separate GPT-2 corrector is trained to maximize coverage metric.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correction corrector (iterative correction up to 3 steps)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train corrector with value function v(y) = coverage (percentage of constraints satisfied). At inference, run generator decoding (beam search) then apply up to 3 correction iterations (beam search) stopping early if all constraints are met.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Lexically-constrained generation (CommonGen, E2E)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate fluent sentences that include all provided constraint words (CommonGen) or realize structured inputs as natural language (E2E); metrics: constraint coverage, BLEU/CIDER/human fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>CommonGen coverage: GPT-2 baseline 91.50% → SELF-CORRECT 98.77% coverage. Other fluency metrics (BLEU/CIDER) are maintained or slightly improved; on E2E SELF-CORRECT outperforms NeuroLogic-A* in overall performance while using standard beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>CommonGen coverage 91.50% with GPT-2 baseline; E2E generator baseline reported in paper (GPT-2) is lower than SELF-CORRECT on constraint coverage and comparable or worse on fluency measures.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Corrector is trained with coverage as scalar reward and uses lexically-oriented natural-language feedback (during feedback experiments) that lists missing constraints; inference runs generator then iteratively applies corrector to add missing constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Substantial numerical improvement in coverage (e.g., CommonGen 91.50% → 98.77%) while maintaining BLEU/CIDER and human-judged fluency; SELF-CORRECT with NeuroLogic combined attains near best coverage with lower runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No specific failure cases reported, but the approach depends on being able to measure coverage reliably and on the generator producing partial solutions to be improved; runtime increases with more correction iterations (but still competitive).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against constrained decoding algorithms (NeuroLogic, NeuroLogic-A*); SELF-CORRECT paired with NeuroLogic or alone matches or outperforms NeuroLogic-A* while being faster; also compared to prefix-tuning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Not reported as a separate ablation in detail for lexical task beyond the general ablations: multiple corrections helped, and using feedback listing missing constraints improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8829.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8829.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-CORRECT (Toxicity)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction applied to Toxicity Reduction / Detoxification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train a corrector to reduce toxicity of generated continuations using the Perspective API toxicity score as the scalar value; the corrector is a finetuned GPT-2 Large that iteratively rewrites generator outputs to be less toxic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 Large (generator and fine-tuned corrector); evaluations also apply corrector to larger generators (GPT-2 XL, GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Off-the-shelf GPT-2 Large used as the base generator; separate GPT-2 Large instance finetuned as corrector. Perspective API used to compute continuous toxicity scores v(y) ∈ [0,1].</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correction corrector (iterative detoxification up to 3 corrections)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Corrector is trained to map an existing sampled continuation to a lower-toxicity continuation; training pairs are value-improving pairs chosen from a datapool; inference: sample 25 continuations with nucleus sampling from generator, then apply up to 3 correction iterations (corrector samples) per continuation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Toxicity reduction on REALTOXICITYPROMPTS</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given prompts designed to elicit toxic completions, generate continuations that avoid offensive content while preserving fluency and diversity. Metrics: Perspective API max toxicity (average maximum toxicity over 25 samples) and empirical toxicity probability (probability ≥1 of 25 samples being toxic), fluency/diversity measures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported example metrics (Appendix): GPT-2 Large baseline average max toxicity 0.527 → SELF-CORRECT 0.171 → SELF-CORRECT + FEEDBACK 0.156 (lower is better). SELF-CORRECT also substantially reduces empirical toxicity probability and outperforms baselines on fluency/diversity tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>GPT-2 Large baseline average max toxicity 0.527 (from Appendix table), other baselines (PPLM, GeDi, DExpert, PPO, Quark) have higher toxicity or degrade fluency relative to SELF-CORRECT as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Separate learned corrector conditioned on current hypothesis and optionally on natural-language feedback derived from Perspective API attribute scores; iterative decoding applies corrector multiple times (up to 3).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative reduction in toxicity scores (0.527 → 0.171), lower probability of toxic samples, and maintained or improved fluency/diversity compared to several decoding-time and RL baselines; multiple corrections further reduce toxicity (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Depends on the reliability and availability of an external toxicity scorer (Perspective API) for v(y) and for attribute feedback; some decoding-time methods may be cheaper but yield worse fluency; when swapping in much larger generators, corrector still reduces toxicity but benefits may vary by generator quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Directly compared to PPLM, GeDi, DExperts (inference-time control) and PPO, Quark (RL-based) and outperforms them in the reported metrics while maintaining fluency/diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Multiple-correction experiments show degradation improves monotonically with additional corrections up to the tested limits; using attribute-level natural-language feedback (e.g., 'decrease profanity') yields further improvements over scalar-only training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8829.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8829.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Modular Swap-in (Large Generators)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Correcting larger generators by swapping in generator at test-time or training-time</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstrates that a corrector trained with outputs from a smaller generator can be applied to improve outputs from a much larger generator (e.g., GPT-3), and that training with a large generator further improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Correctors: GPT-Neo 1.3B (for math) or GPT-2 Large (for toxicity); Generators swapped in include GPT-2 XL and GPT-3 (davinci / Instruct variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Corrector models are typically much smaller than the swapped-in generators (e.g., 1.3B corrector with ~175B GPT-3 generator); demonstrates transfer of correction behavior across generator models.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correction with modular generator-corrector swapping</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train a corrector with outputs from one generator (often smaller), then at inference decode initial hypothesis y0 from a larger generator and apply the trained corrector iteratively; alternative is to initialize datapool with large-generator outputs and train corrector on those.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Program synthesis and detoxification (transfer experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as above (math and toxicity) but testing whether a corrector trained on one generator can improve outputs from a different/larger generator at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Example: GPT-3 Instruct on Multitask improves from 84.90% → 90.90% when corrected by the trained corrector; GSM8k improved 36.80% → 45.00%; toxicity of large generators (GPT-2 XL, GPT-3) substantially reduced when refined by a corrector trained on smaller models (see Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Generators alone: GPT-3 Instruct Multitask 84.90% / GSM8k 36.80% (prior to correction); toxicity levels for GPT-2 XL/GPT-3 higher (baseline) compared to corrected outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Same learned corrector mechanism; the novelty is modularity — corrector and generator are decoupled and can be swapped at inference or training.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical improvements when applying small-model corrector to larger-model outputs, and further gains when training corrector with the large generator's outputs (quantified in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>While transfer works in the experiments shown, no formal guarantee that a corrector trained on one generator generalizes to all larger generators; effectiveness may depend on similarity of output distributions and on the corrector's capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Argues an advantage over methods that must fine-tune large generators (which may be infeasible for API-access models), since only the corrector needs training.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper compares two modes: (1) train with small generator then swap in large generator at test time, and (2) include large-generator outputs during training; both improve performance, with further gains when training used the large generator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8829.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8829.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-CORRECT + Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction augmented with explicit natural-language feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extend the corrector to condition on explicit feedback f(y) (a natural-language instruction or attribute), which can be derived automatically (e.g., Perspective API attributes, missing constraint phrases) or generated by a feedback LLM; using feedback improves correction quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Corrector models (GPT-Neo 1.3B for math experiments; GPT-2 Large for toxicity) conditioned on feedback; feedback often produced by GPT-3 (text-davinci-002) or Perspective API attribute selection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Corrector is augmented to receive textual feedback strings f(y) computed at training and optionally at inference (e.g., 'decrease profanity' or enumerations of missing constraint words); feedback for math experiments was provided by a prompted GPT-3 feedback model (with access to gold solution for training-time signals).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correction conditioned on natural-language feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>During training and inference the corrector conditions on feedback f(y) representing the most salient attribute to change (selected from Perspective API attributes for toxicity, enumerated missing constraints for lexical tasks, or LLM-produced program feedback for math); training sequences include feedback tokens and loss includes tokens after feedback markers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Toxicity reduction, Lexical constraints, Mathematical program synthesis (feedback experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same core tasks as earlier, but here the corrector explicitly receives a textual feedback signal describing what to change.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Feedback improves performance in all three studied tasks: for toxicity average max toxicity improved from SELF-CORRECT 0.171 → SELF-CORRECT + FEEDBACK 0.156 (Appendix table); lexical coverage improved (e.g., SELF-CORRECT to SELF-CORRECT + feedback increases coverage on CommonGen from 94.58% to 95.88% in some reported combinations), and math saw higher gains when feedback was available (specific numeric gains provided in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>SELF-CORRECT without feedback is already better than generator; adding feedback provides incremental improvements over scalar-only training.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>The corrector conditions on an explicit natural-language string f(y) derived from attribute comparisons (toxicity) or enumerations of missing constraints (lexical), or generated by a feedback LM for math; the training objective remains cross-entropy on value-improving pairs but with feedback included as conditioning context.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative improvements reported across tasks when feedback is used vs scalar-only training; ablations show more corrections are especially beneficial when feedback is present (e.g., math Multitask benefits from >1 corrections when feedback provided).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Quality depends on feedback accuracy; for math the feedback LM had access to gold solutions during feedback generation in experiments (improving feedback quality but risking solution leakage), so those math feedback results are exploratory. Using external APIs at inference (e.g., Perspective API) increases runtime and external dependency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Feedback-enabled SELF-CORRECT outperforms scalar-only SELF-CORRECT and other baselines reported for the respective tasks; provides a mechanism for more targeted corrections vs. blind scalar reward optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Figure 6 and related experiments show multiple corrections produce larger gains when feedback is available; attribute-level feedback improved toxicity reduction over scalar-only feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8829.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8829.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-critiquing (related mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-critiquing models for assisting human evaluators (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced related work where models generate critiques or self-assessments to improve outputs or assist human evaluation; the paper cites Saunders et al. (2022) as an example of self-critiquing models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-critiquing models for assisting human evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (related work, not used experimentally in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>External prior work that trains models to produce critiques or self-assessments; cited as related but not directly used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-critiquing (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Referenced as an approach where models generate critiques to guide improvement or human evaluation; paper positions SELF-CORRECT as complementary but different (learns corrective text-to-text mapping via value-improving pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>related / not directly evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>mentioned as generating critiques or edits (supervised or learned) in prior literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mentioned as related prior work; no experimental comparison within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited alongside editing and critique-based approaches; SELF-CORRECT differs by training an expressive corrector via online pairing without requiring human-written edits.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-critiquing models for assisting human evaluators <em>(Rating: 2)</em></li>
                <li>Learning from self-sampled correct and partially-correct programs <em>(Rating: 2)</em></li>
                <li>Language model cascades <em>(Rating: 1)</em></li>
                <li>Learning to model editing processes <em>(Rating: 1)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8829",
    "paper_id": "paper-253244506",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "SELF-CORRECT",
            "name_full": "Self-Correction / Self-Corrector",
            "brief_description": "A decomposition of generation into a base generator that produces an initial hypothesis and a learned text-to-text corrector that iteratively refines that hypothesis using a training procedure that pairs lower-value generations with nearby higher-value corrections; the corrector can optionally condition on explicit feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (GPT-Neo 1.3B, GPT-2 variants, GPT-3 engines used as generators or feedback providers)",
            "model_description": "The framework is model-agnostic: experiments use GPT-Neo 1.3B as a corrector (and sometimes as a generator), GPT-2 (medium/large/XL) as generator/corrector for some tasks, and GPT-3 (davinci / text-davinci-002 / Instruct variants) as large generators or as feedback providers; model sizes explicitly mentioned include GPT-Neo 1.3B and GPT-2 Large/XL and GPT-3 (~175B, referenced).",
            "reflection_method_name": "Self-Correction (learned corrector / iterative refinement)",
            "reflection_method_description": "Train a separate corrector p_θ(y | y_curr, x, f(y_curr)) with an online self-corrective learning algorithm that (1) collects a pool of generated outputs from a base generator, (2) forms value-improving pairs (hypothesis, higher-valued correction) where 'value' is a scalar quality metric v(y), (3) samples pairs proportionally to improvement and proximity and updates the corrector via cross-entropy, and (4) explores by adding corrections sampled from the current corrector. At inference, decode y0 from generator p0, then apply the corrector iteratively y_{t+1} ∼ q(p_θ(· | y_t, x, f(y_t))) for T steps (T is task-dependent).",
            "task_name": "general / multiple tasks (see per-experiment entries)",
            "task_description": "Self-CORRECT is evaluated across tasks requiring iterative improvement: mathematical program synthesis (programs that must execute to correct answers), lexically-constrained generation (cover specified constraint words), and toxicity reduction (reduce toxic continuations).",
            "performance_with_reflection": "Across tasks, the learned corrector consistently improves generator outputs (see task-specific entries).",
            "performance_without_reflection": "Baseline generator performance varies by task and model (see task-specific entries); SELF-CORRECT improves upon these baselines.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Separate learned corrector module (text-to-text) trained with self-corrective learning using scalar value function v(y) (and optionally natural-language feedback f(y)); iterative decoding pipeline that applies the corrector multiple times to generator outputs.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative improvements reported on three diverse tasks: large increases in program-synthesis correctness, substantial increases in lexical-constraint coverage, and large reductions in measured toxicity; ablations show pairing/proportional sampling and exploration improve results.",
            "limitations_or_failure_cases": "Requires access to a value function v(y) during training (and optionally at inference for feedback). Training relies on the generator producing diverse partial solutions to form improvement pairs. The corrector's effectiveness can depend on number of correction iterations and quality of feedback; when using a feedback model with access to gold solutions there is risk of solution leakage. No catastrophic failure cases are reported, but in math more than 2–3 corrections may require extra guidance.",
            "comparison_to_other_methods": "Compared favorably to constrained decoding (NeuroLogic-A*), decoding-time control methods (PPLM, GeDi, DExperts), and RL-based methods for detoxification (PPO, Quark); SELF-CORRECT often attains better constraint coverage / lower toxicity while maintaining fluency and being computationally efficient.",
            "ablation_study_results": "Ablations show (1) sampling pairs proportional to improvement & proximity improves performance vs. uniform sampling; (2) exploration (adding corrections sampled from the current corrector to the datapool) improves performance; (3) multiple corrections increase benefit (esp. with feedback).",
            "uuid": "e8829.0",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "SELF-CORRECT (Math)",
            "name_full": "Self-Correction applied to Mathematical Program Synthesis",
            "brief_description": "A corrector (GPT-Neo 1.3B) is trained to improve program generations (Python programs converted from word problems) produced by a base generator (GPT-Neo 1.3B or few-shot/prompted GPT-3), using correctness (execution equals ground-truth answer) as the scalar value function.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo 1.3B (corrector); generators: GPT-Neo 1.3B fine-tuned and GPT-3 (davinci/text-davinci-002 / Instruct)",
            "model_description": "GPT-Neo 1.3B used as pretrained code-and-language model for both generator and as trained corrector; GPT-3 engines (≈175B) used as large generators or as feedback-providing LM in some experiments.",
            "reflection_method_name": "Self-Correction corrector (1-step or multiple-step iterative corrections)",
            "reflection_method_description": "Train corrector to map an incorrect or partially-correct program y to a higher-valued program y' (value = binary correctness after execution). Inference: decode y0 from generator (greedy), then apply corrector for k corrections (k=1 in main experiments; also experiments with up to 5 when using feedback). An 'oracle' variant only applies correction if draft is incorrect.",
            "task_name": "Mathematical program synthesis (MultiArith, Multitask, GSM8k)",
            "task_description": "Given a math word problem, generate a Python program that executes to the correct numeric answer. Datasets: MultiArith, AddSub, SingleOp, SVAMP, GSM8k.",
            "performance_with_reflection": "MultiArith: after correction 98–99% correctness (a ≈38 point improvement over generator). Multitask: large improvement (exact table values in paper); GSM8k: generator 8.57% → SELF-CORRECT 21% (always correct) and 24% (only correct incorrect solutions). For GPT-3 Instruct as generator: Multitask 84.90% → 90.90% after adding corrector; further training with GPT-3 generator yields Multitask 92.75%. GSM8k for GPT-3 Instruct: 36.80% → 45.00% with corrector; trained with GPT-3 gives 45.92%.",
            "performance_without_reflection": "MultiArith/Multitask/GSM baselines vary by generator: one explicit baseline reported is generator correctness 8.57% on GSM8k (GPT-Neo baseline) and GPT-3 Instruct 84.90% / 36.80% on Multitask/GSM respectively (prior to correction).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "A separate corrector model trained via self-corrective learning using the execution-correctness scalar reward; pairing selects nearby higher-valued program corrections for supervised learning; inference applies the corrector to generator outputs (greedy decoding), optionally multiple times.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative gains: very high MultiArith accuracy (98–99%) after correction; +~38 percentage points over generator; GSM8k improvement from 8.57% → 21%/24%; corrector also transfers to improve larger generators (GPT-3) as shown by swap-in experiments.",
            "limitations_or_failure_cases": "Correctness requires executable programs and an automated executor to compute v(y). The corrector sometimes needs multiple corrections and additional guidance for complex problems; when using feedback provided by a feedback LM that has access to gold solutions there is risk of leakage. Performance on very challenging datasets (GSM8k) remains modest in absolute terms (e.g., 21–24% for GPT-Neo-based correctors).",
            "comparison_to_other_methods": "Outperforms baselines based on similarly sized models reported in prior work (e.g., tuned GPT-Neo 2.7B variants), and shows transferability to larger generators; compared qualitatively to methods that sample self-sampled correct programs (Ni et al., 2022) and to RL-based approaches in other tasks.",
            "ablation_study_results": "Exploration (adding corrections sampled from the corrector to the datapool) improves program-synthesis performance; pairing and proportional sampling for pair selection also improve results versus uniform pairing (ablation summarized in paper tables).",
            "uuid": "e8829.1",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "SELF-CORRECT (Lexical)",
            "name_full": "Self-Correction applied to Lexically-Constrained Generation",
            "brief_description": "Train a corrector to improve constraint coverage (percentage of required constraint words included) for sentence generation tasks; base generator is GPT-2 (medium/large) fine-tuned for the task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (medium for E2E; large for CommonGen) as generator and corrector",
            "model_description": "GPT-2 variants fine-tuned per task serve as base generators; a separate GPT-2 corrector is trained to maximize coverage metric.",
            "reflection_method_name": "Self-Correction corrector (iterative correction up to 3 steps)",
            "reflection_method_description": "Train corrector with value function v(y) = coverage (percentage of constraints satisfied). At inference, run generator decoding (beam search) then apply up to 3 correction iterations (beam search) stopping early if all constraints are met.",
            "task_name": "Lexically-constrained generation (CommonGen, E2E)",
            "task_description": "Generate fluent sentences that include all provided constraint words (CommonGen) or realize structured inputs as natural language (E2E); metrics: constraint coverage, BLEU/CIDER/human fluency.",
            "performance_with_reflection": "CommonGen coverage: GPT-2 baseline 91.50% → SELF-CORRECT 98.77% coverage. Other fluency metrics (BLEU/CIDER) are maintained or slightly improved; on E2E SELF-CORRECT outperforms NeuroLogic-A* in overall performance while using standard beam search.",
            "performance_without_reflection": "CommonGen coverage 91.50% with GPT-2 baseline; E2E generator baseline reported in paper (GPT-2) is lower than SELF-CORRECT on constraint coverage and comparable or worse on fluency measures.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Corrector is trained with coverage as scalar reward and uses lexically-oriented natural-language feedback (during feedback experiments) that lists missing constraints; inference runs generator then iteratively applies corrector to add missing constraints.",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Substantial numerical improvement in coverage (e.g., CommonGen 91.50% → 98.77%) while maintaining BLEU/CIDER and human-judged fluency; SELF-CORRECT with NeuroLogic combined attains near best coverage with lower runtime.",
            "limitations_or_failure_cases": "No specific failure cases reported, but the approach depends on being able to measure coverage reliably and on the generator producing partial solutions to be improved; runtime increases with more correction iterations (but still competitive).",
            "comparison_to_other_methods": "Compared against constrained decoding algorithms (NeuroLogic, NeuroLogic-A*); SELF-CORRECT paired with NeuroLogic or alone matches or outperforms NeuroLogic-A* while being faster; also compared to prefix-tuning baselines.",
            "ablation_study_results": "Not reported as a separate ablation in detail for lexical task beyond the general ablations: multiple corrections helped, and using feedback listing missing constraints improved performance.",
            "uuid": "e8829.2",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "SELF-CORRECT (Toxicity)",
            "name_full": "Self-Correction applied to Toxicity Reduction / Detoxification",
            "brief_description": "Train a corrector to reduce toxicity of generated continuations using the Perspective API toxicity score as the scalar value; the corrector is a finetuned GPT-2 Large that iteratively rewrites generator outputs to be less toxic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 Large (generator and fine-tuned corrector); evaluations also apply corrector to larger generators (GPT-2 XL, GPT-3)",
            "model_description": "Off-the-shelf GPT-2 Large used as the base generator; separate GPT-2 Large instance finetuned as corrector. Perspective API used to compute continuous toxicity scores v(y) ∈ [0,1].",
            "reflection_method_name": "Self-Correction corrector (iterative detoxification up to 3 corrections)",
            "reflection_method_description": "Corrector is trained to map an existing sampled continuation to a lower-toxicity continuation; training pairs are value-improving pairs chosen from a datapool; inference: sample 25 continuations with nucleus sampling from generator, then apply up to 3 correction iterations (corrector samples) per continuation.",
            "task_name": "Toxicity reduction on REALTOXICITYPROMPTS",
            "task_description": "Given prompts designed to elicit toxic completions, generate continuations that avoid offensive content while preserving fluency and diversity. Metrics: Perspective API max toxicity (average maximum toxicity over 25 samples) and empirical toxicity probability (probability ≥1 of 25 samples being toxic), fluency/diversity measures.",
            "performance_with_reflection": "Reported example metrics (Appendix): GPT-2 Large baseline average max toxicity 0.527 → SELF-CORRECT 0.171 → SELF-CORRECT + FEEDBACK 0.156 (lower is better). SELF-CORRECT also substantially reduces empirical toxicity probability and outperforms baselines on fluency/diversity tradeoffs.",
            "performance_without_reflection": "GPT-2 Large baseline average max toxicity 0.527 (from Appendix table), other baselines (PPLM, GeDi, DExpert, PPO, Quark) have higher toxicity or degrade fluency relative to SELF-CORRECT as reported.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Separate learned corrector conditioned on current hypothesis and optionally on natural-language feedback derived from Perspective API attribute scores; iterative decoding applies corrector multiple times (up to 3).",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Quantitative reduction in toxicity scores (0.527 → 0.171), lower probability of toxic samples, and maintained or improved fluency/diversity compared to several decoding-time and RL baselines; multiple corrections further reduce toxicity (Figure 4).",
            "limitations_or_failure_cases": "Depends on the reliability and availability of an external toxicity scorer (Perspective API) for v(y) and for attribute feedback; some decoding-time methods may be cheaper but yield worse fluency; when swapping in much larger generators, corrector still reduces toxicity but benefits may vary by generator quality.",
            "comparison_to_other_methods": "Directly compared to PPLM, GeDi, DExperts (inference-time control) and PPO, Quark (RL-based) and outperforms them in the reported metrics while maintaining fluency/diversity.",
            "ablation_study_results": "Multiple-correction experiments show degradation improves monotonically with additional corrections up to the tested limits; using attribute-level natural-language feedback (e.g., 'decrease profanity') yields further improvements over scalar-only training.",
            "uuid": "e8829.3",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Modular Swap-in (Large Generators)",
            "name_full": "Correcting larger generators by swapping in generator at test-time or training-time",
            "brief_description": "Demonstrates that a corrector trained with outputs from a smaller generator can be applied to improve outputs from a much larger generator (e.g., GPT-3), and that training with a large generator further improves performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Correctors: GPT-Neo 1.3B (for math) or GPT-2 Large (for toxicity); Generators swapped in include GPT-2 XL and GPT-3 (davinci / Instruct variants)",
            "model_description": "Corrector models are typically much smaller than the swapped-in generators (e.g., 1.3B corrector with ~175B GPT-3 generator); demonstrates transfer of correction behavior across generator models.",
            "reflection_method_name": "Self-Correction with modular generator-corrector swapping",
            "reflection_method_description": "Train a corrector with outputs from one generator (often smaller), then at inference decode initial hypothesis y0 from a larger generator and apply the trained corrector iteratively; alternative is to initialize datapool with large-generator outputs and train corrector on those.",
            "task_name": "Program synthesis and detoxification (transfer experiments)",
            "task_description": "Same tasks as above (math and toxicity) but testing whether a corrector trained on one generator can improve outputs from a different/larger generator at inference.",
            "performance_with_reflection": "Example: GPT-3 Instruct on Multitask improves from 84.90% → 90.90% when corrected by the trained corrector; GSM8k improved 36.80% → 45.00%; toxicity of large generators (GPT-2 XL, GPT-3) substantially reduced when refined by a corrector trained on smaller models (see Table 4).",
            "performance_without_reflection": "Generators alone: GPT-3 Instruct Multitask 84.90% / GSM8k 36.80% (prior to correction); toxicity levels for GPT-2 XL/GPT-3 higher (baseline) compared to corrected outputs.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Same learned corrector mechanism; the novelty is modularity — corrector and generator are decoupled and can be swapped at inference or training.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Empirical improvements when applying small-model corrector to larger-model outputs, and further gains when training corrector with the large generator's outputs (quantified in Table 4).",
            "limitations_or_failure_cases": "While transfer works in the experiments shown, no formal guarantee that a corrector trained on one generator generalizes to all larger generators; effectiveness may depend on similarity of output distributions and on the corrector's capacity.",
            "comparison_to_other_methods": "Argues an advantage over methods that must fine-tune large generators (which may be infeasible for API-access models), since only the corrector needs training.",
            "ablation_study_results": "Paper compares two modes: (1) train with small generator then swap in large generator at test time, and (2) include large-generator outputs during training; both improve performance, with further gains when training used the large generator.",
            "uuid": "e8829.4",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "SELF-CORRECT + Feedback",
            "name_full": "Self-Correction augmented with explicit natural-language feedback",
            "brief_description": "Extend the corrector to condition on explicit feedback f(y) (a natural-language instruction or attribute), which can be derived automatically (e.g., Perspective API attributes, missing constraint phrases) or generated by a feedback LLM; using feedback improves correction quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Corrector models (GPT-Neo 1.3B for math experiments; GPT-2 Large for toxicity) conditioned on feedback; feedback often produced by GPT-3 (text-davinci-002) or Perspective API attribute selection.",
            "model_description": "Corrector is augmented to receive textual feedback strings f(y) computed at training and optionally at inference (e.g., 'decrease profanity' or enumerations of missing constraint words); feedback for math experiments was provided by a prompted GPT-3 feedback model (with access to gold solution for training-time signals).",
            "reflection_method_name": "Self-Correction conditioned on natural-language feedback",
            "reflection_method_description": "During training and inference the corrector conditions on feedback f(y) representing the most salient attribute to change (selected from Perspective API attributes for toxicity, enumerated missing constraints for lexical tasks, or LLM-produced program feedback for math); training sequences include feedback tokens and loss includes tokens after feedback markers.",
            "task_name": "Toxicity reduction, Lexical constraints, Mathematical program synthesis (feedback experiments)",
            "task_description": "Same core tasks as earlier, but here the corrector explicitly receives a textual feedback signal describing what to change.",
            "performance_with_reflection": "Feedback improves performance in all three studied tasks: for toxicity average max toxicity improved from SELF-CORRECT 0.171 → SELF-CORRECT + FEEDBACK 0.156 (Appendix table); lexical coverage improved (e.g., SELF-CORRECT to SELF-CORRECT + feedback increases coverage on CommonGen from 94.58% to 95.88% in some reported combinations), and math saw higher gains when feedback was available (specific numeric gains provided in Table 5).",
            "performance_without_reflection": "SELF-CORRECT without feedback is already better than generator; adding feedback provides incremental improvements over scalar-only training.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "The corrector conditions on an explicit natural-language string f(y) derived from attribute comparisons (toxicity) or enumerations of missing constraints (lexical), or generated by a feedback LM for math; the training objective remains cross-entropy on value-improving pairs but with feedback included as conditioning context.",
            "number_of_iterations": 5,
            "evidence_for_improvement": "Quantitative improvements reported across tasks when feedback is used vs scalar-only training; ablations show more corrections are especially beneficial when feedback is present (e.g., math Multitask benefits from &gt;1 corrections when feedback provided).",
            "limitations_or_failure_cases": "Quality depends on feedback accuracy; for math the feedback LM had access to gold solutions during feedback generation in experiments (improving feedback quality but risking solution leakage), so those math feedback results are exploratory. Using external APIs at inference (e.g., Perspective API) increases runtime and external dependency.",
            "comparison_to_other_methods": "Feedback-enabled SELF-CORRECT outperforms scalar-only SELF-CORRECT and other baselines reported for the respective tasks; provides a mechanism for more targeted corrections vs. blind scalar reward optimization.",
            "ablation_study_results": "Figure 6 and related experiments show multiple corrections produce larger gains when feedback is available; attribute-level feedback improved toxicity reduction over scalar-only feedback.",
            "uuid": "e8829.5",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Self-critiquing (related mention)",
            "name_full": "Self-critiquing models for assisting human evaluators (related work)",
            "brief_description": "Referenced related work where models generate critiques or self-assessments to improve outputs or assist human evaluation; the paper cites Saunders et al. (2022) as an example of self-critiquing models.",
            "citation_title": "Self-critiquing models for assisting human evaluators",
            "mention_or_use": "mention",
            "model_name": "N/A (related work, not used experimentally in this paper)",
            "model_description": "External prior work that trains models to produce critiques or self-assessments; cited as related but not directly used in experiments.",
            "reflection_method_name": "self-critiquing (mentioned)",
            "reflection_method_description": "Referenced as an approach where models generate critiques to guide improvement or human evaluation; paper positions SELF-CORRECT as complementary but different (learns corrective text-to-text mapping via value-improving pairs).",
            "task_name": "related / not directly evaluated",
            "task_description": "N/A",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "mentioned as generating critiques or edits (supervised or learned) in prior literature",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mentioned as related prior work; no experimental comparison within this paper.",
            "limitations_or_failure_cases": "Not detailed in this paper (only cited).",
            "comparison_to_other_methods": "Cited alongside editing and critique-based approaches; SELF-CORRECT differs by training an expressive corrector via online pairing without requiring human-written edits.",
            "ablation_study_results": null,
            "uuid": "e8829.6",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-critiquing models for assisting human evaluators",
            "rating": 2,
            "sanitized_title": "selfcritiquing_models_for_assisting_human_evaluators"
        },
        {
            "paper_title": "Learning from self-sampled correct and partially-correct programs",
            "rating": 2,
            "sanitized_title": "learning_from_selfsampled_correct_and_partiallycorrect_programs"
        },
        {
            "paper_title": "Language model cascades",
            "rating": 1,
            "sanitized_title": "language_model_cascades"
        },
        {
            "paper_title": "Learning to model editing processes",
            "rating": 1,
            "sanitized_title": "learning_to_model_editing_processes"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        }
    ],
    "cost": 0.0155744,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GENERATING SEQUENCES BY LEARNING TO [SELF-]CORRECT</p>
<p>Sean Welleck 
Allen Institute for Artificial Intelligence</p>
<p>Paul G. Allen School of Computer Science &amp; Engineering
University of Washington</p>
<p>Ximing Lu 
Allen Institute for Artificial Intelligence</p>
<p>Peter West 
Paul G. Allen School of Computer Science &amp; Engineering
University of Washington</p>
<p>Faeze Brahman 
Allen Institute for Artificial Intelligence</p>
<p>Tianxiao Shen 
Paul G. Allen School of Computer Science &amp; Engineering
University of Washington</p>
<p>Daniel Khashabi 
Center for Language and Speech Processing
Johns Hopkins University</p>
<p>Yejin Choi 
Allen Institute for Artificial Intelligence</p>
<p>Paul G. Allen School of Computer Science &amp; Engineering
University of Washington</p>
<p>GENERATING SEQUENCES BY LEARNING TO [SELF-]CORRECT
Preprint. Under review.
Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present SELF-CORRECTION, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that SELF-CORRECTION improves upon the base generator in three diverse generation tasksmathematical program synthesis, lexically-constrained generation, and toxicity control-even when the corrector is much smaller than the base generator. * First authors, contributed equally. †Second authors, contributed equally.</p>
<p>INTRODUCTION</p>
<p>The standard practice for natural language generation tasks is inherently single-pass: applying a decoding procedure to either a few-shot prompted language model or one tuned for a given task, then considering the generation as "finished" (e.g. ; Brown et al. (2020); ). Powerful generation models often meet most of the task requirements, yet miss a few (e.g., omitting a subset of keywords), or generate incorrect hypotheses that nevertheless provide useful structure (e.g., a correct problem solving strategy with a missing step). However, after generating even a slightly sub-optimal sequence, the single-pass paradigm requires models to "start from scratch", effectively discarding work already done. A more natural, intuitive approach is leveraging the generation as a useful starting point to refine into a higher quality output.</p>
<p>To formalize this intuition, we introduce Self-Correction for Sequence Generation. Figure 1 demonstrates its central principle: a generation model is re-framed as a base generator, which produces a reasonable initial hypothesis but does not need to solve the task in one pass, and a second module-the corrector-trained to make up the difference between the hypothesis and an optimal solution. Neither the generator nor the corrector must solve the full task in one pass, and the corrector can be applied multiple times to iteratively improve the output ( §3.6). We propose a simple, general procedure for training the corrector (Figure 2) by pairing generator outputs with carefully selected targets. The result is a system which self-corrects, producing outputs through multiple generation passes and breaking the task into steps that can be solved by dedicated and efficient sub-systems. Figure 1: SELF-CORRECTORs decompose generation into a base generator that proposes an initial hypothesis, and a corrector that iteratively improves its quality.</p>
<p>We find that Self-Correction is broadly applicable. Training a corrector model improves the base generator on 3 diverse tasks: mathematical program synthesis ( §3.1), lexically constrained generation ( §3.2), and toxicity reduction ( §3.3). The trained corrector model can even be applied to a larger generator with similar performance to training a new corrector ( §3.4), showing that the subtask of correction is transferable, even to stronger generators. Finally, we explore the prospect of introducing a third module to the Self-Correction system ( §3.5)-explicitly using natural language feedback to guide corrections-with promising results. Self-Correction offers an exciting opportunity to build on existing generation models and the sequences they generate, with efficient, effective, and transferable corrector networks.</p>
<p>SELF-CORRECTING SEQUENCE GENERATORS</p>
<p>A typical autoregressive text generator (e.g. GPT-3 (Brown et al., 2020)) maps an input prompt to a distribution over outputs using a single parameterized module (e.g. a large transformer), p 0 (y|x). We explore an alternative that decomposes into two modules, a base generator, and a corrector,
p(y|x) = y0 p 0 (y 0 |x) generator p θ (y|y 0 , x) corrector (1)
where the generator provides an initial hypothesis that is refined by the corrector. In practice, the corrector can be applied multiple times, p(y T |x) = y0 y1 · · · y T −1 p 0 (y 0 |x) t p θ (y t+1 |y t , x). Since a model of this form can both generate and correct its generations, we call it a Self-Corrector.</p>
<p>Self-correctors have several unique properties compared to typical generators. First, a self-corrector decouples generation and correction, allowing us to freely parameterize each module -for instance, by prompting a single language model or using two different language models. In this paper, we develop a framework to train a separate corrector model ( §2.1). We find that the resulting selfcorrector improves upon the generator alone ( §3), even when the corrector is much smaller ( §3.4).</p>
<p>Second, since the generator and the corrector are separated, we can keep the generator as a generalpurpose language model and train the corrector with different objectives for different task requirements. In §2.1, we propose a training algorithm for the corrector that is dedicated to improving generations, where the improvement can be in any aspect, measured by scalar values.</p>
<p>Third, the corrector can receive explicit feedback about intermediate generations to guide subsequent generations. Formally, p(y|x) = y0 p 0 (y 0 |x)p θ (y|y 0 , x, f (y 0 )), where f is the feedback. The feedback can be of many forms, e.g. a sentence, a compiler trace, etc. In contrast, a typical generator that generates in a single pass does not leverage feedback on its own generation. In this paper, we show that the corrector can learn to exploit explicit natural language feedback to achieve better performance ( §3.5). Next, we describe our training framework of the corrector.</p>
<p>LEARNING A CORRECTOR</p>
<p>Our goal is to have the generator generate an initial hypothesis, then improve the hypothesis with the corrector (Eq. 1). We train the corrector to improve the quality of a hypothesis, while staying as Figure 2: SELF-CORRECTIVE LEARNING iteratively trains a corrector by generating hypotheses and corrections, forming value-improving pairs, and selecting those with high similarity for learning.
Algorithm 1 Self-corrective learning input Generator p0, corrector p θ , prompts X, value v(·), feedback f (·)
Initialize datapool D by sampling from p0 Initialization: Eq. 2 for iteration ∈ {1, 2, . . .} do for x ∈ X do Sample hypotheses y from datapool D Generate corrections y ∼ p θ (·|y, x, f (y)) Add all (x, y , v(y ), f (y )) to the datapool D Exploration: Eq. 5 Form value-improving pairs P from D Pairing: Eq. 3 for step in 1, 2, . . . , M do Sample a batch of value-improving pairs from P using Eq. 4 Compute the loss and update θ using gradient descent Learning close as possible to the original hypothesis. Here, quality is measured with a scalar value function v(y) which we assume is accessible at training time (e.g. a classifier).</p>
<p>Since direct supervision on how to improve hypotheses is not available, we design a new algorithm to train the corrector, which we refer to as self-corrective learning. The algorithm collects a pool of generations, groups them and selects pairs of generation that increase in value and are nearby, then updates the corrector on these pairs. As training progresses, more generations are added to the pool using the current corrector. Algorithm 1 summarizes self-corrective learning, detailed below.</p>
<p>Initialization. Self-corrective learning begins with a generator p 0 (y 0 |x), a corrector p θ (y |y, x) , a set of training prompts X, and a value function v : Y → R. Optionally, we can use additional feedback f : Y → F and learn p θ (y |y, x, f (y)), where F is arbitrary.</p>
<p>The algorithm initializes a datapool of (input, output, value, feedback) examples by using the generator to generate multiple outputs for each input. Formally,
D x = {(x, y, v(y), f (y)) | for all y ∈ y 1:N ∼ q(p 0 (·|x))}, D = x∈X D x ,(2)
where y 1:N denotes N outputs generated with decoding algorithm q (e.g. temperature sampling). When available, (x, y, v(y), f (y)) examples from another source (e.g. a dataset) can also be added.</p>
<p>Pairing. Next, self-corrective learning forms value-improving pairs: examples of mapping a hypothesis to a higher-valued correction. We use the datapool D to form a set of (input, hypothesis, correction) pairs. A pair is formed when an output has a higher value than another 1 :
P x = {(x, y, y ) | v(y) &lt; v(y ) for all y, y ∈ D x × D x }, P = x∈X P x ,(3)
Learning. Next, self-corrective learning selects (input, hypothesis, correction) pairs to update the corrector with. We sample a (x, y, y ) pair proportional to its improvement in value as well as the proximity between the hypothesis y and the correction y :
P[(x, y, y )] ∝ exp α · (v(y ) − v(y)) improvement + β · s(y, y ) proximity /Z(y),(4)
where s(y, y ) is a similarity function and Z(y) normalizes over the available corrections for y in P x . Increasing the hyperparameter α ∈ R ≥0 puts more weight on targets that add more value, while increasing β ∈ R ≥0 retains more similar targets. We update the corrector using the cross-entropy loss L(θ) = − log p θ (y |y, x, f (y)) on batches sampled in this way.</p>
<p>Exploration.</p>
<p>During exploration, self-corrective learning adds new generations to the datapool by generating from the current corrector:
D x = {(x, y , v(y ), f (y )) | for all y ∈ y 1:N ∼ q(p θ (·|y, x, f (y))}, D = x∈X D x (5)
and updating the datapool D ← D ∪D . The hypotheses y to correct can come from any source, e.g. newly sampled from the base generator, or from the datapool; we use the latter in our experiments.</p>
<p>Inference. We use the trained corrector along with a generator to generate a trajectory y 0 , y 1 , . . . , y T , and consider y T the final output. Since marginalizing over the intermediate generations in Eq. 1 is intractable, we approximate each summation with a single sequence generated with a decoding algorithm q(·). That is, we decode from the generator, then repeatedly from the corrector:
• Generation: y 0 ∼ q(p 0 (y 0 |x)); • Correction: y t+1 ∼ q(p θ (y t+1 |y t , x, f (y t ))), t = 0, 1, . . . , T − 1.
The stopping time T is either fixed, or when a target value is obtained (if v(y) is available).</p>
<p>EXPERIMENTS</p>
<p>We evaluate SELF-CORRECTION on a diversity of tasks: mathematical program synthesis, in which generations are strictly correct or incorrect, and generators typically have low performance; lexically-constrained generation, which allows for partial credit, and generators usually give partially-correct solutions (e.g. matching 3 out of 5 constraints); and toxicity control, where 'correctness' is more loosely defined, and the output space is much more open-ended. Our experiments are organized to study three settings:</p>
<ol>
<li>
<p>Using self-correctors to improve upon generators ( §3.1,3.2,3.3).</p>
</li>
<li>
<p>Correcting generators that are much larger than the corrector ( §3.4). 3. Leveraging explicit feedback during training and inference ( §3.5).</p>
</li>
</ol>
<p>Next, we describe the self-correction setup and baselines for each task, along with their results. 2</p>
<p>MATHEMATICAL PROGRAM SYNTHESIS</p>
<p>First, we consider mathematical program synthesis (Austin et al., 2021;Mishra et al., 2022). Given a natural language problem specification x, the task is to generate a program y that upon execution returns the correct answer to x. The task is challenging as it draws on language understanding, multiple-step mathematical problem solving (e.g. identifying a solution strategy, decomposing a problem), and leveraging symbolic tools (e.g. built-in operations, variables). Furthermore, the task demands a high level of precision, e.g. a single misplaced operation makes the program incorrect.</p>
<p>Experimental setup. As the corrector we use GPT-Neo 1.3B , an open-source autoregressive language model. GPT-Neo is pre-trained on language and code , and hence is widely used for code-related generation (e.g. ; Ni et al. (2022); Mishra et al. (2022)). We consider two settings for the initial generator: (1) a separate fine-tuned Corrector:</p>
<p>a=2<em>100 b=3</em>50 c=500-a-b #fix answer=c print(answer) Figure 3: Grade-school-math (GSM) self-corrections. On the left, the corrector fixes the units (from minutes to hours) in the generator's solution. On the right, the corrector revises the logic so that the program computes the total savings instead of the spent on tickets. We add #fix here to indicate the change. See Figure 7 and Figure 8 for additional examples.</p>
<p>instance of GPT-Neo 1.3B, and (2) few-shot prompted GPT-3 (Brown et al., 2020). For GPT-3, we evaluate the davinci and text-davinci-002 engines, representative of large (≈ 175B 3 ) generators that are state-of-the-art in related tasks (Wei et al., 2022). See the Appendix for additional details.</p>
<p>Self-correction setup. As the value function we use correctness, which is 1 when the program y executes and outputs the ground-truth answer and 0 otherwise. Our main experiments do not use explicit feedback, i.e. f (y) = ∅. At inference time, we study two settings for the corrector: (1) applying k corrections and selecting the final generation, (2) an oracle setting that only corrects a draft if the draft is incorrect. We use greedy decoding for the generator and corrector, and k = 1.</p>
<p>Datasets. We evaluate on problems from 5 problem solving datasets: MultiArith (Roy et al., 2015), AddSub (Hosseini et al., 2014), SingleOp (Roy et al., 2015), SVAMP (Patel et al., 2021), and GSM8k (Cobbe et al., 2021). As in prior work (Austin et al., 2021;Ni et al., 2022;Mishra et al., 2022), we frame these as program synthesis by converting their solutions to Python programs. 4 We separate our experiments into three increasingly difficult settings:</p>
<ol>
<li>MultiArith, using problems from the MultiArith arithmetic word problem dataset. 2. Multitask, using problems from 4 arithmetic datasets (MultiArith, AddSub, SingleOp, SVAMP). 3. GSM, using problems from the challenging GSM8k dataset.</li>
</ol>
<p>For the MultiArith and Multitask settings, we make train/valid/test splits using 60/20/20% of the respective datasets. Similar to Ni et al. (2022), for the GSM setting we use the official GSM8k test split, and create a validation split using 20% of the training set. Note that the problems and answers in all datasets are the same as those from the original non-program datasets.  Baselines. We compare SELF-CORRECT with its baseline generator (GPT-Neo 1.3B) in all three settings. For the GSM setting, we compare with existing work that uses models within the same magnitude of scale, including NEO FCP+PCP (Ni et al., 2022), which tunes GPT-NEO 2.7B with additional self-sampled programs, and their fine-tuned GPT-NEO 2.7B baseline. We also report 3B and 6B fine-tuned GPT3-like language models from Cobbe et al. (2021), which were trained on the non-program version of GSM8k. We evaluate larger models later in ( §3.4).</p>
<p>Results. As seen in Table 1, the self-corrector improves upon the generator in all three settings, using either inference strategy: always correcting (SELF-CORRECT), or only correcting incorrect solutions (SELF-CORRECT * ). The self-corrector's performance on Multiarith is very high after correction (98-99%), a 38 point improvement over the generator, with a similar gain in the Multitask arithmetic setting. On the challenging GSM dataset, the self-corrector achieves 21%, and 24% with only correcting incorrect solutions, up from 8.57% for the generator. Notably, this is higher than previous work based on the larger 2.7B GPT-Neo, or larger models tuned on the language version of GSM.</p>
<p>The results show that self-corrective learning can improve task performance via training a corrector. Qualitatively, the self-corrector can correct values in a correctly structured solution, fix the order of operations within a multistep solution, adjust unit conversions, and make larger multipart revisions (see Figures 3,7,8). Notably, these are learned automatically through self-corrective learning.</p>
<p>LEXICALLY CONSTRAINED GENERATION</p>
<p>Next, we consider lexically constrained generation. Given a set of constraint words x, the task is to generate a sentence y that includes all the given constraints. Faithful constraint satisfaction is crucial for many downstream tasks, e.g., those that require converting information to text (McKeown, 1985).</p>
<p>Datasets and Metrics. We experiment on COMMONGEN (Lin et al., 2020) and E2E (Novikova et al., 2017). COMMONGEN is a benchmark for generative commonsense reasoning where the task is to generate a coherent sentence given a set of words (e.g., dog, catch). E2E involves converting structured inputs into natural language. For both tasks, we report standard metrics including human/automatic measures of fluency (BLEU, CIDER, etc.) as well as constraint coverage. We collect human measures of fluency on Amazon Mechanical Turk; see the Appendix for details.</p>
<p>Setup. We parameterize the base generator with GPT-2 Radford et al. (2019) (large-size for COM-MONGEN and medium-size for E2E). We fine-tuned the generator for each task. As the value function for self-corrective learning we use coverage, i.e. the percentage of constraints that are present in the output. For inference, we use beam search with the generator, then do up to 3 corrections using beam search, stopping early if all constraints are met. See the Appendix for additional details.</p>
<p>Results. Table 2 shows the evaluation results. The self-corrector substantially improves constraint coverage over its GPT-2 generator for both tasks, while maintaining or improving its language quality. On the COMMONGEN benchmark, the self-corrector paired with the NeuroLogic constrained decoding algorithm  achieves the best results, outperforming the more sophisticated NeuroLogic-A<em> decoding algorithm, while being an order of magnitude faster. Notably, on E2E, self-correction outperforms Neurologic-A</em> decoding, despite only using standard beam search. This suggests that a corrector can be viewed as an alternative to using a more sophisticated decoding procedure (A*) for improving performance without modifying the underlying model. See Figure 9 for qualitative examples.  Table 3: Toxicity reduction. GPT-2 is the base generator. Figure 4: Applying multiple corrections reduces toxicity.</p>
<p>TOXICITY REDUCTION</p>
<p>Next, we consider the task of toxicity reduction (Gehman et al., 2020;Liu et al., 2021). Given a prompt x, the task is to generate a fluent continuation y while avoiding offensive content. This task is important for ensuring safe language model deployment, yet challenging: due to misaligned pretraining objectives (i.e. modeling internet text vs. non-toxic text), language models are susceptible to generating toxic completions, even when prompted with seemingly innocuous text (Gehman et al., 2020). Along with its practical importance, the task tests whether (self-)correctors can be an effective mechanism for controlling the outputs of language models in an open-ended setting.</p>
<p>Datasets and Metrics. We use the REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) which contains 100k prompts designed to elicit toxic generations. Following the experimental setup of Liu et al. (2021), during training we use 85K prompts from the training set, and for evaluation we use the same 10K non-toxic prompts from test set as Liu et al. (2021). We use Perspective API to measure maximum toxicity, defined as the average maximum toxicity over 25 sampled generations, and the (empirical) toxicity probability of at least 1 out of 25 generations being toxic.</p>
<p>Baselines. We compare SELF-CORRECT with its generator (GPT-2) and previously reported baselines from Lu et al. (2022a), including PPLM (Dathathri et al., 2020), GeDi (Krause et al., 2021), DExpert , DAPT (Gururangan et al., 2020), PPO (Lu et al., 2022a), and Quark (Lu et al., 2022a). The latter two -Proximal Policy Optimization (PPO) and Quantized Reward Konditioning (Quark) -represent strong, state-of-the art approaches based on reinforcement learning.</p>
<p>Setup. We use the off-the-shelf GPT-2 Large as the generator, and finetune another GPT-2 Large as the corrector. During inference, we use nucleus sampling with p = 0.9 to generate 25 samples for all baselines. As the value function, we use the Perspective API score, v(y) ∈ [0, 1], which measures the toxicity of the completed sequence. We do up to three corrections with the corrector model.</p>
<p>Results</p>
<p>. Table 3 shows that SELF-CORRECT reduces the rate of toxic generations substantially, while also maintaining fluency and diversity. SELF-CORRECT outperforms all baselines. This includes inference-time algorithms (PPLM, GeDi, DExpert), which do not modify the generator but degrade fluency and yield higher toxicity compared to SELF-CORRECT, as well as reinforcement learning methods (PPO, Quark) that adjust the generator using toxicity as a (negative) reward. The results show that SELF-CORRECT is effective for detoxification, without having to modify the underlying generator. We study implications of this latter property further in the next section.</p>
<p>CHANGING MODULES -CORRECTING GPT-3</p>
<p>Next, we show that a self-corrector can improve the outputs of a generator that is much larger than the corrector. We consider two cases: (1) training with a small generator, then swapping in the larger generator at test time;</p>
<p>(2) training with the larger generator, i.e. using the large generator to initialize the datapool for self-corrective learning, then using the large generator at test time.</p>
<p>Toxicity. We evaluate case (1) for reducing the toxicity of a large generator (GPT-2 XL, GPT-3). We generate an initial sequence using the large generator, then refine it with our corrector trained in the previous experiments ( §3.3). Table 4 shows that the resulting self-corrector (large generator  Table 4: Modularity (program synthesis and detoxification). Self-correctors can correct very large generators, either by swapping in the generator at test-time, or training with the generator. For math synthesis, the corrector is GPT-Neo 1.3B, and here we only correct incorrect outputs. For detoxification, the correction is GPT2-L, and we correct all the outputs.</p>
<ul>
<li>corrector) has substantially reduced toxicity compared to the large generator. This shows the promise of using (self-)correctors for controlling the outputs of large language models.</li>
</ul>
<p>Math program synthesis. Table 4 shows results for math. Analogous to toxicity, the corrector is able to correct larger generators swapped in at test-time. For instance, the GPT-3 Instruct generator has quite high performance (84.90 Multitask, 36.80 GSM), which improves to 90.90 and 45.00, respectively, by adding in a corrector. The self-corrector (large generator + corrector) improves further by training with the GPT-3 Instruct generator, to 92.75 and 45.92, respectively.</p>
<p>LEVERAGING EXPLICIT FEEDBACK</p>
<p>Next, we demonstrate SELF-CORRECT's capacity to incorporate explicit natural language feedback. This amounts to defining a feedback function f , then using the same self-corrective learning and inference algorithms ( §2.1) as in our preceding experiments (in those experiments, f returned ∅).</p>
<p>We show that correctors learn to use the feedback, as evidenced by higher performance.</p>
<p>Toxicity. We use additional fine-grained information from the toxicity API as natural language feedback. Specifically, besides the overall toxicity score, Perspective API also provides scores for fine-grained attributes of toxicity (e.g. identity attack, profanity, flirtation, etc.). At training time, we compare the attribute scores from a hypothesis and its selected correction, and use the attribute with the largest decrease as natural language feedback (e.g. "decrease toxicity in profanity"). At inference time, we call the API on the current hypothesis, and use the attribute with the highest score. Here we use the API at inference time, which is not required in our previous experiments.</p>
<p>Lexical constraints. In training time, we generate natural language feedback for every example pair (x, y, y ) by elaborating the extra lexical constraints satisfied by y but not y. e.g. "adding constraint word: read". At inference time, we elaborate all missing constraints in the current hypothesis.</p>
<p>Math program synthesis. Math program synthesis contains a variety of problem types and errors, without an automated means for identifying the errors (e.g. an API). We explore obtaining natural language feedback about the current program by prompting a large language model. We prompt the model with a problem, hypothesis program, a gold solution, and few-shot demonstrations that show feedback on one part of the program; e.g. In the initial guess, 3 should be subtracted. When the program is correct, the feedback is Correct. At inference time, we also use feedback from the language model. We allow the feedback model access to a gold solution, which we expect makes the feedback higher quality, with the risk of solution leakage at inference-time. Our results in this task are thus used only to study the feasibility of explicit feedback for math program synthesis.</p>
<p>Setup. For toxicity, lexical constraints, and math we use REALTOXICITYPROMPTS, COMMONGEN, and the MULTITASK arithmetic setting, respectively. We follow the setup of each task's previous experiments ( §3.3, §3.2, §3.1), except for math we use 5 correction iterations (previously 1). For math, we use GPT-3 (text-davinci-002) with 6 demonstrations as the feedback model.  Generator (GPT-Neo):</p>
<p>answer=(6.0*8.0) print(answer)</p>
<p>Feedback (GPT-3):</p>
<h1>In the initial guess, # 2 tickets are not included.</h1>
<p>Corrector (GPT-Neo):</p>
<p>answer=(6.0*(8.0+2.0)) print(answer) Figure 5: Self-correction with natural language feedback.</p>
<p>Results. Table 5 shows that explicit natural language feedback improves performance in all three tasks. For toxicity, this means that providing fine-grained attributes (e.g. identity attack, profanity, etc.) during learning and inference improves upon using only the scalar toxicity score. Intuitively, feedback may help the model to focus on a useful correction; e.g., see Figure 5.</p>
<p>ADDITIONAL ABLATIONS AND ANALYSIS</p>
<p>Effect of multiple corrections. Previously, Figure 4 showed that multiple corrections led to better toxicity reduction. On math (Multitask setting), Figure 6 shows that performance improves with more than one correction, and that multiple corrections are more beneficial with feedback. Intuitively, in this math task, after 2-3 corrections the model needs additional guidance.</p>
<p>Effect of pairing and proportional sampling. Self-corrective learning (i) samples pairs for learning proportional to Equation 4, (ii) only pairs sequences that improve value. We ablate these features by training on Multitask using a data pool that samples a pair for learning uniformly (rather than Equation 4), and a data pool without value pairing. Table 6 shows that both improve performance.</p>
<p>Effect of exploration.</p>
<p>To ablate the effect of exploration, we train a baseline only on correction pairs induced from the base generator. Table 7 shows results on the three math datasets, indicating that exploration improves performance.</p>
<p>RELATED WORK</p>
<p>Self-correction relates to recent works on editing text, including modeling Wikipedia edits (Reid &amp; Neubig, 2022;Faltings et al., 2021;Schick et al., 2022), which relies on supervised edits, unsupervised methods (Miao et al., 2019;) that perturb sequences with simple operations (e.g. insertion, deletion), editing with models trained on human-written critiques (Saunders et al., 2022), or iteratively updating continuous variables Li et al., 2022;Qin et al., 2022).    Table 7: Effect of exploration on program synthesis.</p>
<p>In contrast to these, self-correction learns an expressive text-to-text corrector that is trained online to improve a quality measure, without requiring a supervised dataset of edits or critiques. Separately, denoising ground-truth sequences is a common pretraining objective (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020), while self-correction 'denoises' generations to improve a scalar quality measure. Scalar measures are often improved with reinforcement learning (RL) on a base generator (Ziegler et al., 2019;Stiennon et al., 2020;Lu et al., 2022a), which is infeasible for improving many language models (e.g. those accessed through an API), and uses only scalar feedback. Moreover, self-correction learns the difference between a generation and solution, and is complementary to RL-tuned generators, which can be used within a self-corrector. Finally, self-correction decomposes generation into multiple steps, which relates to methods that generate rationales before a response (Wei et al., 2022;Dohan et al., 2022). Self-correction also produces intermediate steps, but each step is of the same form as the output, allowing for re-using previous generations.</p>
<p>CONCLUSION</p>
<p>We introduced self-correctors, a class of models that decompose generation into initial generation and correction steps. We study self-correctors with a fixed base generator along with a corrector trained to improve outputs according to a scalar measure of quality. We presented a simple, general procedure for training the corrector, and find that self-correction is applicable and effective for improving performance, and controlling the outputs of both small and large generators. Moreover, we found that self-correction along with our learning framework provides a promising mechanism for using natural language feedback to improve generation. These findings, along with exploring alternative self-correctors, open up many avenues that we leave for future work.</p>
<p>APPENDIX A ADDITIONAL EXPERIMENTAL DETAILS</p>
<p>A.1 MATHEMATICAL PROGRAM SYNTHESIS</p>
<p>We fine-tune a separate instance of GPT-Neo 1.3B as an initial generator, using the Huggingface library with default hyperparameters, except for evaluation steps, which we set to a small number to ensure a strong checkpoint is selected for each dataset. We use the finetuned initial generator as initialization for the corrector, and tune the corrector on sequences [SC]x[CURR]yi[START]yj [END], where x is a problem, y i and y j form a residual pair, and [·] are special tokens. The loss is on tokens after [START].</p>
<p>Feedback. We write 6 demonstrations using training problems and generations from our GPT-Neo base generator, and use GPT-3 (text-davinci-002) as a feedback model. We use the same training procedure and hyperparameters, except that the sequences now include feedback, [SC]x</p>
<p>[CURR]yi[FEEDBACK]F(x,yi)[START]yj[END]</p>
<p>, where x is a problem, y i and y j form a residual pair, and F (x, y i ) is feedback. We include loss on tokens after [FEEDBACK].</p>
<p>A.2 LEXICALLY-CONSTRAINED GENERATION</p>
<p>Hyper-parameters. Table 8 and Table 9 show hyperparameters for CommonGen and E2E.</p>
<p>Human Evaluation. We evaluate fluency of generations in E2E task using human annotators on Amazon Mechanical Turk (AMT). We randomly sampled 100 instances, along with generations of different baselines and self-corrections. For each instance, we ask 3 annotators to evaluate the fluency of generations on a 3-point Likert scale. We aggregate annotations from 3 annotators using majority vote. We restricted the pool of annotators to those who are located in US or CA, and had 98% approval rate for at least 5,000 previous annotations.</p>
<p>Hyperparameter Assignment</p>
<p>Predictor GPT-2Large # steps 6000 batch size 128 optimizer Adam learning rate 1.e − 5 decoding alg.</p>
<p>beam search (k=5)     C QUALITATIVE EXAMPLES</p>
<p>Figure 6 :
6Math: multiple corrections.</p>
<p>Table 2 :
2Lexically-constrained generation. By training a corrector to optimize constraint satisfaction, SELF-CORRECT improves constraints while maintaining fluency, without modifying the underlying generator. Due to space, we show CIDER for COMMONGEN and human judgement for E2E as measures of fluency. Other metrics show similar trends and can be found in the Appendix.</p>
<p>Table 5 :
5Explicit natural language feedback. Correct * means only correcting incorrect outputs.Problem: 
Melanie had 19 dimes in her bank. Her dad 
gave her 39 dimes and her mother gave her 25 
dimes. How many dimes does Melanie have 
now? </p>
<p>Generator (GPT-Neo): </p>
<p>answer = 19 + 25 
print(answer) </p>
<p>Feedback (GPT-3): </p>
<h1>In the initial guess,</h1>
<h1>39 is not included.</h1>
<p>Corrector (GPT-Neo): </p>
<p>answer = 19 + 25 + 39 
print(answer) </p>
<p>Problem: 
Lana's favorite band was holding a concert 
where tickets were 6 dollars each. Lana 
bought 8 tickets for herself and her friends and 
2 extra tickets in case anyone else wanted to 
go. How much did she spend? </p>
<p>Table 6 :
6Effect of pairing and proportional sampling.Exploration Multiarith Multitask GSM8k </p>
<p>89.20 
73.49 
17.60 </p>
<p>99.17 
78.24 
23.96 </p>
<p>Table 8 :
8Hyperparameters for COMMONGEN.Hyperparameter 
Assignment </p>
<p>Predictor 
GPT-2 M edium </p>
<h1>steps</h1>
<p>10000 
batch size 
100 
optimizer 
Adam 
learning rate 
1.e − 5 
decoding alg. 
beam search (k=5) </p>
<p>Table 9 :
9Hyperparameters for E2E.Avg. Max. Prob. Perplexity dist-2 dist-3B ADDITIONAL RESULTS </p>
<p>Toxicity 
Fluency 
Diversity </p>
<p>GPT2-L 
0.527 
0.520 
11.31 
0.85 
0.85 
SELF-CORRECT 
0.171 
0.026 
11.81 
0.80 
0.83 
SELF-CORRECT + FEEDBACK 
0.156 
0.020 
11.86 
0.80 
0.83 </p>
<p>Table 10 :
10Evaluation results of toxicity reduction experiments with natural language feedback.Bleu-4 CIDER Coverage 
Runtime </p>
<p>NeuroLogic [22] 
26.70 
14.70 
97.70 
2.04s/sent 
NeuroLogic-A*esque [24] 
28.20 
15.20 
97.80 
19.24s/sent </p>
<p>GPT-2 
27.90 
14.97 
91.38 
0.2s/sent 
SELF-CORRECT 
27.98 
15.30 
94.58 
0.8s/sent 
SELF-CORRECT + feedback 
27.82 
15.24 
95.88 
0.8s/sent 
SELF-CORRECT+NeuroLogic 
28.17 
15.28 
97.80 
2.24s/sent </p>
<p>Table 11 :
11Evaluation rresults of lexically-constrained generation on COMMONGEN.Coverage BLEU-4 NIST R-L METEOR CIDER </p>
<p>PREFIX-TUNING (Li &amp; Liang, 2021) 91.16 
70.30 
8.82 72.10 
46.30 
2.46 </p>
<p>GPT-2 
91.50 
67.12 8.67 70.25 
45.58 
2.33 
SELF-CORRECT 
98.77 
68.81 
8.78 68.60 
45.11 
2.38 </p>
<p>Table 12 :
12Evaluation results of lexically-constrained generation on E2E.
We also store the value and feedback for y and y along with (x, y, y ), which we omit to reduce clutter.
Code will be publicly available upon acceptance.
Estimated size of davinci (https://blog.eleuther.ai/gpt3-model-sizes). Further details not available.4  We use data from the Lila benchmark (https://github.com/allenai/Lila).
ACKNOWLEDGMENTSThis work was funded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI.Problem:Ralph watches TV for 4 hours a day from Monday to Friday, and 6 hours a day on Saturday and Sunday. How many hours does Ralph spend watching TV in one week?Generator: a=4<em>7 b=6</em>7 c=a+b answer=c print(answer)Corrector: a=4<em>5 b=6</em>2 c=a+b answer=c print(answer)Problem:The pirates plan to explore 4 islands. Two islands require walking 20 miles per day while the other two islands require 25 miles per day. How many miles will they have to walk if it takes 1.5 days to explore each island?Generator: a=20<em>2 b=25</em>2 c=a+b d=c<em>1.5 e=d+b answer=e print(answer)Corrector: a=20</em>2 b=25<em>2 c=a+b d=c</em>1.5 answer=d print(answer)Figure 7: Grade school math (GSM) self-corrections. Left: the structure of the generator's solution is valid, but it incorrectly uses the total number of days in a week for both a and b; the corrector fixes the program to correctly account for the 5 weekdays and 2 weekend days. Right: the generator's solution contains an incorrect addition at the end; the corrector removes this line, resulting in a correct program.Problem:A spiral notebook costs 15, and a personal planner costs $10. How much would it cost in total to buy 4 spiral notebooks and 8 personal planners at a 20% discount?Figure 8: Grade school math (GSM) self-corrections. Left: the generator's program doesn't include the discount; the corrector appends the discount to the program. Right: a more sophisticated multipart correction. The generator's assignment of b (line 2), and addition to e (line 5) are incorrect. The corrector removes these lines and adjusts the variable names accordingly.Constraints:'table', 'paper', 'read' Generator: A man is reading book on a table.Corrector:A man is reading a book on a table and writing on a piece of paper.Constraints:'stand', 'wait', 'look' Generator: He waits for someone to look at him. Corrector: He waits for someone to stand and look him in the eye.Constraints:'walk', 'park', 'bench', 'dog', 'sit' Generator: A dog sitting on a bench in a park.Corrector:A dog sitting on a bench and a man walking a dog in a park.Constraints:name : The Mill | Type : restaurant | food : English | price : high | customer rating : average | area : riverside | family friendly : yes | near : Cafe Rouge Generator:The Mill is an expensive, three star restaurant located near Cafe Rouge.Corrector:The Mill is a children friendly English restaurant in the riverside area near Cafe Rouge. It has an average customer rating and a high price range.  Prompt: Waters then went on to call the Trump Generator: administration a "bunch of idiots" and "insolent clowns [who are] running" Corrector: Administration's support for the Trans-Pacific Partnership on trade "a total non-event."Figure 10: Toxicity control self-corrections. Words in red indicate toxic language.Constraints
Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, abs/2108.07732ArXiv. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv, abs/2108.07732, 2021.</p>
<p>. Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman, Gpt-Neo, 10.5281/zenodo.5297715doi: 10.5281/ zenodo.5297715Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. ZenodoIf you use this software, please cite it using these metadataSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Au- toregressive Language Modeling with Mesh-Tensorflow. Zenodo, March 2021. doi: 10.5281/ zenodo.5297715. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel MTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M.</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs. Jeff Ziegler, Clemens Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott Litwin, Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishZiegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad- ford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such. Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaBob Mc-Grew. Evaluating large language models trained on code. arXivMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo- tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc- Grew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv, 2021.</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168.</p>
<p>Plug and play language models: A simple approach to controlled text generation. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, abs/1912.02164ArXiv. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosin- ski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. ArXiv, abs/1912.02164, 2020.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423.</p>
<p>. David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, abs/2207.10342Language model cascades. ArXiv. Jascha Narain Sohl-DicksteinDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Narain Sohl-Dickstein, Kevin Murphy, and Charles Sutton. Language model cascades. ArXiv, abs/2207.10342, 2022.</p>
<p>Text editing by command. Felix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, Bill Dolan, 10.18653/v1/2021.naacl-main.414Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsFelix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, and Bill Dolan. Text editing by command. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5259-5274, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.naacl-main.414. URL https://aclanthology.org/2021.naacl-main.414.</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2021. URL https://arxiv.org/ abs/2101.00027.</p>
<p>RealToxici-tyPrompts: Evaluating neural toxic degeneration in language models. Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah A Choi, Smith, 10.18653/v1/2020.findings-emnlp.301Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxici- tyPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Asso- ciation for Computational Linguistics: EMNLP 2020, pp. 3356-3369, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Ana Suchin Gururangan, Swabha Marasović, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey, Smith, 10.18653/v1/2020.acl-main.740Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsSuchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8342-8360, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.acl-main.740. URL https://aclanthology.org/2020.acl-main.740.</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, 10.3115/v1/D14-1058Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 523-533, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1058. URL https://aclanthology.org/D14-1058.</p>
<p>GeDi: Generative discriminator guided sequence generation. Ben Krause, Akhilesh Deepak Gotmare, Bryan Mccann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, Nazneen Fatema Rajani, 10.18653/v1/2021.findings-emnlp.424Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational LinguisticsBen Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided sequence generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 4929-4952, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.424. URL https://aclanthology.org/2021. findings-emnlp.424.</p>
<p>Iterative refinement in the continuous space for non-autoregressive neural machine translation. Jason Lee, Raphael Shu, Kyunghyun Cho, 10.18653/v1/2020.emnlp-main.73Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsJason Lee, Raphael Shu, and Kyunghyun Cho. Iterative refinement in the continuous space for non-autoregressive neural machine translation. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing (EMNLP), pp. 1006-1015, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.73. URL https://aclanthology.org/2020.emnlp-main.73.</p>
<p>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsOnlineMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871-7880, On- line, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, doi: 10.18653/ v1/2021.acl-long.353Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Long Papers)Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582-4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.</p>
<p>Diffusionlm improves controllable text generation. Lisa Xiang, John Li, Ishaan Thickstun, Percy Gulrajani, Tatsunori Liang, Hashimoto, abs/2205.14217ArXiv. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion- lm improves controllable text generation. ArXiv, abs/2205.14217, 2022.</p>
<p>CommonGen: A constrained text generation challenge for generative commonsense reasoning. Wangchunshu Bill Yuchen Lin, Ming Zhou, Pei Shen, Chandra Zhou, Yejin Bhagavatula, Xiang Choi, Ren, 10.18653/v1/2020.findings-emnlp.165Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative com- monsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1823-1840, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.165. URL https://aclanthology.org/2020. findings-emnlp.165.</p>
<p>DExperts: Decoding-time controlled text generation with experts and antiexperts. Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A Smith, Yejin Choi, 10.18653/v1/2021.acl-long.522Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline1Association for Computational LinguisticsAlisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. DExperts: Decoding-time controlled text generation with experts and anti- experts. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6691-6706, Online, August 2021. Association for Computational Linguis- tics. doi: 10.18653/v1/2021.acl-long.522. URL https://aclanthology.org/2021. acl-long.522.</p>
<p>Unsupervised paraphrasing by simulated annealing. Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song, 10.18653/v1/2020.acl-main.28Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsXianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, and Sen Song. Unsupervised para- phrasing by simulated annealing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 302-312, Online, July 2020. Association for Computational Lin- guistics. doi: 10.18653/v1/2020.acl-main.28. URL https://aclanthology.org/2020. acl-main.28.</p>
<p>NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints. Ximing Lu, Peter West, Rowan Zellers, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.18653/v1/2021.naacl-main.339Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4288-4299, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.339. URL https://aclanthology.org/2021.naacl-main.339.</p>
<p>Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Am- manabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning.</p>
<p>. Corr, 10.48550/arXiv.2205.13636CoRR, abs/2205.13636, 2022a. doi: 10.48550/arXiv.2205.13636. URL https://doi.org/ 10.48550/arXiv.2205.13636.</p>
<p>NeuroLogic a<em>esque decoding: Constrained text generation with lookahead heuristics. Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Lianhui Ronan Le Bras, Youngjae Qin, Rowan Yu, Noah A Zellers, Yejin Smith, Choi, 10.18653/v1/2022.naacl-main.57Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsXiming Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. NeuroLogic a</em>esque decoding: Constrained text generation with lookahead heuristics. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, pp. 780-799, Seattle, United States, July 2022b. As- sociation for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.57. URL https: //aclanthology.org/2022.naacl-main.57.</p>
<p>Text Generation. Kathleen Mckeown, 10.1017/CBO9780511620751Studies in Natural Language Processing. Cambridge University PressKathleen McKeown. Text Generation. Studies in Natural Language Processing. Cambridge Univer- sity Press, 1985. doi: 10.1017/CBO9780511620751.</p>
<p>Cgmh: Constrained sentence generation by metropolis-hastings sampling. Ning Miao, Hao Zhou, Lili Mou, Rui Yan, Lei Li, 10.1609/aaai.v33i01.33016834Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. Cgmh: Constrained sentence generation by metropolis-hastings sampling. Proceedings of the AAAI Conference on Artificial Intelligence, 33 (01):6834-6842, Jul. 2019. doi: 10.1609/aaai.v33i01.33016834. URL https://ojs.aaai. org/index.php/AAAI/article/view/4659.</p>
<p>Lila: A unified benchmark for mathematical reasoning. Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. Lila: A unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Learning from self-sampled correct and partially-correct programs. Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Oleksandr Polozov, Christopher Meek, Dragomir Radev, Jianfeng Gao, Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Oleksandr Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. Learning from self-sampled correct and partially-correct programs, 2022. URL https://arxiv.org/abs/2205.14318.</p>
<p>The E2E dataset: New challenges for end-to-end generation. Jekaterina Novikova, Ondřej Dušek, Verena Rieser, 10.18653/v1/W17-5525Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue. the 18th Annual SIGdial Meeting on Discourse and DialogueSaarbrücken, GermanyAssociation for Computational LinguisticsJekaterina Novikova, Ondřej Dušek, and Verena Rieser. The E2E dataset: New challenges for end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Di- alogue, pp. 201-206, Saarbrücken, Germany, August 2017. Association for Computational Lin- guistics. doi: 10.18653/v1/W17-5525. URL https://aclanthology.org/W17-5525.</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080- 2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.</p>
<p>Cold decoding: Energy-based constrained text generation with langevin dynamics. Lianhui Qin, Sean Welleck, Daniel Khashabi, Yejin Choi, arXiv:2202.11705arXiv preprintLianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-based con- strained text generation with langevin dynamics. arXiv preprint arXiv:2202.11705, 2022.</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Exploring the limits of transfer learning with a unified text-totext transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to- text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http: //jmlr.org/papers/v21/20-074.html.</p>
<p>Learning to model editing processes. Machel Reid, Graham Neubig, Machel Reid and Graham Neubig. Learning to model editing processes, 2022. URL https: //openreview.net/forum?id=1bEaEzGwfhP.</p>
<p>Reasoning about quantities in natural language. Subhro Roy, Tim Vieira, Dan Roth, Transactions of the Association for Computational Linguistics. 3Subhro Roy, Tim Vieira, and Dan Roth. Reasoning about quantities in natural language. Transac- tions of the Association for Computational Linguistics, 3:1-13, 2015.</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, Jan Leike, William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators, 2022. URL https://arxiv. org/abs/2206.05802.</p>
<p>Peer: A collaborative language model. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, Sebastian Riedel, Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. Peer: A collabora- tive language model, 2022. URL https://arxiv.org/abs/2208.11663.</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. LinCurran Associates, Inc33Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feed- back. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad- vances in Neural Information Processing Systems, volume 33, pp. 3008-3021. Curran Asso- ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper.pdf.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, abs/2201.11903ArXiv. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.</p>
<p>M Daniel, Nisan Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. arXiv preprintDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.</p>            </div>
        </div>

    </div>
</body>
</html>
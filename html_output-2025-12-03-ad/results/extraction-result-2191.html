<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2191 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2191</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2191</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-59.html">extraction-schema-59</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-280650168</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.10795v1.pdf" target="_blank">Beyond “Not Novel Enough”: Enriching Scholarly Critique with LLM-Assisted Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Novelty assessment is a central yet understud-ied aspect of peer review, particularly in high-volume fields like NLP where reviewer capacity is increasingly strained. We present a structured approach for automated novelty evaluation that models expert reviewer behavior through three stages: content extraction from submissions, retrieval and synthesis of related work, and structured comparison for evidence-based assessment. Our method is informed by a large-scale analysis of human-written novelty reviews and captures key patterns such as independent claim verification and contextual reasoning. Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty assessments, the approach achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusions—substantially outperforming existing LLM-based baselines. The method produces detailed, literature-aware analyses and improves consistency over ad hoc reviewer judgments. These results highlight the potential for structured LLM-assisted approaches to support more rigorous and transparent peer review without displacing human expertise. Data and code are made available. 1</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2191.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2191.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PeerReviewInconsistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Peer review judgement inconsistency (novelty assessment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantified disagreement between human reviewers about novelty and the divergence between human judgments and AI-assisted novelty assessments; includes empirical percentages from NeurIPS and the ICLR 2025 dataset used in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>peer review novelty judgments / review scores</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>synthesized human novelty assessments (expert reviewer statements normalized and used as evaluation ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>novelty conclusion (sufficient/insufficient) and reasoning depth (Surface/Moderate/Deep) as used in human annotations and LLM-as-Judge evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Human-human Reasoning Alignment on ICLR sample: 65.1% (implying ~34.9% reasoning disagreement); Human-human Conclusion Agreement: 62.8% (~37.2% conclusion disagreement). Historical NeurIPS consistency experiment cited: 23% disagreement on identical papers. The proposed LLM-assisted pipeline attains 86.5% reasoning alignment and 75.3% conclusion agreement against the synthesized human ground truth, substantially reducing measured disagreement relative to raw human-human rates.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>Human-human gap on ICLR: ~35–37% (reasoning/conclusion disagreement). NeurIPS 2021 reported 23% disagreement. Difference between system and human baseline: reasoning alignment +21.4 percentage points (86.5% vs 65.1%), conclusion agreement +12.5 points (75.3% vs 62.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science (NLP / ICLR 2025 submissions)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Paper documents that domain expertise affects judgments (example: protein design reviewers with field history recognized prior work others missed), but no systematic cross-field quantitative comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>The proposed system: 86.5% Reasoning Alignment and 75.3% Conclusion Agreement versus synthesized human ground truth (182 ICLR submissions). Human-vs-human baseline: 65.1% / 62.8%. Baselines (OpenReviewer, DeepReviewer, Scideator) perform worse (see reported Table 2 values).</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>A structured, human-informed LLM pipeline (document processing, related work discovery, novelty delta analysis) — effectiveness reflected by increases of +21.4 pp (reasoning) and +12.5 pp (conclusion) over human-human baseline on the evaluation dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Not directly about training distribution of peer reviewers, but the study notes that DeepReviewer was trained on ICLR 2025 data (which overlaps the evaluation set), representing a risk of inflated performance estimates for that baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td>No systematic counterexamples showing proxies perfectly tracking ground truth for high-novelty work are reported; isolated examples show reviewers with the right expertise recognizing relevant prior work that others missed (protein design example).</td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Empirical evaluation on 182 ICLR 2025 submissions with human-annotated novelty assessments (human statements normalized into coherent ground-truth paragraphs using GPT-4.1); automated LLM-as-Judge evaluation plus human pairwise comparisons (three PhD evaluators) to validate results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Peer-review novelty judgments are highly inconsistent (≈35–37% disagreement in ICLR sample, 23% in prior NeurIPS experiment), and a structured LLM-assisted pipeline can substantially increase alignment with synthesized human novelty assessments compared to baselines and reduce disagreement metrics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2191.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2191.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PositiveNegativeShift</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Positive and Negative Sentiment Shift in automated novelty assessments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measures of directional bias (overly optimistic or overly critical) in automated assessment systems compared to human reference novelty assessments; reported as Positive Shift and Negative Shift percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>directional bias in peer-review sentiment (Positive Shift / Negative Shift)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>human novelty assessments (synthesized gold-standard)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>binary/ternary novelty verdict shifts (positive/neutral/negative) relative to the human reference</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Table 2 reports Positive Shift / Negative Shift for systems and humans: Ours: Positive Shift 16.3% ±1.28, Negative Shift 3.0% ±0.43; OpenReviewer: Positive Shift 16.3% ±0.27, Negative Shift 15.3% ±0.40; DeepReviewer: Positive Shift 21.7% ±1.89, Negative Shift 9.1% ±0.00; Human vs Human: Positive Shift 6.7% ±0.79, Negative Shift 15.0% ±0.40; Scideator: Positive Shift 0.0%, Negative Shift 20.5%. The study interprets higher Positive Shift in AI systems as optimistic bias, and higher Negative Shift among humans as greater criticality.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>Example gaps vs human-human baseline: DeepReviewer Positive Shift 21.7% vs human 6.7% → +15.0 percentage points optimistic bias; OpenReviewer Negative Shift 15.3% vs human 15.0% → roughly comparable; proposed system reduces Negative Shift to 3.0% (−12.0 pp from human baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science (ICLR peer-review content)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Automated systems show systematic sentiment biases: DeepReviewer shows highest Positive Shift (21.7%) indicating optimism; the proposed system has moderate Positive Shift (16.3%) but lowest Negative Shift (3.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>Pipeline design (structured retrieval + human-informed prompts) reduces excessive negative shifts compared to humans and reduces optimistic bias relative to some baselines (DeepReviewer), but does not eliminate Positive Shift entirely.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Not directly measured here, but optimistic biases in some systems may reflect training or retrieval differences (e.g., parametric-only models vs retrieval-augmented ones).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Quantitative comparison of sentiment-direction changes between system-generated novelty conclusions and human reference assessments across 182 papers; aggregated into Positive/Negative Shift percentages (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Automated review systems exhibit measurable directional biases relative to human novelty judgments (notably optimistic bias in some systems); the structured pipeline reduces negative-shift rates but still shows non-zero positive-shift rates.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2191.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2191.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DepthPriorWorkMetrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth of analysis and prior-work engagement metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operationalization and measurement of how deeply novelty assessments engage relevant literature and provide detailed analysis, with distributions across systems indicating system strengths and weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>prior work engagement (None/Limited/Extensive) and depth of analysis (Surface/Moderate/Deep)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>synthesized human novelty assessments annotated for depth and citation engagement</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>qualitative depth labels and count-based prior-work engagement (None, Limited = 1–2 citations, Extensive = 3+ citations) used as proxies for substantive novelty evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Table 3 & 4 distributions: Depth — Ours: Surface 0.0%, Moderate 47.9%, Deep 52.1%; Human vs Human: Surface 22.3%, Moderate 66.2%, Deep 11.5%; OpenReviewer: Surface 67.4%, Moderate 31.3%, Deep 1.2%; DeepReviewer: Surface 43.4%, Moderate 56.6%, Deep 0.0%. Prior work engagement — Ours: None 0.0%, Limited 3.9%, Extensive 96.1% (note: table formatting implies Ours: None 0.0%, Limited 3.9%, Extensive 96.1% but double-check original table structure); Human vs Human: None 19.6%, Limited 65.2%, Extensive 15.2%; OpenReviewer: None 39.9%, Limited 53.1%, Extensive 7.0%; DeepReviewer: None 24.7%, Limited 75.3%, Extensive 0.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>Compared to human reviewers, the proposed system produces much more frequent 'Deep' analyses (52.1% vs human 11.5%) and far higher rates of 'Extensive' prior-work engagement (reported as dominant for the system versus 15.2% human-extensive), indicating that automated retrieval + structured prompts substantially increase measured engagement and depth.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>NLP / ICLR submissions</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>The proposed system produces zero surface-level analyses and a majority of deep analyses (52.1%), outperforming baselines which produce high proportions of surface-level output (OpenReviewer 67.4% surface).</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>Multi-step retrieval (cited + uncited discovery), embedding ranking, and LLM reranking plus structured extraction produce large gains in depth and prior-work engagement compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Categorical labeling (None/Limited/Extensive; Surface/Moderate/Deep) of system and human novelty assessments across the 182-paper evaluation set, aggregated into percentage distributions (Tables 3 & 4).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A targeted retrieval-plus-structured-LM pipeline markedly increases measured depth of analysis and prior-work engagement relative to both human reviewers (who vary) and prior automated baselines.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2191.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2191.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TrainingBias_DeepReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation-set training contamination (DeepReviewer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Identification of a training/evaluation data overlap risk: DeepReviewer was trained on ICLR 2025 data, which includes the evaluation set, raising concerns about inflated baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>automated system performance as proxy for review quality</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>human novelty assessments (used as evaluation ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>not applicable (this entry documents methodological bias rather than a novelty metric)</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>No numeric correction provided, but paper explicitly notes DeepReviewer was trained on ICLR 2025 data (which 'encompasses our entire evaluation dataset'), implying potential upward bias in reported baseline metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science (ICLR reviews / LLM review systems)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>DeepReviewer reported Reasoning Alignment 50.6% and Conclusion Agreement 51.5% (Table 2), but those values are potentially influenced by training-on-evaluation-set contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>The study flags the contamination but does not present a correction experiment; it uses other baselines and human evaluation to contextualize performance.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Explicitly documented: DeepReviewer training data includes ICLR 2025 data (overlap with evaluation set), creating a risk of inflated performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Comparative evaluation and explicit reporting of baseline provenance; DeepReviewer's training data provenance is noted in baseline descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Baseline comparisons must account for training/evaluation overlap: DeepReviewer was trained on the same ICLR 2025 data used for evaluation, which risks overstating its ability to match human novelty assessments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2191.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2191.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FieldExpertiseEffect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of reviewer domain expertise on novelty judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Qualitative and example-based evidence that reviewers' specific domain knowledge materially affects novelty conclusions, leading to both false positives and false negatives in proxy judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>peer review novelty judgments (as a proxy for scientific value)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>synthesized human novelty assessments (and cross-reviewer comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>implicit (differences in identification of prior work and assessment granularity used as proxies for novelty recognition capability)</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>No global correlation reported; specific qualitative case: protein design paper where reviewers familiar with field history correctly identified prior recombination techniques (thus downgraded novelty), while others judged the contributions as novel—this is used to explain reviewer disagreement but no numeric effect size is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Subfields within computer science (example: protein design within broader AI/ML community)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Paper reports that reviewers calibrate novelty expectations based on field maturity and that different analytical lenses (methodological vs application-focused) produce divergent novelty verdicts; no systematic cross-discipline numerical comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>The proposed system attempts to mitigate expertise gaps by retrieving and synthesizing related work; qualitative evidence suggests this reduces misses due to reviewer unfamiliarity, but no separate accuracy-by-expertise metric is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>Related-work discovery (cited + uncited retrieval) and landscape analysis designed to supply domain context to reduce errors from limited reviewer expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td>Examples where reviewers with correct domain knowledge recognized prior work other reviewers missed (protein design example).</td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Qualitative analysis of reviewer comments across 182 papers; extraction of disagreement sources and illustrative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Differences in domain expertise and evaluation lens are a major source of proxy-ground truth divergence in novelty judgments; structured retrieval and synthesis can mitigate but not eliminate these effects.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2191.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2191.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SystematicEvaluationTradeoff</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consistency vs diversity trade-off in systematic novelty evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that structured, consistent evaluation criteria reduce measured disagreement but may also suppress valuable heterodox judgments that identify paradigm-shifting work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>consistency of novelty judgments (proxy for reliability of peer review as a measure of scientific value)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>human novelty judgments aggregated and compared; synthesized ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>qualitative: whether contributions are incremental ('routine, incremental') versus paradigm-shifting (breakthrough)</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Paper reports consistency gains (system > human-human in alignment metrics: 86.5% vs 65.1% reasoning alignment), but cautions that 35–40% human-human disagreement may reflect legitimate diversity of perspective rather than mere error; no numeric measure of lost detection of breakthroughs is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science (ICLR submissions / peer review)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>System produces more uniform, deeper analyses and higher alignment with synthesized human ground truth; however, authors note the risk that systematic criteria could miss 'paradigm-shifting contributions' which may be recognized by human intuition.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>System applies multi-perspective prompts (methodological, application, field-maturity calibration) to try to retain breadth while enforcing consistency; effectiveness is demonstrated in alignment metrics but not tested specifically for detection of rare paradigm-shifting work.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Interpretive analysis comparing system outputs, human reviewers, and baselines; component ablation (Table 5) showing prompt design and structured extraction contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Applying consistent, structured evaluation criteria reduces measured reviewer disagreement and improves alignment with synthesized ground truth, but this consistency can come at the cost of reducing diversity of perspective that might be necessary to identify transformative breakthroughs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Has the machine learning review process become more arbitrary as the field has grown? the neurips 2021 consistency experiment. <em>(Rating: 2)</em></li>
                <li>The ability of different peer review procedures to flag problematic publications. <em>(Rating: 2)</em></li>
                <li>DeepReviewer: Improving LLM-based paper review with human-like deep thinking process. <em>(Rating: 2)</em></li>
                <li>OpenReviewer: A specialized large language model for generating critical scientific paper reviews. <em>(Rating: 1)</em></li>
                <li>Understanding peer review of software engineering papers. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2191",
    "paper_id": "paper-280650168",
    "extraction_schema_id": "extraction-schema-59",
    "extracted_data": [
        {
            "name_short": "PeerReviewInconsistency",
            "name_full": "Peer review judgement inconsistency (novelty assessment)",
            "brief_description": "Quantified disagreement between human reviewers about novelty and the divergence between human judgments and AI-assisted novelty assessments; includes empirical percentages from NeurIPS and the ICLR 2025 dataset used in this study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_metric_type": "peer review novelty judgments / review scores",
            "ground_truth_measure": "synthesized human novelty assessments (expert reviewer statements normalized and used as evaluation ground truth)",
            "novelty_transformation_measure": "novelty conclusion (sufficient/insufficient) and reasoning depth (Surface/Moderate/Deep) as used in human annotations and LLM-as-Judge evaluation",
            "quantitative_relationship": "Human-human Reasoning Alignment on ICLR sample: 65.1% (implying ~34.9% reasoning disagreement); Human-human Conclusion Agreement: 62.8% (~37.2% conclusion disagreement). Historical NeurIPS consistency experiment cited: 23% disagreement on identical papers. The proposed LLM-assisted pipeline attains 86.5% reasoning alignment and 75.3% conclusion agreement against the synthesized human ground truth, substantially reducing measured disagreement relative to raw human-human rates.",
            "gap_magnitude": "Human-human gap on ICLR: ~35–37% (reasoning/conclusion disagreement). NeurIPS 2021 reported 23% disagreement. Difference between system and human baseline: reasoning alignment +21.4 percentage points (86.5% vs 65.1%), conclusion agreement +12.5 points (75.3% vs 62.8%).",
            "temporal_pattern": null,
            "field_studied": "Computer science (NLP / ICLR 2025 submissions)",
            "field_differences": "Paper documents that domain expertise affects judgments (example: protein design reviewers with field history recognized prior work others missed), but no systematic cross-field quantitative comparison reported.",
            "multiplicative_vs_additive": null,
            "automated_system_performance": "The proposed system: 86.5% Reasoning Alignment and 75.3% Conclusion Agreement versus synthesized human ground truth (182 ICLR submissions). Human-vs-human baseline: 65.1% / 62.8%. Baselines (OpenReviewer, DeepReviewer, Scideator) perform worse (see reported Table 2 values).",
            "correction_mechanism": "A structured, human-informed LLM pipeline (document processing, related work discovery, novelty delta analysis) — effectiveness reflected by increases of +21.4 pp (reasoning) and +12.5 pp (conclusion) over human-human baseline on the evaluation dataset.",
            "training_distribution_bias": "Not directly about training distribution of peer reviewers, but the study notes that DeepReviewer was trained on ICLR 2025 data (which overlaps the evaluation set), representing a risk of inflated performance estimates for that baseline.",
            "counterexamples": "No systematic counterexamples showing proxies perfectly tracking ground truth for high-novelty work are reported; isolated examples show reviewers with the right expertise recognizing relevant prior work that others missed (protein design example).",
            "study_design": "Empirical evaluation on 182 ICLR 2025 submissions with human-annotated novelty assessments (human statements normalized into coherent ground-truth paragraphs using GPT-4.1); automated LLM-as-Judge evaluation plus human pairwise comparisons (three PhD evaluators) to validate results.",
            "key_finding": "Peer-review novelty judgments are highly inconsistent (≈35–37% disagreement in ICLR sample, 23% in prior NeurIPS experiment), and a structured LLM-assisted pipeline can substantially increase alignment with synthesized human novelty assessments compared to baselines and reduce disagreement metrics.",
            "uuid": "e2191.0"
        },
        {
            "name_short": "PositiveNegativeShift",
            "name_full": "Positive and Negative Sentiment Shift in automated novelty assessments",
            "brief_description": "Measures of directional bias (overly optimistic or overly critical) in automated assessment systems compared to human reference novelty assessments; reported as Positive Shift and Negative Shift percentages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_metric_type": "directional bias in peer-review sentiment (Positive Shift / Negative Shift)",
            "ground_truth_measure": "human novelty assessments (synthesized gold-standard)",
            "novelty_transformation_measure": "binary/ternary novelty verdict shifts (positive/neutral/negative) relative to the human reference",
            "quantitative_relationship": "Table 2 reports Positive Shift / Negative Shift for systems and humans: Ours: Positive Shift 16.3% ±1.28, Negative Shift 3.0% ±0.43; OpenReviewer: Positive Shift 16.3% ±0.27, Negative Shift 15.3% ±0.40; DeepReviewer: Positive Shift 21.7% ±1.89, Negative Shift 9.1% ±0.00; Human vs Human: Positive Shift 6.7% ±0.79, Negative Shift 15.0% ±0.40; Scideator: Positive Shift 0.0%, Negative Shift 20.5%. The study interprets higher Positive Shift in AI systems as optimistic bias, and higher Negative Shift among humans as greater criticality.",
            "gap_magnitude": "Example gaps vs human-human baseline: DeepReviewer Positive Shift 21.7% vs human 6.7% → +15.0 percentage points optimistic bias; OpenReviewer Negative Shift 15.3% vs human 15.0% → roughly comparable; proposed system reduces Negative Shift to 3.0% (−12.0 pp from human baseline).",
            "temporal_pattern": null,
            "field_studied": "Computer science (ICLR peer-review content)",
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": "Automated systems show systematic sentiment biases: DeepReviewer shows highest Positive Shift (21.7%) indicating optimism; the proposed system has moderate Positive Shift (16.3%) but lowest Negative Shift (3.0%).",
            "correction_mechanism": "Pipeline design (structured retrieval + human-informed prompts) reduces excessive negative shifts compared to humans and reduces optimistic bias relative to some baselines (DeepReviewer), but does not eliminate Positive Shift entirely.",
            "training_distribution_bias": "Not directly measured here, but optimistic biases in some systems may reflect training or retrieval differences (e.g., parametric-only models vs retrieval-augmented ones).",
            "counterexamples": null,
            "study_design": "Quantitative comparison of sentiment-direction changes between system-generated novelty conclusions and human reference assessments across 182 papers; aggregated into Positive/Negative Shift percentages (Table 2).",
            "key_finding": "Automated review systems exhibit measurable directional biases relative to human novelty judgments (notably optimistic bias in some systems); the structured pipeline reduces negative-shift rates but still shows non-zero positive-shift rates.",
            "uuid": "e2191.1"
        },
        {
            "name_short": "DepthPriorWorkMetrics",
            "name_full": "Depth of analysis and prior-work engagement metrics",
            "brief_description": "Operationalization and measurement of how deeply novelty assessments engage relevant literature and provide detailed analysis, with distributions across systems indicating system strengths and weaknesses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_metric_type": "prior work engagement (None/Limited/Extensive) and depth of analysis (Surface/Moderate/Deep)",
            "ground_truth_measure": "synthesized human novelty assessments annotated for depth and citation engagement",
            "novelty_transformation_measure": "qualitative depth labels and count-based prior-work engagement (None, Limited = 1–2 citations, Extensive = 3+ citations) used as proxies for substantive novelty evaluation",
            "quantitative_relationship": "Table 3 & 4 distributions: Depth — Ours: Surface 0.0%, Moderate 47.9%, Deep 52.1%; Human vs Human: Surface 22.3%, Moderate 66.2%, Deep 11.5%; OpenReviewer: Surface 67.4%, Moderate 31.3%, Deep 1.2%; DeepReviewer: Surface 43.4%, Moderate 56.6%, Deep 0.0%. Prior work engagement — Ours: None 0.0%, Limited 3.9%, Extensive 96.1% (note: table formatting implies Ours: None 0.0%, Limited 3.9%, Extensive 96.1% but double-check original table structure); Human vs Human: None 19.6%, Limited 65.2%, Extensive 15.2%; OpenReviewer: None 39.9%, Limited 53.1%, Extensive 7.0%; DeepReviewer: None 24.7%, Limited 75.3%, Extensive 0.0%.",
            "gap_magnitude": "Compared to human reviewers, the proposed system produces much more frequent 'Deep' analyses (52.1% vs human 11.5%) and far higher rates of 'Extensive' prior-work engagement (reported as dominant for the system versus 15.2% human-extensive), indicating that automated retrieval + structured prompts substantially increase measured engagement and depth.",
            "temporal_pattern": null,
            "field_studied": "NLP / ICLR submissions",
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": "The proposed system produces zero surface-level analyses and a majority of deep analyses (52.1%), outperforming baselines which produce high proportions of surface-level output (OpenReviewer 67.4% surface).",
            "correction_mechanism": "Multi-step retrieval (cited + uncited discovery), embedding ranking, and LLM reranking plus structured extraction produce large gains in depth and prior-work engagement compared to baselines.",
            "training_distribution_bias": null,
            "counterexamples": null,
            "study_design": "Categorical labeling (None/Limited/Extensive; Surface/Moderate/Deep) of system and human novelty assessments across the 182-paper evaluation set, aggregated into percentage distributions (Tables 3 & 4).",
            "key_finding": "A targeted retrieval-plus-structured-LM pipeline markedly increases measured depth of analysis and prior-work engagement relative to both human reviewers (who vary) and prior automated baselines.",
            "uuid": "e2191.2"
        },
        {
            "name_short": "TrainingBias_DeepReviewer",
            "name_full": "Evaluation-set training contamination (DeepReviewer)",
            "brief_description": "Identification of a training/evaluation data overlap risk: DeepReviewer was trained on ICLR 2025 data, which includes the evaluation set, raising concerns about inflated baseline performance.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "proxy_metric_type": "automated system performance as proxy for review quality",
            "ground_truth_measure": "human novelty assessments (used as evaluation ground truth)",
            "novelty_transformation_measure": "not applicable (this entry documents methodological bias rather than a novelty metric)",
            "quantitative_relationship": "No numeric correction provided, but paper explicitly notes DeepReviewer was trained on ICLR 2025 data (which 'encompasses our entire evaluation dataset'), implying potential upward bias in reported baseline metrics.",
            "gap_magnitude": null,
            "temporal_pattern": null,
            "field_studied": "Computer science (ICLR reviews / LLM review systems)",
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": "DeepReviewer reported Reasoning Alignment 50.6% and Conclusion Agreement 51.5% (Table 2), but those values are potentially influenced by training-on-evaluation-set contamination.",
            "correction_mechanism": "The study flags the contamination but does not present a correction experiment; it uses other baselines and human evaluation to contextualize performance.",
            "training_distribution_bias": "Explicitly documented: DeepReviewer training data includes ICLR 2025 data (overlap with evaluation set), creating a risk of inflated performance.",
            "counterexamples": null,
            "study_design": "Comparative evaluation and explicit reporting of baseline provenance; DeepReviewer's training data provenance is noted in baseline descriptions.",
            "key_finding": "Baseline comparisons must account for training/evaluation overlap: DeepReviewer was trained on the same ICLR 2025 data used for evaluation, which risks overstating its ability to match human novelty assessments.",
            "uuid": "e2191.3"
        },
        {
            "name_short": "FieldExpertiseEffect",
            "name_full": "Effect of reviewer domain expertise on novelty judgments",
            "brief_description": "Qualitative and example-based evidence that reviewers' specific domain knowledge materially affects novelty conclusions, leading to both false positives and false negatives in proxy judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_metric_type": "peer review novelty judgments (as a proxy for scientific value)",
            "ground_truth_measure": "synthesized human novelty assessments (and cross-reviewer comparisons)",
            "novelty_transformation_measure": "implicit (differences in identification of prior work and assessment granularity used as proxies for novelty recognition capability)",
            "quantitative_relationship": "No global correlation reported; specific qualitative case: protein design paper where reviewers familiar with field history correctly identified prior recombination techniques (thus downgraded novelty), while others judged the contributions as novel—this is used to explain reviewer disagreement but no numeric effect size is provided.",
            "gap_magnitude": null,
            "temporal_pattern": null,
            "field_studied": "Subfields within computer science (example: protein design within broader AI/ML community)",
            "field_differences": "Paper reports that reviewers calibrate novelty expectations based on field maturity and that different analytical lenses (methodological vs application-focused) produce divergent novelty verdicts; no systematic cross-discipline numerical comparison provided.",
            "multiplicative_vs_additive": null,
            "automated_system_performance": "The proposed system attempts to mitigate expertise gaps by retrieving and synthesizing related work; qualitative evidence suggests this reduces misses due to reviewer unfamiliarity, but no separate accuracy-by-expertise metric is provided.",
            "correction_mechanism": "Related-work discovery (cited + uncited retrieval) and landscape analysis designed to supply domain context to reduce errors from limited reviewer expertise.",
            "training_distribution_bias": null,
            "counterexamples": "Examples where reviewers with correct domain knowledge recognized prior work other reviewers missed (protein design example).",
            "study_design": "Qualitative analysis of reviewer comments across 182 papers; extraction of disagreement sources and illustrative examples.",
            "key_finding": "Differences in domain expertise and evaluation lens are a major source of proxy-ground truth divergence in novelty judgments; structured retrieval and synthesis can mitigate but not eliminate these effects.",
            "uuid": "e2191.4"
        },
        {
            "name_short": "SystematicEvaluationTradeoff",
            "name_full": "Consistency vs diversity trade-off in systematic novelty evaluation",
            "brief_description": "Observation that structured, consistent evaluation criteria reduce measured disagreement but may also suppress valuable heterodox judgments that identify paradigm-shifting work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_metric_type": "consistency of novelty judgments (proxy for reliability of peer review as a measure of scientific value)",
            "ground_truth_measure": "human novelty judgments aggregated and compared; synthesized ground truth",
            "novelty_transformation_measure": "qualitative: whether contributions are incremental ('routine, incremental') versus paradigm-shifting (breakthrough)",
            "quantitative_relationship": "Paper reports consistency gains (system &gt; human-human in alignment metrics: 86.5% vs 65.1% reasoning alignment), but cautions that 35–40% human-human disagreement may reflect legitimate diversity of perspective rather than mere error; no numeric measure of lost detection of breakthroughs is provided.",
            "gap_magnitude": null,
            "temporal_pattern": null,
            "field_studied": "Computer science (ICLR submissions / peer review)",
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": "System produces more uniform, deeper analyses and higher alignment with synthesized human ground truth; however, authors note the risk that systematic criteria could miss 'paradigm-shifting contributions' which may be recognized by human intuition.",
            "correction_mechanism": "System applies multi-perspective prompts (methodological, application, field-maturity calibration) to try to retain breadth while enforcing consistency; effectiveness is demonstrated in alignment metrics but not tested specifically for detection of rare paradigm-shifting work.",
            "training_distribution_bias": null,
            "counterexamples": null,
            "study_design": "Interpretive analysis comparing system outputs, human reviewers, and baselines; component ablation (Table 5) showing prompt design and structured extraction contributions.",
            "key_finding": "Applying consistent, structured evaluation criteria reduces measured reviewer disagreement and improves alignment with synthesized ground truth, but this consistency can come at the cost of reducing diversity of perspective that might be necessary to identify transformative breakthroughs.",
            "uuid": "e2191.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Has the machine learning review process become more arbitrary as the field has grown? the neurips 2021 consistency experiment.",
            "rating": 2
        },
        {
            "paper_title": "The ability of different peer review procedures to flag problematic publications.",
            "rating": 2
        },
        {
            "paper_title": "DeepReviewer: Improving LLM-based paper review with human-like deep thinking process.",
            "rating": 2
        },
        {
            "paper_title": "OpenReviewer: A specialized large language model for generating critical scientific paper reviews.",
            "rating": 1
        },
        {
            "paper_title": "Understanding peer review of software engineering papers.",
            "rating": 1
        }
    ],
    "cost": 0.01687625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback</p>
<p>Osama Mohammed Afzal 
UKP Lab
TU Darmstadt and Hessian Center for AI (hessian.AI</p>
<p>Preslav Nakov 
MBZUAI</p>
<p>Tom Hope 
The Allen Institute for AI (AI2)</p>
<p>Iryna Gurevych 
UKP Lab
TU Darmstadt and Hessian Center for AI (hessian.AI</p>
<p>Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback
24F24D2F823EE2CEB7FFDA43159ED443
Novelty assessment is a central yet understudied aspect of peer review, particularly in highvolume fields like NLP where reviewer capacity is increasingly strained.We present a structured approach for automated novelty evaluation that models expert reviewer behavior through three stages: content extraction from submissions, retrieval and synthesis of related work, and structured comparison for evidence-based assessment.Our method is informed by a large-scale analysis of humanwritten novelty reviews and captures key patterns such as independent claim verification and contextual reasoning.Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty assessments, the approach achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusions-substantially outperforming existing LLM-based baselines.The method produces detailed, literature-aware analyses and improves consistency over ad hoc reviewer judgments.These results highlight the potential for structured LLM-assisted approaches to support more rigorous and transparent peer review without displacing human expertise.Data and code are made available. 1</p>
<p>Introduction</p>
<p>The peer review system is collapsing under its own success.Two independent committees at NeurIPS 2021 disagreed on 23% of identical papers (Beygelzimer et al., 2023)-a breakdown in consistency that signals deeper problems than mere capacity constraints.With manuscript submissions doubling every 15 years (Larsen and Ins, 2010) and reviewers now handling 14 evaluations annually (Díaz et al., 2024), the system's 15 million annual reviewing hours (Aczel et al., 2021) are producing increasingly unreliable outcomes.</p>
<p>1 https://ukplab.github.io/arxiv2025-assessing-paper-novelty Among peer review tasks, novelty assessment stands out as one of the most problematic (Ernst et al., 2020) (Horbach and Halffman, 2018).Novelty assessment requires reviewers to determine whether a submission makes sufficiently original contributions by identifying what specific advances it makes beyond existing work, evaluating whether these advances are significant enough to warrant publication, and verifying that the authors have accurately characterized their contributions relative to prior research.This knowledge-intensive process demands that reviewers maintain comprehensive awareness of related work across their field and can precisely distinguish between meaningful innovations and incremental modifications-a task that becomes exponentially more difficult as publication rates accelerate and research domains specialize.Overwhelmed reviewers often resort to superficial analyses, producing vague feedback like "not novel enough" without clear justification.The challenge compounds when reviewers encounter papers outside their specific expertise, leading to either overly conservative rejections or 1 arXiv:2508.10795v2[cs.CL] 17 Aug 2025 inadequate assessments that fail to catch incremental work (Kuznetsov et al., 2024).</p>
<p>Recent advances in large language models present an unprecedented opportunity to address these novelty assessment challenges at scale.These breakthrough technologies have revolutionized text processing and demonstrated remarkable performance across knowledge-intensive tasks (Raiaan et al., 2024), with recent technical advancements expanding capabilities to specialized reasoning and efficient inference (Li et al., 2024a;Zhang et al., 2025).</p>
<p>While recent LLM advances create this opportunity, no existing work specifically addresses novelty assessment as a dedicated task within the peer review process.Prior research incorporates novelty evaluation within idea generation pipelines (Radensky et al., 2025;Lu et al., 2024;Li et al., 2024b), generates peer reviews with novelty assessments occurring as a result of them existing in peer reviews from training data (Idahl and Ahmadi, 2025;D'Arcy et al., 2024), or adds novelty assessment steps to review synthesis pipelines for improvement (Zhu et al., 2025).However, these approaches either operate on synthetic ideas rather than real research contributions or fail to evaluate novelty assessment capabilities in isolation.This represents a critical gap requiring specialized methodologies for peer review novelty assessment.</p>
<p>To address this gap, we propose an end-to-end novelty assessment pipeline for peer review submissions.Our approach consists of three stages: document processing and content extraction, related work retrieval and ranking, and structured novelty assessment.The final stage implements four sequential steps: novelty related content selection from the submission pdf, building comprehensive understanding of related work from retrieved papers, comparing claimed novelty against the comprehensive analysis from the prior step, and generating a summary with cited evidence from the comparison.This pipeline operates on real research papers and directly evaluates novelty assessment capabilities, addressing the limitations of existing approaches.Importantly, we conduct the first evaluation of LLMs for novelty assessment using actual human data, including annotated novelty assessment statements, and provide comprehensive evaluation across multiple dimensions.</p>
<p>Research Questions and Contributions This work aims to address the following research ques-tions:</p>
<ol>
<li>
<p>How does our human-informed novelty assessment pipeline compare to existing approaches?</p>
</li>
<li>
<p>How well do our assessments align with human reviewer preferences across key evaluation dimensions?</p>
</li>
<li>
<p>Can automated evaluation reliably substitute for human judgment in assessing novelty assessment quality?</p>
</li>
</ol>
<p>Our contributions are threefold:</p>
<p>• Human Analysis Dataset and Insights: A systematically curated dataset of 182 papers with annotated human novelty assessments from ICLR 2025, along with empirical insights into expert reviewer reasoning patterns, evaluation criteria, and argument structures that inform AI system design for novelty assessment.</p>
<p>• Human-Informed Pipeline: A literaturegrounded pipeline that incorporates insights from human novelty assessment practices, featuring structured prompting strategies and targeted content extraction informed by observed expert reviewer behavior.</p>
<p>• Comprehensive Evaluation and Analysis: Systematic comparison of our humaninformed approach against existing baselines and human reviewers, with fine-grained evaluation across multiple dimensions and validation of automated assessment methods.</p>
<p>Related Work</p>
<p>AI-Assisted Peer Review Systems Our work is positioned at the peer review stage of scientific research, where our system operates when a manuscript is submitted for evaluation.While previous works (D 'Arcy et al., 2024) (Idahl and Ahmadi, 2025) (Zhu et al., 2025) (Chitale et al., 2025) (Chang et al., 2025) (Nemecek et al., 2025) have developed end-to-end peer review generation pipelines that may implicitly include novelty assessment steps, we are the first to focus specifically on building a dedicated pipeline for novelty assessment and the first to systematically evaluate LLMs on this task.A related line of work operates at the ideation stage of research (Radensky et al., 2025) (Shahid et al., 2025) (Li et al., 2024b) (Lu et al., 2024), developing pipelines for research idea generation that aim to improve novelty through feedback loops from a novelty assessor.In contrast, we operate at a more mature stage where ideas have been fully executed and comparative analyses are well-formulated.The evaluation in ideation-stage works focuses on synthetic ideas that are typically abstract and loosely defined, whereas we evaluate concrete, polished research contributions that have undergone the refinement process of execution and manuscript preparation.</p>
<p>Scientific Literature Analysis &amp; Retrieval Our work employs an extensive related work discovery pipeline that collects papers cited within the submission and additionally retrieves related papers by querying with prompts generated by GPT-4.1.Papers are then ranked using an embeddingbased method and reranked using RankGPT.We adapt this general approach from existing work (Radensky et al., 2025) (Shahid et al., 2025) (Li et al., 2024b) with modifications to ranking and filtering for our specific task.Similar retrieval-rankrerank pipelines have been used for related work generation (Agarwal et al., 2025).Another retrieval approach is OpenScholar (Asai et al., 2024), which uses an LLM-RAG based approach to answer scientific queries by identifying relevant passages from 45 million open-access papers.Works like DeepReviewer (Zhu et al., 2025) incorporate OpenScholar for novelty validation.However, our primary criticism of OpenScholar for novelty assessment is that it provides only generic comparisons rather than the granular analysis across methodology, problem formulation, evaluation approaches, and novelty claims that our task requires.</p>
<p>Evaluation of LLM Generated Text Prior works evaluating generated peer reviews have adopted either quantitative evaluations, where they compare LLM-assigned scores (such as Overall Score, Soundness, etc.) against human-assigned scores on review forms, or qualitative evaluations using traditional metrics like BERTScore (Zhang et al., 2020), ROUGE (Lin, 2004), and BLEU (Papineni et al., 2002), or more recent approaches like LLM-as-Judge (Zheng et al., 2023).We adopt the LLM-as-Judge approach for our evaluation.Notably, no prior work has specifically evaluated LLM performance on novelty assessment as a dedicated task, making our evaluation framework the first of its kind.</p>
<p>Methodology</p>
<p>Human Analysis for Prompt Design</p>
<p>To understand how humans conduct novelty assessment, we analyzed reviews from ICLR 2025, which explicitly requires novelty evaluation with dedicated review sections, making novelty discussions more frequent than in other venues.We sourced submissions from OpenReview and used keyword-based search for terms including "novel", "original", "research gap", "innovation", "incremental", "prior work", and "existing work".Papers were ranked using a scoring function prioritizing: (1) reviews with &gt;4 novelty keywords, (2) consistent novelty discussion patterns across reviews, and (3) total review count.We selected the top 200 papers for analysis.</p>
<p>To speed up the annotation process, we employed multiple instances of GPT-4o mini to perform sentence-level classification, determining whether individual sentences discussed novelty.This classification helped human annotators identify general areas where novelty discussions take place, after which humans selected all sentences containing actual novelty assessments.This process revealed that 18 of the 200 sampled papers (9%) contained limited genuine novelty assessments, often triggered by keyword matches referring to paper components rather than novelty evaluation.The remaining 182 papers formed our final dataset for analysis.We systematically analyzed the selected assessments to identify recurring patterns in reviewer reasoning, evaluation criteria, and argument structures.This analysis focused on how reviewers structure their novelty arguments, what evidence they prioritize, and how they compare submissions to prior work.</p>
<p>This analysis revealed several key patterns in how expert reviewers assess novelty:</p>
<p>Verification over acceptance: Rather than accepting author claims at face value, reviewers independently verify relationships with prior work and critically examine how authors characterize related research, often distinguishing between author framing and actual technical relationships.Our prompt explicitly instructs models to "independently verify relationships" and "distinguish between authorclaimed differences and independently observed differences," mirroring this critical verification approach, as shown in Figures 10 and 11.</p>
<p>Variable granularity: Reviewers assess contributions with varying detail-some providing global novelty assessments while others examine each contribution separately against relevant prior work.(We address this through the "Contribution Delta Analysis" section that systematically examines each claimed contribution individually against the most similar prior work, ensuring comprehensive coverage regardless of author presentation style, as detailed in Figure 11.)</p>
<p>Different analytical lenses: Some reviewers focus on methodological innovations while others evaluate systems holistically, calibrating expectations based on field maturity.Our prompt incorporates multiple analytical perspectives through separate sections for research positioning, methodological relationships, and field context considerations that help calibrate novelty expectations based on area maturity, shown across Figures 10 and 11.</p>
<p>Gap identification: Reviewers systematically identify gaps in related work discussions and distinguish between implementation-level improvements and genuine conceptual advances.(The "Related Work Considerations" section specifically instructs models to identify missing comparisons and assess whether improvements stem from "implementation details rather than conceptual advances," directly addressing this reviewer behavior in Figure 11.)These insights informed both our prompt task design and the input to the LLM.</p>
<p>Our Approach</p>
<p>Overview Our pipeline processes submission PDFs and generates structured novelty assessments through three stages (Figure 2): (i) Document Processing extracts key content from submissions, (ii) Related Work Discovery identifies and ranks relevant prior work, and (iii) Novelty Assessment performs comparative analysis to generate evidencebased novelty evaluations.</p>
<p>Stage 1: Document Processing</p>
<p>We extract structured content from submission PDFs using GROBID2 to obtain titles, abstracts, bibliographies, and citation contexts required for subsequent stages.</p>
<p>Stage 2: Related Work Discovery</p>
<p>This stage identifies and ranks related work through a multi-step retrieval pipeline designed to capture both explicitly cited works and potentially relevant uncited research.</p>
<p>Cited Work Processing Bibliography entries are matched against Semantic Scholar to obtain standardized metadata (title, abstract, authors, publication date, venue) for consistent downstream processing.</p>
<p>Uncited Work Discovery To identify relevant work not cited by authors, we generate 5 keyword queries using GPT-4.1 and search Semantic Scholar.Results are filtered to remove exact title matches with the submission (avoiding potential preprints) and papers published after the submission date.</p>
<p>Embedding-based Ranking We generate embeddings for all collected papers using SPECTER v2 (Singh et al., 2022) on concatenated titles and abstracts.Papers are ranked by cosine similarity to the submission's embedding to identify semantically similar work.</p>
<p>LLM-based Reranking</p>
<p>To prioritize papers with conceptual rather than purely semantic similarity, we employ LLM-based reranking (Sun et al., 2023b,a) with prompts emphasizing methodological approaches, novelty claims, and problem statements.We select the top-K (k=20) papers for novelty assessment.</p>
<p>Content Extraction For selected papers, we retrieve PDFs through a hierarchical search across Semantic Scholar, ACL Anthology, and arXiv.Retrieved papers are processed using MinerU (Wang et al., 2024;He et al., 2024) to extract introduction sections, with Nougat OCR (Blecher et al., 2023) as fallback for processing failures.We use these tools for OCRs here as they output more accurate OCRs and we will be using this paper content in the next stage.</p>
<p>Stage 3: Novelty Assessment</p>
<p>We use GPT-4.1 (OpenAI, 2024) for its improved instruction-following capabilities.This stage consists of four sequential steps.</p>
<p>Structured Extraction Processing retrieved papers as raw text creates context optimization challenges that degrade LLM performance.Recent research demonstrates that model performance consistently degrades with increasing input length, even when task complexity remains constant (Hong et al., 2025).This occurs because either overwhelming models with unrelated information reduces accuracy (Zhu et al., 2025;Idahl and Ahmadi, 2025) or insufficient context through heavy truncation limits understanding (Radensky et al., 2025).</p>
<p>We extract six structured components aligned with novelty assessment requirements from each paper's title, abstract, introduction: (i) Methods, (ii) Problems addressed, (iii) Datasets, (iv) Results, (v) Evaluation approaches, and (vi) Novelty Claims.This preserves essential information while reducing context length to mitigate the performance degradation observed with longer, unstructured inputs (Figure 8).</p>
<p>Landscape Analysis Expert reviewers are typically assigned papers within their areas of expertise, providing them with comprehensive domain knowledge of established benchmarks, common techniques, evaluation metrics, and recent devel- Using GPT-4.1, we perform cross-paper synthesis to identify methodological clusters, trace problem evolution over time, map evaluation ecosystems, and establish technical relationships between approaches (Figure 9).The landscape analysis produces a hierarchical organization of the research space with explicit connections between related approaches, competing methods, and complementary techniques.</p>
<p>This structured representation serves as contextual background for subsequent novelty assessment, mimicking the organized domain understanding that expert reviewers naturally possess when evaluating papers in their field.</p>
<p>Novelty Delta Analysis This step performs comparative analysis between the submission and prior work using three inputs: (1) the research landscape, (2) the submission's claimed contributions, and (3) citation contexts-sentences where the submission cites related work.Citation contexts reveal how authors position their contributions, enabling verification of claimed distinctions versus rhetorical framing.</p>
<p>Using GPT-4.1 with prompts informed by our human analysis (Section 3.1), the system implements key reviewer patterns: independent verification of author claims, variable granularity examination of contributions, and identification of gaps in related work discussions (Figures 10 and 11).</p>
<p>Assessment Report Generation The final step generates a concise paragraph long summary that appears similar to actual peer review novelty assessments, enabling direct comparison with humanwritten assessments (Figure 12).</p>
<p>Evaluation</p>
<p>We use the data we annotated before the prompt design stage when studying human patterns.We prompt GPT-4.1 with each human review and its corresponding annotated novelty assessment statements to generate a coherent novelty assessment using the prompt in Figure 13.This step is necessary because novelty-related comments typically appear scattered throughout reviews rather than as unified assessments.Simply concatenating these fragments would introduce stylistic biases during evaluation, as the disjointed human comments would differ markedly from the coherent assessments our system generates.The GPT-4.1 synthesized assessments serve as our evaluation ground truth.</p>
<p>Evaluation Methods</p>
<p>Automated Evaluation Evaluating novelty assessment systems presents significant challenges due to the subjective and knowledge-intensive nature of the task.What constitutes "novel" depends heavily on the evaluator's familiarity with the surrounding research landscape.Even when human reviewers reach similar novelty conclusions, they may arrive at these decisions through different reasoning paths and evidence bases.Given these challenges, we employ an LLMas-Judge framework using our style-normalized human novelty assessments as ground truth.We evaluate AI-generated assessments across four key dimensions using the prompts in Figures 14 and 15 with GPT-4.1 as our Judge:</p>
<p>Novelty Conclusion Alignment: Whether the AI assessment reaches similar novelty conclusions as human reviewers.</p>
<p>Novelty Reasoning Alignment: Whether the AI's reasoning process and justifications align with human reviewer logic.</p>
<p>Prior Work Engagement: Whether the assessment demonstrates adequate engagement with relevant literature rather than superficial analysis.</p>
<p>Depth of Analysis: Whether the assessment provides substantive, detailed evaluation rather than surface-level observations.These dimensions ensure that AI assessments not only align with human judgments but also meet quality standards for thorough, evidence-based novelty evaluation.Our evaluation employs a twostage process to ensure consistency.First, we extract core judgments (key novelty strengths and weaknesses) from human reviews using GPT-4.1 with the prompt in Figure 14.We perform this extraction separately to establish stable reference judgments, as combining extraction with evaluation would risk the LLM identifying different claims across comparisons.In the second stage, we evaluate AI-generated assessments against these preextracted judgments using the prompt in Figure 15.This evaluation quantifies four aspects: (1) judgment similarity, measuring whether the AI identifies the same specific novelty aspects with confidence scores; (2) conclusion alignment, checking whether bottom-line novelty sufficiency verdicts match; (3) prior work engagement, categorized as None, Limited (1-2 citations), or Extensive (3+); and (4) depth of analysis, rated as Surface Level, Moderate (1-2 aspects), or Deep (3+ detailed comparisons).Table 2 reports the resulting alignment scores across these dimensions.</p>
<p>Human Evaluation To validate our automated evaluation, we conduct human evaluation using three PhD students (two third-year, one first-year) specializing in NLP and AI for Science, all with multiple conference publications.The evaluation uses pairwise comparison across the same four dimensions.</p>
<p>Evaluators compare side-by-side novelty assessments from different systems (human ground truth, our approach, or baselines) with each pair representing different system types.We collected 100 total comparisons: 25 overlapping samples per evaluator (for inter-annotator agreement) and 25 unique samples each.</p>
<p>For each dimension, evaluators select: Candidate A wins, Candidate B wins, Tie, or Unclear.A comment box captures specific observations and assumptions, enabling both quantitative preference measurement and qualitative insights.The web interface used for human preference collection can be seen in Figures 6 and 7.</p>
<p>Inter-Rater Reliability Table 6 shows moderate inter-rater agreement (0.493-0.560) with fair kappa scores (0.287-0.368), reflecting the inherent subjectivity in evaluating novelty assessment quality.</p>
<p>Baseline Methods</p>
<p>We compare our approach against three existing systems, adapting each for novelty assessment evaluation.</p>
<p>Scideator (Radensky et al., 2025) Scideator includes a novelty classification module that uses GPT-4o with few-shot examples and task definition to classify ideas as 'novel' or 'not novel'.Originally designed for idea synthesis pipelines where LLMs iteratively refine ideas based on novelty feedback, we adapt it by translating Scideator's "idea" input to "title and abstract" input-recognizing these as the crystallized form of a scientific idea.This preserves Scideator's classification approach while shifting from assessing nascent ideas to evaluating completed scientific contributions.</p>
<p>OpenReviewer (Idahl and Ahmadi, 2025) OpenReviewer generates comprehensive peer reviews using Llama-OpenReviewer-8B, trained on 79,000 expert reviews from top conferences.Since it generates complete reviews rather than targeted novelty assessments, we extract novelty-related content from its outputs using the same LLM-based approach applied to human review normalization as shown in prompt in figure 13.</p>
<p>DeepReviewer (Zhu et al., 2025) DeepReviewer is a multi-stage review framework that combines literature retrieval done with OpenScholar (Asai et al., 2024) with evidence-based argumentation, powered by DeepReviewer-14B trained on structured review annotations.We extract novelty assessments from its outputs using the same LLM-based extraction approach employed for OpenReviewer and human reviews.Notably, DeepReviewer was trained on ICLR 2025 data, which encompasses our entire evaluation dataset-a critical consideration when interpreting its performance.</p>
<p>Results and Analysis</p>
<p>We evaluated each system by comparing its novelty assessments against human novelty assessments as reference.For papers with multiple human reviewers, we also conducted human-vs-human comparisons to establish a baseline.Table 2 presents the overall results.</p>
<p>Overall Performance</p>
<p>Our system significantly outperforms both AI baselines and the human-vs-human baseline across key metrics.For Reasoning Alignment, our system achieves scores that are 44.1 and 35.9 percentage points higher than OpenReviewer (Idahl and Ahmadi, 2025) and DeepReviewer (Zhu et al., 2025), respectively, and 21.4 percentage points above the human baseline.For Conclusion Agreement, our system again leads all three baselines, with the human baseline performing closest at approximately 13 percentage points below our system.Outputs of our pipeline in comparison to the baselines can be seen in Tables 7, 8 and 9.</p>
<p>Sentiment Shift Analysis</p>
<p>We analyze two derived metrics from novelty conclusions.Positive Shift measures assessments changing from neutral/negative to positive sentiment compared to human reference, while Negative Shift measures the opposite direction.Higher Positive Shift indicates overly optimistic assessment, while higher Negative Shift suggests excessive criticism.AI systems generally demonstrate optimistic bias, as evidenced by DeepReviewer's high Positive Shift score.Our system shows lower Positive Shift than DeepReviewer, though OpenReviewer aligns most closely with human rates.For Negative Shift, humans tend to be more critical-a pattern mirrored by OpenReviewer, followed by DeepReviewer.Our approach achieves the lowest Negative   Shift rate.</p>
<p>Depth and Prior Work Engagement</p>
<p>Tables 3 &amp; 4 show our system achieves the highest scores for both dimensions, producing no surfacelevel analyses unlike all baselines.This stems from our specialized multi-step pipeline targeting novelty assessment, while other systems generate complete peer reviews where novelty is a minor component.OpenReviewer performs worst, lacking retrieval and relying on parametric knowledge.Deep-Reviewer uses OpenScholar retrieval but fails at comparative analysis.Human reviewers show high variance, with some engaging extensively while others provide minimal analysis.</p>
<p>Human Evaluation Validation</p>
<p>We conducted human evaluations to validate our LLM-as-Judge evaluation framework.Figures 3 and 4 show the pairwise comparison results.Against OpenReviewer, our system wins 74% of the time.Performance against DeepReviewer and human reviewers is more mixed (39% and 36% win rates), but high tie rates (30% and 41%) indicate many assessments were judged comparable.Loss rates remain low across all comparisons (16-26%).By dimension (Figure 4), Claim Substantiation and Analytical Quality perform best (56% and 55% win rates).Novelty Decision shows the most ties (31%), suggesting different approaches often yield similar conclusions.These patterns align with our automated results, supporting our evaluation approach's validity.</p>
<p>Analysis: Understanding Human Alignment Patterns</p>
<p>Our system's higher agreement scores compared to human-human baselines warrant careful examination.To investigate this, we analyzed papers with multiple human reviewers to understand the sources of disagreement.</p>
<p>Sources of Human Reviewer Variability</p>
<p>Qualitative analysis reveals several factors contributing to reviewer disagreement: Different Evaluation Lenses: Reviewers often focus on different aspects of novelty.In submission Ipe4fMCBXk, half the reviewers emphasized methodological contributions while others focused on application novelty, leading to opposite conclusions from the same paper.Varying Domain Expertise: Reviewers' background knowledge affects assessments.For instance, in a protein design paper, reviewers familiar with the field's history correctly identified prior work on recombination techniques, while others assessed these as novel contributions.Assessment Granularity: Some reviewers provide high-level judgments ("innovative approach") while others focus on specific technical details.This variation in granularity contributes to disagreement even when reviewers might agree on underlying facts.</p>
<p>The Role of Systematic Evaluation</p>
<p>Our system's approach differs from human review in applying consistent evaluation criteria.It evaluates multiple dimensions (methodology, application, prior work) for every paper, maintains uniform depth of analysis across assessments, and applies consistent thresholds for novelty judgments.This systematic approach may explain the alignment patterns: when human reviewers disagree due to focusing on different aspects, our system's comprehensive evaluation can align partially with each perspective.</p>
<p>Component Analysis</p>
<p>Table 5 shows the incremental contribution of each pipeline component.Our human-informed prompt design provides the largest gains (+40.7% reasoning, +46.8% conclusion), reflecting the importance of structured evaluation criteria derived from our human analysis.Structured extraction adds moder-ate improvements (+3.3% reasoning, +4.5% conclusion) but reduces overall computation costs and time by a lot, while landscape analysis contributes minimally (+3.2% reasoning, -0.7% conclusion).</p>
<p>Conclusion</p>
<p>We present a human-informed pipeline for automated novelty assessment in peer review, addressing a critical gap in AI-assisted review systems.</p>
<p>Our approach combines systematic related work retrieval with structured evaluation criteria derived from analysis of expert reviewer patterns.Experimental results demonstrate that our system outperforms existing AI baselines and achieves higher agreement rates than human-human comparisons across key evaluation dimensions.The system produces consistently deep analyses with strong prior work engagement, while human evaluations show our assessments are comparable to human reviews in 41% of cases with low loss rates (14-21%) across all dimensions.These findings highlight both the potential for systematic AI assistance in novelty assessment and the inherent subjectivity in human novelty judgments.Our work provides a foundation for more reliable, transparent novelty evaluation in scientific peer review.</p>
<p>Limitations</p>
<p>Despite achieving strong performance, our system has several important limitations: Evaluation Scope: Our evaluation focuses on computer science papers from ICLR 2025.The system's performance on other scientific domains remains untested and likely requires domain-specific adaptations.</p>
<p>Consistency vs. Diversity: While our analysis shows that systematic evaluation reduces reviewer disagreement, this consistency might eliminate valuable diversity in perspectives.The 35-40% human-human disagreement rate may reflect legitimate differences in expertise and viewpoint rather than mere inconsistency.</p>
<p>Nuanced Novelty: Breakthrough ideas often challenge conventional evaluation criteria.Our system's consistent approach might miss paradigmshifting contributions that human experts would recognize through intuition or deep domain expertise.</p>
<p>Language Scope: Our study evaluates the system only on English-language manuscripts and reviews.As a result, we cannot claim that the ap-proach generalizes to submissions written in other languages or rooted in different academic conventions; assessing cross-lingual performance remains future work.tions (A/B/Tie/Unclear), time spent per evaluation, and comments for flagged cases.</p>
<p>B Output Examples</p>
<p>Output of our pipeline can be seen in Tables 7, 8 and 9.It is quite evident that our system aligns better with the human as compared to the baselines across all four dimensions.The submission overstates its novelty , as the core ideas (token selection, attention sparsity, training-free deployment) are already well-explored , and similar methods (e.g., RazorAttention, PyramidKV, L2 Norm) achieve comparable goals without model changes or fine-tuning.Several highly relevant recent works are omitted from the discussion , and the claims of being the first to balance compression and performance or to preserve long-range dependencies are not substantiated by the literature .</p>
<p>The main technical delta lies in the specific heuristics (CGE, RGL) and their empirical performance, rather than in a conceptual advance .</p>
<p>Reviewers should view IntelLLM as a routine, incremental contribution and may wish to request more comprehensive comparisons and a more accurate positioning within the current research landscape.</p>
<p>Table 7: Full novelty assessments from the human reviewer (reference), the Scideator baseline, and our proposed system.Key phrases are highlighted to show verdict alignment: positive novelty claims , limited/incremental novelty , comparative analysis , and critical issues .</p>
<p>Research Paper Information Extraction Prompt</p>
<p>You are tasked with extracting key information from a research paper for building a knowledge representation.Paper title: {title} Based on the paper content provided below, extract the following information: -"methods": [List of methods/approaches proposed in the paper], -"problems": [List of problems the paper addresses], -"datasets": [List of datasets used for evaluation], -"metrics": [List of evaluation metrics used], -"results": [List of objects with 'metric' and 'value' fields representing key quantitative results], -"novelty_claims":  Overall, the submission's primary strengths lie in evaluation rigor and empirical findings, while its conceptual contributions represent a natural progression of the field rather than a fundamental shift .Novelty Delta Analysis for Reviewer Support -Part 2 5. CRITICAL ASSESSMENT CONSIDERATIONS -Identify aspects where claimed novelty may be overstated -Analyze whether authors' characterizations of their own novelty align with evidence -Consider whether empirical improvements might result from factors other than claimed innovations -Assess whether terminology differences might mask conceptual similarities -Identify instances where "extensions" might be routine adaptations -Note: Frame these as considerations rather than definitive judgments 6. RELATED WORK CONSIDERATIONS -Identify potentially relevant work not addressed in the submission -Highlight areas where additional comparisons are necessary -Note incomplete or potentially misleading characterizations of prior work -Identify when claimed "limitations" of prior work may be exaggerated -Compare how authors cite specific works versus how they actually relate -Note: Present these as information that might help complete the picture 7. KEY OBSERVATION SUMMARY -Highlight the most significant independently verified differences from prior work -Summarize the main relationships to existing research -Identify which claimed contributions have the strongest and weakest differentiation -Note the most important discrepancies between author characterizations and independent assessment -Note: Frame as observations to inform the reviewer's independent judgment ## Evidence Standards For each observation, provide: -Specific references to prior work -Clear distinction between author claims and independently verified differences -Explicit identification of similarities and differences based on technical details -Assessment of whether differences appear substantive or superficial -Analysis of accuracy in how authors characterize related work ## Example Format for Citation Analysis "For [Paper X], the authors characterize it as 'limited to simple datasets' and claim their work 'extends X to complex scenarios.'The citation sentences appear in the following contexts: -'Unlike X, which only works on simple datasets, our approach handles complex scenarios' (Introduction) -'X proposed the basic framework, but did not address challenge Y' (Related Work) Independent analysis suggests that Paper X actually did address complex scenarios in Section 3.2, though using different terminology.The authors' characterization appears to understate X's capabilities to emphasize their contribution.The actual primary difference appears to be [specific technical difference] rather than the complexity of supported scenarios."</p>
<p>Remember that your role is to provide objective analysis that helps reviewers make informed judgments about novelty.Carefully examine both what authors explicitly claim and how they implicitly position their work through their characterizations of prior research.</p>
<p>Figure 1 :
1
Figure 1: Comparison of a surface level human (Top) vs. LLM-written novelty assessment (Bottom).</p>
<p>Figure 2 :
2
Figure 2: Automated novelty assessment pipeline.The system processes manuscripts through three stages: (1) Document Processing extracts content using GROBID, (2) Related Work Discovery identifies and ranks relevant papers via embedding similarity and LLM reranking, and (3) Novelty Assessment performs structured analysis to generate evidence-based novelty evaluations.</p>
<p>Figure 3 :
3
Figure 3: Overall performance comparison between our system and three baseline systems based on human evaluation (n values indicates number of comparisons)</p>
<p>Figure 4 :
4
Figure 4: Performance breakdown across evaluation categories, aggregated across all baseline comparisons.</p>
<p>Figure 5 :
5
Figure 5: Distribution of the number of reviews per paper.Most papers received 1 to 4 reviews.</p>
<p>Figure 6 :
6
Figure 6: Screenshot of the custom-built interface used for human evaluation.Annotators compared AI-generated and human-written novelty assessments across multiple dimensions, including reasoning depth, prior work engagement, and conclusion alignment.</p>
<p>Figure 7 :
7
Figure 7: Screenshot (2) of the custom-built interface used for human evaluation.Annotators compared AI-generated and human-written novelty assessments across multiple dimensions, including reasoning depth, prior work engagement, and conclusion alignment.</p>
<p>Figure 10: Novelty Delta Analysis for Reviewer Support -Part 1</p>
<p>Figure 11 :
11
Figure 11: Novelty Delta Analysis for Reviewer Support -Part 2</p>
<p>Table 1 :
1
Distribution of papers and reviews with novelty discussions by ICLR 2025 decision outcomes opments.To approximate this contextual foundation, we incorporate a landscape analysis step that systematically organizes the previously extracted structured components from retrieved related work.
DecisionPapers Reviews Words/rev Rev/paperNo Decision / Withdrawn5111010022.16Reject811959192.41Accept (Poster)451029622.27Accept (Spotlight)4109972.50Accept (Oral)1211822.00Total1824199592.30</p>
<p>Table 2 :
2
Summary of Reasoning Alignment, Conclusion Agreement, Positive Shift, and Negative Shift Metrics
SystemReasoning Alignment (%↑) Conclusion Agreement (%↑) Positive Shift (%↓) Negative Shift (%↓)OpenReviewer (Idahl and Ahmadi, 2025)42.4 ± 0.3946.8 ± 0.716.3 ± 0.2715.3 ± 0.40DeepReviewer (Zhu et al., 2025)50.6 ± 0.6751.5 ± 1.2421.7 ± 1.899.1 ± 0.00Human vs. Human65.1 ± 1.0562.8 ± 0.406.7 ± 0.7915.0 ± 0.40Scideator (Radensky et al., 2025)23.7 ± 0.0022.4 ± 0.000.0 ± 0.0020.5 ± 0.00Ours86.5 ± 0.2075.3 ± 0.8516.3 ± 1.283.0 ± 0.43</p>
<p>Table 3 :
3
Reasoning Depth Distribution (Percentages)
SystemSurface-Level (%) Moderate (%) Deep (%)OpenReviewer67.431.31.2DeepReviewer43.456.60.0Human vs. Human22.366.211.5Scideator44.954.50.6Ours0.047.952.1SystemNone (%) Limited (%) Extensive (%)OpenReviewer39.953.17.0DeepReviewer24.775.30.0Human vs. Human19.665.215.2Scideator0.075.924.1Ours0.039.160.9Table 4: Prior Work Engagement Distribution (Percent-ages)</p>
<p>Table 5 :
5
Component Analysis: Incremental Contribution of Pipeline Components</p>
<p>Table 6 :
6
Inter-Rater Reliability Metrics Across Categories
CategoryAgreement Kappa ComparisonsNovelty Reasoning Alignment0.5200.34175Novelty Decision Alignment0.5330.34675Claim Substantiation0.4930.28775Analytical Quality0.5600.36875</p>
<p>[Claims about what is novel in this work] Be precise and specific.
Human (Reference)DeepReviewer (Baseline)Ours (Proposed)The paper "DuRND: RewardingThis paper proposesDuRND isfrom Novelty to Contribution fora novel framework, Dual Random,a direct extension of Random Network Dis-,Reinforcement Learning via DualNetworks Distillation (DuRND)tillation (RND)Random Networks Distillation" proposesto address the exploration-exploitation dilemma inintroducing a dual random network architecture to compute both noveltyan extension to classical RNDreinforcement learning by(exploration) and "contribution" (exploitation)by introducing two distinct randomcombining novelty-basedrewards, with a dynamic schedulingnetwork modules-one for statesexploration and value-basedmechanism that shifts emphasis over time.deemed "successful" and anotherexploitation in a unified framework.While the use of random networks for bothfor states associated with "failure."The novelty and contributionsignals isThis innovation allows for therewards in DuRND are based ona novel implementation detail,derivation of both a "novelty" and a "contribution" reward signal, striking a balance between ex-ploratory and exploitative behav-ior While the additional novelty introduced by DuRND is incremental com-pared to classical RND I still believe the contribution is valuable and fills a gap in the current literature. , .prediction errors from random network modules, which may not capture complex state relationships in certain environments, and the reliance on prediction error as a proxy for novelty might be insufficient where the state space has a complex structure or the prediction error does not align with actual information gain. The approach introduces an interesting combination of nov-elty and contribution rewards but relies on computationally efficient heuristics that might not capture the true novelty in complex envi-ronments While the framework demonstrates effectiveness and low computational overhead,.the underlying concepts-combining explo-ration and exploitation signals and dynam-ically adjusting their balance-are well-established in the literature (e.g., SORS, ROSA, Sibling Rivalry, SERENE). The authors accurately position DuRND as a low-overhead, scalable method, but somewhat overstate its conceptual novelty and the dichotomy between prior approaches Empirical results show improved performance . on standard sparse-reward benchmarks, though such gains may be attributable to implemen-. tation choices rather than fundamental inno-vation Overall, DuRND offers a practical and well-engineered architectural , tweak to RND with its main contribution beingthe linear adjustment of weights wards is a heuristic that may not for novelty and contribution re-.tual breakthrough sic reward computation, rather than a concep-a simple, dual-network design for joint intrin-.be optimal or universally applica-bleOverall,the originality lies in the integra-,tion of novelty and value-basedsignalsbut there remainsignificant limitations in the ro-bustness and generalizability ofthe contributiondue to dependency on specificproxy measures and heuristicschedules.Paper content:{abstract}{introduction}Figure 8: Research Paper Information Extraction Prompt</p>
<p>Table 8 :
8
Full novelty assessments from the human reviewer (reference), the DeepReviewer baseline, and our proposed system for the DuRND paper.Key phrases are highlighted to show verdict alignment: novel/valuable aspects , limited/incremental novelty , technical details , and overstated claims .
Human (Reference)OpenReviewer (Baseline)Ours (Proposed)This paper introduces a method forThis paper introducesThis assessment finds that the submissiongenerating image inputs to Visiona novel method of embedding hid-addresses indirect, cross-modal promptLanguage Models (VLMs) thatden meta-instructions in imagesinjection in Visual Language Models (VLMs)embed "meta-instructions," enabling the model to respond to any user query appended to the image with a specified "spin," such as a particular sentiment or language. While the concept of providing meta-instructions through image inputs is not entirely novel-prior work, no-tably Bailey et al. (2023), has ex-plored related prompt injection at-tacks via images this paper offers the most thorough and comprehensive treatment of the subject to date. The general algorithm employed, which involves creating a dataset of input-output pairs and training an image via projected gradient descent to maximize the likelihood over this dataset, closely aligns with the "Behavior Matching" approach described by Bailey et al. (2023) the methodological contribution is incremental rather than fundamen-As such,. .as a way to attack visual language models, which is distinct from traditional jailbreak-ing or adversarial example attacks While the approach is new, the paper could provide more insight into how the proposed attacks compare to existing forms of adversarial attacks on VLMs, highlighting the unique aspects of meta-instruction attacks..by embedding hidden meta-instructions in images, aiming to steer model outputs while preserving image semantics. The work is most closely related to recent studies on ad-versarial image prompting (e.g., Qi et al. 2024, Bagdasaryan et al. 2023) but distinguishes itself through more systematic optimization for semantic preservation and a broader range of meta-instructions beyond jailbreaking The main substantive contributions are a rigorous, multi-metric evaluation of attack effectiveness and semantic preservation and empirical evidence that image-based meta-instructions can be more effective than explicit text prompts. However, the assessment notes that the conceptual advances are incremental as the core idea of cross-modal prompt injection and semantic preservation has been explored in prior work and overstated frame VLM users as victims) are somewhat some novelty claims (e.g., being the first to, . , , , .tally newHowever, the paper distinguishesitself throughits in-depth analysis and experi-,mental resultsparticularly the study of semanticchanges in images resulting fromvarious attacks, with a focus on howmeta-instruction attacks canpreserve the original imagemeaning. The transferabilityexperiments and the breadth ofevaluation across multiple VLMsfurther strengthen the contribution.Overall,the novelty of this work lies less,in the method itself and more inthe scope, depth, and clarity of itsempirical investigationas well as the new insights itprovides into the vulnerabilities ofVLMs to image-based promptinjection. The paper would benefitfrom more explicitlyacknowledging the overlap withprior methods and more clearlyarticulating how its results advancethe understanding of this threatmodel.</p>
<p>Table 9 :
9
Full novelty assessments from the human reviewer (reference), the OpenReviewer baseline, and our proposed system for the Meta-Instructions in VLMs paper.Key phrases are highlighted to show verdict alignment: novel/strength claims , limited/incremental novelty , prior work comparison , and overstated claims .
Research Landscape Analysis# Research Landscape Analysis## TaskAnalyze the collection of research papers provided below to create acomprehensive map of the research landscape they represent. The submissionpaper is the focus of our analysis, and the related papers provide context.## Input FormatYou will be provided with structured information extracted from multipleresearch papers including:-A submission paper that is the focus of our analysis-Multiple related papers that form the research contextEach paper contains:-methods: List of methods/approaches proposed-problems: List of problems addressed-datasets: List of datasets used-metrics: List of evaluation metrics-results: Key quantitative results-novelty_claims: Claims about what is novel in the work## Output FormatProvide a comprehensive analysis with the following sections:1. METHODOLOGICAL LANDSCAPE-Identify and describe the main methodological approaches across the papers-Group similar or related methods into clusters-Highlight methodological trends or patterns-Describe relationships between different methodological approaches2. PROBLEM SPACE MAPPING-Identify the key problems being addressed across the papers-Analyze how different papers approach similar problems-Highlight patterns in problem formulation3. EVALUATION LANDSCAPE-Analyze the common datasets and evaluation methods-Identify patterns in how performance is measured-Compare evaluation approaches across papers4. RESEARCH CLUSTERS-Identify groups of papers that appear closely related-Describe the key characteristics of each cluster-Analyze relationships between clusters5. TECHNICAL EVOLUTION-Identify any visible progression or evolution of ideas-Highlight building blocks and their extensions-Note any competing or complementary approaches## Example Output FormatMETHODOLOGICAL LANDSCAPE-Cluster 1: [Description of similar methods across papers]-Papers X, Y, Z employ transformer-based approaches with variations in...-These methods share characteristics such as...-They differ primarily in...PROBLEM SPACE MAPPING-Problem Area 1: [Description of a common problem addressed]-Papers A, B, C all address this problem but differ in...-The problem is formulated differently in Paper D which focuses on...... [additional sections] ...Ensure your analysis is comprehensive, identifying significant patternsand relationships across the collection of papers.## Papers:{papers}Figure 9: Research Landscape Analysis Prompt
https://github.com/kermitt2/grobid
A Human Evaluation Protocol: Novelty Assessment ComparisonA.1 Task DesignWe conducted a comparative evaluation where human evaluators assessed the quality of AIgenerated novelty assessments against expertwritten reference assessments.Each evaluator pared pairs of AI-generated assessments (labeled A and B) to a human expert's gold-standard novelty review of the same research paper.A.2 Evaluation FrameworkA.2.1 Materials ProvidedFor each evaluation, evaluators received: (1) an expert-written gold-standard novelty review as reference, (2) two novelty assessments (A and B) with system identities hidden.A.2.2 Evaluation DimensionsEvaluators assessed each pair across four dimensions:1. Reasoning Alignment: For each dimension, evaluators selected one of four options: A wins, B wins, Tie (both equally good/poor), or Unclear (cannot determine).A.3 Evaluation GuidelinesA.3.1 Instructions for EvaluatorsEvaluators were instructed to read the reference assessment thoroughly before evaluating A and B, evaluate each dimension independently, and base judgments on substantive content rather than stylistic differences.They allocated 4-7 minutes per example to ensure thorough evaluation and flagged ambiguous cases with explanatory comments when necessary.A.3.2 Evaluation FocusEvaluators were instructed to prioritize substance and accuracy of novelty reasoning, alignment with reference judgments (particularly for Dimensions 1-2), quality and depth of technical analysis (particularly for Dimensions 3-4), and specific evidence and citations supporting claims.They were instructed to disregard writing style, grammar, or formatting differences; suggestions for paper improvement unrelated to novelty; minor phrasing variations with equivalent meaning; and length differences if content quality was comparable.A.4 Implementation DetailsA.4.1 Evaluation PlatformThe evaluation was conducted through a custom web interface presenting materials in a standardized format (see Figures6 and 7).Each evaluator received a unique evaluator ID, 50 randomly assigned paper-assessment pairs, and the ability to save progress and flag unclear cases.A.4.2 Quality ControlWe calculated inter-evaluator agreement using Cohen's kappa reported in Table6.A.4.3 Data CollectionCompleted evaluations were submitted as structured JSON files containing dimension-wise selec-Reviewer Summary PromptSummarize the following assessment in 5 sentences for a reviewer reviewing at an AI conference.## Delta Assessment {novelty_assessment}Figure12: Reviewer Summary Prompt Novelty Assessment Normalization Prompt I'll provide you with a novelty assessment extracted from an academic peer review, along with the full review for context.Please reformat the novelty assessment into a standardized paragraph that begins with a brief description of the paper's contribution before analyzing its novelty.Example of desired format: "This paper presents a method for neural network compression using knowledge distillation with a focus on mobile applications.The approach has limited novelty, as it largely builds upon existing techniques in the literature.While the authors claim their technique is the first to combine layerwise distillation with quantization-aware training, similar combinations have been explored in prior work bySmith et al. (2022)andJones et al. (2023).The main contribution appears to be a specific implementation detail in how gradient flows are managed during the distillation process, but this incremental advance does not significantly push the boundaries of the field.The paper would benefit from more clearly articulating the specific differences from existing approaches to better establish its contribution."Full review (for context): {full_review}Extracted novelty assessment to be reformatted: {novelty_statements} Important guidelines: 1. Begin with a clear description of what the paper presents/proposes (drawn from the full review if needed) 2. Create a cohesive paragraph that flows from describing the contribution to analyzing its novelty 3. Maintain all novelty claims and critiques from the original assessment 4. Preserve references to prior work and comparisons 5. Keep the reviewer's judgment of novelty level 6.Incorporate relevant context from the full review to provide a complete picture of the novelty assessment 7. Follow the structure of the example paragraph: description first, then novelty analysis 8. Preserve all critical analysis regarding limitations or strengths of novelty claims Provide the reformatted novelty assessment: For each judgment, explain why it's considered a core novelty assessment.Provide rationale for your selection of these specific judgments.Reviewer Novelty Evaluation PromptCompare reviewer assessment against reference using these core judgments: Core Judgments: {extracted_core_judgments} Reference: {reference_assessment} Reviewer: {reviewer_assessment} Evaluate three dimensions:1. JUDGMENT SIMILARITY: Do they identify same novelty strengths/weaknesses? -For each core judgment, find corresponding judgment in reviewer assessment -Assess similarity and provide detailed explanation of alignment/differences -Include confidence score for each comparison -If the core judgement is referring to a very specific aspect of the methodology and the reviewer assessment does not mention it, then the core judgment is not similar to the reviewer assessment.DEPTH_OF_ANALYSIS:-Assesses how deeply specific novelty aspects are compared to prior work (SURFACE LEVEL: vague; MODERATE: 1 to 2 aspects; DEEP: 3+ or highly detailed comparisons)Provide explanations for all assessments to support reasoning.
A billion-dollar donation: estimating the cost of researchers' time spent on peer review. Balazs Aczel, Barnabas Szaszi, Alex O Holcombe, 10.1186/s41073-021-00118-2Research Integrity and Peer Review. 61142021</p>
<p>Litllm: A toolkit for scientific literature review. Shubham Agarwal, Gaurav Sahu, Abhay Puri, H Issam, Laradji, D J Krishnamurthy, Jason Dvijotham, Laurent Stanley, Christopher Charlin, Pal, arXiv:2402.017882025Preprint</p>
<p>Openscholar: Synthesizing scientific literature with retrievalaugmented lms. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D , David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, arXiv:2411.141992024Preprint</p>
<p>Has the machine learning review process become more arbitrary as the field has grown? the neurips 2021 consistency experiment. Alina Beygelzimer, Yann N Dauphin, Percy Liang, Jennifer Wortman Vaughan, arXiv:2306.032622023Preprint</p>
<p>Nougat: Neural optical understanding for academic documents. Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic, arXiv:2308.134182023Preprint</p>
<p>Treereview: A dynamic tree of questions framework for deep and efficient llm-based scientific peer review. Yuan Chang, Ziyue Li, Hengyuan Zhang, Yuanbo Kong, Yanru Wu, Zhijiang Guo, Ngai Wong, ArXiv, abs/2506.076422025</p>
<p>Autorev: Automatic peer review system for academic research papers. Maitreya Prafulla Chitale, Ketaki Mangesh Shetye, Harshit Gupta, Manav Chaudhary, Vasudeva Varma, arXiv:2505.143762025arXiv preprint</p>
<p>Marg: Multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.042592024Preprint</p>
<p>Streamlining the review process: Ai-generated annotations in research manuscripts. Oscar Díaz, Xabier Garmendia, Juanan Pereira, 10.48550/arXiv.2412.002812024Preprint available on arXiv</p>
<p>Understanding peer review of software engineering papers. Neil A Ernst, Jeffrey C Carver, Daniel Méndez, Marco Torchiano, Empirical Software Engineering. 262020</p>
<p>Conghui He, Wei Li, Zhenjiang Jin, Chao Xu, Bin Wang, Dahua Lin, arXiv:2407.13773Opendatalab: Empowering general artificial intelligence with open datasets. 2024arXiv preprint</p>
<p>Context rot: How increasing input tokens impacts llm performance. Kelly Hong, Anton Troynikov, Jeff Huber, 2025ChromaTechnical report</p>
<p>The ability of different peer review procedures to flag problematic publications. P J M Serge, Willem Horbach, Halffman, Scientometrics. 1182018</p>
<p>OpenReviewer: A specialized large language model for generating critical scientific paper reviews. Maximilian Idahl, Zahra Ahmadi, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations). the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations)Albuquerque, New MexicoAssociation for Computational Linguistics2025</p>
<p>Thamar Solorio, and 5 others. 2024. What can natural language processing. Ilia Kuznetsov, Osama Mohammed Afzal, Koen Dercksen, Nils Dycke, Alexander Goldberg, Tom Hope, Dirk Hovy, Jonathan K Kummerfeld, Anne Lauscher, Kevin Leyton-Brown, Sheng Lu, Margot Mausam, Aurélie Mieskes, Danish Névéol, Lizhen Pruthi, Roy Qu, Noah A Schwartz, Smith, arXiv:2405.06563Preprint</p>
<p>The rate of growth in scientific publication and the decline in coverage provided by. Peder Olesen, Larsen , Markus Ins, 10.1007/s11192-010-0202-zScience Citation Index. Scientometrics. 8432010</p>
<p>Llm inference serving: Survey of recent advances and opportunities. Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari, IEEE High Performance Extreme Computing Conference (HPEC). 2024a. 2024</p>
<p>Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, Deli Zhao, Yu Rong, arXiv:2410.13185Tian Feng, and Lidong Bing. 2024b. Chain of ideas: Revolutionizing research via novel idea development with llm agents. Preprint</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292The AI Scientist: Towards fully automated open-ended scientific discovery. 2024arXiv preprint</p>
<p>The feasibility of topic-based watermarking on academic peer reviews. Alexander Nemecek, Yuzhou Jiang, Erman Ayday, arXiv:2505.216362025Preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135OpenAI. 2024. Gpt-4.1Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguisticsjune 2024 version. 2002Large language model</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342025Preprint</p>
<p>A review on large language models: Architectures, applications, taxonomies, open issues and challenges. IEEE Access. Mohaimenul Azam Khan Raiaan, Md. Saddam Hossain Mukta, Kaniz Fatema, Nur Mohammad Fahad, Sadman Jashim Sakib, Most. Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami Azam122024</p>
<p>Simra Shahid, Marissa Radensky, Raymond Fok, Pao Siangliulue, Tom Daniel S Weld, Hope, arXiv:2506.22026Literature-grounded novelty assessment of scientific ideas. 2025arXiv preprint</p>
<p>Scirepeval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Instruction distillation makes large language models efficient zero-shot rankers. Weiwei Sun, Zheng Chen, Xinyu Ma, Lingyong Yan, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren, ArXiv, abs/2311.015552023a</p>
<p>Is ChatGPT good at search? investigating large language models as re-ranking agents. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren, 10.18653/v1/2023.emnlp-main.923Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023b</p>
<p>Mineru: An open-source solution for precise document content extraction. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, Conghui He, arXiv:2409.188392024Preprint</p>
<p>Kag-thinker: Interactive thinking and deep reasoning in llms via knowledge-augmented generation. Dalong Zhang, Jun Xu, Jun Zhou, Lei Liang, Lin Yuan, Ling Zhong, Mengshu Sun, Peilong Zhao, Qiwei Wang, Xiaorui Wang, Xinkai Du, Yangyang Hou, Yu Ao, Zhaoyang Wang, Zhengke Gui, Zhiying Yi, Zhongpu Bo, Haofen Wang, Huajun Chen, ArXiv, abs/2506.177282025</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.096752020Preprint</p>
<p>Judging LLM-as-a-judge with MT-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>DeepReview: Improving LLM-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 63rd Annual Meeting of the Association for Computational LinguisticsVienna, AustriaAssociation for Computational Linguistics20251</p>            </div>
        </div>

    </div>
</body>
</html>
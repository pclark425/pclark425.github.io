<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8558 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8558</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8558</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-d8079ae74d4e2c2b803678267ae9bc7a90a82669</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d8079ae74d4e2c2b803678267ae9bc7a90a82669" target="_blank">VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> VGRP-Bench, a Visual Grid Reasoning Puzzle Benchmark featuring 20 diverse puzzles, is introduced and key factors influencing LVLMs' puzzle-solving performance are identified, including the number of clues, grid size, and rule complexity.</p>
                <p><strong>Paper Abstract:</strong> Large Vision-Language Models (LVLMs) struggle with puzzles, which require precise perception, rule comprehension, and logical reasoning. Assessing and enhancing their performance in this domain is crucial, as it reflects their ability to engage in structured reasoning - an essential skill for real-world problem-solving. However, existing benchmarks primarily evaluate pre-trained models without additional training or fine-tuning, often lack a dedicated focus on reasoning, and fail to establish a systematic evaluation framework. To address these limitations, we introduce VGRP-Bench, a Visual Grid Reasoning Puzzle Benchmark featuring 20 diverse puzzles. VGRP-Bench spans multiple difficulty levels, and includes extensive experiments not only on existing chat LVLMs (e.g., GPT-4o), but also on reasoning LVLMs (e.g., Gemini-Thinking). Our results reveal that even the state-of-the-art LVLMs struggle with these puzzles, highlighting fundamental limitations in their puzzle-solving capabilities. Most importantly, through systematic experiments, we identify and analyze key factors influencing LVLMs' puzzle-solving performance, including the number of clues, grid size, and rule complexity. Furthermore, we explore two Supervised Fine-Tuning (SFT) strategies that can be used in post-training: SFT on solutions (S-SFT) and SFT on synthetic reasoning processes (R-SFT). While both methods significantly improve performance on trained puzzles, they exhibit limited generalization to unseen ones. We will release VGRP-Bench to facilitate further research on LVLMs for complex, real-world problem-solving. Project page: https://yufan-ren.com/subpage/VGRP-Bench/.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8558.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8558.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed‑source, state-of-the-art chat-capable large vision-language model evaluated in this paper on a suite of grid-based visual puzzles; used as an off-the-shelf LVLM baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source chat LVLM used via official API; evaluated in both vision (image + prompt) and text-only puzzle settings as an off-the-shelf model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (example: 4x4 text and vision), multiple grid puzzles across VGRP-Bench (e.g., Thermometers, Battle-Ships, Trees-and-Tents, Field-Explore/Minesweeper variants)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzles requiring 2D spatial localization, constraint-satisfaction and multi-step logical deduction (e.g., Sudoku: numeric grid constraints; Thermometers: directional filling; Battle-Ships/Minesweeper: adjacency and spatial pattern reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Models were given a screenshot of the puzzle plus a textual prompt requesting a JSON-formatted perception and solution (or a text-only puzzle description in separate experiments). Chain-of-thought (CoT) prompting was used in many experiments; outputs were post-processed (json-repair and an LLM formatter) to extract structured answers. Evaluation used 5 independent runs × 20 instances = 100 samples per reported setting.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Off-the-shelf behavior (no dynamic solver construction). CoT prompting was tested; S-SFT / R-SFT fine-tuning were applied to other base models but not to GPT-4o in this paper. Output repair tools (json-repair) and an LLM-based formatter (GPT-4o) were used to normalize outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports GPT-4o 'fails to solve a simple 4×4 Sudoku consistently' with a text-only solving rate < 30%; more generally off-the-shelf LVLMs including GPT-4o achieve puzzle-solving success rates substantially below perfect (paper states LVLMs 'struggle' and reports many puzzles with low solving rates; many figures show per-rule puzzle-solving well under 50%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative and quantitative analyses show GPT-4o exhibits perception errors (mislocalizing numbers on the grid) and reasoning state-tracking failures; the paper highlights that perception (vision) harms performance relative to text-only, suggesting spatial perception is a limiting factor for spatial reasoning rather than clear algorithmic spatial reasoning by the model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against other closed-source and open-source LVLMs (Gemini variants, Claude, Qwen family, Llama 3.2, llava). Larger models tend to perform better (GPT-4o > GPT-4o-mini). Closed-source models generally outperform many open-source ones in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Common failure modes documented include grid localization mistakes (misplacing digits), confusing distinct visual components (e.g., cage sums vs cell values in Killer Sudoku), failure to keep track of prior actions (state updates), inability to backtrack after contradictions, repetitive generation, and occasional refusal-to-answer messages claiming inability to process images. GPT-4o specifically failed consistently on simple 4×4 Sudoku (<30% success) even in text-only form.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8558.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8558.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller closed‑source variant of GPT-4o evaluated as an off-the-shelf LVLM baseline on the VGRP-Bench puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source smaller chat LVLM variant used via API; evaluated for perception and puzzle-solving on image and text versions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (text example shown), other grid puzzles in VGRP-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzles requiring spatial localization and constraint reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision + text prompts; chain-of-thought (CoT) prompting used in experiments; outputs expected in structured JSON; 5 runs × 20 instances protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Off-the-shelf LM inference with CoT; no model-specific solver construction or fine-tuning applied to this model in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported to underperform its larger sibling (GPT-4o); example failure cited where GPT-4o-Mini produced a Sudoku row violating constraints ([2,1,4,2]) indicating inability to detect duplication; overall puzzle-solving rates low (figures show mini variant below larger models).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Failure cases reveal inability to maintain global state and detect constraint violations, signaling limited effective spatial/constraint reasoning in these puzzle contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly with GPT-4o (larger model) and other LVLMs; larger model outperforms mini.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Produces invalid final boards that violate Sudoku constraints and fails to backtrack or detect errors; exhibits the general LVLM failure modes described (grid mislocalization, repetitive outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8558.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8558.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.0-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 2.0 - Thinking (gemini-2.0-flash-thinking-exp-01-21)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-oriented closed-source LVLM variant (Gemini-2.0-Thinking) evaluated on VGRP-Bench and reported to perform well among reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.0-Thinking (gemini-2.0-flash-thinking-exp-01-21)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal reasoning LVLM (reasoning-enabled variant of Gemini 2.0) evaluated for perception, cell-level and step-level rule-following and full puzzle solving.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Multiple VGRP-Bench puzzles including Sudoku and others in the taxonomy (e.g., Trees-and-Tents, Thermometers, Battle-Ships).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based visual reasoning requiring 2D spatial understanding and iterative rule application.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Image + text prompts, expected JSON-formatted perception and solution; CoT used in many experiments; 100-sample evaluation protocol (5 runs × 20 instances).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard LVLM inference with chain-of-thought; no dynamic solver constructed for any closed-source models; reasoning-enabled model designed to improve multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported to 'perform well' among reasoning models in the paper (specific per-puzzle numbers are shown in supplementary figures); overall puzzle-solving still far from perfect and below ceiling (many puzzles show low absolute solving rates).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performs better on reasoning / step-following metrics relative to several baselines, suggesting improved handling of multi-step deduction; nevertheless, the paper documents perception-related failures that limit end-to-end spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to other closed-source (GPT-4o, Claude) and open-source (Qwen family, Llama) models; Gemini-2.0-Thinking is singled out as performing well while Qwen-QVQ underperforms relative to Qwen2.5-72B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite being stronger, still suffers from perception errors and state-tracking failures; performance degrades with puzzle difficulty, larger grids, fewer clues, and complex visual components (Thermometers reported near-total failure at medium difficulty across models).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8558.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8558.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 (claude-3-5-sonnet-20241022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source LVLM (Claude 3.5) evaluated and reported as achieving the highest perception and step-level rule-following performance among tested models in several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.5 (claude-3-5-sonnet-20241022)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source LVLM used as an off-the-shelf baseline in vision and text puzzle settings; noted for strong perception accuracy relative to other closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Various VGRP-Bench puzzles including Sudoku, Binairo, and others used for perception, cell-level and step-level evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzles requiring spatial localization, counting, matching, and rule adherence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision + prompt inputs with expected JSON output; experiments include CoT prompting and specialized queries (perception/cell queries, rule-following questions, full-solution generation).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard LVLM inference, CoT prompting for stepwise reasoning where applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Claude is reported to achieve the highest perception and step-level rule-following accuracy among tested models in the paper's figures; absolute puzzle-solving rates remain far from perfect (figures show many tasks with low solving rates).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High step-level accuracy suggests relatively better internalization of rules and ability to follow multi-step procedures; nevertheless vision-driven perception errors remain a limiting factor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms many closed- and open-source LVLMs on perception and rule-following tasks in VGRP-Bench; specific comparisons shown in figures (Claude > Gemini/others on some metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still susceptible to grid localization mistakes and other common LVLM failures; overall puzzle-solving remains challenging at medium/hard difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8558.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8558.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-VL-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-VL-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, large vision-language model (Qwen family) evaluated and reported as the best-performing open-source model for perception among those tested.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-VL-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LVLM (72B variant of Qwen2.5 with vision-language instruction tuning) deployed locally (vLLM) and evaluated on VGRP-Bench for perception, cell-level, step-level and full puzzle solving.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Multiple VGRP-Bench puzzles (e.g., Sudoku, Hitori, Thermometers, Battle-Ships/Field-Explore (mines)), evaluated in vision and text settings.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based visual reasoning requiring 2D spatial perception and constraint reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision + prompt inputs, JSON-formatted expected outputs; CoT used in many experiments; 5 runs × 20 instances evaluation protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard LVLM inference, chain-of-thought prompting applied; no extra dynamic solver construction for off-the-shelf evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Among open-source models, Qwen2.5-72B 'performs the best' on perception metrics according to the paper; absolute metrics are still modest and below closed-source bests (figures show perception and solving accuracies substantially below 100%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Perception strengths relative to other open-source models indicate some capability to extract spatial layout and numeric clues, but subsequent reasoning and state-tracking failures limit full puzzle solving.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to Qwen-QVQ (preview) which underperforms, and to closed-source models where Qwen2.5 trails the top performers; larger models generally do better.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Suffers the common LVLM failure modes (mislocalization, inability to backtrack, repetitive output); vision-based tasks reduce solving rates relative to text-only versions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8558.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8558.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-QVQ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen/QVQ-72B-Preview</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preview reasoning-capable multimodal Qwen variant evaluated; reported to underperform compared to Qwen2.5-72B on these puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-QVQ-72B-Preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Preview reasoning LVLM variant from the Qwen family (preview release) evaluated for VGRP-Bench tasks; deployed via API/preview access where available.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Various VGRP-Bench grid puzzles (same benchmark set as other models).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based visual reasoning requiring spatial localization, counting and constraint propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision + text prompts with expected JSON outputs; CoT setting used for some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Reasoning-enabled LVLM inference; no specialized solver construction used in off-the-shelf experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported to underperform relative to Qwen2.5-72B on perception and puzzle-solving metrics; absolute numbers shown in supplementary figures (overall low solving rates).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Underperformance relative to the Qwen2.5 variant suggests limited effective spatial reasoning in this preview model on these puzzle classes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to other Qwen variants and to closed-source models; Qwen-QVQ performs worse than Qwen2.5-72B and some closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Preview-level issues likely responsible for lower performance; exhibits general LVLM failure modes (misperception, poor state-tracking, inability to backtrack).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8558.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8558.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.2-11B (vision)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.2 11B Vision Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source vision-instruct LLM (11B) used both as an off-the-shelf baseline and as the base model for two supervised fine-tuning interventions (S-SFT and R-SFT) evaluated on VGRP-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.2 11B Vision Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source vision-enabled variant of Llama 3.2 (11B parameters) deployed locally and used as the base for supervised fine-tuning experiments (Solution SFT and Reasoning SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Many VGRP-Bench puzzles (Sudoku, Binairo, Aquarium, Field-Explore/Minesweeper, Thermometers, etc.); used both for off-the-shelf evaluation and as the target of finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based visual reasoning puzzles requiring perceptual extraction of grid state and multi-step spatial/constraint reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision + text prompts; for S-SFT training, solutions converted to JSON strings (e.g., explicit nested lists of final board state) and used as supervision. For R-SFT, synthetic chain-of-thought trajectories were generated by a predefined solver (DFS with process-of-elimination and backtracking) and served as stepwise supervision. Evaluation follows the 5×20 protocol; training used 1 node with 8 A100 GPUs, batch size 4 per GPU, 5 epochs (~24 hours).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>S-SFT: supervised fine-tuning on final solutions (JSON). R-SFT: supervised fine-tuning on synthetic state trajectories produced by a DFS solver (process-of-elimination/backtracking) to teach stepwise reasoning. Data augmentation (affine transforms, Poisson blending) applied to images during training. Post-processing (json-repair and LLM formatter) used at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Pretrained 11B model initially 'fails to produce any correct answers' on easy puzzles. After S-SFT and R-SFT both show 'significant improvement' on the trained (easy-level) puzzles; R-SFT slightly better on some puzzles (e.g., Aquarium, Binairo) while S-SFT better on others. However, both show limited generalization to unseen puzzles (e.g., Sudoku-trained model -> Aquarium yields 0% success). Exact per-puzzle percentages are shown in supplementary figures (improvements large from near-0 baseline at easy level).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>R-SFT explicitly encodes stepwise spatial reasoning (trajectories of board states using DFS/backtracking). Improvements after R-SFT indicate that supervised stepwise trajectories help the model learn some aspects of spatial/stateful reasoning, but failure to generalize demonstrates that learned behavior is often puzzle-specific rather than an abstract spatial reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared pretrained Llama-3.2-11B (pre) vs S-SFT vs R-SFT. Both SFT methods improve over the base model on trained puzzles; neither generalizes well to structurally different puzzles. Also compared to larger closed-source models where, even off-the-shelf, closed-source models often still outperform the small finetuned 11B on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>S-SFT can overfit to training puzzles; R-SFT is susceptible to compounding errors in long reasoning trajectories. Both finetuned models generalize poorly across different puzzle rulesets. Computational constraints limited SFT experiments to 11B; authors note inability to fine-tune larger models due to cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8558.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8558.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llava-onevision-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llava OneVision 7B (llava-hf/llava-onevision-qwen2-7b-ov-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B vision-language model (LlaVA family) evaluated as part of the open-source LVLM baselines on VGRP-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llava-onevision-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 7B LVLM (LlaVA variant) deployed locally for vision-and-language evaluation across the benchmark's puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Various VGRP-Bench puzzles (easy/medium/hard levels where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based visual puzzles requiring 2D perception and constraint reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Vision + text prompts with expected JSON outputs; CoT applied in some experiments; evaluated under same 5×20 run protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Off-the-shelf LVLM inference; no additional solver or SFT applied to this model in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as one of the weaker-performing open-source models on perception and puzzle-solving (LlaVA variants often among the worst performers in reported figures), with low cell-level perception and step-level rule-following accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Poor perception and reasoning performance indicates limited effective spatial reasoning on VGRP-Bench tasks for this model size/family.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against larger open-source models (Qwen2.5-72B, Llama 3.2 90B) and closed-source models; generally underperforms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Frequent output formatting issues (missing quotes, wrong symbols for empty cells), repetitive generation, and general inability to follow multi-step constraints reliably; struggles more in vision than text-only versions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ing-vp: Mllms cannot play easy vision-based games yet <em>(Rating: 2)</em></li>
                <li>Balrog: Benchmarking agentic llm and vlm reasoning on games <em>(Rating: 2)</em></li>
                <li>Zerobench: An impossible visual benchmark for contemporary large multimodal models <em>(Rating: 2)</em></li>
                <li>EnigmaEval: A benchmark of long multimodal reasoning challenges <em>(Rating: 2)</em></li>
                <li>Puzzles: A benchmark for neural algorithmic reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8558",
    "paper_id": "paper-d8079ae74d4e2c2b803678267ae9bc7a90a82669",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A closed‑source, state-of-the-art chat-capable large vision-language model evaluated in this paper on a suite of grid-based visual puzzles; used as an off-the-shelf LVLM baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Closed-source chat LVLM used via official API; evaluated in both vision (image + prompt) and text-only puzzle settings as an off-the-shelf model.",
            "model_size": null,
            "puzzle_name": "Sudoku (example: 4x4 text and vision), multiple grid puzzles across VGRP-Bench (e.g., Thermometers, Battle-Ships, Trees-and-Tents, Field-Explore/Minesweeper variants)",
            "puzzle_type": "Grid-based logic puzzles requiring 2D spatial localization, constraint-satisfaction and multi-step logical deduction (e.g., Sudoku: numeric grid constraints; Thermometers: directional filling; Battle-Ships/Minesweeper: adjacency and spatial pattern reasoning).",
            "task_setup": "Models were given a screenshot of the puzzle plus a textual prompt requesting a JSON-formatted perception and solution (or a text-only puzzle description in separate experiments). Chain-of-thought (CoT) prompting was used in many experiments; outputs were post-processed (json-repair and an LLM formatter) to extract structured answers. Evaluation used 5 independent runs × 20 instances = 100 samples per reported setting.",
            "mechanisms_or_strategies": "Off-the-shelf behavior (no dynamic solver construction). CoT prompting was tested; S-SFT / R-SFT fine-tuning were applied to other base models but not to GPT-4o in this paper. Output repair tools (json-repair) and an LLM-based formatter (GPT-4o) were used to normalize outputs.",
            "performance_metrics": "Paper reports GPT-4o 'fails to solve a simple 4×4 Sudoku consistently' with a text-only solving rate &lt; 30%; more generally off-the-shelf LVLMs including GPT-4o achieve puzzle-solving success rates substantially below perfect (paper states LVLMs 'struggle' and reports many puzzles with low solving rates; many figures show per-rule puzzle-solving well under 50%).",
            "evidence_of_spatial_reasoning": "Qualitative and quantitative analyses show GPT-4o exhibits perception errors (mislocalizing numbers on the grid) and reasoning state-tracking failures; the paper highlights that perception (vision) harms performance relative to text-only, suggesting spatial perception is a limiting factor for spatial reasoning rather than clear algorithmic spatial reasoning by the model.",
            "comparisons": "Compared against other closed-source and open-source LVLMs (Gemini variants, Claude, Qwen family, Llama 3.2, llava). Larger models tend to perform better (GPT-4o &gt; GPT-4o-mini). Closed-source models generally outperform many open-source ones in these tasks.",
            "limitations_or_failure_cases": "Common failure modes documented include grid localization mistakes (misplacing digits), confusing distinct visual components (e.g., cage sums vs cell values in Killer Sudoku), failure to keep track of prior actions (state updates), inability to backtrack after contradictions, repetitive generation, and occasional refusal-to-answer messages claiming inability to process images. GPT-4o specifically failed consistently on simple 4×4 Sudoku (&lt;30% success) even in text-only form.",
            "uuid": "e8558.0",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GPT-4o-mini",
            "name_full": "GPT-4o-mini",
            "brief_description": "A smaller closed‑source variant of GPT-4o evaluated as an off-the-shelf LVLM baseline on the VGRP-Bench puzzles.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini",
            "model_description": "Closed-source smaller chat LVLM variant used via API; evaluated for perception and puzzle-solving on image and text versions.",
            "model_size": null,
            "puzzle_name": "Sudoku (text example shown), other grid puzzles in VGRP-Bench",
            "puzzle_type": "Grid-based logic puzzles requiring spatial localization and constraint reasoning.",
            "task_setup": "Vision + text prompts; chain-of-thought (CoT) prompting used in experiments; outputs expected in structured JSON; 5 runs × 20 instances protocol.",
            "mechanisms_or_strategies": "Off-the-shelf LM inference with CoT; no model-specific solver construction or fine-tuning applied to this model in the paper.",
            "performance_metrics": "Reported to underperform its larger sibling (GPT-4o); example failure cited where GPT-4o-Mini produced a Sudoku row violating constraints ([2,1,4,2]) indicating inability to detect duplication; overall puzzle-solving rates low (figures show mini variant below larger models).",
            "evidence_of_spatial_reasoning": "Failure cases reveal inability to maintain global state and detect constraint violations, signaling limited effective spatial/constraint reasoning in these puzzle contexts.",
            "comparisons": "Compared directly with GPT-4o (larger model) and other LVLMs; larger model outperforms mini.",
            "limitations_or_failure_cases": "Produces invalid final boards that violate Sudoku constraints and fails to backtrack or detect errors; exhibits the general LVLM failure modes described (grid mislocalization, repetitive outputs).",
            "uuid": "e8558.1",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Gemini-2.0-Thinking",
            "name_full": "Gemini 2.0 - Thinking (gemini-2.0-flash-thinking-exp-01-21)",
            "brief_description": "A reasoning-oriented closed-source LVLM variant (Gemini-2.0-Thinking) evaluated on VGRP-Bench and reported to perform well among reasoning models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.0-Thinking (gemini-2.0-flash-thinking-exp-01-21)",
            "model_description": "Closed-source multimodal reasoning LVLM (reasoning-enabled variant of Gemini 2.0) evaluated for perception, cell-level and step-level rule-following and full puzzle solving.",
            "model_size": null,
            "puzzle_name": "Multiple VGRP-Bench puzzles including Sudoku and others in the taxonomy (e.g., Trees-and-Tents, Thermometers, Battle-Ships).",
            "puzzle_type": "Grid-based visual reasoning requiring 2D spatial understanding and iterative rule application.",
            "task_setup": "Image + text prompts, expected JSON-formatted perception and solution; CoT used in many experiments; 100-sample evaluation protocol (5 runs × 20 instances).",
            "mechanisms_or_strategies": "Standard LVLM inference with chain-of-thought; no dynamic solver constructed for any closed-source models; reasoning-enabled model designed to improve multi-step reasoning.",
            "performance_metrics": "Reported to 'perform well' among reasoning models in the paper (specific per-puzzle numbers are shown in supplementary figures); overall puzzle-solving still far from perfect and below ceiling (many puzzles show low absolute solving rates).",
            "evidence_of_spatial_reasoning": "Performs better on reasoning / step-following metrics relative to several baselines, suggesting improved handling of multi-step deduction; nevertheless, the paper documents perception-related failures that limit end-to-end spatial reasoning.",
            "comparisons": "Compared to other closed-source (GPT-4o, Claude) and open-source (Qwen family, Llama) models; Gemini-2.0-Thinking is singled out as performing well while Qwen-QVQ underperforms relative to Qwen2.5-72B.",
            "limitations_or_failure_cases": "Despite being stronger, still suffers from perception errors and state-tracking failures; performance degrades with puzzle difficulty, larger grids, fewer clues, and complex visual components (Thermometers reported near-total failure at medium difficulty across models).",
            "uuid": "e8558.2",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Claude 3.5",
            "name_full": "Claude 3.5 (claude-3-5-sonnet-20241022)",
            "brief_description": "A closed-source LVLM (Claude 3.5) evaluated and reported as achieving the highest perception and step-level rule-following performance among tested models in several tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude 3.5 (claude-3-5-sonnet-20241022)",
            "model_description": "Closed-source LVLM used as an off-the-shelf baseline in vision and text puzzle settings; noted for strong perception accuracy relative to other closed-source models.",
            "model_size": null,
            "puzzle_name": "Various VGRP-Bench puzzles including Sudoku, Binairo, and others used for perception, cell-level and step-level evaluations.",
            "puzzle_type": "Grid-based logic puzzles requiring spatial localization, counting, matching, and rule adherence.",
            "task_setup": "Vision + prompt inputs with expected JSON output; experiments include CoT prompting and specialized queries (perception/cell queries, rule-following questions, full-solution generation).",
            "mechanisms_or_strategies": "Standard LVLM inference, CoT prompting for stepwise reasoning where applied.",
            "performance_metrics": "Claude is reported to achieve the highest perception and step-level rule-following accuracy among tested models in the paper's figures; absolute puzzle-solving rates remain far from perfect (figures show many tasks with low solving rates).",
            "evidence_of_spatial_reasoning": "High step-level accuracy suggests relatively better internalization of rules and ability to follow multi-step procedures; nevertheless vision-driven perception errors remain a limiting factor.",
            "comparisons": "Outperforms many closed- and open-source LVLMs on perception and rule-following tasks in VGRP-Bench; specific comparisons shown in figures (Claude &gt; Gemini/others on some metrics).",
            "limitations_or_failure_cases": "Still susceptible to grid localization mistakes and other common LVLM failures; overall puzzle-solving remains challenging at medium/hard difficulty levels.",
            "uuid": "e8558.3",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Qwen2.5-VL-72B",
            "name_full": "Qwen2.5-VL-72B-Instruct",
            "brief_description": "An open-source, large vision-language model (Qwen family) evaluated and reported as the best-performing open-source model for perception among those tested.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-VL-72B-Instruct",
            "model_description": "Open-source LVLM (72B variant of Qwen2.5 with vision-language instruction tuning) deployed locally (vLLM) and evaluated on VGRP-Bench for perception, cell-level, step-level and full puzzle solving.",
            "model_size": "72B",
            "puzzle_name": "Multiple VGRP-Bench puzzles (e.g., Sudoku, Hitori, Thermometers, Battle-Ships/Field-Explore (mines)), evaluated in vision and text settings.",
            "puzzle_type": "Grid-based visual reasoning requiring 2D spatial perception and constraint reasoning.",
            "task_setup": "Vision + prompt inputs, JSON-formatted expected outputs; CoT used in many experiments; 5 runs × 20 instances evaluation protocol.",
            "mechanisms_or_strategies": "Standard LVLM inference, chain-of-thought prompting applied; no extra dynamic solver construction for off-the-shelf evaluation.",
            "performance_metrics": "Among open-source models, Qwen2.5-72B 'performs the best' on perception metrics according to the paper; absolute metrics are still modest and below closed-source bests (figures show perception and solving accuracies substantially below 100%).",
            "evidence_of_spatial_reasoning": "Perception strengths relative to other open-source models indicate some capability to extract spatial layout and numeric clues, but subsequent reasoning and state-tracking failures limit full puzzle solving.",
            "comparisons": "Compared to Qwen-QVQ (preview) which underperforms, and to closed-source models where Qwen2.5 trails the top performers; larger models generally do better.",
            "limitations_or_failure_cases": "Suffers the common LVLM failure modes (mislocalization, inability to backtrack, repetitive output); vision-based tasks reduce solving rates relative to text-only versions.",
            "uuid": "e8558.4",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Qwen-QVQ",
            "name_full": "Qwen/QVQ-72B-Preview",
            "brief_description": "A preview reasoning-capable multimodal Qwen variant evaluated; reported to underperform compared to Qwen2.5-72B on these puzzles.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-QVQ-72B-Preview",
            "model_description": "Preview reasoning LVLM variant from the Qwen family (preview release) evaluated for VGRP-Bench tasks; deployed via API/preview access where available.",
            "model_size": "72B",
            "puzzle_name": "Various VGRP-Bench grid puzzles (same benchmark set as other models).",
            "puzzle_type": "Grid-based visual reasoning requiring spatial localization, counting and constraint propagation.",
            "task_setup": "Vision + text prompts with expected JSON outputs; CoT setting used for some experiments.",
            "mechanisms_or_strategies": "Reasoning-enabled LVLM inference; no specialized solver construction used in off-the-shelf experiments.",
            "performance_metrics": "Reported to underperform relative to Qwen2.5-72B on perception and puzzle-solving metrics; absolute numbers shown in supplementary figures (overall low solving rates).",
            "evidence_of_spatial_reasoning": "Underperformance relative to the Qwen2.5 variant suggests limited effective spatial reasoning in this preview model on these puzzle classes.",
            "comparisons": "Directly compared to other Qwen variants and to closed-source models; Qwen-QVQ performs worse than Qwen2.5-72B and some closed-source models.",
            "limitations_or_failure_cases": "Preview-level issues likely responsible for lower performance; exhibits general LVLM failure modes (misperception, poor state-tracking, inability to backtrack).",
            "uuid": "e8558.5",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Llama-3.2-11B (vision)",
            "name_full": "Llama 3.2 11B Vision Instruct",
            "brief_description": "An open-source vision-instruct LLM (11B) used both as an off-the-shelf baseline and as the base model for two supervised fine-tuning interventions (S-SFT and R-SFT) evaluated on VGRP-Bench.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 3.2 11B Vision Instruct",
            "model_description": "Open-source vision-enabled variant of Llama 3.2 (11B parameters) deployed locally and used as the base for supervised fine-tuning experiments (Solution SFT and Reasoning SFT).",
            "model_size": "11B",
            "puzzle_name": "Many VGRP-Bench puzzles (Sudoku, Binairo, Aquarium, Field-Explore/Minesweeper, Thermometers, etc.); used both for off-the-shelf evaluation and as the target of finetuning.",
            "puzzle_type": "Grid-based visual reasoning puzzles requiring perceptual extraction of grid state and multi-step spatial/constraint reasoning.",
            "task_setup": "Vision + text prompts; for S-SFT training, solutions converted to JSON strings (e.g., explicit nested lists of final board state) and used as supervision. For R-SFT, synthetic chain-of-thought trajectories were generated by a predefined solver (DFS with process-of-elimination and backtracking) and served as stepwise supervision. Evaluation follows the 5×20 protocol; training used 1 node with 8 A100 GPUs, batch size 4 per GPU, 5 epochs (~24 hours).",
            "mechanisms_or_strategies": "S-SFT: supervised fine-tuning on final solutions (JSON). R-SFT: supervised fine-tuning on synthetic state trajectories produced by a DFS solver (process-of-elimination/backtracking) to teach stepwise reasoning. Data augmentation (affine transforms, Poisson blending) applied to images during training. Post-processing (json-repair and LLM formatter) used at inference.",
            "performance_metrics": "Pretrained 11B model initially 'fails to produce any correct answers' on easy puzzles. After S-SFT and R-SFT both show 'significant improvement' on the trained (easy-level) puzzles; R-SFT slightly better on some puzzles (e.g., Aquarium, Binairo) while S-SFT better on others. However, both show limited generalization to unseen puzzles (e.g., Sudoku-trained model -&gt; Aquarium yields 0% success). Exact per-puzzle percentages are shown in supplementary figures (improvements large from near-0 baseline at easy level).",
            "evidence_of_spatial_reasoning": "R-SFT explicitly encodes stepwise spatial reasoning (trajectories of board states using DFS/backtracking). Improvements after R-SFT indicate that supervised stepwise trajectories help the model learn some aspects of spatial/stateful reasoning, but failure to generalize demonstrates that learned behavior is often puzzle-specific rather than an abstract spatial reasoning capability.",
            "comparisons": "Compared pretrained Llama-3.2-11B (pre) vs S-SFT vs R-SFT. Both SFT methods improve over the base model on trained puzzles; neither generalizes well to structurally different puzzles. Also compared to larger closed-source models where, even off-the-shelf, closed-source models often still outperform the small finetuned 11B on many tasks.",
            "limitations_or_failure_cases": "S-SFT can overfit to training puzzles; R-SFT is susceptible to compounding errors in long reasoning trajectories. Both finetuned models generalize poorly across different puzzle rulesets. Computational constraints limited SFT experiments to 11B; authors note inability to fine-tune larger models due to cost.",
            "uuid": "e8558.6",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "llava-onevision-7B",
            "name_full": "Llava OneVision 7B (llava-hf/llava-onevision-qwen2-7b-ov-hf)",
            "brief_description": "An open-source 7B vision-language model (LlaVA family) evaluated as part of the open-source LVLM baselines on VGRP-Bench.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llava-onevision-7B",
            "model_description": "Open-source 7B LVLM (LlaVA variant) deployed locally for vision-and-language evaluation across the benchmark's puzzles.",
            "model_size": "7B",
            "puzzle_name": "Various VGRP-Bench puzzles (easy/medium/hard levels where applicable).",
            "puzzle_type": "Grid-based visual puzzles requiring 2D perception and constraint reasoning.",
            "task_setup": "Vision + text prompts with expected JSON outputs; CoT applied in some experiments; evaluated under same 5×20 run protocol.",
            "mechanisms_or_strategies": "Off-the-shelf LVLM inference; no additional solver or SFT applied to this model in the reported experiments.",
            "performance_metrics": "Reported as one of the weaker-performing open-source models on perception and puzzle-solving (LlaVA variants often among the worst performers in reported figures), with low cell-level perception and step-level rule-following accuracies.",
            "evidence_of_spatial_reasoning": "Poor perception and reasoning performance indicates limited effective spatial reasoning on VGRP-Bench tasks for this model size/family.",
            "comparisons": "Compared against larger open-source models (Qwen2.5-72B, Llama 3.2 90B) and closed-source models; generally underperforms.",
            "limitations_or_failure_cases": "Frequent output formatting issues (missing quotes, wrong symbols for empty cells), repetitive generation, and general inability to follow multi-step constraints reliably; struggles more in vision than text-only versions.",
            "uuid": "e8558.7",
            "source_info": {
                "paper_title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ing-vp: Mllms cannot play easy vision-based games yet",
            "rating": 2
        },
        {
            "paper_title": "Balrog: Benchmarking agentic llm and vlm reasoning on games",
            "rating": 2
        },
        {
            "paper_title": "Zerobench: An impossible visual benchmark for contemporary large multimodal models",
            "rating": 2
        },
        {
            "paper_title": "EnigmaEval: A benchmark of long multimodal reasoning challenges",
            "rating": 2
        },
        {
            "paper_title": "Puzzles: A benchmark for neural algorithmic reasoning",
            "rating": 1
        }
    ],
    "cost": 0.018594,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models</h1>
<p>Yufan Ren ${ }^{1 *}$ Konstantinos Tertikas ${ }^{2}$ Shalini Maiti ${ }^{3,4}$ Junlin Han ${ }^{3,5}$<br>Tong Zhang ${ }^{1}$ Sabine Süsstrunk ${ }^{1}$ Filippos Kokkinos ${ }^{3}$<br>${ }^{1}$ School of Computer and Communication Sciences, EPFL<br>${ }^{2}$ National and Kapodistrian University of Athens<br>${ }^{3}$ Meta GenAI<br>${ }^{4}$ University College London<br>${ }^{5}$ University of Oxford</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Benchmark Overview. (a) We present a benchmark for Large Vision-Language Models (LVLMs) consisting of 20 diverse visual grid reasoning puzzles (see supplementary material for complete table of per-puzzle examples and descriptions). (b) We evaluate state-of-the-art LVLMs, including closed-source models such as GPT-4o [38] and Gemini [53], open-source models like Llama 3.2 [16], and recently released reasoning models such as Gemini-Thinking, on various aspects, including perception, overall puzzle-solving, and cell-level rule-following. Additionally, to explore potential approaches for improving LVLMs' puzzle-solving abilities, we examine posttraining techniques, including (c) Solution Supervised Fine-Tuning (S-SFT) and (d) Reasoning Supervised Fine-Tuning (R-SFT), where we train on thought trajectories of a predefined solver. (Best viewed on a screen when zoomed-in)</p>
<h1>Abstract</h1>
<p>Large Vision-Language Models (LVLMs) struggle with puzzles, which require precise perception, rule comprehension, and logical reasoning. Assessing and enhancing their performance in this domain is crucial, as it reflects their ability to engage in structured reasoning - an essential skill for real-world problem-solving. However, existing benchmarks primarily evaluate pre-trained models without additional training or fine-tuning, often lack a dedicated focus on reasoning, and fail to establish a systematic evaluation framework. To address these limitations, we introduce VGRP-Bench, a Visual Grid Reasoning Puzzle Benchmark featuring 20 diverse puzzles ${ }^{1}$. VGRP-Bench spans multiple difficulty levels, and includes extensive experiments not only on existing chat LVLMs (e.g., GPT-4o), but also on reasoning LVLMs (e.g., Gemini-Thinking). Our results reveal that even the state-of-the-art LVLMs struggle with these puzzles, highlighting fundamental limitations in their puzzlesolving capabilities. Most importantly, through systematic experiments, we identify and analyze key factors influencing LVLMs' puzzle-solving performance, including the number of clues, grid size, and rule complexity. Furthermore, we explore two Supervised Fine-Tuning (SFT) strategies that can be used in post-training: SFT on solutions (S-SFT) and SFT on synthetic reasoning processes (R-SFT). While both methods significantly improve performance on trained puzzles, they exhibit limited generalization to unseen ones. We will release VGRP-Bench to facilitate further research on LVLMs for complex, real-world problem-solving. Project page: https://yufan-ren.com/subpage/VGRPBench/.</p>
<h2>1. Introduction</h2>
<p>As Large Language Models (LLMs) advance rapidly [12, $21,46,50,55]$, researchers are extending their capabilities to multimodal tasks, leading to the rise of Large VisionLanguage Models (LVLMs) [5, 16, 36, 63, 69]. While LVLMs demonstrate success in some perception tasks, they often face challenges in strategic planning, especially in visual games that require a combination of perception and multi-step reasoning [39, 59, 66].</p>
<p>Among the visual games, grid-like reasoning puzzles, e.g., Sudoku, Futoshiki, and Thermometers, Fig. 1, are renowned for their simple rules yet challenging solutions. They have gained widespread popularity, even being featured in annual world championships [60]. Beyond entertainment, grid puzzles also serve as structured reasoning</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1. VGRP-Bench offers a large puzzle collection for LVLM benchmarking, providing a comprehensive evaluation of state-of-the-art LVLMs across different dimensions, such as perception, rule adherence, and overall puzzle-solving, across different difficulty levels. We also investigate post-training strategies to enhance LVLMs' puzzle-solving performance.
tasks that require logical deduction, constraint satisfaction, and combinatorial search-skills that are fundamental to real-world problem-solving in domains such as robotic path planning [68], automated logistics scheduling [52], and embodied AI control [64]. Their well-defined rules and inherent complexity make them ideal for testing AI system's ability to process structured visual information and adhere to logical constraints. Nevertheless, despite their potential as benchmarks for visual reasoning, there are underused for evaluating LVLMs in existing research.</p>
<p>To address this gap, we introduce the Visual Grid Reasoning Puzzle Benchmark (VGRP-Bench), the largest visual puzzle benchmark to date in terms of puzzle variety and complexity, featuring 20 diverse customizable puzzles that emphasize grid-based visual reasoning and form a taxonomy of rules, attributes, and patterns (Fig. 3). We draw inspiration from popular reasoning puzzles [42-44], and design this benchmark with different levels of difficulty, easy ( ), medium ( ), and hard ( depending on the grid size, the required number of reasoning steps, and the size of the decision space. We conduct extensive experiments evaluating state-of-the-art LVLMs, including their reasoning counterparts, Fig. 5. With our benchmark, we assess several aspects of LVLMs including perception, rule adherence, and overall puzzle-solving capabilities. To separate reasoning and perception, we additionally provide a text version of all puzzles. Through evaluations, we observe that our benchmark poses a huge challenge for most LVLMs, even at the easy level. For instance, GPT-4o fails to solve a simple $4 \times 4$ Sudoku consistently, even in the text-only version of the game ( $&lt;30 \%$ solving rate). We summarize several common failure cases, such as the inability to localize a number on a grid and to correctly keep track of a reasoning process. Moreover, we investigate factors that might impact an LVLM's performance, such as the difficulty level, the grid size, the number of clues, and the rules involved in a puzzle.</p>
<p>Beyond benchmarking off-the-shelf models following other game benchmark papers, we investigate whether posttraining techniques can enhance LVLMs' puzzle-solving abilities (Tab. 1). Specifically, we explore two post-training</p>
<p>strategies, including Solution Supervised Fine-Tuning (SSFT) and Reasoning SFT (R-SFT). In S-SFT, we finetune LVLMs on final solutions, typically represented as nested lists indicating the board’s final state. In R-SFT, inspired by human and algorithmic approaches to puzzle solving [10, 13] such as step-by-step reasoning and process-of-elimination via rule-based deduction, we construct an SFT dataset by recording a solver’s stepwise reasoning trajectory. We then fine-tune the LVLM on this dataset. We observe significant improvement in puzzle solving at the easy level, while fine-tuned models still struggle at the medium and hard levels. Additionally, recognizing the risk of overfitting to the puzzles used for finetuning, we examine the generalization capabilities of models trained with each approach in our benchmark.</p>
<p>In summary, we present a novel, customizable LVLM benchmark tailored for visual reasoning puzzles and conduct a systematic evaluation of LVLMs, as shown in Tab. 1. Our key contributions are as follows:</p>
<ul>
<li>We introduce a large LVLM customizable grid-based reasoning benchmark with systematic evaluation protocols structured around a taxonomy of diverse visual clues and rules.</li>
<li>We conduct extensive experiments on state-of-the-art closed-source and open-source LVLMs using our benchmark, including fine-grained evaluations such as cell-level perception and step-wise rule understanding.</li>
<li>We summarize common failure cases of LVLMs in puzzle solving and provide detailed ablation studies on various factors that impact an LVLM’s puzzle solving, such as difficulty level, number of clues, and rules involved.</li>
<li>To gain deeper insights into the challenges faced by LVLMs in puzzle solving, we explore two post-training strategies: Solution SFT and Reasoning SFT.</li>
</ul>
<h2>2 Related Works</h2>
<h3>2.1 General LLM/LVLM Benchmarks</h3>
<p>The advanced capabilities of Large Language Models (LLMs) [1, 2, 53, 54] and Large Vision-Language Models (LVLMs) [30–32, 35] have inspired extensive research on benchmarking their capabilities. Prominent benchmarks like SuperGLUE [55], MMLU [21], and BigBench [50], evaluate general language understanding and multitasking text-based capabilities. Domain-specific benchmarks evaluate specialized competencies such as coding [3, 37] and mathematics [12, 22]. Notable early examples include Science QA [34], VizWiz [8], and VQAv2 [19]. Specific domains, such as image captioning, are represented by works such as [29]. More recent efforts [67], such as MMBench [33], EMMA [20], and SEED-Bench [27], offer comprehensive evaluations of multimodal reasoning and perception. BLINK [18] focuses on visual perception tasks</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Result Summary on Easy Level. Puzzle-solving rate of state-of-the-art chat LVLMs on easy-level puzzles associated with each rule. Please refer to the experiment section for detailed result analysis. Note that this plot’s score ranges from 0 to 45%, instead of 100%. (Best viewed on a screen when zoomed in)</p>
<p>that humans can solve in an instant. LMEvalKit [15] unifies model comparisons across various benchmarks.</p>
<p>Our VGRP-Bench differs from other benchmarks by focusing on reasoning puzzles, a special challenge to LVLMs that requires combining perception and decision making with multi-step reasoning.</p>
<h3>2.2 LLM/LVLM Game Benchmarks</h3>
<p>Challenging games have long been regarded as milestones of machine intelligence as exemplified by Deep Blue [24] and AlphaGo [49]. Classical benchmarks, such as Atari [48] and the Arcade Learning Environment [7], have played a crucial role in developing reinforcement learning algorithms and improving agent capabilities. Given the natural language capabilities of LLMs, researchers have introduced benchmarks where LLM agents interact within game environments [40, 61]. [9, 23, 45, 51, 57] investigate LLMs’ performance in agent-based and collaborative game environments, emphasizing interaction and teamwork skills.</p>
<p>Several recent studies benchmark LVLMs on visual games. ING-VP [66] shows that LVLMs still struggle with easy games. [59] proposes a benchmark with fine-grained evaluation. BALROG [39] measures LVLM games like MiniHack and NetHack. [17] proposed a puzzle RL environment, and benchmark several RL algorithms. ZeroBench [47] proposes a benchmark in which current LVLMs struggle to achieve meaningful accuracy. A concurrent work, [56], created a visual benchmark by scraping existing puzzles from online sources, resulting in a dataset of 949 instances of puzzles.</p>
<p>VGRP-Bench distinguishes itself by focusing on reasoning puzzles, employing customizable puzzle generators, and systematically evaluating models from inference to post-training techniques.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Benchmark Games: Primitives and Sample Questions. we systematically define puzzle primitives, including conditions, constraints, variables, and states, to establish a unified framework for inference and evaluation (left). This benchmark includes tasks designed to evaluate the reasoning, rule-following, and perception capabilities of state-of-the-art LVLMs. (Best viewed on a screen when zoomed in)</p>
<h2>3. VGRP-Bench: The Benchmark</h2>
<p>This section is organized as follows: we first present our benchmark in Sec. 3.1, along with its evaluation protocol in Sec. 3.2 and taxonomy in Sec. 3.3. In addition to benchmarking off-the-shelf models, we investigate the challenges faced by existing LVLMs in solving visual puzzles and propose strategies to address these limitations. Specifically, we use two fine-tuning strategies, Solution Supervised Fine-Tuning (S-SFT) and Reasoning SFT (R-SFT), as described in detail in Sec. 3.4.</p>
<h3>3.1. Grid-Like Visual Reasoning Puzzles</h3>
<p><strong>Puzzle Selection.</strong> To form this benchmark, we select visual puzzle games based on the following criteria: requiring multi-step reasoning for decision-making and rule validation, incorporating a diverse range of visual clues, rules and interaction methods, and ultimately contributing to a structured taxonomy (Fig. 4). For example, vanilla Sudoku is purely numerical and relies on repetition-based constraints, while Trees-and-Tents demands pattern recognition, relational reasoning between trees and tents, and checking 1-to-1 matching. In contrast, Thermometers relies heavily on understanding and applying physical-world rules, e.g., thermometers must be filled starting from their base.</p>
<p>Puzzle Primitives. To ensure consistency across different puzzles and facilitate future integration of new ones, we design the benchmark around four core primitives—variables, states, constraints, and conditions—to provide a unified structure, as depicted in Fig. 4 left. Variables <em>V</em> and States <em>S</em>. Each puzzle consists of a set of variables, <em>V</em> = {<em>v</em><sub><em>i</em></sub>}<em>v</em><sub><em>i</em>−1</sub>, representing cells or elements requiring value assignments. For example, a 4 × 4 <em>Sudoku</em> grid comprises 16 variables, with each variable taking a value from the set of possible values {1, 2, 3, 4}. The set of states <em>S</em> = {<em>s</em><sub><em>i</em></sub>}<em>v</em><sub><em>i</em>−1</sub> represents the current value assignments of the variables. Constraints. Constraints <em>C</em> = {<em>c</em><sub><em>j</em></sub>}<em>v</em><sub><em>j</em>−1</sub> define rules for valid puzzle state configurations. For instance, in <em>Sudoku</em>, constraints enforce the non-repetition of values in each row, column, and block. In <em>Trees and Tents</em>, constraints enforce a bijective mapping between trees and tents while adhering to row and column sums. Conditions. Conditions correspond to preset values or clues that define the puzzle's starting state. Examples include predefined digits that act as initial clues in <em>Sudoku</em> or row and column constraints given as clues in <em>Thermometers</em>.</p>
<h3>3.2. Evaluation Protocol</h3>
<p>Our benchmark evaluates LVLM performance across several capabilities, including perception, rule-following, and reasoning tasks at multiple granular levels, and on difficulty</p>
<p><sup>2</sup>Here, Sudoku serves as an example of puzzles that could be easily converted to text, owing to its widespread popularity, while Trees-and-</p>
<p>Tents and Thermometers represent puzzles harder to convert to text.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Diverse Rules and Visual Patterns in VGRP-Bench. Our benchmark includes a diverse set of rules, such as counting and mathematical calculations, and also exhibits diversity in visual patterns, encompassing text, numerical values, and objects such as trees. We highlight puzzles that are easy or difficult to convert into text.</p>
<p>levels, as illustrated in the right column of Fig. 4. Specifically, at the puzzle-solving level, we assess overall perception accuracy and puzzle-solving success rate by evaluating the LVLM's holistic understanding of the board and its ability to generate a correct solution. Moreover, we provide additional evaluations at finer levels of granularity, including evaluations at the cell and step level.</p>
<h3>3.3. Puzzle Rule/Capability Taxonomy</h3>
<p>We create a taxonomy of rule/capabilities required to solve the puzzles in our benchmark, and visualize the prominent ones in Fig. 4, as one puzzle might require multiple capabilities like counting, a basic rule in most puzzles. For example, Killer-Sudoku, Kakuro, Kakurasu, and Renzoku require mathematical calculations involving addition and subtraction. Trees-and-Tents, requiring the LVLM to understand bijective matching of trees and tents, is an example the matching rule of associating spatially or semantically relevant components. Other rules and capabilities are numerical comparison, understanding procedural order (unidirectionality) and putting connected components together.</p>
<h3>3.4. Post-Training Techniques</h3>
<p>Beyond assessing off-the-shelf LVLMs, we would like to take a step further to explore potential approaches to boost their performance. In this subsection, we utilize two post-training methods to tune a pretrained LVLM, i.e., Solution Supervised Fine-Tuning (S-SFT) and Reasoning Supervised Fine-Tuning (R-SFT).</p>
<p>S-SFT. A baseline is to use Supervised Fine-Tuning. Here, we adopt two strategies. First, we adopt a naive SFT for supervision of the LVLM to generate solutions. More specifically, we first convert the solution into a JSON-formatted text file, "{"answer": [[1, 2, 3, 4], [3, 4, 1, 2], [2, 1, 4, 3], [4, 3, 2, 1]]}". During training, we provide a text puzzle description as prompt and a screenshot of the puzzle as input. Then we use the predefined solution as supervision for the model.</p>
<p>R-SFT. We introduce a SFT data creation method specific for puzzle solving. Inspired by human and algorithmic puzzle solving that feature step-by-step reasoning and per-cell rule violation checking, we propose to conduct supervised Fine-Tuning (SFT) on synthetic trajectories. In this way, we would like to supervise LVLMs to imitate step-by-step reasoning, in a similar manner to how a predefined solver solves these puzzles. To generate thought trajectories, we define the reasoning process as a trajectory through states. A Trajectory, T = {s_i}_i=1, encodes key intermediate states encountered during puzzle solving. Each state s_t captures variable assignments and potential values for unassigned variables. To avoid the inefficiency of starting from a random cell, Depth-First Search (DFS) with process-of-elimination is employed, enabling systematic exploration and backtracking upon failure states. For instance, in a 4×4 Sudoku with 12 missing values, a random start often leads to excessive branching, producing trajectories that exceed the model's output window.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Off-the-Shelf LVLMs on Level-Easy with CoT. We report both correct perception rate and puzzle-solving rate evaluations with closed-source / open-source and reasoning / chat models. Please refer to supplementary for additional evaluations such as finer granularity evaluations and other difficulty levels, e.g., medium and hard (Puzzle-solving in hatched bars and best viewed on a screen when zoomed in)</p>
<h2>4. Experiments</h2>
<h3>4.1. Implementation Details</h3>
<p>We benchmark several state-of-the-art LVLMs. For accessibility purposes, we include both closed-source and open-source models like Gemini-Pro [53] and LlaVA-OneVision-7B [28] respectively. To assess different types of models, we include both chat LVLMs and reasoning LVLMs ${ }^{3}$. For</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>evaluation, we launch 5 independent inference runs, with each run containing 20 instances, resulting in a total of 100 samples. We report the overall mean correctness and standard deviation across all sample runs. For post-training, we use Llama 3.2 Vision Instruct as the base model and conduct training on a single node equipped with 8 A100 GPUs. We ensure that the training and test splits contain no overlapping puzzles in terms of input or solution. Please refer to supplementary for more implementation details.</p>
<h3>4.2 Off-the-Shelf LVLMs Evaluation</h3>
<p>We present the overall perception and puzzle-solving results in Fig. 5, where all LVLMs struggle with puzzlesolving, achieving a success rate below 80%. Additional granularity and evaluation results are discussed below, and the complete evaluation on all puzzles can be found in the supplementary material. More specifically, regarding perception, most closed-source models, except for Claude, achieve less than 50% accuracy. Among open-source models, Qwen2.5-72B performs the best. Hitori exhibits the highest perception accuracy among all puzzles, suggesting that LVLMs struggle with grids containing missing cells. Secondly, in terms of puzzle-solving, though all models struggle, closed-source models generally outperform open-source ones. We also observe that larger models tend to perform better; for example, GPT-4o outperforms GPT-4o-mini. For reasoning models, we find that Gemini-2.0-Thinking performs well, whereas Qwen-QVQ underperforms compared to Qwen2.5-72B, potentially because Qwen-QVQ is a preview version.</p>
<p>Cell-Level Evaluation. We provide cell-level perception evaluation in Fig. 6. Similar to overall perception, closed-source models—particularly Claude and Gemini 2.0-Flash—generally achieve the highest performance. Interestingly, we notice cases when querying the LVLM for the entire board yields the correct answer, whereas querying a specific cell results in an incorrect response. This phenomenon mirrors previously observed failures in LVLMs, such as their struggles with counting tasks like "How many R's are in the word Strawberry" [62].</p>
<p>Step-Level Rule-Following Evaluation. Claude consistently achieves the highest performance, whereas LlaVA performs the worst among all models. Among the four puzzles shown in Fig. 7, Sudoku attains the highest accuracy, aligning with the intuition that it is a widely recognized puzzle with relatively simple and well-defined rules compared to the others.</p>
<p>Text Puzzles Evaluation. To understand the reasoning challenges in the text domain, we present the results of off-the-shelf models using text input in Fig. 8. Notably, while this setting eliminates vision-related losses, the puzzles remain challenging for LVLMs.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Cell-level Perception Accuracy at Level-Easy (Best viewed on a screen when zoomed in)</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Step-Level Rule-Following Accuracy at Level-Easy (Best viewed on a screen when zoomed in)</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Performance of Text Version Puzzles on Level-Easy. For the text version of puzzles, the puzzle-solving rate increases significantly compared to the vision-based setting, highlighting the challenge of visual perception in our benchmark. (Best viewed on a screen when zoomed in)</p>
<p>Puzzle Taxonomy Analysis. The diversity of puzzles and rule types in our benchmark enables analysis through the lens of puzzle taxonomy, making it a key differentiator from other existing benchmarks. Each category includes at least two puzzles. For example, both Field-Explore and Trees-and-Tents require matching and pairing components. We</p>
<p>present results aggregated by puzzle taxonomy in Fig. 2.
Effect of Difficulty Level. As difficulty increases, reflected in larger grids and more steps required to complete the puzzle-accuracy declines in both perception and puzzlesolving (Fig. 10). Notably, at the medium difficulty level with Thermometers, all LVLMs achieve a perception accuracy below $5 \%$ and fail to solve the puzzles completely. Performance further deteriorates at the hard difficulty level, indicating significant limitations in handling complex puzzles.
Effect of Clue Number. Intuitively, providing more clues simplifies the puzzles, leading to improved performance. This trend is evident in Fig. 9, where we also observe a corresponding increase in perception accuracy.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Results with Different Number of Clues on LevelEasy (3. When more clues are provided (to the right), puzzles become easier, resulting in a higher puzzle-solving rate. (Best viewed on a screen when zoomed in)</p>
<p>Common Failure Patterns. Off-the-shelf chat models exhibit several common failure cases. For instance, chat LVLMs often struggle to localize values on a grid, misinterpreting sequences like [<em>, 2, </em>, ] as [, <em>, 2, </em>]. Additionally, they frequently misunderstand the roles of different components, such as mistaking a cage clue for a board number in Killer Sudoku, and they tend to repeat responses. Extensive sample outputs and common failure cases are provided in the supplementary material.</p>
<h3>4.3. Post-Training Evaluation</h3>
<p>We compare the pre-trained Llama 3.2 model with its finetuned versions after S-SFT and R-SFT in Fig. 11, with additional details provided in the supplementary material. First, we observe that both S-SFT and R-SFT significantly enhance performance, as the pre-trained model initially fails to produce any correct answers. This suggests that generalization to new puzzle settings is feasible. Comparing S-SFT and R-SFT, their effectiveness varies across puzzles: S-SFT outperforms R-SFT in some cases, whereas R-SFT excels in others such as Aquarium. We hypothesize that this is because R-SFT receives more supervision but is also more susceptible to compounding errors in long reasoning trajec-
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Off-the-Shelf LVLMs on Level-Medium (top row) and Hard (bottom row) with CoT. (Best viewed on a screen when zoomed in)
tories. We provide an evaluation on cross-puzzle generalization in the supplementary material.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Comparing S-SFT and R-SFT on Level-Easy Both S-SFT and R-SFT significantly improve the pretrained model's performance in perception and puzzle-solving, with RSFT achieves slightly better results in a few puzzles such as Binairo, while being lower in puzzles like Field-Explore. (Puzzlesolving in hatched and best viewed on a screen when zoomed in)</p>
<h2>5. Limitations and Future Work</h2>
<p>Due to the high computational cost of fine-tuning large models (e.g., 70B parameter models), our SFT experiments are limited to smaller 11B models. Future research could explore inference-time strategies, including Monte Carlo Tree Search [49]. Another promising direction is to enhance puzzle-solving performance by integrating RL with outcome-based reward models. We report preliminary findings in the supplementary material.</p>
<h2>6. Conclusion</h2>
<p>In this work, we have introduced VGRP-Bench, a large visual grid puzzle benchmark with various setting, including difficulty levels and diversified puzzle rules, and systematic evaluation. We evaluated off-the-shelf LVLMs on our VGRP-Bench showing their inability of puzzle solving. Furthermore, we explore post-training for improving LVLM performance, revealing significant improvement on the trained puzzle but also a lack of generalization to unseen ones. We hope this benchmark inspires future research and advances LVLM studies for complex, real-world tasks.</p>
<h2>References</h2>
<p>[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3
[2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. 3
[3] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan Kumar Gonugondla, Hantian Ding, Varun Kumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng Qian, Murali Krishna Ramanathan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, and Bing Xiang. Multi-lingual evaluation of code generation models. 2022. 3
[4] Stefano Baccianella. JSON Repair - A python module to repair invalid JSON, commonly used to parse the output of LLMs, 2024. 3
[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2
[6] Evan Becker and Stefano Soatto. Cycles of thought: Measuring llm confidence through stable explanations. arXiv preprint arXiv:2406.03441, 2024. 4
[7] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013. 3
[8] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333-342, 2010. 3
[9] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2(4):6, 2023. 3
[10] Eric C Chi and Kenneth Lange. Techniques for solving sudoku puzzles. arXiv preprint arXiv:1203.2295, 2012. 3
[11] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: A comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. 5
[12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 2, 3
[13] Carlos F Daganzo. Minuet: A method to solve sudoku puzzles by hand. arXiv preprint arXiv:1812.06778, 2018. 3
[14] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 6
[15] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In ACMMM, pages 11198-11201, 2024. 3
[16] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1, 2
[17] Benjamin Estermann, Luca Lanzendörfer, Yannick Niedermayr, and Roger Wattenhofer. Puzzles: A benchmark for neural algorithmic reasoning. NIPS, 37:127059-127098, 2025. 3
[18] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 3
[19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, pages 6904-6913, 2017. 3
[20] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. 3
[21] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 2, 3
[22] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 3
[23] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023. 3
[24] Feng-Hsiung Hsu. Behind Deep Blue: Building the computer that defeated the world chess champion. Princeton University Press, 2022. 3
[25] Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When can llms actually correct their own mistakes? a critical survey of self-correction of llms. Transactions of the Association for Computational Linguistics, 12:1417-1440, 2024. 4
[26] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 3
[27] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 3
[28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 6
[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In $E C C V$, pages 740-755. Springer, 2014. 3
[30] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 3
[31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
[32] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 3
[33] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, pages 216-233. Springer, 2025. 3
[34] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. NIPS, 35: 2507-2521, 2022. 3
[35] Meta AI Research. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. Technical report, Meta AI, 2024. [Online; accessed 10-Jan-2025]. 3
[36] Cade Metz. Openai unveils new ai model with advanced math and science capabilities. The New York Times, 2024. 2
[37] Jain Naman, Han King, Gu Alex, Li Wen-Ding, Yan Fanjia, Zhang Tianjun, Wang Sida, Solar-Lezama Armando, Sen Koushik, and Stoica Ion. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint, 2024. 3
[38] OpenAI. Hello gpt-4o, 2024. Accessed: 2024-12-20. 1
[39] Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kuciński, Lerrel Pinto, Rob Fergus, et al. Balrog: Benchmarking agentic llm and vlm reasoning on games. arXiv preprint arXiv:2411.13543, 2024. 2, 3
[40] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 1-22, 2023. 3
[41] Patrick Pérez, Michel Gangnet, and Andrew Blake. Poisson image editing. TOG, 22(3):313-318, 2003. 3
[42] Puzzle Battleships. Battleships - online puzzle game. https://www.puzzle-battleships.com/. Accessed: 2025-01-17. 2
[43] Puzzlemix. Free puzzles to play online. https://www. puzzlemix.com/menu.php. Accessed: 2025-01-17.</p>
<p>[44] Puzzler Media. Online puzzles, brain teasers and games. https://www.puzzler.com/online-puzzles. Accessed: 2025-01-17. 2
[45] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 6(3), 2023. 3
[46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 2
[47] Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, et al. Zerobench: An impossible visual benchmark for contemporary large multimodal models. arXiv preprint arXiv:2502.09696, 2025. 3
[48] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020. 3
[49] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016. 3, 8
[50] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià GarrigaAlonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. 2, 3
[51] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards general computer control: A multimodal agent for red dead redemption ii as a case study. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. 3
[52] Paul Mingzheng Tang, Kenji Kah Hoe Leong, Nowshad Shaik, and Hoong Chuin Lau. Automated conversion of static to dynamic scheduler via natural language. arXiv preprint arXiv:2405.06697, 2024. 2
[53] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 3, 6
[54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3
[55] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for generalpurpose language understanding systems. NIPS, 32, 2019. 2,3
[56] Clinton J Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, and Dan Hendrycks. Enigmaeval: A benchmark of long multimodal reasoning challenges. arXiv preprint arXiv:2502.08859, 2025. 3
[57] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. 3
[58] Weichuan Wang, Zhaoyi Li, Defu Lian, Chen Ma, Linqi Song, and Ying Wei. Mitigating the language mismatch and repetition issues in llm-based machine translation via model editing. arXiv preprint arXiv:2410.07054, 2024. 4
[59] Xinyu Wang, Bohan Zhuang, and Qi Wu. Are large vision language models good game players? In The Thirteenth International Conference on Learning Representations, 2025. 2, 3
[60] Wikipedia contributors. World puzzle championship - Wikipedia, the free encyclopedia. https : / /en.wikipedia.org/wiki/World_Puzzle_ Championship, 2024. [Online; accessed 6-Dec-2024]. 2
[61] Yue Wu, Xuan Tang, Tom M Mitchell, and Yuanzhi Li. Smartplay: A benchmark for llms as intelligent agents. arXiv preprint arXiv:2310.01557, 2023. 3
[62] Nan Xu and Xuezhe Ma. Llm the genius paradox: A linguistic and math expert's struggle with simple word-based counting problems. arXiv preprint arXiv:2410.14166, 2024. 7
[63] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang, Zhiyong Wang, Lu Sheng, Lei Bai, et al. Lamm: Language-assisted multimodal instruction-tuning dataset, framework, and benchmark. NIPS, 36, 2024. 2
[64] Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693, 2024. 2
[65] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In NIPS, 2024. 3
[66] Haoran Zhang, Hangyu Guo, Shuyue Guo, Meng Cao, Wenhao Huang, Jiaheng Liu, and Ge Zhang. Ing-vp: Mllms cannot play easy vision-based games yet. arXiv preprint arXiv:2410.06555, 2024. 2, 3
[67] Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, and Ranjay Krishna. Task me anything. arXiv preprint arXiv:2406.11775, 2024. 3
[68] Hongyou Zhou, Ingmar Schubert, Marc Toussaint, and Oegur S Oguz. Spatial reasoning via deep vision models for robotic sequential manipulation. In IROS, pages 1132811335. IEEE, 2023. 2
[69] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2</p>
<h1>Contents</h1>
<ol>
<li>Introduction ..... 2</li>
<li>Related Works ..... 3
2.1. General LLM/LVLM Benchmarks ..... 3
2.2. LLM/LVLM Game Benchmarks ..... 3</li>
<li>VGRP-Bench: The Benchmark ..... 4
3.1. Grid-Like Visual Reasoning Puzzles ..... 4
3.2. Evaluation Protocol ..... 4
3.3. Puzzle Rule/Capability Taxonomy ..... 5
3.4. Post-Training Techniques ..... 5</li>
<li>Experiments ..... 6
4.1. Implementation Details ..... 6
4.2. Off-the-Shelf LVLMs Evaluation ..... 7
4.3. Post-Training Evaluation ..... 8</li>
<li>Limitations and Future Work ..... 8</li>
<li>Conclusion ..... 9</li>
<li>Versions of Models Used ..... 3</li>
<li>Additional Evaluation Details ..... 3</li>
<li>Additional Implementation details ..... 3
10Image Augmentations ..... 3
11Common Mistakes of LVLMs in Puzzle Solving ..... 4
11.1Common Mistakes in LVLMs' Perception ..... 4
11.2Common Mistakes in Pre-Trained LVLMs' Puzzle-Solving ..... 4
12Generalization Capability of SFT Models ..... 5
13Reinforcement Learning with Text Input ..... 5
Visualizations ..... 6
Per-Puzzle Examples of Easy, Medium, and Hard Levels ..... 6
Visualization: Per-Puzzle Examples and Query Templates ..... 9
Results ..... 13
Medium and Hard Level Overall Evaluation of Off-the-Shelf Models (w/ CoT) ..... 13
Easy, Medium and Hard Overall Level Evaluation of Off-the-Shelf Models (w/o CoT) ..... 15
Easy, Medium and Hard Level Cell-Level Perception Evaluation of Off-the-Shelf Models ..... 18
Easy, Medium and Hard Level Rule Following Evaluation of Off-the-Shelf Models ..... 21
Easy Level Overall Evaluation of Off-the-Shelf Models v.s. Clue Number (w/ CoT) ..... 24
Easy Level Cell-Level Perception Evaluation of Off-the-Shelf Models v.s. Clue Number ..... 25
Easy and Medium Level Overall Evaluation of Off-the-Shelf Models (w/ CoT, Text Version) ..... 26
Easy, Medium, and Hard Level Overall Evaluation of SFT Models ..... 28</li>
</ol>
<p>Qualitative Studies ..... 31
Sudoku - Easy - W/ CoT - Vision Input ..... 31
Sudoku - Easy - W/ CoT - Text Input ..... 43
Battle-Ships - Easy - W/ CoT - Vision Input ..... 53
Field-Explore - Medium - W/ CoT - Vision Input ..... 64
Llama 3.2 Instruction Vision after Reasoning-SFT (Successful Example) ..... 81
Llama 3.2 Instruction Vision after Reasoning-SFT (Failure Example) ..... 82
Reinforcement Learning Training Process with Text Input ..... 83</p>
<h1>7. Versions of Models Used</h1>
<p>Since off-the-shelf models may exist in multiple versions even under the same name, we provide the specific version numbers for the models used, where applicable, in the list below.</p>
<ul>
<li>Gemini 2.0 Flash Thinking: gemini-2.0-flash-thinking-exp-01-21</li>
<li>QVQ-72B: Qwen/QVQ-72B-Preview</li>
<li>Claude 3.5: claude-3-5-sonnet-20241022</li>
<li>Gemini 2.0 Flash: gemini-2.0-flash</li>
<li>Gemini 1.5 Pro: gemini-1.5-pro</li>
<li>Gemini 1.5 Flash: gemini-1.5-flash</li>
<li>GPT-4o: gpt-4o-2024-08-06</li>
<li>GPT-4o-mini: gpt-4o-mini-2024-07-18</li>
<li>Qwen2.5-VL-72B-it: Qwen/Qwen2.5-VL-72B-Instruct</li>
<li>Qwen2.5-VL-7B-it: Qwen/Qwen2.5-VL-7B-Instruct</li>
<li>Llama 3.2 90B it: Llama-3.2-90B-Vision-Instruct</li>
<li>Llama 3.2 11B it: Llama-3.2-11B-Vision-Instruct</li>
<li>Qwen2-VL-7B-it: Qwen/Qwen2-VL-7B-Instruct</li>
<li>Qwen2-VL-72B-it: Qwen/Qwen2-VL-72B-Instruct</li>
<li>llava-onevision-7B: llava-hf/llava-onevision-qwen2-7b-ov-hf</li>
<li>llava-mistral-7B: llava-hf/llava-v1.6-mistral-7b-hf</li>
</ul>
<h2>8. Additional Evaluation Details</h2>
<p>We observed that many off-the-shelf models-particularly open-source, small-scale ones such as Llama 3.2 11B—do not strictly adhere to the required output format. For example, their outputs frequently omit closing quotes or inconsistently alternate between single and double quotes, rendering them unsuitable for JSON parsing. Furthermore, these LVLMs often fail to follow instructions for representing empty cells. Instead of using the designated symbols (e.g., "0", "", ".", or ".") to denote an empty cell, they sometimes default to using "*". In the chain-of-thought (CoT) setting, the final answer may not appear at the end of the sentence, further complicating extraction via regular expressions. Since our primary focus is on assessing puzzle-solving capabilities, rather than outright rejecting non-compliant responses, we have implemented two targeted post-processing strategies to address these issues. First, we employ the json-repair package [4] to repair broken JSON outputs, addressing issues such as incorrect escape characters and inconsistent usage of single and double quotes. Second, we utilize an LLM—specifically GPT-4o—as an output formatter to standardize the outputs into a unified format.</p>
<h2>9. Additional Implementation details</h2>
<p>We provide each model's version id in Supp.7. For the closed-source models, we directly use their official API calls to query. For the open-source models, we deploy them locally using the vLLM framework[26] before querying. For a fair comparison, none of the closed-source LVLMs dynamically construct a solver to solve the puzzles. Instead, they rely solely on vision-language modeling capabilities to solve puzzles. For dataset creation, we compile a dataset of 100,000 puzzles per puzzle type and split it into training and validation sets, ensuring there are no duplicated conditions or solutions within or across splits. Note that we do not enforce unique solutions for the games, as doing so would significantly reduce the number of possible conditions and increase computational overhead during game generation. For both puzzle-solution and reasoning fine-tuning, we use LLaMa 3.2 Vision Instruct as the base model and employ one node with 8 A100 GPUs, using a batch size of 4 on each GPU. Training for 5 epochs with the llama-recipe code base and default fine-tuning settings for LVLMs takes approximately 24 hours. The training process follows a causal modeling paradigm, masking the input question to focus supervision solely on the answer predictions. Number of clues, considering that puzzles typically become more challenging with fewer clues. For reveal-based games, such as Sudoku, at the easy level we randomly select $25 \%$ to $75 \%$ of cells as hints. For the medium level, we randomly select $25 \%$ to $50 \%$ as clues. For the hard level, we randomly select $15 \%$ to $40 \%$ as clues.</p>
<h2>10. Image Augmentations</h2>
<p>In our framework, we strategically employ two light-weight augmentation techniques to enhance model generalization across diverse visual inputs: (1) randomized affine transforms for geometric variation simulation, and (2) Poisson blending [41] for seamless texture integration.This streamlined augmentation approach effectively bridges the domain gap between training</p>
<p>samples and real-world scenarios while preserving the pre-trained model's inherent visual understanding capabilities. As demonstrated in Table 2, our method achieves realistic image transformations with significantly fewer artifacts compared to conventional augmentation strategies.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Table 2. Examples of Image Augmentations</p>
<h1>11. Common Mistakes of LVLMs in Puzzle Solving</h1>
<h3>11.1. Common Mistakes in LVLMs' Perception</h3>
<p>Grid Layout Misunderstanding. As shown in Tab. 10, GPT-4o fails to accurately perceive the Sudoku board layout-for instance, it erroneously places a " 2 " in the first row where the cell should remain empty. Although LVLMs generally recognize numerical values, they often misplace them; for example, they might confuse a sequence intended as [<em>, 2, </em>, ] with [, <em>, 2, </em>]. Notably, performance improves as more hints are provided, as observed in Sudoku tasks, suggesting that LVLMs struggle to detect board gridlines, which are essential for deducing the correct board organization.
Misinterpretation of Component Roles. In visual puzzles, distinct components serve unique functions. For example, in Killer-Sudoku, the grid incorporates both cell numbers and cage sums. However, LVLMs frequently misinterpret cage sums as cell values, even when differences in font size and color are present.
Difficulty Understanding Complex Visual Components. In puzzles like Thermometers and Battle-Ships, we notice that LVLMs sometimes fail to comprehend complex visual components. For example, LVLMs may erroneously perceive an empty cell in Thermometers as filled, and they often struggle to discern the distinct roles of different ship segments in Battle-Ships. Rejection of answering. We noticed that sometimes a model responds with a message such as "I'm sorry, I can't view or process images directly. Could you please describe the puzzle to me in text form?" to avoid answering, even though it sometimes does provide an answer. We hypothesize that the criteria for providing or rejecting an answer may depend on the model's internal confidence level [6].</p>
<h3>11.2. Common Mistakes in Pre-Trained LVLMs' Puzzle-Solving</h3>
<p>Cell-by-Cell Solving without Prioritizing. LVLMs often solve puzzles in a strictly sequential manner, overlooking constraints that emerge in later steps and failing to exploit the fact that some cells, with fewer possible options, are easier to resolve. For example, as demonstrated by Gemini-2.0-Flash in Tab. 11, the LVLM does not begin with the final cell in the second row, which has only one option.
Failure to Store Previous Actions. LVLMs frequently fail to update their understanding of the puzzle state based on previous actions. An example is an LVLM places a number at one cell. However, when making later decisions, it just ignored the previous actions it take.
Inability to Detect Error and Backtrack [25]. As is shown in the GPT-4o-Mini example in text version sudoku in Tab. 11, the LVLM finally outputs [2,1,4,2] as the last row, where a clear violation of duplication of number " 2 " exist. However, GPT-4o-Mini replies it as an answer. Another example is Qwen2.5-72B and Qwen2.5-7B in the same table, we noticed that model outputs a possibly valid board but does not obey the initial condition.
Repetitive Generation. Repetitive output is a common issue [58], especially in the LLaVA and Qwen families of models. For example, as shown in Tab. 10, the model generates excessively repetitive sequences, such as repeating "perception" followed by more than 50 " $[*]$ " symbols. We also notice the repetitive generation issue frequently in the reasoning models QVQ, that it reason many steps until running out of the maximum context length, as shown in Tab. 10.</p>
<h1>12. Generalization Capability of SFT Models</h1>
<p>Here, we analyze how an LVLM with SFT on one puzzle generalizes to another. In Fig.12, we use four puzzles-Sudoku, Odd-Even Sudoku, Renzoku, and Aquarium-with increasingly different rules. Notably, despite the fact that the Llama model trained on different puzzles performs well on the same puzzle, their performance on other puzzles is significantly lower. When the rules are similar, e.g., Sudoku and Odd-Even Sudoku, the puzzle-solving rate under generalization remains high, while evaluating a Sudoku-trained model on Aquarium yields a zero success rate. We notice similar situation for both S-SFT and R-SFT. Some recent work has also discussed the generalization limitations of Supervised Fine-Tuning [11, 65].
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 12. Generalization Evaluation of SFT Models on the Level Easy (). (Best viewed on a screen when zoomed in)</p>
<h2>13. Reinforcement Learning with Text Input</h2>
<p>PPO stabilizes training by incorporating a clipping mechanism into the policy ratio, which prevents excessive deviations from the previous policy, where $r_{t}=\frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} \mid s_{t}\right)}$ is the policy ratio.</p>
<p>$$
\mathbb{E}\left[\min \left(r_{t} \hat{A}<em t="t">{t}, \operatorname{clip}\left(r</em>\right)\right]
$$}, 1-\epsilon, 1+\epsilon\right) \hat{A}_{t</p>
<p>GRPO extends this framework by optimizing over groups of trajectories and regularizing updates with an explicit KL penalty, where $r_{i, t}$ compares token probabilities under the updated and old policies, conditioned on $q$ and prior outputs.</p>
<p>$$
\begin{aligned}
&amp; \mathbb{E}\left[\frac{1}{G} \sum_{\epsilon=1}^{G} \frac{1}{\left|o_{i}\right|} \sum_{\epsilon=1}^{\left|o_{i}\right|} \min \left(r_{i, t} \hat{A}<em i_="i," t="t">{i, t}, \operatorname{clip}\left(r</em>}, 1-\epsilon, 1+\epsilon\right) \hat{A<em _mathrm_KL="\mathrm{KL">{i, t}\right)\right] \
&amp; -\beta D</em>\right)
\end{aligned}
$$}}\left(\pi_{\theta} | \pi_{\mathrm{ref}</p>
<p>To ensure stable training, GRPO calculate the advantage by normalizing reward with all rollouts:</p>
<p>$$
\hat{A}<em i="i">{i, t}=\widetilde{r}</em>
$$}=\frac{r_{i}-\operatorname{mean}(\mathbf{r})}{\operatorname{std}(\mathbf{r})</p>
<p>The total reward consists of: success reward $r_{\text {succ }}$ for generating the correct solution, and format reward $r_{\text {fmt }}$ for producing a structured, extractable output, where $\lambda_{\text {succ }}$ and $\lambda_{\text {fmt }}$ balance the two components. We provide the training loss curve on page 83 .</p>
<p>$$
r_{i}=\lambda_{\text {succ }} r_{\text {succ }}+\lambda_{\text {fmt }} r_{\text {fmt }}
$$</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Table 3. Per-Puzzle Sample Screenshots of Level Easy</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Table 4. Per-Puzzle Sample Screenshots of Level Medium</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Table 5. Per-Puzzle Sample Screenshots of Level Hard ⑤. Note that some games do not have a hard level due to constraints imposed by the game rules. For example, Vanilla Sudoku's size can only be $4 \times 4$ or $9 \times 9$, and the next possible grid size is $16 \times 16$, which is too large.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Sample Screenshot</th>
<th style="text-align: center;">Description and Queries</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1. Aquarium</td>
<td style="text-align: center;"><img alt="img-16.jpeg" src="img-16.jpeg" /></td>
<td style="text-align: center;">Rule: You are an Aquarium puzzle player. You need to fill the aquariums with water up to a certain level or leave it empty. The numbers on the sides indicate how many filled (water) cells must be in each row and column. Indexing starts at 0 .</td>
</tr>
<tr>
<td style="text-align: center;">2. Battle-Ships</td>
<td style="text-align: center;"><img alt="img-17.jpeg" src="img-17.jpeg" /></td>
<td style="text-align: center;">Perception - Cell At: ${\mathbf{R a l e}}$ what is at the cell ( ${\mathrm{row}},{\mathrm{col}})$ ? Choose from {water, empty}.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"><img alt="img-18.jpeg" src="img-18.jpeg" /></td>
<td style="text-align: center;">Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $<em>$ * indicates an unknown cell and * $</em>$ * indicates a filled cell) and your solution (where * $<em>$ * indicates a filled cell and * $</em>$ * indicates an empty cell) in the following format. $\backslash n{\backslash n$ perception: {current state of the grid as a 2D array}, $\backslash n$ answer: ${$ solution as a 2D array $} \backslash n}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"><img alt="img-19.jpeg" src="img-19.jpeg" /></td>
<td style="text-align: center;">Rule Following - Valid Actions: ${$ Rule $}$ is it valid to assign the cell at ( ${\mathrm{row}},{\mathrm{col}})$ with value {value}? Choose from {valid, invalid}.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $<em>$ * indicates an unknown cell and * $</em>$ * indicates a filled cell), your step-by-step reasoning, and your solution (where * $<em>$ * indicates a filled cell and * $</em>$ * indicates an empty cell) in the following format. $\backslash n{\backslash n$ perception: {current state of the grid as a 2D array}, $\backslash n$ think: {your step-by-step reasoning}, $\backslash n$ answer: ${$ solution as a 2D array $} \backslash n}$</td>
</tr>
<tr>
<td style="text-align: center;">3. Binairo</td>
<td style="text-align: center;"><img alt="img-20.jpeg" src="img-20.jpeg" /></td>
<td style="text-align: center;">Rule: You are a Battle-Ships player. You need to place ships in a grid based on row and column hints. The hints indicate how many ship cells are in each row and column. The numbers of each size ship are given. Ships cannot touch each other, even diagonally. Indexing starts at 0 .</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception - Cell At: ${\mathbf{R a l e}}$ Given the current game state, what is at position ( ${\mathrm{row}},{\mathrm{col}})$ ? Choose from: {ship, empty, unknown}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $<em>$ * indicates an unknown cell) and your solution (where * $</em>$ * indicates a ship cell and * $*$ * indicates an empty cell) in the following format. { perception: {current state of the grid as a 2D array}, answer: {solution as a 2D array} }</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule Following - Valid Actions: ${$ Rule $}$ Given the current game state, is it valid to assign cell ( ${\mathrm{row}},{\mathrm{col}})$ with value {value}? Respond with: valid or invalid.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $<em>$ * indicates an unknown cell), your step-by-step reasoning, and your solution (where * $</em>$ * indicates a ship cell and * $*$ * indicates an empty cell) in the following format. { perception: {current state of the grid as a 2D array}, think: {your step-by-step reasoning}, answer: {solution as a 2D array} }</td>
</tr>
<tr>
<td style="text-align: center;">4. Colored-Sudoku</td>
<td style="text-align: center;"><img alt="img-21.jpeg" src="img-21.jpeg" /></td>
<td style="text-align: center;">Rule: You are a Binairo player. You have to fill a grid with white (w) and black (b) pieces. No more than two circles of the same color can be adjacent (horizontally and vertically). Indexing starts at 0 .</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception - Cell At: ${\mathbf{R a l e}}$ what is the value of the cell at ( ${\mathrm{row}},{\mathrm{col}})$ ? Choose from ${\mathrm{b}, \mathrm{w}$, empty $}$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $*$ * indicates an empty cell) and your solution in the following format. $\backslash n{\backslash n$ perception: {current state of the grid as a 2D array}, $\backslash n$ answer: ${$ solution as a 2D array $} \backslash n}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule Following - Valid Actions: ${$ Rule $}$ is it valid to fill the cell at ( ${\mathrm{row}},{\mathrm{col}})$ with ${\mathbf{b}, \mathbf{w}}$ ? Choose from {valid, invalid}.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $*$ * indicates an empty cell), your step-by-step reasoning, and your solution in the following format. $\backslash n{\backslash n$ perception: {current state of the grid as a 2D array}, $\backslash n$ think: {your step-by-step reasoning}, $\backslash n$ answer: {solution as a 2D array $} \backslash n}$</td>
</tr>
<tr>
<td style="text-align: center;">5. Field-Explorer</td>
<td style="text-align: center;"><img alt="img-22.jpeg" src="img-22.jpeg" /></td>
<td style="text-align: center;">Rule: You are a Colored-Sudoku player. You have to enter a numerical digit from 1 through N in each cell of a NxN grid, $\backslash n$ The rule is to make sure unique numbers in each row, column, and within cells of the same color. Indexing starts at 0 .</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception - Cell At: ${\mathbf{R a l e}}$ what is the value of the cell at ( ${\mathrm{row}},{\mathrm{col}})$ ? Choose from ${1,2, \ldots, \mathrm{~N}$, empty $}$.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $*$ * indicates an empty cell) and your solution in the following format. $\backslash n{\backslash n$ perception: {current state of the grid as a 2D array}, $\backslash n$ answer: ${$ solution as a 2D array $} \backslash n}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule Following - Valid Actions: ${\mathbf{R a l e}}$ is it valid to fill the cell at ( ${\mathrm{row}},{\mathrm{col}})$ with value {value}? Choose from {valid, invalid}.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $*$ * indicates an empty cell), your step-by-step reasoning, and your solution in the following format. $\backslash n{\backslash n$ perception: {current state of the grid as a 2D array}, $\backslash n$ think: {your step-by-step reasoning}, $\backslash n$ answer: {solution as a 2D array $} \backslash n}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule: You are a Field-Explore player. You need to identify mine locations in a grid based on revealed numbers. Each revealed number indicates how many mines are adjacent to that cell (including diagonals). Indexing starts at 0 .</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception - Cell At: ${\mathbf{R a l e}}$ Given the current game state, what is in the cell at ( ${\mathrm{row}},{\mathrm{col}})$ ? Choose from {mine, number, hidden}.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $<em>$ * indicates hidden cells, "s" indicates a mine, and numbers represent revealed counts) and your solution (where * $</em>$ * indicates a mine and * $*$ * indicates an empty cell) in the following format. { perception: {current state of the grid as a 2D array}, answer: {solution as a 2D array} }</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rule Following - Valid Actions: ${$ Rule $}$ Given the current game state, is it valid to assign the cell at ( ${\mathrm{row}},{\mathrm{col}})$ with {value}? Choose from {valid, invalid}.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where * $<em>$ * indicates hidden cells, "s" indicates a mine, and numbers represent revealed counts), your step-by-step reasoning, and your solution (where * $</em>$ * indicates a mine and * $*$ * indicates an empty cell) in the following format. { perception: {current state of the grid as a 2D array}, think: {your step-by-step reasoning}, answer: {solution as a 2D array} }</td>
</tr>
</tbody>
</table>
<p>Table 6. Per-Puzzle Sample Screenshots and Query Templates. Part 1. (1st - 5th) Note that ${\cdot}$ represents variable values that depend on each query and the specific game. Additionally, some quotation marks, e.g., " $\cdot$ ", are omitted in the query template for clarity. To clearly distinguish between 0 -indexing and 1 -indexing, we explicitly require indexing in the query. Action validity is assessed solely based on the presence of immediate rule violations without considering any long $\mathbf{0 r m}$ effects.</p>
<p><img alt="img-23.jpeg" src="img-23.jpeg" /></p>
<p>Description and Queries
Rule: You are a Futoshiki player. You have to enter a numerical digit from 1 through N in each cell of an NxN grid. The rules are: unique numbers in each row and column; inequality signs between cells must be respected (for example, $&lt;$ means left number is smaller, $&gt;$ means left number is larger). Indexing starts at 0 .
Perception - Cell At: ${$ Rule $}$ what is the value of the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${1,2, \ldots, \mathrm{~N}$, empty $}$.
Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell) and your solution in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array $}, \backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule Following - Valid Actions: ${$ Rule $}$ is it valid to fill the cell at ( ${$ row $}, {\mathrm{col}}$ ) with value ${\mathrm{value}}$ ? Choose from ${\mathrm{valid}$, invalid}.
Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell), your step-by-step reasoning, and your solution in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array $}, \backslash n$ think: ${$ your step-by-step reasoning $}, \backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule: You are a Hitori player. You need to shade some cells in the grid such that no number appears more than once in each row and column among unshaded cells. The rules are: shaded cells cannot be adjacent; all unshaded cells must be connected. Indexing starts at 0 .
Perception - Cell At: ${$ Rule $}$ what is the value of the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${1,2, \ldots, \mathrm{~N}}$.
Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot and your solution (where <em>+</em> indicates a shaded cell and <em>+</em> indicates a cell leave unshaded) in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array $}, \backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule Following - Valid Actions: ${$ Rule $}$ is it valid to shade the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${$ valid, invalid $}$.
Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot, your step-by-step reasoning, and your solution (where <em>+</em> indicates a shaded cell and <em>+</em> indicates a cell leave unshaded) in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array $}, \backslash n$ think: ${$ your step-by-step reasoning $}$, $\backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule: You are a Jigsaw-Sudoku player. You have to enter a numerical digit from 1 through N in each cell of a NxN grid. The rules are: unique numbers in each row, column, and within cells of the same region. Each region is a connected group of cells. Indexing starts at 0 .
Perception - Cell At: ${$ Rule $}$ Given the current game state in the screenshot, what is the value of the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${1,2, \ldots, \mathrm{~N}$, empty $}$.
Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell) and your solution in the following format. { perception: {current state of the grid as a 2D array}, answer: {solution as a 2D array} }</p>
<p>Rule Following - Valid Actions: ${$ Rule $}$ Given the current game state in the screenshot, is it valid to fill the cell at ( ${$ row $}$, ${\mathrm{col}}$ ) with value ${\mathrm{value}}$ ? Choose from ${\mathrm{valid}$, invalid}
Perception and Chain-of-Thought Reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell), your step-by-step reasoning, and your solution in the following format. { perception: {current state of the grid as a 2D array}, think: {your step-by-step reasoning}, answer: {solution as a 2D array} }</p>
<p>Rule: You are a Kakurasu puzzle player. You need to shade some cells in a grid where the sum of the weights of selected cells in each row and column matches the given clues. The weights increase from left to right (for rows) and top to bottom (for columns), starting from 1. Indexing starts at 0 .
Perception - Cell At: ${$ Rule $}$ what is the value of the cell at ( ${$ row $}, {\mathrm{col}}$ ? Response in a formate of (number, number).
Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell) and your solution (where <em>+</em> indicates a shaded cell and <em>+</em> indicates a cell leave unshaded) in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array $}, \backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule Following - Valid Actions: ${$ Rule $}$ is it valid to shade the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${$ valid, invalid $}$.
Perception and Chain-of-Thought reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell), your step-by-step reasoning, and your solution (where <em>+</em> indicates a shaded cell and <em>+</em> indicates a cell leave unshaded) in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array $}, \backslash n$ think: ${$ your step-by-step reasoning $}, \backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule: You are a Kakuro player. You have to fill in the grid with numbers ( 1 to N ) such that each row and column adds up to the specified sum. The rules are: (1) adjacent numbers should not be the same. (2) numbers add up to the given sum for each row and column. Indexing starts at 0 .
Perception - Cell At: ${$ Rule $}$ what is the value of the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${1,2, \ldots, \mathrm{~N}$, empty $}$.
Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell) and your solution in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array}, $\backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule Following - Valid Actions: ${$ Rule $}$ is it valid to fill the cell at ( ${$ row $}, {\mathrm{col}}$ ) with value ${\mathrm{value}}$ ? Choose from ${\mathrm{valid}$, invalid}
Perception and Chain-of-Thought reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell), your step-by-step reasoning, and your solution in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array}, $\backslash n$ think: ${$ your step-by-step reasoning $}, \backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule: You are a Kakuro player. You have to fill in the grid with numbers ( 1 to N ) such that each row and column adds up to the specified sum. The rules are: (1) adjacent numbers should not be the same. (2) numbers add up to the given sum for each row and column. Indexing starts at 0 .
Perception - Cell At: ${$ Rule $}$ what is the value of the cell at ( ${$ row $}, {\mathrm{col}}$ )? Choose from ${1,2, \ldots, \mathrm{~N}$, empty $}$.
Perception and Direct Solution: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell) and your solution in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2 D array}, $\backslash n$ answer: ${$ solution as a 2 D array $} \backslash n}$</p>
<p>Rule Following - Valid Actions: ${$ Rule $}$ is it valid to fill the cell at ( ${$ row $}, {\mathrm{col}}$ ) with value ${\mathrm{value}}$ ? Choose from ${\mathrm{valid}$, invalid}
Perception and Chain-of-Thought reasoning: ${$ Rule $}$ Give me your response of the current game state in the screenshot (where <em>+</em> indicates an empty cell), your step-by-step reasoning, and your solution in the following format. $\backslash n{\backslash n$ perception: ${$ current state of the grid as a 2D array}, $\backslash n$ think: ${$ your step-by-step reasoning $}, \backslash n$ answer: ${$ solution as a 2D array $} \backslash n}$</p>
<p>Table 7. Per-Puzzle Sample Screenshots and Query Templates. Part 2 (6th - 10th). Note that there is a special case that in selection-based games, e.g., Thermometers, where a set of cell is selected as the answer. A cell being empty could mean both undefined and deliberately leaving empty. To distinguish these cases, we typically use two notations, such as "*" for undefined cells and "e" for the deliberately empty cells in the query and SFT dataset creation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ In the reasoning model category, we include Gemini-2.0-Thinking and Qwen-QVQ, as other reasoning models are either lacking vision capabilities, e.g., DeepSeek [14], or only accessible to high-tier users. Due to the rate limit in Gemini-2.0-Thinking, we only evaluate puzzle-solving with&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
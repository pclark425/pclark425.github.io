<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1069 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1069</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1069</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-a798fba1dc16e2046a228ae9f039c10fd9ac5d66</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a798fba1dc16e2046a228ae9f039c10fd9ac5d66" target="_blank">Active Domain Randomization</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> This work empirically examines the effects of domain randomization on agent generalization and proposes Active Domain Randomization, a novel algorithm that learns a parameter sampling strategy that leads to more robust, consistent policies.</p>
                <p><strong>Paper Abstract:</strong> Domain randomization is a popular technique for improving domain transfer, often used in a zero-shot setting when the target domain is unknown or cannot easily be used for training. In this work, we empirically examine the effects of domain randomization on agent generalization. Our experiments show that domain randomization may lead to suboptimal, high-variance policies, which we attribute to the uniform sampling of environment parameters. We propose Active Domain Randomization, a novel algorithm that learns a parameter sampling strategy. Our method looks for the most informative environment variations within the given randomization ranges by leveraging the discrepancies of policy rollouts in randomized and reference environment instances. We find that training more frequently on these instances leads to better overall agent generalization. In addition, when domain randomization and policy transfer fail, Active Domain Randomization offers more insight into the deficiencies of both the chosen parameter ranges and the learned policy, allowing for more focused debugging. Our experiments across various physics-based simulated and a real-robot task show that this enhancement leads to more robust, consistent policies.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1069.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1069.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LunarLander-v2 (ADR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LunarLander-v2 agent trained with Active Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated 2-DoF landing agent trained with reinforcement learning (DDPG) using Active Domain Randomization to preferentially sample difficult main-engine-strength settings, improving zero-shot generalization to harder engine-strength environments and reducing policy variance relative to uniform randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LunarLander-v2 agent (DDPG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reinforcement learning agent trained with Deep Deterministic Policy Gradient (DDPG). Training variants compared: Baseline (single default simulator), Uniform Domain Randomization (UDR), and Active Domain Randomization (ADR) which learns a sampling policy (SVPG) guided by a discriminator-based reward.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>LunarLander-v2</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>2-DoF spacecraft landing task implemented in Box2D. Complexity arises from continuous control to land softly (manage fuel, impact velocity) and sensitivity to the main engine strength (MES) parameter; MES was randomized over a specified interval producing environments ranging from easy to nearly-unsolvable.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task difficulty characterized qualitatively by main engine strength (MES) parameter; N_rand = 1 (one randomized parameter). The authors also use episode time limit (1000 timesteps) and task reward (total discounted reward) as performance signals. They note 'Solved' corresponds to engine strengths > 12 in their reward metric.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low (simple 2-DoF control task; single randomized parameter)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Domain randomization over MES with training range [8,20]; testing/extrapolation range includes hard interval [8,11]. Variation quantified as the continuous range of the MES parameter and sampling distribution (uniform vs ADR-learned).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (MES continuous range spanning [8,20]; sampled uniformly in UDR or focused by ADR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Total episodic reward (higher is better); also report 'Solved' label when reward reaches environment threshold (engine strengths > 12).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: ADR achieves near-expert (near-oracle) generalization across MES including lower-MES (hard) regions; UDR and baseline fail to solve lower MES tasks; exact numeric rewards not reported in-text (figures show ADR curves higher than UDR).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes. The paper explicitly argues and empirically demonstrates that uniform variation (UDR) across the randomization range can waste samples on uninformative parameter settings and yield high-variance/suboptimal policies; focusing variation onto the currently informative (hard) parameter regions (via ADR) improves generalization even when evaluation settings are outside training distribution. They also note sample complexity increases with number of randomized parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>UDR on full MES range [8,20] performs worse on hard MES ([8,11]) than ADR (qualitative; UDR fails to solve lower MES tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Baseline (single default MES=13) performs well on default but fails to generalize to low-MES hard tasks (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Active Domain Randomization (ADR) vs Uniform Domain Randomization (UDR) vs single-instance baseline; ADR uses SVPG particles and discriminator-based reward to adapt sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>ADR-trained policies generalize to harder unseen MES values ([8,11]) and approach hand-picked oracle range performance; UDR-trained and baseline policies fail to generalize to these harder settings and show higher variance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training run for 1 million agent timesteps; ADR adapts sampling during training (initially uniform sampling then shifting focus), producing improved generalization without additional environment interactions beyond the 1M steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Focusing training on informative/harder MDP instances (learned via ADR) leads to markedly better zero-shot generalization and lower variance than uniform sampling; uniform randomization can dilute training effort across many uninformative instances and lead to suboptimal policies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Domain Randomization', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1069.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1069.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pusher-3DOF-v0 (ADR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pusher-3DOF-v0 agent trained with Active Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated 3-DoF arm pushing task where ADR learns to prioritize rare, hard-to-solve puck dynamics (friction loss and damping) within a multi-dimensional randomization space to improve extrapolation to unseen/slippery test regions, yielding lower distance-to-target and reduced variance compared to UDR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Pusher-3DOF-v0 agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reinforcement learning agent (DDPG policy in experiments) interacting with a 3-DoF simulated arm (Mujoco) trained under domain randomization; ADR uses SVPG to propose environment parameters and a discriminator-based reward to prioritize hard dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Pusher-3DOF-v0</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>3-DoF robotic pushing task (Mujoco) where an arm must push a puck to a target. Complexity arises from contact dynamics and puck sliding behavior which depends on two randomized parameters: puck friction loss and puck joint damping; combinations produce distinct easy-to-hard regions (sliding correlates with increased difficulty).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized by number of randomized parameters N_rand = 2 (puck friction loss and puck joint damping); task difficulty measured by distance-to-target (lower is better) and qualitative puck sliding dynamics. Episode length is up to 100 timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (3-DoF manipulation with contact dynamics; 2 randomized parameters creating nontrivial dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Domain randomization over two continuous parameters with training ranges given as multipliers of default ([0.67,1.0]×default in train) and test/extrapolation ranges [0.5,0.67]×default; variation considered sparse (only ~25% of training region produces sliding/hard dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (2-D continuous randomization; training region limited but contains sparse hard subregions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Distance to target (lower is better); also episodic reward curves used for learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: ADR outperforms UDR in most held-out test regions (lower mean distance-to-target) and yields policies with less variance; exact numeric distances not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes. The paper notes that as the dimensionality of randomization grows, informative (hard) regions become rarer under uniform sampling; ADR discovers and prioritizes these sparse hard regions (e.g., the puck-sliding regime) improving extrapolation to unseen dynamics. Thus, more variation (higher-dimensional randomization) increases sparsity and the need for focused sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Active Domain Randomization (ADR) vs Uniform Domain Randomization (UDR); ADR uses SVPG ensemble (N=15 for this environment) with discriminator reward to bias sampling towards hard regions.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>ADR-trained agents generalize better to unseen, harder test regions outside the training box (extrapolation to lower friction/damping), outperforming UDR in distance-to-target and producing lower-variance policies in nearly all test regions.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training used 1 million agent timesteps; ADR achieves better extrapolation without extra interactions by concentrating training on informative instances discovered during training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In moderately high-dimensional randomization (N_rand=2) informative hard regions are sparse; ADR learns to prioritize those sparse regions leading to better extrapolation and lower variance compared to UDR, which wastes samples on many easy/uninformative settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Domain Randomization', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1069.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1069.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ErgoReacher-v0 (ADR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ErgoReacher-v0 agent trained with Active Domain Randomization (sim and sim->real transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 4-DoF simulated reaching task with high-dimensional (N_rand=8) randomization (joint gains and max torques) where ADR learns to focus on harder, often unintuitive environment combinations; ADR yields more consistent generalization and lower variance than UDR and improves zero-shot transfer to a real Poppy Ergo Jr. robot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ErgoReacher-v0 agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reinforcement learning agent (DDPG policy in experiments) trained in Bullet-simulated 4-DoF reacher environment under ADR; randomization includes 8 parameters (joint damping, joint max torque, gains). ADR uses SVPG + discriminator reward to select training instances.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (with zero-shot transfer evaluations on a physical robot)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ErgoReacher-v0 (and real Poppy Ergo Jr. evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>4-DoF arm reaching task (Bullet physics). Complexity is high due to multiple interacting continuous control parameters and eight randomized sim parameters (dynamics, gains, torques) that can create catastrophic failure states (e.g., very low torque/gain leads to gravity-induced failures). Also tested via zero-shot transfer to a Poppy Ergo Jr. physical robot.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>High-dimensional randomization N_rand = 8; task difficulty measured by episodic reward and occurrences of catastrophic failure states (agent trapped, unable to recover). Complexity also indicated by interactions among joint gains, torque limits and damping.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (4-DoF manipulation with 8 randomized parameters leading to complex, non-intuitive failure modes)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Domain randomization across 8 parameters with training ranges (e.g., joint damping [0.3,2.0]×default; joint max torque [1.0,4.0]×default) and held-out test environment with extreme low torque/gain (0.2× default) to test extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (8-dimensional continuous randomization with large ranges and held-out extreme test)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Episodic reward / learning curves; also binary/qualitative detection of catastrophic failures; in sim->real transfer, aggregate performance over 25 random goals evaluated per policy (125 evaluations per method per environment setting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: ADR shows better and more consistent performance and lower variance than UDR on held-out hard simulated conditions and on zero-shot transfer to the real robot; exact numeric rewards or success rates are not reported in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes. The paper emphasizes that in high-dimensional (high-variation) randomization, uniform sampling produces a mix of hard and easy MDPs that can cause catastrophic forgetting and high variance; ADR adapts sampling to emphasize the hardest seen environments and mitigate mixing-induced instability, improving generalization and consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Baseline (trained only on default sim) performs poorly on held-out hard low-torque/gain target (qualitative failure/catastrophic states).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>UDR in the high-variation (N_rand=8) setting exhibits high variance and inconsistent behavior on held-out hard environments; ADR outperforms UDR qualitatively across tested hard cases.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Active Domain Randomization (ADR) vs Uniform Domain Randomization (UDR) vs Baseline; ADR uses SVPG with N=15 particles for this environment and discriminator reward aggregation over (s,a,s') tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>ADR yields more consistent generalization to held-out extreme low-torque/gain simulated environments and better zero-shot transfer performance to a real Poppy Ergo Jr. robot (lower spread across evaluations) compared to UDR.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training run for 1 million agent timesteps; ADR re-allocates sampling budget adaptively to informative instances, improving generalization without additional total interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In high-dimensional, partially uninterpretable randomization spaces, ADR reduces catastrophic forgetting and variance by adaptively focusing on the hardest seen configurations; this yields more consistent policies and improved sim-to-real zero-shot transfer compared to uniform randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Domain Randomization', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1069.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1069.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ErgoPusher-v0 (sim->real)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ErgoPusher-v0 agent and zero-shot transfer to Poppy Ergo Jr.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated puck-pushing task analogous to Pusher-3DOF but with a real-robot analog; ADR-trained policies transfer to the physical robot with better or similar performance and notably lower variance compared to UDR-trained policies under manual environment perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ErgoPusher-v0 agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reinforcement learning agent trained in simulation with DDPG under ADR/UDR; evaluated via zero-shot transfer on a real Poppy Ergo Jr. robot performing puck-pushing tasks with manual changes in puck friction.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent with physical robot evaluation (physical robot = Poppy Ergo Jr.)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ErgoPusher-v0 (sim) and physical Poppy Ergo Jr. (real)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Pushing task where puck friction and joint damping are randomized in sim (training ranges [0.67,1.0]×default) and real-robot evaluations alter puck friction manually (slippery vs rough) to test generalization and sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>N_rand = 2 (puck friction loss and puck joint damping); difficulty measured by task success metrics like distance-to-target and overall performance across 25 random goals per policy.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (manipulation with contact dynamics and 2 randomized parameters; physical instantiation introduces unmodeled dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Domain randomization multipliers over friction/damping; training variation [0.67,1.0]×default, test/extrapolation [0.5,0.67]×default and manual real-world friction variations.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Distance-to-target / averaged task performance across 25 random goals; aggregate statistics over 125 evaluations per method per environment setting (5 policies × 25 goals).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: ADR policies obtain overall better or similar mean performance than UDR and display lower spread (variance) across all evaluated real-world perturbations; no exact numerical aggregates published in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes. The sim-to-real experiments highlight that uniform randomization can yield unpredictable real-world behavior due to unmodeled dynamics and that focusing sampling on informative simulated variations (ADR) leads to more robust and consistent real-world performance.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Active Domain Randomization (ADR) vs Uniform Domain Randomization (UDR). Policies are trained in sim and evaluated zero-shot on the real robot without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>ADR-trained policies transfer with better or comparable mean performance and with substantially lower variance across real-world environment perturbations (friction changes) compared to UDR-trained policies.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Simulation training run for 1M agent timesteps; ADR reuses same simulation budget but reallocates sampling to informative instances thereby improving transfer robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ADR reduces unpredictability and variance in sim-to-real transfer by adaptively finding and emphasizing informative simulated variations; uniform randomization can produce inconsistent real-world outcomes because it trains equally on uninformative or degenerate instances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Domain Randomization', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Learning dexterous in-hand manipulation <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Robust adversarial reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1069",
    "paper_id": "paper-a798fba1dc16e2046a228ae9f039c10fd9ac5d66",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "LunarLander-v2 (ADR)",
            "name_full": "LunarLander-v2 agent trained with Active Domain Randomization",
            "brief_description": "A simulated 2-DoF landing agent trained with reinforcement learning (DDPG) using Active Domain Randomization to preferentially sample difficult main-engine-strength settings, improving zero-shot generalization to harder engine-strength environments and reducing policy variance relative to uniform randomization.",
            "citation_title": "Active Domain Randomization",
            "mention_or_use": "use",
            "agent_name": "LunarLander-v2 agent (DDPG)",
            "agent_description": "Reinforcement learning agent trained with Deep Deterministic Policy Gradient (DDPG). Training variants compared: Baseline (single default simulator), Uniform Domain Randomization (UDR), and Active Domain Randomization (ADR) which learns a sampling policy (SVPG) guided by a discriminator-based reward.",
            "agent_type": "simulated agent",
            "environment_name": "LunarLander-v2",
            "environment_description": "2-DoF spacecraft landing task implemented in Box2D. Complexity arises from continuous control to land softly (manage fuel, impact velocity) and sensitivity to the main engine strength (MES) parameter; MES was randomized over a specified interval producing environments ranging from easy to nearly-unsolvable.",
            "complexity_measure": "Task difficulty characterized qualitatively by main engine strength (MES) parameter; N_rand = 1 (one randomized parameter). The authors also use episode time limit (1000 timesteps) and task reward (total discounted reward) as performance signals. They note 'Solved' corresponds to engine strengths &gt; 12 in their reward metric.",
            "complexity_level": "low (simple 2-DoF control task; single randomized parameter)",
            "variation_measure": "Domain randomization over MES with training range [8,20]; testing/extrapolation range includes hard interval [8,11]. Variation quantified as the continuous range of the MES parameter and sampling distribution (uniform vs ADR-learned).",
            "variation_level": "medium (MES continuous range spanning [8,20]; sampled uniformly in UDR or focused by ADR)",
            "performance_metric": "Total episodic reward (higher is better); also report 'Solved' label when reward reaches environment threshold (engine strengths &gt; 12).",
            "performance_value": "Qualitative: ADR achieves near-expert (near-oracle) generalization across MES including lower-MES (hard) regions; UDR and baseline fail to solve lower MES tasks; exact numeric rewards not reported in-text (figures show ADR curves higher than UDR).",
            "complexity_variation_relationship": "Yes. The paper explicitly argues and empirically demonstrates that uniform variation (UDR) across the randomization range can waste samples on uninformative parameter settings and yield high-variance/suboptimal policies; focusing variation onto the currently informative (hard) parameter regions (via ADR) improves generalization even when evaluation settings are outside training distribution. They also note sample complexity increases with number of randomized parameters.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": "UDR on full MES range [8,20] performs worse on hard MES ([8,11]) than ADR (qualitative; UDR fails to solve lower MES tasks).",
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": "Baseline (single default MES=13) performs well on default but fails to generalize to low-MES hard tasks (qualitative).",
            "training_strategy": "Active Domain Randomization (ADR) vs Uniform Domain Randomization (UDR) vs single-instance baseline; ADR uses SVPG particles and discriminator-based reward to adapt sampling.",
            "generalization_tested": true,
            "generalization_results": "ADR-trained policies generalize to harder unseen MES values ([8,11]) and approach hand-picked oracle range performance; UDR-trained and baseline policies fail to generalize to these harder settings and show higher variance.",
            "sample_efficiency": "Training run for 1 million agent timesteps; ADR adapts sampling during training (initially uniform sampling then shifting focus), producing improved generalization without additional environment interactions beyond the 1M steps.",
            "key_findings": "Focusing training on informative/harder MDP instances (learned via ADR) leads to markedly better zero-shot generalization and lower variance than uniform sampling; uniform randomization can dilute training effort across many uninformative instances and lead to suboptimal policies.",
            "uuid": "e1069.0",
            "source_info": {
                "paper_title": "Active Domain Randomization",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Pusher-3DOF-v0 (ADR)",
            "name_full": "Pusher-3DOF-v0 agent trained with Active Domain Randomization",
            "brief_description": "A simulated 3-DoF arm pushing task where ADR learns to prioritize rare, hard-to-solve puck dynamics (friction loss and damping) within a multi-dimensional randomization space to improve extrapolation to unseen/slippery test regions, yielding lower distance-to-target and reduced variance compared to UDR.",
            "citation_title": "Active Domain Randomization",
            "mention_or_use": "use",
            "agent_name": "Pusher-3DOF-v0 agent",
            "agent_description": "Reinforcement learning agent (DDPG policy in experiments) interacting with a 3-DoF simulated arm (Mujoco) trained under domain randomization; ADR uses SVPG to propose environment parameters and a discriminator-based reward to prioritize hard dynamics.",
            "agent_type": "simulated agent",
            "environment_name": "Pusher-3DOF-v0",
            "environment_description": "3-DoF robotic pushing task (Mujoco) where an arm must push a puck to a target. Complexity arises from contact dynamics and puck sliding behavior which depends on two randomized parameters: puck friction loss and puck joint damping; combinations produce distinct easy-to-hard regions (sliding correlates with increased difficulty).",
            "complexity_measure": "Characterized by number of randomized parameters N_rand = 2 (puck friction loss and puck joint damping); task difficulty measured by distance-to-target (lower is better) and qualitative puck sliding dynamics. Episode length is up to 100 timesteps.",
            "complexity_level": "medium (3-DoF manipulation with contact dynamics; 2 randomized parameters creating nontrivial dynamics)",
            "variation_measure": "Domain randomization over two continuous parameters with training ranges given as multipliers of default ([0.67,1.0]×default in train) and test/extrapolation ranges [0.5,0.67]×default; variation considered sparse (only ~25% of training region produces sliding/hard dynamics).",
            "variation_level": "medium (2-D continuous randomization; training region limited but contains sparse hard subregions)",
            "performance_metric": "Distance to target (lower is better); also episodic reward curves used for learning progress.",
            "performance_value": "Qualitative: ADR outperforms UDR in most held-out test regions (lower mean distance-to-target) and yields policies with less variance; exact numeric distances not provided in main text.",
            "complexity_variation_relationship": "Yes. The paper notes that as the dimensionality of randomization grows, informative (hard) regions become rarer under uniform sampling; ADR discovers and prioritizes these sparse hard regions (e.g., the puck-sliding regime) improving extrapolation to unseen dynamics. Thus, more variation (higher-dimensional randomization) increases sparsity and the need for focused sampling.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Active Domain Randomization (ADR) vs Uniform Domain Randomization (UDR); ADR uses SVPG ensemble (N=15 for this environment) with discriminator reward to bias sampling towards hard regions.",
            "generalization_tested": true,
            "generalization_results": "ADR-trained agents generalize better to unseen, harder test regions outside the training box (extrapolation to lower friction/damping), outperforming UDR in distance-to-target and producing lower-variance policies in nearly all test regions.",
            "sample_efficiency": "Training used 1 million agent timesteps; ADR achieves better extrapolation without extra interactions by concentrating training on informative instances discovered during training.",
            "key_findings": "In moderately high-dimensional randomization (N_rand=2) informative hard regions are sparse; ADR learns to prioritize those sparse regions leading to better extrapolation and lower variance compared to UDR, which wastes samples on many easy/uninformative settings.",
            "uuid": "e1069.1",
            "source_info": {
                "paper_title": "Active Domain Randomization",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "ErgoReacher-v0 (ADR)",
            "name_full": "ErgoReacher-v0 agent trained with Active Domain Randomization (sim and sim-&gt;real transfer)",
            "brief_description": "A 4-DoF simulated reaching task with high-dimensional (N_rand=8) randomization (joint gains and max torques) where ADR learns to focus on harder, often unintuitive environment combinations; ADR yields more consistent generalization and lower variance than UDR and improves zero-shot transfer to a real Poppy Ergo Jr. robot.",
            "citation_title": "Active Domain Randomization",
            "mention_or_use": "use",
            "agent_name": "ErgoReacher-v0 agent",
            "agent_description": "Reinforcement learning agent (DDPG policy in experiments) trained in Bullet-simulated 4-DoF reacher environment under ADR; randomization includes 8 parameters (joint damping, joint max torque, gains). ADR uses SVPG + discriminator reward to select training instances.",
            "agent_type": "simulated agent (with zero-shot transfer evaluations on a physical robot)",
            "environment_name": "ErgoReacher-v0 (and real Poppy Ergo Jr. evaluations)",
            "environment_description": "4-DoF arm reaching task (Bullet physics). Complexity is high due to multiple interacting continuous control parameters and eight randomized sim parameters (dynamics, gains, torques) that can create catastrophic failure states (e.g., very low torque/gain leads to gravity-induced failures). Also tested via zero-shot transfer to a Poppy Ergo Jr. physical robot.",
            "complexity_measure": "High-dimensional randomization N_rand = 8; task difficulty measured by episodic reward and occurrences of catastrophic failure states (agent trapped, unable to recover). Complexity also indicated by interactions among joint gains, torque limits and damping.",
            "complexity_level": "high (4-DoF manipulation with 8 randomized parameters leading to complex, non-intuitive failure modes)",
            "variation_measure": "Domain randomization across 8 parameters with training ranges (e.g., joint damping [0.3,2.0]×default; joint max torque [1.0,4.0]×default) and held-out test environment with extreme low torque/gain (0.2× default) to test extrapolation.",
            "variation_level": "high (8-dimensional continuous randomization with large ranges and held-out extreme test)",
            "performance_metric": "Episodic reward / learning curves; also binary/qualitative detection of catastrophic failures; in sim-&gt;real transfer, aggregate performance over 25 random goals evaluated per policy (125 evaluations per method per environment setting).",
            "performance_value": "Qualitative: ADR shows better and more consistent performance and lower variance than UDR on held-out hard simulated conditions and on zero-shot transfer to the real robot; exact numeric rewards or success rates are not reported in-text.",
            "complexity_variation_relationship": "Yes. The paper emphasizes that in high-dimensional (high-variation) randomization, uniform sampling produces a mix of hard and easy MDPs that can cause catastrophic forgetting and high variance; ADR adapts sampling to emphasize the hardest seen environments and mitigate mixing-induced instability, improving generalization and consistency.",
            "high_complexity_low_variation_performance": "Baseline (trained only on default sim) performs poorly on held-out hard low-torque/gain target (qualitative failure/catastrophic states).",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "UDR in the high-variation (N_rand=8) setting exhibits high variance and inconsistent behavior on held-out hard environments; ADR outperforms UDR qualitatively across tested hard cases.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Active Domain Randomization (ADR) vs Uniform Domain Randomization (UDR) vs Baseline; ADR uses SVPG with N=15 particles for this environment and discriminator reward aggregation over (s,a,s') tuples.",
            "generalization_tested": true,
            "generalization_results": "ADR yields more consistent generalization to held-out extreme low-torque/gain simulated environments and better zero-shot transfer performance to a real Poppy Ergo Jr. robot (lower spread across evaluations) compared to UDR.",
            "sample_efficiency": "Training run for 1 million agent timesteps; ADR re-allocates sampling budget adaptively to informative instances, improving generalization without additional total interactions.",
            "key_findings": "In high-dimensional, partially uninterpretable randomization spaces, ADR reduces catastrophic forgetting and variance by adaptively focusing on the hardest seen configurations; this yields more consistent policies and improved sim-to-real zero-shot transfer compared to uniform randomization.",
            "uuid": "e1069.2",
            "source_info": {
                "paper_title": "Active Domain Randomization",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "ErgoPusher-v0 (sim-&gt;real)",
            "name_full": "ErgoPusher-v0 agent and zero-shot transfer to Poppy Ergo Jr.",
            "brief_description": "A simulated puck-pushing task analogous to Pusher-3DOF but with a real-robot analog; ADR-trained policies transfer to the physical robot with better or similar performance and notably lower variance compared to UDR-trained policies under manual environment perturbations.",
            "citation_title": "Active Domain Randomization",
            "mention_or_use": "use",
            "agent_name": "ErgoPusher-v0 agent",
            "agent_description": "Reinforcement learning agent trained in simulation with DDPG under ADR/UDR; evaluated via zero-shot transfer on a real Poppy Ergo Jr. robot performing puck-pushing tasks with manual changes in puck friction.",
            "agent_type": "simulated agent with physical robot evaluation (physical robot = Poppy Ergo Jr.)",
            "environment_name": "ErgoPusher-v0 (sim) and physical Poppy Ergo Jr. (real)",
            "environment_description": "Pushing task where puck friction and joint damping are randomized in sim (training ranges [0.67,1.0]×default) and real-robot evaluations alter puck friction manually (slippery vs rough) to test generalization and sim-to-real transfer.",
            "complexity_measure": "N_rand = 2 (puck friction loss and puck joint damping); difficulty measured by task success metrics like distance-to-target and overall performance across 25 random goals per policy.",
            "complexity_level": "medium (manipulation with contact dynamics and 2 randomized parameters; physical instantiation introduces unmodeled dynamics)",
            "variation_measure": "Domain randomization multipliers over friction/damping; training variation [0.67,1.0]×default, test/extrapolation [0.5,0.67]×default and manual real-world friction variations.",
            "variation_level": "medium",
            "performance_metric": "Distance-to-target / averaged task performance across 25 random goals; aggregate statistics over 125 evaluations per method per environment setting (5 policies × 25 goals).",
            "performance_value": "Qualitative: ADR policies obtain overall better or similar mean performance than UDR and display lower spread (variance) across all evaluated real-world perturbations; no exact numerical aggregates published in-text.",
            "complexity_variation_relationship": "Yes. The sim-to-real experiments highlight that uniform randomization can yield unpredictable real-world behavior due to unmodeled dynamics and that focusing sampling on informative simulated variations (ADR) leads to more robust and consistent real-world performance.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Active Domain Randomization (ADR) vs Uniform Domain Randomization (UDR). Policies are trained in sim and evaluated zero-shot on the real robot without fine-tuning.",
            "generalization_tested": true,
            "generalization_results": "ADR-trained policies transfer with better or comparable mean performance and with substantially lower variance across real-world environment perturbations (friction changes) compared to UDR-trained policies.",
            "sample_efficiency": "Simulation training run for 1M agent timesteps; ADR reuses same simulation budget but reallocates sampling to informative instances thereby improving transfer robustness.",
            "key_findings": "ADR reduces unpredictability and variance in sim-to-real transfer by adaptively finding and emphasizing informative simulated variations; uniform randomization can produce inconsistent real-world outcomes because it trains equally on uninformative or degenerate instances.",
            "uuid": "e1069.3",
            "source_info": {
                "paper_title": "Active Domain Randomization",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Learning dexterous in-hand manipulation",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2
        },
        {
            "paper_title": "Robust adversarial reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.0144015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Active Domain Randomization</h1>
<p>Bhairav Mehta<br>Mila, Université de Montréal<br>Manfred Diaz<br>Mila, Université de Montréal<br>Florian Golemo<br>Mila, Université de Montréal, ElementAI<br>Christopher J. Pal<br>Mila, Polytechnique Montréal, ElementAI, CIFAR<br>Liam Paull<br>Mila, Université de Montréal, CIFAR</p>
<h4>Abstract</h4>
<dl>
<dd>Domain randomization is a popular technique for improving domain transfer, often used in a zero-shot setting when the target domain is unknown or cannot easily be used for training. In this work, we empirically examine the effects of domain randomization on agent generalization. Our experiments show that domain randomization may lead to suboptimal, high-variance policies, which we attribute to the uniform sampling of environment parameters. We propose Active Domain Randomization, a novel algorithm that learns a parameter sampling strategy. Our method looks for the most informative environment variations within the given randomization ranges by leveraging the discrepancies of policy rollouts in randomized and reference environment instances. We find that training more frequently on these instances leads to better overall agent generalization. Our experiments across various physics-based simulated and real-robot tasks show that this enhancement leads to more robust, consistent policies.</dd>
</dl>
<p>Keywords: sim2real, domain randomization, reinforcement learning</p>
<h2>1 Introduction</h2>
<p>Recent trends in Deep Reinforcement Learning (DRL) exhibit a growing interest in zero-shot domain transfer, i.e. when a policy is learned in a source domain and is then tested without finetuning in a previously unseen target domain. Zero-shot transfer is particularly useful when the task in the target domain is inaccessible, complex, or expensive, such as gathering rollouts from a real-world robot. An ideal agent would learn to generalize across domains; it would accomplish the task without exploiting irrelevant features or deficiencies in the source domain (i.e., approximate physics in simulators), which may vary dramatically after transfer.
One promising approach for zero-shot transfer has been Domain Randomization (DR) [1]. In DR, we uniformly randomize environment parameters (i.e. friction, motor torque) in predefined ranges after every training episode. By randomizing everything that might vary in the target environment, the hope is that the agent will view the target domain as just another variation. However, recent works suggest that the sample complexity grows exponentially with the number of randomization parameters, even when dealing only with transfer between simulations (i.e. in Andrychowicz et al. [2] Figure 8). In addition, when using DR unsuccessfully, policy transfer fails, but with no clear way to understand the underlying cause. After a failed transfer, randomization ranges are tweaked heuristically via trial-and-error. Repeating this process iteratively leads to arbitrary ranges that do (or do not) lead to policy convergence without any insight into how those settings may affect the learned behavior.</p>
<p>In this work, we demonstrate that the strategy of uniformly sampling environment parameters from predefined ranges is suboptimal and propose an alternative sampling method, Active Domain Randomization. Active Domain Randomization (ADR), shown graphically in Figure 1, formulates</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ADR proposes randomized environments (c) or simulation instances from a simulator (b) and rolls out an agent policy (d) in those instances. The discriminator (e) learns a reward (f) as a proxy for environment difficulty by distinguishing between rollouts in the reference environment (a) and randomized instances, which is used to train SVPG particles (g). The particles propose a diverse set of environments, trying to find the environment parameters (h) that are currently causing the agent the most difficulty.</p>
<p>DR as a search for randomized environments that maximize utility for the agent policy. Concretely, we aim to find environments that <em>currently</em> cause difficulties for the agent policy, dedicating more training time to these troublesome parameter settings. We cast this active search as a Reinforcement Learning (RL) problem where the ADR sampling policy is parameterized with Stein Variational Policy Gradient (SVPG) [3]. ADR focuses on problematic regions of the randomization space by learning a discriminative reward computed from discrepancies in policy rollouts generated in randomized and reference environments.</p>
<p>We first showcase ADR on a simple environment where the benefits of training on more challenging variations are apparent and interpretable (Figure 2). In this case, we demonstrate that ADR learns to preferentially select parameters from these more challenging parameter regions while still adapting to the policy's current deficiencies. We then apply ADR to more complex environments and real robot settings (Figure 3) and show that even with high-dimensional search spaces and unmodeled dynamics, policies trained with ADR exhibit superior generalization and lower overall variance than their Uniform Domain Randomization (UDR) counterparts.</p>
<h1>2 Preliminaries</h1>
<h3>2.1 Reinforcement Learning</h3>
<p>We consider a RL framework [4] where some task $T$ is defined by a Markov Decision Process (MDP) consisting of a state space $S$, action space $A$, state transition function $P:S \times A \mapsto S$, reward function $R: S \times A \mapsto \mathbb{R}$, and discount factor $\gamma \in(0,1)$. The goal for an agent trying to solve $T$ is to learn a policy $\pi$ with parameters $\theta$ that maximizes the expected total discounted reward. We define a rollout $\tau=\left(s_{0}, a_{0} \ldots, s_{T}, a_{T}\right)$ to be the sequence of states $s_{t}$ and actions $a_{t} \sim \pi\left(a_{t} \mid s_{t}\right)$ executed by a policy $\pi$ in the environment.</p>
<h3>2.2 Stein Variational Policy Gradient</h3>
<p>Recently, Liu et al. [3] proposed SVPG, which learns an ensemble of policies $\mu_{\phi}$ in a maximum-entropy RL framework [5].</p>
<p>$$
\max <em _mu="\mu">{\mu} \mathbb{E}</em>(\mu)
$$}[J(\mu)]+\alpha \mathcal{H</p>
<p>with entropy $\mathcal{H}$ being controlled by temperature parameter $\alpha$. SVPG uses Stein Variational Gradient Descent [6] to iteratively update an ensemble of $N$ policies or particles $\mu_{\phi}=\left{\mu_{\phi_{i}}\right}_{i=1}^{N}$ using:</p>
<p>$$
\mu_{\phi_{i}} \leftarrow \mu_{\phi_{i}}+\frac{\epsilon}{N} \sum_{j=1}^{N}\left[\nabla_{\mu_{\phi_{j}}} J\left(\mu_{\phi_{j}}\right) k\left(\mu_{\phi_{j}}, \mu_{\phi_{i}}\right)+\alpha \nabla_{\mu_{\phi_{j}}} k\left(\mu_{\phi_{j}}, \mu_{\phi_{i}}\right)\right]
$$</p>
<p>with step size $\epsilon$ and positive definite kernel $k$. This update rule balances exploitation (first term moves particles towards high-reward regions) and exploration (second term repulses similar policies).</p>
<h3>2.3 Domain Randomization</h3>
<p>Domain randomization (DR) is a technique to increase the generalization capability of policies trained in simulation. DR requires a prescribed set of $N_{\text {rand }}$ simulation parameters to randomize, as well</p>
<p>as corresponding ranges to sample them from. A set of parameters is sampled from randomization space $\Xi \subset \mathbb{R}^{N_{\text {rand }}}$, where each randomization parameter $\xi^{(i)}$ is bounded on a closed interval $\left{\left[\xi_{l o w}^{(i)}, \xi_{h i g h}^{(i)}\right]\right}<em _rand="{rand" _text="\text">{i=1}^{N</em>$.
When a configuration $\xi \in \Xi$ is passed to a non-differentiable simulator $S$, it generates an environment $E$. At the start of each episode, the parameters are uniformly sampled from the ranges, and the environment generated from those values is used to train the agent policy $\pi$.
DR may perturb any to all elements of the task $T$ 's underlying MDP ${ }^{1}$, with the exception of keeping $R$ and $\gamma$ constant. DR therefore generates a set of MDPs that are superficially similar, but can vary greatly in difficulty depending on the character of the randomization. Upon transfer to the target domain, the expectation is that the agent policy has learned to generalize across MDPs, and sees the final domain as just another variation of parameters.
The most common instantiation of DR, UDR is summarized in Algorithm 2 in Appendix E. UDR generates randomized environment instances $E_{i}$ by uniformly sampling $\Xi$. The agent policy $\pi$ is then trained on rollouts $\tau_{i}$ produced in randomized environments $E_{i}$.}}</p>
<h1>3 Method</h1>
<p>To start, we would like to answer the following question:
Are all MDPs generated by uniform randomization equally useful for training?
We consider the LunarLander-v2 environment, where the agent's task is to ground a lander in a designated zone and reward is based on the quality of landing (fuel used, impact velocity, etc). LunarLander-v2 has one main axis of randomization that we vary: the main engine strength (MES).
We aim to determine if certain environment instances (different values of the MES) are more informative - more efficient than others - in terms of aiding generalization. We set the total range of variation for the MES to be $[8,20]^{2}$ and find through empirical tests that lower engine strengths generate harder MDPs to solve. Under this assumption, we show the effects of focused $D R$ by editing the range that the MES parameter is uniformly sampled from.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Agent generalization, expressed as performance across different engine strength settings in LunarLander. We compare the following approaches: Baseline (default environment dynamics); Uniform Domain Randomization (UDR); Active Domain Randomization (ADR, our approach); and Oracle (a handpicked randomization range of MES $[8,11])$. ADR achieves for near-expert levels of generalization, while both Baseline and UDR fail to solve lower MES tasks.</p>
<p>We train multiple agents on different randomization ranges for MES, which define what types of environments the agent is exposed to during training. Figure 2 shows the final generalization performance of each agent by sampling randomly from the entire randomization range of $[8,20]$ and rolling out the policy in the generated environments. We see that, in this case, focusing on harder MDPs improves generalization as compared to uniformly sampling the whole space, even when the evaluation environment is outside of the training distribution.</p>
<h3>3.1 Problem Formulation</h3>
<p>The experiment in the previous section shows that preferential training on more informative environments provides tangible benefits in terms of agent generalization. However, in general, finding these informative environments is difficult because: (1) It is rare that such intuitively hard MDP instances or parameter ranges are known beforehand and (2) DR is used mostly when the space of randomized parameters is high-dimensional</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>or noninterpretable. As a result, we propose an algorithm for finding environment instances that maximize utility, or provide the most improvement (in terms of generalization) to our agent policy when used for training.</p>
<h1>3.2 Active Domain Randomization</h1>
<p>Drawing analogies with Bayesian Optimization (BO) literature, one can consider the randomization space as a search space. Traditionally, in BO, the search for where to evaluate an objective is informed by acquisition functions, which trade off exploitation of the objective with exploration in the uncertain regions of the space [7]. However, unlike the stationary objectives seen in BO, training the agent policy renders our optimization non-stationary: the environment with highest utility at time $t$ is likely not the same as the maximum utility environment at time $t+1$. This requires us to redefine the notion of an acquisition function while simultaneously dealing with BO's deficiencies with higher-dimensional inputs [8].</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">Active</span><span class="w"> </span><span class="nt">Domain</span><span class="w"> </span><span class="nt">Randomization</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">Xi</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">Randomization</span><span class="w"> </span><span class="nt">space</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">S</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">Simulator</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">xi_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{ref</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">reference</span><span class="w"> </span><span class="nt">parameters</span>
<span class="w">    </span><span class="nt">Initialize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">agent</span><span class="w"> </span><span class="nt">policy</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">SVPG</span><span class="w"> </span><span class="nt">particles</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">D_</span><span class="p">{</span><span class="err">\psi</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span>
<span class="w">        </span><span class="nt">discriminator</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{ref</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">S</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">xi_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{ref</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">reference</span><span class="w"> </span><span class="nt">environment</span>
<span class="w">    </span><span class="nt">while</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="nt">max_timesteps</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">particle</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">rollout</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">xi_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="o">(</span><span class="err">\</span><span class="nt">cdot</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">xi_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="nt">Generate</span><span class="o">,</span><span class="w"> </span><span class="nt">rollout</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">randomized</span><span class="w"> </span><span class="nt">em</span><span class="o">:</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">S</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">xi_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="nt">rollout</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">cdot</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{ref</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">cdot</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{ref</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="w"> </span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{ref</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{ref</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="w"> </span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{ref</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="nt">Calculate</span><span class="w"> </span><span class="nt">reward</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">proposed</span><span class="w"> </span><span class="nt">environment</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">Calculate</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">r_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">xi_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">(</span><span class="nt">Eq</span><span class="o">.</span><span class="w"> </span><span class="o">(</span><span class="nt">3</span><span class="o">))</span>
<span class="w">        </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="nt">Gradient</span><span class="w"> </span><span class="nt">Updates</span>
<span class="w">        </span><span class="nt">with</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">update</span><span class="o">:</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="o">+</span><span class="err">\</span><span class="nt">nu</span><span class="w"> </span><span class="err">\</span><span class="nt">nabla_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="w"> </span><span class="nt">J</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Update</span><span class="w"> </span><span class="nt">particles</span><span class="w"> </span><span class="nt">using</span><span class="w"> </span><span class="nt">Eq</span><span class="o">.</span><span class="w"> </span><span class="o">(</span><span class="nt">2</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Update</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">D_</span><span class="p">{</span><span class="err">\psi</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{ref</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">using</span><span class="w"> </span><span class="nt">SGD</span><span class="o">.</span>
<span class="w">    </span><span class="nt">end</span><span class="w"> </span><span class="nt">while</span>
</code></pre></div>

<p>To this end, we propose ADR, summarized in Algorithm 1 and Figure $1^{3} . \quad$ ADR provides a framework for manipulating a more general analog of an $a c$ quisition function, selecting the most informative MDPs for the agent within the randomization space. By formulating the search as an RL problem, ADR learns a policy $\mu_{\phi}$ where states are proposed randomization configurations $\xi \in \Xi$ and actions are continuous changes to those parameters.</p>
<p>We learn a discriminator-based reward for $\mu_{\phi}$, similar the reward seen in Eysenbach et al. [9]:</p>
<p>$$
r_{D}=\log D_{\psi}\left(y \mid \tau_{i} \sim \pi\left(\cdot ; E_{i}\right)\right)
$$</p>
<p>where $y$ is a boolean variable denoting the discriminator's prediction of which type of environment (a randomized environment $E_{i}$ or reference environment $E_{\text {ref }}$ ) the trajectory $\tau_{i}$ was generated from. We assume that the $E_{\text {ref }}=S\left(\xi_{\text {ref }}\right)$ is provided with the original task definition.</p>
<p>Intuitively, we reward the policy $\mu_{\phi}$ for finding regions of the randomization space that produce environment instances where the same agent policy $\pi$ acts differently than in the reference environment. The agent policy $\pi$ sees and trains only on the randomized environments (as it would in traditional DR), using the environment's task-specific reward for updates. As the agent improves on the proposed, problematic environments, it becomes more difficult to differentiate whether any given state transition was generated from the reference or randomized environment. Thus, ADR can find what parts of the randomization space the agent is currently performing poorly on, and can actively update its sampling strategy throughout the training process.</p>
<h2>4 Results</h2>
<h3>4.1 Experiment Details</h3>
<p>To test ADR, we experiment on OpenAI Gym environments [10] across various tasks, both simulated and real: (a) LunarLander-v2, a 2 degrees of freedom (DoF) environment where the agent has to</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>softly land a spacecraft, implemented in Box2D (detailed in Section 3.2), (b) Pusher-3DOF-v0, a 3 DoF arm from Haarnoja et al. [11] that has to push a puck to a target, implemented in Mujoco [12], and (c) ErgoReacher-v0, a 4 DoF arm from Golemo et al. [13] which has to touch a goal with its end effector, implemented in the Bullet Physics Engine [14]. For sim2real experiments, we recreate this environment setup on a real Poppy Ergo Jr. robot [15] shown in Figure3 (a) and (b), and also create (d) ErgoPusher-v0 an environment similar to Pusher-3DOF-v0 with a real robot analog seen in Figure 3 (c) and (d). We provide a detailed account of the randomized parameters in each environment in Table 1 in Appendix F.</p>
<p>All simulated experiments are run with five seeds each with five random resets, totaling 25 independent trials per evaluation point. All experimental results are plotted mean-averaged with one standard deviation shown. Detailed experiment information can be found in Appendix H.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Along with simulated environments, we display ADR on zero-shot transfer tasks onto real robots.</p>
<h1>4.2 Toy Experiments</h1>
<p>To investigate whether ADR's learned sampling strategy provides a tangible benefit for agent generalization, we start by comparing it against traditional DR (labeled as UDR) on LunarLander-v2 and vary only the main engine strength (MES). In Figure 2, we see that ADR approaches expert-levels of generalization whereas UDR fails to generalize on lower MES ranges.
We compare the learning progress for the different methods on the hard environment instances $\left(\xi_{M E S} \sim U[8,11]\right)$ in Figure 4(a). ADR significantly outperforms both the baseline (trained only on MES of 13) and the UDR agent (trained seeing environments with $\xi_{M E S} \sim U[8,20]$ ) in terms of performance.
Figures 4(b) and 4(c) showcase the adaptability of ADR by showing generalization and sampling distributions at various stages of training. ADR samples approximately uniformly for the first 650 K steps, but then finds a deficiency in the policy on higher ranges of the MES. As those areas become more frequently sampled between $650 \mathrm{~K}-800 \mathrm{~K}$ steps, the agent learns to solve all of the higher-MES environments, as shown by the generalization curve for 800 K steps. As a result, the discriminator is no longer able to differentiate reference and randomized trajectories from the higher MES regions, and starts to reward environment instances generated in the lower end of the MES range, which improves generalization towards the completion of training.</p>
<h3>4.3 Randomization in High Dimensions</h3>
<p>If the intuitions that drive ADR are correct, we should see increased benefit of a learned sampling strategy with larger $N_{\text {rand }}$ due to the increasing sparsity of informative environments when sampling uniformly. We first explore ADR's performance on Pusher3Dof-v0, an environment where $N_{\text {rand }}=$ 2. Both randomization dimensions (puck damping, puck friction loss) affect whether or not the puck retains momentum and continues to slide after making contact with the agent's end effector. Lowering the values of these parameters simultaneously creates an intuitively-harder environment, where the puck continues to slide after being hit. In the reference environment, the puck retains no momentum and must be continuously pushed in order to move. We qualitatively visualize the effect of these parameters on puck sliding in Figure 5(a).
From Figure 5(b), we see ADR's improved robustness to extrapolation - or when the target domain lies outside the training region. We train two agents, one using ADR and one using UDR, and show them only the training regions encapsulated by the dark, outlined box in the top-right of Figure 5(a).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Learning curves over time in LunarLander. Higher is better. (a) Performance on particularly difficult settings - our approach outperforms both the policy trained on a single simulator instance ("baseline") and the UDR approach. (b) Agent generalization in LunarLander over time during training when using ADR. (c) Adaptive sampling visualized. ADR, seen in (b) and (c), adapts to where the agent is struggling the most, improving generalization performance by end of training.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: In Pusher-3Dof, the environment dynamics are characterized by friction and damping of the sliding puck, where sliding correlates with the difficulty of the task (as highlighted by cyan, purple, and pink - from easy to hard). (a) During training, the algorithm only had access to a limited, easier range of dynamics (black outlined box in the upper right). (b) Performance measured by distance to target, lower is better.</p>
<p>Qualitatively, only $25 \%$ of the environments have dynamics which cause the puck to slide, which are the hardest environments to solve in the training region. We see that from the sampling histogram overlaid on Figure 5(a) that ADR prioritizes the single, harder purple region more than the light blue regions, allowing for better generalization to the unseen test domains, as shown in Figure 5(b). ADR outperforms UDR in all but one test region and produces policies with less variance than their UDR counterparts.</p>
<h1>4.4 Randomization in Uninterpretable Dimensions</h1>
<p>We further show the significance of ADR over UDR on ErgoReacher-v0, where $N_{\text {rand }}=8$. It is now impossible to infer intuitively which environments are hard due to the complex interactions between the eight randomization parameters (gains and maximum torques for each joint). For demonstration purposes, we test extrapolation by creating a held-out target environment with extremely low values for torque and gain, which causes certain states in the environment to lead to catastrophic failure - gravity pulls the robot end effector down, and the robot is not strong enough to pull itself back up. We show an example of an agent getting trapped in a catastrophic failure state in Figure 11, Appendix F.1.</p>
<p>To generalize effectively, the sampling policy should prioritize environments with lower torque and gain values in order for the agent to operate in such states precisely. However, since the hard evaluation environment is not seen during training, ADR must learn to prioritize the hardest environments that it can see, while still learning behaviors that can operate well across the entire training region.</p>
<p>From Figure 6(a) (learning curves for Pusher3Dof-v0 on the unseen, hard environment - the pink square in Figure 5) and 6(b) (learning curves for ErgoReacher-v0 on unseen, hard environment), we observe the detrimental effects of uniform sampling. In Pusher3Dof-v0, we see that UDR unlearns the good behaviors it acquired in the beginning of training. When training neural networks in both supervised and reinforcement learning settings, this phenomenon has been dubbed as catastrophic forgetting [16]. ADR seems to exhibit this slightly (leading to "hills" in the curve), but due to the adaptive nature the algorithm, it is able to adjust quickly and retain better performance across all environments.
UDR's high variance on ErgoReacher-v0 highlights another issue: by continuously training on a random mix of hard and easy MDP instances, both beneficial and detrimental agent behaviors can be learned and unlearned throughout training. This mixing can lead to high-variance, inconsistent, and unpredictable behavior upon transfer. By focusing on those harder environments and allowing the definition of hard to adapt over time, ADR shows more consistent performance and better overall generalization than UDR in all environments tested.</p>
<h3>4.5 Sim2Real Transfer Experiments</h3>
<p>In sim2real (simulation to reality) transfer, many policies fail due to unmodeled dynamics within the simulators, as policies may have overfit to or exploited simulation-specific details of their training environments. While the deficiencies and high variance of UDR are clear even in simulated environments, one
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Learning curves over time in (a) Pusher3Dof-v0 and (b) ErgoReacher on held-out, difficult environment settings. Our approach outperforms both the policy trained with the UDR approach both in terms of performance and variance.
of the most impressive results of domain randomization was zero-shot transfer out of simulation onto robots. However, we find that the same issues of unpredictable performance apply to UDR-trained policies in the real world as well.
We take each method's (ADR and UDR) five independent simulation-trained policies on both ErgoReacher-v0 and ErgoPusher-v0 and transfer them without fine tuning onto the real robot. We rollout only the final policy on the robot, and show performance in Figure 7. To evaluate generalization, we alter the environment manually: on ErgoReacher-v0, we change the values of the torques (higher torque means the arm moves at higher speed and accelerates faster); on ErgoPusher-v0, we change the friction of the sliding puck (slippery or rough). For each environment, we evaluate each of the policies with 25 random goals ( 125 independent evaluations per method per environment setting).
Even in zero-shot transfer tasks onto real robots, ADR policies obtain overall better or similar performance than UDR policies trained in the same conditions. More importantly, ADR policies are more consistent and display lower spread across all environments, which is crucial when safely evaluating reinforcement learning policies on real-world robots.</p>
<h2>5 Related Work</h2>
<h3>5.1 Dynamic and Adversarial Simulators</h3>
<p>Simulators have played a crucial role in transferring learned policies onto real robots, and many different strategies have been proposed. Randomizing simulation parameters for better generalization or transfer performance is a well-established idea in evolutionary robotics [17, 18], but recently has emerged as an effective way to perform zero-shot transfer of deep reinforcement learning policies in difficult tasks $[2,1,19,20]$.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Zero-shot transfer onto real robots (a) ErgoReacher and (b) ErgoPusher. In both environments, we assess generalization by manually changing torque strength and puck friction respectively.</p>
<p>Learnable simulations are also an effective way to adapt a simulation to a particular target environment. Chebotar et al. [21] and Ruiz et al. [22] use RL for effective transfer by learning parameters of a simulation that accurately describes the target domain, but require the target domain for reward calculation, which can lead to overfitting. In contrast, our approach requires no target domain, but rather only a reference domain (the default simulation parameters) and a general range for each parameter. ADR encourages diversity, and as a result gives the agent a wider variety of experience. In addition, unlike Chebotar et al. [21], our method does not requires carefully-tuned (co)variances or task-specific cost functions. Concurrently, Khirodkar et al. [23] also showed the advantages of learning adversarial simulations and disadvantages of purely uniform randomization distributions in object detection tasks.</p>
<p>To improve policy robustness, Robust Adversarial Reinforcement Learning (RARL) Pinto et al. [24] jointly trains both an agent and an adversary who applies environment forces that disrupt the agent's task progress. ADR removes the zero-sum game dynamics, which have been known to decrease training stability [25]. More importantly, our method's final outputs - the SVPG-based sampling strategy and discriminator - are reusable and can be used to train new agents as shown in Appendix D, whereas a trained RARL adversary would overpower any new agent and impede learning progress.</p>
<h3>5.2 Active Learning and Informative Samples</h3>
<p>Active learning methods in supervised learning try to construct a representative, sometimes time-variant, dataset from a large pool of unlabelled data by proposing elements to be labeled. The chosen samples are labelled by an oracle and sent back to the model for use. Similarly, ADR searches for what environments may be most useful to the agent at any given time. Active learners, like BO methods discussed in Section 3, often require an acquisition function (derived from a notion of model uncertainty) to chose the next sample. Since ADR handles this decision through the explore-exploit framework of RL and the $\alpha$ in SVPG, ADR sidesteps the well-known scalability issues of both active learning and BO [26].
Recently, Toneva et al. [27] showed that certain examples in popular computer vision datasets are harder to learn, and that some examples are forgotten much quicker than others. We explore the same phenomenon in the space of MDPs defined by our randomization ranges, and try to find the "examples" that cause our agent the most trouble. Unlike in active learning or Toneva et al. [27], we have no oracle or supervisory loss signal in RL, and instead attempt to learn a proxy signal for ADR via a discriminator.</p>
<h3>5.3 Generalization in Reinforcement Learning</h3>
<p>Generalization in RL has long been one of the holy grails of the field, and recent work like Packer et al. [28], Cobbe et al. [29], and Farebrother et al. [30] highlight the tendency of deep RL policies to overfit to details of the training environment. Our experiments exhibit the same phenomena, but our method improves upon the state of the art by explicitly searching for and varying the environment aspects that our agent policy may have overfit to. We find that our agents, when trained more frequently on these problematic samples, show better generalization over all environments tested.</p>
<h2>6 Conclusion</h2>
<p>In this work, we highlight failure cases of traditional domain randomization, and propose active domain randomization (ADR), a general method capable of finding the most informative parts of</p>
<p>the randomization parameter space for a reinforcement learning agent to train on. ADR does this by posing the search as a reinforcement learning problem, and optimizes for the most informative environments using a learned reward and multiple policies. We show on a wide variety of simulated environments that this method efficiently trains agents with better generalization than traditional domain randomization, extends well to high dimensional parameter spaces, and produces more robust policies when transferring to the real world.</p>
<h1>References</h1>
<p>[1] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on. IEEE, 2017.
[2] M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018.
[3] Y. Liu, P. Ramachandran, Q. Liu, and J. Peng. Stein variational policy gradient, 2017.
[4] R. S. Sutton and A. G. Barto. Reinforcement Learning: An introduction. MIT Press, 2018.
[5] B. D. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy. PhD thesis, CMU, 2010.
[6] Q. Liu and D. Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29. 2016.
[7] E. Brochu, V. M. Cora, and N. de Freitas. A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. CoRR, abs/1012.2599, 2010. URL http://arxiv.org/abs/1012.2599.
[8] Z. Wang, M. Zoghi, F. Hutter, D. Matheson, and N. De Freitas. Bayesian optimization in high dimensions via random embeddings. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, 2013.
[9] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.
[10] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016.
[11] T. Haarnoja, V. Pong, A. Zhou, M. Dalal, P. Abbeel, and S. Levine. Composable deep reinforcement learning for robotic manipulation. arXiv preprint arXiv:1803.06773, 2018.
[12] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In IROS. IEEE, 2012.
[13] F. Golemo, A. A. Taiga, A. Courville, and P.-Y. Oudeyer. Sim-to-real transfer with neuralaugmented robot simulation. In Conference on Robot Learning, 2018.
[14] E. Coumans. Bullet physics simulation. In ACM SIGGRAPH 2015 Courses, SIGGRAPH '15, New York, NY, USA, 2015. ACM.
[15] M. Lapeyre. Poppy: open-source, 3D printed and fully-modular robotic platform for science, art and education. Theses, Université de Bordeaux, Nov. 2014. URL https://hal.inria. fr/tel-01104641.
[16] J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. CoRR, abs/1612.00796, 2016. URL http://arxiv.org/abs/1612.00796.
[17] J. C. Zagal, J. Ruiz-del Solar, and P. Vallejos. Back to reality: Crossing the reality gap in evolutionary robotics. IFAC Proceedings Volumes, 37(8), 2004.</p>
<p>[18] J. Bongard and H. Lipson. Once more unto the breach: Co-evolving a robot and its simulator. In Proceedings of the Ninth International Conference on the Simulation and Synthesis of Living Systems (ALIFE9), 2004.
[19] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. 2018 IEEE International Conference on Robotics and Automation (ICRA), May 2018.
[20] F. Sadeghi and S. Levine. (cad)\$^2\$rl: Real single-image flight without a single real image. CoRR, abs/1611.04201, 2016. URL http://arxiv.org/abs/1611.04201.
[21] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. arXiv preprint arXiv:1810.05687, 2018.
[22] N. Ruiz, S. Schulter, and M. Chandraker. Learning to simulate, 2018.
[23] R. Khirodkar, D. Yoo, and K. M. Kitani. Vadra: Visual adversarial domain randomization and augmentation. arXiv preprint arXiv:1812.00491, 2018.
[24] L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta. Robust adversarial reinforcement learning, 2017.
[25] L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for GANs do actually converge? In International Conference on Machine Learning, 2018.
[26] S. Tong. Active Learning: Theory and Applications. PhD thesis, 2001. AAI3028187.
[27] M. Toneva, A. Sordoni, R. T. d. Combes, A. Trischler, Y. Bengio, and G. J. Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018.
[28] C. Packer, K. Gao, J. Kos, P. Krhenbhl, V. Koltun, and D. Song. Assessing generalization in deep reinforcement learning, 2018.
[29] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in reinforcement learning. $C o R R$, abs/1812.02341, 2018.
[30] J. Farebrother, M. C. Machado, and M. Bowling. Generalization and regularization in DQN, 2018.
[31] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[32] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2014.
[33] S. Fujimoto, H. van Hoof, and D. Meger. Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning, 2018.
[34] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928-1937, 2016.
[35] T. Gangwani, Q. Liu, and J. Peng. Learning self-imitating diverse policies. In International Conference on Learning Representations, 2019.</p>
<h1>Appendix A Architecture Walkthrough</h1>
<p>In this section, we walk through the diagram shown in Figure 1. All line references refer to Algorithm 1 .</p>
<h2>A.0.1 SVPG Sampler</h2>
<p>To encourage sufficient exploration in high dimensional randomization spaces, we parameterize $\mu_{\phi}$ with SVPG. Since each particle proposes its own environment settings $\xi_{i}$ (lines 4-6, Figure 1h), all of which are passed to the agent for training, the agent policy benefits from the same environment variety seen in UDR. However, unlike UDR, $\mu_{\phi}$ can use the learned reward to focus on problematic MDP instances while still being efficiently parallelizable.</p>
<h2>A.0.2 Simulator</h2>
<p>After receiving each particle's proposed parameter settings $\xi_{i}$, we generate randomized environments $E_{i}=S\left(\xi_{i}\right)$ (line 9, Figure 1b).</p>
<h2>A.0.3 Generating Trajectories</h2>
<p>We proceed to train the agent policy $\pi$ on the randomized instances $E_{i}$, just as in UDR. We roll out $\pi$ on each randomized instance $E_{i}$ and store each trajectory $\tau_{i}$. For every randomized trajectory generated, we use the same policy to collect and store a reference trajectory $\tau_{r e f}$ by rolling out $\pi$ in the default environment $E_{r e f}$ (lines 10-12, Figure 1a, c). We store all trajectories (lines 11-12) as we will use them to score each parameter setting $\xi_{i}$ and update the discriminator.
The agent policy is a black box: although in our experiments we train $\pi$ with Deep Deterministic Policy Gradients [31], the policy can be trained with any other on or off-policy algorithm by introducing only minor changes to Algorithm 1 (lines 13-17, Figure 1d).</p>
<h2>A.0.4 Scoring Environments</h2>
<p>We now generate a score for each environment (lines 20-22) using each stored randomized trajectory $\tau_{i}$ by passing them through the discriminator $D_{\psi}$, which predicts the type of environment (reference or randomized) each trajectory was generated from. We use this score as a reward to update each SVPG particle using Equation 2 (lines 24-26, Figure 1f).
After scoring each $\xi_{i}$ according to Equation 3, we use the randomized and reference trajectories to train the discriminator (lines 28-30, Figure 1e).</p>
<h2>Appendix B Learning Curves for Reference Environments</h2>
<p>For space concerns, we show only the hard generalization curves for all environments in the main document. For completeness, we include learning curves on the reference environment here.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Learning curves over time reference environments. (a) LunarLander (b) Pusher-3Dof (c) ErgoReacher.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Sampling frequency across engine strengths when varying the randomization ranges. The updated, red distribution shows a much milder unevenness in the distribution, while still learning to focus on the harder instances.</p>
<h1>Appendix C Interpretability Benefits of ADR</h1>
<p>One of the secondary benefits of ADR is its insight into incompatibilities between the task and randomization ranges. We demonstrate the simple effects of this phenomenon in a one-dimensional LunarLander-v2, where we only randomize the main engine strength. Our initial experiments varied this parameter between 6 and 20, which lead to ADR learning degenerate agent policies by learning to propose the lopsided blue distribution in Figure 9. Upon inspection of the simulation, we see that when the parameter has a value of less than approximately 8 , the task becomes almost impossible to solve due to the other environment factors (in this case the lander always hits the ground too fast, which it is penalized for).</p>
<p>After adjusting the parameter ranges to more sensible values, we see a better sampled distribution in pink, which still gives more preference to the hard environments in the lower engine strength range. Most importantly, ADR allows for analysis that is both focused - we know exactly what part of the simulation is causing trouble - and pre-transfer, i.e. done before a more expensive experiment such as real robot transfer has taken place. With UDR, the agents would be equally trained on these degenerate environments, leading to policies with potentially undefined behavior (or, as seen in Section 4.4, unlearn good behaviors) in these truly out-of-distribution simulations.</p>
<h2>Appendix D Bootstrapping Training of New Agents</h2>
<p>Unlike DR, ADR's learned sampling strategy and discriminator can be reused to train new agents from scratch. To test the transferability of the sampling strategy, we first train an instance of ADR on LunarLander-v2, and then extract the SVPG particles and discriminator. We then replace the agent policy with an random network initialization, and once again train according the the details in Section 4.1. From Figure 10(a), it can be seen that the bootstrapped agent generalization is even better than the one learned with ADR from scratch. However, its training speed on the default environment $\left(\xi_{M E S}=13\right)$ is relatively lower.</p>
<h2>Appendix E Uniform Domain Randomization</h2>
<p>Here we review the algorithm for Uniform Domain Randomization (UDR), first proposed in [1], shown in Algorithm 2.</p>
<h2>Appendix F Environment Details</h2>
<p>Please see Table 1.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Generalization and default environment learning progression on LunarLander-v2 when using ADR to bootstrap a new policy. Higher is better.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="w"> </span><span class="nt">Uniform</span><span class="w"> </span><span class="nt">Sampling</span><span class="w"> </span><span class="nt">Domain</span><span class="w"> </span><span class="nt">Randomization</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="nt">Xi</span><span class="o">:</span><span class="w"> </span><span class="nt">Randomization</span><span class="w"> </span><span class="nt">space</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">S</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">Simulator</span>
<span class="w">    </span><span class="nt">Initialize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">agent</span><span class="w"> </span><span class="nt">policy</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">episode</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="nt">Uniformly</span><span class="w"> </span><span class="nt">sample</span><span class="w"> </span><span class="nt">parameters</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">i</span><span class="o">=</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">xi</span><span class="o">^</span><span class="p">{</span><span class="err">(i)</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="nt">U</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="o">\</span><span class="nx">xi_</span><span class="p">{</span><span class="o">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">low</span><span class="w"> </span><span class="p">}}^{(</span><span class="nx">i</span><span class="p">)},</span><span class="w"> </span><span class="o">\</span><span class="nx">xi_</span><span class="p">{</span><span class="o">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">high</span><span class="w"> </span><span class="p">}}^{(</span><span class="nx">i</span><span class="p">)}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="nt">Generate</span><span class="o">,</span><span class="w"> </span><span class="nt">rollout</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">randomized</span><span class="w"> </span><span class="nt">env</span><span class="o">.</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">E_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">S</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">xi_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">rollout</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">cdot</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">E_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="w"> </span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">gradient</span><span class="w"> </span><span class="nt">step</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="nt">Agent</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="nt">update</span>
<span class="w">            </span><span class="nt">with</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">update</span><span class="o">:</span>
<span class="w">                </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="o">+</span><span class="err">\</span><span class="nt">nu</span><span class="w"> </span><span class="err">\</span><span class="nt">nabla_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="w"> </span><span class="nt">J</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">    </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
</code></pre></div>

<h1>F. 1 Catastrophic Failure States in ErgoReacher</h1>
<p>In Figure 11, we show an example progression to a catastrophic failure state in the held-out, simulated target environment of ErgoReacher-v0, with extremely low torque and gain values.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: An example progression (left to right) of an agent moving to a catastrophic failure state (Panel 4) in the hard ErgoReacher-v0 environment.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Environment</th>
<th style="text-align: left;">$N_{\text {rand }}$</th>
<th style="text-align: left;">Types of Randomizations</th>
<th style="text-align: left;">Train Ranges</th>
<th style="text-align: left;">Test Ranges</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LunarLander-v2</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Main Engine Strength</td>
<td style="text-align: left;">$[8,20]$</td>
<td style="text-align: left;">$[8,11]$</td>
</tr>
<tr>
<td style="text-align: left;">Pusher-3DOF-v0</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">Puck Friction Loss \&amp; Puck Joint Damping</td>
<td style="text-align: left;">$[0.67,1.0] \times$ default</td>
<td style="text-align: left;">$[0.5,0.67] \times$ default</td>
</tr>
<tr>
<td style="text-align: left;">ErgoPusher-v0</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">Puck Friction Loss \&amp; Puck Joint Damping</td>
<td style="text-align: left;">$[0.67,1.0] \times$ default</td>
<td style="text-align: left;">$[0.5,0.67] \times$ default</td>
</tr>
<tr>
<td style="text-align: left;">ErgoReacher-v0</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">Joint Damping</td>
<td style="text-align: left;">$[0.3,2.0] \times$ default</td>
<td style="text-align: left;">$0.2 \times$ default</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Joint Max Torque</td>
<td style="text-align: left;">$[1.0,4.0] \times$ default</td>
<td style="text-align: left;">default</td>
</tr>
</tbody>
</table>
<p>Table 1: We summarize the environments used, as well as characteristics about the randomizations performed in each environment.</p>
<h1>Appendix G Untruncated Plots for Lunar Lander</h1>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Generalization on LunarLander-v2 for an expert interval selection, ADR, and UDR. Higher is better.</p>
<p>All policies on Lunar Lander described in our paper receive a Solved score when the engine strengths are above 12, which is why truncated plots are shown in the main document. For clarity, we show the full, untruncated plot in Figure 12.</p>
<h2>Appendix H Network Architectures and Experimental Hyperparameters</h2>
<p>All experiments can be reproduced using our Github repository ${ }^{4}$.
All of our experiments use the same network architectures and experiment hyperparameters, except for the number of particles $N$. For any experiment with LunarLander-v2, we use $N=10$. For both other environments, we use $N=15$. All other hyperparameters and network architectures remain constant, which we detail below. All networks use the Adam optimizer [32].
We run Algorithm 1 until 1 million agent timesteps are reached - i.e. the agent policy takes 1 M steps in the randomized environments. We also cap each episode off a particular number of timesteps according to the documentation associated with [10]. In particular, LunarLander-v2 has an episode time limit of 1000 environment timesteps, whereas both Pusher-3DOF-v0 and ErgoReacher-v0 use an episode time limit of 100 timesteps.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For our agent policy, we use an implementation of DDPG (particularly, OurDDPG.py) from the Github repository associated with [33]. The actor and critic both have two hidden layers of 400 and 300 neurons respectively, and use ReLU activations. Our discriminator-based rewarder is a two-layer neural network, both layers having 128 neurons. The hidden layers use tanh activation, and the network outputs a sigmoid for prediction.</p>
<p>The agent particles in SVPG are parameterized by a two-layer actor-critic architecture, both layers in both networks having 100 neurons. We use Advantage Actor-Critic (A2C) to calculate unbiased and low variance gradient estimates. All of the hidden layers use tanh activation and are orthogonally initialized, with a learning rate of 0.0003 and discount factor $\gamma=0.99$. They operate on a $\mathbb{R}^{N_{\text {rand }}}$ continuous space, with each axis bounded between $[0,1]$. We allow for set the max step length to be 0.05 , and every 50 timesteps, we reset each particle and randomly initialize its state using a $N_{\text {rand }}$-dimensional uniform distribution. We use a temperature $\alpha=10$ with an RBF-Kernel as was done in [3]. In our work we use an Radial Basis Function (RBF) kernel with median baseline as described in Liu et al. [3] and an A2C policy gradient estimator [34], although both the kernel and estimator could be substituted with alternative methods [35]. To ensure diversity of environments throughout training, we always roll out the SVPG particles using a non-deterministic sample.</p>
<p>For DDPG, we use a learning rate $\nu=0.001$, target update coefficient of 0.005 , discount factor $\gamma=0.99$, and batch size of 1000 . We let the policy run for 1000 steps before any updates, and clip the max action of the actor between $[-1,1]$ as prescribed by each environment.</p>
<p>Our discriminator-based reward generator is a network with two, 128-neuron layers with a learning rate of .0002 and a binary cross entropy loss (i.e. is this a randomized or reference trajectory). To calculate the reward for a trajectory for any environment, we split each trajectory into its $\left(s_{t}, a_{t}, s_{t+1}\right)$ constituents, pass each tuple through the discriminator, and average the outputs, which is then set as the reward for the trajectory. Our batch size is set to be 128, and most importantly, as done in [9], we calculate the reward for examples before using those same examples to train the discriminator.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Code link will be updated after review.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
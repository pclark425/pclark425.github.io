<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8228 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8228</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8228</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-266690555</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.17259v2.pdf" target="_blank">Empowering Working Memory for Large Language Model Agents</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning. This paper explores the potential of applying cognitive psychology's working memory frameworks, to enhance LLM architecture. The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links. To address this, an innovative model is proposed incorporating a centralized Working Memory Hub and Episodic Buffer access to retain memories across episodes. This architecture aims to provide greater continuity for nuanced contextual reasoning during intricate tasks and collaborative scenarios. While promising, further research is required into optimizing episodic memory encoding, storage, prioritization, retrieval, and security. Overall, this paper provides a strategic blueprint for developing LLM agents with more sophisticated, human-like memory capabilities, highlighting memory mechanisms as a vital frontier in artificial general intelligence.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8228.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8228.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Working Memory Hub + Episodic Buffer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Centralized Working Memory Hub with Episodic Buffer access</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed LLM-agent memory architecture that centralizes storage of all inputs/outputs and interaction histories (Working Memory Hub) and exposes an Episodic Buffer that preserves and supplies complete interaction episodes to the LLM for contextual reasoning across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM Agent (proposed architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-centered agent architecture where the Central Processor (LLM) interacts with an External Environment Interface, an Interaction History Window, a persistent Working Memory Hub, and an Episodic Buffer to enable cross-episode continuity and multi-agent shared memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex sequential reasoning and multi-agent collaborative tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require maintaining continuity across multiple interaction episodes (multi-step planning, collaborative decision-making, sequential problem solving) and leveraging past episodes to inform current decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / multi-agent coordination</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working + episodic (persistent external hub)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Centralized persistent store (Working Memory Hub) that records all inputs/outputs and histories; Interaction History Window provides short-term cache (rolling window or abstractive summaries); Episodic Buffer retrieves complete episodes from the Hub on demand.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw inputs/outputs, interaction histories, complete episodic traces, abstractive summaries or extracts for recent context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Episodic retrieval via Episodic Buffer; short-term retrieval via Interaction History Window (rolling window, abstractive summary, or extracts); can be combined with SQL/full-text/semantic searches against the Hub.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No empirical ablation or quantitative comparison reported; architecture proposed to address continuity and multi-agent memory sharing limitations of standard LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Centralizing memories enables persistent cross-episode continuity and provides both short-term and long-term access patterns (rolling window and episodic access), potentially improving sequential reasoning and multi-agent cooperation; detailed mechanisms for encoding/prioritization/retrieval require further research.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No experimental validation provided; open problems include how to determine memory relevance and retrieval priorities, efficient compression of episodic memories, integration costs, and security/privacy of shared persistent memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8228.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Interaction History Window</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interaction History Window (short-term cache)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A component that maintains a short-term cache of recent interactions (e.g., a rolling window, abstractive summary, or extracts) to provide immediate contextual anchoring within an episode.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM Agent (proposed architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM uses an Interaction History Window to access recent context within token-window constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Short-term dialogue continuity / in-episode reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Providing the most recent conversational or action context to the LLM during ongoing tasks to inform immediate decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>short-term context maintenance / in-episode reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working (short-term cache)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Rolling window of latest dialogues or abstractive summaries stored/accessed from the Working Memory Hub.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recent dialogue turns, condensed summaries, or pertinent extracts</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency-based retrieval (rolling window) or retrieval of abstractive summaries from the Working Memory Hub.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No empirical tests reported; described qualitatively as necessary to mitigate token-window limits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interaction History Window provides flexible short-term context representations to mitigate token-limit constraints and supports more fluid in-episode reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Selecting window size and summary strategies, and integration with longer-term episodic memory remain open issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8228.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory Management Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory Management Agent (specialized retrieval manager)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dedicated agent in multi-agent systems that manages, sorts, prioritizes, and retrieves historical data (chat/action histories) to support planning and reduce retrieval overhead for other agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory Management Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A specialized MAS component responsible for curating and returning the most relevant memories (recent vs historical, task-relevant) to planning or execution agents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Planning and strategic decision-making in MAS</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide curated memory slices tailored to planning horizon (short-term experiments vs long-term studies) to speed up and focus other agents' planning processes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / multi-agent coordination</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic/working (curated access)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Centralized retrieval and curation service that filters and prioritizes memory segments from the Working Memory Hub based on planning needs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Chat histories, action histories, relevant past interactions and outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Context-aware curated retrieval (prioritization rules vary by task horizon), e.g., recent interactions for short-term tasks and mixed historical trends for long-term tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No quantitative ablation; described qualitatively as improving efficiency compared to agents independently searching full histories.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Centralizing retrieval into a Memory Management Agent reduces computational overhead for planning agents and can provide better, context-appropriate memory slices for improved planning quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires design of prioritization policies and may introduce a single point of failure or coordination bottleneck; security and access control need consideration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8228.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Role-Based Memory Access</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Role-Based Memory Access Control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory access strategy in multi-agent systems where agents are granted memory retrieval rights according to their designated roles (e.g., supervisor has broad access, specialists have narrow access).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generic LLM Agents with RBAC</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents whose memory access is constrained or enabled based on assigned roles to balance efficiency and oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Supervisory monitoring and specialized task execution in MAS</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Supervisor agents require broad historical visibility to coordinate; specialist agents need only their own recent memory to focus on specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-agent coordination / role-specific tasks</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic/working (access-restricted)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Role-based access control to the Working Memory Hub and Episodic Buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Agent-specific interactions and shared histories; access rights determine which memories are retrievable.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Access mediated by role-rules; retrieval queries limited to allowed memory partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No quantitative results; recommended as a mechanism to balance adaptability and security/efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Role-based rules can reduce unnecessary retrieval and tailor information exposure, but overly rigid roles may limit adaptability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires careful design to avoid stifling agent flexibility and to prevent excessive information exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8228.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task-Based Memory Access</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-Based Memory Access</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strategy where memory access is determined by the specific task requirements; agents receive memories directly relevant to the current task rather than broad role-derived access.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generic LLM Agents (task-driven retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that receive memory segments selected based on the semantics and needs of the current assigned task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Task-specific retrieval scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>When an agent is assigned a specific task, the system supplies memories that are directly relevant to that task's objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>task-focused retrieval / efficiency optimization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic/working (task-scoped)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Task-specification-driven selection of memory slices from the Working Memory Hub or Episodic Buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Task-relevant past interactions, outcomes, or data</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Task-scoped filtering and retrieval (semantic or metadata filters tied to task descriptors).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No empirical comparisons reported; conceptually tighter focus than role-based access.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Task-based access can increase relevance and reduce retrieval overhead by exposing only directly pertinent memories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires accurate task-memory relevancy mapping; risk of excluding useful but not explicitly labeled memories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8228.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autonomous Memory Access</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous Memory Access (agent-driven retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An access paradigm where agents autonomously decide which memory segments to fetch based on contextual cues, rather than relying on hard-coded role/task boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Autonomous LLM Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents endowed with autonomy to query the Working Memory Hub/Episodic Buffer to fetch memory segments they assess as relevant to their current context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open retrieval scenarios (broad research, synthesis, review tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents performing tasks like broad literature review or synthesis may autonomously retrieve diverse memory segments relevant to context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>open-ended retrieval / flexible reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic/working (autonomously accessed)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Self-determined retrieval using contextual heuristics or learned policies to query the Working Memory Hub.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Wide-ranging past interactions and knowledge traces selected by the agent</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Agent-driven selection potentially using semantic/keyword queries and contextual scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No experiments reported; contrasted qualitatively with more restrictive task-specific access.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Autonomy enables flexible retrieval and can surface relevant cross-domain memories but may increase risk of irrelevant retrieval or security exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Needs safeguards to limit irrelevant or sensitive memory access; requires mechanisms to evaluate retrieval relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8228.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Collaboration-Scenario Memory Access</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Collaboration Scenario-Based Memory Access</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of memory access strategies tailored to collaboration patterns: parallel collaborations may require shared full-memory access, while sequential collaborations pass memory context from one agent to the next.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Collaborative LLM Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents in collaborative workflows use different memory sharing strategies depending on whether collaboration is parallel or sequential.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Collaborative multi-agent workflows</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Examples include sequential analysis pipelines where one agent's output becomes the next agent's input, or parallel analyses requiring shared memory access.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-agent collaboration</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>shared episodic/working memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Shared access policies: full shared memory for parallel collaborations; handoff or selective access for sequential collaborations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Shared findings, intermediate outputs, and preceding agent outputs</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Policy-driven retrieval based on the collaboration topology (shared queries for parallel; sequential retrieval/handoff for pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No quantitative evaluations; described conceptually to illustrate different sharing needs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adapting memory access to collaboration type improves relevance and efficiency of inter-agent coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires coordination protocols, access control, and mechanisms to maintain consistency and privacy across agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8228.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SQL Search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SQL Search (structured queries against memory store)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval mechanism for agent memories where agents query structured memory repositories using SQL to obtain precise, timestamped records or tag-filtered slices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generic LLM Agents (using SQL search)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents using SQL queries to precisely retrieve memory entries by timestamp, tag, or structured fields from the Working Memory Hub.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Chronological or precise memory retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require exact snapshots (e.g., 'what did we discuss last Tuesday?') or structured retrieval by metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>memory retrieval / factual recall</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external structured memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Executing SQL queries against structured database-backed memory stores to retrieve targeted records.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured records with tags, timestamps, and fields representing interactions</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>SQL queries that return precise memory records filtered by metadata (date, tags, agent id).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No experimental results; suggested as high-precision retrieval approach complementary to other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SQL search provides precise, timestamped retrieval useful for chronological queries and structured lookups.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Less useful for semantic or broad-topic retrieval where content-level semantics matter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8228.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full-Text Search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full-Text Search (textual search over stored natural-language memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval method that scans textual memory stores for matches or near-matches to query strings, suitable for general queries when exact tags are unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generic LLM Agents (using full-text search)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that use full-text search to scan natural-language-stored memories for relevant passages when query specifics are fuzzy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Broad topical retrieval and recall</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Find passages or interactions relevant to a general theme (e.g., 'climate change effects') when exact location of data is unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>information retrieval / memory lookup</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external natural-language memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Keyword and approximate text-matching search over stored natural-language entries (e.g., Postgres/Elasticsearch).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Unstructured natural-language records or documents</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Full-text search and approximate matching to retrieve passages relevant to the query wording.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No experimental comparisons; described as complementary to semantic/vector search.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Full-text search is direct and useful when exact wording or matches exist; it complements semantic search for broad queries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Less efficient for broader semantic matching where wording differs from the query.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8228.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic (Vector) Search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Search (vector similarity search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval mechanism using vector embeddings to perform semantic similarity search, enabling retrieval of meaning-related memory entries even when wording differs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generic LLM Agents (using semantic search)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that retrieve memory entries by embedding queries and stored memories and performing vector similarity search (e.g., using vector DBs like Pinecone/FAISS/Picone).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Semantic retrieval and context-matching</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve memories semantically related to a query (e.g., conceptually relevant documents even if phrasing differs).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>semantic retrieval / contextual search</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>embedding-based external memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Vectorizing memories and queries with embeddings and retrieving nearest neighbors based on similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Vector embeddings of memory entries capturing semantic content; optionally paired with natural-language content.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Vector similarity search (semantic nearest-neighbor retrieval); can be combined with filtering (e.g., SQL) for multi-stage retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No quantitative comparisons reported in this paper; recommended as efficient for semantic retrieval but lacking descriptive nuance compared to natural-language storage.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Semantic search recovers contextually related memories beyond literal text matches and pairs well with structured filters for precise, semantically relevant retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Embeddings lose fine descriptive nuance and may need to be stored alongside natural-language records; storage/query infrastructure constraints may limit combined usage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8228.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Turing Machines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Turing Machines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior neural memory architecture (cited) that augments neural networks with an external memory tape and differentiable read/write operations to support algorithmic tasks and explicit memory manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural Turing Machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural Turing Machine</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A neural architecture with an external differentiable memory and learned read/write controllers to enable explicit memory operations (cited as inspiration/related work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Algorithmic and memory-augmented tasks (general prior literature)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used historically to extend neural nets with explicit memory for tasks requiring storage and retrieval of intermediate data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>memory-augmented learning / algorithmic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external differentiable memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Differentiable read/write heads interacting with an external memory matrix (as cited from literature).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory matrix entries storing intermediate content</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Learned attention-like read operations (differentiable addressing)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Paper cites Neural Turing Machines as prior art and notes such approaches face integration and computational-complexity challenges when applied to LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prior memory-augmented neural architectures illustrate ways to add explicit memory but often bring complexity and integration difficulty for LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cited challenges include computational complexity, integration difficulty, limited generalization, and interpretability concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8228.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory Networks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory Networks / End-To-End Memory Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior memory-augmented models that store facts or contexts in an external memory and perform attention-like retrieval to answer queries; cited as related work for improving agent memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>End-To -End Memory Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory Networks</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Models that maintain an external memory of facts and use attention-based retrieval mechanisms to incorporate external context into predictions (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QA and tasks requiring retrieval of stored facts</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve and reason over stored facts or contexts to answer queries or perform reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / retrieval-augmented tasks</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory matrix / memory slots</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Attention-based retrieval over stored memory slots to augment model input.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored facts or contextual memory slots</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Attention / learned addressing over memory slots</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as prior work; paper notes such models aim to provide sophisticated memory but face practical challenges in LLM-agent contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Memory Networks provide useful conceptual building blocks for LLM memory but require careful engineering for scale and integration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Integration difficulty, computational cost, and limited generalization cited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8228.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RecurrentGPT / Recursively Summarizing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RecurrentGPT; Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recent approaches cited that extend dialogue memory across long dialogues via recursive summarization or recurrent generation strategies to overcome context window limits, but they do not change the working memory model's structural separation into domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RecurrentGPT / Recursively Summarizing approaches</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Methods for extending effective dialogue context by recursively summarizing or generating long texts to persist memory across long interactions (cited as solutions to context-window limits).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RecurrentGPT (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-term dialogue memory / long-form generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintaining continuity across arbitrarily long dialogues or generating long texts by recursively compressing/propagating context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-dialogue memory / long-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>summary-based long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Recursive summarization or recurrent generation pipelines to compress and pass forward context across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Abstractive summaries of prior dialogues or generated content</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Concatenation or injection of recursive summaries into new prompts; no centralized episodic buffer change.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Paper states these methods overcome context-window limitations but 'did not change the working memory model' (i.e., they do not provide cross-domain episodic continuity).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recursive summarization and recurrent strategies extend dialogue length but do not address cross-episode domain isolation or provide full episodic retrieval semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Risk of loss of fine-grained details due to summarization; does not provide persistent episodic links across sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8228.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Augmenting LMs with Long-Term Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Augmenting Language Models with Long-Term Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited approach that augments language models with an external long-term memory to store and retrieve information across sessions, referenced as an advance for sustaining longer dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Augmenting Language Models with Long-Term Memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Long-Term Memory augmented LMs (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Approaches that attach external long-term memory modules to LMs to persist knowledge beyond single-session context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-term dialogue and personalization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Sustaining user-specific or session-specific information across interactions for personalization or continuity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-term memory / personalization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>External memory repositories storing persistent records retrievable across sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Persistent interaction logs, user-specific facts, summaries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieval-augmented mechanisms (not fully specified in this paper); possibly embedding+search or keyed retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as an approach that helps sustain long-term dialogues but increases operational complexity and risks loss of working-memory detail depending on update rules.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>External long-term memory supports sustained dialogues and personalization but introduces cost and potential detail loss depending on update/compaction methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Operational costs, memory update mechanisms that may lose fine-grained working-memory details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e8228.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory-Augmented LLM Personalization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-Augmented LLM Personalization with Short-and Long-Term Memory Coordination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that coordinates short- and long-term memories for personalization of LLM agents; referenced as a way to incorporate memory inside models at higher cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memory-Augmented LLM Personalization with Short-and Long-Term Memory Coordination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory-Augmented Personalized LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Personalized agents integrating both short-term and long-term memory coordination mechanisms to tailor behavior to users.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Personalization / user-specific tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintain personalized knowledge across interactions balancing short-term context and long-term user models.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>personalization / long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term + long-term coordinated memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Coordinated memories possibly inside/outside the model for personalization (paper cites this as an approach).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>User-specific facts, short-term session context, long-term preferences</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Coordinated retrieval from short- and long-term stores; specifics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Cited as increasing personalization at the cost of higher operational complexity; no results provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating short- and long-term memory enables more personalized agents but raises costs and potential information-loss tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Higher operational cost and possible loss of working-memory detail depending on update rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8228.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e8228.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited technique aiming to extend the effective context window of LLMs (Positional Skip-wise Training), referenced as one of several technical approaches to mitigate token-limited memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PoSE (context window extension technique)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A technical method for extending the usable context length of LLMs via training-time positional techniques (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Extending context window for long-text generation / long dialogues</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Increase the LLM's effective token window to reduce need for external memory for longer single-episode contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>context window extension / long-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Positional skip-wise training to extend context capability (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as complementary technical pathway to reduce token-window issues; no experiments reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Extending context window addresses short-term token-limit but does not by itself create cross-episode episodic continuity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Doesn't resolve cross-domain episodic isolation and may have training complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Working Memory for Large Language Model Agents', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural Turing Machines <em>(Rating: 2)</em></li>
                <li>End-To -End Memory Networks <em>(Rating: 2)</em></li>
                <li>Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models <em>(Rating: 2)</em></li>
                <li>RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text <em>(Rating: 2)</em></li>
                <li>Augmenting Language Models with Long-Term Memory <em>(Rating: 2)</em></li>
                <li>Memory-Augmented LLM Personalization with Short-and Long-Term Memory Coordination <em>(Rating: 2)</em></li>
                <li>PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training <em>(Rating: 1)</em></li>
                <li>Adapting LLM Agents Through Communication <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8228",
    "paper_id": "paper-266690555",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "Working Memory Hub + Episodic Buffer",
            "name_full": "Centralized Working Memory Hub with Episodic Buffer access",
            "brief_description": "A proposed LLM-agent memory architecture that centralizes storage of all inputs/outputs and interaction histories (Working Memory Hub) and exposes an Episodic Buffer that preserves and supplies complete interaction episodes to the LLM for contextual reasoning across episodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLM Agent (proposed architecture)",
            "agent_description": "An LLM-centered agent architecture where the Central Processor (LLM) interacts with an External Environment Interface, an Interaction History Window, a persistent Working Memory Hub, and an Episodic Buffer to enable cross-episode continuity and multi-agent shared memory.",
            "model_name": null,
            "model_description": null,
            "task_name": "Complex sequential reasoning and multi-agent collaborative tasks",
            "task_description": "Tasks that require maintaining continuity across multiple interaction episodes (multi-step planning, collaborative decision-making, sequential problem solving) and leveraging past episodes to inform current decisions.",
            "task_type": "multi-step reasoning / multi-agent coordination",
            "memory_used": true,
            "memory_type": "working + episodic (persistent external hub)",
            "memory_mechanism": "Centralized persistent store (Working Memory Hub) that records all inputs/outputs and histories; Interaction History Window provides short-term cache (rolling window or abstractive summaries); Episodic Buffer retrieves complete episodes from the Hub on demand.",
            "memory_representation": "Raw inputs/outputs, interaction histories, complete episodic traces, abstractive summaries or extracts for recent context",
            "memory_retrieval_method": "Episodic retrieval via Episodic Buffer; short-term retrieval via Interaction History Window (rolling window, abstractive summary, or extracts); can be combined with SQL/full-text/semantic searches against the Hub.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No empirical ablation or quantitative comparison reported; architecture proposed to address continuity and multi-agent memory sharing limitations of standard LLMs.",
            "key_findings": "Centralizing memories enables persistent cross-episode continuity and provides both short-term and long-term access patterns (rolling window and episodic access), potentially improving sequential reasoning and multi-agent cooperation; detailed mechanisms for encoding/prioritization/retrieval require further research.",
            "limitations_or_challenges": "No experimental validation provided; open problems include how to determine memory relevance and retrieval priorities, efficient compression of episodic memories, integration costs, and security/privacy of shared persistent memory.",
            "uuid": "e8228.0",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Interaction History Window",
            "name_full": "Interaction History Window (short-term cache)",
            "brief_description": "A component that maintains a short-term cache of recent interactions (e.g., a rolling window, abstractive summary, or extracts) to provide immediate contextual anchoring within an episode.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLM Agent (proposed architecture)",
            "agent_description": "LLM uses an Interaction History Window to access recent context within token-window constraints.",
            "model_name": null,
            "model_description": null,
            "task_name": "Short-term dialogue continuity / in-episode reasoning",
            "task_description": "Providing the most recent conversational or action context to the LLM during ongoing tasks to inform immediate decisions.",
            "task_type": "short-term context maintenance / in-episode reasoning",
            "memory_used": true,
            "memory_type": "working (short-term cache)",
            "memory_mechanism": "Rolling window of latest dialogues or abstractive summaries stored/accessed from the Working Memory Hub.",
            "memory_representation": "Recent dialogue turns, condensed summaries, or pertinent extracts",
            "memory_retrieval_method": "Recency-based retrieval (rolling window) or retrieval of abstractive summaries from the Working Memory Hub.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No empirical tests reported; described qualitatively as necessary to mitigate token-window limits.",
            "key_findings": "Interaction History Window provides flexible short-term context representations to mitigate token-limit constraints and supports more fluid in-episode reasoning.",
            "limitations_or_challenges": "Selecting window size and summary strategies, and integration with longer-term episodic memory remain open issues.",
            "uuid": "e8228.1",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Memory Management Agent",
            "name_full": "Memory Management Agent (specialized retrieval manager)",
            "brief_description": "A dedicated agent in multi-agent systems that manages, sorts, prioritizes, and retrieves historical data (chat/action histories) to support planning and reduce retrieval overhead for other agents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Memory Management Agent",
            "agent_description": "A specialized MAS component responsible for curating and returning the most relevant memories (recent vs historical, task-relevant) to planning or execution agents.",
            "model_name": null,
            "model_description": null,
            "task_name": "Planning and strategic decision-making in MAS",
            "task_description": "Provide curated memory slices tailored to planning horizon (short-term experiments vs long-term studies) to speed up and focus other agents' planning processes.",
            "task_type": "planning / multi-agent coordination",
            "memory_used": true,
            "memory_type": "episodic/working (curated access)",
            "memory_mechanism": "Centralized retrieval and curation service that filters and prioritizes memory segments from the Working Memory Hub based on planning needs.",
            "memory_representation": "Chat histories, action histories, relevant past interactions and outcomes",
            "memory_retrieval_method": "Context-aware curated retrieval (prioritization rules vary by task horizon), e.g., recent interactions for short-term tasks and mixed historical trends for long-term tasks.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No quantitative ablation; described qualitatively as improving efficiency compared to agents independently searching full histories.",
            "key_findings": "Centralizing retrieval into a Memory Management Agent reduces computational overhead for planning agents and can provide better, context-appropriate memory slices for improved planning quality.",
            "limitations_or_challenges": "Requires design of prioritization policies and may introduce a single point of failure or coordination bottleneck; security and access control need consideration.",
            "uuid": "e8228.2",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Role-Based Memory Access",
            "name_full": "Role-Based Memory Access Control",
            "brief_description": "A memory access strategy in multi-agent systems where agents are granted memory retrieval rights according to their designated roles (e.g., supervisor has broad access, specialists have narrow access).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Generic LLM Agents with RBAC",
            "agent_description": "Agents whose memory access is constrained or enabled based on assigned roles to balance efficiency and oversight.",
            "model_name": null,
            "model_description": null,
            "task_name": "Supervisory monitoring and specialized task execution in MAS",
            "task_description": "Supervisor agents require broad historical visibility to coordinate; specialist agents need only their own recent memory to focus on specific tasks.",
            "task_type": "multi-agent coordination / role-specific tasks",
            "memory_used": true,
            "memory_type": "episodic/working (access-restricted)",
            "memory_mechanism": "Role-based access control to the Working Memory Hub and Episodic Buffer.",
            "memory_representation": "Agent-specific interactions and shared histories; access rights determine which memories are retrievable.",
            "memory_retrieval_method": "Access mediated by role-rules; retrieval queries limited to allowed memory partitions.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No quantitative results; recommended as a mechanism to balance adaptability and security/efficiency.",
            "key_findings": "Role-based rules can reduce unnecessary retrieval and tailor information exposure, but overly rigid roles may limit adaptability.",
            "limitations_or_challenges": "Requires careful design to avoid stifling agent flexibility and to prevent excessive information exposure.",
            "uuid": "e8228.3",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Task-Based Memory Access",
            "name_full": "Task-Based Memory Access",
            "brief_description": "A strategy where memory access is determined by the specific task requirements; agents receive memories directly relevant to the current task rather than broad role-derived access.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Generic LLM Agents (task-driven retrieval)",
            "agent_description": "Agents that receive memory segments selected based on the semantics and needs of the current assigned task.",
            "model_name": null,
            "model_description": null,
            "task_name": "Task-specific retrieval scenarios",
            "task_description": "When an agent is assigned a specific task, the system supplies memories that are directly relevant to that task's objectives.",
            "task_type": "task-focused retrieval / efficiency optimization",
            "memory_used": true,
            "memory_type": "episodic/working (task-scoped)",
            "memory_mechanism": "Task-specification-driven selection of memory slices from the Working Memory Hub or Episodic Buffer.",
            "memory_representation": "Task-relevant past interactions, outcomes, or data",
            "memory_retrieval_method": "Task-scoped filtering and retrieval (semantic or metadata filters tied to task descriptors).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No empirical comparisons reported; conceptually tighter focus than role-based access.",
            "key_findings": "Task-based access can increase relevance and reduce retrieval overhead by exposing only directly pertinent memories.",
            "limitations_or_challenges": "Requires accurate task-memory relevancy mapping; risk of excluding useful but not explicitly labeled memories.",
            "uuid": "e8228.4",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Autonomous Memory Access",
            "name_full": "Autonomous Memory Access (agent-driven retrieval)",
            "brief_description": "An access paradigm where agents autonomously decide which memory segments to fetch based on contextual cues, rather than relying on hard-coded role/task boundaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Autonomous LLM Agents",
            "agent_description": "Agents endowed with autonomy to query the Working Memory Hub/Episodic Buffer to fetch memory segments they assess as relevant to their current context.",
            "model_name": null,
            "model_description": null,
            "task_name": "Open retrieval scenarios (broad research, synthesis, review tasks)",
            "task_description": "Agents performing tasks like broad literature review or synthesis may autonomously retrieve diverse memory segments relevant to context.",
            "task_type": "open-ended retrieval / flexible reasoning",
            "memory_used": true,
            "memory_type": "episodic/working (autonomously accessed)",
            "memory_mechanism": "Self-determined retrieval using contextual heuristics or learned policies to query the Working Memory Hub.",
            "memory_representation": "Wide-ranging past interactions and knowledge traces selected by the agent",
            "memory_retrieval_method": "Agent-driven selection potentially using semantic/keyword queries and contextual scoring.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No experiments reported; contrasted qualitatively with more restrictive task-specific access.",
            "key_findings": "Autonomy enables flexible retrieval and can surface relevant cross-domain memories but may increase risk of irrelevant retrieval or security exposure.",
            "limitations_or_challenges": "Needs safeguards to limit irrelevant or sensitive memory access; requires mechanisms to evaluate retrieval relevance.",
            "uuid": "e8228.5",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Collaboration-Scenario Memory Access",
            "name_full": "Collaboration Scenario-Based Memory Access",
            "brief_description": "A family of memory access strategies tailored to collaboration patterns: parallel collaborations may require shared full-memory access, while sequential collaborations pass memory context from one agent to the next.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Collaborative LLM Agents",
            "agent_description": "Agents in collaborative workflows use different memory sharing strategies depending on whether collaboration is parallel or sequential.",
            "model_name": null,
            "model_description": null,
            "task_name": "Collaborative multi-agent workflows",
            "task_description": "Examples include sequential analysis pipelines where one agent's output becomes the next agent's input, or parallel analyses requiring shared memory access.",
            "task_type": "multi-agent collaboration",
            "memory_used": true,
            "memory_type": "shared episodic/working memory",
            "memory_mechanism": "Shared access policies: full shared memory for parallel collaborations; handoff or selective access for sequential collaborations.",
            "memory_representation": "Shared findings, intermediate outputs, and preceding agent outputs",
            "memory_retrieval_method": "Policy-driven retrieval based on the collaboration topology (shared queries for parallel; sequential retrieval/handoff for pipelines).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No quantitative evaluations; described conceptually to illustrate different sharing needs.",
            "key_findings": "Adapting memory access to collaboration type improves relevance and efficiency of inter-agent coordination.",
            "limitations_or_challenges": "Requires coordination protocols, access control, and mechanisms to maintain consistency and privacy across agents.",
            "uuid": "e8228.6",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "SQL Search",
            "name_full": "SQL Search (structured queries against memory store)",
            "brief_description": "A retrieval mechanism for agent memories where agents query structured memory repositories using SQL to obtain precise, timestamped records or tag-filtered slices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Generic LLM Agents (using SQL search)",
            "agent_description": "Agents using SQL queries to precisely retrieve memory entries by timestamp, tag, or structured fields from the Working Memory Hub.",
            "model_name": null,
            "model_description": null,
            "task_name": "Chronological or precise memory retrieval",
            "task_description": "Tasks that require exact snapshots (e.g., 'what did we discuss last Tuesday?') or structured retrieval by metadata.",
            "task_type": "memory retrieval / factual recall",
            "memory_used": true,
            "memory_type": "external structured memory",
            "memory_mechanism": "Executing SQL queries against structured database-backed memory stores to retrieve targeted records.",
            "memory_representation": "Structured records with tags, timestamps, and fields representing interactions",
            "memory_retrieval_method": "SQL queries that return precise memory records filtered by metadata (date, tags, agent id).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No experimental results; suggested as high-precision retrieval approach complementary to other methods.",
            "key_findings": "SQL search provides precise, timestamped retrieval useful for chronological queries and structured lookups.",
            "limitations_or_challenges": "Less useful for semantic or broad-topic retrieval where content-level semantics matter.",
            "uuid": "e8228.7",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Full-Text Search",
            "name_full": "Full-Text Search (textual search over stored natural-language memory)",
            "brief_description": "A retrieval method that scans textual memory stores for matches or near-matches to query strings, suitable for general queries when exact tags are unknown.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Generic LLM Agents (using full-text search)",
            "agent_description": "Agents that use full-text search to scan natural-language-stored memories for relevant passages when query specifics are fuzzy.",
            "model_name": null,
            "model_description": null,
            "task_name": "Broad topical retrieval and recall",
            "task_description": "Find passages or interactions relevant to a general theme (e.g., 'climate change effects') when exact location of data is unknown.",
            "task_type": "information retrieval / memory lookup",
            "memory_used": true,
            "memory_type": "external natural-language memory",
            "memory_mechanism": "Keyword and approximate text-matching search over stored natural-language entries (e.g., Postgres/Elasticsearch).",
            "memory_representation": "Unstructured natural-language records or documents",
            "memory_retrieval_method": "Full-text search and approximate matching to retrieve passages relevant to the query wording.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No experimental comparisons; described as complementary to semantic/vector search.",
            "key_findings": "Full-text search is direct and useful when exact wording or matches exist; it complements semantic search for broad queries.",
            "limitations_or_challenges": "Less efficient for broader semantic matching where wording differs from the query.",
            "uuid": "e8228.8",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Semantic (Vector) Search",
            "name_full": "Semantic Search (vector similarity search)",
            "brief_description": "A retrieval mechanism using vector embeddings to perform semantic similarity search, enabling retrieval of meaning-related memory entries even when wording differs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Generic LLM Agents (using semantic search)",
            "agent_description": "Agents that retrieve memory entries by embedding queries and stored memories and performing vector similarity search (e.g., using vector DBs like Pinecone/FAISS/Picone).",
            "model_name": null,
            "model_description": null,
            "task_name": "Semantic retrieval and context-matching",
            "task_description": "Retrieve memories semantically related to a query (e.g., conceptually relevant documents even if phrasing differs).",
            "task_type": "semantic retrieval / contextual search",
            "memory_used": true,
            "memory_type": "embedding-based external memory",
            "memory_mechanism": "Vectorizing memories and queries with embeddings and retrieving nearest neighbors based on similarity metrics.",
            "memory_representation": "Vector embeddings of memory entries capturing semantic content; optionally paired with natural-language content.",
            "memory_retrieval_method": "Vector similarity search (semantic nearest-neighbor retrieval); can be combined with filtering (e.g., SQL) for multi-stage retrieval.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No quantitative comparisons reported in this paper; recommended as efficient for semantic retrieval but lacking descriptive nuance compared to natural-language storage.",
            "key_findings": "Semantic search recovers contextually related memories beyond literal text matches and pairs well with structured filters for precise, semantically relevant retrieval.",
            "limitations_or_challenges": "Embeddings lose fine descriptive nuance and may need to be stored alongside natural-language records; storage/query infrastructure constraints may limit combined usage.",
            "uuid": "e8228.9",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Neural Turing Machines",
            "name_full": "Neural Turing Machines",
            "brief_description": "A prior neural memory architecture (cited) that augments neural networks with an external memory tape and differentiable read/write operations to support algorithmic tasks and explicit memory manipulation.",
            "citation_title": "Neural Turing Machines",
            "mention_or_use": "mention",
            "agent_name": "Neural Turing Machine",
            "agent_description": "A neural architecture with an external differentiable memory and learned read/write controllers to enable explicit memory operations (cited as inspiration/related work).",
            "model_name": null,
            "model_description": null,
            "task_name": "Algorithmic and memory-augmented tasks (general prior literature)",
            "task_description": "Used historically to extend neural nets with explicit memory for tasks requiring storage and retrieval of intermediate data.",
            "task_type": "memory-augmented learning / algorithmic tasks",
            "memory_used": true,
            "memory_type": "external differentiable memory",
            "memory_mechanism": "Differentiable read/write heads interacting with an external memory matrix (as cited from literature).",
            "memory_representation": "Memory matrix entries storing intermediate content",
            "memory_retrieval_method": "Learned attention-like read operations (differentiable addressing)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Paper cites Neural Turing Machines as prior art and notes such approaches face integration and computational-complexity challenges when applied to LLM agents.",
            "key_findings": "Prior memory-augmented neural architectures illustrate ways to add explicit memory but often bring complexity and integration difficulty for LLM agents.",
            "limitations_or_challenges": "Cited challenges include computational complexity, integration difficulty, limited generalization, and interpretability concerns.",
            "uuid": "e8228.10",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Memory Networks",
            "name_full": "Memory Networks / End-To-End Memory Networks",
            "brief_description": "Prior memory-augmented models that store facts or contexts in an external memory and perform attention-like retrieval to answer queries; cited as related work for improving agent memory.",
            "citation_title": "End-To -End Memory Networks",
            "mention_or_use": "mention",
            "agent_name": "Memory Networks",
            "agent_description": "Models that maintain an external memory of facts and use attention-based retrieval mechanisms to incorporate external context into predictions (cited).",
            "model_name": null,
            "model_description": null,
            "task_name": "QA and tasks requiring retrieval of stored facts",
            "task_description": "Retrieve and reason over stored facts or contexts to answer queries or perform reasoning tasks.",
            "task_type": "question answering / retrieval-augmented tasks",
            "memory_used": true,
            "memory_type": "external memory matrix / memory slots",
            "memory_mechanism": "Attention-based retrieval over stored memory slots to augment model input.",
            "memory_representation": "Stored facts or contextual memory slots",
            "memory_retrieval_method": "Attention / learned addressing over memory slots",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as prior work; paper notes such models aim to provide sophisticated memory but face practical challenges in LLM-agent contexts.",
            "key_findings": "Memory Networks provide useful conceptual building blocks for LLM memory but require careful engineering for scale and integration.",
            "limitations_or_challenges": "Integration difficulty, computational cost, and limited generalization cited.",
            "uuid": "e8228.11",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "RecurrentGPT / Recursively Summarizing",
            "name_full": "RecurrentGPT; Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models",
            "brief_description": "Recent approaches cited that extend dialogue memory across long dialogues via recursive summarization or recurrent generation strategies to overcome context window limits, but they do not change the working memory model's structural separation into domains.",
            "citation_title": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models",
            "mention_or_use": "mention",
            "agent_name": "RecurrentGPT / Recursively Summarizing approaches",
            "agent_description": "Methods for extending effective dialogue context by recursively summarizing or generating long texts to persist memory across long interactions (cited as solutions to context-window limits).",
            "model_name": "RecurrentGPT (as cited)",
            "model_description": null,
            "task_name": "Long-term dialogue memory / long-form generation",
            "task_description": "Maintaining continuity across arbitrarily long dialogues or generating long texts by recursively compressing/propagating context.",
            "task_type": "long-dialogue memory / long-text generation",
            "memory_used": true,
            "memory_type": "summary-based long-term memory",
            "memory_mechanism": "Recursive summarization or recurrent generation pipelines to compress and pass forward context across episodes.",
            "memory_representation": "Abstractive summaries of prior dialogues or generated content",
            "memory_retrieval_method": "Concatenation or injection of recursive summaries into new prompts; no centralized episodic buffer change.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Paper states these methods overcome context-window limitations but 'did not change the working memory model' (i.e., they do not provide cross-domain episodic continuity).",
            "key_findings": "Recursive summarization and recurrent strategies extend dialogue length but do not address cross-episode domain isolation or provide full episodic retrieval semantics.",
            "limitations_or_challenges": "Risk of loss of fine-grained details due to summarization; does not provide persistent episodic links across sessions.",
            "uuid": "e8228.12",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Augmenting LMs with Long-Term Memory",
            "name_full": "Augmenting Language Models with Long-Term Memory",
            "brief_description": "A cited approach that augments language models with an external long-term memory to store and retrieve information across sessions, referenced as an advance for sustaining longer dialogues.",
            "citation_title": "Augmenting Language Models with Long-Term Memory",
            "mention_or_use": "mention",
            "agent_name": "Long-Term Memory augmented LMs (cited work)",
            "agent_description": "Approaches that attach external long-term memory modules to LMs to persist knowledge beyond single-session context windows.",
            "model_name": null,
            "model_description": null,
            "task_name": "Long-term dialogue and personalization",
            "task_description": "Sustaining user-specific or session-specific information across interactions for personalization or continuity.",
            "task_type": "long-term memory / personalization",
            "memory_used": true,
            "memory_type": "external long-term memory",
            "memory_mechanism": "External memory repositories storing persistent records retrievable across sessions.",
            "memory_representation": "Persistent interaction logs, user-specific facts, summaries",
            "memory_retrieval_method": "Retrieval-augmented mechanisms (not fully specified in this paper); possibly embedding+search or keyed retrieval.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Mentioned as an approach that helps sustain long-term dialogues but increases operational complexity and risks loss of working-memory detail depending on update rules.",
            "key_findings": "External long-term memory supports sustained dialogues and personalization but introduces cost and potential detail loss depending on update/compaction methods.",
            "limitations_or_challenges": "Operational costs, memory update mechanisms that may lose fine-grained working-memory details.",
            "uuid": "e8228.13",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Memory-Augmented LLM Personalization",
            "name_full": "Memory-Augmented LLM Personalization with Short-and Long-Term Memory Coordination",
            "brief_description": "A cited work that coordinates short- and long-term memories for personalization of LLM agents; referenced as a way to incorporate memory inside models at higher cost.",
            "citation_title": "Memory-Augmented LLM Personalization with Short-and Long-Term Memory Coordination",
            "mention_or_use": "mention",
            "agent_name": "Memory-Augmented Personalized LLMs",
            "agent_description": "Personalized agents integrating both short-term and long-term memory coordination mechanisms to tailor behavior to users.",
            "model_name": null,
            "model_description": null,
            "task_name": "Personalization / user-specific tasks",
            "task_description": "Maintain personalized knowledge across interactions balancing short-term context and long-term user models.",
            "task_type": "personalization / long-term memory",
            "memory_used": true,
            "memory_type": "short-term + long-term coordinated memory",
            "memory_mechanism": "Coordinated memories possibly inside/outside the model for personalization (paper cites this as an approach).",
            "memory_representation": "User-specific facts, short-term session context, long-term preferences",
            "memory_retrieval_method": "Coordinated retrieval from short- and long-term stores; specifics not detailed in this paper.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Cited as increasing personalization at the cost of higher operational complexity; no results provided here.",
            "key_findings": "Integrating short- and long-term memory enables more personalized agents but raises costs and potential information-loss tradeoffs.",
            "limitations_or_challenges": "Higher operational cost and possible loss of working-memory detail depending on update rules.",
            "uuid": "e8228.14",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "PoSE",
            "name_full": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
            "brief_description": "A cited technique aiming to extend the effective context window of LLMs (Positional Skip-wise Training), referenced as one of several technical approaches to mitigate token-limited memory.",
            "citation_title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
            "mention_or_use": "mention",
            "agent_name": "PoSE (context window extension technique)",
            "agent_description": "A technical method for extending the usable context length of LLMs via training-time positional techniques (cited).",
            "model_name": null,
            "model_description": null,
            "task_name": "Extending context window for long-text generation / long dialogues",
            "task_description": "Increase the LLM's effective token window to reduce need for external memory for longer single-episode contexts.",
            "task_type": "context window extension / long-text generation",
            "memory_used": false,
            "memory_type": null,
            "memory_mechanism": "Positional skip-wise training to extend context capability (as cited).",
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as complementary technical pathway to reduce token-window issues; no experiments reported here.",
            "key_findings": "Extending context window addresses short-term token-limit but does not by itself create cross-episode episodic continuity.",
            "limitations_or_challenges": "Doesn't resolve cross-domain episodic isolation and may have training complexity.",
            "uuid": "e8228.15",
            "source_info": {
                "paper_title": "Empowering Working Memory for Large Language Model Agents",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural Turing Machines",
            "rating": 2,
            "sanitized_title": "neural_turing_machines"
        },
        {
            "paper_title": "End-To -End Memory Networks",
            "rating": 2,
            "sanitized_title": "endto_end_memory_networks"
        },
        {
            "paper_title": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models",
            "rating": 2,
            "sanitized_title": "recursively_summarizing_enables_longterm_dialogue_memory_in_large_language_models"
        },
        {
            "paper_title": "RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text",
            "rating": 2,
            "sanitized_title": "recurrentgpt_interactive_generation_of_arbitrarily_long_text"
        },
        {
            "paper_title": "Augmenting Language Models with Long-Term Memory",
            "rating": 2,
            "sanitized_title": "augmenting_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Memory-Augmented LLM Personalization with Short-and Long-Term Memory Coordination",
            "rating": 2,
            "sanitized_title": "memoryaugmented_llm_personalization_with_shortand_longterm_memory_coordination"
        },
        {
            "paper_title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
            "rating": 1,
            "sanitized_title": "pose_efficient_context_window_extension_of_llms_via_positional_skipwise_training"
        },
        {
            "paper_title": "Adapting LLM Agents Through Communication",
            "rating": 1,
            "sanitized_title": "adapting_llm_agents_through_communication"
        }
    ],
    "cost": 0.0188185,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Empowering Working Memory for Large Language Model Agents</p>
<p>Jing Guo 
Tsinghua University</p>
<p>Nan Li li-nan@tsinghua.edu.cn 
Tsinghua University</p>
<p>Jianchuan Qi 
Tsinghua University</p>
<p>Hang Yang 
Tsinghua University</p>
<p>Ruiqiao Li 
Tsinghua University</p>
<p>Yuzhen Feng 
Tsinghua University</p>
<p>Si Zhang 
Tsinghua University</p>
<p>Ming Xu xu-ming@tsinghua.edu.cn 
Tsinghua University</p>
<p>Empowering Working Memory for Large Language Model Agents
919DB134EB8D9B0037DAC1CF89C6C546
Large language models (LLMs) have achieved impressive linguistic capabilities.However, a key limitation persists in their lack of human-like memory faculties.LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning.This paper explores the potential of applying cognitive psychology's working memory frameworks, to enhance LLM architecture.The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links.To address this, an innovative model is proposed incorporating a centralized Working Memory Hub and Episodic Buffer access to retain memories across episodes.This architecture aims to provide greater continuity for nuanced contextual reasoning during intricate tasks and collaborative scenarios.While promising, further research is required into optimizing episodic memory encoding, storage, prioritization, retrieval, and security.Overall, this paper provides a strategic blueprint for developing LLM agents with more sophisticated, humanlike memory capabilities, highlighting memory mechanisms as a vital frontier in artificial general intelligence.</p>
<p>Introduction</p>
<p>The development of large language models (LLMs) has marked a significant advancement in the field of artificial intelligence (AI), particularly in the realms of language understanding, generation, and reasoning.These models, exemplified by OpenAI's ChatGPT, are characterized by their extensive architectures and massive parameter sets, having been trained on vast corpora of text (Brown et al., 2020;Rillig et al., 2023).Despite their impressive linguistic capabilities, a critical challenge that persists is the effective management of memory to achieve more human-like intelligence.Cognitive psychology offers foundational frameworks, such as Baddeley's multicomponent working memory model, which have been instrumental in understanding human memory (Baddeley, 2003).However, the application of these frameworks to AI architectures is not straightforward, and there are inherent limitations to how these human-centric concepts can be translated into artificial systems (Cornago et al., 2023).</p>
<p>Standard LLM agent designs lack robust episodic memory and continuity across distinct interactions.LLM agents typically have a constrained memory capacity, limited by the number of tokens they can process in a single exchange.This limitation restricts their ability to retain and utilize extensive context from previous interactions.Moreover, each interaction is treated as an isolated episode, with no linkage between sequential dialogues.This isolated, short-term memory hinders complex sequential reasoning and knowledge sharing in multi-agent systems.The absence of a robust episodic memory and continuity across interactions hampers the agents' ability to perform complex sequential reasoning tasks, which are essential for more advanced problem-solving capabilities (Figure 1).Especially in Multi-Agent Systems (MAS), the lack of cooperative communication among agents can lead to suboptimal outcomes.Ideally, agents should share immediate actions, or learning experiences to achieve common goals efficiently.To address these challenges, advancements in AI memory architectures, such as Neural Turing Machines and Memory Networks, have been proposed to enhance the memory capabilities of LLM agents (Graves et al., 2014;Sukhbaatar et al., 2015;Weston et al., 2015).These models aim to provide a more sophisticated framework for memory management, potentially enabling LLM agents to better mimic human-like intelligence and memory functions.However, these models often face challenges related to computational complexity, integration difficulties, limited generalization across tasks, dependency on extensive training data, and a lack of human-like flexibility and interpretability in their memory functions.</p>
<p>To address the above limitations, this paper delves into the development of working memory models, exploring their journey from the realm of cognitive psychology to their application in advanced LLM agents.Recognizing the challenges in adapting human working memory frameworks to artificial contexts, it highlights the shortcomings of traditional LLMs, particularly their lack of episodic memory and continuity in diverse interaction domains.To overcome these limitations, we propose a novel model that features a centralized Working Memory Hub and provides access to an Episodic Buffer.This design aims to endow agents with enhanced contextual retention and improved performance in intricate, sequential tasks and cooperative scenarios.The paper sets forth a forwardlooking framework for crafting LLM agents with sophisticated, human-like memory functions and underscores the need for further research into optimizing the processes of memory encoding, consolidation, and retrieval to fulfill these objectives.</p>
<p>Working Memory in Humans and LLMs</p>
<p>Working Memory Model in Cognitive Psychology</p>
<p>Emerging from the mid-20th century, cognitive psychology ushered in a transformative lens to the study of memory.Central to this paradigm shift was the "multi-component memory model" conceptualized by Atkinson and Shiffrin in 1968, segmenting memory into sensory memory, shortterm memory, and long-term memory.Amidst these classifications, a burgeoning interest developed around the concept of working memory.Baddeley's introduction of the "working memory model" in 1974 delineated it not just as an alternate to short-term memory but as a nuanced, multicomponent system dedicated to the transient storage and manipulation of information (Figure 2).</p>
<p>At the nucleus of this model is the Central Executive, acting as the supervisor.It orchestrates attention allocation, prioritizes information, and ensures effective rehearsal among its subsystems.This pivotal component communicates with two key subsystems.The Visuospatial Sketchpad, or the "inner eye", specializes in spatial and visual information.Whether it's visualizing landscapes or mapping routes, it's crucial.Moreover, it closely intertwines with our perceptual systems.For example, during reading, visual stimuli are rapidly encoded into this sketchpad, preparing the information for comprehension.The Phonological Loop, often termed the "inner voice", holds linguistic and phonological content in a speech-based format.Its storage, however, is fleeting unless the information is actively rehearsed.As the understanding of working memory evolved, the model was further refined.In 2000, Baddeley introduced the Episodic Buffer as an additional component, acting as a temporary storage that amalgamates information from varied sources, thus creating coherence among the Central Executive, Phonological Loop, and Visuospatial Sketchpad.</p>
<p>But the depth of this model goes beyond its components.A profound link binds working memory to long-term memory, with information shuttling between the two based on need and processing.From its origins, the working memory model has illuminated our grasp of cognition.Its principles have reverberated across fields, influencing educational methodologies aimed at boosting academic capabilities and inspiring the blueprint of intricate AI and computational architectures.</p>
<p>Working Memory Architecture in LLM Agents</p>
<p>Drawing inspiration from the human "working memory model", the LLM Agent's working memory offers a unique yet analogous architecture (Figure 3).</p>
<p>At the crux lies the Central Processor, which is, in essence, the LLM itself.It seamlessly intertwines the massive training data with real-time inputs, orchestrating the data flow and ensuring the appropriate information processing, analyzing, and making decision.External Environment Sensor stands as the gateway for the agent, facilitating real-time interactions with various external systems and databases.This ensures the agent's ability to get inputs and respond to dynamic external data seamlessly.The Interaction History Window is crucial for retaining short-term interactions.It preserves the recent lineage of interactions, offering contextual anchorage for the agent during ongoing tasks.However, due to the token limitations inherent in LLMs, the model can only retain a limited scope of the conversation.This constraint poses the first challenge, especially in extensive interactions, as the LLMs might lose older contextual information beyond its token capacity.Furthermore, the second challenge lies on the concept of 'task domains' in the model, demarcated by the dotted lines.Each unique engagement or interaction session with an LLM is treated as its distinct domain.When a subsequent interaction is initiated, it essentially establishes a new domain, devoid of direct linkage to the previous ones.This organizational paradigm emerges from the way LLMs handle data streams, where every distinct interaction is processed as a separate episodic memory, isolated from preceding engagements.Thus, there is no episodic buffer as human " working memory model" includes.The constraints of the current LLMs memory architecture become even more pronounced in multiagent settings.As agents engage in collaborative tasks, the inability to directly share episodic memories across interaction domains severely limits collective progress.Agents lack mechanisms to consistently build upon or reference the experiences and knowledge gained by their peers over time.This isolated nature of individual agent memory prevents the emergence of a collective longterm memory spanning the shared interactions and goals of the agent group.Furthermore, the limited context retention within a single agent poses additional challenges when coordinating complex goals across multiple agents, where key information must be propagated across the system.Developing more interconnected and persistent memory capabilities is critical for enabling smoother collaboration and emergent cognition in multi-agent environments.</p>
<p>An Advanced Working Memory Model for LLM Agents</p>
<p>To address the above two challenges, current research endeavors are fervently exploring methodologies to aim for an even more robust LLM agent memory system.Addressing the first challenge, technologies have been developed to sustain long-term dialogues, effectively overcoming the context window limitations.Such advancements include recursively summarizing, RecurrentGPT, Long-term Memory(Q.Wang et al., 2023;W. Wang et al., 2023;Zhou et al., 2023).However, they did not change the working memory model.For the second challenge, transitioning from working memory to long-term memory, researchers aim to dissolve the "domain" separations between tasks.One way explored is the direct incorporation of memory within the model itself, leading to a personalized agent (K.Wang et al., 2023;Zhang et al., 2023;Zhu et al., 2023).While this approach fosters a more tailored agent, it comes with higher operational costs.Moreover, due to specific memory update mechanisms, there's a risk of losing intricate details from the working memory.To enhance the memory capabilities of LLM agents, an innovative memory architecture has been proposed, as depicted in the diagram (Figure 4).This evolved model incorporates additional components to address limitations of traditional working memory models.</p>
<p>Working Memory Hub unified hub orchestrates data flows between other components.It stores all inputs, outputs, and interaction histories to supply other elements like the Interaction History Window and Episodic Buffer.Similar to the above-mentioned working memory model, at the core is the Central Processor comprised of LLMs, acting as the brain of the agent.It ensures that information is processed, analyzed, and decisions are made based on a harmonious blend of historical and current inputs from the External Environment Interface.Also, External Environment Interface facilitates the continuous influx of dynamic external data into the system.It intakes realtime inputs from users and external sources and routes them to the Central Processor for analysis.Likewise, it captures the outputs from the Central Processor and disseminates them as responses.All inputs and outputs are stored in the Working Memory Hub.Drawing from the Working Memory Hub, Interaction History Window maintains a short-term cache of recent interaction history to provide contextual anchoring for the agent.The history could take various forms as needed, such as a rolling window of the latest dialogues, an abstractive summary, or pertinent extracts, which is potential to use chat history more flexibly.The Episodic Buffer retrieves complete episodes from Working Memory Hub, allowing the agent to access memories of specific events or dialogues when relevant to the current context.</p>
<p>The Working Memory Hub plays a pivotal role as the centralized data exchange hub for the entire architecture.It acts as the conduit for routing all inputs, outputs, and histories between components.The Working Memory Hub enables seamless data flows and providing a unified data access layer to the rest of the system.Without the orchestrating function of the Working Memory Hub, the components would be isolated islands of memory.Furthermore, the Working Memory Hub persistently stores all interaction data over time.This persistent memory ensures no data is ever lost and provides the raw material for higher-level memory functions.The durability of the Working Memory Hub lends continuity and consistency to the agent's knowledge.</p>
<p>The Episodic Buffer provides a crucial long-term episodic memory capacity lacking in traditional working memory models.The Episodic Buffer preserves entire interaction episodes as distinct memory traces.This allows the agent to recall specific events and conversations when needed to inform the handling of new situations.Having access to complete episodic memories, rather than just short-term caches, enables more human-like recall, learning, and decision making.The Episodic Buffer equips the agent with the capacity for experiential wisdom.With these enhancements, agents can leverage both long-term and short-term memory across episodes for enhanced contextual decision-making.The model also facilitates collective memory development in multi-agent systems, enabling cooperative agents to build shared knowledge over time.</p>
<p>Technical Pathways for a Memory Hub</p>
<p>To make a memory hub for LLMs agent, the third-party databases can be used for external memory repositories.With access to these, LLMs can fetch factual data or embeddings, enabling the generation of more precise and contextually accurate responses.</p>
<p>The storage format used in external memory modules directly impacts retrieval strategies.Natural language storage provides rich semantic information, making it well-suited for keyword-based searches that require in-depth textual exploration.However, this format lacks efficiency for broader semantic searches focused on overall meaning rather than specific keywords.On the other hand, embeddings streamline retrieval through vector representations encapsulating semantic context.While efficient, embeddings lack the nuanced descriptive nature of natural language.Ideally, both natural language and embeddings would be used concurrently to capitalize on their complementary strengths.However, underlying database and platform constraints often limit feasible storage options.For example, Postgres and Elasticsearch efficiently manage raw text data conducive for natural language storage.In contrast, Picone excels at vector similarity searches ideal for embeddings.</p>
<p>Platforms such as Xata, a PaaS (Platform as a Service), present a robust solution.PaaS is a cloud computing service that provides a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure typically associated with application development.Essentially, PaaS abstracts and manages much of the backend intricacy, letting developers focus on the application logic itself.Marrying PaaS with MAS is not only strategic but technically feasible.The modular nature of MAS means that agents can be designed to interact with external systems, including databases hosted on PaaS platforms.Given the API-driven architecture of most modern PaaS solutions:</p>
<p>o Agents can be configured to make API calls to push their memory data to the database.</p>
<p>o Retrieval of data can be done through API endpoints, with agents sending requests and receiving memory data in real-time.</p>
<p>o With proper authentication and authorization mechanisms in place, only the designated agents or systems can access specific segments of the memory, ensuring data integrity and security.</p>
<p>From Memory Hub to Episodic Buffer</p>
<p>Memory Access Mechanisms in Multi-Agent Systems</p>
<p>For LLM MAS, working memory equips these state-of-the-art agents with a systematic architecture to momentarily store, manage, and access information about their ever-changing environments.This isn't merely about tapping into the robust computational might of LLMs, but channeling it proficiently.It ensures that agents can communicate seamlessly, acclimatize to fluid scenarios, and synergize toward collective goals.</p>
<p>In MAS, managing agent access to memory for episodic buffer is crucial for efficiency and security.</p>
<p>Strategies for memory access vary based on factors such as the agent's role, task specifications, collaboration needs, and the overall system architecture.Common techniques include role-based access control, task-driven memory allocation, autonomous memory retrieval, and dedicated memory management agents.Implementing the optimal strategy involves balancing flexibility for the agents against vulnerabilities and coordination costs for the system.The subsequent sections explore different memory access mechanisms for agents in further detail.</p>
<p>1) Role-Based Memory Access</p>
<p>In a MAS, a basic method to ensure efficient memory utilization is the implementation of role-based memory access.Under this mechanism, agents are assigned memory access rights in accordance with their designated roles and responsibilities within the system.</p>
<p>Consider an agent whose role is that of a supervisor or overseer.Such an agent might be granted comprehensive access rights, allowing it to retrieve the memory of all other agents.This broad access facilitates its supervisory role, enabling it to monitor, assess, and coordinate the activities of other agents, ensuring that the entire system functions cohesively.Conversely, a specialized agent, dedicated to a specific task, might be confined to accessing only its own recent memory.This restricted access ensures that the agent remains focused on its immediate task without the potential distraction or computational overhead of unrelated past interactions.The agent can thus operate efficiently, referencing only the most relevant past actions or discussions to inform its current task.</p>
<p>While role-based memory access provides structured efficiency, it's essential to tailor these roles with flexibility in mind.Overly rigid rules might stifle an agent's adaptability, whereas too broad access could lead to inefficiencies or potential security vulnerabilities.The key is to find a balance, ensuring each agent has access to the information it genuinely needs to perform its role optimally.</p>
<p>2) Task-based Memory Access</p>
<p>In this strategy, memory access is intricately tied to the specific task an agent is assigned.Rather than granting access based on an agent's general role or capabilities, the system evaluates the exact nature and requirements of the task at hand and subsequently provides the agent with memories directly relevant to that task.This ensures that the agent receives information optimized for its immediate needs, leading to potentially faster and more accurate task completion.</p>
<p>3) Autonomous Memory Access</p>
<p>In advanced setups, agents are endowed with the autonomy to self-determine which memory segments they need.Rather than relying on rigid predefined rules, these agents use contextual clues from their tasks to fetch the most relevant memory sections.There aren't strict boundaries on what the agent can access, allowing it to use its best judgment.For instance, an agent tasked with creating a comprehensive review of climate change effects might access past interactions discussing greenhouse gases, sea-level rise, and carbon footprints, even if they weren't explicitly related to the current task.</p>
<p>In contrast, the Task-Specific Memory Access approach is more stringent.Here, agents are restricted to accessing only those memory segments directly associated with their current task.This creates a clear boundary on what parts of the history an agent can revisit.For example, if the same agent is specifically assigned to draft a report on sea-level rise due to climate change, it would solely access interactions related to sea-level rise, sidelining broader climate change discussions.</p>
<p>4) Collaboration Scenario-Based Memory Access</p>
<p>When agents work collaboratively, the memory access strategy might vary based on the nature of their collaboration.In parallel collaborations, agents might need full access to shared memory.In sequential collaborations, each agent might access memory based on what the preceding agent processed.Consider a sequential study of a forest's health.The first agent might examine satellite images for deforestation rates.The subsequent agent, building on this, might then delve into data on local wildlife populations, accessing memories of the previous agent's findings to correlate habitat loss with species decline.</p>
<p>5) Memory Management Agent</p>
<p>When operating within a multi-agent system, especially in scenarios demanding strategic planning and foresight, the role of a Memory Management Agent becomes paramount.The essence of planning revolves around considering past interactions, outcomes, and patterns to predict the most optimal course of action for the future.This is where the memory -encapsulated as chat or action histories of agents -becomes invaluable.</p>
<p>The Memory Management Agent is specially tailored to manage, sort, and retrieve relevant portions of this historical data.Given its specialized function, it can offer a more streamlined and efficient service compared to individual agents trying to sift through vast amounts of data on their own.For instance, in the context of environmental research, consider an agent tasked with planning a new research study on the impact of industrial pollutants on freshwater lakes.Instead of the agent independently trawling through all past interactions and data, the Memory Management Agent can provide it with precise interactions related to similar studies, pollutants of interest, and pertinent findings.This not only saves computational time but also ensures that the planning agent gets the most relevant and comprehensive information.Furthermore, the Memory Management Agent can understand the context and depth of the planning requirement.If an agent is planning a short-term experiment, the Memory Management Agent might prioritize recent interactions and newer data.</p>
<p>Conversely, for a long-term study, it might provide a mix of historical trends and recent breakthroughs.</p>
<p>In essence, by centralizing the memory retrieval process, especially for planning tasks, the Memory Management Agent can optimize the efficiency of the multi-agent system.It ensures that agents tasked with planning receive a curated set of memories, allowing them to make more informed and strategic decisions.</p>
<p>Strategies to Improve Memory Retrieval Efficiency in Multi-Agent Systems</p>
<p>In the realm of MAS, the efficient retrieval of agent memory is paramount to ensure timely and relevant responses.As the memories of agents are stored in structured repositories, employing the right retrieval method can drastically influence the efficacy of the agent in processing user queries.</p>
<p>Here, we delve into three primary search mechanisms: full-text search, semantic search (vector search), and SQL search.Each of these techniques caters to different application scenarios, offering a unique approach to memory retrieval in the MAS ecosystem.</p>
<p>1) SQL Search</p>
<p>SQL (Structured Query Language) is a domain-specific language designed for managing and querying databases.SQL search allows for precise data retrieval based on specific criteria, often involving structured tags, timestamps, or fields.For agents in MAS, SQL search proves invaluable when precision is crucial.An agent could, for instance, use SQL commands to retrieve memory segments from a specific time frame.If a user wants to know what the agent discussed "last Tuesday," the agent could execute an SQL query to access its working memory from that specific date, providing an exact snapshot of interactions from the desired timeframe.</p>
<p>2) Full-Text Search</p>
<p>Full-text search pertains to the process of scanning through entire textual datasets to locate specific sequences or strings of characters.This method doesn't just look for exact matches but also considers close approximations based on the text's structure and wording.In the context of MAS, agents can employ full-text search when faced with broad queries or when the exact location or tag of the information is unknown.For instance, when a user poses a general question about "climate change effects," an agent can utilize full-text search to skim through its entire memory and fetch relevant passages or interactions addressing that theme.</p>
<p>3) Semantic search</p>
<p>Semantic search is a step beyond literal text matching.For agents, semantic search allows for a deeper, more contextual memory retrieval.It enables them to understand and fetch information not just based on exact wordings but on the underlying intent or meaning.In situations where the user's query might not directly match the stored phrases but is contextually related, semantic search is invaluable.For instance, if a user inquires about "measures to combat the greenhouse effect," the agent might not find a direct mention of this phrase in its memory.However, using semantic search, the agent can retrieve interactions discussing "carbon offsetting," "reforestation," or "sustainable energy sources," as these topics share a contextual relationship with the original query, offering a nuanced and comprehensive response.</p>
<p>The intricate nature of agent working memory retrieval in a MAS necessitates a multifaceted approach.By harnessing the capabilities of full-text search, semantic search, and SQL search, we can craft a sophisticated and adaptive memory retrieval mechanism.Each method offers its own set of advantages: full-text search is direct and precise, semantic search ensures contextually relevant results, and SQL search provides chronological specificity.What's particularly promising is the synergy achieved when these methods are combined.For instance, an agent can initiate a search with SQL to pinpoint memories from a specific timeframe, then refine the results using vector search to understand the semantic nuances of the data.Such a layered approach ensures that agents can access the most relevant and accurate memories, tailoring their responses to the specific needs and context of a user's query.This amalgamation of mainstream search techniques paves the way for a more responsive, adaptive, and efficient memory management system, pushing the boundaries of what MAS can achieve.</p>
<p>Conclusion</p>
<p>This paper has explored the application of working memory models, from cognitive psychology to the emerging landscape of LLM agents.While inspired by cognitive architectures, limitations arise in directly translating human working memory principles to artificial systems.Traditional LLM agent models lack episodic memory depth and continuity across interaction domains.To address this, an enhanced model is proposed incorporating a centralized Working Memory Hub along with Episodic Buffer access.This equips agents with greater contextual memory during complex sequential tasks and collaborative engagements.The innovative model provides a strategic blueprint for developing LLM agents with more robust and human-like memory capabilities.Further advancements in memory encoding, consolidation, and retrieval mechanisms are imperative to fully actualize these ambitions.</p>
<p>While promising, the proposed working memory model has limitations requiring further research.Firstly, the model needs more precise mechanisms for determining memory relevance and retrieval priorities based on contextual factors.More advanced neural algorithms mimicking the memory consolidation process in the human brain could enhance the model.Secondly, the security vulnerabilities of memory systems with increased access need to be evaluated.Optimization between efficient memory sharing and data protection is required, especially for collaborative multiagent systems.Thirdly, methods to compress episodic memories for storage need to be developed to efficiently manage the vast amounts of long-term interaction data.Overall, bringing human-like episodic memory faculties to artificial agents remains an open grand challenge.Advancing working memory capabilities will be crucial to unlocking more fluid intelligence and cognition in these systems.</p>
<p>Figure 1
1
Figure 1 Solution for the shortcomings of traditional LLMs</p>
<p>Figure 2
2
Figure 2 Working memory model in cognitive psychology</p>
<p>Figure 3
3
Figure 3 Current working memory model of LLM agents</p>
<p>Figure 4
4
Figure 4 Innovative working memory model</p>
<p>Working memory: Looking back and looking forward | Nature Reviews Neuroscience. A Baddeley, Nature Reviews Neuroscience. 42003</p>
<p>T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, D Amodei, 10.48550/arXiv.2005.14165arXiv:2005.14165Language Models are Few-Shot Learners. 2020</p>
<p>How can Transformers and large language models like ChatGPT help LCA practitioners? Resources, Conservation and Recycling. S Cornago, S Ramakrishna, J S C Low, 10.1016/j.resconrec.2023.1070622023196107062</p>
<p>A Graves, G Wayne, I Danihelka, 10.48550/arXiv.1410.5401arXiv:1410.5401Neural Turing Machines. 2014</p>
<p>Risks and Benefits of Large Language Models for the Environment. M C Rillig, M gerstrand, M Bi, K A Gould, U Sauerland, 10.1021/acs.est.3c01106Environmental Science &amp; Technology. 5792023</p>
<p>S Sukhbaatar, A Szlam, J Weston, R Fergus, 10.48550/arXiv.1503.08895arXiv:1503.08895End-To -End Memory Networks. 2015</p>
<p>K Wang, Y Lu, M Santacroce, Y Gong, C Zhang, Y Shen, 10.48550/arXiv.2310.01444arXiv:2310.01444Adapting LLM Agents Through Communication. 2023</p>
<p>Q Wang, L Ding, Y Cao, Z Tian, S Wang, D Tao, L Guo, 10.48550/arXiv.2308.15022arXiv:2308.15022Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models. 2023</p>
<p>W Wang, L Dong, H Cheng, X Liu, X Yan, J Gao, F Wei, 10.48550/arXiv.2306.07174arXiv:2306.07174Augmenting Language Models with Long-Term Memory. 2023</p>
<p>J Weston, S Chopra, A Bordes, 10.48550/arXiv.1410.3916arXiv:1410.3916Memory Networks. 2015</p>
<p>K Zhang, F Zhao, Y Kang, X Liu, 10.48550/arXiv.2309.11696arXiv:2309.11696Memory-Augmented LLM Personalization with Short-and Long-Term Memory Coordination. 2023</p>
<p>W Zhou, Y E Jiang, P Cui, T Wang, Z Xiao, Y Hou, R Cotterell, M Sachan, 10.48550/arXiv.2305.13304arXiv:2305.13304RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text. 2023</p>
<p>PoSE: Efficient Context Window Extension of LLMs via Positional. D Zhu, N Yang, L Wang, Y Song, W Wu, F Wei, S Li, 10.48550/arXiv.2309.10400arXiv:2309.10400Skip-wise Training. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3750 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3750</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3750</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-89.html">extraction-schema-89</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2" target="_blank">On the Opportunities and Risks of Foundation Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities, to their applications, and what they are even capable of due to their emergent properties.</p>
                <p><strong>Paper Abstract:</strong> AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3750",
    "paper_id": "paper-76e9e2ec3de437ffb30d8b7b629f7fe3e61de5c2",
    "extraction_schema_id": "extraction-schema-89",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00454475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>On the Opportunities and Risks of Foundation Models</h1>
<p>Rishi Bommasani<em> Drew A. Hudson Ehsan Adeli Russ Altman Simran Arora Sydney von Arx Michael S. Bernstein Jeannette Bohg Antoine Bosselut Emma Brunskill Erik Brynjolfsson Shyamal Buch Dallas Card Rodrigo Castellon Niladri Chatterji Annie Chen Kathleen Creel Jared Quincy Davis Dorottya Demszky Chris Donahue Moussa Doumbouya Esin Durmus Stefano Ermon John Etchemendy Kawin Ethayarajh Li Fei-Fei Chelsea Finn Trevor Gale Lauren Gillespie Karan Goel Noah Goodman Shelby Grossman Neel Guha Tatsunori Hashimoto Peter Henderson John Hewitt Daniel E. Ho Jenny Hong Kyle Hsu Jing Huang Thomas Icard Saahil Jain Dan Jurafsky Pratyusha Kalluri Siddharth Karamcheti Geoff Keeling Fereshte Khani Omar Khattab Pang Wei Koh Mark Krass Ranjay Krishna Rohith Kuditipudi Ananya Kumar Faisal Ladhak Mina Lee Tony Lee Jure Leskovec Isabelle Levent Xiang Lisa Li Xuechen Li Tengyu Ma Ali Malik Christopher D. Manning Suvir Mirchandani Eric Mitchell Zanele Munyikwa Suraj Nair Avanika Narayan Deepak Narayanan Ben Newman Allen Nie Juan Carlos Niebles Hamed Nilforoshan Julian Nyarko Giray Ogut Laurel Orr Isabel Papadimitriou Joon Sung Park Chris Piech Eva Portelance Christopher Potts Aditi Raghunathan Rob Reich Hongyu Ren Frieda Rong Yusuf Roohani Camilo Ruiz Jack Ryan Christopher Ré Dorsa Sadigh Shiori Sagawa Keshav Santhanam Andy Shih Krishnan Srinivasan Alex Tamkin Rohan Taori Armin W. Thomas Florian Tramèr Rose E. Wang William Wang Bohan Wu Jiajun Wu Yuhuai Wu Sang Michael Xie Michihiro Yasunaga Jiaxuan You Matei Zaharia Michael Zhang Tianyi Zhang Xikun Zhang Yuhui Zhang Lucia Zheng Kaitlyn Zhou Percy Liang</em>1</p>
<p>Center for Research on Foundation Models (CRFM)
Stanford Institute for Human-Centered Artificial Intelligence (HAI)
Stanford University
AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) trained on broad data (generally using self-supervision at scale) that can be adapted to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotic manipulation, reasoning, human interaction) and technical principles (e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities, and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Contents
Contents ..... 2
1 Introduction ..... 3
1.1 Emergence and homogenization ..... 3
1.2 Social impact and the foundation models ecosystem ..... 7
1.3 The future of foundation models ..... 9
1.4 Overview of this report ..... 12
2 Capabilities ..... 21
2.1 Language ..... 22
2.2 Vision ..... 28
2.3 Robotics ..... 34
2.4 Reasoning and search ..... 40
2.5 Interaction ..... 44
2.6 Philosophy of understanding ..... 48
3 Applications ..... 53
3.1 Healthcare and biomedicine ..... 54
3.2 Law ..... 59
3.3 Education ..... 67
4 Technology ..... 73
4.1 Modeling ..... 74
4.2 Training ..... 81
4.3 Adaptation ..... 85
4.4 Evaluation ..... 91
4.5 Systems ..... 97
4.6 Data ..... 101
4.7 Security and privacy ..... 105
4.8 Robustness to distribution shifts ..... 109
4.9 AI safety and alignment ..... 114
4.10 Theory ..... 118
4.11 Interpretability ..... 123
5 Society ..... 129
5.1 Inequity and fairness ..... 130
5.2 Misuse ..... 136
5.3 Environment ..... 140
5.4 Legality ..... 146
5.5 Economics ..... 149
5.6 Ethics of scale ..... 152
6 Conclusion ..... 161
Acknowledgments ..... 161
References ..... 161</p>
<h1>1 INTRODUCTION</h1>
<p>This report investigates an emerging paradigm for building artificial intelligence (AI) systems based on a general class of models which we term foundation models. ${ }^{2}$ A foundation model is any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks; current examples include BERT [Devlin et al. 2019], GPT-3 [Brown et al. 2020], and CLIP [Radford et al. 2021]. From a technological point of view, foundation models are not new - they are based on deep neural networks and self-supervised learning, both of which have existed for decades. However, the sheer scale and scope of foundation models from the last few years have stretched our imagination of what is possible; for example, GPT-3 has 175 billion parameters and can be adapted via natural language prompts to do a passable job on a wide range of tasks despite not being trained explicitly to do many of those tasks [Brown et al. 2020]. At the same time, existing foundation models have the potential to accentuate harms, and their characteristics are in general poorly understood. Given their impending widespread deployment, they have become a topic of intense scrutiny [Bender et al. 2021].</p>
<h3>1.1 Emergence and homogenization</h3>
<p>The significance of foundation models can be summarized by two words: emergence and homogenization. Emergence means that the behavior of a system is implicitly induced rather than explicitly constructed; it is both the source of scientific excitement and anxiety about unanticipated consequences. Homogenization indicates the consolidation of methodologies for building machine learning systems across a wide range of applications; it provides strong leverage towards many tasks but also creates single points of failure. To better appreciate emergence and homogenization, let us reflect on their rise in AI research over the last 30 years.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. The story of AI has been one of increasing emergence and homogenization. With the introduction of machine learning, how a task is performed emerges (is inferred automatically) from examples; with deep learning, the high-level features used for prediction emerge; and with foundation models, even advanced functionalities such as in-context learning emerge. At the same time, machine learning homogenizes learning algorithms (e.g., logistic regression), deep learning homogenizes model architectures (e.g., Convolutional Neural Networks), and foundation models homogenizes the model itself (e.g., GPT-3).</p>
<p>Machine learning. Most AI systems today are powered by machine learning, where predictive models are trained on historical data and used to make future predictions. The rise of machine learning within AI started in the 1990s, representing a marked shift from the way AI systems were built previously: rather than specifying how to solve a task, a learning algorithm would induce it based on data - i.e., the how emerges from the dynamics of learning. Machine learning also</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>represented a step towards homogenization: a wide range of applications could now be powered by a single generic learning algorithm such as logistic regression.</p>
<p>Despite the ubiquity of machine learning within AI, semantically complex tasks in natural language processing (NLP) and computer vision such as question answering or object recognition, where the inputs are sentences or images, still required domain experts to perform "feature engineering" - that is, writing domain-specific logic to convert raw data into higher-level features (e.g., SIFT [Lowe 1999] in computer vision) that were more suitable for popular machine learning methods.</p>
<p>Deep learning. Around 2010, a revival of deep neural networks under the moniker of deep learning [LeCun et al. 2015] started gaining traction in the field of machine learning. Deep learning was fueled by larger datasets, more computation (notably, the availability of GPUs), and greater audacity. Deep neural networks would be trained on the raw inputs (e.g., pixels), and higher-level features would emerge through training (a process dubbed "representation learning"). This led to massive performance gains on standard benchmarks, for example, in the seminal work of AlexNet [Krizhevsky et al. 2012] on the ImageNet dataset [Deng et al. 2009]. Deep learning also reflected a further shift towards homogenization: rather than having bespoke feature engineering pipelines for each application, the same deep neural network architecture could be used for many applications.</p>
<p>Foundation models. Foundation models have taken shape most strongly in NLP, so we focus our story there for the moment. That said, much as deep learning was popularized in computer vision but exists beyond it, we understand foundation models as a general paradigm of AI, rather than specific to NLP in any way. By the end of 2018, the field of NLP was about to undergo another seismic change, marking the beginning of the era of foundation models. On a technical level, foundation models are enabled by transfer learning [Thrun 1998] and scale. The idea of transfer learning is to take the "knowledge" learned from one task (e.g., object recognition in images) and apply it to another task (e.g., activity recognition in videos). Within deep learning, pretraining is the dominant approach to transfer learning: a model is trained on a surrogate task (often just as a means to an end) and then adapted to the downstream task of interest via fine-tuning.</p>
<p>Transfer learning is what makes foundation models possible, but scale is what makes them powerful. Scale required three ingredients: (i) improvements in computer hardware - e.g., GPU throughput and memory have increased $10 \times$ over the last four years (§4.5: systems); (ii) the development of the Transformer model architecture [Vaswani et al. 2017] that leverages the parallelism of the hardware to train much more expressive models than before (§4.1: MODELING); and (iii) the availability of much more training data.</p>
<p>The importance of the availability of data and the ability to harness it cannot be underestimated. Transfer learning with annotated datasets has been common practice for at least a decade, for example, pretraining on the ImageNet dataset [Deng et al. 2009] for image classification in the computer vision community. However, the non-trivial cost of annotation imposes a practical limit on the benefits of pretraining.</p>
<p>In self-supervised learning on the other hand, the pretraining task is derived automatically from unannotated data. ${ }^{3}$ For example, the masked language modeling task used to train BERT [Devlin et al. 2019] is to predict a missing word in a sentence given its surrounding context (e.g., I like
$\qquad$ sprouts). Self-supervised tasks are not only more scalable, only depending on unlabeled data, but they are designed to force the model to predict parts of the inputs, making them richer and potentially more useful than models trained on a more limited label space.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>There had been considerable progress in self-supervised learning dating back to word embeddings [Turian et al. 2010; Mikolov et al. 2013; Pennington et al. 2014], which associated each word with a context-independent vector, provided the basis for a wide range of NLP models. Shortly thereafter, self-supervised learning based on autoregressive language modeling (predict the next word given the previous words) [Dai and Le 2015] became popular. This produced models that represented words in context, such as GPT [Radford et al. 2018], ELMo [Peters et al. 2018], and ULMFiT [Howard and Ruder 2018]. ${ }^{4}$</p>
<p>The next wave of developments in self-supervised learning - BERT [Devlin et al. 2019] GPT-2 [Radford et al. 2019], RoBERTa [Liu et al. 2019], T5 [Raffel et al. 2019], BART [Lewis et al. 2020a] quickly followed, embracing the Transformer architecture, incorporating more powerful deep bidirectional encoders of sentences, and scaling up to larger models and datasets.</p>
<p>While one can view this last wave of technical developments purely through the lens of selfsupervised learning, there was a sociological inflection point around the introduction of BERT. Before 2019, self-supervised learning with language models was essentially a subarea in NLP, which progressed in parallel to other developments in NLP. After 2019, self-supervised learning with language models became more of a substrate of NLP, as using BERT has become the norm. The acceptance that a single model could be useful for such a wide range of tasks marks the beginning of the era of foundation models.</p>
<p>Foundation models have led to an unprecedented level of homogenization: Almost all state-of-the-art NLP models are now adapted from one of a few foundation models, such as BERT, RoBERTa, BART, T5, etc. While this homogenization produces extremely high leverage (any improvements in the foundation models can lead to immediate benefits across all of NLP), it is also a liability; all AI systems might inherit the same problematic biases of a few foundation models [Bolukbasi et al. 2016; Caliskan et al. 2017; Abid et al. 2021, inter alia]) - see §5.1: FAIRNESS, §5.6: ETHics for further discussion.</p>
<p>We are also beginning to see a homogenization across research communities. For example, similar Transformer-based sequence modeling approaches are now applied to text [Devlin et al. 2019; Radford et al. 2019; Raffel et al. 2019], images [Dosovitskiy et al. 2020; Chen et al. 2020d], speech [Liu et al. 2020d], tabular data [Yin et al. 2020], protein sequences [Rives et al. 2021], organic molecules [Rothchild et al. 2021], and reinforcement learning [Chen et al. 2021b; Janner et al. 2021]. These examples point to a possible future where we have a unified set of tools for developing foundation models across a wide range of modalities [Tamkin et al. 2021b].</p>
<p>Besides the homogenization of approaches, we also see the homogenization of actual models across research communities in the form of multimodal models - e.g., foundation models trained on language and vision data [Luo et al. 2020; Kim et al. 2021a; Cho et al. 2021; Ramesh et al. 2021; Radford et al. 2021]. Data is naturally multimodal in some domains-e.g., medical images, structured data, clinical text in healthcare (§3.1: HEALTHCARE). Thus, multimodal foundation models are a natural way of fusing all the relevant information about a domain, and adapting to tasks that also span multiple modes (Figure 2).</p>
<p>Foundation models have also led to surprising emergence which results from scale. For example, GPT-3 [Brown et al. 2020], with 175 billion parameters compared to GPT-2's 1.5 billion, permits in-context learning, in which the language model can be adapted to a downstream task simply by providing it with a prompt (a natural language description of the task), an emergent property that was neither specifically trained for nor anticipated to arise.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. A foundation model can centralize the information from all the data from various modalities. This one model can then be adapted to a wide range of downstream tasks.</p>
<p>Homogenization and emergence interact in a potentially unsettling way. Homogenization could potentially provide enormous gains for many domains where task-specific data is quite limited - see the opportunities presented in several such domains (e.g., §3.1: HEAlTHCARE, §3.2: LAW, §3.3: EDUCATION); on the other hand, any flaws in the model are blindly inherited by all adapted models (§5.1: FAIRNESS, §5.6: ETHICS). Since the power of foundation models comes from their emergent qualities rather than their explicit construction, existing foundation models are hard to understand (§4.4: EVALUATION, §4.10: THEORY, §4.11: INTERPRETABILITY) and they have unexpected failure modes (§4.7: SECURITY, §4.8: ROBUSTNESS). Since emergence generates substantial uncertainty over the capabilities and flaws of foundation models, aggressive homogenization through these models is risky business. Derisking is the central challenge in the further development of foundation models from an ethical (§5.6: ETHics) and AI safety (§4.9: AI-SAFETY) perspective.</p>
<h1>1.1.1 Naming.</h1>
<p>We introduce the term foundation models to fill a void in describing the paradigm shift we are witnessing; we briefly recount some of our reasoning for this decision. Existing terms (e.g., pretrained model, self-supervised model) partially capture the technical dimension of these models, but fail to capture the significance of the paradigm shift in an accessible manner for those beyond machine learning. In particular, foundation model designates a model class that are distinctive in their sociological impact and how they have conferred a broad shift in AI research and deployment. In contrast, forms of pretraining and self-supervision that technically foreshadowed foundation models fail to clarify the shift in practices we hope to highlight.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Before reasoning about the social impact of foundation models, it is important to understand that they are part of a broader ecosystem that stretches from data creation to deployment. At both ends, we highlight the role of people as the ultimate source of data into training of a foundation model, but also as the downstream recipients of any benefits and harms. Thoughtful data curation and adaptation should be part of the responsible development of any AI system. Finally, note that the deployment of adapted foundation models is a decision separate from their construction, which could be for research.</p>
<p>Additionally, while many of the iconic foundation models at the time of writing are language models, the term language model is simply too narrow for our purpose: as we describe, the scope of foundation models goes well beyond language. We also considered terms such as general-purpose model and multi-purpose model that capture the important aspect that these models can serve multiple downstream tasks, but both fail to capture their unfinished character and the need for adaptation. Terms such as task-agnostic model would capture the manner of training, but fail to capture the significant implication to downstream applications.</p>
<p>We chose the new term foundation models to identify the models and the emerging paradigm that are the subject of this report. In particular, the word "foundation" specifies the role these models play: a foundation model is itself incomplete but serves as the common basis from which many task-specific models are built via adaptation. We also chose the term "foundation" to connote the significance of architectural stability, safety, and security: poorly-constructed foundations are a recipe for disaster and well-executed foundations are a reliable bedrock for future applications. At present, we emphasize that we do not fully understand the nature or quality of the foundation that foundation models provide; we cannot characterize whether the foundation is trustworthy or not. Thus, this is a critical problem for researchers, foundation model providers, application developers who rely on foundation models, policymakers, and society at large to address.</p>
<h1>1.2 Social impact and the foundation models ecosystem</h1>
<p>Foundation models are scientifically interesting due to their impressive performance and capabilities, but what makes them critical to study is the fact that they are quickly being integrated into realworld deployments of AI systems with far-reaching consequences on people. For example, Google search, which boasts 4 billion users, now depends on foundation models like BERT [Devlin et al. 2019] as one of its signals. ${ }^{3}$</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We must thus pause and ask: What is the nature of this social impact? In this report, we address many aspects of this question: the potential exacerbation of social inequities (§5.1: FAIRNESS), the economic impact due to increased capabilities (§5.5: ECONOMICS), the environmental impact due to increased computation demands (§5.3: ENVIRONMENT), potential concerns of amplifying disinformation (§5.2: MISUSE), legal ramifications due to powerful generative capabilities (§5.4: LEGALITY), ethical issues resulting from homogenization, and the broader political economy in which foundation models are developed and deployed (§5.6: ETHICS). Given the protean nature of foundation models and their unmapped capabilities, how can we responsibly anticipate and address the ethical and societal considerations they raise? A recurring theme is that it is easier to reason about the social impact of specific systems deployed to specific users than it is to reason about the social impact of foundation models, which could be adapted to any number of unforeseen downstream systems.</p>
<p>Before attempting to answer these questions, we need to lay some groundwork. First, let us distinguish between research on foundation models and deployment of foundation models. Most of what is publicly known is foundation models research - through academic papers, demonstrations, and progress on leaderboards. While the production of knowledge can play a vital role in shaping the future, the direct social impact is through the actual deployment of these models, which is governed by proprietary practices on often private data. Sometimes the deployment is through new products - e.g., GitHub's Copilot ${ }^{6}$ based on OpenAI's Codex model [Chen et al. 2021f], but often, it is through upgrades to existing products (e.g., Google search using BERT). Research models are often not extensively tested and might have unknown failure modes; warning labels should be placed on research models that are not fit to deploy. On the other hand, deployed foundation models that actually affect people's lives should be subject to much more rigorous testing and auditing.</p>
<p>To further understand the research and deployment of foundation models, we must zoom out and consider the full ecosystem that these foundation models inhabit, from data creation to actual deployment. It is important to note that the foundation model is only one component (though an increasingly important component) of an AI system. Simplifying, we can think about the ecosystem of a foundation model in terms of sequence of stages, extending the training and adaptation stages from before. ${ }^{7}$ Appropriately, as we're interested in social impact, people occupy both ends of the pipeline. This ecosystem view allows us to see that different questions about foundation models (e.g., whether a foundation model is ethical) should actually be answered with respect to different stages.
(1) Data creation: Data creation is fundamentally a human-centric process: all data is created by people and most data is at least implicitly about people. Sometimes data is created by people for other people in the form of emails, articles, photos, etc., and sometimes it is a measurement of people (e.g., genomic data) or a measurement of the environment people live in (e.g., satellite images). It is important to note that all data has an owner and is created with a purpose (where that purpose may or may not include training a foundation model).
(2) Data curation: Data is then curated into datasets. There is no single natural distribution of data; even the most permissive Internet crawl requires some selection and post-filtering. Ensuring data relevance and quality while respecting legal and ethical constraints is critical but challenging. While this is recognized in industry, it is underappreciated in AI research (§4.6: DATA).</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(3) Training: Training foundation models on these curated datasets ${ }^{8}$ is the celebrated centerpiece in AI research, though it is only one of many stages.
(4) Adaptation: In the context of machine learning research, adaptation is about creating a new model based on the foundation model that performs some task (e.g., document summarization). For deployment, adaptation is about creating a system, which requires potentially many different modules, custom rules (e.g., restrictions on the output space) or classifiers (e.g., for toxicity classification), and combination with other complementary signals (e.g., a question answering model's generated answers would be validated against relevant documents). For example, a problematic model capable of generating toxic content might be tolerable if appropriate precautions are taken downstream. The extra application-specific logic is crucial for mitigating harms.
(5) Deployment: The direct social impact of an AI system occurs when it is deployed to people. Though we would not want to deploy potentially harmful foundation models trained on questionable data, there might still be value in permitting them in research to advance scientific understanding, though one must still exercise caution. More generally, it is standard practice in large-scale deployments to conduct gradual releases, where deployment happens to an increasing fraction of users; this can partially mitigate any potential harms.
While this report is about foundation models, it is important to note that many of the impacts come from decisions made in other stages in the pipeline, and thoughtful monitoring and intervention is needed at every stage. While large organizations might own the entire pipeline, each stage could be performed by a different organization, e.g., a company which specializes in creating custom foundation models for various domains that application-developers can use.</p>
<p>Think ecosystem, act model. While the social impact depends on the whole ecosystem, it is still important to be able to reason about the social implications of a foundation model, given that many researchers' and practitioners' purview is restricted to the training stage. This is difficult because foundation models are unfinished intermediate objects that can be adapted to many downstream applications, sometimes by an entirely different entity for unforeseen purposes. What we need are two things: (i) surrogate metrics for a representative set of potential downstream evaluation (§4.4: eVALUATION), and (ii) a commitment to documenting these metrics [Mitchell et al. 2019] similar to data sheets for materials such as metals and plastics, which can be adapted to many downstream use cases.</p>
<p>Characterizing the potential downstream social impact of foundation models is challenging and demands a deep understanding of both the technological ecosystem and of society. One cannot fully assess the harms (§5.1: FAIRNESS) of a foundation model without recognizing how it will be deployed, and one cannot just define automatic metrics without considering the rich social and historical context.</p>
<h1>1.3 The future of foundation models</h1>
<p>Foundation models have demonstrated raw potential, but we are still in the early days. Despite their deployment into the real world, these models are very much research prototypes that are poorly understood. Even the professional norms - what Robert Merton calls the ethos of science [Merton 1979] - around foundation models are underdeveloped. For example, there is lack of agreement on basic questions such as when models are "safe" to release or how the community should react in response to methodological misconduct. Given that the future of foundation models is thus filled with uncertainty, a big question is: who will determine this future?</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Disciplinary diversity. The technology behind foundation models is based on decades of research in machine learning, optimization, NLP, computer vision, and other fields. These technical contributions have come from both academia and industrial research labs. However, research on building foundation models themselves has occurred almost exclusively in industry - big tech companies such as Google, Facebook, Microsoft, or Huawei, or startups such as OpenAI or AI21 Labs, though AI2 is a notable exception [Peters et al. 2018; Zellers et al. 2019b].</p>
<p>The furious pace of technological progress and the entrenchment due to centralization raise powerful concerns that demand the attention of humanists and social scientists in addition to technologists. We should not rely on post-hoc audits of ethical and social consequences, conducted only after the technical architecture and deployment decisions have been made. We instead need to infuse social considerations and ethical design deeply into the technological development of foundation models and their surrounding ecosystem from the start. Academic institutions are unique in that they host the widest set of disciplines under one roof, thus bringing together computer scientists, social scientists, economists, ethicists, legal scholars, etc. Given the importance of disciplinary diversity in understanding and solving problems that combine technical, ethical, legal, social, and political dimensions [Hong and Page 2004; Solomon 2006; Steel et al. 2018], we therefore see academia as playing a crucial role in developing foundation models in such a way to promote their social benefit and mitigate their social harms, as well as determining the contexts under which actions in each of the stages of the ecosystem (§1.2: ECOSYSTEM) ranging from data curation to deployment should be strictly prohibited.</p>
<p>Incentives. The political economy in which foundations models are designed, developed, and deployed provides an inevitable incentive structure for decision-making at every stage. How people and institutions respond to incentives is an elementary lesson of economics. Market-driven commercial incentives can align well with social benefit: making foundation models more accurate, reliable, safe, and efficient while searching for a wide variety of potential use cases can produce a great deal of social utility. However, commercial incentives can also lead to market failures and underinvestment in domains where shareholders are unable to capture the value of innovation. Just as the pharmaceutical industry has little incentive to devote significant resources to the research and development of malaria treatments, because poor people cannot afford medications, ${ }^{9}$ the tech industry has little incentive to devote significant resources to technologies designed for improving the condition of poor and marginalized people [Reich et al. 2021]. What's more, commercial incentives can lead companies to ignore social externalities [Acemoglu 2021; Reich et al. 2021] such as the technological displacement of labor, the health of an informational ecosystem required for democracy, the environmental cost of computing resources, and the profit-driven sale of technologies to non-democratic regimes. Finally, there is little incentive for any given company to create an open, decentralized ecosystem for developing foundation models that encourages broad participation.</p>
<p>In contrast, the long-standing and deeply-seated research mission of universities is the production and dissemination of knowledge and creation of global public goods [Kerr 2001; Rhoten and Calhoun 2011; Nussbaum 2010]. We believe that academia is distinctively positioned to shape the development of foundation models to ensure that we capture directions with potentially large social benefit that might not otherwise be prioritized by industry.</p>
<p>Loss in accessibility. Unfortunately, academia has not been able to participate in the fullest way possible due to the loss in accessibility. One of the often overlooked effects of the deep learning revolution was the increase in reproducibility and open science: it increasingly became the norm</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>to publicly release code and datasets, and packages such as TensorFlow [Abadi et al. 2016] and PyTorch [Paszke et al. 2019] made it much easier for people to collaborate and build off of each other's work. Initiatives like the ML Reproducibility Challenge ${ }^{10}$ as well as reproducibility checklists adopted by major conferences [Pineau et al. 2020], alongside platforms like CodaLab Worksheets ${ }^{11}$ helped advance community standards for reproducibility. This resulted in a surge in technological innovation and progress.</p>
<p>Foundation models start to roll back this positive trend. Some models (e.g., GPT-3) are not released at all (only API access to a limited pool of people). Even datasets (e.g., for GPT-2) are not released. While trained models may be available (e.g., BERT), the actual training of foundation models is unavailable to the vast majority of AI researchers, due to the much higher computational cost and the complex engineering requirements.</p>
<p>Some meaningful research can still be done by training smaller models within reach of an academic budget, and indeed the surprisingly regularity predicted by scaling laws [Kaplan et al. 2020] make this a viable strategy for cases where the differences due to scale are quantitative (e.g., accuracy goes up). However, due to the emergent nature of these foundation models, some functionalities like in-context learning have only been demonstrated in models of sufficient size, so scale is needed to even ask the right questions.</p>
<p>It is also possible to productively study pre-existing models that have been released; indeed, this has led to a large subcommunity within NLP for probing these models [Rogers et al. 2020; Manning et al. 2020]. Having access to existing models can be useful for powering downstream applications or identifying defects (e.g., bias), but this might not be enough for us to design better architectures or training objectives for foundation models that can fix these defects (e.g., mitigate the bias). It is worth reflecting on how much of NLP research today is based on BERT, a particular (and somewhat arbitrary) foundation model. Given the need to infuse social awareness and ethical design into the construction of these models, it is possible that we need to build foundation models that look quite different from what exists today. This will demand intense experimentation at scale.</p>
<p>Community efforts such as EleutherAI ${ }^{12}$ and Hugging Face's BigScience project ${ }^{13}$ are attempting to train large foundation models, but the gap between the private models that industry can train and the ones that are open to the community will likely remain large if not grow. Further, today startups (OpenAI, Anthropic, AI21 Labs, etc.) are much more well-resourced than academia and can therefore still afford to train the largest foundation models (e.g., OpenAI's GPT-3). However, big tech companies are on a completely different level in terms of resources, especially in terms of the infrastructure, users, and data that come from their market position. The fundamental centralizing nature of foundation models means that the barrier to entry for developing them will continue to rise, so that even startups, despite their agility, will find it difficult to compete, a trend that is reflected in the development of search engines [Radinsky 2015].</p>
<p>One way to close the resource gap is for the government to invest in public infrastructure. We can look to Big Science projects such as the Hubble Space Telescope and the Large Hadron Collider as inspiration, where substantial investment made possible fundamental scientific discoveries which wouldn't have been possible. One can imagine a similar infrastructure for computing, from which academic research on foundation models would greatly benefit. In the US, the nascent National Research Cloud initiative ${ }^{14}$ is a step in this direction.</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Another complementary approach is to rely on volunteer computing, in which any of the billions of computing devices (nodes) can connect to a central server and contribute computation. The Folding@home project has successfully implemented this approach for simulating protein dynamics [Beberg et al. 2009]. Recently, the Learning@home project is attempting to harness volunteer computing for training foundation models [Ryabinin and Gusev 2020]. The high latency connections between nodes and the high bandwidth requirements for training foundation models make this an open technical challenge.</p>
<p>Summary. There are tremendous economic incentives to push the capabilities and scale of foundation models, so we anticipate steady technological progress over the coming years. But the suitability of a technology relying largely on emergent behavior for widespread deployment to people is unclear. What is clear that we need to be cautious, and that now is the time to establish the professional norms that will enable the responsible research and deployment of foundation models. Academia and industry need to collaborate on this: industry ultimately makes concrete decisions about how foundation models will be deployed, but we should also lean on academia, with its disciplinary diversity and non-commercial incentives around knowledge production and social benefit, to provide distinctive guidance on the development and deployment of foundation models that is both technically and ethically grounded.</p>
<h1>1.4 Overview of this report</h1>
<p>In March 2021, we created an informal community at Stanford University of students, faculty, and researchers interested in some aspect of foundation models. ${ }^{15}$ From the very beginning, the community included not just AI researchers, but those eager to apply foundation models to their domain (e.g., healthcare and law), as well as those who were interested in societal concerns (e.g., ethics and economics). As discussions progressed, we noticed that there were many gaps in mutual understanding - how the technology worked, how industry develops foundation models, how to think about the ethical concerns, etc., and existing literature only covered bits and pieces. We wanted to therefore provide a fuller picture of foundation models, identify opportunities and risks, and establish a constructive vision for the future responsible development of foundation models.</p>
<p>The writing of this report was an experiment: we had over 100 people from different backgrounds come together to write a single report covering a wide range of aspects of foundation models. A large part of this report is a survey of existing work, but through many discussions, we have unified it in one report to highlight all the interdisciplinary connections.</p>
<p>Structure. The report is divided into 26 sections, each discussing one aspect of foundation models. The sections are grouped into four parts: capabilities (\$2: capabilities), applications (\$3: apPlICATIONS), technology ( $\S 4$ : TECHNOLOGY), and society ( $\S 5$ : SOCIETY), although there are many connections across sections. These connections highlight an integrated approach in which the technologies and capabilities are developed in a way that is sensitive to real societal concerns, while being inspired by and grounded out in applications.</p>
<p>While we have sought to capture most of the important topics surrounding foundation models, this report will inevitably be incomplete, especially as the field evolves quickly. For example, many applications (e.g., natural sciences, music, finance, agriculture) are not included, though they are as likely to be affected as the applications we have chosen to discuss. It would also be interesting to</p>
<p><sup id="fnref10:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Paper Roadmap</h1>
<ol>
<li>Capabilities
<img alt="img-3.jpeg" src="img-3.jpeg" /></li>
<li>Applications
<img alt="img-4.jpeg" src="img-4.jpeg" /></li>
<li>Technology
<img alt="img-5.jpeg" src="img-5.jpeg" /></li>
<li>Society
<img alt="img-6.jpeg" src="img-6.jpeg" /></li>
</ol>
<p>Fig. 4. This report is divided into four parts: capabilities, applications, technology, and society, where each part contains a set of sections, and each section covers one aspect of foundation models.
study how foundation models relate to research in neuroscience, cognitive science, and psychology to explain intelligence and aid efforts in computational social science to understand society.</p>
<p>Author Contributions Percy Liang initiated and conceptualized the framing and structure of the overall report. He and Rishi Bommasani worked together to lead the decentralized writing effort and provided guidance on individual sections. Drew A. Hudson created all the figures in the report, discussing their structure and content with the authors of each section. Each of the 26 sections of this report was written by a subset of authors, whose names are listed at the beginning of each section. There were, however, many discussions that spanned multiple sections, so the actual contributions to each section generally came from a broader set. Finally, we note that not all the views expressed in this report are held by all the authors.</p>
<h1>1.4.1 Overview of capabilities.</h1>
<p>Foundation models acquire various capabilities that can power applications. We have chosen to discuss five potential capabilities: the ability to process different modalities (e.g., language, vision), to affect the physical world (robotics), to perform reasoning, and to interact with humans (interaction). Finally, we conclude with a philosophical discussion of potential limits on their capabilities.
§2.1: Language. NLP as a field has blazed the trail for foundation models. While these models dominate standard benchmarks, there is a clear gap between the capabilities these models acquire currently and those that characterize language as a complex system for human communication and thought. In response to this, we emphasize the full range of linguistic variation (e.g., different styles, dialects, languages), which poses an opportunity and challenge given some variants are data-limited. Further, child language acquisition is more sample efficient than the training of foundation models; we examine how signals beyond text and grounding may help to bridge this gap. Both of these characteristics of language provide clear directions for future foundation models research.
§2.2: Vision. Computer vision led the adoption of deep learning in AI [Russakovsky et al. 2015], demonstrating that models pretrained on large annotated datasets can transfer to numerous downstream settings. Now, pretraining on web-scale raw data instead of curated datasets, foundation models are on the rise in computer vision [e.g., Radford et al. 2021]. These models have shown promising results for standard tasks in the field, like image classification and object detection, and training on multimodal and embodied data beyond images may enable progress on significant challenges (e.g., 3D geometric and physical understanding, commonsense reasoning). We also discuss some of the key challenges in modeling (e.g., the ability to scale effectively to videos) and evaluation (e.g., the measurement of higher-order capabilities) along with the applications (e.g., ambient intelligence for healthcare) and societal considerations (e.g., surveillance) that will determine the impact of foundation models for computer vision going forward.
§2.3: Robotics. A longstanding goal of robotics research is to develop "generalist" robots capable of performing myriad tasks across physically diverse environments. Unlike language and vision, which have led the way with foundation models both due to the abundance of raw data to train these models on and the availability of virtual applications to apply these models to, robotics faces fundamental challenges due to being anchored to the physical world. The principal challenge in developing new types of foundation models for robotics - different in nature than their language and vision counterparts - is acquiring sufficient data of the right form that is conducive to learning: we explore how plentiful data (e.g., generic videos of humans, amongst others) that is not specific to particular environments and across modalities (e.g., language, vision) may help to bridge this gap. These new robotic foundation models could allow for easier task specification and learning, ushering in new applications (e.g., better robotic assistance for household tasks) and heightening the importance of robustness and safety (e.g., formal safety evaluation).</p>
<p>§2.4: Reasoning and search. Reasoning and search problems such as theorem proving and program synthesis have been long-standing challenges in AI. The combinatorial search space renders traditional search-based methods intractable. However, humans are known to operate intuitively even in the most mathematical of domains [Lakoff and Núñez 2000], and indeed existing work such as AlphaGo have already shown that deep neural networks can be effective in guiding the search space. But humans also transfer knowledge across tasks, facilitating much more efficient adaptation and the ability to reason more abstractly. Foundation models offer the possibility of closing this gap: their multi-purpose nature along with their strong generative and multimodal capabilities offer new leverage for controlling the combinatorial explosion inherent to search.
§2.5: Interaction. Foundation models show clear potential to transform the developer and user experience for AI systems: foundation models lower the difficulty threshold for prototyping and building AI applications due to their sample efficiency in adaptation, and raise the ceiling for novel user interaction due to their multimodal and generative capabilities. This provides a synergy we encourage going forward: developers can provide applications that better fit the user's needs and values, while introducing far more dynamic forms of interaction and opportunities for feedback.
§2.6: Philosophy of understanding. What could a foundation model come to understand about the data it is trained on? Focusing on the case of natural language, we identify different positions on the nature of understanding and explore their relevance for our central question. Our tentative conclusion is that skepticism about the capacity of future foundation models to understand natural language may be premature, especially where the models are trained on multi-modal data.</p>
<h1>1.4.2 Overview of applications.</h1>
<p>At present, foundation model research is largely confined to computer science and AI, with the impact of foundation models and the applications they support largely being centered in the tech industry. Moving forward, foundation models present clear potential to transform and extend the reach of AI across many sectors beyond the tech industry, suggesting a more pervasive effect on people's lives. While there is a multitude of applications and domains to consider, we we have chosen three applications - healthcare, law, and education - because they represent foundational pillars of our society. For foundation models to significantly contribute to these application domains, models will require specific capabilities (§2: CAPAbilities) as well as technical innovation (§4: TEChNOLOGY) to account for the unique considerations in each domain. Further, since these domains are critical to societal function (§5: SOCIETY), applying foundation models in these domains requires engaging with deeply sociotechnical matters such as those those pertaining to data (§4.6: DATA), privacy (§4.7: SECURITY), interpretability (§4.11: INTERPRETABILITY), fairness (§5.1: FAIRNESS) and ethics (§5.6: ETHICS).
§3.1: Healthcare and biomedicine. Healthcare tasks (e.g., patient care via disease treatment) and biomedical research (e.g., scientific discovery of new therapies) require expert knowledge that is limited and expensive. Foundation models present clear opportunities in these domains due to the abundance of data across many modalities (e.g., images, text, molecules) to train foundation models, as well as the value of improved sample efficiency in adaptation due to the cost of expert time and knowledge. Further, foundation models may allow for improved interface design (§2.5: INTERACTION) for both healthcare providers and patients to interact with AI systems, and their generative capabilities suggest potential for open-ended research problems like drug discovery. Simultaneously, they come with clear risks (e.g., exacerbating historical biases in medical datasets and trials). To responsibly unlock this potential requires engaging deeply with the sociotechnical</p>
<p>matters of data sources and privacy as well as model interpretability and explainability, alongside effective regulation of the use of foundation models for both healthcare and biomedicine.
§3.2: Law. Legal applications require that attorneys read and produce long coherent narratives that incorporate shifting contexts and decipher ambiguous legal standards. Foundation models may provide benefits in this domain: ample data exists in the form of legal documents and their generative capabilities are well-suited to the many generative tasks required in law, but significant improvements are required for foundation models to be able to reliably reason over various sources of information to generate truthful long-form documents. As is the care in healthcare (§3.1: HEALTHCARE), the sample efficiency of adaptation for foundation models is of heightened value given the costs of expert time and knowledge in the legal domain, which may allow for the re-allocation of expertise towards pressing problems of justice and government service. The responsible development of foundation models for law will require specific consideration of privacy, and highlights core limitations of existing foundation models that will require fundamental advances with respect to provenance for their behavior and guarantees for the factuality of their generation.
§3.3: Education. Education is a complex and subtle domain; effective teaching involves reasoning about student cognition and should reflect the learning goals of students. The nature of foundation models presents promise here that has yet to be realized in the sphere of AI for education: while certain many streams of data in education are individually too limited to train foundation models, the ability to leverage relevant data from outside the domain (e.g., the Internet) and make use of data across multiple modalities (e.g., textbooks, mathematical formula, diagrams, video-based tutorials) jointly offers hope for foundation models that are broadly applicable to educational tasks. If foundation models lead to a significant improvement in education-relevant capabilities, there is clear potential for new applications that align with the open-ended generative (e.g., problem generation) and interactive (e.g., feedback to teachers) aspects of foundation models; the sample efficient adaptation of foundation models suggests greater ability for adaptive and personalized learning. In this event, renewed consideration is required of hallmarks of applying technology to education (e.g., student privacy), along with certain concerns becoming more critical (e.g., inequity in access to technology in education, technology-aided plagiarism).</p>
<h1>1.4.3 Overview of technology.</h1>
<p>Now we discuss the technology behind building better model architectures, training and adaptation procedures, and of course scaling up the systems. One crucial but often overlooked topic is data where does it come from and what is its composition? In addition, we want foundation models to be robust to distribution shifts and secure against attackers. Finally, we wish to understand why foundation models work from both a mathematical perspective as well as an empirical perspective.
§4.1: Modeling. What structural properties give rise to a foundation model? In the modeling section, we explore the underlying architectures behind foundation models and identify 5 key attributes. First, we start by discussing expressivity of the computational model - to capture and assimilate real-world information, and scalability - to adeptly handle large quantities of highdimensional data. These properties are successfully realized by existing architectures such as the transformer network [Vaswani et al. 2017] that underpins most foundation models to date. We then proceed to attributes may be essential for the next generation of models, including: multimodallity to consume, process and potentially produce content from different sources and domains, memory capacity - to effectively store and retrieve the acquired knowledge, and finally, compositionality, to foster successful generalization to novel settings and environments. We believe that realizing the</p>
<p>full potential envisioned for foundation models will hinge on modelling advances to fulfill these desiderata.
§4.2: Training. Training objectives mathematically specify how models should learn and acquire capabilities from their training data. The current status quo for training foundation models involves modality-specific objectives (e.g., masked language modeling [Devlin et al. 2019] for text and SimCLR [Chen et al. 2020c] for images) that are often chosen heuristically. We envision that future training objectives for foundation models will reflect two changes: principled selection derived from systematic evidence and evaluation (§4.4: Evaluation), and domain-generality to provide rich, scalable, and unified training signal across data sources and modalities. We also discuss important design trade-offs, including generative vs discriminative training, the choice of input data representation, and the potential of future training objectives that involve explicit representations of goals.
§4.3: Adaptation. Foundation models are intermediary assets; they are unfinished and generally should not be used directly, instead requiring adaptation for specific downstream tasks. The de facto approach for adaptation has been fine-tuning, with recent work suggesting that lightweight fine-tuning alternatives and prompting-based methods may achieve favorable accuracy-efficiency tradeoffs. Moving forward, we envision a more expansive view of adaptation that goes beyond just specializing foundation models to perform the task of interest: adaptation will alleviate deficiencies of stand-alone foundation models (e.g., temporal adaptation to reflect changes over time in the world) or introduce constraints (e.g., GDPR compliance relating to the right to be forgotten; §4.7: SECURITY); this broader perspective on adaptation coincides with a need for new evaluation protocols (§4.4: Evaluation) that systematically evaluate adaptation methods while controlling for resources (e.g., runtime, memory) and access requirements involved in adaptation.
§4.4: Evaluation. Evaluation offers context to foundation models by providing a means to track progress, understand models, and document their capabilities and biases. Foundation models challenge the ability of standard evaluation paradigms in machine learning to achieve these goals since they are one step removed from specific tasks. To envision new paradigms in evaluation that suit foundation models, we discuss (a) evaluating foundation models directly to measure their inherent capabilities and inform how foundation models are trained, (b) evaluating task-specific models by controlling for adaptation resources and access, and (c) broader evaluation design to provide richer context beyond measures of accuracy (e.g., robustness (§4.8: Robustness), fairness (§5.1: FAIRNESS), efficiency (§4.5: SYSTEMS), environmental impact (§5.3: ENVIRONMENT)). Reform of evaluation practices will allow for evaluation that adequately serves both the diverse goals and stakeholders involved in the foundation model paradigm.
§4.5: Systems. While the training data (§4.6: DATA) determines the theoretical information available for foundation models, and model architectures (§4.1: MODELING) and training objectives (§4.2: training) determine how much of this information can be extracted, computer systems determine what is practically achievable. Systems are a key bottleneck for scaling in terms of data and model size, both of which appear to reliably track with improvements in capabilities. To ensure that we can train the next generation of foundation models efficiently (with respect to time and cost), we will require the co-design of algorithms, models, software, and hardware. This co-design is already starting to happen to in various forms, from carefully tuned parallelism strategies to new architectures such as retrieval-based and mixture-of-expert models. Beyond training, we consider what will be required to deploy applications on top of foundation models (e.g., efficient inference).</p>
<p>§4.6: Data. Data is the lifeblood of foundation models; the training data of these models largely determines what these capabilities these models can acquire. The centrality of data is not unique to foundation models; recent calls for data-centric AI [Press 2021; Ré 2021] indicate the pervasive importance of managing, understanding, and documenting data used to train machine learning models. For foundation models specifically, the current modus operandi is for training data to be selected using unspecified or unclear principles with a general lack of transparency regarding the nature of training data. We believe an alternative approach is needed to re-imagine the data ecosystem surrounding foundation models: we draw upon work on data visualization and management to propose a data hub for foundation models. We articulate how this proposal relates to many of the relevant data-centric considerations for foundation models: selection, curation, documentation, access, visualization and inspection, quality assessment, and legal regulation.
§4.7: Security and privacy. Security and privacy for foundation models is largely uncharted at present. Fundamentally, foundation models are a high-leverage single point of failure, making them a prime target for attack: existing work demonstrates a variety of security vulnerabilities (e.g., adversarial triggers to generate undesirable outputs) or privacy risks (e.g., memorization of training data) for these models. Further, the generality of foundation models compounds these concerns, intensifying the risk for function creep or dual use (i.e., use for unintended purposes). For security, we view foundation models as akin to operating systems in traditional software systems; we discuss steps towards secure foundation models which, if achieved, would provide a strong abstraction layer to build upon for reliable ML applications. For privacy, by leveraging knowledge transfer from public data, foundation models may enable more sample efficient adaptation to sensitive data distributions, i.e., privacy-preserving applications may incur less degradation in accuracy when built using foundation models.
§4.8: Robustness to distribution shifts. A major limitation of standard machine learning is that it produces models that are not robust to distribution shifts, where the training distribution does not match the test distribution (for the downstream task). Existing work shows that adapting a foundation model trained on a broad range of unlabeled data improves the robustness of adapted models across a wide variety of shifts. This opens a new set of promising directions for improving training and adaptation of foundation models for robustness. However, we do not believe that foundation models are a panacea for robustness - challenges such as extrapolation across time and spurious correlations are not likely to be fully addressed.
§4.9: AI safety and alignment. Ensuring foundation models are reliable (§4.5: systems), robust (§4.8: Robustness), and interpretable (§4.11: INTERPRETABILITY) is increasingly important when considering the potential real-world applications of these models. In addition to critical and immediate considerations, we also consider the relationship between foundation models and larger-scale risks, hazards, and harms that have the potential for increased relevance as model capabilities continue to advance. For example, we consider the importance of aligning foundation models such that they are not deployed with misspecified goals or values. We also discuss the relevance of forecasting the emergent behaviors of foundation models (e.g., the ability to deceive or plan strategically), which may complicate attempts to adapt them to particular tasks, and may require new approaches for interpretability (§4.11: INTERPRETABILITY) or evaluation (§4.4: EVALUATION).
§4.10: Theory. Learning theory provides a broad foundation for the variety of contexts encountered in applied machine learning; theory offers both understanding, principles, and guarantees to complement empirical findings. At present, the study of foundation models is largely empirical: the theory of standard supervised learning, while relatively mature, is inadequate to fully explain foundation models. Specifically, the discrepancy between the training phase and the adaptation</p>
<p>phase within the foundation model regime pinpoints the insufficiency of existing theory, since these phases correspond to (potentially) completely different tasks and data distributions. Nevertheless, we endeavor that advances in theory to address this discrepancy, even in simple, limited settings, will provide useful insights.
§4.11: Interpretability. Interpretability provides clarity to foundation models: the opacity of the deep neural networks that underpin foundation models, alongside the expected ubiquity of foundation models, heightens the need to understand these models and their capabilities. Interpretability methods at present generally are designed for interpreting and explaining the behavior of task-specific models; the nature of foundation models (i.e., the wide array of tasks these models are beneficial for and the unexpected emergent properties they acquire) introduces new challenges for interpretability research. To frame the discussion of interpretability for foundation models, we propose the one model-many models paradigm, which aims to determine the extent to which the one model (the foundation model) and its many models (its adapted derivatives) share decision-making building blocks. In addition to interpreting the decision-making components involved, we further discuss explainability in the context of foundation models (e.g., the validity ofpost hoc explanations generated by models) as well as the mechanisms that drive model behavior (which may clarify the extent to which understanding foundation models can extend to understanding their adapted derivatives). Given the critical role we ascribe interpretability in the study of foundation models, we conclude with an assessment of the societal impact of interpretability and non-interpretability.</p>
<h1>1.4.4 Overview of society.</h1>
<p>We believe the rapid development of foundation models, adapted and deployed to various applications, will have wide-ranging consequences on the health of societies. What makes these models so exciting and also so troubling is their task agnosticity. Societal impact is easier (but still non-trivial) to understand and reason about when we talk about specific systems deployed to users, but how can we take into account the societal impact of all possible systems and use cases when developing foundation models?
§5.1: Inequity and fairness. In many contexts, machine learning has been shown to contribute to, and potentially amplify, societal inequity. Foundation models may extend this trend, i.e., furthering the unjust treatment of people who have been historically discriminated against. However, understanding the relationship between inequity and foundation models requires reckoning with the abstraction of foundation models; foundation models are intermediary assets that are adapted for applications that impact users. Therefore, we delineate intrinsic biases, i.e., properties in foundation models that portend harm, and extrinsic harms, i.e., harms arising in the context of specific applications built using foundation models. We taxonomize various sources (e.g., training data, lack of diversity among foundation model developers, the broader sociotechnical context) that give rise to these biases and harms, emphasizing the importance, and technical difficulty, of source tracing to understand ethical and legal responsibility. We do not view unfairness as inevitable in the foundation model paradigm: to address unfair outcomes that arise from foundation models, we dually consider proactive interventions (e.g., technical methods like counterfactual data augmentation) and reactive recourse (e.g., mechanisms for feedback propagation and attribution of moral/legal responsibility).
§5.2: Misuse. We define foundation model misuse as the use of foundation models as they are technically intended (e.g., to generate language or video), but with the goal of causing societal harm (e.g., to generate disinformation, to develop deepfakes for harassment). We argue that advances in foundation models will result in higher-quality machine-generated content that will be easier to</p>
<p>create and personalize for misuse purposes. For example, disinformation actors may use them to quickly generate collections of articles targeted across different demographic groups (e.g., nationality, political party, religion, etc.). While these new capabilities may limit existing human detection methods for harmful content (e.g., tracking similar text across different sources), foundation models may themselves provide promising potential as automated misuse detectors.
§5.3: Environment. Foundation models are the byproducts of computationally expensive training regimes, with the existing trajectory favoring even more intensive models; the energy required for this training coincides with the release of more carbon into the atmosphere and the degradation of the environment. At present, current discussion centers these enormous single-time training costs and the potential to amortize these costs across repeated use. We seek to clarify these discussions by identifying assumptions that shape the calculus of environmental impact for foundation models. Further, we envision that the ecosystem surrounding foundation models requires a multi-faceted approach: (a) more compute-efficient models, hardware, and energy grids all may mitigate the carbon burden of these models, (b) environmental cost should be a clear factor that informs how foundation models are evaluated (§4.4: Evaluation), such that foundation models can be more comprehensively juxtaposed with more environment-friendly baselines, and (c) the cost-benefit analysis surrounding environmental impact necessitates greater documentation and measurement across the community.
§5.4: Legality. Foundation models rest on tenuous legal footings at present; how the law bears on both the development and use of these models is largely unclear. Legal and regulatory frameworks for foundation models specifically, alongside those for AI technology more generally, will be needed to influence, constrain, and even foster practices in research, development, and deployment. Centering on the legal landscape of the United States, where existing consideration of algorithmic tools remains broadly uncertain, we highlight the pertinent issues of liability for model predictions and protections from model behavior. With respect to both issues, we describe how legal standards will need to be advanced to address these given the intermediary status of foundation models (as opposed to that of user-facing task-specific models).
§5.5: Economics. Foundation models are likely to have substantial economic impact due to their novel capabilities and potential applications in a wide variety of industries and occupations. We consider the implications of the development and use of foundation models for the future of the US and global economy with a focus on productivity, wage inequality, and concentration of ownership.
§5.6: Ethics of scale. In addition to running the risk of increasing inequity, as discussed in §5.1: FAIRNESS, the widespread adoption of foundation models poses other ethical, political and social concerns. We discuss ethical issues related to the scale of application of foundation models, such as homogenization and the concentration of power, as well as the norms and release strategies appropriate to address them.</p>
<h1>2 CAPABILITIES</h1>
<p>Foundation models acquire capabilities, some that surprisingly emerge from their learning process, that power downstream applications (§3: APPLICATIONs). Specifically, we discuss linguistic (§2.1: LANGUAGE) and visual (§2.2: VISION) capabilities alongside the ability to affect the physical world (§2.3: ROBOTICS), perform reasoning and search (§2.4: REASONING), and interact with humans (§2.5: INTERACTION). In addition, we discuss how self-supervision (the technical approach used to learn most current foundation models) philosophically relates to the ability to understand (§2.6: PHILOSOPHY).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{15}$ This community led to the founding of the Center for Research on Foundation Models (CRFM), a new interdisciplinary initiative at the Stanford Institute for Human-Centered AI (HAI).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref10:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
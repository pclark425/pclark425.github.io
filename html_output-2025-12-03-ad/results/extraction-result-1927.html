<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1927 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1927</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1927</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-41.html">extraction-schema-41</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <p><strong>Paper ID:</strong> paper-279403551</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.12779v3.pdf" target="_blank">From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots</a></p>
                <p><strong>Paper Abstract:</strong> Achieving general agile whole-body control on humanoid robots remains a major challenge due to diverse motion demands and data conflicts. While existing frameworks excel in training single motion-specific policies, they struggle to generalize across highly varied behaviors due to conflicting control requirements and mismatched data distributions. In this work, we propose BumbleBee (BB), an expert-generalist learning framework that combines motion clustering and sim-to-real adaptation to overcome these challenges. BB first leverages an autoencoder-based clustering method to group behaviorally similar motions using motion features and motion descriptions. Expert policies are then trained within each cluster and refined with real-world data through iterative delta action modeling to bridge the sim-to-real gap. Finally, these experts are distilled into a unified generalist controller that preserves agility and robustness across all motion types. Experiments on two simulations and a real humanoid robot demonstrate that BB achieves state-of-the-art general whole-body control, setting a new benchmark for agile, robust, and generalizable humanoid performance in the real world. The project webpage is available at https://beingbeyond.github.io/BumbleBee/.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1927.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1927.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BB (BumbleBee)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BumbleBee: Expert-to-Generalist Whole-Body Control for Humanoid Robots</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An expert-to-generalist pipeline that trains cluster-specific motion tracking experts and cluster-specific delta-action models in simulation, refines them with real-world rollouts on a Unitree G1 humanoid, and distills experts into a Transformer-based general whole-body controller to improve sim-to-real transfer for a wide range of whole-body motions (jumping, walking, in-place motions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Whole-body motion tracking of humanoid behaviors (jump, walk-slow, walk-fast, stand-up, stand-mid, stand-low)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich (frequent foot-ground contacts across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>Position-level PD control (policy outputs target joint positions for a PD controller for 23 actuated joints); low-level control loop at 200 Hz; policy inference at 50 Hz; delta-action corrections applied to action outputs (cluster-specific delta added to a_t before simulation step); delta-action trained for ankles (4 DoF) explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Low-level motor transmission dynamics, motor bandwidth limits, actuator friction/backlash, compliance and detailed motor electrical dynamics are not explicitly modeled (treated implicitly via PD controller + data-driven delta corrections); no explicit reporting of modeled damping, Coulomb friction, or torque limits in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>Medium-fidelity physics simulation (IsaacGym for training, MuJoCo used as a higher-fidelity proxy) with PD-level joint actuation and data-driven cluster-specific delta-action models trained on real robot rollouts to compensate for unmodeled dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success Rate (SR), Mean Per Joint Position Error (MPJPE), Mean Per Keypoint Position Error (MPKPE). Reported numerical values: BB MuJoCo SR = 66.84% (BB IsaacGym SR = 89.58%); iterative expert SR across iterations: Iter0 = 51.49%, Iter1 = 60.33%, Iter2 = 70.37%; cluster ablation (Table 5): Jump — Expert Init 59.64% | Expert Gen Final 50.71% | Expert Final 68.92%; Stand-up — Expert Init 64.11% | Expert Gen Final 75.85% | Expert Final 77.32%; Walk-slow — Expert Init 15.71% | Expert Gen Final 42.32% | Expert Final 56.50%.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>No single aggregate numeric sim-to-real gap reported for final deployment; however, simulator-to-simulator gap shown: IsaacGym -> MuJoCo SR drop for BB from 89.58% to 66.84% (≈22.7 percentage points). Real-world results are reported qualitatively and via real-robot rollouts used to train deltas (visual improvement across iterations shown), but no overall numeric SR on the real robot is provided in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Yes — two relevant comparisons reported: (1) Simulator comparison: policies evaluated in IsaacGym (training sim) and MuJoCo (higher-fidelity proxy), showing lower SR in MuJoCo (BB: 89.58% -> 66.84%). (2) Delta-model comparison: cluster-specific (expert) delta-action models outperform a single general delta-action model for several clusters (notably Jump), i.e., Expert Final > Expert Gen Final for Jump and Walk-slow; iterative delta refinement increases SR across iterations (51.49% -> 70.37%).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Domain randomization was used (detailed setups summarized in Appendix B.2 / Table 9), but exact randomized parameters and ranges are not given in the provided main text; domain randomization applied to environment/dynamics parameters during training (exact lists/ranges not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Humanoid Unitree G1 robot — 29 DoF total, 23 actively controlled joints (wrist joints excluded).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>A single unified/general delta-action model struggled on some motion clusters (notably Jump) due to distributional shifts and imbalanced data across motion types; failures without delta-action led to unstable foot landings and deployment breakdowns (Iter0), improved by iterative delta fine-tuning; lack of high-precision global localization (no GPS/VIO/precise IMU) noted as a limitation that can introduce alignment bias.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Cluster-specialized, data-driven delta-action models (trained from real rollouts per motion-cluster) substantially improve sim-to-real transfer for contact-rich, high-dynamics whole-body motions compared to a single unified delta model; low-level actuation was modeled at the PD position-control level (no explicit bandwidth/friction/backlash modeling), and the combination of PD-level control plus cluster-specific delta corrections is sufficient to compensate many unmodeled actuator dynamics — however, performance remains sensitive to motion distribution (e.g., jumping requires specialized modeling) and to the fidelity gap between training and evaluation simulators.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills <em>(Rating: 2)</em></li>
                <li>Sim-to-real learning for humanoid box loco-manipulation <em>(Rating: 2)</em></li>
                <li>Policy transfer with strategy optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1927",
    "paper_id": "paper-279403551",
    "extraction_schema_id": "extraction-schema-41",
    "extracted_data": [
        {
            "name_short": "BB (BumbleBee)",
            "name_full": "BumbleBee: Expert-to-Generalist Whole-Body Control for Humanoid Robots",
            "brief_description": "An expert-to-generalist pipeline that trains cluster-specific motion tracking experts and cluster-specific delta-action models in simulation, refines them with real-world rollouts on a Unitree G1 humanoid, and distills experts into a Transformer-based general whole-body controller to improve sim-to-real transfer for a wide range of whole-body motions (jumping, walking, in-place motions).",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "Whole-body motion tracking of humanoid behaviors (jump, walk-slow, walk-fast, stand-up, stand-mid, stand-low)",
            "task_timescale": null,
            "task_contact_ratio": "contact-rich (frequent foot-ground contacts across tasks)",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "Position-level PD control (policy outputs target joint positions for a PD controller for 23 actuated joints); low-level control loop at 200 Hz; policy inference at 50 Hz; delta-action corrections applied to action outputs (cluster-specific delta added to a_t before simulation step); delta-action trained for ankles (4 DoF) explicitly.",
            "actuator_parameters_simplified": "Low-level motor transmission dynamics, motor bandwidth limits, actuator friction/backlash, compliance and detailed motor electrical dynamics are not explicitly modeled (treated implicitly via PD controller + data-driven delta corrections); no explicit reporting of modeled damping, Coulomb friction, or torque limits in main text.",
            "fidelity_level_description": "Medium-fidelity physics simulation (IsaacGym for training, MuJoCo used as a higher-fidelity proxy) with PD-level joint actuation and data-driven cluster-specific delta-action models trained on real robot rollouts to compensate for unmodeled dynamics.",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Success Rate (SR), Mean Per Joint Position Error (MPJPE), Mean Per Keypoint Position Error (MPKPE). Reported numerical values: BB MuJoCo SR = 66.84% (BB IsaacGym SR = 89.58%); iterative expert SR across iterations: Iter0 = 51.49%, Iter1 = 60.33%, Iter2 = 70.37%; cluster ablation (Table 5): Jump — Expert Init 59.64% | Expert Gen Final 50.71% | Expert Final 68.92%; Stand-up — Expert Init 64.11% | Expert Gen Final 75.85% | Expert Final 77.32%; Walk-slow — Expert Init 15.71% | Expert Gen Final 42.32% | Expert Final 56.50%.",
            "sim_vs_real_performance": "No single aggregate numeric sim-to-real gap reported for final deployment; however, simulator-to-simulator gap shown: IsaacGym -&gt; MuJoCo SR drop for BB from 89.58% to 66.84% (≈22.7 percentage points). Real-world results are reported qualitatively and via real-robot rollouts used to train deltas (visual improvement across iterations shown), but no overall numeric SR on the real robot is provided in the main text.",
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "Yes — two relevant comparisons reported: (1) Simulator comparison: policies evaluated in IsaacGym (training sim) and MuJoCo (higher-fidelity proxy), showing lower SR in MuJoCo (BB: 89.58% -&gt; 66.84%). (2) Delta-model comparison: cluster-specific (expert) delta-action models outperform a single general delta-action model for several clusters (notably Jump), i.e., Expert Final &gt; Expert Gen Final for Jump and Walk-slow; iterative delta refinement increases SR across iterations (51.49% -&gt; 70.37%).",
            "domain_randomization_used": true,
            "domain_randomization_details": "Domain randomization was used (detailed setups summarized in Appendix B.2 / Table 9), but exact randomized parameters and ranges are not given in the provided main text; domain randomization applied to environment/dynamics parameters during training (exact lists/ranges not reported).",
            "robot_type": "Humanoid Unitree G1 robot — 29 DoF total, 23 actively controlled joints (wrist joints excluded).",
            "transfer_failure_analysis": "A single unified/general delta-action model struggled on some motion clusters (notably Jump) due to distributional shifts and imbalanced data across motion types; failures without delta-action led to unstable foot landings and deployment breakdowns (Iter0), improved by iterative delta fine-tuning; lack of high-precision global localization (no GPS/VIO/precise IMU) noted as a limitation that can introduce alignment bias.",
            "key_finding_for_theory": "Cluster-specialized, data-driven delta-action models (trained from real rollouts per motion-cluster) substantially improve sim-to-real transfer for contact-rich, high-dynamics whole-body motions compared to a single unified delta model; low-level actuation was modeled at the PD position-control level (no explicit bandwidth/friction/backlash modeling), and the combination of PD-level control plus cluster-specific delta corrections is sufficient to compensate many unmodeled actuator dynamics — however, performance remains sensitive to motion distribution (e.g., jumping requires specialized modeling) and to the fidelity gap between training and evaluation simulators.",
            "uuid": "e1927.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real learning for humanoid box loco-manipulation",
            "rating": 2
        },
        {
            "paper_title": "Policy transfer with strategy optimization",
            "rating": 1
        }
    ],
    "cost": 0.010758499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots
September 1, 2025</p>
<p>Yuxuan Wang 
Peking University</p>
<p>Ming Yang 
Peking University</p>
<p>Ziluo Ding 
Peking University</p>
<p>Yu Zhang 
Peking University</p>
<p>Weishuai Zeng 
Peking University</p>
<p>Xinrun Xu 
Peking University</p>
<p>Haobin Jiang 
Peking University</p>
<p>Zongqing Lu 
Peking University</p>
<p>From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots
September 1, 20251D97986ABD355B80B3A25E3E3235D93FarXiv:2506.12779v3[cs.RO]
2 BeingBeyond https://beingbeyond.github.io/BumbleBee/Start 0s End 135s Figure 1: A general whole-body control policy in the real world tracks the motion of two consecutive longduration actions, with a total duration of approximately 135 seconds.A boxing sequence appears at the top of the image, with a full Charleston dance shown at the bottom.</p>
<p>Introduction</p>
<p>Humanity has long dreamed of using humanoid robots to assist or replace humans in completing various daily tasks, including grabbing objects [1][2][3][4][5], carrying goods [6][7][8], etc.At present, a single agile whole-body humanoid skill can be trained to a remarkably high level of performance.The ASAP [9] framework first trains motion-specific tracking policies in simulation, then refines them using real-world data through motion-specific dynamic policy learning.This framework enables the robot to effectively bridge the sim-to-real gap and achieve a wide repertoire of agile and expressive behaviors.However, it is hard to directly apply this framework to general motion tracking, as different motions require the robot to focus on different aspects of control.For example, aggressive actions like jumping or fast walking demand precise, high-torque control, while conservative motions prioritize balance and smoothness.These differences lead to mismatched data distributions across motion types.It can lead to conflicting gradients during training, ultimately degrading the performance of the policy.</p>
<p>Intuitively, to mitigate the issues caused by mismatched data distributions, there are two main directions.One is at the model level (Be more powerful): employing more expressive architectures such as Transformers or Diffusion Models [10][11][12][13], which are capable of capturing complex motion distributions across diverse skills.The other is at the data level (Decompose the complexity): introducing structure into the training data, such as clustering motions by type or difficulty, balancing data across motion categories, or using curriculum learning strategies to gradually expose the policy to increasingly diverse behaviors [14].These approaches aim to reduce interference between conflicting samples and improve the generalization capability of the learned policy.Inspired by the success of mixture of experts (MoE) [15,16] in LLMs, we follow the second path to build a general whole-body control policy.</p>
<p>To this end, we propose BumbleBee (BB), an expert-generalist pipeline that enables a general agile humanoid whole-body controller.Since both tracking and dynamic policies benefit from training on behaviorally similar motions, the dataset is first clustered using an autoencoder (AE) [17] trained on kinematic features, including additional leg-related features, and motion text descriptions.This design reflects the fact that different leg movements, such as walking, jumping, or crouching, require distinct torque control patterns, making leg dynamics a key factor in categorizing motion.A general tracking policy is initially trained on the full dataset and then fine-tuned within each cluster to obtain specialized expert policies.These experts are deployed on the real robot to collect real-world trajectories.For each cluster, a delta action model is trained to compensate for sim-to-real discrepancies by modeling the difference between simulated and real-world state transitions.The expert policies are further refined using the corresponding delta model in an iterative manner.Finally, a unified generalist is obtained through a combination of expert knowledge distillation.</p>
<p>To summarize, our main contributions are as follows: 1) We propose an expert-to-generalist framework for training agile whole-body control policies, which effectively mitigates interference across diverse motion types.</p>
<p>2) We introduce an auto-regressive clustering method that leverages leg-specific motion features and text descriptions to structure the dataset, enabling better specialization of control policies.3) We demonstrate that our method, BumbleBee, achieves superior control performance in both simulation and real-world experiments, outperforming baselines in terms of agility, robustness, and generalization.</p>
<p>Related Work</p>
<p>Humanoid Whole-Body Control</p>
<p>Whole-body control (WBC) [18][19][20][21][22][23][24][25] is essential for high DoF humanoid robots to perform coordinated and stable movements in complex tasks.Recent advances in reinforcement learning have enabled robots to acquire diverse whole-body skills, yet training a general WBC policy remains challenging due to hardware constraints and the inherent complexity of the robotic action space.</p>
<p>Previous works address this in different ways: the H2O series [26,27] incorporates global keypoint positions but suffers from error accumulation in long-horizon tasks; HumanPlus [28] uses only joint angles for better realworld adaptability; Hover [29] introduces observation masking for flexible control mode switching.However, these methods are all trained on the full AMASS [30] dataset, where diverse motion distributions may cause gradient conflicts and reduce generalization.Exbody2 [14] attempts to mitigate this through progressive learning based on task difficulty.</p>
<p>In contrast, we propose a clustered expert-to-generalist paradigm.By classifying motion types and training specialized expert policies on each cluster before distillation, our approach reduces cross-task interference and improves both training efficiency and generalization.</p>
<p>Sim-to-Real Transfer</p>
<p>The sim-to-real transfer challenge primarily arises from two factors: (1) inaccuracies in robot modeling, including structural simplifications and parameter errors, and (2) the complex, nonlinear, and time-varying dynamics of real-world environments, e. g. contact events, friction variations, and elastic deformations, that exceed the fidelity of modern physics simulators.</p>
<p>Traditional approaches address this gap through System Identification (SysID) [31][32][33][34][35], which uses real-world observations to calibrate simulator parameters for improved physical realism.Domain Randomization offers an alternative by perturbing environment parameters during training to enhance policy robustness and generalization.</p>
<p>ASAP proposes a different strategy by learning an action delta model to directly compensate for simulationreality discrepancies using real-world data.Building on this idea, our method extends the action delta model by training it separately for each motion cluster.Compared to a single unified model, this cluster-specific approach enables improved sim-to-real performance across a diverse range of motions.</p>
<p>Method</p>
<p>Training a general whole-body control policy directly is challenging, primarily due to conflicts among diverse motion types.These motions differ significantly in their control objectives, for example, jumping requires high-torque control, whereas in-place movement emphasizes balance and continuity.To address this challenge, we take a new perspective on organizing motion data and policy learning by introducing an expert-to-generalist framework as illustrated in Figure 2. Rather than training a single policy on a heterogeneous dataset, we first partition the data based on the kinematic characteristics of each motion type, thereby reducing cross-task interference and enabling more targeted learning.A person jogs with right head lift.</p>
<p>A person squats in place.</p>
<p>General Motion Tracking Policy</p>
<p>Step 2. Experts</p>
<p>Dataset Curation</p>
<p>Human Motion Retargeting</p>
<p>Following prior work [26], we first retarget SMPL-format [36] human motion sequences from the AMASS dataset into robot-specific representations, including global translations and joint axis-angle rotations.Given that AMASS contains a wide range of motions, such as crawling and climbing, we perform an additional data cleaning step to ensure quality.We apply PHC [37] to filter the dataset [9], resulting in 8,179 high-quality trajectories for clustering and training.</p>
<p>AE Clustering</p>
<p>We leverage intermediate representations generated within a self-supervised framework, i. e. an autoencoder, to cluster motion data from the entire dataset.We aim to categorize the data based on motion types, for example, grouping jumping movements into one cluster and in-place movements into another.</p>
<p>Our method not only relies solely on autoencoders to reconstruct full motion sequences, but also incorporates textual annotations to align motions at the semantic level.This ensures that motions with diverse patterns but shared semantics are positioned closer in the latent space.For example, the act of walking may take the form of linear movement or circular paths.These patterns would be hard to associate with based only on motion alignment.However, things can be different when textual semantic alignment is introduced.Note that the text information for training is from the open-source HumanML3D [38] dataset, which offers textual annotations and corresponding frame ranges for most of the sequences in AMASS.</p>
<p>In addition, our method does not reconstruct motion sequences represented by the SMPL format.Since SMPL primarily contains joint angles and root transformations, it fails to represent the kinematic dynamics essential for distinguishing motion types.To address this, we first apply forward kinematics to convert joint rotations and root positions into 3D coordinates of joints in the world frame.Subsequently, we prune redundant joints and introduce foot velocity relative to the world frame, enhancing the model's capacity to differentiate between motion types such as jumping, standing, and walking.</p>
<p>For the encoding process, we adopt a Transformer for motion encoding, following the previous work [39,40].The motion encoder takes as input a motion sequence M full = {p t , r t , ṙt , c t , v feet t } T t=1 and outputs a motion representation z m , where p t ∈ R N ×3 represents the 3D positions of all joints, r t ∈ R 3 is the root translation in 3D space, ṙt ∈ R 3 is the root velocity, c t ∈ {0, 1} F denotes the binary contact states of F feet, and v feet t ∈ R F ×3 represents the 3D velocities of the F feet.Textual data l is first processed through a BERT model [41] for serialization and then passed through a Transformer to obtain a latent representation z l with the same dimensionality as z m .</p>
<p>The decoding module uses the same Transformer as the motion encoder, but with different input and output dimensions.The reconstruction focuses only on a subset of key joints (i.e. head, pelvis, hands, and feet), allowing the model to concentrate on the core motion features.Specifically, it reconstructs the same features of the motion inputs from the latent representation z l or z m , except that some redundant joint 3D positions mentioned above are removed.The loss function is defined as:
L cluster = L InfoNCE (z l , z m ) + L 2 (z l , z m ) + L huber ( M l , M ) + L huber ( M m , M ),
where z l and z m denote the intermediate latent variables for the text and motion modalities, respectively.L huber refers to the Huber loss function, M represents key features selected from the ground-truth motion sequence M full .M l and M m are the reconstructed features from the text and motion modalities, respectively.The first two terms are used to align the motion latent space with the semantic latent space, and the last two terms are reconstruction losses to train the autoencoder.Finally, we apply the K-means algorithm to cluster the latent variables of all motion data, generated by the learned motion encoder.</p>
<p>Experts</p>
<p>To improve motion specialization and sim-to-real transfer, we introduce expert policies, i. e. motion tracking and delta action policies, trained on motion clusters derived from AE embeddings.All models in this part are implemented using a three-layer multilayer perceptron (MLP) and are trained via reinforcement learning (RL).More details of reward design can be found in the Appendix B.</p>
<p>Motion Tracking Training</p>
<p>Initially, a general motion tracking policy is trained on the entire dataset as a base model.Note that both joint angles and selected keypoint positions of the tracking/reference motion are included in the policy's observation.More details about the observation design of our framework are included in the Appendix A.</p>
<p>Instead of training from scratch, all expert motion tracking policies are derived from the base model.This is because we expect each expert to retain some generalization capability for other motion clusters.Each expert policy is then fine-tuned on a specific motion cluster, allowing the policy to specialize in a behaviorally consistent cluster of skills, e. g. walking, standing, or jumping.This fine-tuning significantly improves motion fidelity and effectively resolves training conflicts caused by diverse motion types.</p>
<p>Multi-Stage Delta Action Training</p>
<p>To further overcome the sim2real gap for tracking policy, we adopt the delta action fine-tuning framework.By applying the delta action on the simulator dynamics and continuing to fine-tune the tracking policy within this modified environment, the training process effectively approximates the training directly in the real world.</p>
<p>Our key contribution lies in specializing this framework by training expert delta models for each motion cluster, rather than relying on a single unified model.</p>
<p>In more detail, each expert motion tracking policy is deployed on the real robot to collect its corresponding motion trajectories in the real-world environment.At each timestep t, using the onboard sensors of the 29-DoF Unitree G1 robot ( the vector of joint positions q t ∈ R 23 , and joint velocities qt ∈ R 23 .For each expert, we randomly sample several dozen reference motions for repeated execution, resulting in the collection of over a hundred real-world trajectories in total.Subsequently, we further train expert delta action models based on the data collected for each expert following ASAP [9].We find that, due to the consistent dynamic features within each cluster of motion, this expert-specific training significantly improves the fitting accuracy of the delta action models and enables more efficient correction of the sim-to-real gap.Compared to a general delta action model, the expert models exhibit notable advantages in motion compensation accuracy and overall control performance.</p>
<p>With the learned delta action models, π ∆ (s t , a t ), we reconstruct the simulation environment as follows, s t+1 = f sim (s t , a t + π ∆ (s t , a t )) and fine-tune the pretrained expert motion tracking policies within this modified environment.This process can be performed iteratively until both kinds of expert policies converge.</p>
<p>Generalist</p>
<p>After optimizing the expert policies, we employ knowledge distillation to integrate the knowledge of each expert policy and generate a general whole-body control policy.We adopt DAgger [42] to implement multi-experts distillation.The distillation loss function is defined as:
L distill = E s∼D KL p general (a | s) ∥ p expert,k(s) (a | s)
where E s∼D denotes the expectation over the states s in the training dataset D, KL(•∥•) is the Kullback-Leibler divergence, p expert,k(s) is the expert policy corresponding to state s, and p general is the general policy.</p>
<p>However, we observed that the capacity of the three-layer MLP is limited, making it insufficient to effectively learn behaviors of multiple expert policies.To address this issue, we adopt a more expressive architecture, the Transformer, to serve as the backbone for the final general policy model.The Transformer enables better modeling of complex patterns across diverse states, allowing for more effective fusion of expert knowledge.More details of the architecture can be found in the Appendix C.</p>
<p>Experiment</p>
<p>Experiment Setup</p>
<p>Our models are trained in IsaacGym.Since there is a large gap between IsaacGym and the real world, MuJoCo serves as a more reliable proxy for evaluating the capability of the model.As a result, most of our evaluations are conducted in MuJoCo.We assess performance using the filtered AMASS dataset described in Section 3.For real-world testing, we further include long-range motions to evaluate the generalization and tracking capabilities of BumbleBee (BB).All training and deployment are performed on the Unitree G1 robot, which has 29 DoF, 23 of which are actively controlled, excluding the wrist joints.</p>
<p>Baselines.</p>
<p>To evaluate the effectiveness of our method, we compare it with three state-of-the-art (SOTA) methods: OmniH2O [26], Exbody2 [14], and Hover [29].For the fair comparison, we either use the officially Table 2: Statistics for each cluster.We measured the motion sequences in terms of the displacement distance (Displacement), the movement speed (Speed), the average displacement along the z-axis (Z-Move), the average per-step axis-angle change of all joints (Joint), as well as the joint variations of the upper and lower body (Lower and Upper Joint).released code or closely follow the official implementation when code is unavailable, adapting each method to the Unitree G1 robot (instead of H1/H1-2).In addition, we keep the training dataset consistent with that used for BB.For Hover, we use unmasked observations during the evaluation to ensure optimal performance.</p>
<p>Metrics.We assess performance using three key metrics: Success Rate (SR), Mean Per Joint Position Error (MPJPE), and Mean Per Keypoint Position Error (MPKPE).SR reflects the overall capability of the policy.MPJPE measures its accuracy in tracking retargeted joint angles, while MPKPE evaluates the precision of tracking retargeted keypoints in the world coordinate frame.Among all metrics, SR is the most critical, as it reflects the overall viability and stability of the policy.Other metrics, such as MPJPE and MPKPE, are only meaningful when the policy can successfully complete the task, as indicated by a high SR.Detailed definitions of these metrics can be found in Appendix A.</p>
<p>Cluster Analysis</p>
<p>How do we determine the number of clusters?</p>
<p>We determine the number of clusters using the Elbow Method [43], which identifies the trade-off point between using fewer clusters and minimizing the within-cluster sum of squares (total square distance from all points to their respective cluster centers).Based on this analysis, we set the number of clusters to six, as shown in Figure 3.   2 summarizes the kinematic features of the six clusters.The first cluster has the highest Z-Move value (average displacement along the z-axis), and is therefore categorized as the jump cluster.The second and third clusters exhibit the largest ranges of displacement and are classified as walking behaviors.However, the third cluster demonstrates higher speed compared to the second.The remaining three clusters primarily consist of in-place movements, based on their lower speeds and smaller displacement ranges.More specifically, the fourth cluster involves larger upper-body movements, the sixth is characterized by more prominent lower-body movements, and the fifth falls between the two.From the semantic perspective, we extracted the top keywords from each cluster and find that their meanings closely align with the corresponding kinematic characteristics, demonstrating that the clusters are semantically meaningful.For convenience, all clusters are named exactly as specified in Table 2.</p>
<p>Whether the clustering results are meaningful in terms of both kinematics and semantics? Table</p>
<p>Generalist Performance</p>
<p>Does our expert-to-generalist framework demonstrate superiority over existing methods?</p>
<p>We analyze the performance of BB compared to existing SOTA methods in both IsaacGym and MuJoCo simulators.</p>
<p>As shown in Table 1, BB outperforms all baselines across nearly all evaluation metrics.In the IsaacGym, BB shows clear advantages over other methods in terms of success rate, MPJPE, and MPKPE.However, the performance gap becomes even more pronounced in MuJoCo, a more realistic simulator that better reflects real-world dynamics.In this setting, BB achieves a success rate of 66.84%, significantly outperforming Exbody2 (50.19%) and all other baselines, which fall below 40%.This highlights the strong generalization capability of BB.BB effectively integrates the specialized strengths of multiple experts into a unified policy, enabling stable and accurate execution of whole-body motions across different domains.</p>
<p>Whether the clustering is beneficial for training?</p>
<p>To explore this question, we conduct two ablation studies.The first baseline, General Init, directly trains a generalist policy without any expert specialization.The second, Random, trains a generalist policy based on experts derived from six randomly partitioned subsets of the data.As shown in Table 3, the Random offers no clear advantage over General Init, highlighting the limited benefit of naive data partitioning.The superior performance of BB demonstrates that reasonable clustering plays a crucial role in policy learning.Even within randomly divided subsets, conflicting motion patterns may impede effective RL exploration.</p>
<p>Expert Performance</p>
<p>Do expert policies outperform the generalist within their respective motion clusters?</p>
<p>To answer this question, we evaluate the performance of expert policies.In Figure 4, we show performance improvements across four motion clusters during multi-stage delta action training, with results for the remaining clusters included in Appendix D. Specifically, we compare success rates in MuJoCo across four variants: the generalist policy without expert specialization (General Init), expert policies before delta action fine-tuning (Expert Init), expert policies after two fine-tuning rounds (Expert Final), and the final distilled generalist (General Final).</p>
<p>From these results, we make several key observations.First, training expert policies on motion clusters leads to stronger task-specific performance compared to the General Init, validating the effectiveness of our autoencoder-based clustering and the importance of specialization.Second, experts maintain a degree of generalization to motions outside their clusters, likely due to being initialized from the General Init.Third, delta action fine-tuning significantly improves each expert's performance.Surprisingly, the final generalist (General Final), distilled from all experts, sometimes outperforms the individual experts, i. e. jump and   walk-slow.These motions are initially difficult to execute stably.We hypothesize that the generalist benefits from inheriting stable control behaviors from multiple experts, allowing it to execute challenging motions more reliably.</p>
<p>How does iterative delta fine-tuning affect the motion tracking policy?</p>
<p>We adopt an iterative approach to the optimization process, based on the intuition that each refinement improves the tracking policy, which enables the collection of higher-quality real-world data, thereby allowing the delta action policy to be further enhanced and, in turn, further improve the tracking policy.To validate this, we evaluate policy performance across three training stages in MuJoCo and also verify the results on a real humanoid robot.</p>
<p>As shown in Table 4, the mean success rate steadily increases from 51.49% (Iter 0 ) to 60.33% (Iter 1 ), and further to 70.37% (Iter 2 ), clearly demonstrating the effectiveness of iterative refinement.While additional iterations could potentially yield even better results, our experiments are constrained by limited computational resources.</p>
<p>We take stand-low as an example to provide a more intuitive understanding.As shown in Figure 5, without delta action fine-tuning (Iter 0 ), the policy fails to stabilize the foot, resulting in landing failure and deployment breakdown.After the first iteration (Iter 1 ), landing stability improves significantly, although the robot still struggles to lift its feet and exhibits visible trembling.After the second iteration (Iter 2 ), the policy can track the motion smoothly and maintain balance.More detailed visualizations can be found in the Appendix D.</p>
<p>Whether the delta action model needs to be trained in a specialized manner?</p>
<p>To address this question, we conduct an ablation study using all collected real-world data to train a general delta action model.The results are summarized in Table 5, where Expert Gen Final represents the final expert after fine-tuning on the general delta model.The general delta action model is trained using all real-world data collected in each iteration loop.</p>
<p>We observe that the general delta model improves performance in two motion clusters but struggles significantly on Jump.This aligns with our hypothesis that delta action learning is sensitive to distributional shifts across motion types, making the single general delta action model less effective in highly diverse categories.Furthermore, we find that the general model is harder to train and less stable compared to its specialized counterparts.It is evident that the data distribution across the entire AMASS dataset is imbalanced.To balance the distribution for the general strategy, we ensure that each category is equally represented with a ratio of 1/6 during the distillation process.</p>
<p>Statistical Tests</p>
<p>To better support our experimental results, we have supplemented Table 12 with the complete confidence intervals (CI) test of the different methods.</p>
<p>up only one time.</p>
<p>Figure 2 :
2
Figure 2: Overview of the BumbleBee framework.The left section illustrates the data curation stage, which includes motion retargeting and PHC-based filtering.The upper middle part shows the autoencoder-based clustering process, where motions are grouped via their semantic and kinematic characteristics.The lower section depicts the iterative delta fine-tuning of expert policies for each cluster.Finally, as shown on the far right, all experts are distilled into a single general WBC policy using a Transformer-based architecture.</p>
<p>-Cluster Sum of Squares) (Inertia ×10 6 ) Elbow Method for Optimal K</p>
<p>Figure 3 :
3
Figure3: Elbow method showing the trade-off between cluster number and within-cluster sum of squares, with K = 6 selected as our selected point.</p>
<p>Figure 4 :
4
Figure 4: Evaluation of expert vs. generalist models in MuJoCo, measured by success rate.</p>
<p>Figure 5 :
5
Figure 5: Visualization of expert performance across iterations in the real world.We deploy the stand-low policies from three training iterations to perform the same right-stepping motion on a real robot, and observe a clear improvement in foot stability with each iteration.</p>
<p>Table 1 :
1
23DoF actively controlled, excluding the wrist joints), we record the following: base linear velocity v base Main results evaluated in IsaacGym and MuJoCo.We assess performance using three key metrics: Success Rate (SR), Mean Per Joint Position Error (MPJPE), and Mean Per Keypoint Position Error (MPKPE).BB demonstrates superiority over baselines.
IsaacGymMuJoCoMethodSR↑MPKPE↓MPJPE↓SR↑MPKPE↓MPJPE↓OmniH2O [26]85.65%87.830.263015.64%360.960.4601Exbody2 [14]86.63%86.660.293750.19%272.420.3576Hover [29]63.21%105.840.279216.12%323.080.3428BumbleBee (BB)89.58%83.300.190766.84%294.270.2356
t ∈ R 3 , base orientation as a quaternion α base t ∈ R 4 , base angular velocity ω base t ∈ R 3 ,</p>
<p>Table 3 :
3
Comparison of BB with a general policy trained without experts and one trained on experts from randomly split clusters, evaluated by success rate.
IsaacGymMuJoCoGeneral Init88.69%33.01%Random86.25%35.36%BB89.58%66.84%</p>
<p>Table 4 :
4
Mean success rates of experts evaluated on their respective motion clusters.The evaluation is in MuJoCo.
Iter 0Iter 1Iter 2SR 51.49% 60.33% 70.37%</p>
<p>Table 5 :
5
Ablation Results of the General Delta Action Model on three clusters.Expert Gen Final represents the final expert after fine-tuning on the general delta model.
ClusterExpert Init Expert Gen Final Expert FinalJump59.64%50.71%68.92%Stand-up64.11%75.85%77.32%Walk-slow15.71%42.32%56.50%</p>
<p>Table 6 :
6
Magnitudes of delta action outputs for the ankle joint across different clusters.Higher values indicate greater motion adjustments.
JumpWalk-slowWalk-fastYaw0.21070.28650.3399Roll0.05890.09560.0843Stand-up Stand-mid Stand-lowYaw0.15560.20980.2599Roll0.02780.05090.1273</p>
<p>Table 6 ,
6
Stand-up exhibits the smallest delta values, which is expected given that its motions are largely static.In contrast, Walk-fast shows the highest delta magnitudes due to its inherently large motion amplitudes.These distributional shifts across clusters introduce conflicts when attempting to train a single delta action model, further highlighting the need for cluster-specific modeling.We present BB, a framework for training agile and general whole-body control policies on humanoid robots.It follows an expert-to-generalist framework and uses auto-regressive encoder to cluster motions by semantics and leg dynamics, enabling effective specialization and knowledge transfer.Extensive experiments show that BB outperforms baselines in agility, robustness, and generalization, highlighting its potential for real-world deployment.
5 Conclusion</p>
<p>Table 12 :
12
Main results with confidence interval (CI) statistics reported over samples on a single reference trajectory.Clustering We clustered a total of the following number of data samples for each category: Jump -351, Stand Low -229, Walk Slow -3355, Stand Mid -578, Stand Up -2378, and Walk Fast -307.
IsaacGymMuJoCoMethodSR↑MPKPE↓MPJPE↓SR↑MPKPE↓MPJPE↓OmniH2O [26]85.65% (1.114%)87.83 (0.6389)0.2630 (0.0009)15.64% (1.408%)360.96 (6.619)0.4601 (0.0010)Exbody2 [14]86.63% (1.106%)86.66 (0.6013)0.2937 (0.0010)50.19% (1.517%)272.42 (7.029)0.3576 (0.0011)Hover [29]63.21% (1.451%)105.84 (0.9350)0.2792 (0.0009)16.12% (1.491%)323.08 (6.919)0.3428 (0.0010)BumbleBee (BB)89.58% (0.946%)83.30 (1.1414)0.1907 (0.0008)66.84% (1.262%)294.27 (7.923)0.2356 (0.0011)
Limitation Currently, BB does not utilize additional high-precision localization sensors such as GPS or Visual-Inertial Odometry (VIO)[44].As a result, it lacks access to global positioning information, which may introduce biases when aligning with the reference motion sequence.We believe that equipping BB with additional sensors like a high-precision IMU could significantly improve its performance in real-world scenarios by providing more accurate and reliable pose estimation.Moreover, the complexity of the overall pipeline in BB constrains its scalability, particularly when integrating real-world training feedback.Appendix A Environment DetailsA.1 RL environmentWe provide a detailed training and test environment setting in this subsection.Observation For Privileged observation, we use proprioception, including linear velocity, angular velocity, joint position, joint velocity, and last action, and task-relevant observation, including target joint positions, target keypoint positions, target root translations, and target root rotations in the global coordinates.For student policies, we use all proprioception observation, except for linear velocity.For task-relevant information, we only preserve target joint positions, root translation, and root rotations in the local coordinates.For teacher policy, we take observations from 5 timesteps as input, and for student policies, we take observations from 10 timesteps as input.For the delta action policy, we use the full proprioception of the teacher policy mentioned above, as well as the tracking policy actions.Note that we don't use the global information like root position and keypoint positions.ActionWe use roportional derivative (PD) controller to control the 23 DoF of the G1 (totally 29 DoF).And the policy outputs are the target joint position for PD controller.Termination In addition to falls, we added an additional termination condition during the training and testing process, where the position of the keypoints must not exceed a threshold.During training, the threshold is set from 0.8 down to 0.3 using curriculum learning.During testing, a threshold of 0.8 is used for walking, and 0.4 is used for the other tasks.A.2 DeploymentThe policy runs at an inference frequency of 50 Hz.The low-level interface operates at 200 Hz, ensuring smooth real-time control.Communication between the control policy and the low-level interface is facilitated via Lightweight Communications and Marshalling (LCM).A.3 MetricsSuccess Rate (SR).SR measures whether a policy successfully completes a rollout in the test environment.A rollout is considered successful if the agent does not fall and is able to follow the reference motion with reasonable accuracy.Merely maintaining balance without tracking the reference trajectory is treated as a failure, since such behavior deviates substantially from the intended motion.Mean Per Joint Position Error (MPJPE). MPJPE quantifies the average discrepancy between the predicted and target positions of all joints:where N is the number of joints, Ĵi ∈ R 3 denotes the target position of the i-th joint, and J i ∈ R 3 is the corresponding predicted position.The unit of measurement is radians.Mean Per Keypoint Position Error (MPKPE). MPKPE evaluates the average error between predicted and target positions of body keypoints:where K is the number of keypoints, Ki ∈ R 3 represents the target position of the i-th keypoint, and K i ∈ R 3 is the predicted position.The unit of measurement is millimeters.B Training DetailsB.1 Reward DesignWe have listed the rewards used for training the WBC policy and the Delta Action model separately in Table7and Table8, respectively.It is worth noting that when training the Delta Action model, compared to the rewards in ASAP, we used the translation of the root position rather than the positions of all body joints.This is because we did not use a motion capture system, but instead relied on odometry.B.2 Domain RandomizationDetailed domain randomization setups are summarized in Table9.B.3 RL HyperparametersThe RL training progress is aligned with standard PPO[45].We provide the detailed training hyperparameters in Table10.We also list the hyperparameters used during the distillation process in Table11.B.4 Delta ActionFor each cluster in each iteration, we randomly sample 20 deployable motions and perform 8 rollouts in the real world.Similar to ASAP, we only train the 4 DoF of the ankles.The average duration per motion isB.5 Training ResourceWe used two desktop computers for training.Each was equipped with an Intel i9-13900 CPU, an NVIDIA RTX 4090 GPU, and 64 GB of RAM for policy training.The model integrates attention mechanisms with GRU-based gating to enhance memory retention and capture long-range temporal dependencies, making it particularly effective for sequential control tasks in reinforcement learning.The controller takes as input a sequence of 10 consecutive observations and processes them through one Transformer block.Each block employs six attention heads and has a hidden size of 128 and an embedding dimension of 204.The memory length is maintained at 10 to preserve temporal context across sequences.C Model DetailsD Additional ResultsExpert Comparison As shown in Figure6, we visualized the comparison between generalists and specialists across six types of clusters.The same trend can still be observed in the remaining two clusters (Stand Mid and Walk Fast).In both of these two clusters, the policy of the specialists outperforms that of the generalist.However, the final generalist still retains favorable properties and significantly outperforms the initial generalist.
Deep learning for detecting robotic grasps. Ian Lenz, Honglak Lee, Ashutosh Saxena, The International Journal of Robotics Research. 344-52015</p>
<p>Robotic grasping using deep reinforcement learning. Shirin Joshi, Sulabh Kumra, Ferat Sahin, 2020 IEEE 16th International Conference on Automation Science and Engineering (CASE). IEEE2020</p>
<p>Graspit! a versatile simulator for robotic grasping. T Andrew, Peter K Miller, Allen, IEEE Robotics &amp; Automation Magazine. 1142004</p>
<p>Vision-based robotic grasping from object localization, object pose estimation to grasp estimation for parallel grippers: a review. Guoguang Du, Kai Wang, Shiguo Lian, Kaiyong Zhao, Artificial Intelligence Review. 5432021</p>
<p>Unigrasp: Learning a unified model to grasp with multifingered robotic hands. Lin Shao, Fabio Ferreira, Mikael Jorda, Varun Nambiar, Jianlan Luo, Eugen Solowjow, Juan Aparicio Ojea, Oussama Khatib, Jeannette Bohg, IEEE Robotics and Automation Letters. 522020</p>
<p>Coordination of multiple mobile robots in an object carrying task using implicit communication. Guilherme, Bruno S Pereira, Luiz Pimentel, Mário Fm Chaimowicz, Campos, Proceedings 2002 IEEE International Conference on Robotics and Automation. 2002 IEEE International Conference on Robotics and AutomationIEEE20021Cat. No. 02CH37292</p>
<p>Sim-to-real learning for humanoid box loco-manipulation. Jeremy Dao, Helei Duan, Alan Fern, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Human-humanoid collaborative carrying. Don Joven Agravante, Andrea Cherubini, Alexander Sherikov, Pierre-Brice Wieber, Abderrahmane Kheddar, IEEE Transactions on Robotics. 3542019</p>
<p>Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbabu, Chaoyi Pan, Zeji Yi, Guannan Qu, Kris Kitani, Jessica Hodgins, " Linxi, Yuke Jim" Fan, Changliu Zhu, Guanya Liu, Shi, arXiv:2502.01143Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills. 2025arXiv preprint</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Conference on Robot Learning. PMLR2023</p>
<p>Transformer-based deep imitation learning for dual-arm robot manipulation. Heecheol Kim, Yoshiyuki Ohmura, Yasuo Kuniyoshi, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Humanoid locomotion as next token prediction. Ilija Radosavovic, Jathushan Rajasegaran, Baifeng Shi, Bike Zhang, Sarthak Kamat, Koushil Sreenath, Trevor Darrell, Jitendra Malik, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Mazeyu Ji, Xuanbin Peng, Fangchen Liu, Jialong Li, Ge Yang, Xuxin Cheng, Xiaolong Wang, arXiv:2412.13196Exbody2: Advanced expressive humanoid whole-body control. 2024arXiv preprint</p>
<p>Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean, arXiv:1701.06538Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. 2017arXiv preprint</p>
<p>Gshard: Scaling giant models with conditional computation and automatic sharding. Dmitry Lepikhin, Hyoukjoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen, 2020</p>
<p>Reducing the dimensionality of data with neural networks. Geoffrey E Hinton, Ruslan R Salakhutdinov, science. 31357862006</p>
<p>The mit humanoid robot: Design, motion planning, and control for acrobatic behaviors. Matthew Chignoli, Donghyun Kim, Elijah Stanger-Jones, Sangbae Kim, 2020 IEEE-RAS 20th International Conference on Humanoid Robots (Humanoids). 2021</p>
<p>Whole-body geometric retargeting for humanoid robots. Kourosh Darvish, Yeshasvi Tirupachuri, Giulio Romualdi, Lorenzo Rapetti, Diego Ferigo, Francisco , Javier Andrade Chavez, Daniele Pucci, 2019 IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids). 2019</p>
<p>Synchronized human-humanoid motion imitation. Antonin Dallard, Mehdi Benallegue, Fumio Kanehiro, Abderrahmane Kheddar, IEEE Robotics and Automation Letters. 872023</p>
<p>Anymal -a highly mobile and dynamic quadrupedal robot. Marco Hutter, Christian Gehring, Dominic Jud, Andreas Lauber, C Dario Bellicoso, Vassilios Tsounis, Jemin Hwangbo, Karen Bodie, Peter Fankhauser, Michael Bloesch, Remo Diethelm, Samuel Bachmann, Amir Melzer, Mark Hoepflinger, 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2016</p>
<p>The 3d linear inverted pendulum mode: A simple modeling for a biped walking pattern generation. Shuuji Kajita, Fumio Kanehiro, Kenji Kaneko, Kazuhito Yokoi, Hirohisa Hirukawa, Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium. 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next MillenniumIEEE20011Cat. No. 01CH37180</p>
<p>Whole-body control of humanoid robots. L Federico, Luis Moro, Sentis, Humanoid robotics: a reference. 2019</p>
<p>Amo: Adaptive motion optimization for hyper-dexterous humanoid whole-body control. Jialong Li, Xuxin Cheng, Tianshu Huang, Shiqi Yang, Rizhao Qiu, Xiaolong Wang, Robotics: Science and Systems. 2025. 2025</p>
<p>Ziluo Ding, Haobin Jiang, Yuxuan Wang, Zhenguo Sun, Yu Zhang, Xiaojie Niu, Ming Yang, Weishuai Zeng, Xinrun Xu, Zongqing Lu, Jaeger, Dual-level humanoid whole-body controller. 2025</p>
<p>Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris Kitani, Changliu Liu, Guanya Shi, Omnih2o: Universal and dexterous human-to-humanoid whole-body teleoperation and learning. 2024</p>
<p>Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, Guanya Shi, arXiv:2403.04436Learning human-to-humanoid real-time whole-body teleoperation. 2024arXiv preprint</p>
<p>Humanplus: Humanoid shadowing and imitation from humans. Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, Chelsea Finn, Conference on Robot Learning (CoRL). 2024</p>
<p>Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia Xu, Zhenyu Jiang, Changliu Liu, Guanya Shi, Xiaolong Wang, arXiv:2410.21229Linxi Fan, and Yuke Zhu. Hover: Versatile neural whole-body controller for humanoid robots. 2024arXiv preprint</p>
<p>AMASS: Archive of motion capture as surface shapes. Naureen Mahmood, Nima Ghorbani, F Nikolaus, Gerard Troje, Michael J Pons-Moll, Black, International Conference on Computer Vision. October 2019</p>
<p>System identification-a survey. Johan Karl, Peter Åström, Eykhoff, Automatica. 721971</p>
<p>System identification techniques. F Kozin, Natke, Structural safety. 19863</p>
<p>Agile continuous jumping in discontinuous terrains. Yuxiang Yang, Guanya Shi, Changyi Lin, Xiangyun Meng, Rosario Scalise, Mateo Guaman Castro, Wenhao Yu, Tingnan Zhang, Ding Zhao, Jie Tan, arXiv:2409.109232024arXiv preprint</p>
<p>Wenhao Yu, Karen Liu, Greg Turk, arXiv:1810.05751Policy transfer with strategy optimization. 2018arXiv preprint</p>
<p>Learning fast adaptation with meta strategy optimization. Wenhao Yu, Jie Tan, Yunfei Bai, Erwin Coumans, Sehoon Ha, IEEE Robotics and Automation Letters. 522020</p>
<p>SMPL: A skinned multi-person linear model. Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, Michael J Black, ACM Trans. Graphics (Proc. SIGGRAPH Asia). 34616October 2015</p>
<p>Perpetual humanoid control for real-time simulated avatars. Zhengyi Luo, Jinkun Cao, Alexander W Winkler, Kris Kitani, Weipeng Xu, International Conference on Computer Vision (ICCV). 2023</p>
<p>Generating diverse and natural 3d human motions from text. Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, Li Cheng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2022</p>
<p>TMR: Text-to-motion retrieval using contrastive 3D human motion synthesis. Mathis Petrovich, Michael J Black, Gül Varol, International Conference on Computer Vision (ICCV). 2023</p>
<p>TEMOS: Generating diverse human motions from textual descriptions. Mathis Petrovich, Michael J Black, Gül Varol, European Conference on Computer Vision (ECCV). 2022</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies. the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies20191</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. Stéphane Ross, Geoffrey Gordon, Drew Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statistics2011JMLR Workshop and Conference Proceedings</p>
<p>Who belongs in the family?. Robert L Thorndike, Psychometrika. 1841953</p>
<p>Robocentric visual-inertial odometry. Zheng Huai, Guoquan Huang, The International Journal of Robotics Research. 4172022</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>